{"pageProps":{"post":{"title":"K8sGPT  Ollama 무료 Kubernetes 자동 진단 솔루션 사용법","description":"","date":"2024-06-23 23:00","slug":"2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution","content":"\n![Kubernetes Automated Diagnosis Tool: k8sgpt-operator](/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png)\n\n주말에 블로그 초고를 확인했더니 이 글이 있었어요. 한 해 전 'Kubernetes Automated Diagnosis Tool: k8sgpt-operator'를 쓸 때의 기억이 떠오르네요. 처음에는 K8sGPT + LocalAI를 써보려 했지만, Ollama로 시도해보니 더 사용하기 편리했어요. 게다가 Ollama는 OpenAI API를 지원하기도 해서 Ollama로 바꾸기로 결정했죠.\n\nk8sgpt-operator를 소개하는 글을 게시한 후 몇몇 독자들이 OpenAI를 사용하기 위한 높은 진입 장벽을 언급했어요. 이 문제는 정말 어려운 문제이지만 극복할 수 있는 문제에요. 하지만 이 글은 그 문제를 해결하는 게 아니라 OpenAI 대안인 Ollama를 소개하기 위한 글이에요. 작년 말에 k8sgpt는 CNCF Sandbox에 들어갔어요.\n\n# 1. Ollama 설치하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Ollama](/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_1.png)\n\nOllama은 로컬이나 클라우드에서 쉽게 설치하고 실행할 수 있는 여러 대형 모델을 지원하는 오픈 소스 대형 모델 도구입니다. 매우 사용하기 편리하며 간단한 명령어로 실행할 수 있습니다. macOS에서는 homebrew를 사용하여 다음 명령어로 쉽게 설치할 수 있습니다:\n\n```js\nbrew install ollama\n```\n\n최신 버전은 0.1.44입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nollama -v\n경고: 실행 중인 Ollama 인스턴스에 연결할 수 없습니다\n경고: 클라이언트 버전은 0.1.44입니다\n```\n\n리눅스에서는 공식 스크립트로도 설치할 수 있습니다.\n\n```js\ncurl -sSL https://ollama.com/install.sh | sh\n```\n\nOllama를 시작하고 컨테이너나 K8s 클러스터에서 접근할 수 있도록 환경 변수를 통해 수신 주소를 0.0.0.0으로 설정하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nOLLAMA_HOST=0.0.0.0 ollama start\n```\n\n```js\n...\ntime=2024-06-16T07:54:57.329+08:00 level=INFO source=routes.go:1057 msg=\"127.0.0.1:11434에서 수신 대기 중 (버전 0.1.44)\"\ntime=2024-06-16T07:54:57.329+08:00 level=INFO source=payload.go:30 msg=\"임베디드 파일 추출 중\" dir=/var/folders/9p/2tp6g0896715zst_bfkynff00000gn/T/ollama1722873865/runners\ntime=2024-06-16T07:54:57.346+08:00 level=INFO source=payload.go:44 msg=\"동적 LLM 라이브러리 [metal]\"\ntime=2024-06-16T07:54:57.385+08:00 level=INFO source=types.go:71 msg=\"추론 계산 중\" id=0 library=metal compute=\"\" driver=0.0 name=\"\" total=\"21.3 GiB\" available=\"21.3 GiB\"\n```\n\n# 2. 큰 모델 다운로드 및 실행하기\n\n4월에 Meta에서 오픈 소스로 공개된 인기 있는 큰 모델 중 하나인 Llama3가 있습니다. Llama3에는 8B와 70B 두 가지 버전이 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n맥OS에서 실행 중이고, 8B 버전을 선택했어요. 8B 버전은 4.7GB이며, 빠른 인터넷 연결로 다운로드하면 3-4분이 소요돼요.\n\n```js\nollama run llama3\n```\n\n제 M1 Pro에서 32GB 메모리를 사용하고 있는데, 실행하는 데 약 12초 정도 걸려요.\n\n```js\ntime=2024-06-17T09:30:25.070+08:00 level=INFO source=server.go:572 msg=\"llama runner started in 12.58 seconds\"\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n각 쿼리마다 약 14초가 소요됩니다.\n\n```js\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n```js\n....\n\"total_duration\":14064009500,\"load_duration\":1605750,\"prompt_eval_duration\":166998000,\"eval_count\":419,\"eval_duration\":13894579000}\n```\n\n# 3. K8sGPT CLI 백엔드 구성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 k8sgpt-operator를 테스트하려면 이 단계를 건너뛸 수 있어요.\n\nk8sgpt의 백엔드로 Ollama REST API를 사용할 거에요. 이 API는 추론 제공자로 기능하며, backend 유형은 localai로 선택했어요. LocalAI는 OpenAI API와 호환되며, 실제 제공자는 여전히 Llama를 실행하는 Ollama일 거예요.\n\n```js\nk8sgpt auth add --backend localai --model llama3 --baseurl http://localhost:11434/v1\n```\n\n이를 기본 제공자로 설정하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nk8sgpt auth default --provider localai\nlocalai로 기본 제공자가 설정되었습니다.\n\n테스트 중:\n\nimage-not-exist 이미지를 사용하여 k8s 내에서 Pod를 생성합니다.\n\nkubectl get po k8sgpt-test\n이름          준비     상태         다시 시작     나이\nk8sgpt-test   0/1     ErrImagePull   0          6초\n\n<!-- ui-station 사각형 -->\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n에러를 분석하려면 k8sgpt를 사용해보세요.\n\nk8sgpt analyze --explain --filter=Pod --namespace=default --output=json\n\n{\n  \"provider\": \"localai\",\n  \"errors\": null,\n  \"status\": \"ProblemDetected\",\n  \"problems\": 1,\n  \"results\": [\n    {\n      \"kind\": \"Pod\",\n      \"name\": \"default/k8sgpt-test\",\n      \"error\": [\n        {\n          \"Text\": \"Back-off pulling image \\\"image-not-exist\\\"\",\n          \"KubernetesDoc\": \"\",\n          \"Sensitive\": []\n        }\n      ],\n      \"details\": \"Error: Back-off pulling image \\\"image-not-exist\\\"\\n\\nSolution: \\n1. Check if the image exists on Docker Hub or your local registry.\\n2. If not, create the image using a Dockerfile and build it.\\n3. If the image exists, check the spelling and try again.\\n4. Verify the image repository URL in your Kubernetes configuration file (e.g., deployment.yaml).\",\n      \"parentObject\": \"\"\n    }\n  ]\n}\n\n# 4. k8sgpt-operator 배포 및 설정하기\n\n<!-- ui-station 사각형 -->\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nk8sgpt-operator은 클러스터 내에서 k8sgpt를 자동화할 수 있습니다. Helm을 사용하여 쉽게 설치할 수 있어요.\n\n```\n\nhelm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt --create-namespace\n\nk8sgpt-operator는 K8sGPT를 구성하고 분석 결과를 출력하는 Result를 위한 두 가지 CRD를 제공합니다.\n\nkubectl api-resources | grep -i gpt\nk8sgpts core.k8sgpt.ai/v1alpha1 true K8sGPT\nresults core.k8sgpt.ai/v1alpha1 true Result\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nOllama의 IP 주소를 baseUrl로 사용하여 K8sGPT를 구성하세요.\n\n```js\nkubectl apply -n k8sgpt -f - << EOF\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-ollama\nspec:\n  ai:\n    enabled: true\n    model: llama3\n    backend: localai\n    baseUrl: http://198.19.249.3:11434/v1\n  noCache: false\n  filters: [\"Pod\"]\n  repository: ghcr.io/k8sgpt-ai/k8sgpt\n  version: v0.3.8\nEOF\n```\n\nK8sGPT CR을 생성한 후, 연산자(operator)가 이를 위한 파드를 자동으로 만듭니다. result CR을 확인하면 동일한 결과가 표시됩니다.\n\n```js\nkubectl get result -n k8sgpt -o jsonpath='{.items[].spec}' | jq .\n{\n  \"backend\": \"localai\",\n  \"details\": \"Error: Kubernetes is unable to pull the image \\\"image-not-exist\\\" due to it not existing.\\n\\nSolution: \\n1. Check if the image actually exists.\\n2. If not, create the image or use an alternative one.\\n3. If the image does exist, ensure that the Docker daemon and registry are properly configured.\",\n  \"error\": [\n    {\n      \"text\": \"Back-off pulling image \\\"image-not-exist\\\"\"\n    }\n  ],\n  \"kind\": \"Pod\",\n  \"name\": \"default/k8sgpt-test\",\n  \"parentObject\": \"\"\n}\n```\n","ogImage":{"url":"/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png"},"coverImage":"/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png","tag":["Tech"],"readingTime":9},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png\" alt=\"Kubernetes Automated Diagnosis Tool: k8sgpt-operator\"></p>\n<p>주말에 블로그 초고를 확인했더니 이 글이 있었어요. 한 해 전 'Kubernetes Automated Diagnosis Tool: k8sgpt-operator'를 쓸 때의 기억이 떠오르네요. 처음에는 K8sGPT + LocalAI를 써보려 했지만, Ollama로 시도해보니 더 사용하기 편리했어요. 게다가 Ollama는 OpenAI API를 지원하기도 해서 Ollama로 바꾸기로 결정했죠.</p>\n<p>k8sgpt-operator를 소개하는 글을 게시한 후 몇몇 독자들이 OpenAI를 사용하기 위한 높은 진입 장벽을 언급했어요. 이 문제는 정말 어려운 문제이지만 극복할 수 있는 문제에요. 하지만 이 글은 그 문제를 해결하는 게 아니라 OpenAI 대안인 Ollama를 소개하기 위한 글이에요. 작년 말에 k8sgpt는 CNCF Sandbox에 들어갔어요.</p>\n<h1>1. Ollama 설치하기</h1>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p><img src=\"/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_1.png\" alt=\"Ollama\"></p>\n<p>Ollama은 로컬이나 클라우드에서 쉽게 설치하고 실행할 수 있는 여러 대형 모델을 지원하는 오픈 소스 대형 모델 도구입니다. 매우 사용하기 편리하며 간단한 명령어로 실행할 수 있습니다. macOS에서는 homebrew를 사용하여 다음 명령어로 쉽게 설치할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\">brew install ollama\n</code></pre>\n<p>최신 버전은 0.1.44입니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">ollama -v\n경고: 실행 중인 <span class=\"hljs-title class_\">Ollama</span> 인스턴스에 연결할 수 없습니다\n경고: 클라이언트 버전은 <span class=\"hljs-number\">0.1</span><span class=\"hljs-number\">.44</span>입니다\n</code></pre>\n<p>리눅스에서는 공식 스크립트로도 설치할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\">curl -sSL <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//ollama.com/install.sh | sh</span>\n</code></pre>\n<p>Ollama를 시작하고 컨테이너나 K8s 클러스터에서 접근할 수 있도록 환경 변수를 통해 수신 주소를 0.0.0.0으로 설정하세요.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">OLLAMA_HOST</span>=<span class=\"hljs-number\">0.0</span><span class=\"hljs-number\">.0</span><span class=\"hljs-number\">.0</span> ollama start\n</code></pre>\n<pre><code class=\"hljs language-js\">...\ntime=<span class=\"hljs-number\">2024</span>-<span class=\"hljs-number\">06</span>-16<span class=\"hljs-attr\">T07</span>:<span class=\"hljs-number\">54</span>:<span class=\"hljs-number\">57.329</span>+<span class=\"hljs-number\">08</span>:<span class=\"hljs-number\">00</span> level=<span class=\"hljs-variable constant_\">INFO</span> source=routes.<span class=\"hljs-property\">go</span>:<span class=\"hljs-number\">1057</span> msg=<span class=\"hljs-string\">\"127.0.0.1:11434에서 수신 대기 중 (버전 0.1.44)\"</span>\ntime=<span class=\"hljs-number\">2024</span>-<span class=\"hljs-number\">06</span>-16<span class=\"hljs-attr\">T07</span>:<span class=\"hljs-number\">54</span>:<span class=\"hljs-number\">57.329</span>+<span class=\"hljs-number\">08</span>:<span class=\"hljs-number\">00</span> level=<span class=\"hljs-variable constant_\">INFO</span> source=payload.<span class=\"hljs-property\">go</span>:<span class=\"hljs-number\">30</span> msg=<span class=\"hljs-string\">\"임베디드 파일 추출 중\"</span> dir=<span class=\"hljs-regexp\">/var/</span>folders/9p/2tp6g0896715zst_bfkynff00000gn/T/ollama1722873865/runners\ntime=<span class=\"hljs-number\">2024</span>-<span class=\"hljs-number\">06</span>-16<span class=\"hljs-attr\">T07</span>:<span class=\"hljs-number\">54</span>:<span class=\"hljs-number\">57.346</span>+<span class=\"hljs-number\">08</span>:<span class=\"hljs-number\">00</span> level=<span class=\"hljs-variable constant_\">INFO</span> source=payload.<span class=\"hljs-property\">go</span>:<span class=\"hljs-number\">44</span> msg=<span class=\"hljs-string\">\"동적 LLM 라이브러리 [metal]\"</span>\ntime=<span class=\"hljs-number\">2024</span>-<span class=\"hljs-number\">06</span>-16<span class=\"hljs-attr\">T07</span>:<span class=\"hljs-number\">54</span>:<span class=\"hljs-number\">57.385</span>+<span class=\"hljs-number\">08</span>:<span class=\"hljs-number\">00</span> level=<span class=\"hljs-variable constant_\">INFO</span> source=types.<span class=\"hljs-property\">go</span>:<span class=\"hljs-number\">71</span> msg=<span class=\"hljs-string\">\"추론 계산 중\"</span> id=<span class=\"hljs-number\">0</span> library=metal compute=<span class=\"hljs-string\">\"\"</span> driver=<span class=\"hljs-number\">0.0</span> name=<span class=\"hljs-string\">\"\"</span> total=<span class=\"hljs-string\">\"21.3 GiB\"</span> available=<span class=\"hljs-string\">\"21.3 GiB\"</span>\n</code></pre>\n<h1>2. 큰 모델 다운로드 및 실행하기</h1>\n<p>4월에 Meta에서 오픈 소스로 공개된 인기 있는 큰 모델 중 하나인 Llama3가 있습니다. Llama3에는 8B와 70B 두 가지 버전이 있습니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>맥OS에서 실행 중이고, 8B 버전을 선택했어요. 8B 버전은 4.7GB이며, 빠른 인터넷 연결로 다운로드하면 3-4분이 소요돼요.</p>\n<pre><code class=\"hljs language-js\">ollama run llama3\n</code></pre>\n<p>제 M1 Pro에서 32GB 메모리를 사용하고 있는데, 실행하는 데 약 12초 정도 걸려요.</p>\n<pre><code class=\"hljs language-js\">time=<span class=\"hljs-number\">2024</span>-<span class=\"hljs-number\">06</span>-17<span class=\"hljs-attr\">T09</span>:<span class=\"hljs-number\">30</span>:<span class=\"hljs-number\">25.070</span>+<span class=\"hljs-number\">08</span>:<span class=\"hljs-number\">00</span> level=<span class=\"hljs-variable constant_\">INFO</span> source=server.<span class=\"hljs-property\">go</span>:<span class=\"hljs-number\">572</span> msg=<span class=\"hljs-string\">\"llama runner started in 12.58 seconds\"</span>\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>각 쿼리마다 약 14초가 소요됩니다.</p>\n<pre><code class=\"hljs language-js\">curl <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//localhost:11434/api/generate -d '{</span>\n  <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"llama3\"</span>,\n  <span class=\"hljs-string\">\"prompt\"</span>: <span class=\"hljs-string\">\"Why is the sky blue?\"</span>,\n  <span class=\"hljs-string\">\"stream\"</span>: <span class=\"hljs-literal\">false</span>\n}<span class=\"hljs-string\">'\n</span></code></pre>\n<pre><code class=\"hljs language-js\">....\n<span class=\"hljs-string\">\"total_duration\"</span>:<span class=\"hljs-number\">14064009500</span>,<span class=\"hljs-string\">\"load_duration\"</span>:<span class=\"hljs-number\">1605750</span>,<span class=\"hljs-string\">\"prompt_eval_duration\"</span>:<span class=\"hljs-number\">166998000</span>,<span class=\"hljs-string\">\"eval_count\"</span>:<span class=\"hljs-number\">419</span>,<span class=\"hljs-string\">\"eval_duration\"</span>:<span class=\"hljs-number\">13894579000</span>}\n</code></pre>\n<h1>3. K8sGPT CLI 백엔드 구성하기</h1>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>만약 k8sgpt-operator를 테스트하려면 이 단계를 건너뛸 수 있어요.</p>\n<p>k8sgpt의 백엔드로 Ollama REST API를 사용할 거에요. 이 API는 추론 제공자로 기능하며, backend 유형은 localai로 선택했어요. LocalAI는 OpenAI API와 호환되며, 실제 제공자는 여전히 Llama를 실행하는 Ollama일 거예요.</p>\n<pre><code class=\"hljs language-js\">k8sgpt auth add --backend localai --model llama3 --baseurl <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//localhost:11434/v1</span>\n</code></pre>\n<p>이를 기본 제공자로 설정하세요.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">k8sgpt auth <span class=\"hljs-keyword\">default</span> --provider localai\nlocalai로 기본 제공자가 설정되었습니다.\n\n테스트 중:\n\nimage-not-exist 이미지를 사용하여 k8s 내에서 <span class=\"hljs-title class_\">Pod</span>를 생성합니다.\n\nkubectl get po k8sgpt-test\n이름          준비     상태         다시 시작     나이\nk8sgpt-test   <span class=\"hljs-number\">0</span>/<span class=\"hljs-number\">1</span>     <span class=\"hljs-title class_\">ErrImagePull</span>   <span class=\"hljs-number\">0</span>          <span class=\"hljs-number\">6</span>초\n\n&#x3C;!-- ui-station 사각형 -->\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">ins</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"adsbygoogle\"</span>\n<span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">\"display:block\"</span>\n<span class=\"hljs-attr\">data-ad-client</span>=<span class=\"hljs-string\">\"ca-pub-4877378276818686\"</span>\n<span class=\"hljs-attr\">data-ad-slot</span>=<span class=\"hljs-string\">\"7249294152\"</span>\n<span class=\"hljs-attr\">data-ad-format</span>=<span class=\"hljs-string\">\"auto\"</span>\n<span class=\"hljs-attr\">data-full-width-responsive</span>=<span class=\"hljs-string\">\"true\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">ins</span>></span></span>\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">script</span>></span><span class=\"javascript\">\n(adsbygoogle = <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">adsbygoogle</span> || []).<span class=\"hljs-title function_\">push</span>({});\n</span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">script</span>></span></span>\n\n에러를 분석하려면 k8sgpt를 사용해보세요.\n\nk8sgpt analyze --explain --filter=<span class=\"hljs-title class_\">Pod</span> --namespace=<span class=\"hljs-keyword\">default</span> --output=json\n\n{\n  <span class=\"hljs-string\">\"provider\"</span>: <span class=\"hljs-string\">\"localai\"</span>,\n  <span class=\"hljs-string\">\"errors\"</span>: <span class=\"hljs-literal\">null</span>,\n  <span class=\"hljs-string\">\"status\"</span>: <span class=\"hljs-string\">\"ProblemDetected\"</span>,\n  <span class=\"hljs-string\">\"problems\"</span>: <span class=\"hljs-number\">1</span>,\n  <span class=\"hljs-string\">\"results\"</span>: [\n    {\n      <span class=\"hljs-string\">\"kind\"</span>: <span class=\"hljs-string\">\"Pod\"</span>,\n      <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"default/k8sgpt-test\"</span>,\n      <span class=\"hljs-string\">\"error\"</span>: [\n        {\n          <span class=\"hljs-string\">\"Text\"</span>: <span class=\"hljs-string\">\"Back-off pulling image \\\"image-not-exist\\\"\"</span>,\n          <span class=\"hljs-string\">\"KubernetesDoc\"</span>: <span class=\"hljs-string\">\"\"</span>,\n          <span class=\"hljs-string\">\"Sensitive\"</span>: []\n        }\n      ],\n      <span class=\"hljs-string\">\"details\"</span>: <span class=\"hljs-string\">\"Error: Back-off pulling image \\\"image-not-exist\\\"\\n\\nSolution: \\n1. Check if the image exists on Docker Hub or your local registry.\\n2. If not, create the image using a Dockerfile and build it.\\n3. If the image exists, check the spelling and try again.\\n4. Verify the image repository URL in your Kubernetes configuration file (e.g., deployment.yaml).\"</span>,\n      <span class=\"hljs-string\">\"parentObject\"</span>: <span class=\"hljs-string\">\"\"</span>\n    }\n  ]\n}\n\n# <span class=\"hljs-number\">4.</span> k8sgpt-operator 배포 및 설정하기\n\n&#x3C;!-- ui-station 사각형 -->\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">ins</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"adsbygoogle\"</span>\n<span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">\"display:block\"</span>\n<span class=\"hljs-attr\">data-ad-client</span>=<span class=\"hljs-string\">\"ca-pub-4877378276818686\"</span>\n<span class=\"hljs-attr\">data-ad-slot</span>=<span class=\"hljs-string\">\"7249294152\"</span>\n<span class=\"hljs-attr\">data-ad-format</span>=<span class=\"hljs-string\">\"auto\"</span>\n<span class=\"hljs-attr\">data-full-width-responsive</span>=<span class=\"hljs-string\">\"true\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">ins</span>></span></span>\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">script</span>></span><span class=\"javascript\">\n(adsbygoogle = <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">adsbygoogle</span> || []).<span class=\"hljs-title function_\">push</span>({});\n</span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">script</span>></span></span>\n\nk8sgpt-operator은 클러스터 내에서 k8sgpt를 자동화할 수 있습니다. <span class=\"hljs-title class_\">Helm</span>을 사용하여 쉽게 설치할 수 있어요.\n\n</code></pre>\n<p>helm repo add k8sgpt <a href=\"https://charts.k8sgpt.ai/\" rel=\"nofollow\" target=\"_blank\">https://charts.k8sgpt.ai/</a>\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt --create-namespace</p>\n<p>k8sgpt-operator는 K8sGPT를 구성하고 분석 결과를 출력하는 Result를 위한 두 가지 CRD를 제공합니다.</p>\n<p>kubectl api-resources | grep -i gpt\nk8sgpts core.k8sgpt.ai/v1alpha1 true K8sGPT\nresults core.k8sgpt.ai/v1alpha1 true Result</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>Ollama의 IP 주소를 baseUrl로 사용하여 K8sGPT를 구성하세요.</p>\n<pre><code class=\"hljs language-js\">kubectl apply -n k8sgpt -f - &#x3C;&#x3C; <span class=\"hljs-variable constant_\">EOF</span>\n<span class=\"hljs-attr\">apiVersion</span>: core.<span class=\"hljs-property\">k8sgpt</span>.<span class=\"hljs-property\">ai</span>/v1alpha1\n<span class=\"hljs-attr\">kind</span>: K8sGPT\n<span class=\"hljs-attr\">metadata</span>:\n  <span class=\"hljs-attr\">name</span>: k8sgpt-ollama\n<span class=\"hljs-attr\">spec</span>:\n  <span class=\"hljs-attr\">ai</span>:\n    <span class=\"hljs-attr\">enabled</span>: <span class=\"hljs-literal\">true</span>\n    <span class=\"hljs-attr\">model</span>: llama3\n    <span class=\"hljs-attr\">backend</span>: localai\n    <span class=\"hljs-attr\">baseUrl</span>: <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//198.19.249.3:11434/v1</span>\n  <span class=\"hljs-attr\">noCache</span>: <span class=\"hljs-literal\">false</span>\n  <span class=\"hljs-attr\">filters</span>: [<span class=\"hljs-string\">\"Pod\"</span>]\n  <span class=\"hljs-attr\">repository</span>: ghcr.<span class=\"hljs-property\">io</span>/k8sgpt-ai/k8sgpt\n  <span class=\"hljs-attr\">version</span>: v0<span class=\"hljs-number\">.3</span><span class=\"hljs-number\">.8</span>\n<span class=\"hljs-variable constant_\">EOF</span>\n</code></pre>\n<p>K8sGPT CR을 생성한 후, 연산자(operator)가 이를 위한 파드를 자동으로 만듭니다. result CR을 확인하면 동일한 결과가 표시됩니다.</p>\n<pre><code class=\"hljs language-js\">kubectl get result -n k8sgpt -o jsonpath=<span class=\"hljs-string\">'{.items[].spec}'</span> | jq .\n{\n  <span class=\"hljs-string\">\"backend\"</span>: <span class=\"hljs-string\">\"localai\"</span>,\n  <span class=\"hljs-string\">\"details\"</span>: <span class=\"hljs-string\">\"Error: Kubernetes is unable to pull the image \\\"image-not-exist\\\" due to it not existing.\\n\\nSolution: \\n1. Check if the image actually exists.\\n2. If not, create the image or use an alternative one.\\n3. If the image does exist, ensure that the Docker daemon and registry are properly configured.\"</span>,\n  <span class=\"hljs-string\">\"error\"</span>: [\n    {\n      <span class=\"hljs-string\">\"text\"</span>: <span class=\"hljs-string\">\"Back-off pulling image \\\"image-not-exist\\\"\"</span>\n    }\n  ],\n  <span class=\"hljs-string\">\"kind\"</span>: <span class=\"hljs-string\">\"Pod\"</span>,\n  <span class=\"hljs-string\">\"name\"</span>: <span class=\"hljs-string\">\"default/k8sgpt-test\"</span>,\n  <span class=\"hljs-string\">\"parentObject\"</span>: <span class=\"hljs-string\">\"\"</span>\n}\n</code></pre>\n</body>\n</html>\n"},"__N_SSG":true}