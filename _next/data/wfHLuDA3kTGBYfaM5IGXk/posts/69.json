{"pageProps":{"posts":[{"title":"도커 대 Podman 안전한 오케스트레이션의 새 시대","description":"","date":"2024-05-23 14:14","slug":"2024-05-23-DockervsPodmanANewErainSecureOrchestration","content":"\n탐구하는 Root vs Rootless Orchestration: 보안 관점에서\n\n안녕하세요, 기술 애호가 여러분! 😊 오늘은 컨테이너 오케스트레이션의 매혹적인 세계로 빠져들어보겠습니다. 이 도구들이 나오기 전에는 개발자들이 수동 배포의 고통, 표준화 부족 (내 컴퓨터에서는 동작하는)으로 인한 복잡하고 오류가 발생하기 쉬운 과정을 겪어야 했습니다. 이러한 기술을 개발한 사람들에게 이해와 감사의 마음을 전해봅시다.\n\n이를 염두에 두고, 우리의 관심을 보안 오케스트레이션의 세계에서 뜨거운 반향을 일으키는 새로운 도구인 Podman으로 돌려봅시다. 이 도구는 특히 안전한 오케스트레이션 분야에서 Docker와 10년간 사랑받아온 기존 강자에 도전하고 있습니다 💪. 이 흥미진진한 발전에 대해 더 깊이 파헤치기 위해 기대해 주세요!\n\n# 🚀 컨테이너 이해: 왜 필요한가부터 어떻게 하는가까지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너는 코드, 런타임, 라이브러리 및 시스템 설정을 포함한 응용 프로그램 실행에 필요한 모든 것이 포함된 독립 실행 가능한 패키지입니다.\n\n이는 응용 프로그램이 어디에서 시작하더라도 동일하게 실행된다는 것을 의미합니다. 즉, 당신의 랩탑, 클라우드 서버 또는 동료의 컴퓨터 등 어디에서든 실행할 수 있습니다. 이 일관성은 '내 컴퓨터에서는 작동하는데'라는 오랜 문제를 해결합니다.\n\n컨테이너 오케스트레이션의 핵심은 컨테이너 실행 환경이며, 컨테이너 생성, 관리 및 실행에 도움을 줍니다.\n\n- 컨테이너가 시작되면 실행 환경은 저장소에서 지정된 컨테이너 이미지를 요청합니다. 이 이미지는 응용 프로그램 및 종속성에 대한 청사진 역할을 합니다.\n- 실행 환경은 Linux 네임스페이스를 사용하여 안전하고 분리된 환경을 제공하여 시스템 리소스(CPU, 메모리, 디스크, 네트워크 등)에 대한 독특한 이해를 제공합니다.\n- Linux 커널의 컨트롤 그룹(cgroups)은 리소스 공정한 분배를 보장하며 어떤 컨테이너도 리소스를 독차지하거나 시스템 성능을 저하시키지 않습니다.\n- 한 번에 한 컨테이너가 격리되면 실행 환경은 해당 환경 내에서 프로그램을 실행하여 호스트 시스템과 쉽게 통신합니다.\n- 데몬 프로세스로 작동하는 컨테이너 실행 환경 도구는 리눅스 커널과 상호작용하여 컨테이너를 관리하며, 관리를 위해 루트 액세스가 필요합니다. 이 상호작용은 효율적인 컨테이너 관리에 중요합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n잘 알려진 컨테이너 런타임 중에는 Docker, k8s, nerdctl 등에서 사용되는 containerd와 cri-o가 있습니다.\n\n# 🏆 도커의 지배 속에서 Podman의 부상\n\n<img src=\"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png\" />\n\n컨테이너 관리 세계에서 인기가 Podman으로 변화되고 있으며, 그 이유에는 몇 가지가 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Rootless Architecture: 도커와는 달리 루트 액세스가 있는 데몬 프로세스로 작동하는 Podman과 달리, Podman은 루트리스 접근 방식을 채택합니다. 이 기본적인 차이가 Podman의 인기 증진에 상당한 기여를 합니다.\n- 보안 취약점: Docker의 루트 액세스는 파일을 읽고 프로그램을 설치하며 애플리케이션을 편집하는 등 컨테이너를 관리할 수 있게 합니다. 그러나 이는 시스템에 보안 취약점을 도입하여 헤커들로 하여금 유혹적인 대상이 되게 합니다.\n- 해커들의 타깃: 해커가 데몬을 compromise하는 데 성공하면 민감한 데이터에 접근하거나 악성 코드를 실행하거나 컨테이너 구성을 변경하거나 시스템 전체를 다운시킬 가능성이 있습니다.\n- SELinux로 보강된 보안: Docker와는 다르게 Podman은 각 컨테이너를 Security-Enhanced Linux (SELinux) 레이블과 함께 시작하여 보안을 강화합니다.\n- 다른 도구에 의존: 루트리스 접근 방식으로 Podman은 컨테이너를 직접 관리하지 않습니다. 대신 이 아래서 설명하는 다른 도구들을 사용하여 컨테이너 관리를 수행합니다.\n- Buildah: OCI (Open Container Initiative) 호환 컨테이너를 빌드하는 데 사용되는 오픈 소스 리눅스 기반 도구입니다. Buildah는 전체 컨테이너 런타임이나 데몬을 설치하지 않고도 컨테이너를 생성하고 관리할 수 있습니다.\n- Skopeo: 컨테이너 이미지 및 이미지 레지스트리를 사용하여 다양한 작업을 수행하기 위한 명령줄 유틸리티입니다. 전체 이미지를 다운로드하지 않고 원격 레지스트리의 이미지를 검사할 수 있어 컨테이너 작업에 대한 가벼운 솔루션입니다.\n- Systemd: Podman은 설정된 컨테이너 런타임을 호출하여 실행 중인 컨테이너를 생성합니다. 그렇지만 전용 데몬이 없는 Podman은 시스템 및 서비스 관리자인 systemd를 사용하여 업데이트를 수행하고 컨테이너를 백그라운드에서 유지합니다.\n\n# 🔒 Podman의 보안에 대한 행동: Docker에 대한 안전한 대체품\n\n<img src=\"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_1.png\" />\n\n- 주어진 시나리오에서 세 명의 리눅스 사용자인 Bob, Dawg, BadBoy가 생성되었습니다. Bob과 Dawg는 Podman을 사용하여 컨테이너를 생성하며, 이러한 컨테이너들은 각 사용자 네임스페이스 내의 리소스에만 액세스할 수 있습니다. 이러한 설정은 각 컨테이너의 액세스를 해당하는 네임스페이스로 제한하여 보안을 강화합니다.\n- BadBoy는 Docker를 사용하며 루트 액세스를 가지고 있어 호스트 시스템의 모든 리소스에 대한 가시성을 허용합니다. 네임스페이스 밖에 있는 리소스까지도 볼 수 있어 시스템에 잠재적인 공격 가능성을 노출시킵니다. 이에 반해 루트리스 아키텍처인 Podman은 사용자 개별 네임스페이스에만 액세스 권한을 제한하여 보안을 강화합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Podman 설정\n\nPodman을 설정하는 실용적인 예제에 대해 알아보겠습니다. macOS에서 설정을 진행할 것이지만 필요에 따라 다른 환경에서 설정하는 방법에 대한 해당 문서를 참조할 수 있습니다.\n\n- Podman 설치: Homebrew를 이용하여 Podman을 설치하려면 brew install podman을 실행하세요.\n- Podman Machine 초기화: Podman 머신을 초기화하려면 podman machine init을 사용하세요.\n- Podman-Compose 설치: Docker Compose를 Podman으로 실행하는 스크립트인 Podman-Compose를 설치하려면 brew install podman-compose을 사용하세요.\n- Podman-Desktop 설치: Podman에 대한 Docker Desktop과 유사한 경험을 제공하는 Podman-Desktop을 설치하려면 brew install podman-desktop을 사용하세요.\n- Podman 세부 정보 확인: 마지막으로, podman info를 사용하여 Podman의 설치 및 구성 세부사항을 확인할 수 있습니다. 아래는 중요한 몇 가지 필드가 강조된 예시입니다.\n\n```js\nhost:\n  arch: amd64\n  buildahVersion: 1.32.0\n  cgroupControllers:\n  - cpu\n  - io\n  - memory\n  - pids\n  cgroupManager: systemd\n  cgroupVersion: v2\n  ociRuntime:\n    name: crun\n    package: crun-1.12-1.fc39.x86_64\n    path: /usr/bin/crun\n    version: |-\n      crun version 1.12\n      commit: ce429cb2e277d001c2179df1ac66a470f00802ae\n      rundir: /run/user/501/crun\n      spec: 1.0.0\n      +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\n  os: linux\n  security:\n    apparmorEnabled: false\n    capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\n    rootless: true\n    seccompEnabled: true\n    seccompProfilePath: /usr/share/containers/seccomp.json\n    selinuxEnabled: true\n  serviceIsRemote: true\nregistries:\n  search:\n  - docker.io\nversion:\n  APIVersion: 4.7.2\n  Built: 1698762721\n  BuiltTime: Tue Oct 31 20:02:01 2023\n  GitCommit: \"\"\n  GoVersion: go1.21.1\n  Os: linux\n  OsArch: linux/amd64\n  Version: 4.7.2\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Podman의 세부 정보를 확인하고 이미지를 검사하며 실행 중인 컨테이너를 관리하는 데 Podman CLI도 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_2.png)\n\n- 아래 제공된 Docker Compose 파일을 사용하여 컨테이너를 시작하려면 다음 명령을 실행하세요: podman compose up -d\n\n```yaml\nservices:\n  postgres:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: podman-psql\n      POSTGRES_USER: podman-psql-user\n      POSTGRES_PASSWORD: podman-pass\n    ports:\n      - \"5432:5432\"\n\n  redis:\n    image: \"redis:6.0.14\"\n    restart: always\n    command: redis-server\n    ports:\n      - \"6379:6379\"\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다음은 Podman 데스크톱 내에서 실행 중인 컨테이너와 이미지를 검사할 수 있습니다.\n\n![image1](/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_3.png)\n\n![image2](/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_4.png)\n\n# 마지막으로\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n디지턈 시대에는 보안이 매우 중요합니다. 침입이 발생하면 심각한 결과를 가져올 수 있습니다. Docker와 Podman은 각각 강점과 약점을 가지고 있습니다. Podman은 안전한 오케스트레이션의 기초를 바탕으로 만들어졌지만 Docker와 같은 기능(예: Docker Swarm)이 부족할 수 있습니다. 반면 Docker는 사용 편의성을 강조하지만 보안 측면에서는 미흡하다고 여겨집니다.\n\n이 토론이 유익했고 안전한 오케스트레이션에 대한 이해력을 높일 수 있었기를 바랍니다. 이 정보가 유용했다면 더 많은 글을 읽고 싶다면 저를 팔로우해주세요. 즐거운 학습되세요! 🚀\n\n# 참고 자료\n\n- Podman이란? (redhat.com)\n- Podman 설치 | Podman\n- Docker를 대체할 도구 및 그 이유 | mkdev의 프로그래밍 글\n- Alfresco와 함께 Podman 사용하기 — Alfresco Hub\n","ogImage":{"url":"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png"},"coverImage":"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png","tag":["Tech"],"readingTime":8},{"title":"러스트 배우기 11부  빌더와 데이터베이스 상호작용","description":"","date":"2024-05-23 14:11","slug":"2024-05-23-LearningRustPart11BuildersandDatabaseInteraction","content":"\n다음 시리즈로 넘어가보겠습니다; 이 부분에서는 데이터 구조에 빌더 패턴을 구현하는 방법을 살펴보겠습니다. 그런 다음 sqlx와 Postgres를 사용한 데이터베이스 상호작용으로 넘어가겠습니다.\n\n![이미지](/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png)\n\n# Rust 시리즈\n\n부분 1 — 기본 개념\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPart 2 — 메모리\n\nPart 3 — 흐름 제어와 함수\n\nPart 4 — 옵션/결과 및 컬렉션\n\nPart 5 — 트레이트, 제네릭 및 클로저\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제 6부 — 매크로, 반복자 및 파일 처리\n\n제 7부 — 스레드 공유 상태 및 채널\n\n제 8부 — Cargo, 크레이트, 모듈 및 라이브러리\n\n제 9부 — 명령행 인수, 워크스페이스 및 테스팅\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제 10부 — 상자 포인터 및 웹 앱\n\n제 11부 — 빌더와 데이터베이스 상호작용 (이 기사)\n\n# 소개\n\n이것은 러스트 학습 시리즈의 열한 번째 섹션입니다. 이번에는 러스트에서 빌더 패턴을 다룰 것입니다. 이 공통된 패턴은 구조체를 안전하고 투명하게 초기화하는 좋은 방법입니다. 다음으로, 우리는 포스트그레스와 SQLX 프레임워크를 사용하여 데이터베이스에서 CRUD 작업을 수행하는 방법을 살펴볼 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 준비물\n\n이 글의 데이터베이스 부분을 위한 유일한 준비물은 Rust와 Cargo가 설치되어 있고 시스템에 Docker가 설치되어 있는 것입니다. 만약 Docker를 가지고 있지 않지만 로컬 Postgres DB가 이미 설치되어 있거나 다른 서버의 DB에 액세스할 수 있다면 Docker Postgres 설정을 건너뛰고 DB에 연결하기 위한 연결 속성만 수정하면 됩니다.\n\n# 빌더 패턴\n\n빌더 패턴은 복잡한 객체의 구성을 해당 표현에서 분리하는 디자인 패턴입니다. 이 패턴을 사용하면 유효성 검사를 수행하고 기본값으로 대체하며 값을 부분적으로 할당한 후에 항목을 생성할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nRust로 그 구현하는 방법을 살펴볼 거예요. 이것이 우리의 코드입니다.\n\n```js\n#[derive(Debug)]\nstruct ChargingSession {\n    id: String,\n    watts: u32,\n    vin: String,\n}\n\nstruct ChargingSessionBuilder {\n    id: String,\n    watts: Option<u32>,\n    vin: Option<String>,\n}\n\nimpl ChargingSessionBuilder {\n    fn new(id: &str) -> ChargingSessionBuilder {\n        ChargingSessionBuilder {\n            id: id.to_string(),\n            watts: None,\n            vin: None,\n        }\n    }\n\n    fn watts(mut self, watts: u32) -> ChargingSessionBuilder {\n        self.watts = Some(watts);\n        self\n    }\n\n    fn vin(mut self, vin: &str) -> ChargingSessionBuilder {\n        self.vin = Some(vin.to_string());\n        self\n    }\n\n    fn build(self) -> ChargingSession {\n        ChargingSession {\n            id: self.id,\n            watts: self.watts.unwrap_or_else(|| 0),\n            vin: self.vin.unwrap_or_else(|| \"Unknown\".to_string()),\n        }\n    }\n}\n\nfn main() {\n    // 이것은 ChargingSession을 생성하는 표준적인 방법입니다.\n    let cs_old_way = ChargingSession {\n        id: String::from(\"11111\"),\n        watts: 420,\n        vin: String::from(\"4Y1SL65848Z411439\"),\n    };\n    println!(\"Regular way to create struct: {:?}\", cs_old_way);\n\n    // 빌더를 사용해서 생성하는 방법입니다.\n    let cs = ChargingSessionBuilder::new(\"11111\")\n        .watts(420)\n        .vin(\"4Y1SL65848Z411439\")\n        .build();\n    println!(\"Builder pattern to create struct: {:?}\", cs);\n\n    // ID만 제공하여 생성하는 예시입니다.\n    let cs_lean = ChargingSessionBuilder::new(\"11111\")\n    .build();\n     println!(\"Builder pattern to create struct (default values): {:?}\", cs_lean);\n}\n```\n\n이걸 한 번에 이해하기에 많지만, 단계적으로 진행해 봅시다.\n\n먼저, 충전 세션 정보를 저장하는 구조체를 정의했습니다. 이 시리즈의 이전 예제에서 하나의 필드인 세션용 차량 ID 번호인 vin을 추가했어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러면 데이터 구조체와 동일한 필드를 가진 빌더를 위한 구조체를 정의합니다.\n\n빌더는 impl 블록에서 구현되며, id 및 각 추가 필드를 설정하는 함수를 정의하는 새 함수가 있습니다. 그러나 몇 가지 중요한 사항이 있습니다.\n\n- 각 함수는 ChargingSessionBuilder 유형을 반환합니다. 기본적으로 self입니다.\n- 추가 속성 필드에는 mut self를 첫 번째 매개변수로 사용하는 메서드가 있습니다. 이는 이러한 함수 호출의 체이닝을 허용하는 데 중요합니다. 또한 여기에 유효성 검사 논리를 코딩할 수 있습니다.\n- build 함수가 모두 통합되는 곳입니다. 존재하는 값들을 할당하고 나면 기본값을 결정하고 대상 구조체를 생성할 수 있습니다.\n\n이를 통해 빌더 패턴의 우아함과 Rust 내에서의 구현 방법을 살펴보았습니다. 이것이 앱이나 라이브러리에 코드를 구현하는 훌륭한 방법임을 알 수 있기를 바랍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음 섹션인 데이터베이스로 넘어가 봅시다.\n\n# 데이터베이스 — sqlx\n\n이 섹션에서는 Rust 프로그램에서 sqlx를 사용하여 데이터베이스 작업을 살펴볼 것입니다. 먼저 일부 설정이 필요합니다.\n\n## 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n새 프로젝트를 시작해봅시다. db_app이라고 이름 짓고, cargo new db_app으로 생성할 수 있어요. 기본 디렉토리에 몇 개의 파일을 생성할 거에요. 첫 번째 파일은 docker-compose.yml이라고 하며 다음 내용이 있어야 합니다. Postgres와 Pgadmin이 노출되는 임의의 포트를 선택했으며, 컴퓨터에 설치된 다른 앱들과 충돌하지 않도록 했어요.\n\n```js\nversion: '3'\nservices:\n  postgres:\n    image: postgres:latest\n    container_name: postgres\n    ports:\n      - '6500:5432'\n    volumes:\n      - postgresDB:/data/postgres\n    env_file:\n      - ./.env\n  pgAdmin:\n    image: dpage/pgadmin4\n    container_name: pgAdmin\n    env_file:\n      - ./.env\n    ports:\n      - \"5050:80\"\nvolumes:\n  postgresDB:\n```\n\n.env이라는 파일이 하나 더 필요하며, 다음 내용이 있어야 해요. 이 파일은 docker-compose 파일과 나중에 Rust 애플리케이션에서 모두 사용할 거에요.\n\n```js\nPOSTGRES_HOST=127.0.0.1\nPOSTGRES_PORT=6500\nPOSTGRES_USER=admin\nPOSTGRES_PASSWORD=password123\nPOSTGRES_DB=charging_session\n\nDATABASE_URL=postgresql://admin:password123@localhost:6500/charging_session?currentSchema=public\n\nPGADMIN_DEFAULT_EMAIL=admin@admin.com\nPGADMIN_DEFAULT_PASSWORD=password123\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 내용과 함께 데이터베이스 사용자, 비밀번호, 그리고 Pgadmin에 대한 연결 정보를 제공해 드렸습니다.\n\n이 두 가지 항목을 만들고 위의 내용을 사용하여 Docker Compose를 사용하여 로컬 DB 인스턴스를 시작할 수 있습니다. 처음에는 항상 전경에서 시작하는 것을 좋아합니다. 이미지 다운로드 및 기타 작업을 하기 때문에 여러분의 컴퓨터 및 네트워크 속도에 따라 시간이 소요될 수 있습니다. docker-compose.yml 및 .env 파일이 있는 디렉토리와 동일한 위치에서 다음을 실행하십시오.\n\n```js\ndocker-compose up\n```\n\n시작된 모든 것을 확인한 후에는 언제든지 -d 스위치를 사용하여 데몬 모드로 시작할 수 있습니다. 완료되면 두 컨테이너가 시작되었는지 확인해 봅시다. 다음과 같이 명령을 실행합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndocker ps\n```\n\n비슷한 결과가 표시됩니다.\n\n<img src=\"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_1.png\" />\n\n만약 두 개의 컨테이너가 표시되지 않는다면, docker-compose를 실행한 터미널에서 에러를 확인해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 번 이들이 실행되면 DB에 연결해 봅시다. Pgadmin을 포트 5050에서 실행하도록 구성했으니 브라우저에서 http://localhost:5050을 입력하면 pgadmin의 로그인 화면이 표시됩니다. .env 파일에 구성된 자격 증명(관리자@관리자.com/비밀번호123)을 사용하여 pgadmin에 로그인할 수 있습니다. 그런데 아직 데이터베이스에 연결되지 않았습니다. 이를 위해 다음을 실행해야 합니다.\n\n```js\ndocker inspect postgres\n```\n\n출력을 확인하여 \"NetworkSettings\" 섹션으로 이동하고 \"IPAddress\" 속성의 값을 복사합니다. 이 값은 DB에 연결하는 데 사용할 호스트(IP)입니다. 제 컴퓨터에서 이 값은 172.23.0.1 이었습니다.\n\n로그인한 후 \"새 서버 추가\" 버튼을 클릭하고, \"호스트 이름/주소\"로 이전에 복사한 IP 주소를 포함한 필수 자격 증명을 제공하고 \"저장\" 버튼을 클릭하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n서버에 로그인하면 데이터베이스 charging_session을 볼 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_2.png)\n\n잘 했어요. 데이터베이스를 사용할 준비가 되었고 Pgadmin에서 관리할 수 있습니다.\n\n이제 Rust 앱의 종속성을 구성해 봅시다. 처음에는 모두 필요하지 않지만 결국 필요하게 될 것이므로 지금 추가해 두는 것이 좋습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nCargo.toml 파일의 종속성 부분을 다음과 같이 수정하세요.\n\n```toml\n[dependencies]\nchrono = { version = \"0.4.31\", features = [\"serde\"] }\ndotenv = \"0.15.0\"\nenv_logger = \"0.10.1\"\nlog = \"0.4.20\"\nserde = { version = \"1.0.193\", features = [\"derive\"] }\nserde_json = \"1.0.108\"\nsqlx = { version = \"0.7.3\", features = [\"runtime-tokio-native-tls\", \"postgres\", \"uuid\", \"chrono\"] }\ntokio = { version = \"1.35.0\", features = [\"macros\", \"rt-multi-thread\"]}\nuuid = { version = \"1.6.1\", features = [\"serde\", \"v4\"] }\n```\n\n다양한 종속성에 대해 활성화된 기능을 검토하는 데도 시간을 할애하는 것이 좋습니다.\n\n다음으로, 필요한 테이블을 만들기 위해 sqlx-cli 마이그레이션 기능을 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우선 명령 줄에서 다음을 실행하여 CLI를 설치해야 합니다.\n\n```js\ncargo install sqlx-cli --no-default-features --features rustls,postgres\n```\n\n그런 다음 마이그레이션 파일을 초기화해야 합니다. 다음과 같이 실행합니다.\n\n```js\nsqlx migrate add initial-tables\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 명령어는 우리가 마이그레이션 스크립트를 작성하기 위해 새 파일 migrations/`timestamp`\\_initial-tables.sql을 생성합니다.\n\n![이미지](/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_3.png)\n\n이 파일을 열고 아래 SQL 문을 추가하여 테이블을 생성하세요.\n\ncreate table locations (\nid bigserial primary key,\nname varchar(255) unique not null\n);\n\ncreate table sessions (\nid bigserial primary key,\nlocation_id bigint not null,\nwatts bigint not null,\nvin varchar(255) not null,\nconstraint fk_location foreign key (location_id) references locations(id) on delete cascade\n);\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 예제에서 사용할 두 개의 테이블 정의입니다. 이 시리즈의 충전 세션에 대한 표준 예제를 확장하여, 충전 장치의 위치를 저장할 locations 테이블을 추가했습니다.\n\n이제 다음을 실행하여 테이블을 생성하세요.\n\n```js\nsqlx migrate run\n```\n\n그러면 즉시 20231212235431/migrate initial-tables (타임스탬프 부분은 달라질 수 있음)과 같은 메시지가 표시됩니다. 이제 Pgadmin에 가서 테이블을 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_4.png\" />\n\n위에서 보듯이 두 개의 테이블이 생성되었고, 문서 목적을 위해 Pgadmin 내에 ERD 다이어그램도 생성했습니다. 이제 초기 설정과 프로젝트 구성을 완료했습니다.\n\n# 데이터베이스 함수\n\n이 섹션에서는 데이터베이스 상호 작용의 다양한 유형과 Rust 구현을 살펴볼 것입니다. 이는 데이터베이스에 연결하는 것으로 시작됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDB에 연결하기\n\n데이터베이스에 연결하는 방법은 PgPoolOptions connect 함수를 통해 연결을 생성하는 것입니다. 우리의 .env 파일에서 데이터베이스 연결 문자열을 읽고 로깅을 구성하며 성공 또는 오류를 기록하는 코드 전체는 아래와 같습니다.\n\n```js\nuse sqlx::{postgres::PgPoolOptions, Pool, Postgres};\nuse dotenv::dotenv;\nuse log::{info, error};\n\n#[tokio::main]\nasync fn main() {\n    if std::env::var_os(\"RUST_LOG\").is_none() {\n        std::env::set_var(\"RUST_LOG\", \"info\");\n    }\n    dotenv().ok();\n    env_logger::init();\n\n    let database_url = std::env::var(\"DATABASE_URL\").expect(\"DATABASE_URL must be set\");\n    let pool = match PgPoolOptions::new()\n        .max_connections(10)\n        .connect(&database_url)\n        .await\n    {\n        Ok(pool) => {\n            info!(\"✅ 데이터베이스에 연결되었습니다!\");\n            pool\n        }\n        Err(err) => {\n            error!(\"🔥 데이터베이스 연결에 실패했습니다: {:?}\", err);\n            std::process::exit(1);\n        }\n    };\n}\n```\n\n시작 부분의 #[tokio::main] 매크로를 주목해주세요. 이는 async fn main()을 동기 fn main()로 변환하여 런타임 인스턴스를 초기화하고 async main 함수를 실행하게 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 코드를 실행하면 출력 로깅에서 다음 메시지가 표시됩니다.\n\n[2023-12-13T00:16:42Z INFO db_app] ✅ 데이터베이스에 연결되었습니다!\n\n우리는 데이터베이스에 연결할 수 있습니다. .env 파일에서 구성한 모든 연결 정보를 기억해 주세요. 이 정보는 docker-compose, sqlx-cli, 그리고 우리의 어플리케이션에서 공유되었습니다.\n\n## Inserts\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 연결 풀이 준비되었으니, 우리가 살펴볼 첫 번째 데이터베이스 로직은 삽입입니다. sqlx를 사용하여 삽입하는 방법은 다음과 같습니다.\n\n```rust\nlet insert_result = sqlx::query_as!(\n    Locations,\n    \"INSERT INTO locations (id,name) VALUES (1, 'Location A') RETURNING *\"\n)\n.fetch_one(&pool)\n.await;\n\nmatch insert_result {\n    Ok(location) => {\n        info!(\"✓Inserted: {:?}\", location);\n    }\n    Err(e) => {\n        error!(\"Error Insert: {}\", e.to_string())\n    }\n}\n```\n\n이 코드는 하나의 레코드를 삽입할 것입니다. 물론 매개변수를 사용할 수도 있으며, 이는 업데이트를 살펴볼 때 살펴볼 것입니다.\n\n## 질의하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 데이터베이스를 조회하는 방법을 살펴보겠습니다.\n\n```js\n    let query_result = sqlx::query_as!(Locations, \"SELECT * FROM Locations\")\n        .fetch_all(&pool)\n        .await;\n    if query_result.is_err() {\n        let message = \"모든 위치를 가져오는 동안 문제가 발생했습니다.\";\n        error!(\"{}{}\", message, query_result.err().unwrap());\n    } else {\n        info!(\"😎 위치에 대한 쿼리 결과 {:?}\", query_result.unwrap());\n    }\n```\n\n이것은 성공적으로 작동했다면 unwrap을 통해 결과에 접근할 수 있는 결과를 반환합니다. 쿼리 내의 이슈가 발생했다면 해당 에러에 접근할 수도 있습니다.\n\n## 업데이트\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음 작업은 데이터베이스를 업데이트하는 것입니다. 이는 이전에 삽입한 것과 비슷할 것입니다.\n\n```js\n    let update_result = sqlx::query_as!(\n        Sessions,\n        \"UPDATE sessions SET watts = 415 WHERE id = $1 RETURNING *\",\n        1i64,\n    )\n    .fetch_one(&pool)\n    .await;\n\n    match update_result {\n        Ok(session) => {\n            info!(\"✓Update: {:?}\", session);\n        }\n        Err(e) => {\n            error!(\"Error Update: {}\", e.to_string())\n        }\n    }\n```\n\n이 예제에서 주목할 점은 $1이라는 매개변수 자리 표시자를 사용하는 준비된 문(statement)를 사용하고 있다는 것입니다. 그런 다음 SQL 문자열 뒤에 매개변수를 전달합니다.\n\n## Deletes\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로, 데이터베이스에서 레코드를 삭제하는 CRUD 작업을 완료합니다. 우리의 목적은 데이터베이스를 정리하는 데 사용할 것이며, 이렇게 하면 언제든지 응용 프로그램을 실행할 수 있습니다.\n\n```js\nlet rows_deleted = sqlx::query!(\"DELETE from sessions\")\n    .execute(&pool)\n    .await\n    .unwrap()\n    .rows_affected();\n\ninfo!(\"✕ 세션 테이블에서 {}개의 행 삭제됨\", rows_deleted);\n```\n\n여기서는 연산에서처럼 sqlx::query 대신 sqlx::query_as를 사용합니다. 또한 언랩 이후 .rows_affected를 추가하여 삭제된 행 수를 얻습니다.\n\n## 트랜잭션\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```rust\n   let tx = pool.begin().await.expect(\"트랜잭션을 시작할 수 없습니다\");\n\n   // 데이터베이스 작업 수행(데이터 삽입 또는 변경)\n\n   tx.commit().await.expect(\"트랜잭션을 커밋할 수 없습니다\");\n```\n\n커밋을 호출하지 않으면 트랜잭션이 범위를 벗어나면 자동으로 롤백됩니다.\n\n## 완전한 응용 프로그램\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 코드는 우리가 이전에 논의한 모든 다양한 기능이 하나의 애플리케이션에 모두 포함된 완전한 애플리케이션입니다.\n\n```js\nuse dotenv::dotenv;\nuse log::{error, info};\nuse sqlx::{postgres::PgPoolOptions, Pool, Postgres};\n\n#[derive(Debug)]\nstruct Locations {\n    id: i64,\n    name: String,\n}\n\n#[derive(Debug)]\nstruct Sessions {\n    id: i64,\n    location_id: i64,\n    watts: i64,\n    vin: String,\n}\nasync fn insert_into_locations(pool: Pool<Postgres>) {\n    let tx = pool.begin().await.expect(\"Unable to begin transaction\");\n\n    let insert_result = sqlx::query_as!(\n        Locations,\n        \"INSERT INTO locations (id,name) VALUES (1, 'Location A') RETURNING *\"\n    )\n    .fetch_one(&pool)\n    .await;\n\n    match insert_result {\n        Ok(location) => {\n            info!(\"✓Inserted: {:?}\", location);\n        }\n        Err(e) => {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n    let insert_result = sqlx::query_as!(\n        Locations,\n        \"INSERT INTO locations (id,name) VALUES (2, 'Location B') RETURNING *\"\n    )\n    .fetch_one(&pool)\n    .await;\n\n    match insert_result {\n        Ok(location) => {\n            info!(\"✓Inserted: {:?}\", location);\n        }\n        Err(e) => {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n\n    tx.commit().await.expect(\"Unable to commit the transaction\");\n}\n\nasync fn insert_into_sessions(pool: Pool<Postgres>) {\n    let tx = pool.begin().await.expect(\"Unable to begin transaction\");\n\n    let insert_result = sqlx::query_as!(\n        Sessions,\n        \"INSERT INTO sessions (id,location_id, watts, vin) VALUES (1, 1, 420, '2FMZA52286BA02033') RETURNING *\"\n    )\n    .fetch_one(&pool)\n    .await;\n\n    match insert_result {\n        Ok(session) => {\n            info!(\"✓Inserted: {:?}\", session);\n        }\n        Err(e) => {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n    let insert_result = sqlx::query_as!(\n        Sessions,\n        \"INSERT INTO sessions (id,location_id, watts, vin) VALUES (2, 2, 393, '1GMYA52286BA04055') RETURNING *\"\n)\n    .fetch_one(&pool)\n    .await;\n\n    match insert_result {\n        Ok(session) => {\n            info!(\"✓Inserted: {:?}\", session);\n        }\n        Err(e) => {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n\n    tx.commit().await.expect(\"Unable to commit the transaction\");\n}\n\nasync fn update_sessions(pool: Pool<Postgres>) {\n    let tx = pool.begin().await.expect(\"Unable to begin transaction\");\n\n    let update_result = sqlx::query_as!(\n        Sessions,\n        \"UPDATE sessions SET watts = 415 WHERE id = $1 RETURNING *\",\n        1i64,\n    )\n    .fetch_one(&pool)\n    .await;\n\n    match update_result {\n        Ok(session) => {\n            info!(\"✓Update: {:?}\", session);\n        }\n        Err(e) => {\n            error!(\"Error Update: {}\", e.to_string())\n        }\n    }\n\n    tx.commit().await.expect(\"Unable to commit the transaction\");\n}\n\nasync fn clean_db(pool: Pool<Postgres>) {\n    let rows_deleted = sqlx::query!(\"DELETE from sessions\")\n        .execute(&pool)\n        .await\n        .unwrap()\n        .rows_affected();\n\n    info!(\"✕Deleted {} rows from sessions table\", rows_deleted);\n\n    let rows_deleted = sqlx::query!(\"DELETE from locations\")\n        .execute(&pool)\n        .await\n        .unwrap()\n        .rows_affected();\n    info!(\"✕Deleted {} rows from locations table\", rows_deleted);\n}\n\nasync fn query_locations(pool: Pool<Postgres>) {\n    let query_result = sqlx::query_as!(Locations, \"SELECT * FROM Locations\")\n        .fetch_all(&pool)\n        .await;\n    if query_result.is_err() {\n        let message = \"Something bad happened while fetching all locations\";\n        error!(\"{}\", message);\n    } else {\n        info!(\"😎Query Result For Locations {:?}\", query_result);\n    }\n}\n\nasync fn query_sessions(pool: Pool<Postgres>) {\n    let query_result = sqlx::query_as!(Sessions, \"SELECT * FROM Sessions\")\n        .fetch_all(&pool)\n        .await;\n    if query_result.is_err() {\n        let message = \"Something bad happened while fetching all sessions\";\n        error!(\"{}\", message);\n    } else {\n        info!(\"😎Query Result for Sessions {:?}\", query_result);\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    if std::env::var_os(\"RUST_LOG\").is_none() {\n        std::env::set_var(\"RUST_LOG\", \"info\");\n    }\n    dotenv().ok();\n    env_logger::init();\n\n    let database_url = std::env::var(\"DATABASE_URL\").expect(\"DATABASE_URL must be set\");\n    let pool = match PgPoolOptions::new()\n        .max_connections(10)\n        .connect(&database_url)\n        .await\n    {\n        Ok(pool) => {\n            info!(\"✅Connection to the database is successful!\");\n            pool\n        }\n        Err(err) => {\n            error!(\"🔥 Failed to connect to the database: {:?}\", err);\n            std::process::exit(1);\n        }\n    };\n    clean_db(pool.clone()).await;\n\n    insert_into_locations(pool.clone()).await;\n    query_locations(pool.clone()).await;\n\n    insert_into_sessions(pool.clone()).await;\n    query_sessions(pool.clone()).await;\n\n    update_sessions(pool.clone()).await;\n    query_sessions(pool.clone()).await;\n}\n```\n\n위 애플리케이션을 실행한 결과는 다음과 같습니다.\n\n<img src=\"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_5.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위와 같이 데이터베이스 상호작용 토론을 마쳤습니다. sqlx를 사용하여 데이터베이스 작업을 수행하는 방법에 대해 좋은 개요를 제공했을 겁니다.\n\n# 요약\n\n저희 러스트 학습 시리즈의 이 부분을 즐기셨기를 바랍니다. 시리즈 이번 섹션에서는 먼저 러스트에서 객체 생성에 대한 매우 유용한 패턴인 빌더 패턴을 살펴보았습니다. 이는 다른 언어에서 익숙할 수 있지만, 러스트에서 어떻게 구현하는지 살펴보았습니다.\n\n다음으로, 우리는 Rust를 사용하여 데이터베이스인 특히 Postgres와 상호작용하는 방법을 검토했습니다. 우리는 마이그레이션을 실행하고 데이터베이스에 연결하는 방법을 보았으며, 그 후 DB에 대해 여러 가지 CRUD 작업을 수행하는 방법을 살펴보았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n러스트 학습 여정에 함께해줘서 고마워요.\n\n좋은 여행 되세요!\n","ogImage":{"url":"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png"},"coverImage":"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png","tag":["Tech"],"readingTime":24},{"title":"데브 컨테이너로 Rails 앱을 도커라이즈하기","description":"","date":"2024-05-23 14:10","slug":"2024-05-23-DockerizeRailsappwithDevContainers","content":"\n![이미지](/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png)\n\n만약 Rails 프로젝트 또는 다른 프레임워크에 대해 Docker를 시도해보려고 망설이고 있다면, 꼭 한 번 시도해보기를 추천합니다. VSCode의 Dev Containers 확장 프로그램을 사용하면 프로세스가 놀랄 만큼 스무스해지고 로컬 개발 과정이 간편해집니다.\n\n이미 CI/CD 파이프라인이나 배포에 Docker를 사용해본 적이 있다면 컨테이너화의 이점에 익숙할 것입니다. 그러나 솔직히 말해서 로컬 개발에 있어서 Docker는 항상 편리하지는 않습니다. 모든 것에 docker 또는 docker-compose 명령을 추가해야 한다는 것은 조금 귀찮은 일일 수 있습니다.\n\n이때 DevContainers가 등장합니다. 이를 통해 로컬 Rails 개발 환경 (및 다른 프레임워크)을 위한 전체 Docker 설정이 간단해집니다. 더 이상 명령줄이 혼잡해지지 않고, 매끄럽고 일관된 개발 경험만을 가져다줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nVSCode에서 개발 컨테이너를 사용하여 Rails를 도커화하는 간단한 튜토리얼을 제공합니다. 또한 도커나 도컴포즈 명령어를 실행할 필요 없이 컨테이너 환경 내에서 작업하는 방법을 안내합니다.\n\n본 튜토리얼에서는 다음을 배울 수 있습니다:\n\n- VSCode 편집기의 DevContainers 확장 프로그램을 사용하여 Docker 컨테이너 내에서 새로운 Rails 프로젝트를 신속하게 생성하는 방법\n- 터미널에서 복잡한 Docker 명령어를 필요로하지 않도록 함\n- 데이터 지속성을 위해 MySQL 데이터베이스 컨테이너를 사용하여 이중 구조 아키텍처 구축\n- Rails와 함께 MySQL을 컨테이너 내에서 실행\n- 멀티 스테이지 도커 파일을 사용하여 개발 및 프로덕션 환경에 대한 이미지 빌드 최적화\n- VSCode 내에서 간편하고 효율적인 Rails 개발 워크플로우 달성\n\n이 글과 관련된 튜토리얼을 통해 Rails 프로젝트에서 Dev Containers를 사용해보기를 고려하게끔 도와드리기를 바랍니다. 개발 프로세스를 최적화하고, 협업을 개선하며, 일관된 환경을 유지하는 환상적인 방법입니다. 즐거운 코딩 되세요!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 도움이 되었다면, 추가적인 팁과 튜토리얼을 보기 위해 LinkedIn을 팔로우하고 YouTube 채널을 구독해주세요.\n\n## 추가 학습 내용:\n\n- 멀티 스테이지 도커 빌드 배우기\n- DevContainers로 로컬 DevOps 환경 설정하는 방법 배우기\n- VSCode 확장 프로그램 DevContainers\n","ogImage":{"url":"/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png"},"coverImage":"/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png","tag":["Tech"],"readingTime":2},{"title":"꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake","description":"","date":"2024-05-23 14:09","slug":"2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines","content":"\n<img src=\"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png\" />\n\n빅 데이터 시대에는 정보 관리가 지속적인 싸움입니다. 데이터 양이 급증함에 따라 저장뿐만 아니라 가치 있는 통찰을 추출하기 위한 효율적인 처리와 분석도 필요합니다. 이것이 꿈의 팀이 나타나는 곳입니다: Snowflake, Databricks 및 Delta Lake. 각각이 강력한 솔루션으로 클라우드 기반의 솔루션을 결합하여 효율적인 데이터 파이프라인을 만들어내어 당신을 챔피언처럼 데이터를 관리할 수 있게 합니다.\n\n플레이어와 그들의 역할:\n\n- Snowflake: 주역 쿼터백. 구조화된 데이터를 저장하고 분석하는 데 뛰어난 클라우드 기반 데이터 웨어하우스인 Snowflake는 데이터 분석가에게 친숙한 사용자 친화적 SQL 인터페이스를 통해 있는대로 데이터 탐색 및 효율적인 쿼리를 가능하게 합니다. 또한, Snowflake는 확장성과 보안을 자랑하며 데이터가 항상 접근 가능하고 보호되도록 보장합니다.\n- Databricks: 만회하는 와이드 리시버. Apache Spark를 기반으로 한 Databricks는 대규모 데이터 처리와 고급 분석의 달인입니다. 다양한 소스에서 데이터 수집, 데이터 변환 (정제 및 가공) 및 심지어 머신러닝 모델을 구축하고 배포하는 것과 같은 작업에서 빛을 발합니다. Databricks는 분석을 위해 데이터를 준비하는 일꾼 역할을 합니다.\n- Delta Lake: 신뢰할 수 있는 러닝 백. 데이터 호수 위에 구축된 오픈 소스 저장 레이어인 Delta Lake는 데이터 안정성의 챔피언입니다. 기존 또는 반구조적 데이터에 구조와 관리성을 추가하여 데이터 호수 안에 위치한 Delta Lake는 ACID 트랜잭션 (데이터 일관성 보장), 스키마 강제 사항 (데이터 구조 정의), 데이터 버전 관리 (데이터 변경 추적)와 같은 기능을 제공합니다. 데이터의 정확성과 신뢰성을 보장하는 데이터의 기초로 생각할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n승리의 플레이북: 간소화된 데이터 파이프라인 구축\n\n성능과 효율성을 최적화하여 각 단계를 효율적으로 흐르는 데이터 파이프라인을 상상해보세요. Snowflake, Databricks 및 Delta Lake가 이를 달성하는 방법은 다음과 같습니다:\n\n- Touchdown — 데이터 수집: Databricks는 다양한 소스(데이터베이스, API 또는 데이터 레이크에 있는 raw 파일 등)에서 데이터를 가져오는데 사용되는 다양한 커넥터를 통해 데이터를 추출합니다.\n- The Handoff — 데이터 변환: Databricks가 중심에 위치합니다. Spark의 처리 능력이 빛나며 주입된 데이터를 노트북을 사용하여 정제, 변환 및 준비합니다. 이 단계에서는 누락된 값을 처리하고 데이터 유형을 변환하거나 새로운 기능을 도출하는 등 분석 전에 모두 중요한 과정입니다.\n- The Choice of Plays — ETL 대 ELT: 여기서 데이터 저장을 위해 두 가지 옵션이 있습니다:\n\n  - ETL (추출, 변환, 로드): 이 플레이에서 Databricks는 변환된 데이터를 데이터 웨어하우스인 Snowflake로 이관하는 중심 역할을 합니다. Snowflake는 데이터를 안전하게 저장하여 쿼리 및 분석에 쉽게 사용할 수 있게 합니다.\n  - ELT (추출, 로드, 변환): 이 플레이는 Delta Lake의 강점을 활용합니다. 원본 데이터는 데이터 레이크에 직접 들어가며, Databricks는 클라우드 환경 내에서 데이터를 직접 Delta 테이블에 변환합니다. 이 접근 방식은 데이터 이동을 최소화하지만 분석을 위해 Delta 테이블에 의존하기 전에 데이터 품질을 보장해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n4. 승리의 춤 — 데이터 분석 및 소비: Snowflake가 슬기롭게 떠나다. 데이터 분석가 및 데이터 과학자들은 Snowflake의 SQL 인터페이스를 활용하여 Snowflake에 저장된 데이터에 대한 철저한 탐색, 시각화 및 고급 분석을 할 수 있습니다. 또한, 구체적인 사용 사례를 위해 Delta 테이블을 직접 쿼리하거나 실시간 스코어링과 분석을 위해 훈련된 머신러닝 모델을 Snowflake에 배포할 수 있습니다. 이를 통해 데이터로부터 가치 있는 통찰력을 얻을 수 있습니다.\n\n샘플 코드 실행 (Delta Lake를 사용한 ETL):\n\n```js\n# Databricks 노트북 - Python\n\n# 1. Snowflake에 연결\nsnowflake_conn = {\n    \"host\": \"your_snowflake_account.snowflakecomputing.com\",\n    \"user\": \"your_username\",\n    \"password\": \"your_password\",\n    \"database\": \"your_database\",\n    \"schema\": \"your_schema\"\n}\n\n# 2. CSV 파일에서 데이터 로드 (데이터 레이크에 저장된 것으로 가정)\ndf = spark.read.csv(\"path/to/your/data.csv\", header=True)  # 헤더가 있는 CSV를 가정\n\n# 3. 데이터 변환 (예시: 결측값 처리)\ndf = df.fillna(0, subset=[\"column_with_missing_values\"])  # 결측값을 0으로 대체\n\n# 4. 변환된 데이터를 Snowflake 테이블에 작성\ndf.write.format(\"snowflake\").options(**snowflake_conn).mode(\"overwrite\").saveAsTable(\"your_snowflake_table_name\")\n```\n\n게임 넘어서: 파이프라인 최적화\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n팀의 승리를 위해서는 지속적인 최적화가 필요해요. 여기 몇 가지 추가 팁이 있어요:\n\n- 올바른 전략 선택하기: ETL 및 ELT 방법 중 선택할 때 데이터 양, 처리 복잡성 및 원하는 대기 시간과 같은 요소를 신중하게 고려해주세요.\n- 데이터 품질이 중요해요: 통찰력의 무결성을 보장하기 위해 데이터 품질 점검을 파이프라인 전체에 구현해주세요.\n- 전략 실행하기: Airflow나 Databricks Workflows와 같은 도구를 활용하여 데이터 파이프라인의 실행을 일정화하고 조정하여 정보가 원활하게 흐를 수 있도록 해주세요.\n- 비용 관리: Snowflake와 Databricks는 사용량 기반의 가격 모델을 갖고 있어요. 자원 사용량을 모니터링하고 Databricks의 유휴 클러스터를 중지하는 등 비용 절감 방안을 도입해주세요.\n\n최종 엔딩: 데이터의 힘을 발휘해보세요\n\nSnowflake, Databricks, 그리고 Delta Lake를 결합하여 견고하고 확장 가능한 데이터 관리 시스템을 만들 수 있어요. 이 꿈의 팀은 대용량 데이터의 과제에 대처하고, 원시 정보를 실행 가능한 통찰력으로 변화시키는 능력을 부여해줘요. 상상해보세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 정보에 대한 빠른 통찰력: 간소화된 데이터 파이프라인은 유용한 데이터에보다 빠르게 액세스할 수 있도록 도와주어 데이터 기반의 결정을 이전보다 빨리 내릴 수 있습니다.\n- 개선된 데이터 거버넌스: Delta Lake 및 Snowflake의 보안 기능은 데이터의 보호와 신뢰성을 보장하여 데이터 분석에 대한 신뢰를 증진시킵니다.\n- 미래를 위한 유연성: 이 아키텍처는 확장 가능하게 설계되었습니다. 데이터 요구 사항이 발전함에 따라, 새로운 데이터 소스, 처리 요구 사항 및 분석 요구 사항을 수용하기 위해 파이프라인을 조정하고 확장할 수 있습니다.\n\n그러니 자신감 있게 필드에 발을 들이세요. Snowflake, Databricks 및 Delta Lake가 곁에 있는 당신은 데이터를 관리하고 그 참된 잠재력을 개방할 수 있는 승리팀입니다.\n","ogImage":{"url":"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png"},"coverImage":"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png","tag":["Tech"],"readingTime":5},{"title":"데이터베이스DBT 간단 정리","description":"","date":"2024-05-23 14:08","slug":"2024-05-23-DBTinaNutshell","content":"\n이 게시물은 일반적으로 DBT(Data Build Tool)에 대한 내 이해와 누가 사용해야 하고 사용하지 말아야 하는지에 중점을 둘 것입니다. 앞으로의 게시물에서 더 자세히 다룰 예정입니다.\n\nDBT(Data Build Tool)와 Snowflake❄️를 통해 비즈니스 문제 해결하기👨‍💻\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_0.png)\n\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 1 : [링크](https://lnkd.in/gASCckRR)\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 2 : [링크](https://lnkd.in/gMqfKZRW)\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 3 : [링크](https://lnkd.in/g8BuWy66)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n◉ 참여하기:\n👉 GitHub: [링크](https://lnkd.in/ggt3ZzUx)\n🚀 기여하고, 복제하고, 공유하세요!\n\n◉ YouTube 채널 찾기🎥: [링크](https://lnkd.in/esW5M3vb)\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_1.png)\n\n- DBT란 무엇인가요?\n- DBT는 무료인가요?\n- DBT가 해결하고 있는 문제는 무엇인가요?\n- ETL의 어느 부분에서 DBT를 사용하나요?\n- DBT가 지원하는 데이터 어댑터(데이터 플랫폼)는 무엇인가요?\n- DBT가 Databricks와 같은 기존 제품과 다른 점은 무엇인가요?\n- DBT를 배우고 사용하는 방법은 무엇인가요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## DBT이란 무엇인가요?\n\nDBT(data build tool)는 SQL 스크립트(.sql)와 YAML 스크립트(.yml)를 사용하여 워크플로우를 변환하는 데 사용되는 오픈 소스 도구(Core 버전)입니다.\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_2.png)\n\n- SQL 스크립트는 CTE를 사용하여 데이터를 모듈화된 방식으로 변환하는 데 도움이 됩니다.\n- YAML 스크립트는 스키마, 설명 및 열에 대한 테스트 규칙(Null이 아닌 값, 고유 값 등)을 정의하는 데 도움이 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## DBT은 무료인가요?\n\nDBT에는 DBT 코어와 DBT 클라우드 두 가지 버전이 있습니다.\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_3.png)\n\n- DBT 코어: CLI(Command Line Interface) 버전으로, 간단한 pip 명령어 `pip install dbt-core`를 사용하여 설치할 수 있습니다. 연결을 위해 어댑터(snowflakes, SQL Server)를 설치하려면 `pip install dbt-snowflake`를 사용하세요.\n- DBT 클라우드: GIT 및 어댑터(데이터 소스)를 통합하고 드래그 앤 드롭 기능을 사용하여 워크스페이스를 구성할 수 있는 GUI(Graphical User Interface)를 제공하며, 모델(당신의 .sql 파일)을 예약할 수 있는 기능이 추가되어 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## DBT가 해결하려는 문제는 무엇인가요?\n\n![DBTinaNutshell_4](/assets/img/2024-05-23-DBTinaNutshell_4.png)\n\n원활한 협업:\n\n- 협업을 위한 통합 플랫폼을 즐기세요.\n- 효율적인 팀워크를 위한 CI/CD-Git 통합을 활용하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n편리한 데이터 변환:\n\n- 번거로움 없이 간단한 SQL 선택 문을 사용하여 데이터를 변환하세요.\n\n테스트로 신뢰성 확보:\n\n- 사용자 정의 테스트 케이스를 포함하여 데이터 변환을 테스트하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n편리하게 배포하고 일정을 계획하세요:\n\n- 코딩을 개발 및 프로덕션과 같은 다양한 환경에 배포하고 일정을 조정하세요.\n\n간편하게 작업 문서화하세요:\n\n- 간단한 .yml 파일로 전체 프로세스를 문서화하세요.\n- 데이터 변환 여정에 대한 포괄적인 문서를 작성하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## ETL에서 DBT는 어느 부분에 사용되나요?\n\nDBT는 다른 ELT✅ 방법을 사용합니다❌ ETL이 아닌데요, 여기서는 데이터 플랫폼으로 모든 데이터를 추출 및 로드하고 DBT를 사용하여 변형한 후 다양한 사용 사례를 위해 다시 데이터 플랫폼에 로드합니다.\n\n<img src=\"/assets/img/2024-05-23-DBTinaNutshell_5.png\" />\n\n## DBT는 어떤 데이터 어댑터(데이터 플랫폼)를 지원하나요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금은 다음 데이터 플랫폼을 직접 지원합니다:\n\n- Snowflakes\n- Google Big Query\n- Data Bricks\n- AWS Redshift\n- Trino\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_6.png)\n\nTrino는 직접적으로가 아닌 Trino를 통해 여러 데이터 소스에 연결하는 데 사용됩니다. 자세한 내용은 이미지를 참조하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-23-DBTinaNutshell_7.png\" />\n\n## DBT와 Databricks와 같은 다른 기존 제품과의 차이점은 무엇인가요?\n\n- DBT: DBT는 주로 데이터웨어하우스 내에서 데이터 변환 및 문서화 문제를 .sql 및 .yml 파일을 사용하여 CICD-git 환경에서 간단하고 효과적으로 처리하는 것에 중점을 둡니다.\n- Databricks (유명한 예시): Databricks는 대용량 데이터 분석을 위한 협업 환경을 제공하는 통합 분석 플랫폼입니다. 데이터 엔지니어링, 머신 러닝, 협업 데이터 과학 기능이 포함되어 있으며 주로 분산 데이터 처리를 위해 Spark와 함께 사용됩니다.\n\n## 어떻게 DBT를 학습하고 사용할 수 있을까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- DBT를 배우기 시작하려면 해당 웹사이트에서 시작할 수 있어요. 총 16개의 다양한 코스가 초급, 중급 및 고급 수준으로 분류되어 있어요.\n\n![DBTinaNutshell_8](/assets/img/2024-05-23-DBTinaNutshell_8.png)\n\n- DBT를 더 잘 배울 수 있도록 많은 매체 기사와 유튜브 비디오들이 있어요 (DBT 콘텐츠에 대한 업데이트 받으시려면 지켜봐주세요! 다가오는 게시물 및 DBT 관련 비디오에 대한 업데이트를 받으시려면 저를 팔로우해주세요).\n\nLeo Godin의 DBT 시리즈 게시물: 링크\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n나에 대해 더 알아보기:\n\n저는 데이터 과학 애호가🌺이며, 수학, 비즈니스 및 기술이 데이터 과학 분야에서 더 나은 결정을 내리는 데 어떻게 도움이 될 수 있는지에 대해 배우고 탐구하고 있습니다.\n\n더 많은 내용 보기: https://medium.com/@ravikumar10593/\n\n내 모든 핸들 찾기: https://linktr.ee/ravikumar10593\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n나의 뉴스레터를 찾아보세요: [https://substack.com/@ravikumar10593](https://substack.com/@ravikumar10593)\n","ogImage":{"url":"/assets/img/2024-05-23-DBTinaNutshell_0.png"},"coverImage":"/assets/img/2024-05-23-DBTinaNutshell_0.png","tag":["Tech"],"readingTime":7},{"title":"데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리","description":"","date":"2024-05-23 14:05","slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka","content":"\n이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\n\n목차:\n\n1. 스트림 처리란\n2. 카프카의 특징\n3. 카프카 구성\n4. 카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\n5. 스파크 구조화 스트리밍 API\n6. 데이타브릭스 델타 레이크\n7. 실전 프로젝트\n\n이전 데이터 보안 부분으로 이동하는 링크입니다:\n\n## 스트림 처리란?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\n\n![데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png)\n\n스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\n\n카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 카프카 특징\n\n- 견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\n- 유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\n- 확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\n\n  - 파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\n  - 수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\n\n## 카프카 구성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\n\n- 파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\n- 복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\n- 유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\n- 자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\n- ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\n\n## 다른 카프카 서비스\n\nKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\" />\n\nksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\" />\n\n스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png)\n\n## Spark Structured Streaming API\n\nSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\n\n또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png)\n\n## Databricks Delta Lake\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\n\nDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\n\n스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 실시간 스트리밍 아키텍처\n\n카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\n\n문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\n\n- 단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\n- 단계 2: 토픽 생성\n- 단계 3: 클러스터 API 키 쌍 생성\n\n위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\n\n- 단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png)\n\nUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\n\n- Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\n\n- 단계 7: Kafka 토픽에서 스트림 읽기\n\n```js\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Environmental Reporting\").getOrCreate()\n\nkafkaDF = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"abcd.us-west4.gcp.confluent.cloud:9092\") \\\n    .option(\"subscribe\", \"illegal_dumping\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\" password=\"\";\"\"\") \\\n    .load()\n\nprocessedDF = kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\ndisplay(processedDF)\n```\n\n- 단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport pyspark.sql.functions as F\nfrom  pyspark.sql.functions import col, struct, to_json\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\njson_schema = StructType(\n  [\n    StructField(\"incident_type\", StringType(), nullable = False),\n    StructField(\"location\", StringType(), nullable = False),\n    StructField(\"description\", StringType(), nullable = True),\n    StructField(\"contact\", StringType(), nullable = True)\n  ]\n)\n\n# Using Spark SQL to write queries on the streaming data in processedDF\n\nquery = processedDF.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n      .select(F.col(\"value.incident_type\"),F.col(\"value.location\"))\ndisplay(query)\n```\n\nWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\n\nDefine a UDF(User Defined Function) to find out the region from the location:\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the regions_to_states dictionary\nregions_to_states = {\n    'South': ['West Virginia', 'District of Columbia', 'Maryland', 'Virginia',\n              'Kentucky', 'Tennessee', 'North Carolina', 'Mississippi',\n              'Arkansas', 'Louisiana', 'Alabama', 'Georgia', 'South Carolina',\n              'Florida', 'Delaware'],\n    'Southwest': ['Arizona', 'New Mexico', 'Oklahoma', 'Texas'],\n    'West': ['Washington', 'Oregon', 'California', 'Nevada', 'Idaho', 'Montana',\n             'Wyoming', 'Utah', 'Colorado', 'Alaska', 'Hawaii'],\n    'Midwest': ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota',\n                'Iowa', 'Missouri', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana',\n                'Ohio'],\n    'Northeast': ['Maine', 'Vermont', 'New York', 'New Hampshire', 'Massachusetts',\n                  'Rhode Island', 'Connecticut', 'New Jersey', 'Pennsylvania']\n}\n\n#from geotext import GeoText\nfrom geopy.geocoders import Nominatim\n\n# Define a function to extract state names from location text\ndef extract_state(location_text):\n    geolocator = Nominatim(user_agent=\"my_application\")\n    location = geolocator.geocode(location_text)\n    #print(location)\n    #print(type(location.raw))\n    if location:\n        state = location.raw['display_name'].split(',')[-2]\n        return state\n    else:\n        return \"Unknown\"\n\n# Create a UDF to map states to regions\n@udf(StringType())\ndef map_state_to_region(location):\n    state = extract_state(location).strip()\n    for region, states in regions_to_states.items():\n        if state in states:\n            return region\n    return \"Unknown\"  # Return \"Unknown\" for states not found in the dictionary\n\n# Apply the UDF to map states to regions\ndf_with_region = query.withColumn(\"region\", map_state_to_region(query[\"location\"]))\n\ndisplay(df_with_region)\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# VADER 감정 분석기 초기화\nanalyzer = SentimentIntensityAnalyzer()\n\n# Description 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef analyze_sentiment(description):\n    # VADER에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.polarity_scores(description)['neg']\n\n    # 감정 점수를 기반으로 심각도 분류\n    if sentiment_score >= 0.4:\n        return \"High\"\n    elif sentiment_score >= 0.2 and sentiment_score < 0.4:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# 감정 분석을 위한 UDF 생성\nsentiment_udf = udf(analyze_sentiment, StringType())\n\n# 처리된 DataFrame(processedDF)의 description 열에 UDF 적용\n# 실제 DataFrame 및 열 이름으로 \"processedDF\" 및 \"description_column\"을 대체합니다.\nprocessedDF_with_severity = query.withColumn(\"severity\", sentiment_udf(\"description\"))\n\n# 추가된 심각도 열이 있는 DataFrame 표시\ndisplay(processedDF_with_severity)\n```\n\n환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\n\n데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Real-time Stream Processing](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png)\n\n위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\n\n또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\n\n```js\n# 스트리밍 DataFrame을 임시 뷰로 등록\nprocessedDF_with_severity.createOrReplaceTempView(\"incident_reports\")\n\n# 집계를 위한 SQL 쿼리 정의\ntotal_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\"\"\n\nseverity_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\"\"\n\n# 집계 수행\ntotal_incidents_df = spark.sql(total_incidents_query)\nseverity_incidents_df = spark.sql(severity_incidents_query)\n\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Data Engineering Concepts](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png)\n\nSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\n\n1. 비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\n2. 전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\n\n- 단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\n\n```js\n# Delta Lake에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \"`result_delta_table`\"\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \"/FileStore/tables/checkpoints\"\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.writeStream\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_location)\\\n  .trigger(processingTime='10 seconds')\\ # 10초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .format(\"delta\")\\\n  .toTable(delta_table_path)\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\n\n![Delta Table as Dataframe](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png)\n\n또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\n\n![Query Delta Table with SQL](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png","tag":["Tech"],"readingTime":17},{"title":"데이터 엔지니어링 디자인 패턴","description":"","date":"2024-05-23 14:04","slug":"2024-05-23-DataEngineeringDesignPatterns","content":"\n디자인 패턴은 소프트웨어 엔지니어들만을 위한 것은 아닙니다. 최신 데이터 솔루션을 구축하는 데 도움이 되는 인기있는 데이터 엔지니어링 디자인 패턴을 알아봅시다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png)\n\nELT 패턴: 추출, 로드, 변환\n\n이는 RDBMS 세계에서 인기있던 ETL 패턴의 후속입니다. 데이터 엔지니어들이 다양한 소스(RDBMS, API 또는 스크래핑)에서 데이터를 추출하고, S3, ADLS Gen 2, 또는 GCS와 같은 객체 스토어에 로드한 후 Databricks와 같은 현대적인 도구를 사용하여 효율적으로 변환하는 인기있는 공통 패턴입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Lakehouse Pattern](/assets/img/2024-05-23-DataEngineeringDesignPatterns_1.png)\n\nDatabricks은 Lakehouse 아키텍처의 선두주자입니다. 이는 데이터 레이크(Data Lake)와 데이터 웨어하우스(Datawarehouse)를 통합합니다. 원시 또는 가공된, 구조화된 또는 반구조화된 데이터가 모두 하나의 환경 안에 모두 사용 가능하며 하나의 플랫폼(Databricks)에서 액세스할 수 있습니다.\n\n![Lakehouse Pattern](/assets/img/2024-05-23-DataEngineeringDesignPatterns_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n중개 아키텍처 패턴:\n\n메달리온 아키텍처 패턴은 주로 Databricks와 함께 사용되며 이제는 사실상의 표준이 되었습니다. 데이터 처리를 위해 원본 데이터인 청동, 정리 및 풍부화된 데이터인 은, 그리고 비즈니스 수준의 집계한 데이터인 금 레이어로 이루어져 있습니다.\n\nDeltaLake 아키텍처 패턴:\n\nDelta Lake은 초기에 Databricks에서 개발된 오픈 소스 프로젝트입니다. 이 프로젝트는 데이터 호수에 안정성을 제공합니다. Deltalake는 ACID 트랜잭션, 타임 트래벌, z-order, CDC, 스키마 진화 및 기타 최적화로 데이터 호수를 개선합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKappa Architecture Pattern:\n\nKappa Architecture에서는 모든 데이터가 스트림으로 처리됩니다. 이는 기존에 배치 처리로 다뤄졌을 것들을 연속적인 데이터 스트림으로 처리하는 것을 의미합니다. 시스템은 데이터가 도착하는 즉시 실시간으로 처리하며 별도의 배치로 처리하지 않습니다. Kappa Architecture는 실시간 처리에 최적화되어 있지만, \"배치\" 데이터로 간주될 수 있는 대규모의 과거 데이터를 스트림으로 재생하여 처리할 수 있습니다. Databricks Autoloader는 Kappa Architecture를 구현하는 데 가장 적합합니다.\n\nServerless Architecture Pattern:\n\n데이터 엔지니어링에서 서버리스는 클라우드 지역/IP 제약 사항이나 기타 기본 인프라 제약 조건에 대해 걱정하지 않고 데이터 파이프라인을 구축할 수 있습니다. 특히 변수 사용 패턴을 갖는 워크로드에 대한 즉시 클러스터 가용성과 비용 효율적인 스케일링을 위해 유용합니다. 많은 클라우드 제공업체가 이를 제공하고 있습니다. Databricks SQL은 Serverless를 제공하며, SQL 웨어하우스를 3초 미만의 시간 안에 론칭할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMLFlow 아키텍처 패턴:\n\nMLflow는 Databricks가 개발한 오픈 소스 프로젝트로, 실험, 재현성, 모델 배포 및 모델 제공을 포함한 기계 학습 라이프사이클을 관리하는 데 사용됩니다.\n\n이러한 패턴은 견고하고 확장 가능한 데이터 시스템을 설계하는 데 중요합니다. Databricks를 이용하면 이러한 패턴을 더욱 간편하게 구현할 수 있으며 효율적인 데이터 처리 및 분석을 위한 통합 도구를 제공합니다.\n\n좋아하는 패턴이 빠졌나요? 댓글 섹션에서 알려주세요. 흥미로운 내용이라고 생각되면 반가워 하지 마세요.\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터브릭의 DBRX를 사용하면 실시간으로 학습할 수 있어요, 미세 조정 필요 없어요","description":"","date":"2024-05-23 14:02","slug":"2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning","content":"\n![DBRX](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_0.png)\n\nDBRX이란 무엇인가요?\n\nDBRX는 Databricks의 최신 언어 모델 중 하나입니다. 언어 모델이란 충분한 예제를 학습하여 인간의 언어나 다른 유형의 복잡한 데이터를 인식하고 해석할 수 있는 컴퓨터 프로그램입니다. 많은 언어 모델들은 수십억 또는 수백만 기가바이트에 달하는 인터넷에서 수집한 데이터를 기반으로 학습됩니다.\n\nDBRX는 Databricks에서 개발한 오픈, 일반 목적의 언어 모델입니다. 다양한 표준 벤치마크를 통해 DBRX는 이미 알려진 오픈 언어 모델들에 대한 최신 결과를 보여주고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDBRX는 고밀도 전문가 혼합(MoE) 아키텍처 덕분에 오픈 모델 중 효율성 면에서 최첨단 기술을 선도합니다. LLaMA2-70B보다 추론이 최대 2배 빠르고, 전체 및 활성 매개변수 개수 측면에서 Grok-1의 약 40% 크기입니다.\n\nDBRX는 총 1320억 개의 매개변수 중 360억 개가 어떤 입력에 대해서도 활성인 미세 구조 MoE 아키텍처를 사용하는 대형 언어 모델(LLM)입니다. 디코더 전용이며, 12조 개의 텍스트 및 코드 데이터 토큰을 포함하는 대규모 데이터 세트에서 다음 토큰 예측을 사용하여 교육되었습니다. Mixtral 및 Grok-1과 같은 유사한 모델과 달리, DBRX는 세부 접근 방식을 채용하며, 16개 전문가를 활용하고 그 중 4개를 선택합니다. 반면 다른 모델들은 8개 전문가를 가지고 2개를 선택합니다.\n\n전통적인 언어 모델은 최근 이벤트나 트레이닝 데이터 외의 정보를 예측하는 능력에 제한이 있어서 현재 주제에 대한 쿼리에 대해 효과적이지 않을 수 있습니다. 이 제한으로 인해 언어 모델의 생성 능력을 보완하기 위해 검색 증강 생성(RAG)이 필요해졌습니다. 외부 소스를 통합함으로써 RAG는 특히 모델 훈련 데이터를 넘어서는 최근 이벤트나 주제에 대한 질의에 대한 정확도와 시기적절성을 향상시킵니다.\n\nDBRX를 선택하는 이유는 무엇일까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 오픈 소스 모델 우위: 데이터브릭스의 DBRX 모델은 LLaMA2-70B, Mixtral 및 Grok-1과 같은 주요 오픈 소스 모델과 비교하여 우수한 성능을 보여줍니다. 이는 데이터브릭스가 언어 이해, 프로그래밍, 수학 및 논리와 같은 여러 영역에서 오픈 소스 모델 품질 향상에 기여하겠다는 의지를 나타냅니다. 이는 데이터브릭스가 지원하기를 자랑스럽게 생각하는 트렌드입니다.\n\n![이미지 1](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_1.png)\n\n- 전용 모델보다 우위: DBRX는 다양한 벤치마크에서 GPT-3.5를 능가하여, 데이터브릭스의 다양한 고객 기반 중에서 전용 모델 대신 오픈 소스 모델을 선호함에 대한 주목할만한 변화와 조화를 이룹니다. 데이터브릭스는 고객이 오픈 소스 모델을 사용자 지정하여 특정 요구 사항에 맞게 맞춤화하여 더 나은 품질과 속도를 달성할 수 있다는 능력을 강조합니다. 이는 기업과 조직에서 오픈 소스 모델의 도입을 가속화시킬 수 있는 가능성을 제시합니다.\n\n![이미지 2](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- DBRX의 MoE 아키텍처로 향상된 효율성과 확장성: MegaBlocks 연구 및 오픈소스 프로젝트에서 개발된 DBRX의 Mixture-of-Experts(MoE) 모델 아키텍처는 초당 처리된 토큰 수에 대해 높은 속도를 제공합니다. Databricks는 이 혁신이 미래 오픈소스 모델이 MoE 구조를 채택하도록 이끌어내며, 대규모 모델의 훈련을 유지하면서 빠른 처리량을 유지할 수 있도록 할 것으로 기대합니다. DBRX는 총 1320억 개의 매개변수 중 언제든지 360억 개의 매개변수만을 활용하며, 속도와 성능 사이의 균형을 제공하여 사용자에게 효율적인 솔루션을 제공합니다.\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_3.png)\n\n언어 모델을 위한 지식 창고!!!\n\nLLM을 위한 지식 창고는 외부 소스의 실시간 정보를 추가함으로써 언어 모델을 더 스마트하게 만듭니다. 이는 다양한 주제에 걸쳐보다 정확하고 의미 있는 응답을 제공하는 데 도움을 줍니다. RAG, 또는 검색 증강 생성,은 이의 주요 구성 요소로서 정확한 정보를 찾아 활용하여 응답을 개선하는 데 도움을 줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![그림](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_4.png)\n\nRAG(검색 보강 생성)이 무엇인가요?\n\n전통적인 언어 모델은 최근 사건이나 학습 데이터 외의 정보를 예측하는 능력이 제한되어 있어서 현재 주제에 대한 질의에 덜 효과적입니다. 이 한계로 인해 검색 보강 생성이 필요해졌습니다.\n\nRAG 또는 검색 보강 생성은 언어를 이해하고 생성하는 새로운 방법입니다. 이 방법은 두 가지 종류의 모델을 결합합니다. 먼저, 관련 정보를 검색하고, 그 정보를 기반으로 텍스트를 생성합니다. 이 두 가지를 함께 사용함으로써 RAG는 놀라운 성과를 거두었습니다. 각 모델의 강점이 서로의 약점을 보완하기 때문에, RAG는 자연어 처리의 혁신적인 방법으로 각광을 받고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 표를 변환한 내용입니다.\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_5.png)\n\n벡터 저장소(Vector Store)는 무엇인가요?\n\n벡터 저장소와 벡터 검색은 현대 정보 검색 시스템의 필수 구성 요소입니다.\n\n- **벡터 저장소(Vector Store)**: 각 정보를 벡터로 나타내어 데이터를 저장하는 데이터베이스와 같은 역할입니다. 이러한 방식을 사용하면 각 정보를 다차원 공간에서 수학적으로 표현한 벡터로 저장할 수 있습니다. 이는 기존 인덱싱 방법이 아닌 유사성 측정 기준에 따라 데이터를 효율적으로 저장하고 검색할 수 있도록 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_6.png)\n\n벡터 검색: 벡터 간 유사성을 비교하여 관련 정보를 찾는 과정입니다. 키워드 매칭이나 기타 전통적인 검색 기술 대신 벡터 검색은 벡터 저장소에서 유사한 벡터를 식별하여 정확한 쿼리 용어를 포함하지 않더라도 의미론적으로 관련된 결과를 반환합니다.\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_7.png)\n\nHands-on RAG 데모:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 Databricks를 플랫폼으로 선택한 이유는 DBRX라는 기본 모델과 엔드 투 엔드 기계 학습 및 딥 러닝 워크플로에 맞춘 도구 및 라이브러리들을 제공하기 때문입니다.\n\n코드 개요:\n\n- Databricks Foundational Models에서 Chat 모델(DBRX)과 임베딩 모델을 가져오기\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_8.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. Hugging Face에서 GPT Tokenizer를 가져오기\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_9.png)\n\n3. 챗 어시스턴트를 위한 클래스 생성: 이 클래스는 Tokenizer, Embedding, Docs 및 LLM을 입력으로 사용하여 클래스 객체를 인스턴스화합니다.\n\n- get_pdf_text(): 제공된 문서에서 텍스트를 추출합니다.\n- Chunk_return(): 텍스트를 입력으로 받아 토큰화하고 거대한 텍스트를 청크로 분할합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_10.png)\n\n- get_vector_text(): 청크를 삽입하고 FAISS 인덱스를 사용하여 내장된 콘텐츠를 색인화하여 Vector Library를 반환합니다.\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_11.png)\n\n- get_conve_chain(): 대화형 Q&A 체인을 가져오는 메서드이며, DBRX의 프롬프트 템플릿 및 언어 모델과 함께 반환합니다.\n- query(): 이 메서드는 LLM 체인과 Vector Library를 호출하여 언어 모델 (DBRX)에 공급하는 데 사용됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예시:\n\n- RAG에 대해 DBRX에 쿼리하는 방법\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_12.png)\n\n- 지식 창과 함께 DBRX에 쿼리하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![DBRX](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_13.png)\n\n결론:\n\nDatabricks의 최고 수준 언어 모델인 DBRX는 언어 및 코드를 이해하는 데 뛰어나지만 최근 업데이트에 대한 처리가 약간 어려울 수 있습니다. 따라서 우리는 이를 지원하기 위해 검색 증강 생성(Retrieval-Augmented Generation, RAG) 기술을 사용합니다. 이 기술은 정보를 찾아 새로운 텍스트를 생성하는 방식을 결합하여 DBRX를 더욱 똑똑하게 만듭니다. Databricks에서 구현된 RAG 및 DBRX는 머신 러닝을 보다 쉽고 효과적으로 만들어줍니다.\n","ogImage":{"url":"/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_0.png"},"coverImage":"/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_0.png","tag":["Tech"],"readingTime":8},{"title":"넷플릭스 데이터 쇄도를 탐색하며 효과적인 데이터 관리의 필수성","description":"","date":"2024-05-23 14:00","slug":"2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement","content":"\nBy Vinay Kawade, Obi-Ike Nwoke, Vlad Sydorenko, Priyesh Narayanan, Shannon Heh, Shunfei Chen\n\n소개\n\n오늘날, 디지털 시대에 데이터가 전례없는 속도로 생성되고 있습니다. 예를 들어, Netflix를 살펴보겠습니다. 세계 각지의 Netflix 스튜디오에서 매년 수백 PB의 에셋이 생성됩니다. 콘텐츠는 텍스트와 이미지 시퀀스부터 소스 인코딩용 큰 IMF¹ 파일에 이르기까지 다양합니다. 때로는 생성된 프록시와 중간 파일도 있습니다. 스튜디오로부터 흘러나오는 이 방대한 데이터 양은 실질적인 통찰을 얻기 위한 효율적인 데이터 관리 전략에 대한 중대한 필요성을 강조합니다. 주목할 점은 모든 콘텐츠의 상당 부분이 미사용 상태로 남아있는 것입니다.\n\n본 기사에서, 저희 미디어 인프라스트럭처 플랫폼 팀은 생산 데이터를 효과적으로 관리하기 위한 해결책인 Garbage Collector의 개발을 개요로 설명합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nThe Magnitude of Data Generation\n\n매주 전 세계의 수백 개의 Netflix 스튜디오에서 약 2 페타바이트의 데이터가 생산됩니다. 이는 텍스트, 이미지, 이미지 시퀀스, IMF 등 다양한 소스로부터 생성된 복합 데이터입니다. 이 규모의 데이터는 엄청납니다. 이 정보를 효과적으로 총정리하고 분석하는 것이 더 어려워지고 있습니다. 또한 이로 인해 저장 비용이 크게 증가했습니다. 우리는 역대 기록적인 저장 비용 증가률인 매년 50% 증가를 보고 있습니다. 동시에 내부 연구에 따르면 적어도 데이터의 40%가 사용되지 않고 낭비되고 있다고 합니다.\n\n[이미지]\n\n효과적인 데이터 관리의 필요성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터가 계속해서 증가함에 따라, 특히 스튜디오에서 수집된 데이터는 일반적으로 업로드 - 읽기 - 재업로드 - 미해당을 따르는 수명주기를 거칩니다. 효율적으로 관리하는 작업은 점점 중요성을 갖게 됩니다. Netflix에서는 미디어 인프라 및 저장 플랫폼 팀이 사용자 조치 또는 사전 구성된 수명 주기 정책에 따라 파일 객체를 모니터링하고 정리하는 확장 가능하고 비동기적인 Garbage Collector (GC)를 사용한 데이터 수명주기 관리 솔루션을 개발했습니다. GC는 팀의 Baggins 서비스의 구성 요소로 S3 위에 미디어 특정 사용 사례에 맞춘 내부 추상화 계층입니다.\n\n아키텍처\n\n상위 수준에서 수명주기 관리자를 설계하여 안전하게 삭제하거나 더 차가운 저장소로 이동할 수 있는 데이터를 수동으로 모니터링하고 제거합니다. Garbage Collection에 대해 \"표시 및 정리\"의 관점을 취합니다.\n\n먼저, 우리는 모든 바이트를 AWS의 Simple Storage Service (S3)에 저장합니다. 그러나, S3의 각 파일에 대해 Baggins에 각 파일의 일부 메타데이터를 유지합니다. 이 메타데이터는 카산드라 데이터베이스에 저장되며 파일의 메타해시 목록 체크섬인 SHA-1, MD5 및 전체 파일에 대한 XXHash, 클라이언트 측 암호화된 객체를 위한 암호화 키와 같은 여러 필드가 포함되어 있습니다. S3에서 이러한 파일과 상호 작용하는 수백 개의 내부 넷플릭스 응용 프로그램이 있으며, 종종 여러 프록시, 파생물, 클립 등을 생성합니다. 이러한 응용 프로그램에는 프로모 미디어 생성, 마케팅 통신, 콘텐츠 인텔리전스, 자산 관리 플랫폼 등의 워크플로우가 포함됩니다. 이러한 응용 프로그램은 불필요한 파일을 필요할 때마다 삭제하거나 객체의 TTL을 사전 구성하여 파일이 자동으로 생성된 이후 일정 간격마다 삭제할 수 있습니다. 이 기간은 7일, 15일, 30일, 60일 또는 180일로 설정할 수 있습니다. 삭제 API를 통해 데이터베이스에서 해당 객체를 소프트 삭제로 표시하고 S3에서 해당 객체에 대한 액세스를 중지합니다. 이 소프트 삭제는 99% 분위수에서 삭제 API의 초당 15밀리초 미만의 초저지연을 유지합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위에서 언급한 것처럼, 우리는 얼마 지나지 않아 '하드 삭제' 문제에 직면하게 될 것입니다. 이것은 삭제된 파일과 관련된 모든 흔적이 우리 시스템에서 제거되어야 함을 의미합니다. 이는 S3에서 바이트를 정리하고, Elastic Search에서 수행된 모든 색인을 삭제하며, 마지막으로는 Cassandra DB에서 모든 메타데이터를 삭제하는 것을 포함합니다. 우리는 다음과 같은 요구 사항을 가지고 시작했습니다.\n\n- 삭제 API(소프트 삭제)를 낮은 대기 시간으로 유지합니다.\n- 24시간 내내 수동 정리를 수행합니다.\n- 온라인 데이터베이스에 영향을 미치기 시작하면 정리를 제한합니다.\n- 정리가 급격히 증가해도 쉽게 확장할 수 있습니다.\n- 매일 정리를 편리하게 시각화합니다.\n- 아카이빙과 같은 다른 데이터 최적화 작업을 실행하기 위한 일반적인 프레임워크를 구축합니다.\n\n시스템 하의 구조\n\n다음은 시스템의 고수준 아키텍처입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_1.png\" />\n\n클라이언트 관점에서는 ~15밀리초의 대기 시간을 소비하는 삭제 API를 간단히 호출합니다. 그러나 이후에는 이 객체 및 모든 메타데이터를 적절하게 정리하고 모든 위치에서 제대로 정리하는 일이 많이 발생합니다. 이 다이어그램은 처음에 약간 복잡해 보일 수 있지만, 화살표를 추적하면 이해가 쉬워지고 전체 그림이 명확해집니다. 위의 주기는 매일 수행되며, 수동 및 자동화된 방식으로 작동하여 그 날과 지금까지 누적된 삭제 대상 데이터와 백로그를 정리합니다.\n\n- 객체 키 가져오기: 모든 S3 객체는 Cassandra 데이터베이스에 해당 레코드가 있습니다. Cassandra 메타데이터 테이블의 항목들은 주요 참조 지점으로 기능합니다. 대응하는 행에 삭제 표시자를 추가하여 해당 행의 객체 키를 소프트 삭제하도록 표시합니다. 동일한 데이터베이스에는 버킷 수준 구성 정보를 보유하는 테이블도 있으며, 버킷 수준 사전 구성 TTL과 같은 정보를 포함합니다. 그러나 Cassandra는 파티션 키를 제공하지 않으면 TTL이 지정되거나 소프트로 삭제된 행을 필터링하는 것을 허용하지 않습니다. 또한, 메타데이터는 여러 테이블에 분산되어 있으며 정규화가 필요합니다. 여기서 Casspactor와 Apache Iceberg가 필요합니다.\n- Casspactor: 이는 Netflix의 내부 도구로, Cassandra 테이블을 Iceberg로 내보내는 데 사용됩니다. Casspactor는 Cassandra의 백업 복사본을 기반으로 작동하며 온라인 데이터베이스에 대한 지연시간을 발생시키지 않습니다. Iceberg는 Netflix에서 만들어진 오픈 소스 데이터 웨어하우스 솔루션으로, 구조화된/구조화되지 않은 데이터에 대한 SQL과 유사한 액세스를 제공합니다. TTL이 지정된/소프트 삭제된 행을 얻기 위해 Cassandra를 쿼리하는 대신 Iceberg의 복사된 행을 대상으로 쿼리할 수 있게 되었습니다.\n- 우리는 Netflix의 Workflow Orchestration 프레임워크 Maestro를 사용하여 매일 Iceberg를 호출하여 그 날 정리 작업 가능한 키를 필터링하는 일일 워크플로우를 설정합니다. 이 흐름은 버킷 수준 TTL 구성 및 객체 수준 삭제 표시자를 고려합니다.\n- Maestro 워크플로우는 다른 임시 Iceberg 테이블에 결과를 집계하여 이후 작업의 소스로 사용합니다.\n- 그 날 삭제해야 할 모든 관심 있는 행이 준비된 후, 우리는 데이터 이동 및 처리를 위한 홈그로운 솔루션을 사용하여 이를 모두 Kafka 큐로 내보냅니다. Iceberg 커넥터와 Kafka 싱크로 구성된 Data Mesh 파이프라인을 만들었습니다.\n- 우리는 이 Kafka 주제를 청취하는 몇 개의 Garbage Collector (GC) Worker를 실행하고 각 행에 대해 삭제 작업을 수행합니다. 이러한 작업은 수평 확장 가능하며 Kafka 파이프라인에서 발생하는 스파이크나 백로그에 기반해 자동으로 확장됩니다.\n- GC 워커는 객체의 완전한 정리를 처리합니다. 먼저 S3에서 모든 바이트를 삭제합니다. 이는 AWS S3에서 부과하는 5TB 최대 파일 크기 제한을 초과하는 경우 단일 파일 또는 다중 파일이 될 수 있습니다.\n- 그런 다음 우리는 로컬 Elastic Search에서 이 소프트 삭제된 파일에 대한 모든 참조를 정리합니다.\n- 마지막으로, 근무자들은 온라인 Cassandra 데이터베이스에서 이 항목을 제거하여 루프를 완료합니다. GC 워커의 자동 스케일링도 현재 부하 및 지연 시간에 대한 Cassandra 데이터베이스의 입력을 받습니다. 이러한 매개 변수들은 데이터 삭제 속도에 대한 제어된 속도 조절에 반영됩니다.\n- 마지막 단계는 매일 삭제하는 파일 수와 크기에 대한 세부 내용을 제공하는 대시보드를 강화합니다. 이를 위해 Apache Superset을 사용합니다. 이를 통해 AWS 송장과 미래 비용을 예측하는 데 사용할 수 있는 상당한 데이터를 확보할 수 있습니다.\n\n저장 통계\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이렇게 우리 대시보드가 어떻게 보이는지 알려드릴게요,\n\n![대시보드 스냅샷](/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_2.png)\n\n![대시보드 스냅샷](/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_3.png)\n\n필수 통찰력\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 정리 전략 수립을 우선시하세요. 데이터 정리 프로세스는 초기 설계의 중요한 부분이어야 합니다. 후속 고려 사항이 아닙니다. 시간이 흐를수록 데이터 정리는 적절히 관리되지 않으면 압도적인 작업으로 번질 수 있습니다.\n- 지출을 지속적으로 모니터링하고 기록하세요. 비용 없는 부분은 없습니다. 데이터의 각 바이트는 금전적 영향을 가지고 있습니다. 따라서 저장된 모든 데이터에 대한 포괄적인 계획이 필수적입니다. 이는 특정 기간 이후 데이터를 삭제하거나 사용되지 않을 때 더 비용 효율적인 저장 계층으로 이전하거나, 적어도 미래 참조 및 의사 결정을 위해 무기한 보유를 위한 사유를 유지하는 것을 포함할 수 있습니다.\n- 디자인은 다양한 작업과 환경에서 사용할 수 있는 유연성을 가져야 합니다. 활성 데이터부터 비활성 데이터 보관, 그리고 중간에 있는 모든 것까지 관리할 수 있어야 합니다.\n\n**결론**\n\n지수적인 데이터 증가 시대를 계속해서 탐색함에 따라 효과적인 데이터 관리의 필요성은 지나치게 강조될 수 없습니다. 데이터의 양을 다루는 것뿐만 아니라 품질과 관련성을 이해하는 것입니다. 포괄적인 데이터 관리 전략에 투자하는 기ꢣ치는 새로운 데이터 중심 환경에서 선도할 기업들이 될 것입니다.\n\n용어\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nIMF (Interoperable Master Format): 이것은 오디오와 비디오 마스터 파일의 디지털 전송과 저장에 사용되는 표준화된 형식입니다. 더 많은 정보를 원하시면 이 개요를 방문해보세요.\n\nMHL (미디어 해시 목록): 이것은 미디어 파일에서 체크섬을 보존하기 위해 전송 중에 그리고 정적 상태에서 저장하는 데 사용되는 표준입니다. 더 많은 정보를 원하시면 https://mediahashlist.org 를 방문해보세요.\n\nPB (페타바이트): 디지털 정보 저장의 단위로, 천 테라바이트 또는 백만 기가바이트에 해당합니다.\n\nTTL (Time-to-Live): 이 용어는 데이터가 폐기되기 전에 저장되는 기간을 의미합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n감사의 말씀\n\n마에스트로 팀, 캐스팩터 팀, 아이스버그 팀의 업무에 기여한 동료인 엠리 쇼, 앙쿠르 케트라팔, 에샤 팔타, 빅터 예레비치, 미나크시 진달, 페이지 후, 동동 우, 챈텔 양, 그레고리 알몬드, 아비 칸다사미, 그 외 훌륭한 동료들께 특별한 감사를 전합니다.\n","ogImage":{"url":"/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_0.png"},"coverImage":"/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_0.png","tag":["Tech"],"readingTime":9},{"title":"데이터 안전 보장 암호화되지 않은 RDS 데이터베이스를 암호화된 데이터베이스로 이전하기","description":"","date":"2024-05-23 13:59","slug":"2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne","content":"\n![태그](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_0.png)\n\nAmazon RDS DB 인스턴스에 대한 암호화는 생성 시에만 활성화할 수 있습니다. 기존 인스턴스를 암호화하려면 스냅샷을 생성한 후 해당 스냅샷의 암호화된 복사본을 만들어 새로운 암호화된 인스턴스로 복원합니다. 다운타임이 허용되는 경우에는 새 인스턴스로 애플리케이션을 전환하십시오. 최소한의 다운타임을 위해 AWS 데이터베이스 마이그레이션 서비스(AWS DMS)를 사용하여 데이터를 지속적으로 마이그레이션하고 복제함으로써 새로운 암호화된 데이터베이스로의 원활한 전환을 허용할 수 있습니다.\n\n## 1. 암호화 상태 확인\n\n첫 번째 단계는 현재 사용 중인 RDS 데이터베이스가 암호화되었는지 확인하는 것입니다. AWS 관리 콘솔에 로그인하고 RDS 서비스로 이동합니다. 대상 데이터베이스를 찾고 \"구성\" 탭을 클릭합니다. \"암호화\" 섹션을 찾아보십시오. 만약 \"활성화되지 않음\"으로 표시된다면 데이터베이스가 암호화되지 않았음을 의미합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_1.png)\n\n## 2. 스냅샷 생성\n\n암호화하려는 인스턴스의 DB 스냅샷을 만듭니다. 스냅샷을 만드는 데 걸리는 시간은 데이터베이스의 크기에 따라 다릅니다. 이제 왼쪽 메뉴에서 \"스냅샷\" 옵션으로 이동하고 \"스냅샷 촬영\"을 클릭합니다. 쉽죠?\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 스냅샷을 만들 데이터베이스를 선택해야 해요:\n\n![image](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_3.png)\n\n이를 \"UnencryptedSnapshot\"이라고 이름 짓을 수 있어요. 실제 데이터베이스의 사본이 될 거에요. 스냅샷이 생성되기를 기다려 주세요. 제 경우에는 약 2분이 걸렸어요.\n\n## 3. 사본 암호화\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 생성한 DB 스냅샷을 선택해 주세요. 작업에서 '스냅샷 복사'를 선택하세요. 대상 AWS 지역 및 DB 스냗샷 사본의 이름을 해당 필드에 제공해 주세요. '암호화 사용' 확인란을 선택하세요. 마스터 키로는 DB 스냅샷 사본을 암호화하는 데 사용할 KMS 키 식별자를 지정하세요. '스냅샷 복사'를 선택하세요.\n\n그리고 보시다시피 제가 암호화를 활성화했으며 (기본) aws/rds를 AWS KMS 키로 선택했습니다.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_4.png)\n\n두 번째 스냅샷이 완료될 때까지 다시 기다릴 것입니다. (참고: 5분이 걸렸습니다)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4. 암호화된 데이터베이스 복원\n\n스냅샷을 사용할 수 있는 상태가 되면, 데이터베이스를 복원해야 합니다. \"암호화된 스냅샷\"을 선택한 후 \"작업\"을 클릭한 다음 \"스냅샷 복원\"을 선택하세요. DB 인스턴스 식별자에는 새로운 DB 인스턴스를 위한 고유한 이름을 제공하세요. 동일한 구성을 유지하겠으며 필요에 따라 편집할 수도 있습니다. 모든 옵션을 확인한 후 복원을 클릭하세요.\n\n지금은 DB가 생성 중인 상태입니다:\n\n![image](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. DMS를 사용한 데이터 마이그레이션\n\n마지막 단계는 DMS로 이동하여 작업을 만들어야 합니다. 그러기 전에 소스 엔드포인트, 대상 엔드포인트 및 복제 인스턴스를 만들어야 합니다.\n\n먼저 복제 인스턴스에서 시작해야 합니다. 왼쪽 메뉴 탭에서 '복제 인스턴스'를 선택하고 \"복제 인스턴스 생성\"을 선택하세요.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다른 구성에 대한 선호도를 포함할 수 있습니다. 인스턴스가 생성되었으므로 엔드포인트로 진행할 수 있습니다.\n\n소스 엔드포인트의 경우 DMS 콘솔에서 \"엔드포인트\"를 선택하고 \"엔드포인트 생성\"을 해야 합니다.\n\n아래 이미지에서 보이는 대로 소스 엔드포인트로 비암호화된 RDS를 선택해야 합니다:\n\n![image](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_7.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_8.png)\n\n수동으로 자격 증명을 추가하는 경우, 비밀번호를 추가해야할 것입니다. 비밀번호를 추가하려면 \"검색\"을 클릭하면 모든 데이터베이스 정보를 찾을 수 있는 시크릿 매니저를 확인해야 합니다.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_9.png)\n\n이번에도 대상 엔드포인트에 대해 동일한 단계를 수행할 것입니다. 새 데이터베이스를 선택하기만 하면 됩니다. 이제 두 개의 엔드포인트가 준비되었으므로 작업을 생성할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식의 텍스트입니다.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_10.png)\n\n작업을 만들려면 왼쪽 메뉴에서 \"데이터 마이그레이션 작업\"으로 이동하여 작업을 만들어야 합니다:\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_11.png)\n\n소스, 대상, 복제 인스턴스 및 마이그레이션 유형을 포함한 설정을 올바르게 구성하고, \"기존 데이터 마이그레이션 및 지속적인 변경 복제\"를 선택해야 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래의 표태그를 Markdown 형식으로 변경해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\n","ogImage":{"url":"/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_0.png"},"coverImage":"/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_0.png","tag":["Tech"],"readingTime":7}],"page":"69","totalPageCount":116,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}