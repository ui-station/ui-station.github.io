{"pageProps":{"posts":[{"title":"스프링 부트 대 Go 프레임워크 데이터베이스 조회 성능","description":"","date":"2024-05-20 15:41","slug":"2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance","content":"\n\n<img src=\"/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_0.png\" />\n\n안녕하세요! 이 글은 요청받은 기사입니다. 독자들이 Spring Boot와 가상 스레드를 비교하고 Gin, Fiber, Echo와 같은 인기 있는 Go 프레임워크를 최신 버전으로 비교해 달라고 요청했습니다. Spring Boot는 매우 포괄적이고 다양한 기능 세트를 갖춘 프레임워크인 반면에 언급된 Go 프레임워크들은 비교적 간단하며(Express와 유사한) 기능이 간단한 것에 유의해 주세요.\n\n이전 기사에서 우리는 가장 간단한 \"hello world\" 케이스에서 Spring Boot가 Go 프레임워크와 어떻게 맞붙는지 살펴보았습니다.\n\n이번 기사에서는 좀 더 실제적인 케이스에 초점을 맞출 것입니다: 데이터베이스 조회입니다. 간단한 사용 예는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- HTTP 요청 받기\n- 요청 본문(JSON)에서 userEmail 매개변수 추출하기\n- 추출된 이메일로 데이터베이스 조회 수행\n- HTTP 응답에서 사용자 레코드 반환\n\n# 테스트 설정\n\n모든 테스트는 MacBook Pro M2에서 16GB RAM 및 8+4 CPU 코어를 사용하여 실행되었습니다. 소프트웨어 버전은 다음과 같습니다:\n\n- SpringBoot 3.2.5 (Java 21.0.3)\n- Go 1.22.3\n\n<div class=\"content-ad\"></div>\n\n\n테스트는 Bombardier 로드 테스터를 사용하여 진행됩니다.\n\n모바일 앱에서는 Bun ORM을 사용하고 있습니다 (Bun JS 런타임과 혼동하지 마세요).\n\n애플리케이션 코드는 다음과 같습니다:\n\nSpringBoot\n\n<div class=\"content-ad\"></div>\n\n기록을 표시해왔던 다음과 같은 과정을 반복하셨습니다.\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 차트 형식으로 결과가 제공됩니다:\n\n- ![이미지1](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_1.png)\n- ![이미지2](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_2.png)\n- ![이미지3](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_3.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_4.png)\n\n![Image 2](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_5.png)\n\n![Image 3](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_6.png)\n\n![Image 4](/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_7.png)\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_8.png\" />\n\n<img src=\"/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_9.png\" />\n\n<img src=\"/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_10.png\" />\n\n# 분석\n\n<div class=\"content-ad\"></div>\n\n먼저, Spring Boot와 Go 프레임워크는 매우 다릅니다. Spring의 기능 세트는 간단한 Go 프레임워크에 비해 너무 방대합니다.\n\n데이터베이스 읽기와 같이 I/O 집중적인 경우에는 Go 프레임워크가 Spring Boot에 비해 2배 빠릅니다. Go 측에서 가장 빠른 것은 38K RPS를 제공하는 Fiber입니다.\n\nSpring Boot의 CPU 사용량은 모든 Go 프레임워크와 비교할 만큼 유사합니다. 그러나 Spring Boot의 메모리 사용량은 Go 프레임워크와 비교했을 때 너무 높습니다.\n\n승자: Fiber\n\n<div class=\"content-ad\"></div>\n\n여기에 비슷한 \"hello world\" 비교가 있습니다:","ogImage":{"url":"/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_0.png"},"coverImage":"/assets/img/2024-05-20-SpringBootvsGoFrameworksDatabasereadPerformance_0.png","tag":["Tech"],"readingTime":3},{"title":"스프링 클라우드 게이트웨이  스프링 부트 30  로드 밸런싱","description":"","date":"2024-05-20 15:39","slug":"2024-05-20-SpringCloudGatewaySpringBoot30Loadbalancing","content":"\n\n\n![Spring Cloud Gateway](/assets/img/2024-05-20-SpringCloudGatewaySpringBoot30Loadbalancing_0.png)\n\nSpring Cloud Gateway는 Spring Boot 애플리케이션에서 API 게이트웨이를 구축하는 강력하고 유연한 솔루션을 제공합니다. 다양한 기능을 갖춘 Spring Cloud Gateway는 라우팅, 요금 제한, 보안 및 장애 관리를 처리할 수 있습니다.\n\n![Spring Cloud Gateway](/assets/img/2024-05-20-SpringCloudGatewaySpringBoot30Loadbalancing_1.png)\n\n마이크로서비스 아키텍처 시대에는 견고하고 확장 가능한 시스템 구축이 중요합니다. Spring Boot와 Spring Cloud의 발전으로 개발자들은 이전 Netflix Zuul에 의존하지 않고도 견고한 마이크로서비스 기반 애플리케이션을 생성할 수 있는 강력한 도구를 사용할 수 있습니다. 이 블로그 포스트에서는 Spring Boot 3.0과 함께 Spring Cloud Gateway를 사용하여 현대적인 마이크로서비스 아키텍처를 어떻게 만들 수 있는지 살펴보겠습니다.\n\n\n<div class=\"content-ad\"></div>\n\nSpring Boot이 계속 발전함에 따라 더 최신 기술과 모범 사례에 적응하는 것이 중요합니다. Spring Boot 3.0에서는 마이크로서비스를 구축하기 위한 현대적인 솔루션에 중점을 두고 있습니다. Spring WebFlux 기반으로 구축된 강력한 API 게이트웨이인 Spring Cloud Gateway가 그러한 솔루션 중 하나로, 라우팅, 필터링, 로드 밸런싱과 같은 기능을 제공합니다.\n\n게이트웨이를 설명하기 위해 적합한 아키텍처가 필요합니다.\n\n- Eureka 서버: 마이크로서비스를 관리하기 위한 서비스 레지스트리 및 디스커버리 서버입니다. Eureka 서버와 함께 스프링 부트 프로젝트를 생성하세요.\n- Payment MS: 테스트를 위한 사용자 정의 마이크로서비스로, 웹 및 Eureka 클라이언트 종속성 및 설정이 있는 간단한 마이크로서비스입니다.\n- Spring Cloud Gateway: 적절한 마이크로서비스로 요청을 라우팅하는 API 게이트웨이입니다.\n\n아래 종속성을 가진 Spring Boot 애플리케이션을 만들어 보세요. Eureka는 서비스 디스커버리 서버로 사용될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\n의존성 테이블을 다음과 같이 변경해보세요:\n\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n</dependency>\n\n\n- @EnableEurekaServer 주석을 사용하여 주 클래스에 Eureka 서버를 활성화하세요.\n\n```java\n@SpringBootApplication\n@EnableEurekaServer\npublic class MyEurekaServerApplication {\n```\n\n```java\n public static void main(String[] args) {\n  SpringApplication.run(MyEurekaServerApplication.class, args);\n }\n}\n```\n\n그리고 My application.properties 파일에도 변경을 적용해주세요.\n\n<div class=\"content-ad\"></div>\n\n```bash\n// application.properties\neureka.client.fetch-registry=false\neureka.client.register-with-eureka=false\nserver.port=8761\nspring.application.name=MY-EUREKA-SERVER\n```\n\n아래 종속성을 사용하여 Spring Boot 어플리케이션을 생성하세요.\n\n```js\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n<dependency>\n   <groupId>org.springframework.cloud</groupId>\n   <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n</dependency>\n```\n\n```js\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-test</artifactId>\n   <scope>test</scope>\n</dependency>\n```\n\n<div class=\"content-ad\"></div>\n\n서버 포트를 출력하는 REST API를 작성해보세요. 이것은 테스트 중에 호출된 서비스의 어떤 인스턴스인지 식별하는 데 도움이 될 것입니다 [로드 밸런싱]\n\n```java\n@RestController\n@RequestMapping(\"/payment\")\npublic class PayController {\n```\n\n```java\n@Value(\"${server.port}\")\nprivate String serverPort;\n    \n@GetMapping(\"/say\")\npublic String getMethodName() {\n    return new String(\"안녕하세요, Payment MS에서 온 메시지입니다. \"+serverPort);\n}\n```\n\n그리고 Eureka 서버 세부 정보를 추가하고 메인 클래스에 Eureka 클라이언트를 활성화하는 주석을 추가하는 것을 잊지 마세요 (@EnableDiscoveryClient)\n\n<div class=\"content-ad\"></div>\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class PaymentMsApplication {\n \n public static void main(String[] args) {\n  SpringApplication.run(PaymentMsApplication.class, args);\n }\n}\n```\n\n- 그리고 나의 MS를 위한 어플리케이션 프로퍼티\n\n```java\nspring.application.name=PAYMENT-SERVICE\neureka.client.service-url.defaultZone=http://localhost:8761/eureka\nserver.port=9811 (어플리케이션을 실행하기 전에 다른 포트를 사용 중입니다)\neureka.instance.ip-address=localhost\n```\n\n<div class=\"content-ad\"></div>\n\n```md\n<dependency>\n   <groupId>org.springframework.cloud</groupId>\n   <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n</dependency>\n```\n\n```md\n<!--\nhttps://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-gateway -->\n<dependency>\n   <groupId>org.springframework.cloud</groupId>\n   <artifactId>spring-cloud-starter-gateway</artifactId>\n   <version>4.1.2</version>\n</dependency>\n```\n\n그래서 최신 버전(내 경우 3.2.4v)의 스프링 부트 프로젝트를 만들고 위의 종속성을 추가합니다.\n\n- application.yml 파일에 라우팅 구성을 추가하십시오. `service-name` 프로토콜을 사용하여 API 게이트웨이에 서비스 검색을 지시합니다. 두 개의 payment-service 인스턴스가 있으므로 부하 처리가 필요하고 동적으로 처리해야 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n```yaml\nspring:\n  application:\n    name: gateway\n  cloud:\n    gateway:\n      routes:\n        - id: payment-service\n          uri: lb://payment-service\n          predicates:\n            - Path=/payment/**\n```\n\n- API Gateway는 마이크로서비스로도 불리우며 유레카 서버에 등록되어야 합니다. 메인 클래스에 `@EnableDiscoveryClient`를 추가하지 않는 것을 잊지 마세요.\n\n```java\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class ApiGtApplication {\n```\n\n```java\n public static void main(String[] args) {\n  SpringApplication.run(ApiGtApplication.class, args);\n }\n}\n```\n\n<div class=\"content-ad\"></div>\n\n# 지금 테스트해 보세요!\n\n진행 방법:\n\n- 유레카 서버 실행\n- 포트 9810 및 9811에서 결제 서비스 실행\n- API 게이트웨이 서비스 실행\n\n![이미지](/assets/img/2024-05-20-SpringCloudGatewaySpringBoot30Loadbalancing_2.png)\n\n<div class=\"content-ad\"></div>\n\n브라우저에서 게이트웨이 엔드포인트를 방문해서 확인해보세요 (http://localhost:8080/payment/say) \n\n/payment은 술어 패턴입니다 — 이 패턴이 발생하면 API 게이트웨이는 요청을 지불 서비스로 라우팅하고 지불 서비스에는 컨트롤러 /payment과 GET 메소드 /say가 있습니다. 이것이 실행되어 응답을 게이트웨이로 반환합니다. 유레카는 API 게이트웨이로 인스턴스 정보를 보내기 위한 것이며 내부 통신을 위한 것이 아닙니다.\n\n독자 여러분의 읽어주셔서 감사합니다. 도움이 되었다면 공유하고 좋아요를 눌러주세요.","ogImage":{"url":"/assets/img/2024-05-20-SpringCloudGatewaySpringBoot30Loadbalancing_0.png"},"coverImage":"/assets/img/2024-05-20-SpringCloudGatewaySpringBoot30Loadbalancing_0.png","tag":["Tech"],"readingTime":6},{"title":"도메인을 최우선으로 하는 생각을 Hexagonal Architecture를 활용한 모듈식 단일체에 도입하기","description":"","date":"2024-05-20 15:36","slug":"2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture","content":"\n\n\n![Image](/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_0.png)\n\nPart 1:\n\nPart 2:\n\nIn the first blog, we explored the concept of Modular Monolith, what is a module and how DDD strategy patterns can be used to create modular applications.\n\n\n<div class=\"content-ad\"></div>\n\n두 번째 블로그에서는 Spring Modulith가 모듈 모노리스 응용 프로그램에서 아키텍처 유효성 검사, 독립 모듈의 테스팅 및 이벤트 기반 통합을 어떻게 용이하게 하는지 살펴보았어요.\n\n이 블로그에서는 단일 모듈을 자세히 살펴보고 헥사고날 아키텍처가 핵심 비즈니스 로직의 유지보수성 및 테스트를 어떻게 향상시키는지 이해할 거에요. 또한 헥사고날 아키텍처가 DDD 및 모듈 모노리스 응용 프로그램의 맥락 속에 어떻게 맞는지도 이해할 거에요.\n\n헥사고날 아키텍처를 배우는 데 한 가지 어려움은 용어로 오인될 수 있다는 것이에요. 헥사곤 모양이나 숫자 여섯과는 아무런 상관이 없답니다. \"포트\"와 \"어댑터\"라는 용어가 과부하되어 있어서 여러분은 무엇을 의미하는지 배우기 전에 스스로의 해석을 잊어야 해요. 아키텍처의 층이 없다는 점(바깥쪽과 안쪽 이외에)과 의견이 분분하지 않은 아키텍처의 특성으로 인해 누구나 자신의 방식대로 구현할 수 있어요.\n\n나는 헥사고날 아키텍처를 이해하고 Java와 Spring Boot 응용 프로그램에서 적용하는 방법에 대해 시도했을 때, 헥사고날을 사용하지 않고 응용 프로그램을 작성한 후 문제를 겪고 하나씩 해결하여 어떤 것이 헥사고날인지 알아냈어요. 하지만 그것은 몇 년이 걸릴 수 있어요! 이 블로그(시리즈)에서는 이 과정을 더 짧은 시간 안에 기록하고 싶어요.\n\n<div class=\"content-ad\"></div>\n\n기업 문제를 해결하기 위한 애플리케이션을 개발하는 업무를 맡았습니다. 해당 애플리케이션은 웹을 통해 접속되며, 일부 비즈니스 로직을 구현하고 데이터를 데이터베이스에 저장할 것입니다 (당연히). 애플리케이션을 설계하는 과정에서 어떤 생각을 가지게 되시나요?\n\n- 웹 애플리케이션이어야 하므로, 최신하면서 반짝이는 자바스크립트 프레임워크(React?)를 사용하여 JSON을 제공하는 REST API 및 멋진 UI를 구축할 것입니다.\n- 비즈니스 요구 사항이 NoSQL DB를 필요로 하지 않는 한 관계형 데이터베이스를 사용할 것입니다.\n- 비즈니스 요구 사항을 바탕으로 여러 엔티티를 식별하고 ERD에 매핑할 것입니다.\n\n이런 내용이 익숙하시나요?\n\n3번 항목에 집중해 봅시다. ERD 여부와 상관없이, 데이터 모델을 도출하기 위해 여러 엔티티를 식별하고 그것을 시간이 흘러도 발전시킬 수 있는 것은 모든 웹 백엔드 엔지니어가 해야 하는 일이며 시작점이 될 수도 있습니다! 저는 이를 여러 차례 경험해 보았습니다.\n\n<div class=\"content-ad\"></div>\n\n이 방법은 (의식적인 아키텍처가 적용되지 않을 때 기본으로 사용되는) 계층화된/N계층 아키텍처로 구성된 애플리케이션을 만들게 됩니다. 데이터 모델은 JPA 엔티티로 구성되며, Repositories를 통해 영속화되고, 비즈니스 로직은 Services에 구현됩니다.\n\n이러한 구현의 핵심 특성 중 하나는 비즈니스 로직(Services)이 데이터 모델에서 작동하며 데이터베이스에 의존하는 것입니다. 특히 JPA/JDBC가 실제 데이터베이스 기술을 추상화하고 있기 때문에 바로 알아차리기 어렵습니다. 하지만 데이터베이스 구현(테이블 구조, 외래 키 관계, 인덱스 등)이 비즈니스 로직 구현을 주도하는 것이 아니라 그 반대로 작용하고 있습니다. 아래 그림에서 보듯이, 애플리케이션 서비스는 Spring과 JPA/Hibernate와 같은 기술과 강하게 결합됩니다.\n\n![이미지](/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_1.png)\n\n## 의존성 역전\n\n<div class=\"content-ad\"></div>\n\n자, 이제 우리는 다르게 생각하고 문제에 접근해 보겠습니다. 데이터 모델에 초점을 맞추는 대신, 비즈니스 로직과 제약 조건을 포착하는 여러 클래스를 만들어 보겠습니다. 이를 도메인 모델이라고 부르겠습니다.\n\n이 접근 방식에서는, 서비스가 스프링 데이터 리포지토리를 직접 사용하는 대신, 의존성을 반전시키기 위해 인터페이스를 도입하여 서비스가 필요로 하는 동작을 정의하고 이것이 어떻게 구현되었는지 신경쓰지 않습니다. 실은 스프링 데이터 리포지토리는 Java 인터페이스로 정의되어 있지만, 다이내믹 프록시를 통해 구현을 결합하는 인터페이스는 아닙니다.\n\n의존성을 반전시키기 위해 사용된 인터페이스를 포트(port)로, 인터페이스의 구현은 어댑터(adapter)로 부릅니다. 이들은 도메인 모델에 의해 주도(들어온)되므로 주도 또는 보조(secondary)로 알려져 있습니다. 이는 헥사고날 아키텍처에서 사용되는 용어입니다.\n\n<div class=\"content-ad\"></div>\n\n한편, 유스 케이스를 구현하는 데 사용되는 Application 서비스도 포트입니다. 차이점은 Java 인터페이스가 아니라 일반 클래스임을 의미합니다. 또한 해당 어댑터와 함께 사용됩니다. 이 어댑터는 기술을 접근하여 우리의 사용 사례에 연결합니다(REST API, GraphQL, 람다 등). 이러한 포트와 어댑터는 도메인 모델을 사용하므로 드라이버 또는 주요 요소로 알려져 있습니다. 이들은 우리 모델의 사용을 이끌어냅니다.\n\n![그림 안내](/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_3.png)\n\n기본 및 보조 포트와 어댑터를 결합하면 헥사고날(또는 포트 및 어댑터) 아키텍처로 이어집니다.\n\n새로운 접근 방법을 적용하는 데 도움이 되도록 몇 가지 제약 조건을 소개해 보겠습니다. 우리는 프레임워크/구현이 우리의 생각을 이끌도록 허용하지 않겠습니다. 이는 데이터베이스 라이브러리 (JPA/JDBC), 이벤트 프레임워크 (Apache Kafka) 및 애플리케이션 프레임워크 (Spring)를 배제한다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n남은 것은 순수한 자바 (또는 응용 프로그램을 개발하는 언어)와 해당 비즈니스 로직 구현의 단위 테스트입니다.\n\n잠시 후에 기존 응용 프로그램을 위의 과정을 따르도록 리팩터링할 것입니다. 하지만, 이것을 왜 신경 써야 할까요? 어떤 이점이 있을까요?\n\n## 핵심 비즈니스 로직은 구현 세부 정보에서 자유롭습니다\n\n비즈니스 로직을 반영하는 도메인 모델(도메인 또는 내부라고 부르겠습니다)은 구현 세부 정보(인프라스트럭처 또는 외부라고 부르겠습니다)에 영향을 받지 않습니다. 새로운 기능 요청을 구축해야 하나요? 도메인 모델에서 구현하고 단위 테스트로 유효성을 검사하세요. Spring Boot나 Hibernate를 업데이트해야 하나요? 걱정 마세요, 도메인 모델은 영향을 받지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n## 도메인 모델이 구현을 주도합니다\n\n도메인 모델은 아키텍처의 핵심에 있습니다. 다른 모든 것은 도메인 모델에서 출발합니다. 비즈니스 로직의 구현을 주도합니다. 이전에 구현을 주도했던 데이터 모델은 이제 영속성에만 관련된 사소한 세부사항입니다. REST API가 필요하다면, 스케줄러가 필요하다면 밤에 실행되는 확인 작업을 주도해야 한다면, AWS 람다를 트리거해야 한다면 — 이 모든 것은 도메인 모델과는 상관이 없습니다.\n\n## 지연된 결정\n\n어떤 사용 사례에 관계형 데이터베이스를 사용해야 할지 아직 확실하지 않나요? 아키텍트가 REST 대 GraphQL 사용에 대한 궁금증에 대답하지 않았나요? 그런 결정들이 비즈니스 로직의 구현을 막아선 안됩니다.\n\n<div class=\"content-ad\"></div>\n\n좋아요, 이제 새로운 사고 방식으로 비즈니스 로직을 작성하여 도서관 사용 사례를 구현하는 방법을 살펴볼까요?\n\n## 대여 Bounded Context (헥사고날 아키텍처로 구현)\n\n아래의 패키지 구조로 대여 모듈을 구성할 것입니다. 응용 프로그램 및 도메인 패키지는 헥사곤의 \"내부\"를 나타내고 인프라스트럭처 패키지는 \"외부\"를 나타냅니다.\n\n```js\nsrc/main/java\n└── example\n    ├── borrow\n    │   ├── application\n    │   │   └── ...\n    │   ├── domain\n    │   │   └── ...\n    │   └── infrastructure\n    │       └── ...\n    └── LibraryApplication\n```\n\n<div class=\"content-ad\"></div>\n\n아래 규칙을 패키지에 대해 따를 것입니다 (모두에게 해당되는 보편적인 규칙이 아니고, 우리가 직접 정한 것입니다). 이 구현의 복잡성과 아키텍처 원칙을 엄격하게 준수하는 현실적인 접근 사이의 적절한 균형을 유지하고 있다고 생각합니다.\n\n- 도메인 패키지에는 도메인 모델(Aggregates, Entities, Value Objects) 및 인프라스트럭처가 구현할 인터페이스를 제공하는 Secondary / Driven 포트가 포함됩니다. 우리는 표준 자바, Lombok(많은 코드를 줄이기 위한) 및 JMolecules(스테레오타입을 문서화하기 위해) 라이브러리만 사용할 것입니다.\n- 애플리케이션 패키지에는 애플리케이션의 유스케이스를 구현하는 Primary / Driver 포트가 포함됩니다. 우리는 Spring의 @Transactional 및 Modulith의 @ApplicationModuleListener 주석을 사용할 수 있습니다.\n- 인프라스트럭처 패키지에는 REST API 및 JPA 어댑터를 빌드하는 코드가 포함됩니다. 이것은 야생 서부입니다. 제한 사항이 없으며, 모든 라이브러리를 자유롭게 사용할 수 있습니다.\n\n책을 대출하는 사용 사례를 살펴보겠습니다. 사용자는 이용 가능한 책에 예약을 할 수 있습니다. 아래는 사용 사례를 구현하는 주요 포트의 Java 예시입니다. @PrimaryPort 주석이 어디서 온 것인지 궁금하다면 계속 읽어보세요!\n\n```java\n@PrimaryPort\npublic class CirculationDesk {\n  \n    private final BookRepository books;\n    private final HoldRepository holds;\n    private final HoldEventPublisher eventPublisher;\n\n    @Transactional\n    public HoldDto placeHold(PlaceHold command) {\n        books.findAvailableBook(command.inventoryNumber())\n                .orElseThrow(() -> new IllegalArgumentException(\"Book not found\"));\n\n        return Hold.placeHold(command)\n                .then(holds::save)\n                .then(eventPublisher::holdPlaced)\n                .to(HoldDto::from);\n    }\n  \n    record PlaceHold(Barcode inventoryNumber, \n                     String patronId, \n                     LocalDate dateOfHold) {\n    }\n} \n```\n\n<div class=\"content-ad\"></div>\n\n의존성을 역전시키기 위해, 모든 집합체를 위한 Repository 인터페이스를 도입했습니다 (Spring Data와는 혼동하지 마세요).\n\n```js\n@SecondaryPort\n인터페이스 BookRepository {\n\n    Optional<Book> findAvailableBook(Book.Barcode inventoryNumber);\n\n    Optional<Book> findOnHoldBook(Book.Barcode inventoryNumber);\n\n    Book save(Book book);\n\n    Optional<Book> findByBarcode(String barcode);\n}\n\n@SecondaryPort\n인터페이스 HoldRepository {\n\n    Hold save(Hold hold);\n\n    Optional<Hold> findById(Hold.HoldId id);\n\n    List<Hold> activeHolds();\n}\n```\n\nCirculationDesk 응용 프로그램 서비스는 위의 리포지터리 인터페이스를 사용합니다. 이를 통해 구현 기술(Spring Data, JPA, JDBC 등)과 독립시켰습니다.\n\n## 집합체로부터 도메인 이벤트 트리거하기\n\n<div class=\"content-ad\"></div>\n\n\n하나의 사용 사례 결과는 Aggregate의 업데이트된 상태를 지속시키는 것입니다. 또 다른 결과는 도메인 이벤트를 발생시켜 모노리스의 다른 모듈 (또는 동일 모듈 내의 다른 Aggregate)에서 대응할 수 있도록 하는 것일 수 있습니다.\n\nAggregate를 지속시키는 것 (Repository 사용)과 이벤트를 발생시키는 것 (Event Publisher 사용)은 헥사고날 아키텍처에서 이차/드라이븐 포트입니다. 구현은 비즈니스 로직과 관련이 없습니다.\n\n사용 사례 실행 후에는 BookPlacedOnHold 이벤트가 발생합니다. 이 이벤트는 Book Aggregate에서 도서 상태를 데이터베이스에서 업데이트하기 위해 사용됩니다. 이벤트 처리는 비즈니스 로직의 중요한 부분이므로 주/드라이버 포트의 책임입니다.\n\n```java\n// 실제로 이벤트가 발행되는 방법에 대한 구현에 대한 이해가 없는\n// 이벤트 퍼블리셔 인터페이스\n@SecondaryPort\npublic interface HoldEventPublisher {\n\n    void holdPlaced(BookPlacedOnHold event);\n\n    default Hold holdPlaced(Hold hold) {\n        BookPlacedOnHold event = new BookPlacedOnHold(hold.getId().id(), hold.getOnBook().barcode(), hold.getDateOfHold());\n        this.holdPlaced(event);\n        return hold;\n    }\n}\n\n// 이벤트 핸들러는 Spring Modulith 리스너를 사용하여\n// 이벤트에 신뢰성있게 대응합니다.\n@PrimaryPort\nclass CirculationDeskEventHandler {\n\n    private final BookRepository books;\n    private final HoldEventPublisher eventPublisher;\n\n    @ApplicationModuleListener\n    public void handle(BookPlacedOnHold event) {\n        books.findAvailableBook(new Book.Barcode(event.inventoryNumber()))\n                .map(Book::markOnHold)\n                .map(books::save)\n                .orElseThrow(() -> new IllegalArgumentException(\"중복 예약?\"));\n    }\n}\n```\n\n<div class=\"content-ad\"></div>\n\n## 육각형 구조의 \"내부\" (비즈니스 로직) 단위 테스트\n\n![이미지](/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_4.png)\n\n프레임워크 종속성 없이 비즈니스 로직을 구현하는 큰 장점은 통합 테스트가 필요하지 않고 코드를 단위 테스트할 수 있다는 것입니다. Repository 포트의 인메모리 구현을 사용하여 placeHold 유스케이스를 유효성 검사하는 단위 테스트의 예시가 여기 있습니다(CirculationDeskTest.java 참조).\n\n```java\nclass CirculationDeskTest {\n\n    CirculationDesk circulationDesk; // 애플리케이션 서비스\n\n    BookRepository bookRepository; // 보조 포트\n\n    HoldRepository holdRepository; // 보조 포트\n\n    @BeforeEach\n    void setUp() {\n        bookRepository = new InMemoryBooks();\n        holdRepository = new InMemoryHolds();\n        circulationDesk = new CirculationDesk(bookRepository, holdRepository, new InMemoryHoldsEventPublisher());\n    }\n\n    @Test\n    void patronCanPlaceHold() {\n        var command = new Hold.PlaceHold(new Book.Barcode(\"12345\"), LocalDate.now());\n        var holdDto = circulationDesk.placeHold(command);\n        assertThat(holdDto.getBookBarcode()).isEqualTo(\"12345\");\n        assertThat(holdDto.getDateOfHold()).isNotNull();\n    }\n\n    @Test\n    void bookStatusUpdatedWhenPlacedOnHold() {\n        var command = new Hold.PlaceHold(new Book.Barcode(\"12345\"), LocalDate.now());\n        var hold = Hold.placeHold(command);\n        circulationDesk.handle(new BookPlacedOnHold(hold.getId().id(), hold.getOnBook().barcode(), hold.getDateOfHold()));\n        //noinspection OptionalGetWithoutIsPresent\n        var book = bookRepository.findByBarcode(\"12345\").get();\n        assertThat(book.getStatus()).isEqualTo(ON_HOLD);\n    }\n}\n\n// 테스트 픽처 (보조 포트를 구현하는 보조 어댑터 역할)\nclass InMemoryBooks implements BookRepository {\n\n    private final Map<String, Book> books = new HashMap<>();\n\n    public InMemoryBooks() {\n        var booksToAdd = List.of(\n                Book.addBook(new Book.AddBook(new Book.Barcode(\"12345\"), \"A famous book\", \"92972947199\")),\n                Book.addBook(new Book.AddBook(new Book.Barcode(\"98765\"), \"Another famous book\", \"98137674132\"))\n        );\n        booksToAdd.forEach(book -> books.put(book.getInventoryNumber().barcode(), book));\n    }\n\n    ...\n}\n\n// 테스트 픽처 (보조 포트를 구현하는 보조 어댑터 역할)\nclass InMemoryHolds implements HoldRepository {\n  // InMemoryBooks와 유사한 구현\n  ...\n}\n```\n\n<div class=\"content-ad\"></div>\n\n도메인 모델이 집계를 지속하거나 이벤트를 트리거하기 위해 인터페이스에 의존하고 있는 점을 감안할 때, 유닛 테스트는 이 인터페이스 메서드를 실행할 수 있도록 테스트 픽스처가 필요합니다. 유당 테스트가 작동하려면 모든 보조/드라이븐 포트에 대한 구현이 필요합니다.\n\n전체 코드는 아래 GitHub 저장소에서 확인할 수 있습니다.\n\n## 아키텍처 규칙 테스트하기\n\nSpring Modulith 및 jMolecules 라이브러리는 ArchUnit과 함께 사용되어 우리의 솔루션에서 적용된 아키텍처와 Modular Monolith 애플리케이션의 모듈 간 상호 작용을 테스트할 수 있게 합니다. 이는 새로운 개발자들이 작업을 시작함에 따라 우리의 애플리케이션이 선택한 아키텍처에서 벗어나지 않도록 하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n저희 모듈형 모놀리스 응용 프로그램에는 이제 Catalog 및 Borrow 두 개의 모듈이 있습니다. Catalog 모듈은 Layered/N-tier 아키텍처를 사용하고 Borrow 모듈은 Hexagonal 아키텍처를 사용하고 있어요. 다음과 같이 테스트할 수 있어요 (BorrowJMoleculesTests 및 CatalogJMoleculesTests 참고).\n\n```js\n@AnalyzeClasses(packages = \"example.catalog\")\npublic class CatalogJMoleculesTests {\n\n    @ArchTest\n    ArchRule dddRules = JMoleculesDddRules.all();\n\n    @ArchTest\n    ArchRule layering = JMoleculesArchitectureRules.ensureLayering();\n}\n\n@AnalyzeClasses(packages = \"example.borrow\")\npublic class BorrowJMoleculesTests {\n\n    @ArchTest\n    ArchRule dddRules = JMoleculesDddRules.all();\n\n    @ArchTest\n    ArchRule hexagonal = JMoleculesArchitectureRules.ensureHexagonal();\n}\n```\n\n@AnalyzeClasses 주석을 사용하여 테스트할 모듈을 선택할 수 있어요.\n\n## Hexagonal 아키텍처의 \"외부\"를 통합 테스트하기\n\n<div class=\"content-ad\"></div>\n\n기술이 도메인 모델과 상호 작용하는 외부 부분은 동작을 확인하기 위해 통합 테스트로 테스트해야 합니다. 여기에 예제가 있습니다 (영속성인 CirculationDeskIT.java 및 REST인 CirculationDeskControllerIT.java):\n\n```js\n// from CirculationDeskIT\n@Test\nvoid patronCanPlaceHold(Scenario scenario) {\n    var command = new Hold.PlaceHold(new Book.Barcode(\"13268510\"), LocalDate.now());\n    scenario.stimulate(() -> circulationDesk.placeHold(command))\n            .andWaitForEventOfType(BookPlacedOnHold.class)\n            .toArriveAndVerify((event, dto) -> {\n                assertThat(event.inventoryNumber()).isEqualTo(\"13268510\");\n            });\n}\n\n// from CirculationDeskControllerIT\n@Test\nvoid placeHoldRestCall() throws Exception {\n    mockMvc.perform(post(\"/borrow/holds\")\n                    .contentType(MediaType.APPLICATION_JSON)\n                    .content(\"\"\"\n                            {\n                              \"barcode\": \"64321704\",\n                              \"patronId\": 5\n                            }\n                            \"\"\"))\n            .andExpect(status().isOk());\n}\n```\n\n## jMolecules로 클래스 스테레오타입 시각화\n\n이전 블로그 시리즈에서 언급했듯이 jMolecules Intellij 플러그인을 사용하여 다양한 스테레오타입을 시각화할 수 있습니다. 그러나 jMolecules 라이브러리가 헥사고날 아키텍처에 대한 주석을 지원하더라도 플러그인은 해당 주석을 지원하지 않습니다. 저는 관련 주석을 추가하여 플러그인을 수정했습니다. 플러그인을 활성화하고 관련 주석을 추가한 후 대여 모듈 내부 클래스가 어떻게 보이는지 확인하세요.```\n\n<div class=\"content-ad\"></div>\n\n\n![](/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_5.png)\n\n주요 포트 및 어댑터 클래스를 쉽게 식별할 수 있습니다.\n\n## 헥사고날 아키텍처에 헌신해야 할까요?\n\n모듈형 단일체 애플리케이션을 구축할 때, 모든 모듈에 대해 헥사고날 아키텍처를 사용해야 할지에 대해 고려해야 합니다. 만약 모듈이 자체 비즈니스 규칙을 갖고 있거나 (또는 나중에 갖게 될 것으로 예상된다면), 헥사고날 아키텍처가 좋은 아이디어일 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n하지만 해결책에 더 많은 클래스와 장황성을 추가합니다. Layered 아키텍처에는 보조 포트와 어댑터가 없습니다. Spring Data 리포지토리 인터페이스는 Service 클래스에 직접 의존성을 추가하는 대가로 보조 포트와 어댑터의 역할을 취합니다.\n\n결국, 헥사고날 아키텍처를 채택하는 이점이 비용을 크게 상회합니다. 최소한 코드베이스에서 불필요한 결합을 줄이기 위해 의존성 역전의 원칙을 적용할 수 있는지 항상 확인해야 합니다.\n\n## 헥사고날 아키텍처를 구현하는 데 DDD가 필요한가요?\n\n아니요. 도메인 주도 설계(DDD)는 도메인이 모델링되는 방식에만 관련이 있습니다. 아키텍처와는 관련이 없습니다(헥사고날이든 그 외). DDD를 사용해야 하는가요? 클래스와 필드를 명명할 때 항상 모든 지식의 언어를 사용하는 것을 추천합니다. 도메인 전문가가 사용하는 용어와 동일한 용어를 사용하는 코드는 항상 이해하기 쉽습니다.\n\n<div class=\"content-ad\"></div>\n\nDDD의 Aggregate 및 기타 전략적 패턴을 사용하는 것은 애플리케이션의 필요성과 복잡성에 따라 다릅니다. 정당화되지 않은 경우에는 트랜잭션 스크립트나 액티브 레코드를 대신 사용할 수 있습니다.\n\n## 헥사고날 아키텍처의 순수 구현 회피\n\n헥사곤의 내부는 핵심 비즈니스 로직의 구현이며, 헥사곤 외부에서 사용된 프레임워크와 기술로부터 자유롭게 유지해야 합니다. 몇몇 구현이 이 규칙을 신앙적으로 따라 코드를 복잡하게 만드는 것을 본 적이 있습니다.\n\n예를 들어, 애플리케이션 서비스 코드는 트랜잭션 내에서 실행되어야 합니다. Spring의 @Transactional을 사용하면 쉽게 달성할 수 있습니다. 그러나 이를 피하려면 애플리케이션 서비스를 Spring으로부터 자유롭게 유지하기 위해 사용자 정의 어노테이션과 AOP를 도입해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n헥사곤 안의 내부는 Application과 Domain으로 구성되어 있습니다. 도메인은 이상적으로는 프레임워크 없이 유지되어야 하지만, Application은 Spring을 사용할 수 있습니다. 이 질문을 Vaughn Vernon에게 던졌고, 그는 좋은 설명을 제공했습니다.\n\n![이미지](/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_6.png)\n\n## 도메인 모델로부터 만들어진 분리된 JPA 엔티티를 병합하기\n\n아래 코드 스니펫을 확인하여 Hold를 배치하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\npublic HoldDto placeHold(Hold.PlaceHold command) {\n    ...\n\n    Hold.placeHold(command) // Hold 도메인 모델 객체 생성\n            .then(holds::save) // JPA 엔티티로 변환하여 데이터베이스에 저장\n            .then(eventPublisher::holdPlaced) // HoldPlaced 이벤트를 트리거\n\n    ...\n  );\n}\n```\n\nholds::save는 Secondary 포트 인터페이스에서의 호출로, Secondary 어댑터 구현을 트리거합니다:\n\n```js\npublic Hold save(Hold hold) {\n    holds.save(HoldEntity.fromDomain(hold));\n    return hold;\n}\n\n// HoldEntity 클래스에서\npublic static HoldEntity fromDomain(Hold hold) {\n    var entity = new HoldEntity();\n    entity.id = hold.getId().id(); // 도메인 모델 식별자와 JPA 엔티티 PK가 동일합니다!\n    entity.book = hold.getOnBook();\n    entity.dateOfHold = hold.getDateOfHold();\n    entity.status = HoldStatus.HOLDING;\n    entity.version = 0L;\n    return entity;\n}\n```\n\nHold 도메인 모델은 영속화되기 전에 분리된 HoldEntity JPA 엔티티로 변환되어야 합니다. 엔티티 생성에는 기본 키 ID를 알아야 합니다. 도메인 모델이 PK를 따로 저장하거나 모델 식별자로 사용해야 합니다. 저의 예시에서는 후자를 선택했지만, 이 문제를 어떻게 해결할지 궁금합니다.```\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\n이 블로그에서는 계층 구조 대신 헥사고날 아키텍처를 사용하여 대출 모듈을 다시 구현했습니다. 이 과정에서 동일한 단일 코드 베이스의 일부임에도 불구하고 카탈로그 모듈에는 절대적으로 변경이 필요하지 않았음을 보여주었습니다. 이는 이벤트 기반 통합 덕분에 느슨한 결합이 활성화되었음을 확인했습니다.\n\n헥사고날에 대한 여러분의 경험은 어떠한가요? 댓글에서 공유해 주시면 감사하겠습니다.\n\n아래에서 업데이트된 코드를 확인할 수 있습니다.","ogImage":{"url":"/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_0.png"},"coverImage":"/assets/img/2024-05-20-AdoptingDomain-FirstThinkinginModularMonolithwithHexagonalArchitecture_0.png","tag":["Tech"],"readingTime":16},{"title":"LeetCode 3068번 문제 해결하기 노드 값의 최대 합 찾기","description":"","date":"2024-05-20 15:34","slug":"2024-05-20-SolvingLeetCode3068FindtheMaximumSumofNodeValues","content":"\n\n# 문제 설명:\n\n0부터 n - 1까지 번호가 매겨진 노드로 구성된 무방향 트리가 있습니다. 길이가 n - 1인 0을 기준으로 한 2차원 정수 배열 edges가 주어지는데, edges[i] = [ui, vi]는 트리의 노드 ui와 vi 사이에 간선이 있음을 나타냅니다. 또한 양의 정수 k와 길이가 n인 0을 기준으로 한 음이 아닌 정수 배열 nums가 주어집니다. 여기서 nums[i]는 i번째로 번호가 매겨진 노드의 값을 나타냅니다.\n\n앨리스는 트리 노드의 값의 합을 최대로 하려고 합니다. 이를 위해 앨리스는 다음 작업을 트리에서 원하는 횟수만큼 (포함하여) 수행할 수 있습니다:\n\n- 노드 u와 v를 연결하는 [u, v] 간선을 선택하고, 그들의 값들을 다음과 같이 업데이트합니다:\n- nums[u] = nums[u] XOR k\n- nums[v] = nums[v] XOR k\n\n<div class=\"content-ad\"></div>\n\nAlice가 작업을 여러 번 수행하여 얻을 수 있는 값의 최대 합을 반환합니다.\n\n예시 1:\n\n![image](/assets/img/2024-05-20-SolvingLeetCode3068FindtheMaximumSumofNodeValues_0.png)\n\n```js\nInput: nums = [1,2,1], k = 3, edges = [[0,1],[0,2]]\nOutput: 6\nExplanation: Alice가 다음을 수행하여 최대 합 6을 얻을 수 있습니다:\n- [0,2] 엣지를 선택합니다. nums[0] 및 nums[2]는 다음과 같이 변경됩니다: 1 XOR 3 = 2이고, 배열 nums는 [1,2,1] -> [2,2,2] 로 변합니다.\n값의 총 합은 2 + 2 + 2 = 6 입니다.\n가능한 최대 값 합은 6임을 보여줄 수 있습니다.\n```\n\n<div class=\"content-ad\"></div>\n\n예제 2:\n\n![그림](/assets/img/2024-05-20-SolvingLeetCode3068FindtheMaximumSumofNodeValues_1.png)\n\n```js\nInput: nums = [2,3], k = 7, edges = [[0,1]]\nOutput: 9\nExplanation: Alice can achieve the maximum sum of 9 using a single operation:\n- Choose the edge [0,1]. nums[0] becomes: 2 XOR 7 = 5 and nums[1] become: 3 XOR 7 = 4, and the array nums becomes: [2,3] -> [5,4].\nThe total sum of values is 5 + 4 = 9.\nIt can be shown that 9 is the maximum achievable sum of values.\n```\n\n예제 3:\n\n<div class=\"content-ad\"></div>\n\n```js\nInput: nums = [7,7,7,7,7,7], k = 3, edges = [[0,1],[0,2],[0,3],[0,4],[0,5]]\nOutput: 42\nExplanation: 최대 가능한 합은 42로, Alice가 작업을 수행하지 않고 이를 달성할 수 있습니다.\n```\n\n제한사항:\n\n- 2 `= n == nums.length `= 2 * 104\n- 1 `= k `= 109\n- 0 `= nums[i] `= 109\n- edges.length == n - 1\n- edges[i].length == 2\n- 0 `= edges[i][0], edges[i][1] `= n - 1\n- 입력은 edges가 유효한 트리를 나타내도록 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 방법: 탐욕법 (정렬 기반 접근)\n\n## 직관\n\n노드 U에 대해 실행된 작업이 있다면, 노드의 새로운 값은 nums[U] XOR k가 됩니다. 각 노드에 대해, 작업을 수행한 후 값의 순 변화는 netChange[U] = nums[U] XOR k - nums[U]로 주어집니다.\n\n만약 이 순 변화가 양수라면, 모든 노드 값의 총합은 증가합니다. 그렇지 않으면 감소합니다.\n\n<div class=\"content-ad\"></div>\n\n\"가정해보겠습니다. 노드 쌍에서 노드 합에 가장 큰 증가를 제공하는\" 효율적인 작업 \"을 수행하려고 한다고 가정해봅시다. 가장 큰 양수 netChange 값을 가진 노드를 선택하면 노드 합에 가장 큰 증가를 제공할 것입니다.\n\n모든 노드에 대해 위에서 논의한 공식을 사용하여 net change 값을 계산할 수 있습니다. 이 값들을 내림차순으로 정렬한 후에는 positive sum을 가진 pair를 정렬 된 netChange 배열의 시작부터 선택할 수 있습니다.\n\n쌍의 합이 양수이면이 쌍에 대한 작업을 수행할 때 총 노드 합의 값을 증가시킵니다.\n\n## 알고리즘\"\n\n<div class=\"content-ad\"></div>\n\n1. nums의 크기인 n의 netChange 배열과 현재 nums의 합을 저장하는 정수 nodeSum을 초기화합니다.\n\n2. nums 배열을 반복합니다 (0부터 n-1까지):\n\n- 각 인덱스마다 intuition에서 논의한 아이디어를 사용하여 netChange의 값을 저장합니다.\n\n3. netChange 배열을 내림차순으로 정렬합니다.\n\n<div class=\"content-ad\"></div>\n\n4. netChange 배열을 반복합니다 (0부터 n-1까지, 단계 크기 = 2):\n\n- 인접한 요소의 쌍을 만들 수 없는 경우, 반복을 중단합니다.\n- 인접한 요소의 쌍의 합이 양수인 경우 이 합을 nodeSum에 추가합니다.\n\n5. 모든 netChange 요소를 반복한 후, 수행한 작업 이후 노드의 최대 가능한 합인 nodeSum을 반환합니다.\n\n## 구현\n\n<div class=\"content-ad\"></div>\n\n```java\nclass Solution {\n    public long maximumValueSum(int[] nums, int k, int[][] edges) {\n        int n = nums.length;\n        int[] netChange = new int[n];\n        long nodeSum = 0;\n\n        for (int i = 0; i < n; i++) {\n            netChange[i] = (nums[i] ^ k) - nums[i];\n            nodeSum += nums[i];\n        }\n\n        Arrays.sort(netChange);\n\n        for (int i = n-1; i >= 1; i -= 2) {\n            // If netChange contains odd number of elements break the loop\n            if (i - 1 == -1) {\n                break;\n            }\n            long pairSum = netChange[i] + netChange[i - 1];\n            // Include in nodeSum if pairSum is positive\n            if (pairSum > 0) {\n                nodeSum += pairSum;\n            } else {\n                return nodeSum;\n            }\n        }\n        return nodeSum;\n    }\n}\n```\n\n## 복잡도 분석\n\n노드 값 목록에 포함된 요소 수를 n이라고 합시다.\n\n- 시간 복잡도: O(n⋅logn)\n  - 정렬을 제외하고 목록에 대해 단순 선형 작업을 수행하기 때문에 런타임은 정렬의 O(n⋅logn) 복잡성에 지배됩니다.\n- 공간 복잡도: O(n)\n  - 새로운 크기 n의 netChange 배열을 만들고 정렬하기 때문에 추가 공간은 netChange 배열에 대한 O(n)이고 정렬에 대해 O(logn) 또는 O(n)이므로 순 공간 복잡도는 O(n)입니다.```","ogImage":{"url":"/assets/img/2024-05-20-SolvingLeetCode3068FindtheMaximumSumofNodeValues_0.png"},"coverImage":"/assets/img/2024-05-20-SolvingLeetCode3068FindtheMaximumSumofNodeValues_0.png","tag":["Tech"],"readingTime":4},{"title":"의료 기록의 개인 정보는 신뢰에 달려 있습니다","description":"","date":"2024-05-18 21:12","slug":"2024-05-18-ThePrivacyoftheMedicalRecordReliesonTrust","content":"\n\n## 프린세스 케이트의 의료 기록 뉴스\n\n![의료 기록의 개인정보 보호는 신뢰에 달려 있습니다](/assets/img/2024-05-18-ThePrivacyoftheMedicalRecordReliesonTrust_0.png)\n\n저는 저희 병원 부서의 입원 결정 지점이었습니다. 이 말은 즉, 환자가 우리 부서로 추천된 경우 입원 간호사가 해당 환자의 적절성에 대해 중요한 의문을 제기했을 때, 저는 해당 환자와 의무기록을 검토하고 최종 결정을 내렸다는 것을 의미했습니다.\n\n다양한 의료 서비스 유형의 면허에 대해 많은 규정과 규정이 존재합니다. 부서장으로서, 저는 해당 규정을 알아야만 했습니다.\n\n<div class=\"content-ad\"></div>\n\n전자 의료 기록이 신뢰에 의존한다는 사실에 놀랐어요. 환자에 할당된 의료진만이 컴퓨터를 보고 그 환자의 기록을 볼 수 있는 권한이 있었죠. 병원 관련 의사 결정을 위해 차트를 검토하고 다른 제한적이고 합법적인 이유로 의료 기록을 열어 본 사람들도 모두 필요에 따라 열어본 거예요.\n\n## 의료 기록에 무단 접근 금지\n\n의료 기관들은 명확하게 밝혔어요. 환자 의료 차트에 무단 접근하면 즉시 해고 또는 다른 조치의 대상이 된다는 거였죠. 예를 들어, 어른 부모라도 어린 아동의 차트를 엿보지 못했어요. 배우자의 차트에 접근할 수도 없었고요. 특히 궁금한 사람의 차트에 접근하는 건 금지돼 있었어요 - 이웃, 친구 또는 연예인 같은 사람의 차트를 확인하는 건 특히 안 돼요.\n\n뉴스 매체에서는 무단으로 연예인 의료 기록을 엿본 직원들에 대한 이야기를 다루었어요. 프린세스 케이트와 알려지지 않은 수술로 입원한 일이 지금 의료 기록 유출 사례로 뉴스에 소개되고 있어요.\n\n<div class=\"content-ad\"></div>\n\n대부분의 의료 센터들은 자신들의 단점을 공개하고 싶어하지 않을 것이기 때문에, 이 뉴스 이야기가 언론에서 판매되거나 보도될 수 있는 것을 막기 위한 예방적인 움직임이라고 생각해요. 런던 클리닉은 이 이야기가 공개될 수 있는 다른 이유가 있다면 적극적으로 대응할 지도 모를 거예요. 또한 다른 직원들에게 의료 기록에 접근하지 말라는 경고로 작용합니다.\n\n환자는 주기적으로 자신의 차트에 접근한 사람을 검토할 권리가 있어요. 대부분의 사람들은 이 옵션을 알지 못하거나 사용하지 않아요. 제가 일했던 병원은 목적지 병원이었기 때문에 유명인들도 왔어요. 그들의 차트는 무단 접근을 더 자주 확인했으므로, 호기심이 많은 사람들은 빠르게 발각되곤 했어요. 개인 정보 보호를 위한 이 규정은 엄격하게 집행되었어요.\n\n근무 중인 간호사나 차트에 접근해야 하는 전문의와 같이 담당 환자를 할당받은 어떤 간호사도 그 차트에 접근할 수 있는 능력은, 모든 의료 직원에게 차트가 사용 가능해야 한다는 것을 의미해요. 이것은 대규모 병원의 의료 기록의 난제입니다.\n\n저희 병원에서 행동 건강 기록을 검토해야 할 경우에는, 그 기록을 검토할 필요가 있는 의료 직원이 그것에 접근할 수 있도록 하기 위해, 컴퓨터 화면에 메디컬 필요성에 관한 추가적인 경고문을 건너야 했어요. 판단력에 영향을 미치는 상태로 치료받으면서 시스템에서 활동 중인 의료 시스템 직원에 대해 제가 부사장인 상급 간부에게 상담을 요청했어요.\n\n<div class=\"content-ad\"></div>\n\n의료기록의 개인정보 보호 목적이 절대적이라는 것을 그녀가 말했습니다. 그 정보를 전달할 수 없었어요. 의사와 환자 사이의 것이기 때문이에요. 유일한 결정권은 입원 여부 뿐이었습니다.\n\n의료 이사가 알려져 있는 의학적 상태를 가진 정신 보건 사회복지사의 의료 기록을 검토할 때 비슷한 딜레마를 알게 되었어요. 의료 이사는 의료 기록을 검토할 때 관련된 치료 이유를 가져야 했습니다. 일자리에 어떤 영향을 미쳤는지 궁금해하는 게 아니었어요. 결과를 몰랐지만 의료 이사가 남았다는 사실을 알고 있어요. 아마도 그녀의 인사 기록에 메모가 되었거나 만약 불이행이었다면 경고가 있었을지도 몰라요.\n\n병원에서 일하기 전에 의료 기록의 개인정보 보호가 보호되고 있다고 생각했었습니다. 그렇죠, 하지만 단순한 기계적이거나 기술적 장벽이 아니라 꼭 알아야 할 열람 권한의 신뢰에 의해 보호되고 있어요. 대형 병원은 일년 동안 수천 명의 환자를 만날 수 있으므로 발생하는 무시무시한 일은 결코 발견되지 않을 수도 있어요. 그 차트에 접근한 사람과 그들의 역할에 대한 목록을 찾아볼 수 있지만, 그건 적극적인 단계, 기술적 요청이어야 합니다.\n\n한 명의 의사가 쓴 한 문장으로부터 우리를 판단하던 주요 수사관과 오해가 있었어요. 수백 개의 전자 페이지로 이루어진 차트에 숨겨진 그 문장 때문에 책임을 져야 했어요. 그 지시사항에 대해 우리가 책임을 져야 했었어요. 의사는 자신의 메모에 관심을 끌지 못해 매우 미안해했고, 컴퓨터 다른 곳에 입력했어야 했어요.\n\n<div class=\"content-ad\"></div>\n\n국가 조사관은 전체 차트를 읽는 데 약 여덟 시간이 필요하다는 것을 이해하지 못했어요. 전체 차트를 읽는 간호사는 교대를 시작하지 못할 거예요. 그래서 그녀는 최신 업데이트를 읽었어요. 의료 상태에 변화가 있는지 여부로만 차트를 확인했어요. 국가 조사관은 환자와 작업하는 의료진에게 \"전체 차트를 읽을 것을 예상했다\"고 언급했어요.\n\n## 의료 기록은 매우 복잡한 전자 기록입니다.\n\n각 우려 사항에는 별도의 탭, 페이지 및 검토가 있어요. 약국, 물리 치료, 행동 건강, 간호, 진단 목록... 페이지가 계속 이어져요. 복잡성 자체가 위험이 될 수 있어요. 역사가 더 복잡하고 길수록 간단한 체크 상자를 놓치기 쉬워지거든요.\n\n우리 시스템을 업데이트했을 때, 각 부서에서 필요한 여러 작은 수정이 있었어요. 우리에게 그 업데이트의 예가 눈에 보이지 않던 기록에 날린 의사 선언이었답니다.\n\n<div class=\"content-ad\"></div>\n\n언제나 선제적인 환자가 되는 것을 믿어왔어요. 이제 내 의사를 방문할 때, 그녀는 노트북에 타이핑을 하는 기술을 마스터했어요. 컴퓨터를 보면서 저를 살펴보고 눈을 맞추면서 말이에요. 예전의 행정 데이터 포인트가 여전히 유효하다면, 그녀는 표준 스무 분 진료 방문 당 환자 한 명을 보고 있을 거예요.\n\n건강 제공자들에게 질문하세요. 그들의 삶은 당신이 알고 있는 것보다 더 복잡할 수 있어요. 중요한 의료 상태를 제공자에게 상기시켜 주거나 건강에 작은 변화가 있었는지 알려주세요. 상태 치료에 중요한 정보는 내게는 사소한 것처럼 보일 수 있어요.\n\n당신의 건강 데이터를 올바르게 관리하는 데 가장 큰 투자를 한 사람은 당신이에요.","ogImage":{"url":"/assets/img/2024-05-18-ThePrivacyoftheMedicalRecordReliesonTrust_0.png"},"coverImage":"/assets/img/2024-05-18-ThePrivacyoftheMedicalRecordReliesonTrust_0.png","tag":["Tech"],"readingTime":4},{"title":"당신만의 개인 AI 코드 어시스턴트 로컬 머신에서 오프라인 LLM 사용 초보자 가이드","description":"","date":"2024-05-18 21:11","slug":"2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine","content":"\n\n# 당신의 개인 AI 코딩 어시스턴트 — 인터넷 연결이 필요 없음\n\n![이미지](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_0.png)\n\n로컬 머신에 개인 생성 AI 모델을 설정하고 사용하는 방법을 배우세요 — 완전 무료입니다. 기본적인 \"Mac을 어떻게 사용하나요\" 지식은 포함되어 있습니다 — 어쨌든 우리가 설정하는 것은 코드 어시스턴트니까요.\n\n이 비교적 빠른 가이드는 클라우드 서비스로 데이터가 업로드되지 않는 개인용 로컬 오프라인 대규모 언어 모델 (LLM)을 시작하는 방법을 설명합니다.\n\n<div class=\"content-ad\"></div>\n\n- 이 프로세스가 작동하는 방식부터 시작해서 모델을 선택하는 데 도움을 드립니다.\n- 노트북에 구성하는 방법을 설명해 드립니다.\n\n## 지구를 위해 비용을 지불하지 않는 로컬 오프라인 GenAI 어시스턴트가 필요하세요?\n\n지금 제가 여러분이 \"생성적 AI\"를 손끝에서 경험하고 싶어하실 것이라고 확신합니다. 하지만 아마 염려하시는 것은... 여러분이 실행 중인 메가 비밀 코드가 StackOverflow에는 없을 것이고, 여러분이 처음부터 복사한 곳에 데이터가 훈련되고 있다는 점이 아닌가요. 맞죠? 그렇죠, 맞죠?\n그러나 여러분은 여전히 특별하고 안전한 느낌을 가지길 원하시며, '집에서 일하기' #밴라이프 여행 중에 숲, 들판, 해변 또는 고원에서 전화를 기다리는 동안 항상 누군가와 대화할 사람 — 맞죠, 코드와 함께 대화할 사람이 필요하실 것입니다.\n\n하지만, 여기서 얼마 후에 우리는 공감을 얻을 수 있을 거라고 확신합니다. Google Astra 또는 GPT-4의 실제 리테일 사용이 가능해질 때에 대해 별도로 다룰 것입니다.\n\n<div class=\"content-ad\"></div>\n\n요즘 우리는 개인용 개인 AI 어시스턴트를 찾고 있죠. 개인 정보를 존중하며 인터넷 연결이 없어도 마법을 부리는 일이 가능한 어시스턴트 말이에요. 이제 과학 소설이 아니에요. 노트북에서 강력한 AI 모델을 직접 실행하는 것은 가능할 뿐만 아니라 놀랍게도 쉽답니다.\n\n이번 달에는 Gemma, Llama3, Phi3와 Mixtral의 변형체(예: Dolphin) 등 새로운, 놀라운 능력을 가진 모델이 나왔어요. 이들은 Apple의 M-시리즈 칩으로 작동하는 일상적인 개발용 머신에서 원활하게 작동할 만큼 충분히 작아졌어요.\n\n자, 이제 직접 개인용 개인 AI 어시스턴트를 만드는 방법을 알아볼까요?\n\n# 여기서 시작해보세요 — 기본 사항을 먼저 알아봐요\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_1.png)\n\n## \"최고의 모델\"이란 것은 없어요 — 항상 절충이 필요해요.\n\n가장 풍부한 모델, 다시 말해 매개변수가 가장 많은 모델은 일반적으로 크고 무겁고 실행하기 비싸며, CPU 및 메모리를 소모하여 마치 난로로 사용할 수 있을 정도로 비효율적일 수 있어요.\n\n이 글을 쓰는 시점에서 Huggingface의 LLM Repo에는 658,281개의 모델이 있어요. 맞아요, 잘 이해하셨어요.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_2.png)\n\nIt’s mostly found on Huggingface, but there are other repos like Kaggle — Think of it as a “Dockerhub for LLMs”. We won`t go into LLM Model formats available, as we can be here all day, and this post will shortly be long enough.\n\n## Checkout Model Leaderboard to help pick the LLMs you’re after\n\nHuggingface Does have handy leaderboards, where you can pick the “Best Performing Model for it’s purpose” be it for chat, reasoning, Coding and other use cases, may want to poke around here to have an idea of what model you may want to pick for You use case — perhaps even in the future.\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_3.png)\n\n신나시나요? 정말 멋진 거죠?\n하지만 너무 흥분하지 마세요. 알고 계셔야 할 것은 여러 LLM/모델이 있습니다. 이것들 중에서는 일부가 클라우드에서 온디맨드로 실행되어야 하기 때문에 당신의 노트북에서 실행할 수 없을 수도 있어요.\n\n우리는 성능 우수한 것을 로컬에서 오프라인으로 실행하고 싶어요. 저는 여러분을 도울 거예요!\n\n## 들어보세요. 주의해야 할 점이 있어요. 쉽거나 어려울 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n생성적(오프라인) AI 모델의 세계에서 \"최고\"를 찾는 것은 단순히 벤치마킹에서 성능이 우수한 것을 선택하는 것만큼 간단하지 않습니다. 성능과 기계가 처리할 수 있는 것 사이의 적절한 균형을 찾는 것입니다.\n\n그것은 미묘합니다.\n\n예를 들어, M 클래스 MacBook을 가진 개발자로써 여러 능숙한 모델을 실행할 수 있는 능력이 있습니다. 그러나 이상적인 모델을 선택하는 것은 귀하의 특정한 요구 사항에 따라 달라집니다.\n\n- 빠른 JavaScript 스니펫을 생성하기 위한 가벼운 모델을 찾고 있습니까, 아니면 복잡한 작업을 위한 보다 강력한 모델을 찾고 있습니까?\n\n<div class=\"content-ad\"></div>\n\nKaggle 및 Hugging Face와 같은 리소스는 모델 기능 및 요구 사항에 대한 자세한 정보를 제공합니다. 평가 결과를 찾아보고 모델 크기 및 계산 요구 사항을 고려할 수 있습니다.\n\n# 이제 뭘 해야 할까요?\n\n알았어요, 알았어요, — 우리에게 \"시작\"은 간단함을 의미합니다. 그리고 위의 내용이 너무 많다고 느낀다면, 2-3단계로 더 요약된 옵션을 제시해 드리겠습니다.\n\n## 1. Ollama로 시작해보세요. 다운로드하세요\n\n<div class=\"content-ad\"></div>\n\nLM Studio와 유사하지만 내 개인 의견으로 Ollama는 훨씬 간단하고 깔끔하다고 생각해요.\n\n![image](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_4.png)\n\n- LLMs 다운로드 - 문서를 확인해보세요. 하지만, 터미널에서 'ollama3'을 실행하면 최소한의 작업량으로 즉시 로컬 오프라인 모델과 대화할 수 있어요. 웹사이트의 정갈하게 정리된 목록에서 볼 수 있는 어떤 모델이든 빠르고 쉽게 설치할 수 있는 방법이죠.\n- 선택 사항: 특정 포트에 로컬로 LLM 서비스 제공. 저는 RAG/스크립트를 사용하여 동일한 포트를 사용하는 LM Studio에 대한 별칭(alias)이 있어요. 그래서 터미널에서 'ollamaserve'는 'OLLAMA_HOST=0.0.0.0:11434 ollama serve'로 설정되어 있어요.\n\n## 2. IDE에 로컬 모델 LLM 확장프로그램 다운로드하기.\n\n<div class=\"content-ad\"></div>\n\n- 저는 VSCode를 사용하고 있어요. Continue.dev를 좋아하며, 또는 Cody LLM을 사용할 수도 있어요. 이것 또한 시작하기 쉽고 깔끔해요.\n\n![image1](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_5.png)\n\n- 확장 프로그램을 구성하여 Ollama/Locally 호스트된 모델을 사용해 보세요.\n\n![image2](/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_6.png)\n\n<div class=\"content-ad\"></div>\n\n- 완료\n\n여기 Ollama와 함께 사용할 수 있는 내 모델에 대한 내 견해입니다. 내가 원하는 목적에 맞게 특정 모델을 선택하고 선택할 수 있습니다.\n\n다양한 모델의 장단점을 이해함으로써, AI 지원 개발 목표를 달성하면서 시스템을 압도하지 않는 모델을 선택할 수 있습니다.\n\n# 마무리. 끝났고, 설정되었으니 — 행운을 빕니다!\n\n<div class=\"content-ad\"></div>\n\n상상해보세요 — 창의적인 텍스트 생성, 빠른 코드 작성, 아이디어 떠올리기, 심지어 매력적인 대화를 나누기까지, 모두 클라우드로 데이터를 보내지 않아도 가능합니다. 이것이 오프라인 생성 AI 모델을 실행하는 마법입니다.\n\n이것이 명백한 사실이 될 수도 있고 아닐 수도 있지만, 혜택을 다시 요약해보겠습니다:\n\n- 개인정보 보호: 데이터에 완전한 통제권 — 제3자 접근에 대한 우려 없음.\n- 비용 효율성: 구독료나 사용량 제한 없음. 완전히 무료입니다! (최대한 멍청한 질문을 해도 부담 없고 아무도 알아차리지 못합니다.)\n- 오프라인 신뢰성: 그리드에서 벗어나도 AI 도구에 액세스 가능합니다.\n- 맞춤 설정: 모델을 필요에 맞게 세밀조정하고 선호도에 맞게 조정합니다. (나만의 데이터셋과 맥락에 바탕을 둬보세요 — 고급 사용법)\n\n도움이 되었으면 좋겠습니다. LinkedIn에서 #GenAI의 #지수 및 #혁신적 시대에 대한 나의 생각을 공유해주시기 바랍니다. 좋아요, 공유하기, 구독하기, 팔로우해주시면 감사하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# OwnYourAI를 통해 AI를 소유하여 당신과 조화를 이루세요.\nJP","ogImage":{"url":"/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_0.png"},"coverImage":"/assets/img/2024-05-18-YourPrivateAICodeAssistantABeginnersGuidetoOfflineLLMonYourLocalMachine_0.png","tag":["Tech"],"readingTime":6},{"title":"2023년에 블록하지 않고 알려지지 않은 번호로부터 WhatsApp 메시지 수신을 중지하는 방법","description":"","date":"2024-05-18 21:10","slug":"2024-05-18-HowtoStopReceivingWhatsAppMessagesfromUnknownNumbersWithoutBlocking2023","content":"\n\n![How to Stop Receiving WhatsApp Messages from Unknown Numbers Without Blocking](/assets/img/2024-05-18-HowtoStopReceivingWhatsAppMessagesfromUnknownNumbersWithoutBlocking2023_0.png)\n\nIf you want to stop receiving WhatsApp messages from unknown numbers without resorting to blocking, there are a few steps you can take to manage your privacy and reduce unwanted messages. Here’s a guide on how to do it:\n\n# How to Stop receiving spam calls and messages from Unknown Numbers.\n\n## Privacy settings\n\n<div class=\"content-ad\"></div>\n\nWhats App을 열고 메뉴에서 \"설정\"으로 이동하세요. \"계정\"을 선택한 다음 \"개인정보\"를 클릭하세요. 여기에서 프로필 사진, 상태 및 최종 활동을 볼 수 있는 사람을 사용자 정의할 수 있습니다. 이러한 옵션을 연락처에만 제한하면 알 수 없는 번호가 이 정보에 액세스하거나 이를 기반으로 대화를 시작할 수 없습니다.\n\n## 메시징을 위한 개인 정보 설정 변경\n\n같은 \"개인정보\" 섹션에서 메시지를 보낼 수 있는 사람을 수정할 수 있습니다. \"누가 나에게 메시지를 보낼 수 있는지\"를 선택하고 \"내 연락처\"를 선택하여 주소록에 있는 사람만이 메시지를 보낼 수 있도록 설정하세요. 이 설정은 무작위 알 수 없는 번호가 당신의 수신함에 도달하는 것을 방지합니다.\n\n## 그룹 초대 제한\n\n<div class=\"content-ad\"></div>\n\n알 수 없는 번호에 의해 무작위 그룹에 추가되는 것을 피하려면 \"개인 정보\" 섹션으로 돌아가 \"그룹\"을 탭하세요. 거기에는 \"모두\", \"내 연락처\", \"제외한 연락처\" 세 가지 옵션이 제공됩니다. \"내 연락처\" 또는 \"제외한 연락처\"를 선택하면 동의 없이 타인이 당신을 그룹에 추가하는 것을 방지할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-HowtoStopReceivingWhatsAppMessagesfromUnknownNumbersWithoutBlocking2023_1.png)\n\n## 연락처 차단\n\n특히 성가신 알 수 없는 번호에서 메시지를 받으면 해당 번호를 개별적으로 차단할 수 있습니다. 채팅을 열고 상단의 연락처 이름을 탭한 후 \"차단\"을 선택하세요. 이 동작은 해당 특정 번호로부터의 추가 메시지를 방지합니다.\n\n<div class=\"content-ad\"></div>\n\n## 스팸 신고\n\nWhats App은 스팸 메시지를 신고할 수 있는 기능을 제공합니다. 알 수 없는 번호에서 스팸으로 의심되는 수신 메시지를 받으면 Whats App에 신고할 수 있습니다. 채팅을 열고 해당 연락처의 이름을 누르고 \"신고\"를 선택하세요. 이 정보는 Whats App이 스팸 탐지를 개선하고 다른 사용자를 보호하는 데 도움이 됩니다.\n\n이러한 단계를 따르면 Whats App에서의 개인 정보 보호를 강화하고 모르는 번호에서 오는 메시지 수를 줄일 수 있으며 모든 사용자를 차단하지 않아도 됩니다. 선호도와 필요에 맞게 개인 정보 설정을 정기적으로 확인하도록 하세요.\n\n자세한 내용 및 더 많은 기능은 TechToTools.com을 방문하여 전체 기사를 읽어보세요. 감사합니다. 좋아요 | 댓글 | 공유.","ogImage":{"url":"/assets/img/2024-05-18-HowtoStopReceivingWhatsAppMessagesfromUnknownNumbersWithoutBlocking2023_0.png"},"coverImage":"/assets/img/2024-05-18-HowtoStopReceivingWhatsAppMessagesfromUnknownNumbersWithoutBlocking2023_0.png","tag":["Tech"],"readingTime":2},{"title":"로컬 LLMs를 활용하는 초보자를 위한 설정 및 사용 가이드 Ollama와 함께 AI 파워 지역적으로 해제하기","description":"","date":"2024-05-18 21:06","slug":"2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama","content":"\n\n<img src=\"/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_0.png\" />\n\n이 가이드는 Ollama와 같은 지역 대형 언어 모델 (LocalLLMs)의 혁신적인 세계를 소개합니다. 이들이 AI의 개인 정보 보호, 맞춤화 및 접근성을 어떻게 재정의하는지 깊이 파헤치며, 초보자든 기술 애호가든 사용자 친화적인 방식과 현실적인 통찰력을 통해 기기에서 LLM의 힘을 활용하는 방법을 배웁니다. AI가 더 개인적이고 안전하며 당신의 필요에 맞게 맞춤화된 미래로 뛰어들어보세요. 프로그래머가 아닌 사람들을 위해 빠르게 시작할 수 있는 AnythingLLM 튜토리얼도 확인해보세요.\n\n# 목차\n\n- 소개\n- 지역 대형 언어 모델 (LocalLLMs)이란?\n– 정의 및 설명\n– 지역에서 LLM 실행의 장점\n- Ollama 탐구: 지역 추론의 필수품\n– 지역 추론을 위한 Ollama의 다른 프레임워크들과의 차이\n– 대안 프레임워크 및 독특한 특징\n– 컴퓨터에 Ollama 설정하기: 단계별 안내서\n- 모델에 대한 심층 탐구\n– Llama2\n– Llama2-비격식\n– Code Llama\n– Mistral\n– LLaVA\n- 과정: 문서부터 답변까지\n– 자세한 단계: 문서 불러오기부터 응답 검색까지\n- 지역 대형 언어 모델과 상호작용하기 위한 프론트엔드 옵션\n– 1. AnythingLLM: 기술을 잘 모르는 사람을 위해\n– 2. 터미널: 기본 명령어에 익숙한 사람을 위해\n– 3. 코드 (Python + LangChain + Streamlit): 기술 애호가를 위해\n- 결론\n- 추가 자료 및 참고 문헌\n\n<div class=\"content-ad\"></div>\n\n# 소개\n\n인공지능(AI)의 빠르게 변화하는 환경에서 대규모 언어 모델(Large Language Models, LLMs)은 혁신의 중심 엔진으로 등장했습니다. 이 모델들은 자연어 처리, 콘텐츠 생성 및 기타 영역을 혁신적으로 변화시키고 있습니다. 기존에는 LLMs의 활용이 주로 클라우드 기반 서비스에 의존했지만, 이는 강력한 계산 능력을 갖추었지만 개인 정보 보호 문제, 제한된 제어권, 사용자 정의의 부재 등과 같은 단점을 가지고 있습니다. 이러한 배경은 사용자의 개인 장치나 개인 서버에서 실행할 수 있는 이러한 강력한 모델의 변형인 로컬 LLMs로 크게 전환되고 있음을 보여줍니다. 이러한 변화는 단순히 개인 정보 보호 및 데이터 보안을 강화하기 위한 것뿐만 아니라, 소유 시스템 내에서 전례없는 사용자 정의 및 통합 능력을 열어줍니다. 결과적으로, LLMs가 더 접근 가능하고 유연하며 다재다능해지는 환경이 조성됩니다. 이는 그들의 유틸리티와 응용 프로그램을 근본적으로 변경합니다.\n\n오픈 소스 LLMs로 가는 움직임은 이러한 장점을 더욱 강화시키고, 사실상 AI 기술의 정점에 접근할 수 있도록 접근성을 민주화하고 있습니다. 메타의 Llama와 Mistral AI의 Mistral과 같은 오픈 소스 모델이 투명성, 유연성 및 커뮤니티 중심의 지원의 중요성을 강조하며, 이 혁신적인 파도를 주도하고 있습니다. 닫힌 시스템과는 달리 이러한 오픈 소스 LLMs는 기업과 개발자들에게 고객 서비스 경험 개선이나 코드 생성을 최적화하는 등에 AI 도구를 자사 요구 사항에 맞게 맞춤화할 수 있는 기회를 제공합니다. 이 사용자 정의 능력은 혁신을 촉진하고 다양한 분야에서 효율성을 높입니다.\n\n이 변화 속에서 데이터 보안에 대한 쉬운 설정, 적응성 및 강력한 제어력으로 뛰어난 프레임워크인 Ollama가 돋보이고 있습니다. 이 글은 더 넓은 AI 개발 환경에서 로컬 LLMs의 중요성을 밝히고 오픈 소스 모델을 수용하는 현실적인 이점을 강조하는 데 초점을 맞추었습니다. Ollama를 효과적으로 설정하고 배포하는 방법에 대해 상세히 설명하여, 사용자들이 다양한 응용 프로그램에서 로컬 LLMs의 능력을 완전히 활용할 수 있도록 돕고자 합니다.\n\n<div class=\"content-ad\"></div>\n\n로컬에서 LLM을 실행하는 가치를 강조하는 Reddit 토론이, 개인 및 기관이 LocalLLMs로의 이동을 선택하는 다양한 이유를 적절하게 강조합니다. 데이터 개인 정보 보호가 강화되고 AI 능력이 특정 요구에 맞게 맞춤화되는 등 다양한 이유가 그중에 있습니다. 이러한 전환은 보다 포괄적인 산업의 오픈 소스 솔루션으로 향하는 흐름과 일치하여, AI 개발 및 배포에 대한 더 포괄적이고 유연하며 안전한 접근 방식을 제공합니다.\n\n시작하기 전에 강조하는 것은, 글쓴이의 로컬LLMs에 대한 탐구가 OpenAI, Anthropic, 또는 Gemini과 같은 클라우드 기반 LLM 서비스에 반대로 비롯된 것이 아니라는 것입니다. 이러한 플랫폼들은 의심할 여지 없이 놀라운 도구들이며, 여러 측면에서는 맞춤화되고 정교한 수준의 능력을 제공합니다. 이 글의 목적은 그 가치를 감소시키거나 LocalLLMs가 모든 사용 차원에서 우수하다고 제안하는 것이 아닙니다. 대신, LocalLLMs에 초점을 맞춘 것은 특히 개인 정보 보호, 맞춤화, 그리고 데이터 보안과 같은 독특한 장점과 기회를 강조하고자 하는 욕구에서 비롯된 것입니다. 여기서 논의된 많은 개념과 기술은 클라우드 LLM을 활용할 때에도 동등하게 적용되며 막대한 혜택을 제공합니다. 로컬과 클라우드 기반 모델 중에서 선택하는 것은 최종적으로 각 사용자 또는 기관의 특정한 요구사항, 제약 조건, 그리고 목표에 따라 다릅니다. 두 접근 방식은 가치 있으며, 많은 경우에 넓은 AI 도구 및 기술 생태계의 보완적 구성 요소입니다.\n\n# LocalLLMs란 무엇인가요?\n\nLLM(Large Language Models)은 인공 지능에서 가능한 범위를 재정의하며, 자연어의 이해, 생성, 상호작용 등에서 이전에 없던 능력을 제공합니다. 이러한 모델은 일반적으로 클라우드 기반 플랫폼을 통해 접근되어 왔지만 새로운 패러다임이 주목받고 있습니다: Local Large Language Models 또는 LocalLLMs입니다. 이러한 LLM 버전은 개인 컴퓨터, 개인 서버 또는 엣지 디바이스와 같은 로컬 하드웨어에 설치되고 실행할 수 있습니다. 로컬 배포로의 이동은 LLM 기술의 접근성과 적용 가능성에 있어서 중요한 진화를 나타냅니다.\n\n<div class=\"content-ad\"></div>\n\n## 정의 및 설명\n\nLocalLLMs는 최신 기술로 주목받고 있는 인공지능 모델로, 기사 작성, 코드 생성, 질문 답변 등 다양한 작업을 수행할 수 있는 능력으로 유명합니다. 그러나 이 모델들의 핵심적인 차이점은 운영 프레임워크에 있습니다. LocalLLMs는 인터넷을 통해 액세스하는 원격 서버에 의존하는 대신, 사용자의 하드웨어에서 직접 작동합니다. 이는 모든 데이터 처리 및 모델 추론 작업이 로컬에서 수행되므로, 지속적으로 인터넷 연결이나 외부 서버로 데이터 전송이 필요하지 않다는 것을 의미합니다.\n\n## LocalLLMs를 로컬 환경에서 실행하는 장점\n\nLocalLLMs로의 전환은 기술적인 새로움뿐만 아니라 디지털 시대의 다양한 요구사항과 우려에 대한 대응입니다. 개인 정보 보호와 데이터 보안이 가장 중요한 장점으로 부각됩니다. LocalLLMs를 사용하면 기밀 데이터가 사내에 머묾하면서도 데이터 유출이나 불법적인 액세스와 같은 위험을 감소시킬 수 있습니다. 게다가 LocalLLMs를 로컬에서 실행하면 모델이 언제, 어떻게 업데이트 또는 사용자 정의할지를 포함하여 사용자가 모델을 완전히 제어할 수 있습니다. 이러한 제어는 성능 최적화에도 확장됩니다. 사용자는 모델 매개변수를 자신의 하드웨어와 응용 프로그램 요구 사항에 맞게 조정할 수 있습니다. 더불어 지연 시간을 크게 줄일 수 있는데, 인터넷을 통해 데이터를 클라우드 서버로 전송하고 다시 받아오지 않아도 되기 때문에 비상 상황에서 빠른 응답 시간을 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Ollama 탐험: 로컬 인퍼런스를 위한 당신의 선택지\n\nOllama는 로컬 머신에서 언어 모델을 구축하고 실행하기 위한 매우 인기 있는 가벼운 확장 가능한 프레임워크입니다. 백만 회 이상의 Docker pull을 기록하면서, 대용량 언어 모델(LLM)과 상호작용하려는 사람들에게 개인 데이터를 제 3자 서비스로 전송하지 않으면서 이를 수행하기 위한 해결책으로 자리매김했습니다. Ollama를 특히 매력적으로 만드는 주요 기능은 사용 편의성, 확장 가능성 및 강력한 모델 기능입니다. 텍스트 생성, 언어 번역, 창의적인 콘텐츠 작성, 유익한 질의응답을 지원합니다. Ollama는 연구자와 개발자를 위한 가치 있는 도구이며, 챗봇, 요약 도구 및 창의적인 글쓰기 어시스턴트와 같은 언어 기반 응용 프로그램을 구축하기 위한 훌륭한 플랫폼입니다.\n\n## Ollama가 로컬 인퍼런스를 위한 다른 프레임워크들 사이에서 빛나는 이유\n\n![이미지](/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_1.png)\n\n<div class=\"content-ad\"></div>\n\nOllama는 몇 가지 독특한 장점을 통해 자신을 구별합니다:\n\n- 사용 편의성: Ollama는 사용자 친화적으로 설계되어 간단한 설치 및 사용 프로세스로 이전 언어 모델 경험이 없는 사람들에게도 접근할 수 있습니다.\n- 확장성: 사용자는 사용자 지정 모델을 만들거나 다른 소스에서 모델을 가져와 도구를 특정 요구 사항에 맞게 조정할 수 있는 막강한 유연성을 제공합니다.\n- 강력하고 다재다능: Ollama의 모델은 다재다능하며 뛰어난 성능을 제공하여 다양한 자연어 처리 작업을 수행할 수 있습니다.\n- 커뮤니티 및 생태계: 활발한 Discord 커뮤니티(3만명 이상의 회원)와 다양한 커뮤니티 통합을 통해 Ollama는 로컬 추론 주변의 활기찬 생태계를 육성합니다.\n\n## 대안 프레임워크 및 그들의 독특한 기능\n\nOllama는 로컬 추론을 위한 폭넓은 기능 세트를 제공하지만, 각각 고유한 강점을 가진 다른 프레임워크도 언급할 가치가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- llama.cpp: 고도로 최적화된 성능으로 알려진 llama.cpp는 ROCm으로 인해 상당한 크기를 차지하지만 지원되는 GPU에서 특히 성능 향상을 제공하는 별도의 이미지입니다.\n- GPT4All: 제공된 소스에서 명시적으로 언급되지는 않았지만, 일반적으로 GPT 모델의 접근성과 배포의 용이성에 중점을 둔 것으로 알려져 있습니다.\n- Mozilla-Ocho의 llamafile: 제공된 소스에서는 이 프레임워크에 대해 자세히 설명하지 않았지만, Mozilla의 오픈 소스 프로젝트에 대한 기여는 주로 개인 정보 보호, 보안 및 커뮤니티 협업을 강조합니다.\n\nOllama의 사용 편의성, 확장성, 강력한 기능은 서버의 대형 언어 모델에 대한 로컬 추론을 이용하려는 모든 사용자에게 뛰어난 선택지가 됩니다. llama.cpp와 같은 대안 프레임워크가 최적화된 성능을 제공하는 반면, Ollama의 넓은 인기 요인은 해당 프레임워크의 접근성, 활기찬 커뮤니티, 및 지원하는 통합 환경에 있습니다.\n\n# 컴퓨터에 Ollama 설정하기: 단계별 가이드\n\nOllama는 LocalLLM 세계에서 쉬운 설치와 사용으로 두각을 나타냅니다. macOS와 Windows에서 추론을 실행하는 간편한 방법을 제공합니다. 이 안내서를 통해 두 운영 체제에 Ollama를 설정하는 방법을 안내해 드릴 것이며, 여러분의 프로젝트에 로컬 LLM의 성능을 활용할 준비를 할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## macOS 사용자를 위한 안내\n\n옵션 1: Homebrew 사용하기\n\n- 터미널 열기: 유틸리티 폴더에서 터미널 애플리케이션을 찾거나 Spotlight를 사용하여 검색하세요.\n- Homebrew 설치: Homebrew가 이미 설치되어 있지 않은 경우, 다음 명령을 터미널에 붙여넣고 Enter 키를 눌러 Homebrew를 설치할 수 있습니다. 이 명령은 Homebrew 웹사이트에서도 사용할 수 있습니다.\n\n```js\n/bin/bash -c \" $(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh) \"\n```\n\n<div class=\"content-ad\"></div>\n\n화면에 나오는 안내에 따라 설치를 완료해 주세요.\n\n3. Ollama 설치: Homebrew를 설치한 후에는 다음 명령어를 터미널에서 실행하여 Ollama를 설치할 수 있습니다:\n\n```js\nbrew install ollama\n```\n\n이 명령은 Homebrew 저장소에서 Ollama 패키지를 가져와 설치합니다.\n\n<div class=\"content-ad\"></div>\n\n옵션 2: 설치 프로그램 사용하기\n\n- 설치 프로그램 다운로드: Ollama 웹사이트를 방문하여 macOS 설치 프로그램을 다운로드할 수 있습니다. 컴퓨터로 설치 프로그램을 다운로드하세요.\n- 설치 프로그램 실행: 다운로드한 파일을 찾아서(일반적으로 다운로드 폴더에 위치함) 더블 클릭하여 열어주세요. 시스템에 Ollama를 설치하려면 설치 안내에 따라 진행하세요.\n\n## Windows 사용자를 위한 안내\n\n설치 프로그램 사용하기\n\n<div class=\"content-ad\"></div>\n\n- 설치 프로그램 다운로드: 올라마 웹사이트로 이동해서 다운로드 섹션을 찾아서 윈도우용 설치 프로그램을 선택하세요. 파일을 컴퓨터로 다운로드합니다.\n- 설치하기: 다운로드한 설치 프로그램을 저장된 위치(일반적으로 다운로드 폴더)로 이동해서 두 번 클릭하세요. 안내에 따라 설치 단계를 진행하면서 시스템에 올라마를 설정할 수 있습니다.\n\n## 올라마 서버 시작 및 모델 실행\n\n![Ollama 이미지](/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_2.png)\n\n올라마가 설치되면 서버를 시작하고 모델을 실행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- Ollama Server 시작: 터미널(macOS)이나 명령 프롬프트(Windows)를 열고 다음 명령을 입력하세요:\n\n```js\nollama serve\n```\n\n이 명령을 입력하면 Ollama 서버가 시작되어 요청을 처리할 준비가 됩니다.\n\n- 모델 실행: 특정 모델을 실행하려면 아래 명령을 사용하고, `[LLM]`을 사용하려는 모델의 이름으로 바꿔주세요:\n\n<div class=\"content-ad\"></div>\n\n```bash\nollama run [LLM]\n```\n\n예를 들어, Llama2 모델을 실행하려면 다음을 입력하십시오:\n\n```bash\nollama run llama2\n```\n\n## 모델 라이브러리 탐색하기\n\n<div class=\"content-ad\"></div>\n\nOllama는 다양한 요구 사항과 응용 프로그램에 적합한 모델 라이브러리를 제공합니다. 사용 가능한 모델을 살펴보려면 Ollama 웹사이트를 방문하거나 다음 명령을 사용하여 터미널이나 명령 프롬프트에서 모델 목록을 직접 확인할 수 있습니다:\n\n```js\nollama list-models\n```\n\n이 명령을 통해 Ollama와 실행할 수 있는 모델의 개요를 제공받아 프로젝트 요구 사항에 가장 잘 맞는 모델을 선택할 수 있습니다.\n\n## 추가 자료\n\n<div class=\"content-ad\"></div>\n\nOllama 튜토리얼을 참조하여 더 자세한 지침, 문제 해결 및 고급 기능을 확인해보세요. 이 자료는 macOS 및 Windows에서 Ollama의 기능을 최대한 활용할 수 있도록 종합적인 지침을 제공합니다.\n\n컴퓨터에 Ollama를 설정하는 것은 간단하며, 개인 또는 전문적인 용도로 LocalLLMs의 방대한 잠재력을 발휘할 수 있도록 합니다. AI 모델을 실험하거나 애플리케이션을 개발하거나 연구를 수행하든, Ollama는 여러분의 노력을 지원하기 위한 강력하고 유연한 플랫폼을 제공합니다.\n\n# 모델의 심층 탐구\n\n로컬 대형 언어 모델(Local Large Language Models, LocalLLMs)의 영역은 자연어 처리 및 이를 넘어 다양한 필요에 부응하는 각종 모델들을 보여줍니다. 최신 세부 정보를 기반으로 여러분이 관심 있는 모델들에 대한 업데이트된 개요는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n## 라마2\n\n2023년에 Meta AI에 의해 출시된 라마2는 사전 학습 및 미세 조정된 다양한 크기의 LLM 패밀리입니다. 이 모델들은 70억(7B), 130억(13B), 700억(70B) 등 다양한 크기로 제공되며 텍스트 생성 및 프로그래밍 코드와 같은 다양한 자연어 처리 작업에 능숙합니다. 특히 대화 시나리오에 최적화된 라마2 모델은 Google 변환기 아키텍처를 기반으로 구축되었으며 다양한 공개 데이터를 통해 훈련되었습니다. 그들의 효율성과 공개적인 이용 가능성은 더 작은 조직 및 연구자들에게 특히 유용하며, 상당한 컴퓨팅 자원이 없어도 지역 인스턴스를 배포할 수 있게 합니다.\n\n## 라마2-무자비\n\n라마2-무자비는 라마2 모델 계통에서 나온 것이지만, 보편적으로 LLM에서 내재되어 있는 콘텐츠 중재 및 안전 메커니즘을 피해 특징을 갖추고 있습니다. George Sung과 Jarrad Hope가 제작했으며, Eric Hartford가 제시한 방법론을 따릅니다. 이 변형본은 더 넓은 범위의 필터되지 않은 응답을 제공하며, 콘텐츠 생성의 자유도를 높여 주지만, 불안전하거나 부적절한 결과물을 생산할 가능성도 높입니다. 창의적인 자유와 콘텐츠 안전성 사이에서 트레이드 오프를 강조합니다.\n\n<div class=\"content-ad\"></div>\n\n## 코드 람마\n\n코드 람마 가족의 정상에 위치한 Code Llama 70B는 코딩 작업을 위해 명시적으로 설계되어 있으며, 산업 기준을 제시하는 코드 생성 및 이해 능력을 자랑합니다. 7B에서 70B 매개변수로 확장되는 모델은 코드 완성, 채움, 코딩과 관련된 자연어 지시사항 해석에서 뛰어난 성과를 거두고 있습니다. 프로그래밍 언어의 다양한 배열을 지원하며, 연구 및 상업적 노력에 접근 가능하며, 개발자와 프로그래머들에게 상당한 자산을 제공합니다.\n\n## 미스트랄\n\nYounes Belkada와 Arthur Zucker가 개발한 Mistral은 Hugging Face를 통해 제공되며, 지시에 맞게 조정된 모델링의 진보를 대표합니다. 이전 모델을 향상한 Mistral-7B-Instruct-v0.2는 인과적 언어 모델링 작업을 위해 맞춤화되어 있으며, 보다 간략한 버전인 Mistral-tiny에 대한 적응이 가능하며, 다양한 컴퓨팅 자원에 사용할 수 있습니다. 이 모델은 강력한 LLM(Large Language Model)을 보다 쉽게 사용할 수 있도록 한 발전을 증명하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## LLaVA\n\n2023년 NeurIPS에서 소개된 LLaVA (Large Language and Vision Assistant)는 언어와 시각 처리 능력을 융합한 협업 프로젝트입니다. LLaVA 모델의 구체적인 세부 사항은 상세히 문서화되어 있지 않지만, 그 기초는 LLaMA 팀의 기여에 기반합니다. 이는 Alpaca와 Vicuna와 같은 다른 오픈 소스 프로젝트와 통합됩니다. LLaVA의 창시는 크리에이티브 커먼즈 저작자표시-동일조건변경허락 4.0 국제 라이센스 하에 이루어졌으며, AI 분야에서의 오픈 소스 협업 및 혁신에 대한 약속을 강조합니다.\n\n이러한 LocalLLMs는 각각의 능력과 전문성을 갖춘 구별된 모델로, 자연 언어 및 코딩 애플리케이션에서의 AI 혁신의 광범위한 영역을 엿볼 수 있습니다. Llama2, Llama2-Uncensored, Code Llama 70B, Mistral 및 LLaVA와 같은 모델이 각각 독특한 강점을 가지고 있지만, 그것들은 더 크고 빠르게 진화하는 AI 분야의 일부입니다. 간략히 설명하기 위해 이러한 모델만을 강조했지만, 분야가 다양한 두드러진 모델로 가득 차 있으며, 각각이 장단점을 제공합니다.\n\nLocalLLMs의 세계는 새로운 모델과 진보가 규칙적으로 등장하며, 이러한 모델의 잠재력에 흥미를 느끼고 이 흥미진진한 분야를 더 깊이 파고들기를 원하는 분들을 위해 Hugging Face와 같은 플랫폼 방문을 권장합니다. 여기서는 여기서 언급된 것 외에도 상세한 문서와 커뮤니티 통찰을 제공하는 다양한 모델을 탐색할 수 있습니다. Hugging Face는 AI 연구 및 응용 분야에 대한 활기찬 허브로, 사용자들은 다양한 모델뿐만 아니라 최신 개발 및 AI 커뮤니티 내의 창의적 노력에 대한 다양한 게시물 및 토론에 참여할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 탐험에 초대합니다. 오늘 사용 가능한 도구를 발견하는 것뿐만 아니라 AI의 미래를 형성하는 커뮤니티에 참여하는 것에 관한 것입니다. 개발자, 연구자 또는 AI 애호가이든지, 다양한 지식과 자원을 통해 당신이 필요로 하는 완벽한 모델을 찾을 수 있도록 돕거나 이 커다란 분야에 기여하도록 영감을 줄 수 있습니다. 그래서 LocalLLMs의 세계로 뛰어들어 Hugging Face와 그 이상을 탐험하고 싶은 호기심 많은 분들을 기다리고 있습니다.\n\n# 과정: 문서부터 답변까지\n\n원시 문서부터 통찰력 있는 답변까지의 여정은 Local Large Language Models (LocalLLMs)을 통해 일련의 핵심 구성 요소를 거칩니다: 원본 문서, 임베딩 및 벡터 데이터베이스(Vector DB). 이러한 요소들이 상호 작용하는 방법을 이해하면 LLMs의 능력이 빛을 발하게 됩니다. 정보를 처리하는 능력 뿐만 아니라 가치 있는 통찰력으로 변화시키는 것도 가능합니다. 이 과정을 단계별로 살펴보면, 그 간단함과 효율성을 보여줄 것입니다.\n\n![image](/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_3.png)\n\n<div class=\"content-ad\"></div>\n\n- 문서: 이들은 프로세스에 사용되는 원자료입니다. 문서는 간단한 텍스트 파일이나 기사부터 레포트나 이메일과 같은 보다 구조화된 데이터까지 다양할 수 있습니다. 이 문서들의 내용을 분석하거나 검색하거나 응답 생성하는 데 사용할 것입니다.\n- 임베딩: 문서 내용을 LLM이 이해하고 사용할 수 있도록 하기 위해 이를 임베딩으로 변환합니다. 임베딩은 텍스트의 의미적 의미와 관계를 캡처하는 고차원 숫자 표현으로, LLM이 처리할 수 있는 형식입니다.\n- 벡터 데이터베이스 (벡터 DB): 이 데이터베이스에는 유사성 검색을 지원하는 방식으로 임베딩이 저장됩니다. 사용자 쿼리에서 생성된 임베딩으로 벡터 DB를 쿼리함으로써 가장 관련성 있는 문서 임베딩 및 따라서 문서 자체를 찾을 수 있습니다. 저는 현재 Chroma를 선호하지만, 개발과 빠른 설정을 위한 몇 가지 기본 기능이 있습니다. 더 많은 정보는 이 문서에서 얻을 수 있습니다.\n\n## 자세한 단계: 문서로드부터 응답 검색까지\n\n- 문서 로드 (PDF): 프로세스는 원문서로 시작하며, PDF 파일로 표시됩니다. 이 문서에는 작업할 텍스트가 포함되어 있습니다. 임베딩의 종류를 변경하여 항상 다른 유형의 문서를 추가할 수 있습니다.\n- 텍스트 청크 (문서): 다음으로, PDF에서 텍스트가 추출되어 관리 가능한 청크로 분할됩니다. 이 분할은 텍스트를 작은, 더 집중된 세그먼트로 처리하는 데 중요합니다.\n- 임베딩: 각 텍스트 청크는 그런 다음 임베딩으로 변환됩니다. 이러한 숫자 표현은 텍스트에서 의미 정보를 인코딩하여 기계 학습 모델에서 해석할 수 있게 합니다. 데이터 유형에 따라 다른 임베딩 유형이 더 잘 작동합니다.\n- 벡터 데이터베이스: 생성된 임베딩은 벡터 데이터베이스에 저장됩니다. 이 데이터베이스는 임베딩의 빠른 검색 및 비교를 지원하도록 설계되었습니다.\n- 질문 임베딩: 질문이 제시되면 해당 질문이 임베딩으로 변환되어 질문 임베딩이라고 알려진다. 이를 통해 시스템은 저장된 문서 임베딩과 질문의 의도를 의미적으로 비교할 수 있습니다.\n- 의미적 검색: 질문 임베딩을 사용하여 시스템은 벡터 스토어 내에서 의미적 검색을 수행하여 질문 의도와 가장 일치하는 문서 임베딩을 찾습니다.\n- 순위 매긴 결과: 의미적 검색은 순위 매긴 결과를 제공하며, 질문 임베딩과 가장 일치하는 최상위 임베딩이 연관된 문서 청크를 보여줍니다. 이러한 결과는 쿼리와의 관련성에 따라 순위가 매겨집니다.\n- LLM (답변): 최상위 임베딩과 관련된 문서 청크는 대형 언어 모델(LLM)에 입력됩니다. LLM은 이 문맥을 사용하여 쿼리에 대한 일관된 상세한 답변을 생성합니다. 이 답변은 사용자에게 제공되어 문서로드에서 통찰력있는 응답을 검색하는 프로세스를 완료합니다.\n\n여기에 제공된 개요는 로컬 대규모 언어 모델(Local Large Language Models, LocalLLMs)의 광범위한 능력과 복잡한 프로세스를 간략히 소개한 것입니다. 중요한 구성 요소와 고수준 워크플로우에 대해 다루었지만, 해당 분야는 다양한 개념과 기술이 들어가는 넓은 지식 영역이며 더 많은 개념 및 기술이 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사는 미필의 세부사항을 탐구하거나 철저한 논고를 제시하는 것을 목표로 하지 않습니다. 대신에 어떤 복잡한 프로세스들을 간단하게 정리해줄만한 개요를 제공하고자 합니다. 때로는 설명이 기술적 세부사항을 완전히 포착하지 못하거나 그러한 정교한 시스템에 내재된 미묘한 부분을 생략할 수도 있습니다.\n\n임베딩, 벡터 데이터베이스 및 LLM에 대한 깊은 이해를 원하는 독자들은 다양한 자원과 기술 논문을 찾을 수 있습니다.\n\n# 로컬 LLM과 상호작용하기 위한 프론트엔드 옵션\n\nOllama와 대규모 언어 모델의 능력을 활용하는 것이 어려운 과제일 필요는 없습니다. 코딩 경험이 전혀 없는 경우든 Python과 같은 스크립팅 언어에 익숙한 경우든, 여러분에게 알맞은 방법이 있습니다. 아래에서는 Ollama와 상호작용하는 세 가지 사용자 친화적인 방법을 소개하고 있으며, 각각 다른 기술 전문성 수준을 고려하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 1. AnythingLLM: 비 기술자를 위한\n\n![이미지](/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_4.png)\n\nAnythingLLM은 몇 번의 클릭으로 자신의 Ollama 서버를 실행하거나 기존 서비스에 연결할 수 있는 그래픽 사용자 인터페이스(GUI)를 제공합니다. 코딩은 필요하지 않습니다. 시작하는 방법은 다음과 같습니다:\n\n- 설정: AnythingLLM을 설치하고 Ollama 서버에 연결하도록 구성합니다.\n- 사용법: 직관적인 GUI를 사용하여 쿼리를 입력하고 응답을 받습니다.\n- 자원: AnythingLLM 자습서를 참조하여 서비스 설정 및 사용에 대한 단계별 안내를 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_5.png\" />\n\nAnythingLLM은 로컬 LocalLLM과 손쉽게 상호 작용할 수 있도록 사용자 친화적인 인터페이스를 제공하는 플랫폼입니다. 여기 Ollama를 사용하여 AnythingLLM을 시작하는 빠른 안내서가 있습니다:\n\n- Ollama 선택: AnythingLLM을 열고 나면 LLM 제공자로 Ollama를 선택하십시오. 이렇게 하면 인터페이스가 로컬 Ollama 서버에 연결됩니다.\n- Ollama 기본 URL: Ollama 기본 URL로 http://127.0.0.1:11434를 입력하십시오. 이는 AnythingLLM이 Ollama 서버에서 요청을 수신하는 로컬 주소를 가리킵니다.\n- 채팅 모델 선택: 모델 선택 드롭다운에서 mistral:latest 또는 Ollama 서버에 설치된 다른 모델을 선택하십시오. 이렇게 하면 AnythingLLM이 응답 생성에 사용할 특정 LLM을 알 수 있습니다.\n- 토큰 컨텍스트 윈도우: Mistral 모델의 경우 토큰 컨텍스트 윈도우를 4096로 설정하여 컨텍스트 크기가 모델 용량과 일치하도록 합니다. 서로 다른 모델은 서로 다른 컨텍스트 크기를 요구할 수 있음을 유의하십시오.\n\n임베딩을 위해:\n\n\n| 병합 테스트용 | 테이블 | 변환 |\n|-----------|-------|-----|\n| 열1     | 열2  | 열3 | \n| 아이템1  | 아이템2| 아이템3|\n\n\n여기까지입니다! 문제가 발생하면 언제든지 물어보세요. 😉\n\n<div class=\"content-ad\"></div>\n\n- 최대 임베딩 청크 길이: 임베딩의 최대 청크 길이를 8192로 설정하세요. 이는 모델이 임베딩 생성을 위해 처리하는 텍스트 세그먼트의 크기를 결정합니다.\n\n벡터 데이터베이스를 위해:\n\n- LanceDB: 벡터 데이터베이스로 AnythingLLM 내에 포함된 LanceDB를 사용하세요. LanceDB는 추가 설정이 필요하지 않으며 즉시 사용할 수 있습니다.\n\n이 튜토리얼은 기본에 초점을 맞추어 전사 모델을 건너뛰며 진행됩니다. 이 간단한 단계를 따르면 코드 한 줄을 작성하지 않고도 AnythingLLM을 통해 Ollama 서버와 상호 작용할 수 있습니다. 다가오는 안내서는 시각적인 자세한 지시사항과 추가 자원을 제공하여 프로세스를 쉽게 따를 수 있게 해줄 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 2. 터미널: 기본 명령어에 익숙한 분들을 위한\n\n터미널을 사용하면 간단한 명령어를 사용하여 Ollama 서버와 직접 상호작용할 수 있어서, 노력형 접근 방식을 선호하는 분들에게 적합합니다.\n\n- 설정: Ollama가 설치되어 있고 기계에서 실행 중인지 확인하세요 (ollama serve).\n- 사용법: 쿼리를 직접 입력하고 터미널 창에서 응답을 확인하세요 (ollama run mistral).\n- 자료: Ollama 명령줄 튜토리얼을 통해 LLM과 상호작용하는 데 필요한 기본 명령어가 안내됩니다.\n\n![이미지](/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_6.png)\n\n<div class=\"content-ad\"></div>\n\n## 3. 코드 (Python + LangChain + Streamlit): 기술 열정가를 위해\n\n코딩에 도전하고 싶다면, Python과 LangChain 그리고 Streamlit을 사용하여 대화형 웹 애플리케이션을 만들 수 있습니다.\n\n기본적으로, 선택한 LocalLLM으로 Ollama를 사용하고, LangChain(언어모델 프레임워크)과 Streamlit(웹 프레임워크)을 사용할 것입니다.\n\n![image](/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_7.png)\n\n<div class=\"content-ad\"></div>\n\n- 설치: Python, LangChain 및 Streamlit을 설치한 후 Ollama에 연결하는 기본 스크립트를 설정합니다.\n- 사용법: Ollama에 쿼리를 보내고 결과를 표시하는 사용자 정의 프론트엔드 응용 프로그램을 구축합니다.\n- 자료: 자신의 AI 기반 앱을 만드는 방법에 대한 지침은 Streamlit 및 LangChain 튜토리얼을 참고하십시오.\n\n# 결론\n\n로컬 대형 언어 모델(Local Large Language Models, LocalLLMs)의 세분화를 통해 Ollama가 제공하는 편리함과 접근성을 통해, 인공 지능의 세계는 이전보다 더 접근하기 쉬운 것으로 나타났습니다. 이러한 고급 모델을 로컬 서버에서 실행할 수 있는 능력은 데이터 개인 정보 보호 및 보안을 극대화하며, AI 도구를 우리 고유한 요구에 맞게 조정할 수 있는 유연성을 제공합니다. 사용자 친화적인 설정과 다재다능성을 갖춘 Ollama는 LocalLLMs의 세계로 진입하는 사람들에게 특별한 존재로 빛을 발하고 있습니다.\n\nOllama의 간소화된 과정을 통해 원본 텍스트에서 실용적인 통찰력을 얻는 이 여정은, 저희가 사용할 수 있는 잠재력과 역량을 강조합니다. 제시된 단계와 방법론을 따라가며, 여러분도 이 잠재력에 참여하시기를 초대합니다. 여러분이 AI의 세계에 처음 발을 딛거나 보다 고급 AI 기능을 자신의 작업에 통합하려 한다면 더 나은 방향으로 나아가실 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 글 전체를 통해 제공되는 자원과 링크를 탐색할 것을 장려합니다. 이들은 LocalLLM의 깊은 이해와 실용적인 적용을 위한 여정의 시작점 역할을 합니다. 튜토리얼을 살펴보고, 다양한 프론트엔드 옵션을 실험해보며, LocalLLM이 데이터와 기술과의 상호 작용을 어떻게 변화시킬 수 있는지 직접 경험해보세요.\n\nAI 분야가 계속 발전함에 따라, 토론의 내용도 변화할 것입니다. Qe will delve into the intricate world of vector databases and explore alternative LocalLLM frameworks. These topics will open new doors and reveal further possibilities in AI.\n\n개발자이든, 비즈니스 리더이든, AI 애호가이든, 미래는 기회로 넘쳐납니다. 이 도구들을 받아들이고 혁신가들의 커뮤니티에 참여하여 AI 발전의 다음 단계를 함께 모색합시다. AI로의 여정은 계속 됩니다. 각 단계는 발견과 잠재력을 약속합니다. 계속해서 함께 배우고, 창조하고, 혁신해봅시다.\n\n# 추가 자원과 참고 자료\n\n<div class=\"content-ad\"></div>\n\n <img src=\"/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_8.png\" />\n\nLocalLLM 활용 및 커뮤니티 통찰:\n\n- Reddit: Local LLM의 목적에 대한 토론\n- Ollama Discord 채널\n\nLocalLLMs에 대한 추가 정보 및 지식 확장을 위해 아래 자료들을 참고하시기 바랍니다!\n\n<div class=\"content-ad\"></div>\n\n## 오픈 소스 LLMs 개요:\n\n- IBM 블로그: 오픈 소스 LLMs의 장단점, 위험 및 종류\n- Substack: 설명하는 오픈 소스 LLMs의 역사\n\n로컬 LLMs 설정 안내:\n\n- The New Stack: Ollama와 Llama 2로 로컬 LLM 설정 및 실행 방법\n- LangChain 문서: 로컬 LLMs를위한 안내서\n\n<div class=\"content-ad\"></div>\n\n벡터 데이터베이스 통찰:\n\n- 뉴 스택: AI 프로젝트용 상위 5개 벡터 데이터베이스 솔루션\n\n이러한 리소스들은 현재의 환경을 포괄적으로 이해할 수 있는 지침, 설정 및 사용을 위한 실용적인 안내서, 역사적 맥락, LocalLLMs 및 관련 기술에 대한 기술적 깊이를 제공하기 위해 선정되었습니다. 시작하려는 초보자든지 이미 숙련된 개발자로서 LocalLLMs 사용을 최적화하고 싶은 분들이라도, 이들 참고 자료는 정보의 보물창고입니다.\n\nLocalLLMs와 함께 계속 탐험하며 개발하는 동안 이 리소스들을 책갈피에 추가해 두세요. 이들은 넓은 실무 커뮤니티로 이어지는 여정의 발판이자 AI 도구 상자의 자산입니다.","ogImage":{"url":"/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_0.png"},"coverImage":"/assets/img/2024-05-18-UnlockingAIPowerLocallyABeginnersGuidetoSettingUpandUsingLocalLLMswithOllama_0.png","tag":["Tech"],"readingTime":18},{"title":"믹스 네트워크 소개","description":"","date":"2024-05-18 21:04","slug":"2024-05-18-IntroductiontoMixNetworks","content":"\n\n\n![Mixnet introduction](/assets/img/2024-05-18-IntroductiontoMixNetworks_0.png)\n\nMixnets are important because they can provide strong anonymity, even stronger than Tor. Tor is widely known to have weak anonymity as described in academic literature. By utilizing advanced cryptographic methods and sophisticated mixing strategies, mixnets offer a more secure option for anonymous communication, effectively shielding users from both passive and active network observers.\n\n## Defining Network Anonymity\n\nA mixnet falls under the category of anonymous communication networks. Anonymous communication essentially refers to resistance against traffic analysis. In simpler terms, network anonymity implies that a client can interact with a network entity, and it becomes challenging for passive or active network observers to identify which client is communicating with which network entity. More clients using the network improve the privacy and anonymity features since anonymity thrives with greater participation.\n\n\n<div class=\"content-ad\"></div>\n\n# 익명성을 위한 패킷 혼합\n\n토르와는 달리, 믹스넷은 익명성 속성을 위해 경로의 예측 불가능성에 의존하지 않습니다. 대신, 믹스넷의 각 믹스는 패킷을 혼합하는 유형의 작업을 수행하며, 이 혼합이 익명성과 개인 정보 보호 속성을 제공합니다. 이 과정은 네트워크 경로가 관찰되더라도 들어오고 나가는 패킷 간의 상관 관계가 가려짐을 보증합니다.\n\n# 에니트러스트 모델\n\n에니트러스트 모델은 믹스넷 문헌에서 중요한 개념으로, 적어도 경로에서 하나 이상의 믹스가 손상되지 않고 정직하게 혼합을 수행하는 한 믹스넷 사용자가 개인 정보 보호 속성을 유지하는 위협 모델을 의미합니다. 이 모델은 일부 노드가 손상되었을 때에도 견고한 익명성을 보장하기 때문에 중요합니다. 에니트러스트 모델의 강점은 그 탄력성에 있어서, 이것이 현대적인 믹스넷 설계의 근간이 되게 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 제한 사항과 감시 자본주의 문제\n\n분명히 믹스네트들은 감시 자본주의 문제를 해결하지는 않습니다; 사람들은 감시 데이터를 수집하여 광고 기관에 판매하는 일부 매우 큰 기업이 제공하는 서비스를 사용하기를 선택합니다. 그런 감시 문제는 문화적 및 사회적 해결책이 필요합니다. 여기서 논의하는 것은 익명 통신 네트워크가 일반적으로 모든 인터넷 프로토콜이 통신 인프라로 메타데이터를 누출하도록 되어 있어 수동 관찰자가 누가 누구에게 얘기하는지 알 수 있는 기술적 문제에 대한 기술적 해결책입니다.\n\n# 메타데이터 누출 이해\n\n수동 네트워크 관찰자는 암호화되어 있더라도 통신 세트에 대해 많은 정보를 얻을 수 있습니다. 특히, 그들은 다음을 알 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 지리적 위치\n- 메시지 순서\n- 보낸 및 받은 메시지 크기\n- 통신이 발생한 시간대\n- 모든 통신 파트너의 신원 및 전체 소셜 그래프\n\n# 익명성 삼단논법\n\n익명성 삼단논문을 고려해 봅시다. 여기에는 다음이 포함됩니다:\n\n- 강력한 익명성\n- 낮은 지연 시간\n- 높은 대역폭\n\n<div class=\"content-ad\"></div>\n\n삼중 선택지는 우리에게 강력한 익명성을 선택한다면 낮은 지연 또는 높은 대역폭 중 하나만 선택할 수 있다는 것을 말해줍니다. 따라서 Tor 및 I2p와 같은 높은 대역폭과 낮은 지연 익명 시스템은 약한 익명성을 제공한다고 여겨지며, 이들이 강한 익명성을 갖도록 만드는 것은 지연 또는 대역폭 중 어느 하나를 희생해야만 가능하다는 것입니다.\n\n## 해독 믹스 네트워크 및 비트 단위 연결 불가능성\n\n해독 믹스 네트워크에서 믹스 노드는 암호적으로 패킷을 변환하여 하나의 암호화 층을 제거합니다. 따라서 입력 패킷은 이 변환으로 인해 출력 패킷과 완전히 다르게 보일 것입니다. 이렇게 함으로써 우리에게 비트 단위 연결 불가능성 속성을 제공한다고 말할 수 있으며, 패킷 내의 비트를 보기만으로도 패시브 네트워크 관찰자가 입력 메시지를 출력 메시지와 연결할 수 없게 만듭니다.\n\n## 믹싱 전략 및 지연\n\n<div class=\"content-ad\"></div>\n\n혼합 노드는 패킷을 암호화하는 것 외에도 자신들의 혼합 전략에 따라 지연을 추가합니다. 먼저 임계값 혼합 전략에 대해 생각해 봅시다. 이 전략은 패킷이 일정 임계값에 도달할 때까지 패킷을 축적하고, 그 후에 패킷을 섞어 다음 홉으로 전송합니다. 이 경우, 임계값을 5로 설정했지만 혼합 대기열에는 4개의 메시지만 있는 경우, 해당 메시지는 영원히 기다리거나 다섯 번째 메시지가 도착할 때까지 기다릴 것입니다. 적대자는 입력 메시지와 출력 메시지 간의 연결을 추측할 확률이 5분의 1입니다. 따라서 실제 혼합 넷에서 우리는 임계값을 1000이나 10000 또는 매우 높은 값으로 설정하여 적대자가 우리의 개인 정보 개념을 도전적으로 파괴하기 어렵게 만들어야 합니다.\n\n# 혼합 노드의 연쇄 및 가용성 해결\n\n혼합 노드의 연쇄 A - B - C - D - E가 있다고 가정해 봅시다.\n\n클라이언트가 메시지를 혼합 A로 보내고, 혼합 B로 전송하고, 그러면 계속 진행합니다. 이 설계는 우리에게 Anytrust 모델을 제공합니다. 라우팅하는 경로에 둘 이상의 혼합이 있으므로 여러 엔티티(보안 도메인으로도 알려짐)가 운영하는 것으로 가정됩니다. 그러나 이 설계는 고가용성을 제공하지 않습니다. 경로의 어떤 혼합 노드라도 실패하면 전체 네트워크가 실패하고 메시지를 라우팅할 수 없게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n학술 믹스넷 문헌에서는 이 문제를 두 가지 다른 방식으로 해결합니다:\n\n- 다중 카스케이드\n- 계층화된 토폴로지\n\n## 적절한 토폴로지 선택 방법\n\n익명 통신 네트워크용으로 많은 다른 토폴로지가 있습니다. 그러나 \"자유 경로\"라는 단점이 있는데, 이는 모든 믹스 노드가 어떤 믹스 노드와든 통신할 수 있는 것입니다. 이는 믹싱 엔트로피의 양을 줄이게 됩니다. 네트워크 토폴로지의 영향과 저대없액 미확인성에 대한 결과로, 계층화된 토폴로지나 다중 카스케이드를 사용할 때 적대자들은 최대한 불확실할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 계층화된 위상 대비 다중 카스케이드\n\n믹스넷의 사용자에게 여러 개의 믹스 노드 카스케이드를 제공하거나 믹스 노드를 계층화된 위상으로 배열할 수 있습니다. 계층화된 위상은 모든 믹스 노드가 \"라우팅 위상 계층\"으로 배열된 네트워크로, 이는 믹스 노드의 서로 다른 집합인 순서가 지정된 집합입니다. 각 계층은 고유한 믹스 노드 집합을 갖습니다. 1계층의 믹스 노드는 오직 2계층의 믹스 노드에게만 메시지를 보낼 수 있으며, 마찬가지로 2계층의 믹스 노드는 오직 3계층의 믹스 노드에게만 메시지를 보낼 수 있습니다.\n\n# 결론\n\n믹스 네트워크는 교통 분석에 효과적으로 대응하여 디지털 통신에서 강력한 익명성을 제공합니다. Tor와 달리 믹스넷은 통신 패턴을 숨기기 위해 암호 변환과 믹싱 전략을 사용하여 높은 개인 정보 보호 수준을 보장합니다. Anytrust 모델 및 계층화된 위상 또는 다중 카스케이드를 활용하여, 믹스넷은 사용자 익명성을 보존하기 위한 견고한 솔루션을 제공합니다. 메타데이터 누출 문제를 해결하지만 감시주의적 자본주의 등 보다 넓은 문제 해결은 아직 이뤄지지 않습니다. 연구가 진행됨에 따라 믹스넷은 계속 발전하며 안전하고 익명의 통신을 강화할 것입니다. 0 Knowledge Network(0KN)는 이러한 발전을 선도하며 이러한 고급 기술을 활용하여 디지털 통신의 강력한 익명성과 보안을 제공하면서 탈중앙화된 개인 정보 보호 응용 프로그램의 새로운 세대를 육성하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-18-IntroductiontoMixNetworks_0.png"},"coverImage":"/assets/img/2024-05-18-IntroductiontoMixNetworks_0.png","tag":["Tech"],"readingTime":5},{"title":"비즈니스를 보호하기 위해 3가지 주요 LLM 보안 위협 완화하기","description":"","date":"2024-05-18 21:02","slug":"2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness","content":"\n\n![Image](/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_0.png)\n\n요즘 대형 언어 모델(LLMs)은 데이터 엔지니어링의 발전으로 그 발전을 이룩했으며, 데이터 분석 및 모델 생성의 기본 개념은 오랫동안 존재해왔습니다. 그럼에도, GPT-3와 같은 LLMs의 대중적인 채택은 이 기술들을 대중과 전문가들의 주요 관심사로 끌어올렸습니다.\n\n이 통합은 스마트폰의 AI 번역 기능이나 온라인 회의 응용프로그램에서의 가상 비서와 같은 다양한 소비자 제품에서 명백하게 볼 수 있습니다.\n\nLLM 채택에 대한 토의는 종종 잠재적인 직업 저해를 중점으로 하지만, 보안 전문가들은 이런 혁신 기술들과 함께 새로운 보안 취약점의 출현을 강조하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_1.png\" />\n\n요즘 빠르게 진행되는 기술 통합은 LLM의 보안 영향에 대한 중요한 우려를 불러일으킵니다. 오늘날, AI 통합의 널리 사용으로 인해 나타나는 주요 문제 및 공격 벡터를 검토하는 것이 중요합니다.\n\n비즈니스가 LLM 보안 문제에 대해 보다 정보화되고 준비되도록 돕기 위해 저희는 Master of Code Global의 Application Security Leader인 Anhelina Biliak에게 몇 가지 통찰을 공유해 달라고 요청했습니다. 이 기사에서는 비즈니스가 대응 가능한 완화 전략과 함께 주요 LLM 취약성을 탐색할 것입니다.\n\n# #1. 민감한 데이터 노출\n\n<div class=\"content-ad\"></div>\n\nLLM(언어 모델)을 사이버 보안 워크플로에 통합하는 것은 생산성 향상을 약속하며, 루틴 작업을 자동화하고 문제 식별을 돕고, 심지어 보고서 작성을 지원합니다. 이러한 시스템 내에서 민감한 데이터의 잠재적 노출은 중요한 보안 우려를 불러일으킵니다.\n\n![이미지](/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_2.png)\n\n중요한 점은 공개 LLM(언어 모델)이 클라이언트 데이터의 직접적인 저장소로 사용되어서는 안 된다는 것을 강조하는 것이 핵심입니다. 이러한 모델은 의도치 않은 유출 가능성을 내포하고 있으며, 민감한 정보가 신중하게 작성된 프롬프트로 추출될 수 있습니다. 대신, 이러한 도구들은 연구 및 취약성 식별을 보조하는 강력한 도구로 취급하는 것이 가장 좋습니다. 프로젝트별 작업에 있어서, 그들은 지원적인 역할로 작용해야 합니다.\n\n또한, LLM(언어 모델) 통합을 갖춘 최근에 출시된 제품도 최종 사용자들의 개인 정보 보호를 고려해야 합니다. 사람들은 여괄적인 응용 프로그램에 여권 번호나 기밀 의료 세부사항과 같은 개인 식별 정보(PII)를 입력할 수 있습니다. 이러한 데이터를 책임 있게 처리하여 잠재적인 도난을 예방하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n# 민감한 데이터 노출을 완화하는 전략\n\nLLM과 상호 작용할 때 기밀 정보를 보호하는 핵심 단계는 다음과 같습니다:\n\n- 견고한 데이터 산화. AI에 사용되는 교육 데이터셋은 민감한 기록의 추적을 제거하기 위해 면밀히 살균되어야 합니다. 개인화된 취약성이나 프로젝트 특정 세부 사항을 포함하지 않도록 주의하십시오.\n- 최소 권한 원칙. 공개 LLM을 다룰 때, 필요한 최소한의 권한을 가진 가상의 사용자가 접근할 수 있는 데이터만 전송합니다. 이렇게 하면 모델의 지식이 compromise되었을 때 노출되는 정보를 최소화할 수 있습니다.\n- 제어된 외부 접속. LLM이 외부 데이터베이스에 연결하는 능력을 제한하고 데이터 흐름 전체 과정에서 엄격한 접근 제어를 유지합니다.\n- 정기적인 지식 감사. 시스템의 지식베이스를 정기적으로 감사하여 기밀 사항이 유지된 경우를 식별하고 해결합니다.\n\n더 읽어보기: LLM에서 환각: 통합 전에 알아야 할 사항\n\n<div class=\"content-ad\"></div>\n\n# #2. 프롬프트 주입 공격\n\n프롬프트 주입은 LLM(대형 언어 모델)을 이용한 앱에서 또 다른 중대한 문제로 부상하여, OWASP Top 10 등의 순위에서 상위권에 올랐습니다. 이 취약점은 공격자가 전략적으로 설계된 입력을 사용하여 LLM의 동작을 조작할 수 있게 하며, 데이터 유출, 무단 액세스, 시스템 전체의 무결성 침해에 이르기까지 다양한 위협을 초래할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_3.png)\n\n이 위협 유형을 이해하기 위해서는 프롬프트를 LLM에 제공하는 지침으로 상상해보세요. 공격자는 이러한 지침과 관련 데이터를 신중하게 설계하여 도구가 원래의 프로그래밍을 무시하거나 의도와는 다르게 행동하도록 속일 수 있습니다. 이러한 조작은 현실 세계 응용 프로그램에서 광범위한 영향을 미칠 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n최근 삼성 데이터 누출 사건은 빠른 인젝션의 피해 가능성을 보여줍니다. 회사의 ChatGPT 사용 제한은 언어 모델에 의한 민감한 정보 노출 위험을 강조합니다. 이 사례는 LLM 구성 요소로써 다양한 솔루션과 시스템에서 점점 더 보편화되는 공격에 대한 이해와 대비의 중요성을 강조합니다.\n\n# 텍스트를 넘어서: 멀티모달 프롬프트 인젝션\n\n전통적인 텍스트 기반 프롬프트 인젝션에 초점이 맞춰져 있지만, 고급 기술은 계속 발전하고 있는 위협을 제시합니다. 공격자들은 이미지, 음성, 비디오와 같은 모데일티에 대한 인젝션을 통해 기본적인 보호장치를 우회하는 데 성공했습니다:\n\n- 이미지: 이미지 처리 기능을 가진 LLM은 사진 내에 내장된 콘텐츠를 사용하여 속임수를 쓸 수 있습니다. 이는 시각 데이터로 위장된 악의적인 명령을 기존하는 창구를 엽니다.\n- 음성: 음성 입력은 최종적으로 텍스트로 변환되므로, 공격자는 음성-텍스트 변환 LLM 구성 요소를 이용해 프롬프트 인젝션을 할 수 있습니다.\n- 비디오: 진정한 비디오 기반 모델이 아직 널리 보급되지 않았지만, 비디오를 이미지와 오디오로 분해하는 것은 프롬프트 인젝션이 추후에 위험할 수 있다는 가능성을 시사합니다.\n\n<div class=\"content-ad\"></div>\n\n## 중요한 프롬프트 주입 방어 전략\n\n성장하는 위협으로부터 AI 애플리케이션을 보호하기 위한 주요 전술을 살펴보겠습니다:\n\n- 컨텍스트 인식형 프롬프트 필터링 및 응답 분석. 사용자 입력과 LLM 응답의 보다 광범위한 컨텍스트를 이해하는 지능적인 필터링 시스템을 통합하세요. 이는 미묘한 조작 시도를 감지하고 차단하는 데 도움이 됩니다.\n- LLM 패치 및 방어 세부 조정. 잠재적인 취약점을 해결하고 LLM의 프롬프트 주입 공격에 대한 저항력을 향상시키기 위해 정기적인 업데이트 및 방어 세부 조정을 진행하세요.\n- 철저한 입력 유효성 검사 및 살균 프로토콜. 사용자가 제공한 모든 프롬프트를 유효성 검사하고 살균하기 위해 엄격한 프로토콜을 수립하고 시행하세요. 이러한 조치는 악성 콘텐츠를 걸러내어 주입의 위험을 최소화하는 데 도움이 됩니다.\n- 포괄적인 LLM 상호 작용 로깅. LLM과의 모든 상호 작용을 추적하기 위한 견고한 로깅 메커니즘을 구현하세요. 이를 통해 프롬프트 주입 시도를 실시간으로 감지하고 분석 및 완화를 위한 유용한 데이터를 제공할 수 있습니다.\n\n## #3. LLM이 전통적인 웹 애플리케이션 취약점을 강화하는 방법\n\n<div class=\"content-ad\"></div>\n\nLLM을 고객을 대상으로 한 서비스에 통합하는 급발진은 새로운 위협 벡터를 만들어냅니다 - LLM 웹 공격입니다. 사이버 범죄자는 모델이 원래 데이터, API 및 사용자 정보에 액세스할 수 있는 특성을 활용하여 직접 수행할 수 없는 악의적인 조치를 취할 수 있습니다.\n\n이러한 사건은 다음과 같은 목적을 가질 수 있습니다:\n\n- 민감한 데이터 집합 추출. 프롬프트 내의 데이터, 훈련 세트 또는 액세스 가능한 API 내의 데이터를 대상으로 합니다.\n- 악의적인 API 동작을 시작. 대규모 언어 모델은 악의적인 SQL 공격과 같은 동작을 수행할 수 있는 우발적인 프록시가 될 수 있습니다.\n- 다른 사용자 또는 시스템 공격. 해커는 해당 도구를 사용하고 있는 다른 사람들에 대해 출발점으로서 LLM을 사용할 수 있습니다.\n\n개념적으로, 많은 LLM 기반 공격은 서버 측 요청 위조(SSRF) 취약점과 유사점을 공유합니다. 두 경우 모두, 공격자는 서버 측 구성 요소를 조작하여 직접 액세스할 수 없는 시스템에 대한 사건을 촉진합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_4.png\" />\n\n## LLM이 익숙한 취약점을 악용하는 방법\n\nLLM 통합은 새로운 위험 요인을 소개하는 한편, 기존의 웹 및 모바일 애플리케이션 취약점을 새로운 시각에서 새롭게 조명합니다. 사이버 범죄자들은 다음을 표적으로 할 수 있습니다:\n\n- SSRF. LLM을 통해 데이터를 HTTP 요청을 통해 가져오는 경우 취약할 수 있습니다. 공격자들은 내부 호스트를 조사하거나 클라우드 메타데이터 서비스에 접근하여 광범위한 제어를 얻을 수도 있습니다.\n- SQL Injection. 데이터베이스와 상호 작용하는 LLM은 입력 정제가 충분하지 않다면 취약할 수 있습니다. 해커들은 임의의 데이터베이스 쿼리를 실행하여 데이터를 훔치거나 수정할 수 있습니다.\n- 원격 코드 실행 (RCE). LLM이 사용자가 제공한 코드 스니펫을 수락하고 실행하는 경우, 위협 요소는 악성 코드를 주입하여 기본 시스템을 손상시킬 수 있습니다.\n- Cross-Site Scripting (XSS). 인공 지능 도구가 사용자가 입력한 정보에 따라 출력을 표시하는 웹 인터페이스가 있는 경우, XSS 공격 가능성이 있습니다. 사용자들은 세션 데이터나 기타 기밀 정보를 훔치는 악성 스크립트를 받을 수 있습니다.\n- 보안되지 않은 직접 개체 참조 (IDOR). LLM이 객체(파일 또는 데이터베이스 레코드 등)와 상호 작용할 때 사용자 입력에 따라, 공격자들은 양적인 권한 없이 객체에 액세스하거나 수정하는 IDOR 결함을 악용할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이전의 보안 문제와 유사하게, 이러한 증폭된 취약점을 완화하는 것은 견고한 입력 필터링 및 유효성 검사, 제로 트러스트 아키텍처, 최소 권한 원칙 등과 같은 동일한 전략에 의존합니다.\n\nResponsible Generative AI: Limitations, Risks, and Future Directions of Large Language Models (LLMs) Adoption를 참고하세요.\n\n## LLM 보안의 도전과 미래\n\n![이미지](/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_5.png)\n\n<div class=\"content-ad\"></div>\n\nLLM 기술은 엄청난 속도로 발전하고 있습니다. 따라서 보안 관행에 적응할 수 있도록 유동적으로 유지하는 것이 중요합니다. 위험을 선제적으로 완화하기 위해 전문가들은 LLM에 특화된 취약점을 이해하고 효과적으로 대처할 수 있는 실용적인 도구가 필요합니다.\n\n대규모 언어 모델의 미래를 안전하게 보호하기 위해 기존 프레임워크를 적응시키고, NLP를 위한 CVE와 같은 취약성 데이터베이스를 확대하며, 명확하고 벤더 중립적인 규제를 개발해야 합니다. 협력, 지속적인 연구, 그리고 선제적 사고 방식은 이러한 진화하는 도전에 대처하기 위한 중요한 열쇠입니다.\n\n마스터 오브 코드 글로벌에서는 보안부서도 문제에 대한 연구를 계속하며, 당사의 AI 프로젝트의 안전을 보장할 수 있는 다양한 접근 방식을 탐색하고 있습니다. 예를 들어, 저희는 솔루션의 침투 테스트를 수행하고 엔지니어들에게 안전한 LLM의 사용과 통합에 대해 교육하는 포괄적인 교육 프로그램을 개발하고 있습니다.\n\n또한 고객의 요구 사항에 따라 응용 프로그램의 가능한 위험 목록을 도입했습니다. 이를 통해 관리자들은 잠재적 위험을 인식하고 이에 대처하기 위한 필요 조치를 마련하여 사용자의 안전을 보장할 수 있습니다. 이는 모든 기업에게 최우선 과제입니다.\n\n<div class=\"content-ad\"></div>\n\nLLM 보안 우려가 혁신을 막지 못하도록 합시다. 우리와 협력하여 안전하고 믿을 수 있는 AI 애플리케이션을 구축하는 데 우리의 전문지식을 활용해 보세요. 귀하의 정확한 요구 사항을 논의하고 언어 모델의 전체 잠재력을 최대한 발휘하며 안전성을 최우선에 두기 위해 오늘 MOCG에 연락해 주세요.\n\nLLM 보안을 높이고 사이버 위협에 앞서 나가세요. AI 안전성을 향상시키기 위해 준비가 되셨나요? 시작하려면 저희에게 연락하세요!\n\n![이미지](/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_6.png)\n\n이 이야기는 Generative AI에서 발행됩니다. LinkedIn에서 저희와 연락하고 최신 AI 이야기에 대한 소식을 받아보려면 Zeniteq를 팔로우해 주세요. 함께 AI의 미래를 함께 만들어 봅시다!\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_7.png\" />\n","ogImage":{"url":"/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_0.png"},"coverImage":"/assets/img/2024-05-18-Mitigate3MajorLLMSecurityThreatstoProtectYourBusiness_0.png","tag":["Tech"],"readingTime":7}],"page":"36","totalPageCount":61,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}