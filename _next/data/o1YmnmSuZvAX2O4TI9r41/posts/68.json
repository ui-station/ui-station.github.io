{"pageProps":{"posts":[{"title":"도커 도컴포즈를 사용하여 PostgreSQL을 설치하는 방법","description":"","date":"2024-05-20 17:06","slug":"2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose","content":"\n\nPostgreSQL은 개발 커뮤니티에서 널리 사용되는 인기 있는 관계형 데이터베이스 관리 시스템입니다.\n\n![이미지](/assets/img/2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose_0.png)\n\nDocker Compose는 여러 컨테이너로 구성된 Docker 응용 프로그램을 정의하고 실행할 수 있는 강력한 도구입니다. 여러 컨테이너를 쉽게 관리하기 위해 구성을 단일 YAML 파일에 정의할 수 있어요.\n\nPostgreSQL은 개발 커뮤니티에서 널리 사용되는 인기 있는 관계형 데이터베이스 관리 시스템입니다. Docker Compose를 사용하면 PostgreSQL 인스턴스를 컨테이너에서 쉽게 설정하고 실행할 수 있어요. 이는 개발, 테스트 및 배포 목적으로 훌륭한 솔루션일 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n이 블로그 포스트에서는 Docker Compose를 사용하여 PostgreSQL을 설치하는 단계를 안내해 드리겠습니다.\n\n단계 1: Docker 설치\n\nDocker Compose를 설치하고 사용하기 전에 시스템에 Docker가 설치되어 있는지 확인해야 합니다. Docker는 Windows, macOS 및 Linux 운영 체제용으로 제공됩니다. 공식 Docker 웹사이트에서 적절한 버전의 Docker를 다운로드할 수 있습니다.\n\n단계 2: Docker Compose 설치\n\n<div class=\"content-ad\"></div>\n\n도커를 시스템에 설치한 후에는 공식 도커 컴포즈 설치 안내에 따라 도커 컴포즈를 설치할 수 있어요. 운영 체제에 따라 설치 단계가 조금씩 다를 수 있지만, 일반적으로 직관적입니다.\n\nStep 3: 도커 컴포즈 파일 생성하기\n\n도커 컴포즈를 사용하여 PostgreSQL 컨테이너를 생성하기 위해 구성을 도커 컴포즈 파일에 정의해야 해요. 원하는 디렉토리에 docker-compose.yml이라는 새 파일을 만들고 다음 코드를 붙여넣어 주세요:\n\n```js\nversion: '3.9'\n\nservices:\n  postgres:\n    image: postgres:14-alpine\n    ports:\n      - 5432:5432\n    volumes:\n      - ~/apps/postgres:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=S3cret\n      - POSTGRES_USER=citizix_user\n      - POSTGRES_DB=citizix_db\n```\n\n<div class=\"content-ad\"></div>\n\n위와 같은 명령어입니다:\n\n- up은 컨테이너를 구동합니다.\n- -d는 백그라운드 모드로 실행합니다.\n\n이 파일에서는 official PostgreSQL 이미지를 사용하는 db라는 단일 서비스를 정의합니다. 또한 default postgres 사용자의 암호를 지정하기 위해 POSTGRES_PASSWORD 환경 변수를 설정합니다. 마지막으로 컨테이너 내의 /var/lib/postgresql/data 디렉토리에 로컬 ./data 디렉토리를 장착하여 PostgreSQL 데이터를 저장하는 위치를 지정합니다.\n\n단계 4: PostgreSQL 컨테이너 시작\n\n<div class=\"content-ad\"></div>\n\nPostgreSQL 컨테이너를 시작하려면 터미널에서 docker-compose.yml 파일이 있는 디렉토리로 이동하고 다음 명령을 실행하세요:\n\n```js\ndocker-compose up -d\n```\n\n```js\n➜ docker-compose up -d\nCreating network \"pg_default\" with the default driver\nCreating pg_postgres_1 ... done\n```\n\n위 명령을 사용하세요.\n\n<div class=\"content-ad\"></div>\n\n- 'up' 명령어를 사용하면 컨테이너를 올릴 수 있습니다.\n- '-d' 옵션은 detached 모드로 실행합니다.\n\n이 명령어는 컨테이너를 detached 모드로 시작합니다. 이는 백그라운드에서 실행됨을 의미합니다. 컨테이너가 실행 중인지 확인하려면 다음 명령어를 실행하세요:\n\n```js\ndocker ps\n```\n\n```js\n➜ docker-compose ps\n    Name                   Command              State                    Ports\n------------------------------------------------------------------------------------------------\npg_postgres_1   docker-entrypoint.sh postgres   Up      0.0.0.0:5432-\n```\n\n<div class=\"content-ad\"></div>\n\n스텝 5: PostgreSQL 컨테이너 중지 및 제거\n\nPostgreSQL 컨테이너를 중지하고 제거하려면 터미널에서 docker-compose.yml 파일이 있는 디렉토리로 이동한 후 다음 명령을 실행하세요:\n\n```js\ndocker-compose down\n```\n\n이렇게 하면 컨테이너가 중지되고 제거됩니다.\n\n<div class=\"content-ad\"></div>\n\n도커 컴포즈를 사용하여 PostgreSQL을 설치하는 것은 컨테이너 내에서 빠르게 PostgreSQL 인스턴스를 설정할 수 있는 간단한 과정입니다. 도커 컴포즈 파일에서 컨테이너의 구성을 정의하여 여러 컨테이너를 쉽게 관리하고 확장할 수 있습니다.\n\n도커 컴포즈를 사용하여 PostgreSQL을 설치하려면 먼저 시스템에 도커와 도커 컴포즈를 설치해야 합니다. 그 후 도커 컴폏 파일을 만들고 PostgreSQL 컨테이너의 구성을 정의할 수 있습니다. 그런 다음 컨테이너를 시작하고 psql과 같은 PostgreSQL 클라이언트를 사용하여 연결할 수 있습니다.\n\n도커 컴포즈를 사용하면 PostgreSQL 컨테이너를 쉽게 관리하고 확장할 수 있으며 개발 및 테스트 환경을 설정하는 간단하고 반복 가능한 방법을 제공합니다. 전반적으로 도커 컴포즈는 복잡한 애플리케이션을 관리하고 배포하는 프로세스를 간소화할 수 있는 강력한 도구입니다.","ogImage":{"url":"/assets/img/2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose_0.png"},"coverImage":"/assets/img/2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose_0.png","tag":["Tech"],"readingTime":4},{"title":"도커 컨테이너에서 상호 TLS를 구현하는 방법","description":"","date":"2024-05-20 17:05","slug":"2024-05-20-HowtoImplementMutualTLSwithDockerContainers","content":"\n\n컨테이너와 마이크로서비스의 등장으로 인해 서비스 간의 통신이 HTTP와 같은 프로토콜을 통해 더 자주 발생할 가능성이 높아졌습니다. 그러나 서비스가 신뢰할 수 없는 네트워크를 통과하는 경우(예: 클라우드 내), 그들의 통신이 안전한지 어떻게 확인할 수 있을까요? 한 가지 방법은 상호 전송 계층 보안(mTLS)을 통해 가능합니다. 이를 통해 서비스들은 상호 인증(서비스가 자신이라고 주장하는 대로인지 어떻게 알 수 있을까요?) 및 통신 암호화를 할 수 있습니다. 그렇다면 mTLS는 어떻게 작동할까요? 한 번 자세히 알아봅시다.\n\n두 당사자 간의 상호 인증 및 암호화는 분명히 새로운 것이 아닙니다. SSH 및 IPSec(대부분의 VPN 기술을 구동하는 프로토콜의 기반이 되는)과 같은 프로토콜의 기초가 되었으며 최근에는 Istio와 Linkerd와 같은 서비스 메시 프로젝트에서도 채택되었습니다.\n\n운영용 사례에서 서비스 메시는 mTLS를 쉽게 활용할 수 있는 좋은 방법이지만, 서비스 메시를 채택하기 전에 두 개의 도커 컨테이너 간의 단순한 mTLS 구현이 어떻게 작동하는지 궁금할 수 있습니다.\n\n자세한 내용은 다음 GitHub 저장소를 참조해주세요: https://github.com/blhagadorn/mutual-tls-docker\n\n<div class=\"content-ad\"></div>\n\n# 기본 클라이언트 및 서버 설정\n\nGo를 사용하여 기본 클라이언트 및 서버를 설정해 봅시다 - GitHub 저장소의 01-client-server-basic 디렉토리로 이동하여 따라해 보세요. 기본 클라이언트 및 서버를 설정한 후에는 mTLS를 추가할 것입니다.\n\n기본 서버의 요약은 다음과 같습니다 (여기서 찾을 수 있습니다)\n\n```js\nfunc helloHandler(w http.ResponseWriter, r *http.Request) {\n  io.WriteString(w, \"Hello, world without mutual TLS!\\n\")\n}\nfunc main() {\n  http.HandleFunc(\"/hello\", helloHandler)\n  log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n```\n\n<div class=\"content-ad\"></div>\n\n기본적으로 우리는 8080포트에서 /hello 경로를 청취하고 호출되면 문자열을 반환합니다.\n\n여기 기본 클라이언트의 주요 내용입니다 (여기에서 찾을 수 있습니다):\n\n```js\n r, err := http.Get(\"http://localhost:8080/hello\")\n if err != nil {\n   log.Fatal(err)\n }\n defer r.Body.Close()\n body, err := ioutil.ReadAll(r.Body)\r\n```\n\n기본적으로, http://localhost:8080/hello로 HTTP GET 요청을 만들고 응답을 쓰기로 합니다.\n\n<div class=\"content-ad\"></div>\n\n우리가 지금까지 한 것을 빌드하고 실행해 보세요. 01-client-server-basic/ 디렉토리 안에서 모두 수행합니다.\n\n```js\n$ docker build -t basic-server -f Dockerfile.server . && docker run -it --rm --network host basic-server\n```\n\n서버를 계속 실행한 채로 유지하고, 동일한 디렉토리에서 새 창을 열어서 클라이언트를 실행하세요:\n\n```js\n$ docker build -t basic-client -f Dockerfile.client . && docker run -it --rm --network host basic-client\n> Hello, world without mutual TLS!\n```\n\n<div class=\"content-ad\"></div>\n\n성공했습니다! 이제 각각의 도커 컨테이너에서 클라이언트와 서버가 대화하고 있습니다. 도커 컨테이너 간에 공유 네트워크를 만드는 --network host 옵션의 사용에 주목해 주세요. 따라서 두 컨테이너에서 localhost가 동일합니다.\n\n선택적으로 클라이언트를 실행하는 동안 tcpdump를 사용하여 평문 전송이 확인됩니다:\n\n```js\n$ docker run -it --network host --rm dockersec/tcpdump tcpdump -i any port 8080 -c 100 -A\n> Date: Sat, 03 Feb 2024 15:05:20 GMT\n> Content-Length: 33\n> Content-Type: text/plain; charset=utf-8\n> Hello, world without mutual TLS!\n```\n\n우리는 TLS가 사용되지 않았음을 알 수 있습니다. 간단히 말해서, 텍스트를 읽을 수 있기 때문에 (암호화되었다면 가독성이 떨어졌을 것입니다).\n\n<div class=\"content-ad\"></div>\n\n# 상호 TLS 추가하기\n\n상호 TLS를 추가하려면 먼저 연결에 사용할 개인 키와 해당 인증서를 생성해야 합니다. 만약 GitHub 저장소를 따라가고 있다면 예제의 나머지 부분을 확인하기 위해 02-client-server-mtls 디렉토리로 이동해주세요.\n\n```js\nopenssl req -newkey rsa:2048 \\\n-nodes -x509 \\\n-days 3650 \\\n-keyout key.pem \\\n-out cert.pem \\\n-subj \"/C=US/ST=Montana/L=Bozeman/O=Organization/OU=Unit/CN=localhost\" \\\n-addext \"subjectAltName = DNS:localhost\"\n```\n\n여기서는 로컬호스트의 CN(공통 이름)과 SAN(대체 주체 이름)을 포함하는 대응하는 공개 키가 포함된 개인 키(key.pem)와 인증서(cert.pem)를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n참고: CN은 폐기되었으며 대부분의 최신 TLS 라이브러리에서 SAN을 포함하여 설정해야 합니다. 예를 들어 Golang의 net/http도 그렇습니다. 이 예제에서는 일부 라이브러리가 아직 CN을 설정하거나 이를 예비 수단으로 사용하는 경우를 고려하여 둘 다 설정했습니다.\n\n인증서(공개 키)와 개인 키는 여기서 몇 가지 역할을 합니다. 첫째, 개인/공개 키 조합은 세션 설정을 위해 통신을 암호화하는 데 사용됩니다. 둘째, 인증을 위해 인증서 정보가 사용되며, 해당 인증서로 보호하려는 도메인 이름은 localhost(즉, SAN)입니다.\n\n이제 클라이언트 코드(client-mtls.go 파일)를 살펴봅시다. 다음 함수는 주어진 인증서와 키로 HTTPS 클라이언트를 반환합니다:\n\n```go\nfunc getHTTPSClientFromFile() *http.Client {\n  caCert, err := ioutil.ReadFile(\"cert.pem\")\n  if err != nil {\n    log.Fatal(err)\n  }\n  caCertPool := x509.NewCertPool()\n  caCertPool.AppendCertsFromPEM(caCert)\n  cert, err := tls.LoadX509KeyPair(\"cert.pem\", \"key.pem\")\n  if err != nil {\n    log.Fatal(err)\n  } \n  client := &http.Client{\n    Transport: &http.Transport{\n      TLSClientConfig: &tls.Config{\n        RootCAs:      caCertPool,\n        Certificates: []tls.Certificate{cert},\n      },\n    },\n  }\n  return client\n}\n```\n\n<div class=\"content-ad\"></div>\n\n여기에서 몇 가지가 발생하고 있습니다. 먼저, RootCAs가 생성한 인증서 풀(하나의 인증서만 포함)로 설정되고 있습니다. 이는 클라이언트가 다른 인증 기관을 확인하는 데 사용할 루트 인증서 집합입니다. 예제에서 중간 인증서를 생성하지 않았기 때문에 이것은 별 의미가 없지만, 여러 거래에서 이것은 신뢰의 루트를 정의합니다(루트가 서명한 모든 인증서가 유효할 것입니다). 둘째로, 우리 클라이언트가 안전한 연결을 설정할 때 서버에 전달할 인증서인 cert.pem을 전달하고 있습니다. 또한, 인증서 키 쌍은 통신을 암호화하는 데 사용할 비공개 키인 key.pem을 포함하고 있습니다.\n\n이제 관련 서버 코드를 살펴보겠습니다:\n\n```js\nfunc main() {\n  http.HandleFunc(\"/hello\", helloHandler)\n  caCert, err := ioutil.ReadFile(\"cert.pem\")\n  if err != nil {\n    log.Fatal(err)\n  }\n \n  caCertPool := x509.NewCertPool()\n  caCertPool.AppendCertsFromPEM(caCert)\n  tlsConfig := &tls.Config{\n    ClientCAs:  caCertPool,\n    ClientAuth: tls.RequireAndVerifyClientCert,\n  }\n  tlsConfig.BuildNameToCertificate()\n  server := &http.Server{\n    Addr:      \":8443\",\n    TLSConfig: tlsConfig,\n  }\n  log.Fatal(server.ListenAndServeTLS(\"cert.pem\", \"key.pem\"))\n}\n```\n\n서버 구성은 클라이언트 구성과 매우 유사합니다(이는 상호 인증이기 때문에 이해되는 바입니다). 루트 인증기관이 비슷하게 정의되어 있고, TLS 구성이 설정되며, 최종적으로 서버가 인증서와 인증서 키 쌍을 사용하여 수신 대기를 시작합니다. 클라이언트와 마찬가지로 서버도 연결하려는 모든 이해 관계자에게 자신의 인증서를 전달합니다(클라이언트가 인증서의 공개 키에 따라 통신을 암호화할 수 있도록 하는 TLS 핸드셰이크의 일부로), 그리고 또한 이 키는 메시지를 암호화하고 인증서의 공개 키의 소유권을 확인하는 데 사용됩니다.\n\n<div class=\"content-ad\"></div>\n\n이제 02-client-server-mtls 디렉토리 안에서 예제를 실행해 보겠습니다.\n\n먼저 서버를 실행하겠습니다:\n\n```js\n$ docker build -t mtls-server -f Dockerfile.server . && docker run -it --rm --network host mtls-server\n```\n\n이후 클라이언트를 실행해주세요:\n\n<div class=\"content-ad\"></div>\n\n```js\n$ docker build -t mtls-client -f Dockerfile.client . && docker run -it --rm --network host mtls-client\n> 안녕하세요, 상호 TLS로 세계여!\r\n```\n\n다시 성공했어요!\n\n우리는 다시 tcpdump로 확인할 수 있습니다. 평문이 없고 컨테이너 간 통신이 암호화되어 있는지 확인할 수 있어요.\n\n```js\n$ docker run -it --network host --rm dockersec/tcpdump tcpdump -i any port 8443 -c 100 -A\n>..V.(.@.................................. .&............0.........\nO.f........\n```\n\n<div class=\"content-ad\"></div>\n\n결과가 완전히 알아볼 수 없고 암호화를 사용하고 있어서 읽을 수 없는 것을 주목해주세요.\n\n# 빠른 다이어그램\n\n아래 다이어그램을 참조하시면 방금 수행한 mTLS 상호 작용에 대해 시각적으로 설명한 것을 보실 수 있습니다.\n\n![mTLS 상호 작용 다이어그램](/assets/img/2024-05-20-HowtoImplementMutualTLSwithDockerContainers_0.png)\n\n<div class=\"content-ad\"></div>\n\n# 마무리\n\n지금까지 상호 TLS 없이 한 클라이언트-서버 상호 작용과 상호 TLS가 포함된 다른 클라이언트-서버 상호 작용을 성공적으로 만들었습니다. 우리는 TLS를 추가하기 위해 로컬호스트를 SAN으로 사용한 키와 인증서를 생성했습니다. 이후에 우리는 클라이언트 코드를 편집하여 루트 CA에 대한 TLS 구성을 포함시키고 통신을 암호화할 인증서와 개인 키를 지정했습니다. 서버 코드도 마찬가지로 루트 CA를 지정하고, 서버가 청취해야 할 인증서와 키를 지정했습니다.\n\n이제 mTLS를 마이크로서비스 간에 서비스 메시를 통해 고려할 수 있는 기초가 생겼기를 바랍니다. 저희의 예에서는 한 번 인증서와 키를 생성하고 수동으로 구성에 입력했지만, 서비스 메시는 종종 짧은 갱신 주기로 자동으로 해당 인증서를 회전하고, 일반 통신을 측면 프록시를 통해 라우팅하며 — 그런 다음 일반 통신을 mTLS로 업그레이드하고 목적지에 도달하면 복호화합니다. 본질적으로 mTLS는 보이지 않고, 이것이 강력한 프록시 구성과 제어 계층의 마법입니다.\n\n이 글을 통해 컨테이너화된 작업을 보호하는 방법에 대해 조금이나마 배우셨기를 바랍니다. 그리고 언제나 — 더 많은 글을 보기 위해 저를 Medium에서 팔로우하거나 Twitter에서 확인해 보세요.","ogImage":{"url":"/assets/img/2024-05-20-HowtoImplementMutualTLSwithDockerContainers_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoImplementMutualTLSwithDockerContainers_0.png","tag":["Tech"],"readingTime":7},{"title":"도커 다이어트 보안 및 속도를 위한 데비안 이미지","description":"","date":"2024-05-20 17:03","slug":"2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed","content":"\n\n<img src=\"/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_0.png\" />\n\n컨테이너화된 세상에서 이미지 크기가 중요합니다. 작은 이미지는 더 빠른 배포, 줄어든 저장 요구 사항 및 향상된 보안으로 이어집니다. Google의 Distroless Docker 이미지가 등장하는 곳입니다.\n\nDistroless 이미지는 극단적으로 최소한한 이미지입니다. 패키지 관리자, 셸 및 불필요한 유틸리티를 포함한 무겁고 복잡한 운영 체제 계층을 제거합니다. 대신, 이들은 당신의 애플리케이션이 실행하기 위해 필요한 것에만 집중합니다: 애플리케이션 자체와 필수 런타임 종속성입니다.\n\n# 왜 Distroless를 사용해야 할까요?\n\n<div class=\"content-ad\"></div>\n\nDistroless 이미지를 사용하는 이점은 매우 많습니다:\n\n- 크기가 작음: Distroless 이미지는 극히 작아서 종종 몇 메가바이트에 불과합니다. 이는 배포가 빨라지고 컨테이너 레지스트리 및 클러스터의 저장 공간을 줄이는 것을 의미합니다.\n- 향상된 보안: 불필요한 구성 요소를 제거함으로써 Distroless 이미지는 잠재적인 취약점을 위한 공격 표면을 줄입니다. 관리해야 할 패키지가 적기 때문에 보안 패치 작업도 매우 간단해집니다.\n- 성능 향상: 작은 이미지는 컨테이너의 시작 시간이 빨라지도록 돕습니다.\n\n# Distroless로 시작하기\n\nDistroless는 다양한 언어 및 런타임을 지원하는 기본 이미지를 제공합니다. 다음은 이를 사용하는 빠른 가이드입니다:\n\n<div class=\"content-ad\"></div>\n\n- 이미지 선택: 사용 가능한 이미지를 탐색하려면 GitHub의 Distroless 프로젝트(https://github.com/GoogleContainerTools/distroless)로 이동하세요. 인기 있는 옵션으로는 일반 목적의 애플리케이션용 gcr.io/distroless/base 및 언어별 이미지인 gcr.io/distroless/java17이 있습니다.\n- Dockerfile 빌드: Dockerfile에서 선택한 Distroless 이미지를 기본 레이어로 사용하세요.\n- 애플리케이션 복사: COPY 명령을 사용하여 애플리케이션 코드와 필요한 종속성을 이미지로 복사하세요.\n- 엔트리포인트 정의: Distroless 이미지에는 셸이 없으므로 ENTRYPOINT 명령을 사용하여 애플리케이션의 엔트리포인트를 정의해야 합니다. 이는 벡터 형태로 지정해야 합니다. (예: [\"/app/my_app\"])\n\n# JAVA를 위한 Distroless 이미지 사용\n\nDistroless 이미지 사용의 장점을 설명하기 위해 Java 애플리케이션 개발 프로세스에 이를 원활하게 통합하는 방법을 탐색해볼까요?\n\n애플리케이션 이미지를 빌드하는 데 사용된 샘플 Dockerfile입니다.\n\n<div class=\"content-ad\"></div>\n\n```docker\nFROM gcr.io/distroless/java17:nonroot\nEXPOSE 8080\nCOPY target/*.jar app.jar\nCMD [\"app.jar\"]\n```\n\n취약점 수와 이미지 크기를 비교한 분석 결과, 상당한 개선이 있었습니다. 이러한 향상을 보여주는 자세한 통계는 다음과 같습니다.\n\n이전에 사용한 openjdk 베이스 이미지\n\n이전 취약점 통계: 총 96개 | 중요: 0 | 높음: 23 | 중간: 73\n\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_1.png)\n\nJava distroless 이미지 사용 중\n\n현재 취약점 통계: 총 1 | 심각: 0 | 높음: 0 | 중간: 1\n\n![이미지](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n베이스 Java 이미지 크기: 226MB [이미지 크기의 50% 감소]\n\n# NGINX에 Distroless 이미지 사용하기\n\n미리 빌드된 Nginx Distroless 이미지가 즉시 사용 가능하지 않기 때문에, 베이스 Distroless 이미지를 사용하여 커스텀 이미지를 만드는 방법을 살펴보겠습니다.\n\n애플리케이션 이미지를 빌드하는 데 사용된 샘플 Dockerfile입니다.\n\n<div class=\"content-ad\"></div>\n\n```docker\n# Nginx를 빌드 이미지로 사용\nFROM nginx:1.25 as build\n\n# 메인 이미지로 distroless nonroot 사용\nFROM gcr.io/distroless/base-debian11:nonroot\nCOPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt\nCOPY --from=build /etc/passwd /etc/passwd\nCOPY --from=build /etc/group /etc/group\nUSER nginx\n\n# 빌드 이미지로부터 필요한 Nginx 라이브러리 복사\nCOPY --from=build /lib/x86_64-linux-gnu/libdl.so.2 /lib/x86_64-linux-gnu/libdl.so.2\nCOPY --from=build /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libc.so.6\nCOPY --from=build /lib/x86_64-linux-gnu/libz.so.1 /lib/x86_64-linux-gnu/libz.so.1\nCOPY --from=build /lib/x86_64-linux-gnu/libcrypt.so.1 /lib/x86_64-linux-gnu/libcrypt.so.1\nCOPY --from=build /lib/x86_64-linux-gnu/libpthread.so.0 /lib/x86_64-linux-gnu/libpthread.so.0\nCOPY --from=build /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2\nCOPY --from=build /usr/lib/x86_64-linux-gnu/libssl.so.3 /usr/lib/x86_64-linux-gnu/libssl.so.3\nCOPY --from=build /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0\nCOPY --from=build /usr/lib/x86_64-linux-gnu/libcrypto.so.3 /usr/lib/x86_64-linux-gnu/libcrypto.so.3\n\n# 빌드 이미지로부터 필요한 이진 파일 및 디렉토리 복사\nCOPY --from=build /usr/sbin/nginx /usr/sbin/nginx\nCOPY --from=build /etc/nginx /etc/nginx\nCOPY --from=build --chown=nginx:nginx /var/cache/nginx /var/cache/nginx\nCOPY --from=build --chown=nginx:nginx /var/log/nginx /var/log/nginx\nCOPY --from=build --chown=nginx:nginx /usr/share/nginx/html /usr/share/nginx/html\n\nCOPY nginx.conf /etc/nginx/nginx.conf\nCOPY default.conf /etc/nginx/conf.d/\n\nENTRYPOINT [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"]\n```\n\nNginx 베이스 이미지의 취약점 카운트와 이미지 크기를 비교 분석하였습니다.\n\n이전에 사용한 nginx 알파인 베이스 이미지\n\n이전 취약성 통계 \"Total: 32 | Critical: 1 | High: 13 | Medium: 18\"\n\n\n<div class=\"content-ad\"></div>\n\n\n![Dockerona Diet Distroless Images for Security Speed 3](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_3.png)\n\nUsing base distroless image crafted with nginx\n\nCurrent Vulnerability Stats: Total: 0 | Critical: 0 | High: 0 | Medium: 0\n\n![Dockerona Diet Distroless Images for Security Speed 4](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_4.png)\n\n\n<div class=\"content-ad\"></div>\n\n기본 Nginx 이미지의 크기: 28MB [이미지 크기 30% 감소]\n\n# 주의 사항\n\nDistroless는 큰 장점을 제공하지만 몇 가지 주의할 점이 있습니다:\n\n- 쉘 액세스 없음: Distroless 이미지에는 디자인상 셸이 없습니다. 컨테이너 내에서 디버깅을 하려면 추가 도구나 멀티 스테이지 빌드가 필요할 수 있습니다.\n- 기능 제한: 패키지 관리자가 없기 때문에 애플리케이션이 필요로 하는 추가 라이브러리, 유틸리티 또는 인증서를 명시적으로 포함해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# Distroless 또는 Alpine...어느 쪽을 선택해야 할까요?\n\nDistroless와 Alpine 이미지의 장단점을 이해하여, 귀하의 애플리케이션 요구사항과 개발 흐름에 가장 적합한 결정을 내릴 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_5.png)\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\nDistroless 이미지는 안전하고 효율적인 도커 컨테이너를 만드는 매력적인 옵션입니다. 미니멀리즘을 포용함으로써 더 작은 공격 표면, 빠른 배포 및 향상된 성능을 얻을 수 있습니다. 다음 번에 도커 이미지를 만들 때는 여분의 부담을 줄이고 Distroless 기본 이미지를 선택하는 것을 고려해보세요.","ogImage":{"url":"/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_0.png"},"coverImage":"/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_0.png","tag":["Tech"],"readingTime":6},{"title":"스파크와 데이터브릭스에서 파티셔닝으로 성능 향상을 달성하세요 파트 13","description":"","date":"2024-05-20 17:02","slug":"2024-05-20-SuperchargingPerformancewithPartitioninginDatabricksandSparkPart13","content":"\n\n## 데이터 엔지니어가 파티셔닝을 이해해야 하는 이유!\n\nSpark 및 Databricks의 가장 중요한 기능 중 하나를 다루는 세 개의 기사 중 첫 번째 기사입니다: Partitioning에 대해 얘기할 것입니다.\n\n- 이 첫 장은 파티셔닝의 일반 이론과 Spark에서의 파티셔닝에 초점을 맞출 것입니다.\n- 제2부에서는 테이블 파티셔닝에 대한 구체적인 내용을 다루고 데이터셋을 준비할 것입니다.\n- 제3부에서는 철저한 사례 연구를 다루고 성능 비교를 수행할 것입니다.\n\n# 소개\n\n<div class=\"content-ad\"></div>\n\n데이터 관련해서 말씀드리면, \"빅데이터\" 개념은 종종 3V(또는 소스에 따라 가끔 더)로 정의됩니다: 양(volume), 다양성(variety), 그리고 속도(velocity).\n\n- 양은 데이터셋의 크기에 관련되며, 이는 테라바이트 이상까지 될 수 있습니다.\n- 다양성은 다양한 데이터 유형과 소스를 처리하는 과제를 의미하며, 효과적으로 종합되어야 합니다.\n- 반면에 속도는 데이터를 받아들이는 속도에 관한 것이며, 매초 100MB 파일이 수신된다면 금방 방대한 양의 데이터로 축적될 수 있습니다.\n\n\"빅데이터\"를 처리하는 높은 성능의 솔루션을 개발하기 위해서는 다양한 기법과 패턴을 사용할 수 있습니다. Partitioning이라는 한 가지 기법이 있습니다. 대용량 데이터셋을 다룰 때 개별 기계의 처리 능력이 한계에 도달할 수 있어, Databricks와 Spark 같은 도구를 통해 제공되는 분산 및 병렬 처리 기능을 사용해야 할 때가 있습니다.\n\nPartitioning은 이것이 가능하게 하는 핵심입니다. 대용량 데이터셋을 파티션이라는 더 작고 관리하기 쉬운 조각으로 나누는 것을 포함합니다.\n\n<div class=\"content-ad\"></div>\n\n딥레닉과 같은 분산 데이터 처리 시스템에서는 파티션을 사용하여 데이터를 여러 노드에 분산시켜 병렬 처리와 뛰어난 성능을 확보합니다.\n\n데이터를 파티션으로 나누면 각각을 독립적으로 처리할 수 있어 처리 시간을 단축하고 확장성을 향상시킬 수 있습니다.\n\n파티션화는 작업 부하를 균형있게 분산시키고 데이터 이동을 줄이며 데이터의 불균형이나 불균형의 영향을 최소화하는 데도 도움이 됩니다.\n\n반면에 비효율적인 파티셔닝은 성능 병목 현상, 자원 낭비 및 처리 시간 증가로 이어질 수 있습니다. 따라서 딥레닉과 같은 분산 데이터 처리 시스템에서 성능을 최적화하기 위해 올바른 파티셔닝 전략을 선택하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n# Databricks와 Spark에서 파티셔닝 이해하기\n\n먼저, DataFrame / RDD 수준에서의 파티셔닝과 테이블 수준에서의 파티셔닝을 구분해야 합니다.\n\n## RDDs와 DataFrames\n\nDatabricks의 계산 엔진은 Spark입니다. Databricks에서 SQL, Scala, R 또는 Python을 사용하여 코드를 작성할 때, 우리는 단지 해당 API를 사용하여 내부 Spark 엔진에 액세스합니다.\n\n<div class=\"content-ad\"></div>\n\n스파크는 기본 데이터 구조로 RDDs(탄력적 분산 데이터 집합)를 사용합니다. 그들의 이름에서 알 수 있듯이, RDDs는 분산 데이터 집합입니다. 요즘에는 RDDs와 직접 작업하는 것보다는 DataFrame API와 함께 작업하는 것이 인기가 없지만, DataFrame API는 이름을 붙일 수 있는 열을 제공하는 RDDs의 상위 수준 추상화인 DataFrame 객체를 제공합니다. 따라서 DataFrame은 전통적인 관계형 데이터베이스의 테이블과 유사합니다. 게다가, DataFrames는 많은 성능 최적화 옵션을 제공합니다.\n\n우리의 데이터를 이러한 DataFrame으로 로드할 때, 이미 파티션되어 있습니다. 이를 아래의 스크린샷에서 볼 수 있습니다. 만약 DataFrame의 기본 RDD에 액세스하면, .rdd.getNumPartitions()를 사용하여 파티션 수를 얻을 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-SuperchargingPerformancewithPartitioninginDatabricksandSparkPart13_0.png)\n\nRDDs 또는 DataFrames로 작업할 때, 스파크는 파티션 프로세스를 자동으로 처리해 줍니다. 심지어 우리가 명시적으로 지정하지 않았더라도요. 이미 파티션된 테이블에서 데이터를 가져올 때, 기존 파티션 구조가 고려된다는 점을 언급하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n화면 캡처에서 보듯이 나는 파티션이 없는 테이블에서 데이터를 로드하고 있습니다. 이는 Spark가 아래에서 다룰 여러 구성에 기반하여 파티셔닝을 처리할 것을 의미합니다.\n\n또한 파티션과 파일을 구분해야 합니다. 왜냐하면 이 둘은 같은 것이 아니기 때문입니다. 파티션은 분산 컴퓨팅 환경에서 데이터의 논리적 분할을 의미합니다. 반면 파일은 실제로 디스크에 저장된 데이터의 저장 단위입니다.\n\n데이터를 섞는 일부 작업을 수행할 때, Spark는 \"spark.sql.shuffle.partitions\" 구성을 기반으로 데이터를 파티션으로 분할합니다.\n\n<div class=\"content-ad\"></div>\n\n그러나 이 같은 속성은 데이터를 간단히 읽을 때 파티션 수도 제어합니다. 디폴트로 200으로 설정되어 있어 Spark는 200개의 파티션을 생성합니다. 그러나 \"spark.sql.files.maxPartitionBytes\" 구성 설정도 이 프로세스에 한도를 두고 있습니다.\n\n각 파티션의 실제 저장 크기는 사용 가능한 메모리와 데이터셋의 크기와 같은 다양한 요인에 따라 달라집니다. 그러나 Databricks는 \"spark.sql.files.maxPartitionBytes\" 구성 속성에 의해 정의된 최대 크기로 파티션을 생성합니다. 기본값으로 이 값은 128MB로 설정되어 있습니다.\n\n이미지 링크:\n![이미지](/assets/img/2024-05-20-SuperchargingPerformancewithPartitioninginDatabricksandSparkPart13_2.png)\n\n데이터프레임 파티션의 크기를 제어하는 다른 방법은 .repartition() 또는 .coalesce() 메서드를 사용하여 DataFrame을 다시 파티션하고 파티션 수 또는 파티션화하려는 열을 지정하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- repartition()은 DataFrame 또는 RDD의 파티션 수를 증가 또는 감소시키는 데 사용됩니다. .repartition()을 사용하면 Spark가 데이터를 섞고 지정된 파티션 수에 기반하여 새로운 파티션을 생성합니다.\n- coalesce()는 DataFrame 또는 RDD의 파티션 수를 감소시키는 데 사용됩니다. .coalesce()를 사용하면 Spark가 기존 파티션을 결합하여 새로운 파티션을 생성하려고 합니다. repartition()과 달리, coalesce()는 데이터를 섞지 않습니다.\n\n# 요약\n\n파티셔닝은 성능이 우수한 \"빅 데이터\" 솔루션을 구축하는 데 필수적인 기술이며, 적절한 파티셔닝 전략을 선택하는 것은 좋은 성능과 확장성을 달성하는 데 중요합니다. 그러나 최적의 파티셔닝 설정을 조사할 리소스가 없는 경우, Databricks의 기본 최적화 설정과 옵션을 사용하는 것이 좋습니다.\n\n모든 테이블에 대한 보편적인 해결책은 없으며, 우선 순위와 목표를 균형 있게 유지해야 합니다. 각 시나리오는 요구 사항과 도전에 대응하기 위한 맞춤형 접근 방식이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\nDataFrame / RDD 수준의 파티셔닝은 데이터를 병렬 처리를 위해 클러스터 노드에 분산하는 것을 다루고, 테이블 수준의 파티셔닝은 저장 시스템 내에서 데이터를 조직화하여 쿼리 성능을 최적화하는 데 초점을 맞춥니다.\n\n이제 파티셔닝의 중요성과 RDD 및 DataFrame이 어떻게 파티셔닝되는지 이해했으니, 다음 글에서는 Databricks의 테이블이 어떻게 파티션되는지 설명하고 최종 챕터를 위해 데이터를 준비할 것입니다.\n\n읽어주셔서 감사합니다!","ogImage":{"url":"/assets/img/2024-05-20-SuperchargingPerformancewithPartitioninginDatabricksandSparkPart13_0.png"},"coverImage":"/assets/img/2024-05-20-SuperchargingPerformancewithPartitioninginDatabricksandSparkPart13_0.png","tag":["Tech"],"readingTime":5},{"title":"Azure Data Factory에서 Databricks 작업 클러스터에 동적 태그 적용하기","description":"","date":"2024-05-20 17:01","slug":"2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory","content":"\n\n아래는 Markdown 형식으로 변경한 텍스트입니다.\n\n\n![이미지](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_0.png)\n\n최근 몇 명의 고객이 Databricks 노트북 액티비티를 Azure Data Factory에서 실행할 때, 노트북을 실행할 작업 클러스터에 동적으로 태그를 설정하는 방법에 대해 물어봤어요.\n\n다행히도 Azure Data Factory의 매개변수와 동적 콘텐츠 기능 덕분에 해결책을 상당히 쉽게 찾을 수 있어요.\n\n# Prerequisites\n\n\n<div class=\"content-ad\"></div>\n\n아래는 Azure Databricks 워크스페이스에 액세스 권한, Azure Data Factory 인스턴스, 그리고 워크스페이스에서 작업을 만들고 실행할 수 있는 능력이 있다고 가정합니다.\n\n# 링크드 서비스를 매개변수화하기\n\n- Azure Data Factory에서 새로운 Azure Databricks 링크드 서비스를 만듭니다.\n- 편집기의 매개변수 섹션에 동적으로 지정하고자 하는 각 태그 값에 대해 매개변수를 추가하세요. 이 예시에서는 CustomTag라는 매개변수를 추가하고 기본값을 설정했습니다.\n\n![이미지](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_1.png)\n\n<div class=\"content-ad\"></div>\n\n3. 클러스터 사용자 정의 태그 섹션에서 태그 이름을 추가하고 값 필드를 선택하세요. 텍스트 상자 아래에 동적 콘텐츠를 추가할 링크가 나타납니다. 이를 클릭해주세요:\n\n![Cluster custom tags](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_2.png)\n\n4. 'Pipeline Expression Builder' 패널이 열립니다. 하단에는 이전에 생성한 'ClusterTag' 매개변수가 파라미터 목록에 있을 겁니다. 이 매개변수를 클릭하여 파이프라인 표현식에 삽입하세요. 확인을 클릭해주세요.\n\n![ClusterTag parameter](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_3.png)\n\n<div class=\"content-ad\"></div>\n\n5. 클러스터의 나머지를 보통대로 구성하세요.\n\n최종 연결된 서비스 구성은 다음과 같아야 합니다.\n\n![Linked Service Configuration](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_4.png)\n\n# 파라미터에 동적 값을 바인딩하세요\n\n<div class=\"content-ad\"></div>\n\n이제 우리가 연결된 서비스에서 태그 값에 대한 매개변수를 갖고 있으므로 해당 연결된 서비스를 사용하는 각 활동에 대해 값을 바인딩할 수 있습니다.\n\n- 새로운 Databricks 노트북 활동을 만듭니다.\n- 활동 구성 패널에서 Azure Databricks 탭을 참고하면 Linked Service 속성에 우리가 만든 매개변수가 포함되어 있음을 알 수 있습니다:\n\n![Linked Service Properties](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_5.png)\n\n3. 여기에는 세 가지 옵션이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 이 속성값을 비워두면 클러스터는 태그의 값에 이전에 설정한 기본값을 사용합니다.\n- 이곳에 정적 값을 지정할 수 있으며, 이 값은 태그의 값으로 매개변수를 통해 전달됩니다.\n- 다시 동적 콘텐츠 옵션을 사용하여 이 매개변수에 대한 표현식을 제공할 수 있습니다. 전체 ADF 파이프라인으로 전달되는 매개변수, 파이프라인 자체의 이름, 파이프라인이 트리거된 날짜 또는 다른 활동의 출력값일 수 있습니다.\n\n이 튜토리얼에서는 여기에 정적 값만 할당하겠습니다:\n\n![image](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_6.png)\n\n참고로 만일 우리 파이프라인에 여러 활동이 있다면, 각 활동에 대해 이 속성에 고유한 값을 할당할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n수동으로 이 파이프라인을 실행한 후에는 Azure Databricks의 작업 클러스터로 사용자 정의 동적 태그 값이 전파된 것을 볼 수 있습니다:\n\n![동적 태그 적용 예시](/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_7.png)\n\n이 태그는 클러스터에서 사용되는 Azure VM에도 전파되어, Azure 포털의 Azure 비용 관리 도구에서 청구 및 감사 목적으로 사용할 수 있습니다.\n\n이제 Databricks 시스템 테이블에서 이 태그를 조회하여 클러스터와 관련 청구 사용 내역을 찾을 수도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식의 테이블입니다.\n\n\n| 파일 이름 | 경로 |\n|---|---|\n| 2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_8.png | /assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_8.png |\n| 2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_9.png | /assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_9.png |\n\n\n이 내용이 Databricks에서의 컴퓨팅 및 파이프라인의 가시성 관리에 도움이 되기를 바랍니다. 피드백은 언제나 환영합니다!","ogImage":{"url":"/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_0.png"},"coverImage":"/assets/img/2024-05-20-ApplyingDynamicTagsToDatabricksJobClustersinAzureDataFactory_0.png","tag":["Tech"],"readingTime":4},{"title":"모든 데이터 엔지니어를 위한 필수 PySpark 차트 시트","description":"","date":"2024-05-20 16:59","slug":"2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers","content":"\n\n## 모든 Pyspark 스크립트 작성을 수정하고, 시간을 절약한 상태로 현명하게 연습하세요.\n\n\"일을 맹목적으로 끝내는 것보다는 최적의 효율성과 효과적인 방법으로 실행하는 것이 중요합니다.\"\n\n여기 제공된 치트 시트는 PySpark의 주요 측면을 신속하게 검토하고, 인터뷰 준비 또는 Databricks나 Python 기반 코딩 환경과 같은 다양한 플랫폼에서 데이터 분석 작업에 대비할 수 있게 돕습니다.\n\n이 도구를 이용하면 PySpark 및 관련 프레임워크에 필수적인 중요한 변환 기술 및 데이터 분석 방법론을 신속하게 검토할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 시작해봅시다\n\nPySpark를 사용하여 변환 작업이나 데이터 분석 작업을 시작하기 전에 Spark 세션을 설정하는 것이 중요합니다. 이 초기 단계는 Spark 프레임워크 내에서 코드 실행의 기반을 제공합니다. 더 중요한 것은 후속 작업을 위한 기반을 구축하고 Spark 기능과 원활하게 상호 작용할 수 있도록 합니다.\n\n먼저 의도한 작업에 필요한 모듈을 가져와야 하며, 모든 중요한 구성 요소가 사용 가능하도록 보장해야 합니다. 이 기본 절차를 준수함으로써 개발자와 데이터 전문가는 PySpark의 강력한 기능을 활용하여 데이터 처리와 분석 작업을 원활하게 수행할 수 있습니다.\n\n## 시작해봅시다!!\n\n<div class=\"content-ad\"></div>\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import types\nfrom pyspark.sql.types import SparkType, StructField, StringType, IntegerType, DataType\nfrom pyspark.sql.functions import col, date, year, time, sum, avg, upper, count, Broadcast, expr\nfrom pyspark.sql import window\nfrom pyspark.sql import functions as F\n\n\n\nspark = SparkSession.builder.appName(\"application\").getOrCreate()\n# read any file as given either csv, excel, parquet, or Avro any format of data\ndata = spark.read.csv(\"filePath\", header=True, inferschema=True) # if we want given data types as it is\nschema = StructType([StructField(\"id\", IntegerType), StructField(\"name\", StringType),\nStructField(\"dept\", StringType)]) # if we want our required data types then we use this \n# also for better performance of executions we will be using our custom schema rather depending on inferschema\n\n\nLet’s kickstart our PySpark application by first creating a Spark Session, the entry point to PySpark functionality.\n\nWe’ll then proceed with performing various transformations and analyses on sample data.\n\n\n<div class=\"content-ad\"></div>\n\n```js\n데이터 = [(1, 'mahi', 100), (2, 'mahendra', 200), (3, 'harish', 300), (4, 'desh', 400)]\n\n스키마 = ['id', 'name', 'salary']\n# 데이터 프레임 생성\ndf = spark.createDataFrame(data, schema)\ndf.head()\ndf.show()\ndisplay(df)\n```\n\n![image](/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_0.png)\n\nPySpark에서 윈도우 함수를 사용하여 급여의 누적 합을 계산할 수 있습니다.\n\n```js\na = Window().orderBy('id')\n누적_합 = df.withColumn(\"cumulative_sum\", sum(\"salary\").over(a))\n결과 = cumulative_sum.orderBy('id')\n결과.show()\n```\n\n<div class=\"content-ad\"></div>\n\n```python\na = Window().orderBy('id')\n누적합 = df.withColumn(\"누적합\", avg(\"salary\").over(a))\n결과 = 누적합.orderBy('id')\n결과.show()\n```\n\n![이미지](/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_1.png)\n\n```python\nemp=[(1,'마히', 100,1),(2,'마헨드라', 200,2),(3,'하리쉬',300,3),\n(4,'데시',400,4)]\n\n스키마=['id', 'name', 'salary', 'dept_id']\n# 데이터 프레임 생성\ndf=spark.createDataFrame(data,schema)\ndf.head()\ndf.show()\ndisplay(df)\n\ndept=[(1,'인사'),(2,'영업'),(3,'데이터 분석'),(4,'IT')]\n스키마=['dept_id', 'department']\n부서=spark.createDataFrame(dept,schema)\ndisplay(부서)\n```\n\n![이미지](/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_2.png)\n\n\n<div class=\"content-ad\"></div>\n\n공통 속성인 dept_id를 사용하여 두 개의 데이터 프레임을 결합해 봅시다. 예시를 들어보겠습니다:\n\n```js\ndf=employee.join(department, \"dept_id\", \"inner\").select('id','name','salary','department')\ndisplay(df)\n```\n\n```js\ndf=employee.join(department, \"dept_id\", \"right\").select('name','department')\ndisplay(df)\n```\n\n<img src=\"/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n위에서는 다양한 PySpark 변환, 액션 및 기능에 대해 상세히 설명하겠습니다. 실행 가능한 코드 예제와 함께 다룰 예정이에요.\n\n기본적인 변환인 필터링 및 그룹화부터 창 함수와 같은 고급 기술까지 다양한 작업을 다룰 거에요.\n\n각 코드 조각은 개념을 포괄적으로 이해할 수 있도록 설명하는 문장과 함께 제공됩니다.\n\n데이터 조작 작업부터 시작하여 PySpark 데이터 프레임을 사용하여 데이터 필터링, 선택 및 집계를 살펴보겠습니다. 그 후에는 조인, 정렬 및 창 함수와 같은 변환을 통해 다수의 테이블에서 데이터를 조작하고 분석하는 방법을 살펴볼 것입니다.\n\n<div class=\"content-ad\"></div>\n\n또한, 우리는 샘플 데이터셋을 활용하여 모델 학습과 평가를 통해 머신러닝 작업을 지원하는 Pyspark을 소개할 것입니다.\n\n이 치트 시트를 통해 각 코드 스니펫은 해당 개념의 실제 시연을 제공하여 빠른 참고와 이해를 돕게 됩니다.\n\n이 예제를 따라가며 사용자들은 Pyspark 능력을 향상시키고 데이터 엔지니어링 및 데이터 과학 인터뷰나 실제 데이터 처리 작업에 대비할 수 있습니다.\n\n## 필터링, 선택, 집계, 그룹화 및 정렬 조건:\n\n<div class=\"content-ad\"></div>\n\n```js\ndf = orders.join(products, \"order_id\", \"inner\") # 어떤 조인 적용\ndf.join(df2, '공통 열').groupBy('표시할 열').count().orderBy(desc('count'))\n\n\ndf1=df.groupBy(\"cust_id\").agg(sum(\"amount\").alias(\"bill\")) # 그룹화 함수 적용 및 집계 조건을 지정\n\ndf.groupBy(\"col1\").agg(count(\"col2\").alias(\"count\"), \n                          sum(\"col2\").alias(\"sum\"),\n                          max(\"col2\").alias(\"maximum\"),\n                          min(\"col2\").alias(\"minimum\"), \n                          avg(\"col2\").alias(\"average\")).show()\n\n\ndf.drop(\"column_name1\", \"column_name2\", \"column_name3\") # 열 삭제\ndf.drop(col(\"column_name\")) # 다른 열 삭제 방법\n\ndf.createOrReplaceTempView(\"원하는 이름 지정\") # 데이터 프레임을 테이블로 변환\n\ndf.orderBy(F.desc(\"column_name\")).first() # 특정 열(예: 급여)의 내림차순 첫 번째 행 반환\ndf.orderBy(col(\"column_name\").desc()).first() # 가장 높은 값을 반환하는 또 다른 방법\ndf.orderBy(col(\"column_name\").desc()).limit(5) # 상위 5개의 값을 반환\n\n# 원하는 열에 필터 적용\ndf.filter(df.column_name==값).show()\n\n# 필터링된 결과와 함께 필요한 열 선택\ndf.select(\"column1\", \"column2\", \"column3\").where(col(\"any column\")==\"any value\")\ndf.select(\"column1\").where(col(\"column1\")> 값).show(5)\ndf.sort(\"원하는 열 이름\")\n\n# 열 이름 변경\ndf.withcolumn Renamed(\"기존 열 이름\", \"원하는 열 이름으로 변경\")\n```\n\nPySpark는 데이터 프레임 내에서 날짜 속성을 추출하고 조작하는 편리한 방법을 제공하며, 사용자가 연도, 월, 일과 같은 다양한 기준으로 통찰력을 얻을 수 있도록 합니다.\n\n또한, 이러한 속성들은 오름차순이나 내림차순으로 정렬하여 분석과 시각화를 용이하게 할 수 있습니다.\n\n## 날짜 열에서 일, 월, 연도 추출하기: \n\n<div class=\"content-ad\"></div>\n\n\n# 데이터 프레임에서 연도, 월, 일 세부 정보 추출하기\ndf.select(year(\"date column\").distinct().orderBy(year(\"date column\")).show()\ndf.select(month(\"date column\").distinct().orderBy(month(\"date column\")).show()\ndf.select(day(\"date column\").distinct().orderBy(day(\"date column\")).show()\n\ndf.withColumn(\"orderyear\", year(\"df.date column\"))\ndf.withColumn(\"ordermonth\", month(\"df.date column\"))\ndf.withColumn(\"orderday\", day(\"df.date column\"))\ndf.withColumn(\"orderquarter\", quarter(\"df.date column\"))\n\n\n특정 열에서 null 값을 필터링하고 그 다음에 지정된 순서로 그룹화 작업을 수행하기 위해 조건을 적용할 수 있습니다.\n\n\ndf.select(\"column name we want to retrieve\").where(col(\"column name we want to retrieve\").isNotNull())\\\n.groupBy(\"column name we want to retrieve\").count().orderBy(\"count\", ascending=False).show(10)\n\n\n## 함수 작성하기:\n\n<div class=\"content-ad\"></div>\n\n```js\ndf.write.format(\"CSV\").mode(\"overwrite\").save(\"원하는 파일 저장 경로\")\ndf.write.format(\"CSV\").mode(\"append\").save(\"원하는 파일 저장 경로\")\ndf.write.format(\"Parquet\").mode(\"overwrite\").save(\"원하는 파일 저장 경로\")\ndf.write.format(\"parquet\").mode(\"append\").save(\"원하는 파일 저장 경로\")\n```\n\n## 윈도우 함수:\n\n```js\nwind_a=Window.partitionBy(\"col1\").orderBy(\"col2\").rangeBetween(Window.unboundedpreceeding, 0)\n\ndf_w_coloumn= df.withColumn(\"col_sum\", F.sum(\"salary\").over(wind_a) #롤링 합계 또는 누적 합계:\n\n\n#행 번호\na=Window.orderBy(\"date_column\") #예시로 날짜 열을 고려하였지만 원하는 열을 선택할 수 있습니다\nsales_data=df.withColumn(\"row_number\", row_number().over(a))\n\n#순위\nb=Window.partitionBy(\"date\").orderBy(\"sales\")\nsales_data=df.withColumn(\"sales_rank\", rank() over(b))\n\n#조밀한 순위\nb=Window.partitionBy(\"date\").orderBy(\"sales\")\nsales_data=df.withColumn(\"sales_dense_rank\", desne_rank() over(b))\n\n\n#지연\nc=Window.partitionBy(\"Item\").orderBy(\"date\") #예시 열을 고려하여 원하는 열을 선택할 수 있습니다\nsales_data=df.withColumn(\"pre_sales\", lag(col(\"sales\"),1).over(c))\n\n\n#리드\nd=Window.partitionBy(\"Item\").orderBy(\"date\") #예시 열을 고려하여 원하는 열을 선택할 수 있습니다\nsales_data=df.withColumn(\"next_sales\", lead(col(\"sales\"),1).over(d))\n```\n\n<div class=\"content-ad\"></div>\n\n이 기사는 데이터 엔지니어링 면접에 대비하는 개인들에게 가치 있는 도구로 작용하며, Databricks 플랫폼을 위해 구체적으로 맞춘 PySpark 함수와 수식들에 대한 간결하면서 포괄적인 요람을 제공합니다.\n\n구조화된 레이아웃과 각 예제와 함께 제공되는 자세한 설명으로, 본 자료는 독자들이 10분만 투자하여 주요 개념들을 효율적으로 검토하고 숙지할 수 있도록 도와줍니다.\n\n이러한 기본 기능들을 숙달함으로써 희망하는 데이터 엔지니어들은 자신감을 키우고 다양한 면접 상황을 쉽게 해결할 수 있는 준비를 갖추게 되며, 이를 통해 더욱 흥미로운 데이터 엔지니어링 직무를 얻을 성공 기회를 극대화할 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_4.png)\n\n<div class=\"content-ad\"></div>\n\n아래에서 데이터 엔지니어 인터뷰를 준비하는 데 도움이 되는 몇 가지 더 많은 기사를 찾을 수 있습니다.\n\n- SQL 사용 사례 - 데이터 엔지니어 인터뷰\n- 데이터 엔지니어 인터뷰에서 가장 일반적으로 묻는 빅데이터(Apache Spark) 개념\n- 데이터 엔지니어 인터뷰를 위한 Python 코딩 문제 제1부 (쉬운 난이도)\n\n<div class=\"content-ad\"></div>\n\n제 소중한 내용을 더 많이 공유할 수 있도록 함성 소리로 응원해주신다면 감사하겠습니다. \n\n저를 팔로우하고 구독하여 제 소식을 즉시 받아보세요. \n\n감사합니다 :)","ogImage":{"url":"/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_0.png"},"coverImage":"/assets/img/2024-05-20-TheEssentialPySparkCheatSheetforAllDataEngineers_0.png","tag":["Tech"],"readingTime":9},{"title":"데이터브릭스 리퀴드 클러스터링","description":"","date":"2024-05-20 16:57","slug":"2024-05-20-DatabricksLiquidClustering","content":"\n\n이전에 데이터 레이크하우스 세계에서 데이터 분할의 끊임없는 도전에 대한 동적 해결책이 있는지 궁금했나요?\n\n음, 저는 궁금했어요! 그럼 함께 이야기해보죠.\n\n## 고정된 데이터 레이아웃의 도전\n\n다음 그래프를 살펴보세요.\n\n<div class=\"content-ad\"></div>\n\n\n![DatabricksLiquidClustering](/assets/img/2024-05-20-DatabricksLiquidClustering_0.png)\n\n이 그래프는 연도별 테이블 행 수를 예측하고 데이터 분포에서 상당한 치우침을 보여줍니다. 이 치우침은 소비자가 쿼리에서 연도 열을 자주 필터로 사용하기 때문에 특히 관련이 있습니다.\n\n이 테이블은 생성 시 year 및 month 열을 사용하여 파티션으로 설정되었습니다. 이것이 바로이 테이블을 위한 DDL의 형태입니다.\n\n```js\n%sql\nCREATE TABLE kaggle_partitioned (\n  year_month STRING,\n  exp_imp TINYINT,\n  hs9 SMALLINT,\n  Customs SMALLINT,\n  Country BIGINT,\n  quantity BIGINT,\n  value BIGINT,\n  year STRING,\n  month STRING\n) USING delta PARTITIONED BY (year, month);\n```\n\n<div class=\"content-ad\"></div>\n\n여기 문제가 있어요. 테이블의 전체 데이터 중 약 83%가 2개의 파티션에 있습니다.\n\n![이미지](/assets/img/2024-05-20-DatabricksLiquidClustering_1.png)\n\n위의 정보를 바탕으로 테이블이 너무 적게 파티셔닝되었는지, 아니면 너무 많이 파티셔닝되었는지 어떻게 생각하시나요?\n\n이 테이블의 데이터 분포를 더 깊게 살펴보겠습니다. 다음 차트는 연간 행 수의 월별 분할을 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n\n![데이터 분포의 그림적 표현](/assets/img/2024-05-20-DatabricksLiquidClustering_2.png)\n\n더 자세히 데이터 분포를 나타낸 그림을 살펴보면, 2020년 3월에 가장 많은 데이터가 있고, 그 뒤를 이어서 1월과 2월이, 다른 달들은 더 작은 파티션으로 이어지고 있습니다.\n\n그래서, 테이블이 과분할되었는지? 아니면 과소분할되었는지? 아니면 둘 다인가요?\n\n다음 그림이 어떤 연관성이 있나요? 무슨 의미를 전달하나요?\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-DatabricksLiquidClustering_3.png\" />\n\n현재 파티셔닝 전략대로,\n\n- 2020–03과 같은 파티션의 경우, 한 시간치 데이터를 쿼리하기 위해 많은 양의 데이터를 읽어야 할 지도 모릅니다. \n- 반면에 다른 극단적인 상황에서는, 적은 양의 데이터를 가진 고객을 위한 쿼리를 수행하기 위해 여러 파티션을 가로질러 많은 작은 파일을 스캔해야 할 수도 있습니다. \n- 마지막으로, 테이블을 주/일/월 단위로 다시 파티셔닝해야 할 경우, 전체 테이블을 다시 작성해야 합니다. 또 다시요!\n\n이제 우리 테이블에서 데이터 쓰기 시나리오를 논의해 봅시다. 제 생각에는 이미지가 제 의견을 요약해 주고 있으므로 글로 다시 작성할 필요는 없을 것 같아요 ;)\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-20-DatabricksLiquidClustering_4.png\" />\n\n지금! 이 기사의 맨 첫 줄을 한 번 더 반복합시다!\n\n데이터 레이크하우스 세계에서 데이터 파티셔닝의 끊임없는 도전에 대한 동적 솔루션이 있는지 궁금했던 적이 있나요?\n\n리퀴드 클러스터링이 등장했습니다! 데이터 레이아웃 결정을 간소화하고 쿼리 성능을 향상시키며, 지속적인 모니터링 및 조정을 요구하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n그래서 어떻게 하는 건가요?\n\n- 빠름: 튜닝된 분할 테이블과 유사한 쓰기 및 읽기 속도\n- 자체 튜닝: 과다 및 부족한 분할을 피함\n- 점진적: 새 데이터의 자동 부분 클러스터링\n- 스쿠 내성: 일관된 파일 크기와 낮은 쓰기 증폭\n- 유연함: 클러스터링 열을 변경하고 싶나요? 문제 없어요!\n- 더 나은 병행성: 테이블에서 행 수준의 병행성 활성화\n\n좀 더 자세히 이해해 보겠습니다. 이전에 보여드린 샘플 레이아웃 다이어그램을 사용할 거예요.\n\n![다이어그램](/assets/img/2024-05-20-DatabricksLiquidClustering_5.png)\n\n<div class=\"content-ad\"></div>\n\n요렇게 Liquid Clustering이 어떻게 도와주는지 확인해보세요! Liquid Clustering은 군집화와 파일 크기의 효율적인 균형을 이룹니다.\n\n![Liquid Clustering](/assets/img/2024-05-20-DatabricksLiquidClustering_6.png)\n\n작은 파티션을 자동으로 처리할 뿐만 아니라, 큰 파티션에서 시간별 데이터만 가져오려면 더 효율적인 쿼리를 위해 더 나눌 수 있습니다.\n\n![Liquid Clustering](/assets/img/2024-05-20-DatabricksLiquidClustering_7.png)\n\n<div class=\"content-ad\"></div>\n\n실제로 보여드릴게요! 여기 파티션된 테이블의 파일 크기 분포입니다.\n\n![이미지](/assets/img/2024-05-20-DatabricksLiquidClustering_8.png)\n\n이 파티션된 테이블에서 클러스터된 테이블을 생성해보겠습니다. CTAS를 사용할 거에요.\n\n```js\nCREATE TABLE kaggle_clustered CLUSTER BY(year, month) AS\nSELECT\n  *\nFROM\n  kaggle_partitioned;\n```\n\n<div class=\"content-ad\"></div>\n\n그리고 군집 테이블의 파일 크기 분포도가 여기 있어요.\n\n![file size distribution](/assets/img/2024-05-20-DatabricksLiquidClustering_9.png)\n\n작은 파일 대부분이 통합되어 더 최적화된 파일이 생성된 것이 명백하게 나타나네요.\n\n액체 클러스터링은 일부/게으른 클러스터링을 활용하여 효율적으로 적재를 돕습니다.\n\n<div class=\"content-ad\"></div>\n\n그걸 어떻게 이해하는지 알아봅시다.\n\n- 2021-01은 제가 파티션 테이블에 데이터가 없는 파티션입니다.\n- 해당 날짜 범위의 데이터를 적재하기 시작하면, 모든 고객을 포함하는 파일이 생성됩니다.\n- 데이터 집합이 증가함에 따라, Liquid Clustering은 고객을 위한 파일을 분할하기 시작합니다.\n- 분할 작업은 가끔 작은 파일로 나누어지지만, 테이블 유지 관리는 이 작은 파일을 자동으로 큰 파일로 병합하여 읽기 성능에 영향을 미치지 않도록 합니다.\n\n# 그래서! 테이블에서 어떻게 사용하나요?\n\n첫 번째로! 클러스터링은 파티셔닝이나 ZORDER와 호환되지 않으며, Databricks 클라이언트가 테이블의 데이터에 대한 모든 레이아웃 및 최적화 작업을 관리해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n이제 리퀴드 클러스터링을 사용하여 델타 테이블을 생성하는 방법을 살펴보겠습니다.\n\n```js\n--빈 테이블 생성하기\nCREATE TABLE table1(col0 int, col1 string) USING DELTA CLUSTER BY (col0);\n\n\n--CTAS 문 사용하기\nCREATE EXTERNAL TABLE table2 CLUSTER BY (col0) --테이블 이름 뒤에 클러스터링을 지정하고, 서브쿼리에는 사용하지 않기\nLOCATION 'table_location' AS\nSELECT\n  *\nFROM\n  table1;\n\n\n--구성 복사를 위해 LIKE 문 사용하기\nCREATE TABLE table3 LIKE table1;\n```\n\n# 클러스터링을 트리거하는 방법\n\n간단히 테이블에 OPTIMIZE 명령을 사용하면 됩니다. 아래 예시를 참고하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n테이블 이름을 최적화하세요;\n```\n\n리퀴드 클러스터링은 증분 방식으로 작동합니다. 따라서 클러스터링해야 하는 데이터를 수용하기 위해서만 데이터가 다시 작성됩니다. 클러스터링 키를 갖는 데이터 파일이 클러스터링해야 하는 데이터와 일치하지 않는 경우 재작성되지 않습니다.\n\n데이터를 클러스터링하고 최상의 성능을 얻기 위해 정기적으로 최적화 작업을 실행해야 합니다. 리퀴드 클러스터링은 증분 방식이므로 대부분의 클러스터링된 테이블의 최적화 작업이 빠르게 실행됩니다.\n\n# 리퀴드 클러스터링은 무엇에 사용되나요?\n\n<div class=\"content-ad\"></div>\n\nDatabricks 문서에 따르면, 모든 새로운 Delta 테이블에는 리퀴드 클러스터링을 사용하는 것이 권장됩니다. 클러스터링이 유용한 시나리오의 예시는 다음과 같습니다:\n\n- 높은 카디널리티 열을 기반으로 자주 필터링되는 테이블.\n- 데이터 분포에서 상당한 불균형이 있는 테이블.\n- 빠르게 성장하여 유지보수와 조정 노력이 필요한 테이블.\n- 동시에 쓰기 요구사항이 있는 테이블.\n- 시간이 지남에 따라 액세스 패턴이 변경되는 테이블.\n- 전형적인 파티션 키가 테이블에 너무 많거나 너무 적은 파티션을 남길 수 있는 테이블.\n\n## 리퀴드 클러스터링 사용 시 고려해야 할 사항\n\n- 클러스터링 키로 수집된 통계가 있는 열만 지정할 수 있습니다. 기본적으로 Delta 테이블의 처음 32열에는 통계가 수집됩니다.\n- 최대 4개의 열을 클러스터링 키로 지정할 수 있습니다.\n- 구조화된 스트리밍 워크로드는 라이트 시 클러스터링을 지원하지 않습니다.","ogImage":{"url":"/assets/img/2024-05-20-DatabricksLiquidClustering_0.png"},"coverImage":"/assets/img/2024-05-20-DatabricksLiquidClustering_0.png","tag":["Tech"],"readingTime":5},{"title":"데이터 인프라 및 대형 언어 모델","description":"","date":"2024-05-20 16:53","slug":"2024-05-20-DataInfraandLargeLanguageModel","content":"\n\n데이터 인프라 산업, CRM 및 사이버 보안 산업은 언제나 전 세계에서 소프트웨어 부문 중 상위 세 분야 중 하나였습니다. (가트너 2023: 데이터 인프라 15%, CRM 14%, 사이버 보안 10%). 데이터 인프라 분야에서는 Oracle과 같은 거물 기업이 3000억 달러 이상의 가치를 지니고 있습니다. 또한 Snowflake, Databricks, MongoDB와 같은 신생 기술 스택, 포괄적인 제품 포트폴리오를 갖춘 세 가지 주요 클라우드 제공업체와 함께 수백 개의 데이터베이스가 DB-Engines.com에 의해 모니터링되고 있습니다.\n\n지난 5년 동안 데이터 인프라는 클라우드 네이티브 기술을 수용하는 데 중점을 두었다면, 앞으로 5년은 대형 언어 모델의 변혁적인 힘을 받아들이는 데 중점을 둘 것입니다.\n\n# Snowflake CEO 변경\n\n2024년 2월 28일, Snowflake는 회계 연도의 제4 분기 재무 결과를 발표했습니다. 실망스러운 전체 연간 지침을 제공한 후, 회사는 미국 데이터 인프라 산업을 충격받게 할 또 다른 소식을 발표했습니다.\n\n<div class=\"content-ad\"></div>\n\n미국 소프트웨어 역사상 가장 전설적인 CEO 중 한 명인 프랭크 슬룻만이 Snowflake의 CEO로서 사임을 발표했습니다. 새 CEO는 Neeva의 인도계 창업자 인 스리다르 라마스와미입니다. 스리다르는 지난해 Neeva를 Snowflake에 판매한 후 Snowflake에 합류했고, 모든 새 AI 관련 비즈니스를 감독하는 Snowflake의 AI SVP로 임명되었습니다. 단 년 만에 매각되었던 회사의 창업자에서 상위 회사의 새 CEO로 전환했습니다.\n\nSnowflake CFO 마이클 스카르펠리는 수익 보고서 이후 일주일 뒤 투자자 통화에서 \"저희는 프랭크의 퇴임 소식을 수요일에 (수익 보고서를 발표한 날)\"만 알게 된 사실을 언급했습니다. 그리고 \"그 동안 프랭크가 이사회와 스리다르와 더 많은 시간을 보낸 것으로 보아, 그는 프랭크의 후임자가 될 수 있을 것이라 여겼습니다.\" 스카르펠리는 프랭크와 오래된 친구이며, 둘은 함께 ServiceNow에서 황금 콤비를 이루었으며, 함께 Snowflake로 합류하기 전에 두 사람은 모두 몬태나 주 보즈만에 거주하고 있어, 저희처럼 놀라실지 모르겠습니다.\n\nSnowflake의 천사 투자자이자 창업 CEO 마이크 스페이서도 프랭크의 사임에 대해 다음과 같이 언급했습니다:\n\n- 스페이서와 두 창업자가 Snowflake를 공동 창업할 때, 제품이 제공될 때 CEO로서 사임할 것을 합의했습니다.\n- CEO로서 사임한 후, 스페이서가 다음 CEO로 마이크로소프트의 밥 머글리아를 데려왔고, 제품을 시장에 내놓고 비즈니스 모델을 확립하는 단계에 중점을 둔 \"명확한 업그레이드\"라고 평가했습니다.\n- 이후 이사회는 다음 큰 도전이 공개 상장하고 확장하는 것인 것을 깨닫고, 회사 전체에 높은 강도와 긴급함을 전파할 수 있는 프랭크 슬룻만을 찾아내어 성장을 가속화하고 최종적으로 회사를 공개시켰습니다.\n- 스페이서와 슬룻만 모두 스리다르가 Snowflake의 다음 세대 리더로 가장 적절한 지도자라고 믿습니다.\n\n<div class=\"content-ad\"></div>\n\nCEO 교체는 칼을 바꾸는 것과 같아요. LLM이 주도하는 데이터 인프라의 다가오는 시대에는, Frank가 Snowflake를 위한 가장 적합한 CEO가 아닐 수도 있어요. 이것이 Mike Speiser를 설득했는데, 그는 각 CEO 교체에서 좋은 결과를 보았기 때문에 Sridhar가 다음 단계에 더 적합할 수도 있다고 믿게 되었어요.\n\n나중에 Sridhar는 Snowflake뿐만 아니라 세 대형 클라우드 제공업체가 그를 자신들의 AI 리더로 초대했다고 언급했지만, Sridhar는 최종적으로 Snowflake를 선택했어요. Sridhar는 데이터베이스, LLM 및 경영의 복합 기술 세트를 갖춘 희귀한 재능이에요. 데이터베이스 관련 분야의 박사 학위를 보유하고 있으며, Google에서 10,000명 이상의 팀을 이끈 'Google 광고의 왕'으로서 Meta보다 추천 알고리즘에서 뒤처지지 않고 추월하도록 도왔어요. 그는 나중에 AI 검색 회사 Neeva를 설립했어요.\n\n이것이 데이터 인프라 산업이 LLM 시대의 결정적 전투 이전으로 급속히 밀려들고 있다는 것을 생각하게 해줘요. 이번 전투의 도래에 절박함을 느끼는 기업들만이 Frank와 같은 이전 시대의 훌륭한 지도자를 교체할 결정을 내릴 것이에요.\n\n이 변화는 단순히 Snowflake의 선택일 뿐만 아니라 많은 소프트웨어 회사들이 내릴 결정일 수 있어요. AI 배경을 갖춘 CEO는 어디에 AI에 투자해야 하는지 명확히 이해할 수 있고, 어떤 제품과 기술 능력을 보충해야 하는지, 이 사업을 운영하기 위한 적절한 인재를 찾을 수 있는 곳을 알 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n선점자 이점, 시간은 아무도 기다려주지 않아요. 다음 섹션에서 이에 대해 더 깊이 파고들어보겠습니다.\n\n# 데이터 인프라에서 수익을 올리는 방법: 교육 파이프라인에 진입함으로써\n\n작년 Q1에 시작된 AI 붐으로 인해 데이터 인프라 기업이 혜택을 받기 시작한 이야기는 스노우플레이크(Snowflake)와 몽고디비(MongoDB) 같은 주요 기업들에게 명확한 AI 수익 기여로 이어지지 않았습니다.\n\n몽고디비의 2023년 Q4 실적 보고서에서, 기업은 이번에 처음으로 전통적인 데이터 인프라 기업들이 이 분야에서 아직 큰 수익을 내지 못한 이유에 대해 설명했습니다:\n\n<div class=\"content-ad\"></div>\n\n- 데이터 인프라 기업은 대형 언어 모델 (LLM) 영역에서 모델 학습, 미세 조정 및 추론 세 가지 계층에 참여할 수 있습니다.\n- MongoDB의 기존 기술 스택은 주로 후자 두 계층 (미세 조정 및 추론)과 관련이 있지만, 현재 고객 사용 사례를 기반으로 하면 대다수의 고객이 여전히 첫 번째 계층 (모델 학습)에 있습니다.\n- 고객이 세 번째 계층 (추론)으로 이동할 때까지, 중요한 AI 수익이 MongoDB로 흘러들지 않을 것입니다.\n\n이것은 데이터 인프라 분야의 현재 상업적 현실을 반영합니다. 학습 기술 스택에 관여한 새로운 세대의 데이터 인프라 기업만이 이 분야에서 수익을 얻었습니다. 일반적으로 ETL/특성 엔지니어링, 데이터 레이크, 벡터 데이터베이스, 학습 최적화 프레임워크, 그리고 전통적인 기계 학습에서 자주 사용되는 라이프사이클 관리 및 실험 추적 도구를 포함합니다. Databricks, Pinecone 및 Zilliz, Myscale과 같은 중국 기업을 비롯한 이러한 새로운 세대의 도구들이 이미 AI 학습에서 첫 번째 돈을 벌어들였습니다.\n\n![image](/assets/img/2024-05-20-DataInfraandLargeLanguageModel_0.png)\n\n이전 Relit 블로그 게시물에서 언급된대로, 그들의 대형 언어 모델은 Databricks의 기술 스택과 함께 세 대 주요 클라우드 제공업체의 인프라를 많이 활용하여 모델 학습 프로세스를 완료했습니다.\n\n\n<div class=\"content-ad\"></div>\n\n# Databricks — 검증된 솔루션으로 지난 10년을 보냈습니다\n\nDatabricks은 가장 주목받는 새로운 세대의 데이터 인프라 플레이어 중 하나입니다.\n\n최근 공개된 비즈니스 데이터에 따르면:\n\n- Databricks는 2023년에 16억 달러의 매출을 달성했으며, 연간 약 55%의 성장을 이룩했습니다.\n- 16억 달러의 매출은 경쟁사인 Snowflake의 60% 미만이지만, Databricks의 매출 모델은 Snowflake와 다릅니다. Databricks는 SQL Serverless 제품뿐만 아니라, 클라우드 제공업체의 컴퓨팅 및 저장 서비스를 번들로 제공하여 소프트웨어 수익과 클라우드 제공업체의 마진을 모두 확보합니다. 또한, Databricks의 다른 제품 대부분은 소프트웨어 가치만을 판매합니다.\n- 더 합리적인 비교는 총 마진입니다. 클라우드 제공업체의 매출을 제외한 경우, Databricks의 총 마진은 Snowflake의 약 65%에 해당하며, 빠른 성장을 고려했을 때, Databricks의 마진은 2023년 제 4 분기에 약 70%로 Snowflake의 마진에 가까워졌습니다 (이는 Databricks의 높은 마진 소프트웨어 비즈니스 비중을 반영합니다).\n- 추세를 살펴보면, Databricks는 2023년에 매출 성장을 가속화했으며, 2024년에도 성장률이 60%까지 가속화될 것으로 예상됩니다. 이를 뒷받침하는 증거로, Databricks는 2023년 제 4 분기에 거의 100%의 연간 예약 성장을 기록했습니다.\n\n<div class=\"content-ad\"></div>\n\nDatabricks의 현재 성공에 비해, 지난 10년간의 개발은 원활하지 않았습니다. 이는 검증 과정을 거치는 10년의 여정이었습니다.\n\nDatabricks는 오픈 소스 Spark에서 시작하여 후에 대표 제품인 Delta Lake으로 데이터 레이크를 확장했습니다:\n\n- Spark는 Databricks의 초기 제품이자 현재 핵심 제공품으로, 기계 학습과 데이터 엔지니어링을 지원하는 플랫폼으로 처음에 위치했습니다.\n- 딥러닝 붐이 일어나기 전까지 Spark는 거의 모든 기계 학습 작업을 처리할 수 있었지만, 딥러닝의 부상으로 인해 Spark는 더 이상 주류 기계 학습 플랫폼이 되지 않았습니다. TensorFlow 이후 PyTorch가 주류가 되었습니다.\n- 독립적인 기계 학습 플랫폼 이상으로, Spark는 데이터 엔지니어링 분야를 지배하며 가장 주류인 ETL 도구가 되었고, Databricks에게 ETL/기능 엔지니어링을 통한 대용량 언어 모델 시대로 가는 열쇠를 제공했습니다.\n\n다른 대표 제품 Delta Lake도 Databricks를 가장 큰 상업용 데이터 레이크 서비스 제공업체로 만들었습니다:\n\n<div class=\"content-ad\"></div>\n\n- 머신 러닝 데이터를 처리할 때는 대량의 구조화되지 않은 데이터가 필요하며, 데이터 레이크가 가장 비용 효율적인 저장 방법이 되었습니다.\n- 그러나 스노우플레이크가 거대한 시장 기회와 더 쉽게 이해할 수 있는 데이터 웨어하우스 개념으로 급속히 성장함에 따라, 데이브릭스는 그림자를 드리운 채로 남아 있었습니다.\n- \"보다 맛있는\" 데이터 웨어하우스 사업을 잡기 위해 데이브릭스는 레이크하우스 컨셉을 제안했습니다. \n- 데이브릭스 역시 고객에게 더 많은 자율성을 부여하여 고객이 큰 부피로 인해 구름에서 큰 할인을 받을 수 있는 매우 큰 고객들에게 매우 친숙합니다.\n- 데이브릭스 SQL은 스노우플레이크에 비해 높은 계산량의 복잡한 시나리오에서 데이터 웨어하우스 능력의 고유한 결함으로 인해 성능 대 가격 비율이 다소 떨어집니다.\n\n<div class=\"content-ad\"></div>\n\nDatabricks는 개발 과정에서 여러 가지 어려움을 겪었지만 결국 승리를 거뒀습니다. 오랫동안 홍보되어온 Spark와 Lakehouse 제품은 대규모 언어 모델 시대에 대한 주요 무기가 되었습니다. \n\n- 현재 단계에서 기술 스택의 완성도와 플랫폼 기능성(목표를 종단-to-종단으로 구현하는 능력)은 단일 기능의 우수한 성능보다 중요합니다.\n- 대규모 언어 모델 시대에 비정형 데이터 처리의 폭발적인 성장에서 Delta Lake + Databricks Spark는 비정형 데이터 처리의 황금 콤보가 되어 ETL/Feature Engineering 워크로드의 상당 부분을 차지하게 되었습니다.\n- 기계 학습 분야에서의 포괄적인 노하우를 통해 MosaicML을 인수한 후, Databricks는 세 주요 클라우드 및 NVIDIA 이후 또 다른 풀스택 대규모 언어 모델 훈련 플랫폼이 되어, 거의 마지막 퍼즐 조각을 완성하게 되었습니다.\n- 그리고 2024년에 Snowflake이 오픈 포맷을 완전히 받아들이고 고객이 자체 저장로드를 사용할 수 있도록 하는 것으로 Lakehouse 경로를 완전히 수용함에 따라, Lakehouse가 빅데이터 시대의 주류가 되었습니다. 호수나 창고에서 들어오든, 궁극적인 해결책은 Lakehouse가 될 것입니다.\n\n# Snowflake의 따라잡기 계획\n\n언제나 비정형 데이터와 기계 학습에 중점을 둔 Databricks와 달리, Snowflake의 경로는 더 다양했으며, 기계 학습 분야에는 상대적으로 적은 투자를 한 다고 말할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nSnowflake의 공동 창업자인 Benoit Dageville은 2023년 이전에 Unistore와 Snowpark에 중점을 둔 Snowflake의 기술 로드맵을 책임지고 있습니다. 먼저 Unistore에 대해 논의해 봅시다:\n\n- Unistore는 HTAP와 유사한 제품으로, 하단에 KV 스토어 디자인이 있습니다. Benoit은 이 제품이 Snowflake이 대규모 데이터베이스 (OLTP) 영역으로의 시장 기회 확대에 도움이 될 것으로 기대했습니다. 그러나 KV 스토어 디자인 때문에 Oracle과 같은 주류 OLTP와 직접적으로 경쟁할 수는 없으며, 주로 OLAP가 주력이고 OLTP가 보조인 기업들이 채택할 수 있는 솔루션이 적합합니다.\n- Unistore를 구현하는 기술적 어려움도 상당히 높으며, 데이터 웨어하우스에 약하지만 Snowflake의 고품질 데이터 처리와 안정성에 대한 요구 또한 엄청나게 높습니다. 한편, HTAP도 새로운 기술 솔루션이며, HTAP 선구자들은 이 분야에서 벽을 만나고 있어 HTAP 비즈니스 모델이 증명되었는지 여부를 확신할 수 없게 만듭니다.\n\nUnistore와 비교하여 Snowpark의 논리는 더 직관적입니다:\n\n- Snowpark는 더 간소화된 제품 논리를 갖고 있습니다. 고객이 데이터를 Snowflake에 적재할 때 ETL 처리를 수행해야 하며, 과거의 주류 처리 방법은 Open-Source Spark와 Databricks Spark였습니다. 이제 Snowflake의 네이티브 ETL 도구인 Snowpark를 사용하면 고객은 전송 비용을 절감할 수 있으며 기능적인 차이가 없어 고객이 Snowpark로 전환하여 비용 효율성을 높이는 것이 자연스러운 선택이어야 합니다.\n- Open-Source Spark(주로 AWS 고객들 사이에서 EMR 제품으로 판매되는 제품)과 비교하여, Snowpark의 비용 효율성 이점은 매우 명확합니다. 그러나 최적화된 상업용 제품인 Databricks Spark와 비교하면, Snowpark는 이미 Snowflake 제품을 사용 중인 고객들을 위해 데이터 처리에 더 초점을 맞추고 있습니다.\n- Snowpark는 데이터 엔지니어링 작업을 신속하게 따라잡을 수 있고 기술적 장벽도 높지 않습니다. 그러나 머신러닝 영역에서는 아직 많은 작업이 남아 있으며, 특히 Spark의 오픈소스 이점에 직면하여 Snowpark는 특정 전통적인 산업을 위한 머신러닝 기능을 제공하는데 초점을 맞추고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n스노우파크가 2022년 말에 상용화된 이후, 수익 규모는 Databricks의 ETL 수익의 약 5~10%로 빠르게 성장하고 있어요. Databricks의 경쟁 제품인 Databricks SQL과 비교하면, 그 규모는 Databricks SQL의 약 1/3 수준으로, 출시 시기도 Databricks SQL보다 1년 뒤입니다.\n\n스노우파크는 Snowflake가 대형 언어 모델 시대에 진입하기 위한 열쇠를 제공했고, Snowflake의 미래 대형 모델 지원 제품은 모두 스노우파크를 중심으로 구축될 것입니다:\n\n- 스노우파크는 비정형 데이터를 처리할 수 있는 능력을 Snowflake에 제공하여 대형 언어 모델 시대에서 ETL 및 피처 엔지니어링 요구를 충족할 수 있게 했습니다.\n- 스노우파크를 더 발전시킴으로써, Snowflake는 Iceberg 오픈 포맷을 지원하기 시작했고, 이는 Snowflake가 더 많은 비정형 데이터를 유치하고 완전한 Lakehouse 솔루션을 구축하는 기초가 되었습니다.\n- 동시에 Snowflake는 Snowpark 컨테이너 서비스를 출시했고, 이는 Snowflake의 주요 업무로 자리 잡았습니다. GPU Workloads를 Snowflake로 가져오며 컨테이너 서비스에서 모델을 Fine-tune하고 배포할 수 있게 했습니다.\n\nSridhar가 Snowflake에 합류한 후, 그는 새 제품 Cortex에 집중하기도 했다요:\n\n<div class=\"content-ad\"></div>\n\n- Cortex는 대화 및 관련 분석을 위한 제품을 주로 하는 새 투자인 Mistral AI를 포함한 Snowflake를 위해 외부 대규모 언어 모델 파트너를 가져왔어요.\n- Cortex에는 Document AI와 Databricks의 LakehouseIQ와 유사한 Snowflake Copilot도 포함되어 있어요. Text2SQL 및 지식베이스에 대한 솔루션을 제공합니다.\n- Sridhar는 이전 회사 Neeva에서 사용한 RAG-Vector Search 솔루션을 Cortex로 통합하고, Snowflake에 벡터 스토리지 및 처리 능력을 곧 추가할 예정이에요. 미래에는 더 많은 컨테이너 서비스 고객을 지원하여 컨테이너 서비스에서 직접 모델을 배포하고 추론할 수 있도록 해줄 거예요.\n\nSridhar는 Snowflake가 부족한 점을 잘 알고 있고 투자해야 할 노력을 알고 있어요. 이는 Snowflake가 DeepSpeed의 창업자와 그 핵심 팀을 빼앗는 것에서도 확인할 수 있어요:\n\n- Snowflake의 CFO는 후속 커뮤니케이션에서 DeepSpeed로부터 빼앗은 5명의 연간 비용이 2000만 달러였다고 언급했어요. \"놀랍게도 높고, 너무 비싸고 뛰어납니다.\"\n- 그러나 Sridhar는 Snowflake가 최고의 타겟인 MosaicML과 같이 훌륭한 대상을 찾을 수 있어야 한다는 것을 잘 알고 있어요. 그들을 인수할 수 없는 경우 직접 인사할 필요가 있어요. DeepSpeed 팀은 지금 가장 인기 있는 대규모 언어 모델 훈련/추론 프레임워크로 거의 최상의 선택이에요.\n- Frank 시대에는 상상조차 할 수 없었지만, 새 CEO의 탑-다운 추진 아래에서만 실현할 수 있었던 높은 비용과 \"올드 가드\"의 회사 의도를 이해하는 데 어려움이 있었어요.\n\nCEO 교체 후, Snowflake는 \"All in AI\" 전략을 취하고 모든 제품이 AI를 중심으로 한 것으로 나타났어요.\n\n<div class=\"content-ad\"></div>\n\n회사의 주요 사업이 데이터 웨어하우징인 회사에서 AI를 하는 것은 두 번째 스타트업을 하는 것과 같습니다. 하지만 Snowflake는 아직 멀은 길이 남아 있어요.\n\n# MongoDB의 RAG 이야기\n\nDatabricks와 Snowflake와 달리 MongoDB는 분석 쪽이 아니며, 제품은 OLTP에서 비즈니스 데이터의 흐름과 저장을 지원하는 데 더 중점을 두고 있어요.\n\n2023년 초에는 MongoDB가 데이터 인프라 공간에서 최고의 타깃이었는데, 당시 시장 논리는 다음과 같았습니다:\n\n<div class=\"content-ad\"></div>\n\n- 몽고DB는 문서 데이터베이스를 기반으로 개발되어서 데이터 구조에 대한 과도한 고려 없이 데이터를 먼저 수용하고, 그런 다음 매우 높은 사용 편의성으로 데이터를 처리할 수 있습니다.\n- 대규모 언어 모델의 교육과 추론은 많은 비구조화 데이터를 사용하며, 몽고DB의 주요 제품은 반구조화 및 비구조화 데이터를 저장, 읽기, 쓰기 및 쿼리하기 위한 것입니다.\n- 교육 측면에서 몽고DB는 비구조화 데이터의 저장 매체로 사용될 수 있으며, 이는 고객의 기술 스택에서 몽고DB의 중요성을 더욱 높일 수 있습니다.\n- 몽고DB에는 자체 벡터 데이터베이스를 구축하고 모델 추론 측면에 진입할 기회가 있습니다.\n- 더 많은 LLM 응용 프로그램은 더 많은 앱을 의미하며, 이는 LLM 워크플로우에서 반드시 MDB를 사용하지는 않지만, 여전히 챗봇 채팅 기록과 전통 OLTP 워크로드를 저장하기 위해 몽고DB가 필요합니다.\n\n몽고DB는 2023년 제1분기에 Hugging Face와 Tekion과 같은 잘 알려진 기업을 포함한 200개의 새로운 AI 고객을 확보하면서 협력했습니다. 그러나 이후 분기에는 몽고DB가 AI 고객 정보를 더 이상 공개하지 않았습니다.\n\n몽고DB의 주요 관심사는 주로 추론 측면에 집중되어 있으며, 이로 인해 최근 분기에 대형 모델 시나리오가 교육 단계에 여전히 머물러 추론 단계에 진입하지 않았다는 것을 언급해도 매출 기여도가 미미한 것입니다.\n\n추론 측면에서 몽고DB의 기회를 조사하는 것:\n\n<div class=\"content-ad\"></div>\n\n- 이전 두 회사와 비교했을 때, 추론 측면에서 데이터 애플리케이션 및 API 레이어에 더 중점을 둔 MongoDB는 엔드 유저에게 서비스를 제공할 수 있으며, 이는 OLTP 포지셔닝과 밀접하게 관련되어 있습니다.\n- MongoDB의 Atlas Vector Search 서비스는 2024년 초에 가장 먼저 GA 및 상용화된 벡터 검색 기능을 제공했습니다.\n- 기존 고객들을 대상으로 전통 기술 스택이 더 신뢰성 있을 수 있으며, 특히 RAG 요구 사항이 아직 초기 단계에 있고 대규모로 확장되지 않은 경우, MongoDB의 벡터 검색 서비스가 이미 요구 사항을 충족시킬 수 있습니다.\n\n하지만 다른 RAG 솔루션과 비교하면, MongoDB는 여전히 추론 개발 초기 단계에 있습니다:\n\n- 대량 데이터 양과 높은 동시성을 갖는 시나리오에서, MongoDB는 아직 인공지능 기반 벡터 데이터베이스보다 뒤쳐지고 있습니다(주로 MongoDB의 벡터 데이터베이스 엔진 알고리즘의 축적이 이러한 전문화된 벡터 데이터베이스와 비교해 약하며, 추론 시나리오가 확대될수록 엔진 기능에 대한 요구사항도 상당히 증가할 것).\n- 새로운 세대의 RAG 방법은 밀집 임베딩과 벡터 데이터베이스를 결합하기 때문에 전통적인 BM25에 대해 매우 높은 수준의 요구사항이 있으며, 이 부분에서 MongoDB의 솔루션이 Elastic보다 못할 수도 있습니다.\n- MongoDB에는 아직 많은 기능적 격차가 남아 있습니다.\n\n# 세계에는 엔드투엔드 기술 스택이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n다음 표에는 세 회사의 LLM 진척 상황이 나와 있습니다. 첫 번째는 교육 분야입니다:\n\n- Databricks는 완전한 프로세스 교육 기술 스택을 갖추고, MosaicML을 통해 마지막 퍼즐 조각을 채웠습니다. 그러나 여전히 대규모 모델 훈련에서 공용 클라우드에 한 발짝 뒤처지고 있습니다.\n- Snowflake는 현재 노트북, 데이터 레이크, 모델 훈련 최적화 및 MLFlow 레이어에서 상당한 차이가 있어 보완 작업 중이며, 현재는 고객이 컨테이너 서비스에서 세밀 조정을 수행할 수 있도록 하는 데 보다 집중하고 있습니다.\n- MongoDB는 추론 쪽에 초점을 맞추어 기본적으로 교육에 개입하지 않습니다.\n- Databricks의 RAG 솔루션이 아직 공개 베타 상태이며, 원 스탑 추론 능력이 아직 갖춰지지 않았지만 올해 중에 완료될 것으로 예상됩니다.\n- Snowflake의 Snowpark ML 및 RAG 솔루션이 또한 공개 베타 상태이며, 컨테이너 서비스 데이터 응용프로그램에서 추론 배포에 대한 미래 지원이 더 있으며, 이는 챗봇 및 기업 지식베이스와 같은 시나리오일 수 있습니다.\n- MongoDB는 세밀 조정 및 컨테이너에는 관여하지 않지만, 최종 사용자를 위한 RAG 솔루션이 더 중시되며, 보다 폭넓은 고객 기반을 대상으로 합니다.\n\n기술 선도 기업들은 이미 세 가지 큰 클라우드 및 다양한 AI-네이티브 플랫폼의 LLM 기술 스택을 채택하고 있으며, 세 회사에 대한 미래의 주요 증가 기회는 전통적인 기업 시나리오입니다:\n\n- 전통 기업의 경우, 종단 간 기술 스택이 매우 중요합니다. LLM 인재 부족 시대의 고객은 최고의 LLM 팀을 구축할 수 없으므로 교육/추론 프로세스가 간단할수록 좋습니다.\n- 전통 기업은 또한 LLM 예산을 늘리고 있습니다. 이는 고객 서비스와 같은 시나리오에 대해 오픈 소스 모델을 교육하거나 다른 써드파티 소프트웨어 응용 프로그램 솔루션을 구매하는 것을 통해 이루어질 수 있습니다.\n- 그러나 역사적으로 초기 응용 솔루션 제공 업체는 자체 내장 데이터 인프라를 제공할 수 있지만, 생태계가 통합되면 고객은 모든 써드파티 솔루션을 지원하기 위해 자체 데이터 인프라를 사용할 가능성이 더 높습니다.\n\n<div class=\"content-ad\"></div>\n\n# 전투는 새로운 장을 여는 새로운 시작이기도 합니다\n\n지난 몇 년간 데이터 인프라 주변의 경쟁은 언제나 다음을 초점으로 했습니다: 클라우드 아키텍처 또는 온프레미스 아키텍처, 레이크 또는 데이터 웨어하우스, NoSQL TP 또는 SQL TP.\n\n이제 LLM에 의해 주도되는 새로운 데이터 인프라 수요로 인해, 이제는 다음과 같은 질문이 되었습니다:\n\n- 그들은 가능한 한 빨리 새로운 제품을 만들어 추가적인 시장을 확보할 수 있을까요?\n- 새로운 제품을 만들 수 없고 LLM 팀을 영입하지 못한다면, 영원히 뒤처지고 예전 제품에서도 점유율을 잃을 것인가요?\n\n<div class=\"content-ad\"></div>\n\n그래서 우리는 Snowflake과 같은 기업이 AI에 올인하기 위해 CEO를 대체하는 움직임을 보게 됩니다.\n\nDatabricks와 Snowflake 이외의 다른 회사가 MosaicML을 인수하거나 DeepSpeed 팀을 영입할 수 있을지 상상하기 어렵습니다. 새로운 LLM 인재들은 주요 데이터베이스 기업에만 끌릴 수 있으며, 이는 오픈 소스, 온프레미스, 그리고 남은 데이터베이스와의 격차를 더 확대할 수 있습니다.\n\n이것은 싸움이지만, 더 큰 점증적 기회일 것으로 보입니다.\n\n6월의 연간 제품 이벤트에서 이러한 기업들이 새로운 제품을 대거 선보일 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- 데이터브릭스는 Vector 검색 및 컨테이너 서비스 솔루션을 GA할 수 있습니다.\n- Snowflake는 Cortex 기능, 컨테이너 서비스, SnowparkML, 노트북, Iceberg, Streamlit 솔루션을 GA할 수 있으며, 진행 속도가 빠르다면 Vector 검색도 GA할 수 있습니다.\n- MongoDB를 포함한 각 회사는 RAG 기능을 지속적으로 개선하며 시간과의 경쟁을 벌이고 있습니다.","ogImage":{"url":"/assets/img/2024-05-20-DataInfraandLargeLanguageModel_0.png"},"coverImage":"/assets/img/2024-05-20-DataInfraandLargeLanguageModel_0.png","tag":["Tech"],"readingTime":14},{"title":"Netflix의 미디어 랜드스케이프 진화 321에서 클라우드 스토리지 최적화로","description":"","date":"2024-05-20 16:50","slug":"2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization","content":"\n\nby Esha Palta Vinay Kawade Ankur Khetrapal Meenakshi Jindal Peijie Hu Dongdong Wu Avinash Dathathri\n\n# 소개\n\nNetflix는 매년 다양한 콘텐츠를 제작합니다. 각 콘텐츠 제작 단계에서 넷플릭스는 다양한 콘텐츠 자산(이미지 시퀀스, 비디오, 텍스트 등)을 다양한 제작에서 확보합니다. 이러한 자산은 나중에 안전하게 Amazon S3 스토리지 서비스에 저장됩니다. 이 데이터의 상당 부분은 제작 중에 일시적으로 액세스되며 해당 콘텐츠 출시 시까지만 사용됩니다. 대부분의 자산은 해당 콘텐츠가 출시될 때까지 활성 또는 '핫' 스토리지 계층에만 유지되는 것이 목적입니다. 이 블로그에서는 사용자 액세스 패턴을 활용하여 스토리지의 효율성과 비용 효과를 스마트하게 최적화하는 방법을 탐색할 것입니다. 본 탐사에서는 다양한 AWS 스토리지 계층에 맞게 맞춤형 아카이브 및 삭제 전략의 비용 효율성을 명확히 검토하는 수명 주기 정책의 비용 분석에 대해 다루어볼 것입니다.\n\n# Netflix의 콘텐츠 제작 및 스토리지 관행 개요\n\n<div class=\"content-ad\"></div>\n\n콘텐츠 제작의 다이내믹한 환경에서 전통 스튜디오들은 시험된 3/2/1 규칙에 오랜 기간 동참해왔습니다. 이 전략은 최소 두 가지 다른 유형의 미디어에 저장된 원본 카메라 영상과 오디오의 세 개 복사본 유지 및 한 개의 백업을 오프사이트에 보관하는 방식을 포함하고 있습니다.\n\n제작 라이프사이클의 활동적인 부서로 들어가 미디어 저장 및 백업 규모를 이해해보겠습니다. LTO 테이프 백업은 제작 및 후반 제작 라이프사이클 동안 지속적으로 존재합니다.\n\n![Production workflow](/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_0.png)\n\n<div class=\"content-ad\"></div>\n\n한 번 미디어가 카메라와 사운드 녹음기에서 추출되면, 디스크 파일로 변환되어 편집, 사운드 및 음악, 시각 효과 (VFX) 및 이미지 완성을 포함한 각 부서별 도구를 사용하여 조작됩니다. 이 미디어는 이 프로세스의 각 단계와 단계마다 포괄적인 백업 루틴을 거칩니다. 물리적 백업 및 아카이브를 기반으로 한 이 방법은 우연한 삭제, 공급 업체별 오류 및 자연 재해의 잠재적 위험을 줄이기 위해 설계되었습니다. 데이터 손실이 기획 및 촬영 단계에서 상당한 비용 손실로 이어질 수 있음을 명확히 인식하여 이러한 백업 프로세스의 중요성을 보여주고 있습니다.\n\n현대 클라우드 스토리지 시스템의 등장은 스토리지 관행에서 패러다임 전환이라는 것을 알려줍니다. AWS S3와 같은 플랫폼은 11 9 이상의 내구성, 복원력 및 가용성을 자랑하는 높은 내구성을 제공합니다. 이 발전은 Netflix와 같은 미디어 회사가 데이터 아카이빙 및 삭제 정책과 같은 도구를 활용하여 그들의 스토리지 방법론을 재정의할 수 있도록 했습니다. 현장에서 촬영된 카메라와 사운드 시스템에서 캡처된 미디어는 인접한 데이터 센터 시설로부터 직접 Netflix의 클라우드 스토리지로 업로드되며, 백업이 필요하지 않습니다. 이 업로드 이후, 데일리, 편집, 시각 효과 및 이미지 완성과 같은 다양한 단계가 다운로드되어 수정되고, 최종 데이터 버전이 클라우드 스토리지로 전송될 수 있습니다. 이 구조는 콘텐츠 제작에 부합된 추적, 접근, 제어 및 확장성을 용이하게 합니다.\n\n또한, 데이터 수명 주기 정책과 통합함으로써 저장 비용 절감과 데이터 관리를 위한 에너지 수요 감소, 이에 따라 탄소 발자국을 줄일 수 있습니다. 이러한 관행을 실행함으로써 기업은 에너지 소비를 줄이고, 결과적으로 환경 영향을 줄이면서 운영 효율성과 비용 효율성을 높일 수 있습니다.\n\n이러한 전략을 도입하는 조직은 지속가능성에 기여하며, 데이터 성장을 더 효과적으로 관리할 수 있는 민첩성과 준비성을 높일 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n# 사용자 액세스 패턴 활용\n\n넷플릭스의 미디어 콘텐츠 Orchestration의 핵심에는 중앙 집중식 자산 관리 플랫폼 (AMP)이 있습니다. 이 강력한 시스템은 제작 및 후기 단계에서 제작된 모든 미디어 자산을 지속하고 발견하는 데 전념되어 있습니다. 이 중앙 집중식 허브는 광범위한 미디어 콘텐츠 라이브러리의 관리와 접근성을 간소화하는 데 도움이 됩니다. 또한 우리는 소중한 통찰력을 추출하고 미디어 자산의 사용 및 액세스 패턴에 대한 상세한 보고서를 생성할 수 있습니다. 이 대국적인 시각은 콘텐츠가 어떻게 활용되는지에 대한 우리의 이해를 높이며, 정보에 기반한 의사 결정을 위한 기초를 마련합니다.\n\n우리의 가설을 검증하기 위해, 발매 후 다양한 간격에서 사용자 및 애플리케이션에 의한 자산 액세스 속도를 조사했습니다. 조사 결과는 의미 있는 것들을 드러내었으며, 연관된 제목이 발매된 후 자산 사용량이 상당히 감소한다는 것을 밝혔습니다. 액세스 패턴의 인사이트:\n\n![](/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_1.png)\n\n<div class=\"content-ad\"></div>\n\nFif 2: 타이틀 발매 전후 자산 접근 패턴\n\n이 트렌드는 여러 자산 유형에 걸쳐 상당히 일관성있게 발견되었습니다. 런칭 이후 자산에 대한 접근이 드물기 때문에 저장 비용을 최적화할 수 있는 매력적인 기회가 발생합니다. 예를 들어, 자산은 초기 런칭 후 6개월 후에 아카이브로 이동될 수 있습니다. 이 정책은 시간이 지남에 따라 더 많은 데이터를 축적함으로써 더 지능적으로 발전하고 정확도를 키울 수 있도록 설계되었습니다.\n\n런칭 이후 자산에 대한 액세스가 매우 드문 점을 감안하면, 아카이브 준비가 된 자산을 AWS Glacier 스토리지로 이전할 수 있는 중요한 기회가 있습니다. AWS Glacier는 월간 저장 비용의 60% 낮은 비용으로 3-5시간 검색 시간을 제공합니다.\n\n# 기회 규모\n\n<div class=\"content-ad\"></div>\n\n저희의 가설은 보관 자산을 더 저렴한 저장 공간에 보관한다면 비용 절감 가능성이 매우 크다는 것입니다. 저희의 재무 및 데이터 파트너들은 현재 데이터 규모, 성장률, 그리고 미래 비즈니스 사용 사례를 기반으로 서로 다른 모델과 추정치를 개발하는 데 도움을 주었습니다.\n\n![Fig 3: 미디어 스토리지 비용 분석](/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_2.png)\n\n상태 쿼: 계속해서 진행한다면, 예상 비용 부담이 급격히 증가할 것으로 예상됩니다.\n\n<div class=\"content-ad\"></div>\n\n단기적으로는 사용되지 않는 자산을 보관하여 비용 효율적인 S3 Glacier 유연한 검색 스토리지로 이전하는 것이 50% 이상의 절감을 가져올 것입니다.\n\n장기적으로는 자산과 타이틀 라이프사이클의 다양한 단계(제작, 후기, 출시 이후 단계 등)에서 다양한 유형의 자산에 더 세부적인 라이프사이클 정책을 적용한다면 더 많은 절감을 이룰 수 있습니다.\n\n# 데이터 라이프사이클 전략\n\n데이터의 저장 계층을 결정하는 방법은 무엇일까요? 데이터 라이프사이클 관리의 첫 단계로 사용 패턴에 의존하는 간단한 전략을 선택했습니다. 사용에 근거한 이 접근은 단순하지만 상당한 가치를 제공합니다. 이 전략은 서비스에 프로그램이 출시된 후 특정 기간이 지난 후 자산을 낮은 비용의 아카이브 저장소로 이전하는 것을 포함하고 있습니다. 동시에 출시 후 임시 데이터는 정리되어 저장 리소스를 최적화하고 비용을 최소화합니다.\n\n<div class=\"content-ad\"></div>\n\n# 저장소 라이프사이클 관리 옵션 평가\n\n저희 콘텐츠 중심 도메인에서는 Amazon S3가 광범위한 미디어 콘텐츠를 호스팅하는 기반 역할을 합니다. 하지만 우리는 기본적인 것을 넘어서 S3 상에 견고하고 높은 확장성을 갖춘 저장소 인프라 계층을 구축했습니다. 이 복잡한 프레임워크는 우리의 거대한 미디어 파일을 안전하고 효율적으로 저장, 정리, 추적하며 글로벌 분산 스튜디오의 요구를 충족시키기 위해 설계되었습니다.\n\n자산 아카이빙 프로세스는 미래 참조를 위해 필요하지만 정기적으로 접근되지 않는 데이터를 기존의 고에너지를 사용하는 주 저장소 환경에서 더 에너지 효율적이고 저비용 아카이브로 전략적으로 이동하는 것을 포함합니다. 이 아카이빙 정책은 에너지 소비를 줄이고 고성능 저장 장치의 수명을 연장하는 데 기여합니다. 반대로, 삭제 정책은 오래된 또는 중복된 데이터를 체계적으로 삭제하고 저장 공간 요구 사항을 최적화하며 데이터 센터의 에너지 소비를 낮출 것을 목표로 합니다.\n\n아카이빙 솔루션을 찾는 동안 우리는 AWS S3의 Intelligent Tiering을 평가했습니다. S3 Intelligent Tiering은 탁월한 기본 제품이지만, 우리가 원하는 데이터에 대한 사용자 정의 세밀한 조정 기능이 부족합니다. 접근 패턴 통계로부터 우리는 포스트 프로덕션 단계의 여러 플레이어가 우리 데이터에 접근하는 방법에 대한 훨씬 더 풍부하고 상세한 데이터 세트를 얻었습니다. 이 지식을 활용하여 S3 Intelligent Tiering이 더 저렴한 저장 등급으로 객체를 이동하는 데 30일 감시를 기다리는 대신 보다 저렴한 저장 등급으로 페타바이트의 데이터를 적극적으로 아카이브할 수 있습니다. 우리는 또한 이 지식을 활용하여 데이터를보다 적극적으로 삭제하고 더 많이 절약할 수 있었습니다.\n\n<div class=\"content-ad\"></div>\n\nS3 위에 위치한 저장소 인프라레이어는 두 가지 입력을 기반으로 다른 저장 클래스 간에 데이터를 이동할 수 있습니다.\n\n- 엔드 유저 클라이언트 애플리케이션에서 지정한 정책.\n- 접근 패턴 통계에서 유도한 정책.\n\n# 저장소 라이프사이클 서비스 아키텍처\n\n넷플릭스의 저장소 라이프사이클 관리 아키텍처를 간단히 살펴봅시다. 넷플릭스의 콘텐츠 생성을 고려할 때, 자산은 제작 중에 생성된 미디어 파일 집합을 나타냅니다. 고수준에서 저장소 라이프사이클 아키텍처는 다음 구성 요소로 구성되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![Netflix's Media Landscape Evolution](/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_3.png)\n\n**Fig 4: Storage Lifecycle Manager Architecture**\n\n- **자산 관리 플랫폼**: Netflix에서 생산 및 후생산 단계의 미디어 자산 메타데이터를 저장하고 관리합니다. 자산 당 파일 수는 1개부터 1백만 개까지 다양합니다.\n- **정책 관리자**: 자산 및 임시 미디어 파일을 포함한 저장 객체의 라이프사이클 정책을 관리합니다. 이 서비스는 파일 저장을 효율적으로 관리하기 위해 다양한 정책 기반 자동 및 임시 작업을 지원합니다.\n- **콘텐츠 드라이브(Content Drive)**: 기존 파일 시스템 인터페이스를 사용하여 방대한 자산을 추적, 저장, 조직화, 관리하고 액세스 및 전송을 효과적으로 제어하기 위한 중앙화된 안전한 고도로 확장 가능한 솔루션을 제공합니다. 콘텐츠 드라이브는 미디어 파일의 상태에 대한 진리의 궁극적인 원천입니다. 임시로부터 우선순위가 높은 미디어 자산까지 다양한 미디어 자산은 시간이 지나거나 다른 버전에서 중요성을 갖습니다. 콘텐츠 드라이브는 데이터를 통해 우리의 이해를 높여주며, 자산 접근 패턴, 생산 관련 파일 수, 파일 크기 및 사용자 상호작용에 대한 통찰력을 제공합니다. 이 정보의 풍부함으로 생산 자산에 라이프사이클 정책을 정의하고 첨부하여 저장 풋프린트와 비용을 최적화할 수 있습니다.\n- **저장 라이프사이클 관리자**: 저장 라이프사이클 작업을 처리하고 조정하는데 사용되는 워크플로를 처리합니다. 주요 워크플로는 다음과 같습니다:\n  - Content Drive에 의해 관리되는 파일의 아카이빙을 AWS 아카이브 저장 계층으로 이동\n  - 아카이브된 파일의 복원. 파일은 다시 아카이빙되기 전에 특정 기간 동안 S3 표준 계층에서 사용 가능합니다.\n  - Content Drive에 의해 관리되는 파일의 삭제. 이것은 완전 삭제입니다.\n- **S3 객체 관리자**: 미디어 작업을 최적화하기 위해 S3 상에 구축된 추상화 계층입니다.\n\n이 게시물에서는 몇 가지 구성 요소를 높은 수준으로 다루고 다음 블로그 시리즈에서 자세한 아키텍처와 흐름을 다뤄 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 디자인 원칙\n\n- 확장성\n\n- 수십억 개의 파일을 처리할 수 있도록 설계되었으며, 모든 저장 객체가 아카이브하거나 삭제 대상이 될 수 있습니다.\n- 쇼 종료 후 대규모 아카이빙과 같은 시나리오를 다룰 수 있는 쓰러러닝 허드 요청을 관리할 수 있습니다.\n\n2. 내구성\n\n<div class=\"content-ad\"></div>\n\n- 저장 메타데이터의 데이터 무결성을 보장하고 안티 엔트로피 메커니즘을 활용합니다.\n- 적어도 한 번의 활동 보고를 보장하여 신뢰성을 강화합니다.\n\n3. 내구성\n\n- 재시도 메커니즘을 포함한 아카이브/복원/제거 작업에 대한 보장을 제공합니다.\n- 하루에 수천만 개의 파일 삭제 요구를 처리할 수 있는 기능으로 발전합니다.\n\n4. 보안\n\n<div class=\"content-ad\"></div>\n\n- 지정된 응용 프로그램만 라이프사이클 작업을 트리거할 수 있도록 인가하여 안전한 환경을 유지합니다.\n\n자산 라이프사이클 작업은 다음과 같습니다:\n\n- 클라우드 응용 프로그램은 자산을 저장 백엔드에 업로드합니다.\n- 제품 관리자와 응용 프로그램 소유자는 데이터 라이프사이클 정책을 정의하기 위해 Policy Manager API를 사용합니다. 해당 정책은 자산 유형에 적용되며, 예를 들어 제목 발매일로부터 180일 후에 일시적 자산을 삭제하거나 제작 후 30일 후에 최종 자산을 아카이브하는 등의 정책을 정의합니다.\n- 정책 관리자는 자산에 대한 정책을 만들고 관리할 수 있습니다. 정책 엔진은 정책 정의를 평가하고 어떤 자산이 라이프사이클 작업의 대상이 되는지 판단합니다. 이후 정책 실행 워커에 대한 작업을 대기열에 추가합니다. 정책 실행 워커는 Content Drive를 대상으로 아카이브/복원/삭제와 같은 작업을 예약(생성)합니다.\n- 정책 실행 시, Content Drive는 저장소 라이프사이클 관리자와의 라이프사이클 작업 실행을 예약합니다.\n- 모든 라이프사이클 작업은 저장소 라이프사이클 관리자에 지속됩니다.\n- 저장소 라이프사이클의 각 인스턴스에 대해 관리자는 실행할 자산을 평가하고 Content Drive에 실행 라이프사이클 이벤트를 전송합니다. Content Drive는 미디어 파일에 대한 충돌 작업 요청을 처리하기 위해 메타데이터 상태를 업데이트하며, 각 작업의 대상이 되는 자산 목록을 수집하고 이를 저장소 라이프사이클 관리자로 전송합니다.\n- 필요한 처리량을 달성하기 위해 비동기 작업은 클러스터의 모든 노드에 균등하게 분산되며, 특정 작업을 처리하는 노드가 하나뿐입니다. 작업을 공유함으로써 병렬성을 달성하고, 이 솔루션은 데이터베이스 트랜잭션 충돌을 피하기도 합니다. Kafka를 사용하여 비동기 작업을 \"소유\"하는 노드로 이동합니다. Kafka의 리더 선출은 클러스터 전체에 균등하게 소유권을 설정하는 데 사용됩니다. Kafka는 최소 한 번의 메시징과 내구성 보장을 제공합니다.\n- 저장소 라이프사이클 관리자는 S3 Object Manager를 사용하여 동시에 객체를 다른 저장 티어로 이동/삭제하고 비동기 완료/실패 이벤트를 기다립니다.\n- 저장소 라이프사이클 관리자는 작업 완료를 모니터링하고 작업 완료 이벤트를 생성합니다. 실패한 이벤트는 작업 상태를 실패로 반영하기 전에 여러 번 다시 시도됩니다.\n- S3 Object Manager는 작업 완료 이벤트를 처리하고 미디어 파일/폴더에 대한 메타데이터 상태 업데이트로 변환한 후 미디어 자산에 대한 라이프사이클 변경 완료 이벤트를 생성합니다. 스튜디오 응용 프로그램은 저장소 관리자에서 생성된 완료 이벤트에 가입하고 업무 업데이트를 최종 사용자에게 보냅니다.\n\n# 저장소 아카이빙 통계\n\n<div class=\"content-ad\"></div>\n\n저장 수명주기 관리를 위해 Policy manager와 함께 생산 환경에서 자동화 작업을 시작했습니다. 이렇게 생긴 저장 수명주기 대시보드의 중요한 예시 몇 가지가 있습니다:\n\n![그림 5: 하루에 아카이브된 파일 수(백만 단위)](/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_4.png)\n\n![그림 6](/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_5.png)\n\n<div class=\"content-ad\"></div>\n\nFig 6: Files Archived in a backfill job\n\n# Netflix의 저장소 수명주기 진화\n\n기존의 사용 패턴 및 콘텐츠 저장 및 보존 복잡성을 고려하여, Netflix는 많은 미디어 자산을 효율적이고 비용 효율적이며 지속 가능하게 관리하는 방법을 개척하려고 합니다. 곧 우리는 정책 관리자를 개선하여 모든 미디어 저장소 자산 및 임시 파일에 대한 데이터 수명주기 정책을 자동화할 계획입니다. 이 전략적인 움직임은 운영 효율성을 향상시키고 콘텐츠 관리 분야의 기술 발전을 선도하기 위한 저희의 약속과 일치합니다. 장기적으로는 데이터 수명주기 관리 솔루션을 자동화하고 확장하며, 우리의 목표는 충돌과 우선순위 뿐만 아니라 Netflix의 하이브리드 저장소의 데이터 수명주기 관리도 처리할 수 있는 정책 관리자를 개선하는 것입니다.\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n시스템화된 데이터 라이프사이클 관리는 높은 효율성, 비용 효율성 및 확장 가능한 저장 솔루션을 제공하기 위해 반드시 고려되어야 합니다. 이는 클라우드 또는 하이브리드 저장소를 위한 새로운 워크플로우의 설계나 아키텍처에 포함되어야 합니다.\n\nNetflix에서 전형적인 실사 제작은 후속 제작 단계에서 단독으로 20,000개에서 80,000개의 에셋을 도출할 수 있으며, 이는 수백 테라바이트에 달하는 데이터량으로 이어집니다. 에셋 라이프사이클 정책에 따라 저렴한 저장 계층으로 아카이빙하여 약 70%의 비용 절감을 달성할 수 있습니다.\n\n초기 단계에서의 성공을 고려할 때, 우리는 시스템의 능력을 향상시키고 있으며, 저장 계층과 정책 관리 계층을 모두 포함하여 두 가지 측면에서 에셋 라이프사이클 정책을 확대하고 있습니다:\n\n- 제목 수명주기의 다른 단계에서 생성된 다양한 유형의 에셋에 대한 범위를 확장합니다.\n- 완전한 라이프사이클을 정의하고 아카이빙되었을 수도, 아닐 수도 있는 에셋을 정리합니다.\n- 정책 관리자를 확장하여 온프레미스, NFS, FSX, EBS 등 하이브리드 저장 환경의 데이터 계층화 및 라이프사이클 관리를 지원합니다.\n\n<div class=\"content-ad\"></div>\n\n# 감사의 말\n\nVinod Viswanathan, Sera Leggett, Obi-Ike Nwoke, Yolanda Cheung, Olof Johansson, Shailesh Birari, Patrick Prothro, Gregory Almond, John Zinni, Chantel Yang, Vikram Singh, Emily Shaw, Abi Kandasamy, Zile Liao, Jessica Gutierrez, Shunfei Chen 같은 멋진 동료들에게 특별히 감사드립니다.\n\n# 용어\n\nLTO: Linear Tape-Open의 약자로, 백업, 아카이빙 및 데이터 전송에 주로 사용되는 기술입니다.\n\n<div class=\"content-ad\"></div>\n\nVFX: 비주얼 이펙트의 약자로, 라이브 액션 미디어를 위해 비디오나 이미지를 수정하는 것을 포함합니다.\n\nOCF: 오리지널 카메라 푸티지의 약자로, 필름 카메라에 의해 처음으로 촬영된 원본이며 편집되지 않은 콘텐츠를 나타냅니다.","ogImage":{"url":"/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_0.png"},"coverImage":"/assets/img/2024-05-20-NetflixsMediaLandscapeEvolutionFrom321toCloudStorageOptimization_0.png","tag":["Tech"],"readingTime":11},{"title":"DevOps 웃고 배우고 반복하기 - Bhavesh","description":"","date":"2024-05-20 16:49","slug":"2024-05-20-DevOpsLaughLearnRepeatByBhavesh","content":"\n\n안녕하세요, 저는 클라우드 솔루션 아키텍트가 되기를 희망하는 바베시 아난드파라입니다. 대학생들과 마찬가지로, 이 새로운 기술에 압도당하고 완전히 탐험하는 데 어려움을 겪었습니다.\n\n그래서 저는 데브옵스를 탐험하고 제 학습 과정을 Medium 블로그 시리즈를 통해 문서화해보려고 합니다.\n\n이 아이디어는 이러한 개념을 정말로 잘 파악한 다음, 제 고유한 말로 설명하는 것입니다.\n\n매우 중요한 참고사항:\n\n<div class=\"content-ad\"></div>\n\n만약 DevOps를 시작하려는데 압도당하는 느낌이 들거나 항상 시작하고 싶었지만 어려움을 겪었다면, 학생의 시각에서 컨셉에 대한 아이디어를 얻기 위해 따라해 볼 수 있어요.\n\nDevOps에 뛰어들기 전에 시스템 디자인의 기본 개념을 이해하는 것이 중요하다고 생각해요.\n\n저는 대학생을 위해 초보자 친화적인 시스템 디자인 시리즈를 준비했어요. 이를 읽고 시스템 디자인의 기본에 대해 알아보세요.\n\n내용:\n\n<div class=\"content-ad\"></div>\n\n지금 시스템 디자인 시리즈를 준비하고 있어요. 기대해주세요! 곧 이 시리즈를 시작할 거에요! 😉","ogImage":{"url":"/assets/img/2024-05-20-DevOpsLaughLearnRepeatByBhavesh_0.png"},"coverImage":"/assets/img/2024-05-20-DevOpsLaughLearnRepeatByBhavesh_0.png","tag":["Tech"],"readingTime":1}],"page":"68","totalPageCount":98,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}