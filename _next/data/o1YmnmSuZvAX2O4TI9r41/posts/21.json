{"pageProps":{"posts":[{"title":"아저라 데이터브릭스 SQL 웨어하우스 인스턴스의 실제 비용을 계산하는 방법","description":"","date":"2024-06-19 12:28","slug":"2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances","content":"\n\n<img src=\"/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_0.png\" />\n\n이야기에서는 Azure Databricks SQL Warehouse 인스턴스의 비용을 계산하는 방법에 대해 배워보겠습니다.\n\nDatabricks SQL Warehouse는 Azure Databricks에서 데이터를 쿼리하고 탐색할 수 있는 컴퓨팅 리소스입니다.\n\n현재 Azure Databricks에서는 3가지 유형의 SQL Warehouse를 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n- SQL Warehouse Classic: SQL Warehouse Classic의 컴퓨팅 레이어는 저희 Azure 구독 계정에 존재하며 Photon을 지원하지만 Predictive IO나 Intelligent Workload Management은 지원하지 않습니다.\n- SQL Warehouse Pro: SQL Warehouse Pro의 컴퓨팅 레이어는 저희 Azure 구독 계정에 존재하며 Photon과 Predictive IO를 지원하지만 Intelligent Workload Management는 지원하지 않습니다.\n- SQL Warehouse Serverless: Azure Databricks 서버리스 아키텍처를 사용하여 Databricks SQL Warehouse Serverless가 Azure Databricks 계정에 존재하며 Databricks SQL의 모든 성능 기능(Phton, Predictive IO 및 Intelligent Workload Management)을 지원합니다.\n\n위 목록에서 볼 수 있듯이, SQL Warehouse Classic과 SQL Warehouse Pro 사이의 가장 중요한 차이점은 컴퓨팅 레이어가 저희 Azure 구독 계정에 있고, SQL Warehouse Serverless가 저희 Azure Databricks 계정에 있다는 것입니다.\n\n## 관련 이야기:\n- Microsoft 및 Databricks API를 사용하여 Azure Databricks 클러스터의 비용을 최적화하고 90%까지 줄이는 방법\n\n<div class=\"content-ad\"></div>\n\n# 1. DIY 방법\n\n첫 번째 섹션은 비용 분석 논리를 이해하고 아마도 자신만의 도구나 스크립트를 작성하여 Azure Databricks SQL Warehouse 인스턴스의 비용을 계산하고자 하는 개발자 또는 기술 직군을 위해 제공됩니다.\n\n# 1.1. DIY 방법 — SQL Warehouse Classic\n\nAzure Databricks SQL Warehouse Classic 또는 Azure Databricks SQL Warehouse Pro의 비용을 계산하려면 다음 구성 요소를 고려해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n- SQL 계산 시간: SQL Warehouse가 실행되었던 시간.\n- Databricks Unit (DBU) 시간: SQL Warehouse에서 사용된 컴퓨팅 단위가 시간당 청구됩니다.\n- 저장 비용: 적용된 경우 데이터 저장에 연관된 비용.\n- 대역폭 비용: 적용된 경우 대역폭 전송에 연관된 비용.\n\n# 1.1.1. Databricks API에서 SQL Warehouse Classic 인스턴스 목록 가져 오기\n\n첫 번째 단계는 Databricks API \"/api/2.0/sql/warehouses\"를 사용하여 SQL Warehouse 인스턴스 목록을 검색하는 것입니다.\n\nAPI 호출을 실행한 후, 모든 Databricks SQL Warehouse가 포함 된 JSON 응답을 받게됩니다.\n\n<div class=\"content-ad\"></div>\n\nSQL Warehouse Classic의 JSON은 다음과 같습니다:\n\n```js\n\"warehouses\":[\n{\n  \"id\":\"2b1613d995c81e7d\",\n  \"name\":\"Classic Warehouse\",\n  \"size\":\"XXSMALL\",\n  \"cluster_size\":\"2X-Small\",\n  \"min_num_clusters\":1,\n  \"max_num_clusters\":1,\n  \"auto_stop_mins\":45,\n  \"auto_resume\":true,\n  \"creator_name\":\"terraform@kopicloud.net\",\n  \"creator_id\":1036471128901988,\n  \"tags\":{ },\n  \"spot_instance_policy\":\"COST_OPTIMIZED\",\n  \"enable_photon\":true,\n  \"channel\":{\n    \"name\":\"CHANNEL_NAME_CURRENT\"\n  },\n  \"enable_serverless_compute\":false,\n  \"warehouse_type\":\"CLASSIC\",\n  \"num_clusters\":0,\n  \"num_active_sessions\":0,\n  \"state\":\"STOPPED\",\n  \"jdbc_url\":\"jdbc:spark://adb-xxxxxxxx.x.azuredatabricks.net:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/2b1613d995c81e7d;\",\n    \"odbc_params\":{\n      \"hostname\":\"adb-xxxxxxxx.x.azuredatabricks.net\",\n      \"path\":\"/sql/1.0/warehouses/2b1613d995c81e7d\",\n      \"protocol\":\"https\",\n      \"port\":443\n    }\n   }\n  }\n]\n```\n\n위와 유사한 데이터를 얻을 수 있습니다(쉽게 이해할 수 있도록 형식화됨)\n\n<img src=\"/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_1.png\" />\n\n<div class=\"content-ad\"></div>\n\n# 1.1.2. Microsoft API를 통해 SQL Warehouse Classic 자원의 비용 가져오기\n\n검색한 사용 데이터와 비용 정보를 결합하여 SQL Warehouse Classic의 비용을 계산해야 합니다.\n\n저희는 Microsoft Generate Cost Details Report API를 사용하여 Databricks 클러스터가 실행되는 Azure 구독에 대한 모든 데이터를 가져올 것입니다.\n\nAPI를 사용할 때 주의할 점들:\n\n<div class=\"content-ad\"></div>\n\n- Azure 구독의 비용 세부 정보 데이터를 가져올 때, Databricks와 관련된 데이터만 필터링하고 다른 서비스와 비용이 연관되지 않은 데이터는 제거해야 합니다.\n- 결과를 Pay-as-you-go 또는 Enterprise 유형의 Azure 구독에 따라 조정해야 하며, 결과가 서로 다릅니다.\n- API는 한 달 이하의 데이터만 가져오도록 허용하며 13개월 이전의 데이터는 제공하지 않습니다.\n\nMicrosoft API에서 데이터를 추출할 때, 보고서에서 다음 열을 선택해야 합니다:\n- SqlEndpointId: SQL Warehouse 인스턴스의 ID\n- ProductName: 사용된 Azure 리소스의 설명\n- MeterName: 청구되는 서비스 또는 리소스 유형을 식별하는 레이블\n- CostInBillingCurrency: 사용된 Azure 리소스의 비용\n- BillingCurrency: 청구 통화 코드 (USD, EUR 등)\n\n# 1.1.3. SQL Warehouse 클래식 인스턴스 비용 계산\n\n<div class=\"content-ad\"></div>\n\nMicrosoft Databricks 및 Azure API에서 데이터를 검색한 후, 마지막 단계는 SQL Endpoint ID를 사용하여 Azure 비용 데이터를 필터링하고 Databricks API의 SQL Endpoint ID와 일치시키는 것입니다.\n\n우리는 이와 유사한 데이터를 받을 것입니다 (이해하기 쉽게 형식화된 데이터입니다):\n\n![이미지](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_2.png)\n\nSQL Warehouse Classic 인스턴스에서 제품은 Azure Databricks - Premium - SQL Analytics이며, 미터 이름은 Premium SQL Analytics DBU입니다.\n\n<div class=\"content-ad\"></div>\n\n그럼, SQL Warehouse 인스턴스의 최종 비용을 계산하기 위해 모든 레코드를 추가합니다.\n\n# 1.2. 직접 계산 방법 — SQL Warehouse Pro\n\nAzure Databricks SQL Warehouse Pro의 비용을 계산하기 위해 다음 구성 요소를 고려해야 합니다:\n\n- SQL 컴퓨트 시간: SQL Warehouse가 실행된 시간입니다.\n- 데이터브릭스 유닛(DBU) 시간: SQL Warehouse에서 사용된 컴퓨트 유닛으로, 매 시간마다 청구됩니다.\n- 저장 비용: 데이터 저장에 관련된 비용(해당하는 경우).\n- 대역폭 비용: 대역폭 전송에 관련된 비용(해당하는 경우).\n\n<div class=\"content-ad\"></div>\n\nAPI 호출을 실행한 후, 모든 Databricks SQL Warehouse에 대한 JSON 응답을 받게 됩니다.\n\n다음은 SQL Warehouse Pro의 JSON입니다:\n\n```js\n\"warehouses\":[\n{\n  \"id\":\"aedd502a582f673a\",\n  \"name\":\"Starter Warehouse\",\n  \"size\":\"XXSMALL\",\n  \"cluster_size\":\"2X-Small\",\n  \"min_num_clusters\":1,\n  \"max_num_clusters\":1,\n  \"auto_stop_mins\":10,\n  \"auto_resume\":true,\n  \"creator_name\":\"terraform@kopicloud.net\",\n  \"creator_id\":1036471128901988,\n  \"tags\":{ },\n  \"spot_instance_policy\":\"COST_OPTIMIZED\",\n  \"enable_photon\":true,\n  \"channel\":{ },\n  \"enable_serverless_compute\":false,\n  \"warehouse_type\":\"PRO\",\n  \"num_clusters\":0,\n  \"num_active_sessions\":0,\n  \"state\":\"STOPPED\",\n  \"jdbc_url\":\"jdbc:spark://adb-xxxxxxxx.x.azuredatabricks.net:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/aedd502a582f673ad;\",\n    \"odbc_params\":{\n      \"hostname\":\"adb-xxxxxxxx.x.azuredatabricks.net\",\n      \"path\":\"/sql/1.0/warehouses/aedd502a582f673a\",\n      \"protocol\":\"https\",\n      \"port\":443\n    }\n   }\n  }\n]\n```\n\n이와 유사한 데이터를 얻게 될 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_3.png)\n\n# 1.2.2. Microsoft API를 통한 SQL Warehouse PRO 자원 비용 가져오기\n\nSQL Warehouse Pro 비용을 계산하기 위해서는 검색된 사용 데이터와 비용 정보를 결합하여 총 비용을 계산해야 합니다.\n\nMicrosoft Generate Cost Details Report API를 사용하여 Databricks 클러스터가 실행 중인 Azure 구독에 대한 모든 데이터를 가져올 것입니다.\n\n<div class=\"content-ad\"></div>\n\nAPI를 사용할 때 중요한 사항:\n\n- Azure 구독에 대한 비용 세부 데이터를 가져올 때, Databricks와 관련된 데이터만 필터링하고 다른 서비스와 관련된 데이터 또는 비용이 연관되지 않은 데이터를 제거해야 합니다.\n- Azure 구독 유형(유연한 요금제 또는 기업용)에 따라 결과를 조정해야 합니다. 왜냐하면 출력 결과물이 다르기 때문입니다.\n- API는 한 달 이하의 데이터만 가져올 수 있으며 13개월 이전의 데이터는 가져올 수 없습니다.\n\nMicrosoft API에서 데이터를 추출할 때, 보고서에서 다음 열을 선택해야 합니다.\n\n- SqlEndpointId: SQL Warehouse 인스턴스의 ID\n- ProductName: 사용된 Azure 리소스의 설명\n- MeterName: 청구되는 서비스 또는 리소스 유형을 식별하는 레이블\n- CostInBillingCurrency: 사용된 Azure 리소스의 비용\n- BillingCurrency: 청구 통화 코드 (USD, EUR 등)\n\n<div class=\"content-ad\"></div>\n\n# 1.2.3. SQL Warehouse PRO Instances 비용 계산하기\n\nMicrosoft Databricks와 Azure API에서 데이터를 검색한 후, 최종 단계는 SQL Endpoint ID를 사용하여 Azure 비용 데이터를 필터링하고 Databricks API에서 SQL Endpoint ID와 일치시키는 것입니다.\n\n다음과 유사한 데이터를 얻게 됩니다:\n\n![이미지](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_4.png)\n\n<div class=\"content-ad\"></div>\n\nSQL Warehouse Pro 인스턴스에서 Product가 Azure Databricks Regional — Premium — SQL Compute Pro이고 MeterName이 Premium SQL Compute Pro DBU인 경우, SQL Warehouse 인스턴스의 최종 비용을 계산하기 위해 모든 레코드를 추가합니다.\n\n# 1.3. 직접 만들기 방법 — SQL Warehouse Serverless\n\nAzure Databricks SQL Warehouse Serverless 인스턴스는 Azure Databricks 계정 내에서 실행되므로 Databricks Unit (DBU) 시간 단위로 청구됩니다.\n\n<div class=\"content-ad\"></div>\n\nAPI 호출을 실행한 후, Databricks SQL Warehouse에 관한 JSON을 받게 됩니다.\n\n다음은 Serverless SQL Warehouse의 JSON입니다:\n\n```js\n\"warehouses\":[\n{\n  \"id\":\"e6e7601bde7c3a43\",\n  \"name\":\"Serveless Warehouse\",\n  \"size\":\"XXSMALL\",\n  \"cluster_size\":\"2X-Small\",\n  \"min_num_clusters\":1,\n  \"max_num_clusters\":1,\n  \"auto_stop_mins\":10,\n  \"auto_resume\":true,\n  \"creator_name\":\"terraform@kopicloud.net\",\n  \"creator_id\":1036471128901988,\n  \"tags\":{ },\n  \"spot_instance_policy\":\"COST_OPTIMIZED\",\n  \"enable_photon\":true,\n  \"enable_serverless_compute\":true,\n  \"warehouse_type\":\"PRO\",\n  \"num_clusters\":0,\n  \"num_active_sessions\":0,\n  \"state\":\"STOPPED\",\n  \"jdbc_url\":\"jdbc:spark://adb-xxxxxxxx.x.azuredatabricks.net:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/e6e7601bde7c3a43;\",\n    \"odbc_params\":{\n      \"hostname\":\"adb-xxxxxxxx.x.azuredatabricks.net\",\n      \"path\":\"/sql/1.0/warehouses/e6e7601bde7c3a43\",\n      \"protocol\":\"https\",\n      \"port\":443\n    }\n   }\n  }\n]\r\n```\n\n이처럼 유사한 데이터를 얻을 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_5.png\" />\n\n# 1.3.2. Microsoft API를 통해 SQL Warehouse Serverless 리소스 비용 얻기\n\n검색된 사용량 데이터를 비용 정보와 결합하여 SQL Warehouse Serverless 비용을 계산해야 합니다.\n\nDatabricks 클러스터가 실행 중인 Azure 구독의 모든 데이터를 가져 오기 위해 Microsoft Generate Cost Details Report API를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nAPI를 사용할 때 중요한 사항:\n\n- Azure 구독에 대한 비용 세부 내역 데이터를 가져올 때, Databricks와 관련된 데이터만 필터링하고 다른 서비스와 관련 없는 데이터를 제거해야 합니다.\n- 결과를 조정해야 하는 이유는 Azure 구독 유형(사용한 만큼 지불 또는 기업 서비스)에 따라 출력이 다르기 때문입니다.\n- API는 한 달 이하의 데이터만 검색할 수 있으며 13개월 이전의 데이터는 가져올 수 없습니다.\n\nMicrosoft API에서 데이터를 추출할 때 다음 보고서 열을 선택해야 합니다:\n\n- SqlEndpointId: SQL Warehouse 인스턴스의 ID\n- ProductName: 사용한 Azure 리소스 설명\n- MeterName: 청구되는 서비스 또는 리소스 유형을 식별하는 레이블\n- CostInBillingCurrency: 사용한 Azure 리소스 비용\n- BillingCurrency: 청구 통화 코드 (USD, EUR 등)\n\n<div class=\"content-ad\"></div>\n\n# 1.3.3. SQL Warehouse Serveless 인스턴스 비용 계산하기\n\nMicrosoft Databricks와 Azure API에서 데이터를 검색한 후, 최종 단계는 SQL Endpoint ID를 사용하여 Azure 비용 데이터를 필터링하고 Databricks API에서 SQL Endpoint ID를 매칭하는 것입니다.\n\n우리가 얻게 되는 데이터는 이와 비슷할 것입니다:\n\n![이미지](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_6.png)\n\n<div class=\"content-ad\"></div>\n\nSQL 웨어하우스 서버러스 인스턴스에서, 제품은 Azure Databricks Regional — Premium — Serverless SQL이고 미터 이름은 프리미엄 서버러스 SQL DBU입니다.\n\n그런 다음 SQL 웨어하우스 인스턴스의 최종 비용을 계산하기 위해 모든 레코드를 추가합니다.\n\n## 2. 쉬운 방법\n\nSQL 웨어하우스의 비용을 분 단위로 결정해야하는 경우, KopiCloud Azure Databricks 비용 도구를 사용하는 것이 쉽습니다.\n\n<div class=\"content-ad\"></div>\n\n내가 API 데이터를 가져오고 필터링하며 추출하는 경험을 토대로, Microsoft 및 Databricks API에서 검색한 데이터를 읽고 관리하는 프로세스를 간소화하기 위해 이 도구를 개발했습니다.\n\n이 도구는 간단한 사용자 인터페이스를 사용하며, 몇 분 내에 서식이 지정된 데이터를 검색하려는 FinOps 전문가들을 위해 설계되었습니다.\n\n먼저 \"Databricks 비용 찾아보기\" 버튼을 클릭합니다.\n\n도구는 Microsoft 및 Databricks API와 연결하고 화면 및 Excel 파일에서 서식이 지정된 데이터를 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_7.png](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_7.png)\n\n두 번째로 마지막 단계에서는 Cost per SQL Warehouse Report 버튼을 클릭하여 SQL Warehouse 및 해당 비용 목록을 생성합니다.\n\n이 도구는 화면에 정보를 표시하고 Excel 파일로 내보냅니다.\n\n![2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_8.png](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_8.png)\n\n<div class=\"content-ad\"></div>\n\n도구는 모든 데이터를 Excel 파일로 출력하여 데이터를 조작하거나 사용자 정의 보고서를 작성할 수 있도록 합니다.\n\n![이미지](/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_9.png)\n\n또한 도구는 원시 및 형식화된 데이터를 생성하고 일일 및 총 일일 비용을 로컬 스토리지나 Azure Blob Storage에 저장하여 사용자 정의 PowerBI 보고서를 생성할 수 있습니다.\n\n그게 다에요. 만약 이 이야기가 마음에 드셨다면 👏을 눌러주세요. 읽어 주셔서 감사합니다!\n\n<div class=\"content-ad\"></div>\n\n- KopiCloud Azure Databricks Cost 도구는 KopiCloud 웹사이트에서 다운로드할 수 있어요.\n- 만약 Databrick 클러스터 비용을 줄이는 데 도움이 필요하다면 Linkedin에서 저에게 연락해 주세요.\n- 게시된 이미지는 Flaticon의 Dewi Sari가 만든 Cost 아이콘을 사용하여 생성되었어요.\n\n## 관련 이야기:\n\n- Microsoft 및 Databricks API를 사용하여 Azure Databricks 클러스터 비용을 최대 90%까지 최적화하고 줄이는 방법","ogImage":{"url":"/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_0.png"},"coverImage":"/assets/img/2024-06-19-HowToCalculatetheRealCostofAzureDatabricksSQLWarehouseInstances_0.png","tag":["Tech"],"readingTime":12},{"title":"눈꽃 폴라리스와 데이타브릭스 유니티 카탈로그 오픈 및 상호 운용 가능한 메타스토어 시대","description":"","date":"2024-06-19 12:26","slug":"2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores","content":"\n\n<img src=\"/assets/img/2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_0.png\" />\n\n이번 달에는 두 대형 클라우드 데이터 플랫폼인 Snowflake와 Databricks에서 비슷한 성격의 주요 발표가 이뤄졌습니다.\n\n6월 초 한 주쯤에, Snowflake는 매년 개최되는 컨퍼런스에서 Apache Iceberg 위에 구현된 오픈 카탈로그인 Polaris Catalog를 발표했습니다. 그리고 그 한 주 뒤에, Databricks가 데이터 거버넌스를 위한 통합 솔루션을 제공하는 Unity Catalog 제품을 오픈소스로 공개했습니다. 이 짧은 기사에서 자세히 살펴보도록 하겠습니다.\n\n# 데이터 호수(Datalake)에서의 메타데이터 카탈로그\n\n<div class=\"content-ad\"></div>\n\n![SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_1.png](/assets/img/2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_1.png)\n\n이 제품에 대해 파헤치기 전에, 데이터 레이크의 맥락에서 정의에 대해 먼저 간단히 논의해보겠습니다. 메타데이터 카탈로그, 또는 메타스토어라고도 알려진 것은 데이터셋을 테이블로 표현하여 객체 저장소에 있는 데이터에 대한 추상화 레이어를 만듭니다. 데이터에 대한 접근은 메타스토어를 통해 관리되며, 상호 작용을 테이블에 저장된 것처럼 변환하여 저장소에서 필요한 작업을 수행합니다.\n\n# Databricks Unity Catalog\n\n처음 접하는 분들을 위해 - 2013년 Apache Spark의 창시자들에 의해 설립된 Databricks는 데이터 레이크와 데이터 웨어하우스를 결합하여 기업이 데이터 및 AI 솔루션을 구축, 관리 및 확장하는 데 도움이 되는 클라우드 기반 데이터 인텔리전스 플랫폼입니다.\n\n<div class=\"content-ad\"></div>\n\n약 2021년 중반쯤, 유니티 카탈로그가 독점 소스로 출시되었습니다. 이는 플랫폼 에코시스템 내에서 데이터 및 AI 자산을 접근하고 관리하기 위한 솔루션이었습니다. 이는 중앙 집중식 접근 제어, 감사, 계보, 공유 및 데이터 발견 기능과 같은 여러 기능을 제공했습니다.\n\n![이미지](/assets/img/2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_2.png)\n\n오픈 테이블 형식 (OTFs)인 Iceberg, Delta 및 Hudi가 인기를 얻으면서, 주요 데이터 플랫폼 공급 업체들은 이 세 가지 중 하나를 선택해야 했습니다. 당연히 Databricks의 창립자로서 델타 레이크가 주요 형식으로 선택되었습니다.\n\n그러나 오픈 델타 형식을 사용하는 플랫폼과의 밀접한 결합은 Apache Iceberg 또는 Hudi와 호환되는 쿼리 엔진과의 상호 운용성이 제한되었음을 의미했습니다. Databricks가 이 문제를 해결하기 위한 최초의 시도는 Delta UniForm이었습니다. 이는 복사 또는 변환의 필요성을 제거하고 레이크하우스 상호 운용성을 위한 범용 형식을 제공했습니다.\n\n<div class=\"content-ad\"></div>\n\n최신 발표에 따르면, Unity Catalog를 오픈 API와 아파치 2.0 라이센스로 빌드한 오픈 소스 서버로 제공하는 Databricks가 기업에게 오픈 데이터 형식(UniForm을 통한)을 지원하며 다양한 쿼리 엔진, 도구, 클라우드 플랫폼 간의 상호 운용성을 제공하는 범용 인터페이스를 제공하여 다음 수준으로 나아갔습니다.\n\n# Snowflake Polaris Catalog\n\n2012년 개발된 Snowflake는 데이터 웨어하우징, 데이터 레이크, 데이터 엔지니어링 및 데이터 과학을 위한 완전히 관리되는 SaaS 플랫폼입니다. Snowflake는 스토리지와 컴퓨팅의 분리, 온디맨드 확장 가능한 컴퓨팅, 데이터 공유, 데이터 복제, 그리고 성장하는 기업의 요구를 처리하기 위한 타사 도구 지원과 같은 기능들을 제공합니다.\n\nSnowflake는 자사의 프로프라이어터리 테이블 형식을 시작한 후, 재미있게도 얼마 전부터 Apache Iceberg와의 통합에 헌신하여 왔습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Snowflake Polaris and Databricks Unity Catalog](/assets/img/2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_3.png)\n\n최근 출시된 Polaris 카탈로그는 Iceberg의 오픈 소스 REST 프로토콜을 기반으로 하여 사용자가 Iceberg Rest API를 지원하는 Apache Spark, Flink, Trino 등과 같은 원하는 엔진을 사용하여 데이터에 액세스하고 검색할 수 있는 오픈 표준을 제공합니다. Polaris는 다음 90일간(약 2024년 4분기) 오픈 소스로 공개될 예정입니다.\n\n# 개방성은 호환성을 의미하지 않을 수 있습니다\n\n이러한 프로젝트들을 오픈 소스 Apache 이니셔티브로 오픈하는 것은 긍정적인 단계이지만, 데이터 솔루션 아키텍처적인 측면에서 보면, 코드를 오픈할 필요는 없고 오히려 노출되는 인터페이스가 오픈되어야 합니다. 이 글을 쓰는 동안, Polaris는 원래 Iceberg만을 지원하며, Unity는 UniForm을 사용하여 네이티브 Delta 이외의 다른 OTF(Open Table Format)를 간접적으로 지원하고, Tabular 인수 이후 Iceberg와의 새로운 통합을 수행하고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\"이상적인 '오픈' 정의에서 이 메타스토어는 모든 표준 테이블 형식을 지원하고, 구성 가능한 저장 계층과 Hive Metastore가 한동안 있었던 것과 같은 메타스토어에 대한 표준 인터페이스를 가져야 합니다.\n\n# 결론\n\n대부분의 기업은 벤더 락인을 피하면서 데이터 생태계를 더 열린, 유연한, 상호 운용 가능한 것으로 하고 싶어합니다. 데이터브릭스 없이 유니티 카탈로그를 구현하거나, 스노우플레이크 없이 폴라리스를 사용하는 것은 흥미로운 전망입니다. 엔지니어링 팀은 이제 이러한 기능을 구매한 플랫폼이나 컨테이너를 사용하여 자체 인프라에서 독립적으로 호스팅하는 유연성을 가지게 되었습니다. 다가오는 몇 달 동안, 개방 커뮤니티의 협력으로 이 제품들의 성장과 채택을 지켜보는 것이 흥미로울 것입니다.\"","ogImage":{"url":"/assets/img/2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_0.png"},"coverImage":"/assets/img/2024-06-19-SnowflakePolarisandDatabricksUnityCatalogTheageofOpenandInteroperableMetastores_0.png","tag":["Tech"],"readingTime":4},{"title":"마이크로소프트 패브릭과 데이타브릭스 유니티 카탈로그 - 통합 시나리오 해석하기","description":"","date":"2024-06-19 12:25","slug":"2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios","content":"\n\n올해 첫 번째 Microsoft Fabric 커뮤니티 컨퍼런스가 개최되었습니다. 첫 번째 날 키노트에서 Fabric와 Databricks Unity Catalog (UC) 통합을 쇼케이스하는 두 가지 미리보기가 있었어요.\n\n이전 블로그 게시물에서는 Databricks에서 OneLake로 쓰는 옵션 및 Fabric Spark에서 ADLS Gen2로 쓰는 옵션(Unity Catalog가 활성화된 클러스터가 아닌 경우)에 대해 알아보았습니다. 이 블로그 게시물은 Fabric + Unity Catalog 통합(Unity Catalog가 활성화된 클러스터와 관련된 다양한 시나리오를 밝히는 것을 목표로 합니다. Lakehouse 시나리오에 대한 자세한 내용은 Piethein의 게시물에서 확인해주세요.\n\n- Unity Catalog 테이블을 OneLake 카탈로그로 동기화할 수 있을까요? 어떻게요?\n- Unity Catalog가 활성화된 클러스터에서 OneLake로 쓸 수 있을까요?\n- Unity Catalog를 OneLake와 통합할 수 있을까요? SQL 엔드포인트 / Fabric 데이터 웨어하우스에 대해 페더레이티드 쿼리를 실행할 수 있을까요?\n\n참고: 본 글은 제 개인적인 경험과 견해를 반영한 것이며, Microsoft나 Databricks의 공식 입장을 대변하는 것은 아닙니다. 또한, 이 블로그 게시물은 잠재적인 시나리오를 개요로 제시하였지만, Fabric 로드맵이나 의도를 반영하는 것은 아닙니다. 미래에 모든 언급된 옵션이 운영되지는 않을 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Databricks Unity Catalog and Microsoft Fabric\n\n통합 시나리오는 기본적으로 진입점에 따라 볼 수 있습니다. Unity Catalog 및 Fabric:\n\n- Fabric에서 Unity Catalog에 액세스하기 (Fabric → Unity Catalog): 이 기능을 통해 사용자는 Fabric 내에서 Unity Catalog 카탈로그, 스키마 및 테이블에 원활하게 액세스할 수 있습니다.\n- Unity Catalog에서 Fabric 활용하기 (DBX/Unity Catalog → Fabric): 이 기능을 사용하면 사용자는 Unity Catalog 내부에서 OneLake에 직접 액세스하고 SQL 엔드포인트 또는 Fabric 데이터 웨어하우스를 통해 연합 쿼리를 실행할 수 있습니다.\n\n이러한 시나리오를 자세히 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# Fabric → Unity Catalog\n\n## Fabric에서 Unity Catalog 사용하기\n\nFabric에서 Unity Catalog 테이블에 액세스할 수 있는 몇 가지 옵션이 있습니다. Fabric Spark에서 ADLS Gen2로 직접 읽기 및 쓰기도 가능합니다.\n\n<img src=\"/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png\" />\n\n<div class=\"content-ad\"></div>\n\n현재 옵션\nUC 테이블에 바로 가기를 생성할 수 있는 옵션은 현재 두 가지 있습니다: 수동(지루한) 또는 반자동, 후자는 노트북을 통해 가능합니다. 반자동 방법을 사용하면 사용자들은 UC Delta 외부 테이블을 OneLake에 통합하여 바로 가기를 생성할 수 있습니다. 동기화를 위해 카탈로그와 스키마 이름을 지정하면, 해당 스키마 내 테이블에 대한 바로 가기가 Fabric 레이크하우스 내에 생성됩니다. 실행 유틸리티 노트북에 대한 추가 지침을 참조하세요.\n\n```js\n# configuration\ndbx_workspace = \"<databricks_workspace_url>\"\ndbx_token = \"<pat_token>\"\ndbx_uc_catalog = \"catalog1\"\ndbx_uc_schemas = '[\"schema1\", \"schema2\"]'\n\nfab_workspace_id = \"<workspace_id>\"\nfab_lakehouse_id = \"<lakehouse_id>\"\nfab_shortcut_connection_id = \"<connection_id>\"\nfab_consider_dbx_uc_table_changes = True\n\n# sync UC tables to lakehouse\nsc.addPyFile('https://raw.githubusercontent.com/microsoft/fabric-samples/main/docs-samples/onelake/unity-catalog/util.py')\nfrom util import *\ndatabricks_config = {\n    'dbx_workspace': dbx_workspace,\n    'dbx_token': dbx_token,\n    'dbx_uc_catalog': dbx_uc_catalog,\n    'dbx_uc_schemas': json.loads(dbx_uc_schemas)\n}\nfabric_config = {\n    'workspace_id': fab_workspace_id,\n    'lakehouse_id': fab_lakehouse_id,\n    'shortcut_connection_id': fab_shortcut_connection_id,\n    \"consider_dbx_uc_table_changes\": fab_consider_dbx_uc_table_changes\n}\nsync_dbx_uc_tables_to_onelake(databricks_config, fabric_config)\n```\n\n가능한 미래 옵션\n\n- Fabric에서 Unity Catalog 네이티브 항목: 하이브 메타스토어 메타데이터 이동과 유사하게, Unity Catalog 메타데이터를 Fabric 레이크하우스로 동기화하여 Unity Catalog 테이블에 액세스할 수 있게 합니다. 이 시나리오의 한 예시는 FabCon에서 시연되었는데, 사용자들이 Fabric UI를 통해 Unity Catalog 테이블에 직접 액세스하고 쿼리할 수 있는 것을 보여주었습니다.\n- Fabric에 Unity Catalog 바로 가기: Dataverse 바로 가기와 유사하게, OneLake 바로 가기 UX는 잠재적으로 Unity Catalog 테이블에 대한 바로 가기 생성을 지원할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n참고: 델타 공유, Databricks에서 JDBC/ODBC, Fabric 데이터 파이프라인 Databricks 활동 등의 옵션은 여기에 언급되지 않았습니다.\n\n# Databricks/Unity 카탈로그 → Fabric\n\n## Databricks/Unity 카탈로그에서 Fabric 및 OneLake 사용하기\n\nUnity 카탈로그는 클라우드 객체 저장소 연결(예: ADLS Gen2)을 활용하고 외부 데이터 시스템에 연결하여 연합 쿼리를 실행하는 다양한 방법을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 현재 옵션입니다. 사용자들은 UC 활성화된 클러스터에서 OneLake를 다음과 같이 사용할 수 있습니다: (i) Service Principal (SPN) 기반 인증을 사용하여 OneLake에 r/w, 그리고 (ii) SPN 인증을 사용하여 mount 지점으로 OneLake에 r/w.\n\n```js\n# spn을 사용한 r/w\nworkspace_name = \"<워크스페이스_이름>\"\nlakehouse_name = \"<레이크하우스_이름>\"\ntenant_id = \"<테넌트_ID>\"\nservice_principal_id = \"<서비스_프린시펄_ID>\"\nservice_principal_password = \"<서비스_프린시펄_비밀번호>\"\n\nspark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id\", service_principal_id)\nspark.conf.set(\"fs.azure.account.oauth2.client.secret\", service_principal_password)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n\n# 읽기\ndf = spark.read.format(\"parquet\").load(f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse/Files/data\")\ndf.show(10)\n\n# 쓰기\ndf.write.format(\"delta\").mode(\"overwrite\").save(f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse/Tables/dbx_delta_spn\")\n```\n\n```js\n# spn으로 Mount\nworkspace_id = \"<워크스페이스_ID>\"\nlakehouse_id = \"<레이크하우스_ID>\"\ntenant_id = \"<테넌트_ID>\"\nservice_principal_id = \"<서비스_프린시펄_ID>\"\nservice_principal_password = \"<서비스_프린시펄_비밀번호>\"\n\nconfigs = {\n    \"fs.azure.account.auth.type\": \"OAuth\",\n    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n    \"fs.azure.account.oauth2.client.id\": service_principal_id,\n    \"fs.azure.account.oauth2.client.secret\": service_principal_password,\n    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n}\n\nmount_point = \"/mnt/onelake-fabric\"\ndbutils.fs.mount(\n    source = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com\",\n    mount_point = mount_point,\n    extra_configs = configs\n)\n\n# 읽기\ndf = spark.read.format(\"parquet\").load(f\"/mnt/onelake-fabric/{lakehouse_id}/Files/data\")\ndf.show(10)\n\n# 쓰기\ndf.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/onelake-fabric/{lakehouse_id}/Tables/dbx_delta_mount_spn\")\n```\n\n<div class=\"content-ad\"></div>\n\n알림: OneLake abfss 경로 또는 마운트 경로를 사용하여 외부 테이블을 만들 경우 UC에서 예외가 발생할 수 있습니다. 현재, OneLake를 기본 저장소로 사용하여 UC에 외부 테이블을 등록할 수 없습니다. 이는 잠재적인 미래 시나리오로 이어질 수 있습니다.\n\n- INVALID_PARAMETER_VALUE: 클라우드 파일 시스템 스키마 누락\n- 목록을 위한 SAS 토큰을 획득하지 못했습니다. 유효하지 않은 Azure 경로\n\n잠재적인 미래 옵션\nADLS Gen2와 Azure Synapse와 유사하게, 미래에는 다른 옵션이 존재할 수 있습니다:\n\n- 기본 관리 저장소로서 OneLake: Databricks는 Unity Catalog의 자동 활성화를 시작했습니다. 즉, 자동으로 ADLS Gen2와 같이 Databricks 관리 저장소로 Unity Catalog 메타스토어를 자동으로 프로비저닝합니다. 그러나 사용자는 Unity Catalog 메타스토어를 OneLake를 가리키도록하면서 사용자 관리 수준 저장소를 생성할 수도 있습니다. 참고: 아직 이 기능은 불가능합니다.\n- 외부 위치로서 OneLake: 외부 위치는 카탈로그 및 스키마의 관리 저장소 위치를 정의하고, 외부 테이블 및 외부 볼륨의 위치를 정의하는 데 사용됩니다. 예를 들어, Spark에서 외부 테이블을 사용하는 경우 OneLake를 외부 위치로 활용할 수 있습니다. 참고: 아직 이 기능은 불가능합니다.\n- 볼륨용 OneLake: 볼륨은 클라우드 객체 저장소 위치의 논리적 저장 볼륨을 나타내며 비 탭식 데이터셋에 대한 관리 방식을 추가합니다. ADLS Gen2와 같이 OneLake를 사용하여 외부 및 관리 볼륨을 생성할 수 있습니다. 참고: 아직 이 기능은 불가능합니다.\n- 연합 Lakehouse: SQL 엔드포인트나 Fabric Data Warehouse의 데이터에 대한 읽기 전용 액세스는 UC 외부 카탈로그를 사용하여 미래 옵션으로 가능할 수 있습니다. 현재 Azure Synapse 및 SQL 액세스 인증은 사용자 이름/암호를 기반으로 하며 SPN은 아직 지원되지 않으므로 이 옵션은 아직 불가능합니다. 외부 카탈로그는 현재 객체 저장소를 지원하지 않으므로, 아직 OneLake/Lakehouse로의 외부 카탈로그 연결이 가능한지 여전히 불분명합니다.\n\n<div class=\"content-ad\"></div>\n\n표 태그를 Markdown 형식으로 변경해주세요.\n\n## 잠깐, 액세스 정책은 어떻게 되나요?\n\n다른 옵션으로는 Databricks의 ODBC를 SQL 엔드포인트로 사용하거나 Partner Connect( Power BI + Databricks) 및 스트리밍 옵션이 여기에 언급되지 않았네요.\n\n계속해서 Unity 카탈로그 액세스 정책이 OneLake RBAC 및 OneSecurity와 어떻게 조화를 이루고 있는지, Unity 카탈로그에서 Fabric로 보안 및 액세스 정책을 이동하거나 그 반대로 이동할 수 있는지에 대해 살펴볼 수 있을 것입니다.\n\n참고 문헌:\n\n<div class=\"content-ad\"></div>\n\n- Non-UC 시나리오: Databricks와 Fabric — OneLake 및 ADLS Gen2에 쓰기 | Aitor Murguzur 작성 | Medium\n- Lakehouse 시나리오: Azure Databricks와 Microsoft Fabric 통합 | Piethein Strengholt 작성 | 2024년 6월 | Medium\n- Databricks Unity Catalog를 OneLake와 통합 — Microsoft Fabric | Microsoft Learn","ogImage":{"url":"/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png"},"coverImage":"/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png","tag":["Tech"],"readingTime":8},{"title":"데이터브릭스, 데브옵스 및 파이테스트","description":"","date":"2024-06-19 12:24","slug":"2024-06-19-DatabricksDevOpsandpytest","content":"\n\nDatabricks에서 코드 품질을 지속적으로 보장하고 DevOps 작업 프로세스에 통합하는 방법에 궁금증을 풀어 보셨나요? 더 이상 망설이지 마세요.\n\n다음 예시에서는 Databricks에서 pytest와 DevOps를 사용하여 구현된 테스트 기능을 쉽게 시작하는 방법에 대해 살펴볼 것입니다.\n\n# 목표\n\n이 글을 마치면 Databricks에 구현된 함수를 테스트하고, DevOps에서 pull request가 제출될 때마다 테스트 스위트를 실행할 수 있게 될 것입니다. 아래에서 이를 어떻게 구현할지 살펴보세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png\" />\n\n# DevOps에서 프로젝트 설정하기\n\nPytest는 두 가지 테스트 레이아웃을 지원하는데, 이 예시에서는 테스트가 애플리케이션 코드 외부에 배치되는 테스트 레이아웃을 사용할 것입니다. 이 분리는 나중에 데브옵스와 데이타브릭스 자산 번들을 사용하여 자동 릴리스를 다루는 기사에서 유용할 것입니다.\n\n```js\nproject.toml\npipelines/\n  pipeline_pytest.yml\nsrc/\n  functions/\n    column_funtions.py\n  soultion/\n    demo_notebook.py\ntests/\n  test_column_funtions.py\n```  \n\n<div class=\"content-ad\"></div>\n\n이 예시에서는 src(소스 코드)와 tests(테스트)로 구성된 두 개의 주요 폴더를 갖춘 간단한 설정이 있습니다. src 폴더는 지원하는 함수를 포함하는 functions와 Lakehouse를 구현하는 노트북을 포함하는 solution 폴더로 구분됩니다.\n\ncolumn_functions.py에서는 주어진 열을 제곱하는 간단한 함수를 구현했습니다.\n\n```python\n# Databricks notebook source\ndef column_squared(df, columnname):\n    df_squared = df.withColumn(columnname + \"_squared\", df[columnname] * df[columnname])\n    return df_squared\n```\n\ntest_column_functions.py에서는 column_functions.py의 함수 기능을 유효성 검사하는 간단한 테스트를 구현했습니다. 여기서 중요한 부분은 외부 데이터 소스나 스파크 세션에 의존하지 않고 독립적으로 유닛 테스트를 구현하고 있다는 것입니다. 입력과 예상 출력을 비교하기 위해 Databricks의 내장 기능을 사용합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.testing import assertDataFrameEqual\nfrom src.functions.column_functions import column_squared\n\nclass TestColumnFuntions(object):\n    def test_column_squared(self):\n        spark = SparkSession.builder.getOrCreate()\n\n        source_data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 35)]\n        source_df = spark.createDataFrame(source_data, [\"name\", \"age\"])\n\n        df_actual = column_squared(source_df,'age')\n\n        expected_data = [(\"John\", 25, 625), (\"Alice\", 30, 900), (\"Bob\", 35, 1225)]\n        df_expected = spark.createDataFrame(expected_data, [\"name\", \"age\", \"age_squared\"])\n\n        assertDataFrameEqual(df_actual, df_expected)\n```\n\n# DevOps 파이프라인\n\n함수와 관련된 테스트를 구현한 후에는 이제 Azure DevOps에서 파이프라인을 설정할 수 있습니다.\n\n파이프라인(pipeline_pytest.yml)에 대해 가상 환경을 Python용으로 생성하고 필요한 패키지를 설치한 다음 'tests' 디렉토리에서 pytest를 실행합니다. 이 경우 pytest-azurepipelines를 사용하여 pytest를 DevOps 파이프라인에 통합합니다. 이제 pytest는 'test_.py'로 시작하거나 '_test.py'로 끝나는 모든 테스트를 찾아 이 경로를 따라 이동합니다.\n\n<div class=\"content-ad\"></div>\n\n```yaml\ntrigger:\n  none\n\nsteps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: \"3.9\"\n      addToPath: true\n\n  - script: |\n      python -m venv .venv\n      source .venv/bin/activate\n      python -m pip install --upgrade pip\n      pip install numpy==1.22.4\n      pip install pyspark\n      pip install pandas\n      pip install pyarrow\n      pip install pytest-azurepipelines\n      python -m pytest -vv tests\n    displayName: \"pytest\"\n```\n\n이제 메인 브랜치에서 빌드 검증을 설정하여, 개발자가 풀 리퀘스트를 만들 때마다 정의된 테스트가 실행되도록 할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-DatabricksDevOpsandpytest_1.png\" />\n\n# 결론\n\n<div class=\"content-ad\"></div>\n\n이 예시에서는 Databricks에서 구현된 기능에 대한 테스트 슈트를 설정하고 DevOps 워크플로에 통합하는 것이 얼마나 간단한지 살펴보았습니다. 이제 프로젝트에 필요한 테스트를 구현하여 지속적으로 고품질 코드를 제공할 수 있도록 만들어 보세요.","ogImage":{"url":"/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png"},"coverImage":"/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png","tag":["Tech"],"readingTime":4},{"title":"단위 테스트 및 코드 모듈화를 위한 Databricks","description":"","date":"2024-06-19 12:22","slug":"2024-06-19-UnitTestingandCodeModularizationinDatabricks","content":"\n\n노트북은 Databricks에서 데이터를 다루는 인기 있는 방법입니다. 노트북 사용자는 데이터를 빠르게 읽고 변환하며 상호적으로 탐색할 수 있습니다. 게다가, 노트북을 공유하고 협업하는 것은 간단합니다. 그러나 프로젝트가 확장될수록 코드 중복을 방지하고 재사용성을 용이하게 하는 모듈화 기능이 필요해집니다.\n\n이를 달성하는 한 가지 방법은 공유 함수를 포함하는 노트북을 생성하고 각 노트북의 시작 부분에서 실행하는 것입니다. 또는 모듈을 만들어 일반적인 Python 개발과 유사한 Python import 명령어를 사용할 수 있습니다. 긴 코드 블록을 함수로 나누면 코드의 재사용을 촉진할 뿐만 아니라 테스트도 용이해집니다.\n\n![이미지](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png)\n\n## 모듈화\n\n<div class=\"content-ad\"></div>\n\n파이썬에서 모듈화란 프로그램을 작은 관리 가능한 모듈로 나누는 것을 말합니다. 파이썬에서 코드를 모듈화하는 것에는 여러 가지 이점이 있습니다:\n\n- 재사용성: 모듈은 다른 프로젝트에서 다시 사용할 수 있어 재작성이 필요하지 않습니다.\n- 유지보수성: 작은 중점적인 모듈로 인해 업데이트와 디버깅이 쉬워집니다.\n- 확장성: 프로젝트가 성장할 때 효율적인 확장이 가능합니다.\n- 협업: 다른 개발자들이 동시에 작업하기를 용이하게 합니다.\n- 테스트: 단위 테스트가 간소화되어 더 신뢰할 수 있는 코드를 작성할 수 있습니다.\n- 가독성: 특정 작업에 집중함으로써 코드 이해가 향상됩니다.\n\nDatabricks에서 모듈을 사용하기 위해서는 클래스 또는 함수를 포함한 파일들로 구성된 폴더와 __init__.py 파일을 생성해야 합니다. 이는 Databricks에 모듈임을 알려줍니다. 아래는 공통 모듈과 함께 공유 함수, 변환 로직을 포함한 변환 모듈, 그리고 테스트 데이터가 포함된 내 솔루션의 구조입니다.\n\n코드 구조:\n\n<div class=\"content-ad\"></div>\n\n```js\n워크스페이스\n├── test_data\n│    └── testdata.csv\n├── common\n│    └── __init__.py\n│    └── utilis.py\n├── transform\n│    └── __init__.py\n│    └── operations.py\n├── test_utils.py\n├── test_tran.py\n├── test\n```\n\ntestdata.csv:\n\n```js\nentity,iso_code,date,indicator,value\nUnited States,USA,2022-04-17,Daily ICU occupancy,\nUnited States,USA,2022-04-17,Daily ICU occupancy per million,4.1\nUnited States,USA,2022-04-17,Daily hospital occupancy,10000\nUnited States,USA,2022-04-17,Daily hospital occupancy per million,30.3\nUnited States,USA,2022-04-17,Weekly new hospital admissions,11000\nUnited States,USA,2022-04-17,Weekly new hospital admissions per million,32.8\n```\n\nulits.py:\n\n\n<div class=\"content-ad\"></div>\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef mask_func(col_val):\n    if col_val is not None:\n        if len(col_val)>=16:\n            charList=list(col_val)\n            charList[4:12]='x'*8\n            return \"\".join(charList)\n        else:\n            return col_val\n    else:\n        return col_val\n```\n\noperations.py:\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, col\n\ndef deduplicate(df, uniq_col, orderby_col):\n    df = df.withColumn(\"rn\", row_number()\n        .over(Window.partitionBy(uniq_col)\n        .orderBy(col(orderby_col).desc())))\n\n    df = df.filter(col(\"rn\") == 1).drop(\"rn\")\n    return df\n\ndef clean_clients(df):\n    df = df.where(col(\"name\") != \"\").withColumn(\"timestamp\", col(\"timestamp\").cast(\"date\"))\n\n    return df\n```\n\n모듈에서 이러한 함수를 사용하려는 사람은 아래 예시와 같이 import 명령을 사용하여 노트북에 쉽게 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![UnitTestingandCodeModularizationinDatabricks1](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_1.png)\n\nSimilarly, it’s possible to import transformation functions from the module and remove duplicated records from the DataFrame.\n\n![UnitTestingandCodeModularizationinDatabricks2](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_2.png)\n\n# Unit Testing in Databricks\n\n\n<div class=\"content-ad\"></div>\n\n소프트웨어 개발에서 단위 테스트는 코드의 정확성, 안정성, 유지 보수 가능성을 보장하는 중요한 요소입니다. 데이터 처리를 위해 노트북이 일반적으로 사용되는 Databricks에서는 단위 테스트가 더욱 중요해집니다.\n\nDatabricks에서 단위 테스트를 시작하려면 코드를 테스트할 수 있는 함수로 분해해야 합니다. 이 프로세스는 코드의 모듈성을 향상시키는 것뿐만 아니라 포괄적인 테스트 스위트를 작성하는 데 도움이 됩니다. Python에서 유닛 테스트를 수행하는 두 가지 인기있는 프레임워크인 Unittest와 pytest가 있습니다.\n\nUnittest 예시:\n\n```python\nimport unittest\n \nclass ExampleTestSuite(unittest.TestCase):\n\n    def test_import(self):\n        self.assertTrue(True)\n\n    def test_addition(self):\n        self.assertEqual(1 + 2, 3)\n\n    def test_subtraction(self):\n        self.assertNotEqual(1 - 2, 0)\n```\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경하세요.\n\n<div class=\"content-ad\"></div>\n\n## 왜 단위 테스팅을 해야 할까요?\n\n처음 테스트 중에 코드가 정상적으로 작동하는 것처럼 보이더라도, 단위 테스트는 여러 가지 이유로 중요한 역할을 합니다:\n\n- 정확성 확인: 단위 테스트는 코드의 개별 단위 기능을 확인하여 다양한 조건에서 예상대로 작동하는지 확인합니다.\n- 초기 버그 탐지: 개발 과정 초기에 버그를 식별함으로써, 개발자는 이를 신속히 해결하여 시스템의 다른 부분으로 전파되는 가능성을 줄일 수 있습니다.\n- 리팩토링 및 유지보수: 단위 테스트는 코드 리팩토링 및 유지보수 과정에서 안전망 역할을 하며, 개발자가 확신을 갖고 변경을 가할 수 있으면서도 일관된 동작을 보장합니다.\n- 회귀 테스트: 단위 테스트는 회귀 테스트로 작용하여 새로운 변경사항이나 기능이 기존의 기능을 망가뜨리지 않도록 하여 시스템의 안정성을 유지합니다.\n\n마스킹 기능에 대한 단위 테스트의 간단한 예제를 살펴보겠습니다. 이 단위 테스트는 입력 숫자가 올바르게 마스킹되거나 None을 반환하는지를 확인하여, 함수가 변경되더라도 예상되는 동작이 유지되도록 합니다.\n\n<div class=\"content-ad\"></div>\n\ntest_utils.py\n\n```python\nfrom common.utils import mask_func\n\ndef test_mask_func():\n    assert \"1234xxxxxxxx4568\" == mask_func(\"1234567891234568\")\n    assert mask_func(None) is None\n```\n\nETL(Extract, Transform, Load)과 같은 복잡한 프로세스의 경우, 데이터 변환 과정의 다양한 측면을 확인하는 데 개선된 테스트를 개발할 수 있습니다. 이러한 테스트에는 스키마 확인, 데이터프레임 비교, 행 수 유효성 검사 또는 특정 값의 존재 여부 확인이 포함될 수 있습니다.\n\ntest_tran.py:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pytest\nfrom transform.operations import *\nfrom pyspark.testing.utils import assertDataFrameEqual\nfrom pyspark.sql import SparkSession\nfrom pyspark.testing import assertDataFrameEqual, assertSchemaEqual\nfrom pyspark.sql.types import *\nimport pandas as pd\n\n@pytest.fixture()\ndef spark():\n    return SparkSession.builder.appName(\"integrity-tests\").getOrCreate()\n\n@pytest.fixture()\ndef raw_input_df(spark):\n    df = pd.read_csv('test_data/testdata.csv')\n    return spark.createDataFrame(df)\n\n@pytest.fixture()\ndef test_df(spark):\n\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Los Angeles\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df = spark.createDataFrame(input_data, schema)\n\n    return df\n\ndef test_deduplicate(test_df, spark):\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df = spark.createDataFrame(input_data, schema)\n\n    df1 = deduplicate(test_df, \"Name\", \"timestamp\")\n    assertDataFrameEqual(df1, df)\n\ndef test_schema_deduplicated(test_df, spark):\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df_expected = spark.createDataFrame(input_data, schema)\n\n    test_df = deduplicate(test_df, \"Name\", \"timestamp\")\n    assertSchemaEqual(test_df.schema, df_expected.schema)\n\ndef test_clean_clients(test_df, spark):\n    df = clean_clients(test_df)\n    assert df.where(\"name == '' \").count() == 0\n\ndef test_readfromfile(raw_input_df):\n    assert raw_input_df.count() > 0\n```\n\n## Initializing Spark Session for Tests:\n\n테스트 파일은 주피터 노트북이 아니기 때문에, Spark 세션을 초기화하는 것이 필요합니다. 이를 위해서 `spark` 함수와 `fixture` 데코레이터를 사용해서 Spark 세션을 만들 수 있습니다. `fixture` 데코레이터는 자동으로 실행되며 각 테스트 함수에 해당하는 테스트 객체를 제공해주어 테스트 데이터의 생성 및 공유를 간편하게 할 수 있습니다.\n\n```js\n@pytest.fixture()\ndef spark():\n    return SparkSession.builder.appName(\"integrity-tests\").getOrCreate()\n```\n\n<div class=\"content-ad\"></div>\n\n선언했던 fixture 데코레이터의 동작 방식을 주목하는 것이 중요합니다. 이 기법을 사용하면 테스트 데이터를 원활하게 실행하고 전달할 수 있습니다. 이 기법을 이용하면 Spark 세션을 생성하여 테스트 데이터를 로드하고 이를 테스트 함수 사이에서 공유할 수 있습니다. 테스트 데이터는 목록을 기반으로 생성하거나 테스트 파일에서 로드할 수 있습니다.\n\n```js\n@pytest.fixture()\ndef raw_input_df(spark):\n df = pd.read_csv('test_data/testdata.csv')\n\n return spark.createDataFrame(df)\n```\n\n테스트용 데이터 원본으로 샘플 파일을 사용하는 것도 가능합니다. 그러나 워크스페이스에서 로드해야 하는 경우에는 Spark가 워크스페이스로부터 파일을 직접 로드하는 것을 지원하지 않기 때문에 Pandas를 사용해야 합니다.\n\n## 모듈의 지연 변경 사항 처리하기:\n\n<div class=\"content-ad\"></div>\n\n모듈을 다룰 때 변경 내용을 구현하는 데 지연이 발생하는 것은 일반적입니다. 이 동작을 해결하기 위해 매직 함수를 사용할 수 있습니다:\n\n```js\n%load_ext autoreload\n\n%autoreload 2\n\n%aimport test_tran\n```\n\n# Databricks에서 단위 테스트 실행\n\nDatabricks에서 단위 테스트를 실행하려면 pytest 모듈을 호출하는 노트북을 생성해야 합니다. 아래는 지정된 저장소에서 테스트 실행을 트리거하는 코드 스니펫입니다.\n\n<div class=\"content-ad\"></div>\n\n테스트 노트북:\n\n```js\n%pip install pytest\n```\n\n```js\nimport pytest\nimport sys\nimport os, sys\n\nrepo_name = \"<저장소 위치>\"\n\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nrepo_root = os.path.dirname(os.path.dirname(notebook_path))\n\nos.chdir(f\"/Workspace/{repo_root}/{repo_name}\")\nprint(os.getcwd())\n# 읽기 전용 파일 시스템에 pyc 파일을 쓰지 않도록 설정합니다.\nsys.dont_write_bytecode = True\n\n# pytest 실행.\nretcode = pytest.main([\".\", \"-v\", \"-p\", \"no:cacheprovider\"])\n\n# 테스트 실패가 있는 경우 셀 실행 실패 처리합니다.\nassert retcode == 0, \"pytest 호출에 실패했습니다. 자세한 내용은 로그를 확인하세요.\"\n```\n\npytest를 실행하면 현재 디렉토리와 서브디렉토리에서 이름이 test_*.py 또는 *_test.py 패턴을 따르는 모든 파일을 자동으로 실행합니다.\n\n<div class=\"content-ad\"></div>\n\n스크립트를 실행하면 다음과 비슷한 보고서가 표시됩니다:\n\n![보고서](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_4.png)\n\n# 요약\n\n요약하면 모듈화와 유닛 테스팅은 소프트웨어 개발에서 널리 사용되는 관행이며, 이러한 적용은 데이터 엔지니어링 활동에 매끄럽게 확장됩니다. 코드의 모듈화 및 유닛 테스트를 구현하여 데이터 처리 솔루션이 더욱 신뢰성 있고 유연해집니다. 모듈화는 코드 구성 요소의 더 나은 조직화와 재사용을 가능하게 하며, 유닛 테스트는 각 구성 요소가 다양한 조건에서 예상대로 동작하는지 확인합니다. 이러한 기술들이 함께 사용되면 데이터 엔지니어링 솔루션의 전체적인 견고성과 유지보수성에 기여하며, 마지막으로 데이터 처리 파이프라인 및 워크플로의 품질을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n만약 이 기사를 유익하게 여기셨다면, 'clap' 버튼을 클릭하거나 LinkedIn에서 좋아요를 표시해 주시면 감사하겠습니다. 여러분의 지원을 감사히 여깁니다. 궁금한 점이나 조언이 있으시다면 언제든 LinkedIn에서 연락해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png"},"coverImage":"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png","tag":["Tech"],"readingTime":10},{"title":"데이터  AI 서밋 by Databricks 2024의 주요 통찰 결과","description":"","date":"2024-06-19 12:21","slug":"2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024","content":"\n\nData + AI Summit by Databricks에서 얻은 주요 인사이트가 공개되었습니다! 다음과 같은 데이터 및 분석 공간의 흥미로운 업데이트가 있습니다.\n\n🚀 Unity Catalog가 이제 오픈 소스로 공개되었습니다! Duck DB와 같은 도구에서 Unity Catalog의 Delta 테이블에 외부에서 액세스할 수 있습니다. Azure Synapse, Azure SQL, Amazon Redshift, Snowflake를 추가하여 새로운 외부 데이터 소스에 연결할 수 있습니다.\n\n🛡️ Delta 테이블에 대한 규칙을 만들어 Unity Catalog에서 행 수준 보안 및 열 수준 가리기가 일반적으로 사용 가능해졌습니다.\n\n빌트인 데이터 품질 세부 정보로 GA로 된 Lake House 모니터링은 Delta 테이블을 모니터링하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n📊 Unity Catalog metrics가 Databricks 및 Power BI, Tableau와 같은 외부 사용자를 위해 도입되어 데이터를 신뢰하고 사용할 수 있습니다.\n\n🔥 Delta 4.0은 파티셔닝과 관련된 문제를 극복하기 위해 더 나은 성능을 제공하는 Liquid 클러스터링을 도입했습니다. 작성 시 7배, 읽기 시 12배 빠른 성능을 제공합니다. JSON을 variant 데이터 유형으로 저장하는 VARIANT도 제공됩니다.\n\n💻 7월 1일부터 스파크 클러스터를 위한 100% 서버리스 인프라를 제공합니다. 초고속 클러스터를 즐기고 사용한 만큼 지불하며, 유휴 시간 요금은 없습니다. 클러스터 크기 조정, 자동 확장성, 스팟 인스턴스와 같은 복잡성에 작별을 고하세요.\n\n🔄 Databricks Lakeflow (곧 미리 보기 예정)는 DLT와 워크플로에 기반하여 데이터 수집, 변환, 오케스트레이션, 데이터 파이프라인의 모니터링을 간편화하는 솔루션입니다.\n\n<div class=\"content-ad\"></div>\n\n🔍 Databricks SQL은 이제 빠른 클러스터 시작, 향상된 성능, SQL UDF, 세션 변수, 그리고 SQL 분석가를 위한 AI 기능을 갖춘 데이터 웨어하우징 핵심 기능을 제공합니다.\n\n🤖 Databricks AI/BI에는 챗봇 \"Genie\"가 포함되어 있으며, Gen AI는 자연어 쿼리에서 자동으로 BI 대시보드를 만들어서 셀프 서비스 보고를 가능하게 합니다.\n\nApache Iceberg Tabular의 인수로 Delta lake에 이제 UniForm 형식을 통합하여 Parquet 및 Delta 형식과 함께 제공됩니다.\n\n스파크 4.0이 곧 출시될 예정이며 흥미로운 업데이트가 예정되어 있습니다. 더 많은 혁신을 기대해 주세요!\n\n<div class=\"content-ad\"></div>\n\n새로운 기능들을 탐험하는 것을 기대하고 있어요!\n\n만약 이 기사를 좋아하신다면, 저의 링크드인 페이지에서 저를 팔로우해주세요. https://www.linkedin.com/in/kaviprakash-selvaraj/","ogImage":{"url":"/assets/img/2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024_0.png"},"coverImage":"/assets/img/2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024_0.png","tag":["Tech"],"readingTime":2},{"title":"Azure Databricks와 Microsoft Fabric 통합하기","description":"","date":"2024-06-19 12:19","slug":"2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric","content":"\n\n고지: 본 글은 Microsoft나 Databricks의 공식 입장이 아닌 저의 개인적인 경험과 견해를 반영하고 있습니다.\n\n본 글은 고객 상호작용 중 자주 언급되는 핫 토픽인 Azure Databricks와 Microsoft Fabric의 조합과 통합에 대해 다룹니다. 두 서비스는 각자의 분야에서 최고 수준을 자랑합니다. Azure Databricks는 데이터 엔지니어링, 데이터 과학 및 머신 러닝 워크로드의 확장에 능합니다. 마찬가지로 Microsoft Fabric은 다양한 데이터 사용을 위한 간편성과 셀프 서비스 기능으로 빛을 발합니다. 보통 제기되는 핵심 질문은: 이 두 강자를 어떻게 통합할 수 있을까요?\n\n현재 고려해야 할 다섯 가지 옵션이 있습니다. 이 글은 새로운 기능이 추가됨에 따라 발전할 수 있음을 염두에 두십시오.\n\n- 보고 및 분석 레이어를 추가하여 Databricks를 활용한 아키텍처를 더욱 강화합니다.\n- OneLake 골드 레이어를 통합하여 Databricks를 활용한 아키텍처를 보완합니다.\n- Databricks가 모든 데이터를 OneLake에 기록하도록 합니다. 권장되지는 않지만 논의할 가치가 있습니다.\n- V-ORDERED 활성화된 소비 레이어를 통해 Databricks를 확장합니다.\n- 추가 구성 요소를 추가하여 Databricks 및 Microsoft Fabric의 데이터 처리 효율성을 향상시킵니다. 이는 좀 더 개인적인 접근입니다.\n\n<div class=\"content-ad\"></div>\n\n금일 제공되는 옵션은 다음 섹션에서 철저히 검토될 것입니다. 미묘한 차이점을 제공하고 장단점을 고려하며 관련 문서를 참조할 것입니다. 그러나 그보다 앞서, 두 강력한 도구를 활용하기로 선택하는 조직이 그 이유를 이해하는 데 도움이 됩니다.\n\n## 왜 이 조합을 선택하는가?\n\n조직은 Azure Databricks와 Microsoft Service Fabric을 결합하는 이유 때문에 이 조합이 제공하는 독특한 기능들을 선호합니다.\n\n다양한 규모의 조직에서 선호하는 종합 데이터 처리, 분석 및 데이터 과학 플랫폼인 Azure Databricks는 긴 역사와 다양한 조직에서의 성공적인 채택으로 신뢰할 수 있는 플랫폼으로 자리매김했습니다. Spark의 창시자들에 의해 설립된 Databricks는 주로 엔지니어들을 위해 제공되며, 대규모로 Spark 워크로드를 관리하고 노트북 작성 및 복잡한 작업을 처리할 수 있는 플랫폼을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n마이크로소프트 패브릭의 매력은 그 간단함에 있습니다. 2023년에 출시되어 파워 BI에서 진화한 이 제품은 기존 파워 BI 사용자에게 쉬운 전환을 제안합니다. 사용자 친화적인 인터페이스, 통합 셀프 서비스 기능, 그리고 마이크로소프트 365와의 심플한 통합으로 비즈니스 사용자들의 특히 매력을 끌고 있습니다. 마이크로소프트 패브릭은 데이터 사용을 민주화하고 진입 장벽을 낮추기 위해 설계되어 있어, 모든 사용자에게 접근성 있는 플랫폼으로 인기를 끌고 있습니다.\n\n본질적으로, Azure Databricks와 마이크로소프트 서비스 패브릭의 결합은 기술적, 비즈니스적 요구를 모두 충족하는 종합적인 솔루션을 제공하여, 많은 조직들 사이에서 인기를 얻고 있습니다.\n\n이제 조직들이 종종 이 결합을 선택하는 이유를 알게 되었습니다. 이제 두 서비스를 어떻게 통합할 수 있는지 알아보겠습니다.\n\n## 리포팅 및 분석 레이어를 추가하여 데이터브릭스 지원 아키텍처를 강화하기\n\n<div class=\"content-ad\"></div>\n\n첫 번째 디자인 고려 사항은 일반적인 Azure Databricks Medallion Lakehouse 아키텍처를 개선하는 데 관여합니다. 이 아키텍처는 Azure Data Lake Storage (ADLS) gen2, Azure Data Factory 및 Azure Databricks와 같은 서비스를 활용합니다. 이 설정에서 Databricks는 데이터 투입, 처리, 검증 및 보강의 모든 측면을 관리합니다. PowerBI는 일반적으로 보고와 분석적인 통찰을 전달하는 것을 포함한 나머지 작업을 처리합니다.\n\nMicrosoft Fabric을 포함한 Databricks 중심 아키텍처를 확장하여 자기 서비스 기능을 강화하고 비즈니스 사용자를 위한 사용자 경험을 향상시키는 것은 인기 있는 전략입니다. Databricks와 PowerBI에 새로운 기능과 능력을 갖추어 더욱 매력적이고 효율적인 경험을 제공한다고 생각해보세요.\n\nMicrosoft는 최근 Microsoft Fabric을 위한 '바로 가기'라는 새로운 기능을 소개했습니다. 이 기능은 다양한 소스에서 데이터를 읽어내어 데이터 중복을 제거하고 직접 데이터를 사용할 수 있게 하는 가벼운 데이터 가상화 엔진 역할을 합니다. 예를 들어, PowerBI를 사용할 때 PowerBI에 데이터를 복사하거나 가져오지 않고 필요한 데이터에 즉시 액세스할 수 있습니다.\n\n우리가 이전에 이야기한 Databricks 중심 디자인과 관련해서, Databricks가 모든 데이터를 ADLS에 쓰기 때문에 ADLS Gen2 바로 가기 기능을 활용할 수 있습니다. 그러나 주의해야 할 중요한 고려 사항이 몇 가지 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 단축키를 사용하려면 패브릭 레이크하우스가 필요합니다. 이미 보유하고 있지 않다면, 하나를 만드는 것을 잊지 마세요. \n- 테이블에 대한 바로 가기는 Delta Lake 형식의 데이터에만 액세스할 수 있습니다.\n- 다브릭 관리 테이블 대신 외부 테이블에 대한 바로 가기를 가능한 한 사용하세요. 다음 설계 고려 사항을 논의할 때 이 부분에 다시 언급하겠습니다.\n- 각 바로 가기는 단일 Delta 폴더를 참조할 수 있습니다. 그러므로 여러 Delta 폴더에서 데이터에 액세스해야 한다면, 각 폴더에 대해 개별적인 바로 가기를 만들어야 할 것입니다.\n- 이러한 테이블 디렉토리에서 파일을 직접 조작하지 마세요. 대신, ADLS에서 Delta 파일을 읽는 읽기 전용 접근 방식을 사용하세요. 이 접근 방식에서 ADLS는 중간 저장소로 작동합니다. Databricks에서 테이블을 직접 읽지 않습니다.\n- Lakehouse에 바로 가기를 만드는 것은 Fabric UI를 통해 수동으로 수행해야 합니다. 또는 REST API를 사용하여 모든 바로 가기를 자동으로 제공할 수 있습니다. 여기 튜토리얼 및 노트북 스크립트 링크가 있습니다.\n- ADLS에서 데이터를 직접 읽을 때는 Unity 카탈로그의 보안 모델의 데이터 액세스 정책이 적용되지 않습니다.\n\nDatabricks와 Microsoft Fabric을 통합하는 과정에서 흥미로운 발전이 동행되고 있습니다! 이러한 기능들은 Microsoft Build 2024 컨퍼런스에서 발표되었습니다. 곧 Azure Databricks Unity Catalog를 Fabric과 통합할 수 있을 것입니다. Fabric 포털을 통해 새로운 Azure Databricks Unity Catalog 항목을 만들고 구성할 수 있을 것입니다. 이 단계를 거치면 Unity Catalog에서 관리되는 모든 테이블이 즉시 바로 가기로 업그레이드될 수 있을 것입니다. 이 예정된 통합은 Azure Databricks 데이터를 Fabric에 효율적으로 통합하여 Fabric 워크로드 전반에 걸쳐 원활한 운영을 지원할 것입니다. 이 새로운 기능의 데모는 여기에서 찾아볼 수 있습니다: https://www.youtube.com/watch?v=BYob0cGW0Nk&t=4434s\n\n데이터 사용을 위해 Microsoft Fabric을 사용하는 확장된 Databricks 중심 아키텍처는 Databricks에 아주 만족한 고객들 사이에서 흔히 관찰됩니다. 이들 고객은 이미 Databricks를 사용하여 레이크하우스를 구축하는 데 상당한 시간과 자원을 투자하였으며, 이를 계속 활용할 계획입니다. Microsoft Fabric는 Delta 형식을 사용하는 레이크하우스 접근 방식의 강점과 다재다능성을 인식합니다. 이는 데이터 소비에 최적화된 레이어를 추가하여 (기존) 아키텍처를 향상시킬 수 있게 해줍니다. 이를 통해 조직은 Databricks 중심 설정에 데이터 사용을 위한 추가적인 레이어를 특별히 추가함으로써 기존 아키텍처를 강화할 수 있습니다.\n\n## OneLake 골드 레이어를 통합하여 Databricks가 가능한 아키텍처를 칭찬하세요.\n\n<div class=\"content-ad\"></div>\n\n두 번째 디자인은 초기 디자인 패턴을 수정하여 OneLake 골드 레이어를 구조에 통합합니다. 이것은 Azure Databricks의 Azure Blob 파일 시스템 (ABFS) 드라이버 덕분에 가능합니다. 이 드라이버는 ADLS와 OneLake를 모두 지원합니다. 아래에 이 접근 방식의 그림을 보실 수 있고 MS Learn 페이지에서 Notebook 예제를 찾을 수 있습니다.\n\n이 구조 내에서 전반적인 워크플로 및 데이터 처리 단계 — 데이터 수집, 처리, 유효성 검사, 데이터 보강 — 는 크게 변경되지 않습니다. 모든 것은 Azure Databricks 내에서 관리됩니다. 핵심 차이점은 이제 데이터 사용을 위한 데이터가 Microsoft Fabric에 더 가까워졌다는 것입니다. 왜냐하면 Databricks는 데이터를 OneLake에 저장된 Gold 레이어에 기록하기 때문입니다. 이것이 최선의 방법이며 이것이 왜 유익한지 궁금할 수 있습니다.\n\n중요한 점은 이 통합 방식이 Databricks에서 공식적으로 지원되지 않는다는 것이며, 데이터 관리에 영향을 미칩니다. 이에 관해 다음에 자세히 다룰 것입니다. 더 많은 정보를 원하시면 Databricks 문서를 참조해 주세요.\n\nDatabricks는 관리형 테이블과 외부 테이블 두 가지 유형의 테이블을 구분합니다. 관리형 테이블은 기본적으로 생성되며 Unity Catalog에 의해 관리되며 수명주기 및 파일 레이아웃을 제어합니다. 외부 도구를 사용하여 이러한 테이블에서 파일을 직접 조작하는 것은 권장되지 않습니다. 이에 반해, 외부 테이블은 메타스토어, 카탈로그 또는 스키마에 지정된 관리형 저장 위치 외부에 데이터를 저장합니다.\n\n<div class=\"content-ad\"></div>\n\n그래서, 문서에서 제공한 지침을 기반으로, 이 접근 방식을 사용하여 OneLake에 직접 쓰여진 모든 테이블은 외부 테이블로 분류하는 것이 권장됩니다. 이는 데이터가 메타스토어의 범위 바깥에서 관리되기 때문입니다. 그 결과, 이러한 테이블의 관리는 Fabric 내부와 같은 다른 곳에서 수행해야 합니다. 이 방식의 동기는 다음과 같을 수 있습니다:\n\n첫째, OneLake에 데이터를 물리적으로 저장하는 것은 Microsoft Fabric 내에서 성능을 향상시킵니다. 이는 OneLake 테이블이 특히 조인 및 집계와 관련된 쿼리에 최적화되어 있기 때문입니다. 그에 반해, ADLS Gen2를 통해 바로가기를 통해 데이터를 읽는 경우, 이러한 작업을 포함하는 쿼리에 대해 성능이 느릴 수 있습니다.\n\n둘째, OneLake에서 데이터를 관리하는 것은 Microsoft Fabric 내에서 보안 조치를 적용하는 데 유용합니다. 예를 들어, OneLake 테이블은 역할 기반 액세스 제어 (RBAC)를 사용하여 보안할 수 있어 데이터 액세스를 관리하는 과정이 단순해집니다. 그러나 ADLS Gen2를 사용할 경우, ADLS Gen2 저장소 계정의 권한을 처리해야 하므로 더 복잡할 수 있습니다.\n\n셋째, OneLake 테이블은 정책에 따라 관리될 수 있어 데이터가 규정에 따라 사용되도록 보장하기가 더 쉽습니다. 예를 들어, 다른 곳에 있는 도메인과 테이블을 (외부적으로) 공유할 때입니다.\n\n<div class=\"content-ad\"></div>\n\n데이터를 읽는 것 이외에도 Microsoft Fabric 내에서 새로운 데이터를 생성하는 것을 고려해 볼 수 있습니다. 만약 이것이 여러분의 계획 중 일부라면, 다가오는 기능이 매우 흥미로울 수 있습니다. 곧 Fabric 사용자들은 Unity 카탈로그를 통해 Azure Databricks에서 lakehouses와 같은 데이터 항목에 액세스할 수 있게 될 것입니다. 데이터는 여전히 OneLake에 남아 있겠지만, Azure Databricks에서 해당 데이터의 계보 및 다른 메타데이터에 직접 액세스하고 보는 능력을 갖게 될 것입니다. 이 향상된 기능은 Fabric에서 Databricks로 다시 데이터를 읽는 것을 용이하게 할 것입니다. 예를 들어, Azure Databricks의 Mosaic AI를 활용해 AI를 활용하려는 경우 Microsoft Fabric에서 다시 읽음으로써 가능할 것입니다. 이 기술은 아마도 Lakehouse Federation일 것입니다. 이 내용에 대한 자세한 정보는 이 비디오의 해당 부분에서 확인할 수 있습니다: https://youtu.be/BYob0cGW0Nk?t=4125\n\n마지막으로, 모든 통합 및 데이터 처리를 Databricks에서 처리하고 소비 레이어를 Fabric에서 관리하는 전략은 각 응용 프로그램 영역에서 가장 좋은 기능을 활용할 수 있도록 하는 편리함을 기관들에게 제공합니다. 이 접근 방식은 데이터 처리의 최적 성능과 보안을 보장합니다.\n\n## Databricks가 모든 데이터를 OneLake에 기록하도록 설정 (권장되지 않음)\n\nDatabricks를 OneLake와 통합하는 경험을 바탕으로, OneLake가 ADLS Gen2와 동일한 API를 지원한다는 것을 알고 있습니다. 이를 감안해 보면, 가상의 설계 가능성을 고려해 보겠습니다: 모든 Medallion 레이어를 OneLake에 저장하는 것입니다. 이 가능할까요? 알아보도록 하죠.\n\n<div class=\"content-ad\"></div>\n\n이 접근 방식에 대한 인센티브는 신규 배포로부터 비롯될 수 있습니다. 여기서의 목표는 데이터 엔지니어링 작업을 효율적으로 확장하면서 Microsoft Fabric을 사용하여 모든 계층에서 데이터 사용 및 소비에 대한 설계 간결성과 셀프 서비스를 장려하는 데 Databricks의 기본 기능을 활용하는 것입니다.\n\n유감스럽게도, 이 설계는 효율적인 데이터 관리에 적합하지 않습니다. 이 구성은 각 워크스페이스 계층이 Microsoft Fabric의 자체 Lakehouse 엔터티를 필요로 하기 때문에 작업공간이 증가함에 따른 관리 오버헤드로 이어질 수 있습니다. 이 증식은 데이터 공유 시 통제, 메타데이터 관리 및 협업 오버헤드와 같은 추가적인 도전 과제를 야기할 수 있습니다. 또한 Databricks는 관리형 테이블을 사용할 때 이 접근 방식을 지원하지 않습니다. 따라서, 비록 이 아키텍처가 이론적으로 매력적으로 보일 수 있지만, 저는 최선의 실천으로 이를 사용하는 것을 강력히 비추합니다.\n\n## V-ORDERED 활성화 소비 계층으로 Databricks 확장\n\n다음 설계 고려사항은 Microsoft Fabric의 사용과 V-Order 기능을 활용하는 데 더 중점을 둘 것입니다. 이 기능은 파케이 파일 형식에 대한 라이트 타임 최적화로, Microsoft Fabric 컴퓨팅 엔진(예: Power BI)에서 빠른 데이터 읽기를 가능케 합니다.\n\n<div class=\"content-ad\"></div>\n\nDatabricks와 Microsoft은 오픈 소스 열 기반 파일 형식인 Delta Lake를 채택하기로 선택했습니다. 그러나 Microsoft은 V-Order 압축의 추가 레이어를 통합했는데, 이것은 최대 50%의 더 많은 압축을 제공합니다. V-Order는 오픈 소스 parquet 형식과 완전히 호환되며, 모든 parquet 엔진이 일반 parquet 파일처럼 읽을 수 있습니다.\n\n참고로 V-Order를 적용하여 Fabric의 유지 관리 기능을 활용하면 V-Order가 없는 테이블에 적용할 수 있습니다.\n\nV-Order는 Microsoft Fabric에 중요한 이점을 제공하는데, 특히 Power BI 및 SQL 엔드포인트와 같은 구성 요소에게 도움이 됩니다. 예를 들어, Power BI를 사용하여 데이터 쿼리 중에 뛰어난 성능을 유지하면서 실시간 데이터에 직접 연결할 수 있습니다. 데이터 소스의 변경 사항이 Power BI에 즉시 반영되므로 새로 고침을 기다릴 필요가 없어집니다.\n\nV-Order로 최적화된 테이블을 사용하는 것은 현재 Microsoft Fabric에게만 제한되어 있습니다. Databricks는 아직이 기능을 통합하지 않았습니다. 따라서 그런 경우에는 V-Order로 최적화된 테이블을 활용하기 위해 Microsoft Fabric 내의 서비스를 활용해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 실버(Silver)와 골드(Gold) 단계 사이의 Databricks를 통한 처리 단계가 V-Order 최적화가 필요하지 않을 경우에도 여전히 중요하다는 주장이 가능하다는 점을 감안해 볼 수 있습니다. 이 부분은 반복적으로 보일 수 있지만, Databricks를 이용한 지속적인 데이터 처리를 가능하게 하는 타당한 선택지입니다.\n\n다른 주목할만한 측면은 왜 조직이 이 설계를 선택하는지에 대한 것이며, 이는 여러 테이블 간의 거래 일관성을 유지하는 것입니다. 특히, 이러한 일관성을 Gold에서 유지하는 것은 매우 중요합니다. 현재 Spark는 개별 테이블에 대한 트랜잭션만 지원합니다. 따라서, 테이블 간에 데이터 불일치가 있는 경우 보상 조치를 통해 해결해야 할 수 있습니다. 예를 들어, 구매 주문에 대한 세 가지 테이블에 영향을 미치는 변경 사항을 하는 경우, 이러한 변경 사항을 하나의 트랜잭션으로 묶을 수 있습니다. 이것은 해당 테이블을 쿼리할 때 모든 변경 사항이 있거나 전혀 없을 것임을 의미합니다. 이러한 무결성 문제는 다수의 테이블에 걸쳐 복잡한 트랜잭션을 관리할 수 있는 환경의 중요성을 강조하며, Delta Lake 위에서 이를 지원할 수 있는 유일한 플랫폼은 Microsoft Fabric Warehouse입니다. 자세한 내용은 여기에서 확인할 수 있습니다.\n\n위 이미지에서 나타난 업데이트된 아키텍처에서는 Synapse Engineering이 이제 실버(Silver)에서 골드(Gold)로의 처리 엔진으로 작동합니다. 이 접근 방식은 모든 테이블이 V-Order 최적화되도록 보장합니다. 추가로, 트랜잭션 기능이 필요한 사용 사례를 위해 Synapse Warehouse가 추가되었습니다. 그러나 이러한 아키텍처 변경 사항은 데이터 엔지니어가 서로 다른 독특한 데이터 처리 서비스를 탐색해야 한다는 의미입니다. 따라서 모든 팀에게 명확한 지침을 제공하는 것이 중요합니다. 예를 들어, Databricks의 기본 기능인 AutoLoader와 Delta Live Tables를 활용한 데이터 품질 검증을 위해 브론즈(Bronze) 및 실버(Silver)에 대한 원칙을 수립하고, 골드(Gold)에서는 Microsoft Fabric와만 연동되는 소비특화 통합 로직 구축에 초점을 맞출 수 있습니다.\n\n## 추가 구성 요소를 추가하여 Databricks 및 Microsoft Fabric의 데이터 처리 효율성 향상하기\n\n<div class=\"content-ad\"></div>\n\n우리 이전 토론에서는 엔지니어들이 다른 데이터 처리 서비스를 탐험해야 하는 과제에 대해 논의했습니다. 이 문제는 메타데이터 주도 접근법과 DBT와 같은 템플릿 프레임워크를 채택하여 해결할 수 있습니다. 새로운 아키텍처에서는 Databricks와 Microsoft Fabric에 추가 구성 요소를 결합했습니다. 이 변경 사항에 대해 자세히 알아보도록 하겠습니다.\n\nDatabricks 측면에서는 메타데이터 주도 프레임워크(메타데이터 저장소), Great Expectations 및 Data Build Tool (DBT)를 추가했습니다. 메타데이터 주도 프레임워크는 작성하고 유지해야 하는 코드 양을 크게 줄일 수 있습니다. 다중 노트북을 생성하는 대신 이 접근 방식을 통해 모든 데이터의 수집 및 유효성 검사를 위한 범용 파이프라인을 활용할 수 있으며 Great Expectations라는 다른 오픈 소스 프레임워크로 데이터를 수집하고 유효성 검사할 수 있습니다. 이 접근 방식은 메타데이터 저장소에서 읽어와 다른 스크립트를 동적으로 호출함으로써 달성됩니다. 이 접근 방식에 대해 더 알고 싶다면 이 주제에 대한 또 다른 블로그 글을 추천합니다.\n\n다음으로 DBT에 대해 이야기해 봅시다. 이 오픈 소스 명령줄 도구인 데이터 빌드 툴(DBT)은 Python으로 작성되었습니다. 그 강점은 SQL의 SELECT 문과 유사한 구문을 사용하여 템플릿을 활용해 변환을 정의하는 범용 인터페이스를 제공하는 데 있습니다. Databricks는 dbt-databricks 패키지를 통해 지원됩니다. DBT와 Databricks를 사용하는 데 대한 자세한 정보를 원한다면 이 주제에 대한 다른 블로그 글을 읽는 것을 권장합니다.\n\nMicrosoft Fabric 측면에서도 DBT가 중요한 역할을 할 수 있습니다. Microsoft Fabric 내에서 Synapse Warehousing을 위해 dbt-fabric 또는 Synapse Spark을 위해 dbt-fabricspark 중에서 선택할 수 있습니다. 이 템플릿 접근 방식의 장점은 개발자가 모든 데이터 변환 사용 사례를 위한 단일 프론트엔드에 익숙해지기만 하면 두 서비스를 모두 활용할 수 있다는 점입니다. 이 방법론은 프로세스를 간소화하고 효율성을 높일 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 결론\n\nAzure Databricks와 Microsoft Fabric의 통합은 조직에 수많은 혜택과 가능성을 제공합니다. Azure Databricks의 유연성과 확장성이 Microsoft Fabric의 간편하고 사용자 친화적인 기능과 결합되면 모든 계층에서 데이터 사용 및 관리를 혁신적으로 개선할 수 있습니다. Databricks 중심 아키텍처를 Microsoft Fabric 레이어로 향상시키거나 성능 및 보안을 향상시키기 위해 아키텍처에 OneLake 골드 레이어를 통합하기까지 여러 아키텍처 디자인 선택지가 있습니다.\n\n또한, Microsoft Fabric의 V-Order 최적화 소개와 추가 구성 요소 사용은 데이터 처리 효율성을 현격히 향상시키고 간소화할 수 있습니다. 그러나 이러한 조합 또는 통합은 서비스 탐색과 유연성, 데이터 보안 및 격리를 균형있게 고려해야 할 수도 있습니다.\n\n요약하자면, Azure Databricks와 Microsoft Fabric의 통합은 Microsoft Build 2024 Conference에서 발표된 흥미로운 혁신과 함께 빅 데이터 처리 워크로드를 위한 희망찬 미래를 암시합니다.","ogImage":{"url":"/assets/img/2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric_0.png"},"coverImage":"/assets/img/2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric_0.png","tag":["Tech"],"readingTime":12},{"title":"2024년 데이타브릭스 데이터  AI 서밋에서의 소감들","description":"","date":"2024-06-19 12:18","slug":"2024-06-19-ReflectionsfromDatabricksDataAISummit2024","content":"\n\n\n![Reflections from Databricks Data + AI Summit 2024](/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png)\n\n# 개요:\n\n2024년 Databricks Data + AI Summit에 참석한 것은 눈을 뜨게 하는 경험이었어요. 놀라운 주제 발표에서부터 분과 세션, 그리고 연구 내용까지 깊게 들여다보며, 이번 세미나는 오늘날의 디지턈 환경에서 높은 품질의 데이터와 AI의 변혁적인 힘을 강조했어요.\n\n이 개인적인 관점에서, 나에게 정말 기억에 남는 이벤트로 만든 몇 가지 주요 포인트와 영감을 주는 이야기들을 탐구해보겠어요.\n\n\n<div class=\"content-ad\"></div>\n\n# 주요 연설:\n\n데이터브릭스는 이러한 연설들이 대부분 자가 마케팅과 자기 홍보에 관한 것이라는 내 생각을 바꿨어.\n\n- 나는 첫 번째 주요 연설에 마음이 없이 참석했어. 시간에 맞춰 도착했음에도 불구하고, 객석에는 16,000명 이상이 참석하여 수용 능력을 초과했어. 두 연설 모두 놀라운 발표와 데모로 이뤄진 롤러코스터 경험이었어 (나는 다음 섹션에서 조금 더 자세히 다룰 거야).\n- 데이터브릭스는 간단하게 유지했고, 그들이 실제 세계와 실제 사용 사례/문제를 해결하려고 했다는 것을 깨달았어. \"쉽게 얻을 수 있는 열매(l﻿ow-hanging fruit)\" 라는 용어를 들어봤어? 데이터 세계의 모든 사람들 — 엔지니어, 데이터 과학자, 분석가, 제품 관리자 및 리더들 — 매일 이러한 문제에 직면해. 그들의 모든 발표와 연설은 이 주제와 일치하며, Gen AI 시대의 기회와 혁신을 강조했어.\n- 대부분의 연설자는 분명히 데이터브릭스 출신이었어. 그들은 유용한 통찰, 새로운 구현 및 산업 트렌드를 공유했어. 몇 명의 다른 기술 기관 출신인 추가적인 중요 인물들로부터 이러한 트렌드가 확인되었어. 가죽 자켓을 입은 록스타 CEO를 포함한 다른 기술 기관 출신 중요 인물들도 포함돼. 내 새로운 조언, 내 3살 아이에게까지 하는 것은 이제 \"고통을 바라\"야!\n\n# 기술적 발표:\n\n<div class=\"content-ad\"></div>\n\nDatabricks에서는 상당수의 릴리스가 있었는데, 그중 몇 가지를 발견해서 흥미롭다고 생각했습니다. 몇 개를 소개해 드리겠습니다. 뭔가 놓친 부분이 있을 수 있으니 양해 부탁드립니다.\n\n- 완전히 서버리스: 휴, 정말 감사합니다. 엔지니어의 업무를 훨씬 쉽게 만들어 줍니다. 오랫동안 기다려왔고 유지보수 및 비용 절감 가능성에 흥분하고 있어요!\n- AI BI: 이제 새로 발표된 GA 기능이 제공하는 것을 달성하기 위해 제3자 시각화 도구를 적게 사용할 것이라 확신합니다. 추가 보너스로 제공되는 Genie 기능을 활용하여 NLP를 사용해 제품용으로 준비된 즉석 대시보드를 생성할 수 있습니다!\n- 노코드 파인 튜닝: 데이터의 이 측면에 대해 전문가는 아니지만, 노코드를 사용한 모델 파인 튜닝이 게임 체인저가 될 것이라는 동료 데이터 과학자로부터 흥미로운 확언을 받았습니다!\n- LakeFlow ETL: 이제 다른 도구보다 Databricks를 선호하지 않았던 엔지니어들을 위한 노코드 솔루션입니다. 일괄 처리 또는 스트리밍 데이터의 가져오기를 위해 많은 코드를 작성해야 하는가요? 이제 그럴 필요가 없어졌습니다!\n- LLMOps 및 Mosaic AI 에이전트 프레임워크/평가: MlFlow의 ML 모델 프레임워크가 게임 체인저였다고 생각한다면, 이 기능도 즐길 것입니다. MLOps와 유사한 LLMOps(ML 오퍼레이션)가 공개되었을 때 Databricks는 이 분야에서 각 플랫폼보다 앞서 있다는 것을 증명했습니다.\n- 데이터 거버넌스, 모니터링 및 운영 탐지: 개인적으로 이것은 모든 ETL 도구에 내장되어야 한다고 생각하는 제가 특별히 좋아하는 부분입니다. Lakehouse로 자동화된 방식으로 이를 얼마나 구현했는지에 대해 너무나 흥미롭습니다!\n- Spark 4.0 발표: 현재는 이에 대해 넘어갈 거예요. Spark 3.x에서 제공되는 최신 기능을 모두 사용하지는 않겠다고 자신합니다. 그래도 제공되는 기능을 따르는 것에는 여전히 흥미를 느끼고 있습니다!\n- Unity Catalog OSS: \"마지막이지만 최고의\" 또는 여기에서 맥락을 두기 위해 \"위에 소개된 기능을 즐기고 싶다면 필수\"라고 말할 수 있습니다. Unity 표를 통해 모든 주요 SQL 엔진 및 데이터 형식에 액세스할 수 있습니다. 게임 체인저! 채택은 어려울 수 있지만 Databricks는 필요한 모든 지원을 제공하기 위해 열정적인 것으로 보입니다. 그리고, 'Spark'ing CTO가 라이브 영상에서 Git 리포를 공개한 것도 그만한 멋진 일이었습니다.\n\nLakeFlow 엔지니어링 디렉터와 쿠키 광고에 관한 마케팅 캠페인 제품 관리자의 멋진 데모에 큰 찬사를 보냅니다(실시간 디버깅이 재미있었고 잘 작동해서 기뻤습니다!). 또한 ‘X 배 더 저렴하고/빠르다’와 같은 기능 수준의 KPI가 매우 참신했습니다.\n\n# 특별한 언급 — Compound AI Systems Workshop:\n\n<div class=\"content-ad\"></div>\n\n위의 것들은 키노트 중에 고수준 발표/데모였습니다. 우리가 다 알다시피, \"악마는 세부 사항에 있다\"고 합니다. 그러한 세션 중에서 더 자세히 다루는 유용한 세션들이 몇 가지 있었습니다.\n\n- 모든 세션 또는 최고의 세션을 참석하는 것은 인간적으로 불가능하지만, 많은/대부분은 가상으로도 이용 가능할 것입니다, 이미 제공되고 있지 않다면.\n- 특별히, '워크샵'이라고 명시된 세션 중 어떤 것이라도 참석하시는 것을 강력히 권장합니다. 그리고 또한 7시간 동안 진행되는 세션 중 어떤 것이라도 참석해보세요. 이것이 개인적으로 저에게 가장 깨우침을 주는 경험이었으며 제게 Gen AI 분야의 놀라운 현재 연구자들의 마음에 들어가볼 수 있는 기회를 제공해주었습니다.\n- 영감을 주는 20-30개의 연구 주제 포스터가 있었습니다. 초록문을 차분히 읽거나 작가들의 이야기를 듣거나 심지어 데이터 애호가들로부터의 질문들을 관찰하는 것은 새로운 경지로 나를 인도해주었습니다.\n- 또한 교육, 로봇 공학 등 분야의 선도적인 연설자들이 참석하였으며 Anthropic, DeepMind, OpenAI, Microsoft, LangChain 등 최신 트렌드의 Gen AI 기관에서 온 패널리스트들과 함께 논의가 마무리되었습니다.\n\n# 마지막으로:\n\n가죽자켓을 입은 CEO가 정확하게 지적했습니다 - \"무엇이든 시작해보세요. 이것은 빠르게 움직이는 기차입니다. 지수적인 추세를 기다려보거나 관찰하고 있기 원치 않으실 겁니다.\"\n\n<div class=\"content-ad\"></div>\n\n저번 주에 이 서밋 전에, 제가 따라가며 읽어본 상대적인 AI 구현을 강화하기 위해 Snowflake가 발표한 멋진 기능들에 대해 알아보았어요. 이 글을 큰 생각과 함께 마무리 짓는군요 — \"다음 단계는 너야, Snowflake. 이미!\"\n\nPS: 서밋을 통해 제게 지지를 보내준 리더, 팀 그리고 조직에게 정말 큰 감사를 표합니다!","ogImage":{"url":"/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png"},"coverImage":"/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png","tag":["Tech"],"readingTime":4},{"title":"DSPy와 Amazon Bedrock을 사용하여 견고한 AI 시스템 구축하기","description":"","date":"2024-06-19 12:16","slug":"2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock","content":"\n\n## 프롬프트 매직에서 프롬프트 엔지니어링으로 변경\n\n![이미지](/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png)\n\n인공 지능이 다양한 산업을 혁신하면서, AI 모델을 개발하고 배포하기 위한 견고하고 확장 가능한 도구에 대한 필요성은 이제껏 없었습니다. 이 분야의 주목할만한 발전로는 Stanford의 최신 데이터 과학 도구인 DSpy와 AWS의 기계 학습을 위한 혁신적인 기반인 Amazon Bedrock이 있습니다. 이 블로그 글은 DSpy와 Amazon Bedrock 사이의 특징, 기능 및 독특한 시너지에 대해 파헤치며, 개발자와 데이터 과학자가 AI의 경계를 넓히는 데 어떻게 도움을 주는지 강조합니다.\n\n# DSPy란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n```js\n%pip 설치 dspy-ai\n```\n\nDSPy는 스탠포드 NLP에서 개발한 오픈 소스 라이브러리로, 데이터 과학 워크플로우를 만들고 관리하는 프로세스를 간소화하기 위해 설계되었습니다. 이는 세 가지 핵심 구성 요소인 Signatures, Modules 및 Optimizers을 중심으로 구축되어 있습니다.\n\n## Signatures\n\nDSPy의 서명은 언어 모델(LM) 작업의 입력/출력 동작을 모듈식이고 적응적인 방식으로 정의합니다. Signatures는 길고 취약한 프롬프트에 의존하는 대신, 깨끗하고 재현 가능한 코드를 허용합니다. 서명의 예로는 질문 답변을 위한 `»question -` answer»`나 요약을 위한 `»document -` summary»`가 있습니다. 작업 요구 사항에 따라 서명은 간단하거나 복잡할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 모듈\n\nDSPy의 모듈은 LM 프로그램의 구성 요소입니다. 각 모듈은 chain-of-thought나 retrieval-augmented generation과 같은 특정 프롬프팅 기술을 추상화합니다. 모듈은 다양한 시그니처를 처리할 수 있으며, PyTorch와 같은 프레임워크의 신경망 레이어처럼 더 큰 프로그램으로 구성될 수 있습니다. 이를 통해 유연하고 확장 가능한 프로그램 구성이 가능해집니다.\n\n## 옵티마이저\n\nDSPy의 옵티마이저는 DSPy 프로그램의 매개변수를 세밀하게 조정하여 프로그램의 출력을 최적화합니다. 그들은 기울기 하강법과 이산 최적화 기술의 조합을 사용하여 메트릭을 최대화하거나 일반적으로 프로그램의 출력을 평가하는 함수에 점수를 부여합니다. 다양한 종류의 옵티마이저가 제공되며, 각각 다른 데이터 시나리오와 최적화 요구에 맞게 맞춤화됩니다. 옵티마이저가 가장 잘 작동하도록하려면 일부 학습 입력을 제공해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# Amazon Bedrock을 사용하는 방법\r\n\r\n## 구성\r\n\r\n첫 번째 단계는 DSPy를 구성하여 기본적으로 Amazon Bedrock을 사용하도록 설정하는 것입니다:\r\n\r\n```js\r\nimport dspy\n\nbedrock_haiku = dspy.AWSAnthropic(\n    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n)\ndspy.configure(lm=bedrock_haiku)\r\n```\n\n<div class=\"content-ad\"></div>\n\nLLM 구성이 마무리되었으니 문제 해결을 시작할 수 있어요.\n\n## 서명 및 모듈\n\n공식 DSPy 설명서에서 제안하는 대로 \"DSPy를 사용하는 8 단계\"를 따라서 작업을 정의해 보겠습니다. 처음에는 간단하게 질문과 답변 프로그램을 만들어 보죠. 따라서 우리의 입력은 질문이 되고, 출력은 답변이 될 거예요. 이를 위해 우리의 서명과 모듈을 정의할 수 있습니다:\n\n\\js\nqa = dspy.Predict(\"question -> answer\")\n\\\n\n<div class=\"content-ad\"></div>\n\n아래 예시에서 예측은 우리의 모듈이며, 예측을 생성하는 것이 목표이며, 서명은 질문 - 답변입니다. 이는 우리가 DSPy에게 질문에서 답변을 찾고 있다는 것을 간결하게 설명하는 줄임표기법입니다. qa를 출력하면 다음 출력이 나타납니다:\n\n```js\nPredict(StringSignature(question -> answer\n    instructions='주어진 필드 `question`으로 `answer` 필드를 생성하십시오.'\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n))\n```\n\nDSPy는 질문과 답변이 문자열임을 추론하고, 지시사항에서 강조된 프롬프트를 언어 모델의 입력으로 사용합니다. 타입을 직접 제어할 수도 있습니다:\n\n```js\ndspy.TypedPredictor(\"question:str -> answer:int\")\n\n# 출력\nTypedPredictor(StringSignature(question -> answer\n    instructions='주어진 필드 `question`으로 `answer` 필드를 생성하십시오.'\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n    answer = Field(annotation=int required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n))\n```\n\n<div class=\"content-ad\"></div>\n\nModule Predict에서 정의된 프롬프트를 통해, DSPy는 프롬프트 엔지니어링 프로세스를 반복하고 제어할 수 있는 개념을 소개합니다. 이 클래스를 사용하여 다음 질문에 대한 답변을 생성해보겠습니다:\n\n```js\nqa(question=\"Sergio Mattarella는 누구인가?\").answer\n\n# 출력\n주어진 질문에 대한 답변은 다음과 같습니다:\n질문: Sergio Mattarella는 누구인가?\n답변: Sergio Mattarella는 현재 이탈리아의 대통령입니다. 그는 2015년부터 대통령으로 재직하고 있습니다.\n```\n\n## 서명 및 모듈을 위한 고급 구성\n\n이제 프로그램의 동작을 수정하기 위해 서명과/또는 모듈을 사용자 정의할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\ndspy.Modules에 대해 이야기해보겠습니다. 이 라이브러리에서 제공하는 다른 모듈을 사용하거나 사용자 정의 모듈을 만들 수 있습니다:\n\n- dspy.Predict: 기본 예측자입니다. 서명을 수정하지 않습니다. 학습의 주요 형태(즉, 지시 및 데모의 저장 및 LM 업데이트)를 처리합니다.\n- dspy.ChainOfThought: 서명의 응답을 확정하기 전 단계별로 생각하도록 LM에 가르칩니다.\n- dspy.ProgramOfThought: 실행 결과에 따라 응답을 결정할 코드를 출력하도록 LM에 가르칩니다.\n- dspy.ReAct: 주어진 서명을 구현하기 위해 도구를 사용할 수 있는 에이전트입니다.\n- dspy.MultiChainComparison: ChainOfThought에서 여러 출력을 비교하여 최종 예측을 생성할 수 있습니다.\n\n이전 출력인 dspy.Predict와 dspy.ChainOfThought를 비교해보겠습니다. 그러나 질문을 바꿔볼까요:\n\n```js\nquestion = \"True or False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\"\n\npredictor = dspy.Predict(\"question -> answer\")\npredictor(question=question)\n\n# 결과\nPrediction(\n    answer='Question: True of False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\\nAnswer: True. The numbers 17, 9, 10, 12, 13, 4, and 2 add up to 67, which is an even number.'\n)\n\n------\n\ncot = dspy.ChainOfThought(\"question -> answer\")\ncot(question=question)\n\n# 결과\nPrediction(\n    rationale=\"Question: True or False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\\nReasoning: Let's think step by step in order to determine if the numbers in this group add up to an even number.\\n1. We need to add up all the numbers in the group: 17 + 9 + 10 + 12 + 13 + 4 + 2 = 67.\\n2. 67 is an odd number, not an even number.\",\n    answer='False, the numbers in this group do not add up to an even number.'\n)\r\n```\n\n<div class=\"content-ad\"></div>\n\n두 출력 결과를 보면, 답변이 다르며, 후자가 올바른 것을 알 수 있습니다. 이는 DSPy가 Chain of Thought (CoT)를 통해 우리가 제공한 프롬프트를 확장하기 때문입니다. CoT를 사용하면 LM(Language Model)에게 답변을 제공하기 전에 \"단계별로\" 추론하도록 강요합니다. 이 근거는 답변에서 제공되며, 더 자세한 지침은 cot.extended_signature에서 확인할 수 있습니다.\n\n```js\ncot.extended_signature\n\n# 출력\nStringSignature(question -> rationale, answer\n    instructions='`question` 필드를 주어 `answer` 필드를 생성하십시오.'\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '질문:', 'desc': '${question}'})\n    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"추론: 정답을 만들기 위해 '단계별로 생각해 봅시다. ${produce the answer}. We ...\", '__dspy_field_type': 'output'})\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': '답변:', 'desc': '${answer}'})\n)\n```\n\ndspy.Signature의 경우, 예를 들어 RAG에 매우 유용한 컨텍스트를 소개하려면 축약 표기를 확장할 수 있습니다:\n\n```js\ndspy.Predict(\"context, question -> answer\")\n\n# 출력\nPredict(StringSignature(context, question -> answer\n    instructions='`context`, `question` 필드를 주어 `answer` 필드를 생성하십시오.'\n    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '컨텍스트:', 'desc': '${context}'})\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '질문:', 'desc': '${question}'})\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': '답변:', 'desc': '${answer}'})\n))\n```\n\n<div class=\"content-ad\"></div>\n\n아니면 더 많은 제어를 위해 더 긴 표기를 사용해보세요:\n\n```js\nclass BasicQA(dspy.Signature):\n    \"\"\"문맥에 기반한 짧은 답변으로 질문에 대답합니다\"\"\"\n    context = dspy.InputField()\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"문맥에서 추출된 짧은 답변\")\n\n# 출력\nBasicQA(context, question -> answer\n    instructions='문맥에 기반한 짧은 답변으로 질문에 대답합니다'\n    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '문맥:', 'desc': '${context}'})\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '질문:', 'desc': '${question}'})\n    answer = Field(annotation=str required=True json_schema_extra={'desc': '문맥에서 추출된 짧은 답변', '__dspy_field_type': 'output', 'prefix': '답변:'})\n)\n```\n\n서명은 TypedPredictor 모듈 덕분에 pydantic 표기를 지원합니다:\n\n```js\nimport dspy\nfrom pydantic import BaseModel, Field\nfrom dspy.functional import TypedPredictor\nfrom datetime import datetime\nfrom textwrap import dedent\n\nclass TravelInformation(BaseModel):\n    origin: str = Field(pattern=r\"^[A-Z]{3}$\")\n    destination: str = Field(pattern=r\"^[A-Z]{3}$\")\n    date: str\n    confidence: float = Field(gt=0, lt=1)\n\nclass TravelSignature(dspy.Signature):\n    \"\"\" 주어진 이메일에서 모든 여행 정보를 추출합니다 \"\"\"\n    email: str = dspy.InputField()\n    flight_information: list[TravelInformation] = dspy.OutputField()\n\npredictor = TypedPredictor(TravelSignature)\npredictor(email=dedent(\"\"\"\n    Amazon Web Services Airlines로 예약해 주셔서 감사합니다.\n    2024년 6월 18일 바리에서 라스베이거스로 가는 XYZ123 편에 예약이 완료되었으며 탑승을 환영합니다.\n    즐거운 여행 되세요.\n\"\"\"))\n```\n\n<div class=\"content-ad\"></div>\n\n## 리트리버\n\n프로그램은 dspy.Retrieve 클래스를 확장하여 검색 시스템을 구현할 수도 있습니다. 사용 가능한 리트리버의 최신 목록을 확인하려면 DSPy GitHub 저장소의 dspy.retrievers 모듈을 참조해주세요.\n\nAmazon Bedrock와 함관해서 리트리버를 사용하려면 사용자 정의 SentenceVectorizer 클래스를 만들어야 합니다. 미리 해당 작업을 수행해 두었습니다. (그런데, 이를 DSPy 팀이 공식적으로 구현하길 원하시면 PR #1151에 +1을 부탁드립니다):\n\n```python\nimport boto3\nimport json\nimport numpy as np\nfrom typing import List, Optional\nfrom dsp.modules.sentence_vectorizer import BaseSentenceVectorizer\n\nclass AmazonBedrockVectorizer(BaseSentenceVectorizer):\n    '''\n    이 벡터화기는 텍스트를 임베딩으로 변환하기 위해 Amazon Bedrock API를 사용합니다.\n    '''\n    SUPPORTED_MODELS = [\n        \"amazon.titan-embed-text-v1\", \"amazon.titan-embed-text-v2:0\",\n        \"cohere.embed-english-v3\", \"cohere.embed-multilingual-v3\"\n    ]\n\n    def __init__(\n        self,\n        model_id: str = 'amazon.titan-embed-text-v2:0',\n        embed_batch_size: int = 128,\n        region_name: str = 'us-west-2',\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n    ):\n        self.model_id = model_id\n        self.embed_batch_size = embed_batch_size\n\n        # Bedrock 클라이언트 초기화\n        self.bedrock_client = boto3.client(\n            service_name='bedrock-runtime',\n            region_name=region_name,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key\n        )\n\n    def __call__(self, inp_examples: List[\"Example\"]) -> np.ndarray:\n        text_to_vectorize = self._extract_text_from_examples(inp_examples)\n        embeddings_list = []\n\n        n_batches = (len(text_to_vectorize) - 1) // self.embed_batch_size + 1\n        for cur_batch_idx in range(n_batches):\n            start_idx = cur_batch_idx * self.embed_batch_size\n            end_idx = (cur_batch_idx + 1) * self.embed_batch_size\n            cur_batch = text_to_vectorize[start_idx: end_idx]\n            \n            # Bedrock API Body 구성\n            if self.model_id not in self.SUPPORTED_MODELS:\n                raise Exception(f\"지원하지 않는 모델: {self.model_id}\")\n            \n            if self.model_id == \"amazon.titan-embed-text-v1\":\n                if self.embed_batch_size == 1:\n                    body = json.dumps({\"inputText\": cur_batch[0]})\n                else:\n                    raise Exception(f\"모델 {self.model_id}은 배치 크기 1을 전용으로 지원합니다.\")\n            elif self.model_id == \"amazon.titan-embed-text-v2:0\":\n                if self.embed_batch_size == 1:\n                    body = json.dumps({\n                        \"inputText\": cur_batch[0],\n                        \"dimensions\": 512\n                    })\n                else:\n                    raise Exception(f\"모델 {self.model_id}은 배치 크기 1을 전용으로 지원합니다.\")\n            elif self.model_id.startswith(\"cohere.embed\"):\n                body = json.dumps({\n                    \"texts\": cur_batch,\n                    \"input_type\": \"search_document\"\n                })\n            else:\n                raise Exception(\"여기서 어떻게 나타났나요?\")\n                \n            \n            # Bedrock API 호출\n            response = self.bedrock_client.invoke_model(\n                body=body,\n                modelId=self.model_id,\n                accept='application/json',\n                contentType='application/json'\n            )\n\n            response_body = json.loads(response['body'].read())\n            if self.model_id.startswith(\"cohere.embed\"):\n                cur_batch_embeddings = response_body['embeddings']\n            elif self.model_id.startswith(\"amazon.titan-embed-text\"):\n                cur_batch_embeddings = response_body['embedding']\n            else:\n                raise Exception(f\"아직 구현되지 않았습니다! Amazon Bedrock 문서에서 모델 {self.model_id}의 응답 형식을 확인하세요: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\")\n            embeddings_list.extend(cur_batch_embeddings)\n\n        embeddings = np.array(embeddings_list, dtype=np.float32)\n        return embeddings\n\n    def _extract_text_from_examples(self, inp_examples: List) -> List[str]:\n        if isinstance(inp_examples[0], str):\n            return inp_examples \n        return [\" \".join([example[key] for key in example._input_keys]) for example in inp_examples]\n```\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 코드를 사용하여 선호하는 DSPy 검색기에서 이 클래스를 사용할 수 있습니다:\n\n```js\nfrom dspy.retrieve.faiss_rm import FaissRM\n\ndocument_chunks = [\n    \"...\"\n]\n\nfrm = FaissRM(\n    document_chunks=document_chunks,\n    vectorizer=AmazonBedrockVectorizer(\n        embed_batch_size=128, model_id=\"cohere.embed-english-v3\"\n        # OR:\n        # embed_batch_size=1, model_id=\"amazon.titan-embed-text-v2:0\"\n    )\n)\nprint(frm([\"여기에 질문을 입력하세요\"]))\n```\n\n## 사용자 정의 프로그램\n\n이 지식을 활용하여 프로그램의 동작을 정의하는 사용자 정의 클래스를 정의할 수 있습니다! 예를 들어, RAG 클래스는 다음과 같이 보일 것입니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        # 'Retrieve' will use the user's default retrieval settings unless overriden.\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        # 'ChainOfThought' with signature that generates answers given retrieval & question.\n        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        return self.generate_answer(context=context, question=question)\n```\n\n이 코드를 실행하기 전에 선호하는 검색기를 구성해야 합니다.\n\n# 결론\n\nDSPy와 Amazon Bedrock은 인공지능(AI) 개발 도구의 진화에서 중요한 발전을 나타냅니다. DSPy의 데이터 과학 능력과 Bedrock의 확장 가능하고 효율적인 모델 관리를 결합하여 개발자와 데이터 과학자는 복잡한 AI 과제에 대처할 강력한 도구 상자를 갖추게 됩니다. 이러한 도구들이 계속 발전함에 따라, 그들은 의심할 여지 없이 AI의 미래를 형성하는 데 중추적인 역할을 할 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n자세한 정보는 DSPy GitHub 저장소와 Amazon Bedrock 문서를 살펴보세요. 이 흥미로운 분야에서의 미래 업데이트와 진전에 주목해 주세요!","ogImage":{"url":"/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png","tag":["Tech"],"readingTime":14},{"title":"5개의 무료 엔드 투 엔드 데이터 엔지니어링 프로젝트","description":"","date":"2024-06-19 12:15","slug":"2024-06-19-5FREEEnd-To-EndDataEngineeringProjects","content":"\n\n이 5개의 프로젝트를 수행함으로써 AWS, GCP 및 Azure를 배울 수 있습니다.\n\n이 프로젝트들은 총 200만 회 이상의 조회수를 갖고 있습니다. 그 이유가 있을 텐데요.\n\n![이미지](/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png)\n\n데이터 엔지니어링은 복잡한 분야로 들릴 수 있지만, 솔직히 말해서 그렇습니다. 하지만 자전거를 타는 것을 배우는 것처럼 연습을 통해 더 쉽고 직관적으로 되는 법이죠.\n\n<div class=\"content-ad\"></div>\n\n시작하거나 기술을 향상시키는 데 도움이 되기 위해, 무료로 작업할 수 있는 멋진 다섯 가지 프로젝트를 골라왔어요. 이 프로젝트들은 어떤 것이든 아니에요; 즐겁고 매력적이며 다양한 주제와 도구를 다루고 있어요. 함께 시작해봐요!\n\n# 1. YouTube 데이터 분석\n\n유튜브 비디오가 인기있는 이유가 궁금했던 적이 있나요? 이 프로젝트에서 실제 유튜브 데이터를 활용하여 그것을 알아볼 수 있어요. 데이터 마법사가 되어야 하는 것은 아니에요. Python이라는 학습이 쉬운 프로그래밍 언어와 큰 데이터를 처리하는데 사용되는 PySpark을 익힐 수 있을 거예요. 또한 데이터를 관리하는 언어인 SQL과 Athena, Glue, Redshift, S3와 같은 다양한 AWS(아마존 웹 서비스) 도구를 사용해볼 수 있어요. 이 도구들은 데이터 세계에서 큰 이름이며, 어떻게 함께 문제를 해결하는지 배울 수 있을 거예요.\n\n개발할 수 있는 기술:\n\n<div class=\"content-ad\"></div>\n\n- Python 및 PySpark으로 코딩하기\n- 데이터 관리를 위한 기본 SQL\n- 현실 세계 문제의 이해 및 해결\n- 데이터 프로젝트를 위한 AWS 도구 사용\n\n## 2. Airflow를 활용한 Twitter 데이터 파이프라인\n\nTwitter는 데이터의 보고이에요. 이 프로젝트에서는 Twitter 데이터 수집, 처리 및 저장 프로세스를 자동화하는 방법을 배울 거예요. Airflow를 사용하여 이러한 작업을 예약하고 조직화하는 방법을 직접 확인하게 될 거예요. 또한, 이 프로젝트는 Twitter 데이터에 액세스하기 위한 Tweepy 및 데이터 분석을 위한 Python 라이브러리인 Pandas를 소개합니다. 데이터 엔지니어들에게 널리 사용되는 ETL (추출, 변환, 로드) 작업 작성에 익숙해지는 좋은 기회가 될 거예요.\n\n개발할 기술:\n\n<div class=\"content-ad\"></div>\n\n- Python 프로그래밍\n- Airflow를 사용하여 작업 자동화\n- 데이터 수집 및 분석\n- API 및 클라우드 저장소 사용\n\n## 3. 실시간 주식 시장 분석\n\n주식 시장 트렌드를 실시간으로 예측할 수 있다면 어떨까요? 이 프로젝트는 당신을 그 현실에 더 가깝게 이끌어줍니다. Python과 Kafka(실시간 데이터 스트림 처리 플랫폼)를 사용하여 주식 시장 데이터를 실시간으로 분석하는 애플리케이션을 구축할 것입니다. EC2 인스턴스(서버 유형)에 Kafka를 설정하고 데이터 파이프라인을 생성하는 것이 재미있는 부분입니다. 마법 같은 프로젝트이지만 실용적인 기술에 근거해 있습니다.\n\n개발할 기술:\n\n<div class=\"content-ad\"></div>\n\n- 실시간 애플리케이션 구축\n- 데이터 스트림을 위한 Kafka 이해\n- 클라우드 서버 설정\n- 실시간 데이터 분석\n\n## 4. GCP에서 Uber 데이터 분석\n\nUber의 데이터는 방대하고 다양하여 분석 프로젝트에 이상적입니다. 원시 데이터를 이해하고 데이터 모델을 작성하며 ETL 작업을 위한 스크립트를 작성하는 방법을 배울 수 있습니다. 이 프로젝트에서는 데이터 파이프라인을 구축하는 현대적인 도구인 mage와 데이터 분석을 위한 SQL도 소개됩니다. 게다가 Google Cloud Platform (GCP)에서 작업하게 되어 선도적인 클라우드 서비스 중 하나인 GCP의 최신 기술 스택을 직접 경험할 수 있습니다.\n\n개발할 수 있는 기술:\n\n<div class=\"content-ad\"></div>\n\n- 데이터 모델링 및 분석\n- ETL 스크립트 작성 및 자동화\n- SQL 쿼리 작성\n- 클라우드 기반 데이터 도구 사용\n\n# 5. Azure에서 올림픽 데이터 분석\n\n올림픽은 많은 데이터를 생성합니다. 이 프로젝트에서는 이 데이터를 API에서 추출하고 Microsoft의 클라우드 플랫폼인 Azure를 사용하여 분석하는 방법을 배울 수 있습니다. DataBricks와 같은 대용량 데이터용 서비스, 데이터 통합용 DataFactory, 대규모 데이터 분석용 Synapse Analytics 등을 사용할 수 있습니다. 이 프로젝트는 규모에 맞는 데이터 처리 방법을 가르치며 향후 올림픽 전략에 영향을 줄 수 있는 통찰을 제공할 것입니다.\n\n개발할 수 있는 기술:\n\n\n<div class=\"content-ad\"></div>\n\n- API에서 데이터 추출하기\n- 데이터 엔지니어링을 위해 Azure 서비스 사용하기\n- 데이터 처리를 위한 Spark 코드 작성하기\n- SQL을 사용한 고급 데이터 분석\n\n도움이 되었다면 저의 게시물을 팔로우하는 것을 잊지 마세요 :)\n\n관심이 있다면 여기에서 데이터 엔지니어링 기초 과정을 확인할 수 있습니다 —\n\n데이터 엔지니어링/과학/분석/LLM에 관한 놀라운 블로그를 더 많이 게시할 예정입니다\n\n<div class=\"content-ad\"></div>\n\n환영합니다!","ogImage":{"url":"/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png"},"coverImage":"/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png","tag":["Tech"],"readingTime":3}],"page":"21","totalPageCount":98,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}