{"pageProps":{"posts":[{"title":"Kubernetes K8s ConfigMap 또는 Secret가 업데이트될 때 배포 자동 재시작하기","description":"","date":"2024-06-19 13:11","slug":"2024-06-19-KubernetesAutoRestartDeploymentswhenK8sConfigMaporSecretisUpdated","content":"\n\n## 자동 롤아웃 재시작: K8s ConfigMap 또는 Secret가 업데이트될 때 배포 다시 시작하기\n\n쿠버네티스 배포는 모든 쿠버네티스 클러스터에서 가장 일반적인 리소스 중 하나입니다. 우리는 모두 pod를 K8s 배포를 사용하여 실행하여 높은 가용성을 보장하고, pod가 삭제되면 자동으로 생성되도록합니다.\n\n애플리케이션이 항상 여러 환경에서 원활하게 실행되도록하기 위해 구성이 필요한 것은 매우 흔합니다. 데이터베이스 사용자 이름, 비밀번호 등과 같은 중요한 정보가 필요할 수도 있습니다. 쿠버네티스에서는 구성 맵과 시크릿을 사용하여 응용 프로그램별 데이터를 저장하고 pod로 주입하여 응용 프로그램에서 사용할 수 있도록 할 수 있습니다.\n\n그렇다면 구성 맵이나 시크릿의 값을 업데이트했을 때는 어떨까요? 최신 값을 반영하려면 pod를 다시 시작해야합니다, 맞죠? 또는 롤아웃을 다시 시작하여 새로운 pod를 생성하게 할 수도 있습니다. 이제 상상해보세요. 공통 configmap 또는 secret을 사용하는 수백 개의 배포가 있고 그 값을 업데이트하고 사용하는 것이 최신 값이라는 것을 확실하게 해야한다고 가정해보세요.\n\n<div class=\"content-ad\"></div>\n\n저희는 AWS Secrets Manager에 비밀을 저장하고, Kubernetes Secrets Store CSI Driver를 위해 AWS Secrets 및 구성 제공자(ASCP)로부터 Kubernetes Secrets를 생성합니다. \n\n이 블로그 게시물에서는 Secret 또는 ConfigMap이 업데이트될 때 Kubernetes 배포를 자동으로 롤아웃 및 다시 시작하는 방법에 대해 설명하겠습니다.\n\n# 아키텍처 다이어그램:\n\n![Architecture Diagram](/assets/img/2024-06-19-KubernetesAutoRestartDeploymentswhenK8sConfigMaporSecretisUpdated_0.png)\n\n<div class=\"content-ad\"></div>\n\n## 시작해 봅시다!\n\n### 준비물:\n\n- EKS 클러스터\n- EKS를 위한 OIDC 제공자 구성 필요\n- Kubectl\n- AWS CLI\n\n### 단계 1: ASCP를 위한 IAM 역할 및 정책 생성\n\n<div class=\"content-ad\"></div>\n\nASCP(Amazon EKS Security Token Service)는 Amazon EKS 파드 ID를 검색하여 IAM 역할로 교환합니다. 해당 IAM 역할에 대한 IAM 정책에서 권한을 설정합니다. ASCP가 IAM 역할을 가정하면 권한을 부여받은 시크릿에 액세스할 수 있습니다. 다른 컨테이너는 IAM 역할과 연결되지 않는 한 시크릿에 액세스할 수 없습니다.\n\n- IAM 정책 문서 작성\n\"secrets_policy\"라는 이름의 파일을 만들고 다음 내용을 추가하십시오.\n\n```js\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n}\n```\n\n2. 다음 명령을 실행하여 IAM 정책을 만듭니다.\nIAM 정책을 IAM 역할에 연결할 때 policy ARN을 메모하십시오.\n\n<div class=\"content-ad\"></div>\n\n```js\naws iam create-policy \\\n    --policy-name my-secret-manager-policy \\\n    --policy-document file://secrets_policy\n```\n\n3. IAM 역할에 신뢰 정책 생성하기\n\"trust_policy\"라는 이름의 파일을 생성하고 다음 내용을 추가하세요. 올바른 값으로 대체해야 합니다. `SERVICE_ACCOUNT_NAME`은 임의로 지정할 수 있지만 Kubernetes에서 실제 서비스 계정을 생성할 때 동일한 이름을 사용해야 합니다.\n\n```js\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/oidc.eks.<AWS_REGION>.amazonaws.com/id/<OIDC_ID>\"\n      },\n      \"Condition\": {\n        \"StringEquals\": {\n          \"oidc.eks.<AWS_REGION>.amazonaws.com/id/<OIDC_ID>:aud\": \"sts.amazonaws.com\",\n          \"oidc.eks.<AWS_REGION>.amazonaws.com/id/<OIDC_ID>:sub\": \"system:serviceaccount:<K8S_NAMESPACE>:<SERVICE_ACCOUNT_NAME>\"\n        }\n      }\n    }\n  ]\n}\n```\n\n4. 다음 명령어를 실행하여 IAM 역할을 만드세요. \n\n<div class=\"content-ad\"></div>\n\n```js\naws iam create-role --role-name my-secret-manager-role --assume-role-policy-document file://trust_policy\n```\n\n5. Attach IAM policy to IAM Role\n\n```js\naws iam attach-role-policy --policy-arn <your_policy_arn> --role-name my-secret-manager-role\n```\n\n우리는 필요한 모든 IAM 역할과 정책을 생성했습니다.\n\n<div class=\"content-ad\"></div>\n\n# 단계 2: ASCP 설치 및 구성\n\n이제 2개의 Helm 차트를 설치해야 합니다.\n\n- AWS Secrets and Configuration Provider (ASCP) 차트 설치\n\n```js\n# ASCP Helm 차트 리포지토리 추가\nhelm repo add aws-secrets-manager https://aws.github.io/secrets-store-csi-driver-provider-aws\n\n# ASCP Helm 차트 설치\nhelm install -n kube-system secrets-provider-aws aws-secrets-manager/secrets-store-csi-driver-provider-aws\n```\n\n<div class=\"content-ad\"></div>\n\n2. Secrets Store CSI Driver 차트 설치\n\n- Secrets Store CSI Driver 차트를 위한 helm 레포지토리 추가\n\n```js\n# Secrets Store CSI Driver 차트 레포지토리 추가\nhelm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts\n```\n\n- 기본값 확인\n\n<div class=\"content-ad\"></div>\n\n\n# 기본 값 가져오기\nhelm show values secrets-store-csi-driver/secrets-store-csi-driver > secrets-store-csi-driver.yaml\n\n\n- secrets-store-csi-driver.yaml 파일에서 다음 값을 업데이트하세요.\n\n\n## K8S Secrets 동기화에 필요한 RBAC 역할 및 바인딩 설치 여부\nsyncSecret:\n  enabled: true\n\n## 시크릿 로테이션 기능 활성화 [알파]\nenableSecretRotation: true\n\n\n위의 구성은 \"secrets-store-csi-driver\"가 AWS Secret Manager에서 최신 값을 가져와 해당 값을 Kubernetes Secrets 객체에 업데이트할 수 있게 합니다.\n\n<div class=\"content-ad\"></div>\n\n회전-투표-간격은 기본적으로 2분으로 설정되어 있지만, 속성 rotationPollInterval을 설정함으로써 변경할 수 있습니다.\n\n- Helm 차트 설치\n\n```js\n# Helm 차트 설치\nhelm install -n kube-system csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver -f secrets-store-csi-driver.yaml\n```\n\n# 단계 3: AWS Secret Manager에 테스트 시크릿 생성\n\n<div class=\"content-ad\"></div>\n\nAWS Secret Manager에서 테스트 시크릿을 생성할 것입니다.\n\n```js\naws secretsmanager create-secret \\\n    --name my-test-secret \\\n    --description \"CLI로 생성한 내 테스트 시크릿.\" \\\n    --secret-string \"{\\\"user\\\":\\\"my-user\\\",\\\"password\\\":\\\"예시-비밀번호\\\"}\"\n```\n\n# 단계 4: Kubernetes ServiceAccount 생성\n\n이제 IAM 역할을 가정할 수 있도록 파드에 허용하는 ServiceAccount를 생성할 수 있습니다. 이 ServiceAccount는 K8s 배포에서 사용될 것입니다.\n\n<div class=\"content-ad\"></div>\n\nserviceaccount.yaml이라는 이름의 파일을 생성해주세요.\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: <your_service_account_name> # 이 이름은 IAM 신뢰 정책을 만들 때 지정한 이름과 일치해야 합니다.\n  annotations:\n    eks.amazonaws.com/role-arn: <IAM_ROLE_ARN>\n```\n\n다음 명령을 실행하여 K8s에서 서비스 계정을 생성합니다.\n\n```bash\nkubectl apply -f serviceaccount.yaml\n```\n\n<div class=\"content-ad\"></div>\n\n# 단계 5: 테스트 객체 생성하기\n\n이제 필요한 모든 리소스를 배포했습니다. 이제 테스트 객체를 만들어 봅시다.\n\n- 이름이 “my-test-secret-manifest.yaml”인 파일 생성\n\n```js\n---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: aws-secrets-providerclass\nspec:\n  provider: aws\n  secretObjects:\n    - secretName: my-test-k8s-secret\n      type: Opaque\n      data:\n        - objectName: user\n          key: user\n        - objectName: password\n          key: password\n  parameters:\n    objects: |\n      - objectName: arn:aws:secretsmanager:<AWS_REGION>:<AWS_ACCOUNT_ID>:secret:my-test-secret\n        jmesPath:\n          - path: user\n            objectAlias: user\n          - path: password\n            objectAlias: password\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secret-rotation-test-ubuntu-deployment\n  labels:\n    app: ubuntu\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubuntu\n  template:\n    metadata:\n      labels:\n        app: ubuntu\n    spec:\n      serviceAccountName: <your_service_account_name> # 이 이름은 단계 4에서 만든 서비스 계정 이름과 일치해야 합니다\n      volumes:\n      - name: mount-secrets-access\n        csi:\n          driver: secrets-store.csi.k8s.io\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: \"aws-secrets-providerclass\"\n      containers:\n      - name: ubuntu\n        image: ubuntu\n        command: [\"sleep\", \"123456\"]\n        env:\n        - name: USER\n          valueFrom:\n            secretKeyRef:\n              name: my-test-k8s-secret\n              key: user\n        - name: PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: my-test-k8s-secret\n              key: password\n        volumeMounts:\n        - name: mount-secrets-access\n          mountPath: \"/mnt/aws-secrets\"\n          readOnly: true\n```\n\n<div class=\"content-ad\"></div>\n\n2. manifest를 적용하십시오\n\n```js\nkubectl apply -f my-test-secret-manifest.yaml\n```\n\n3. 다음 리소스가 생성됩니다.\n\n- SecretProviderClass 리소스 - AWS Secret Manager에서 데이터를 가져와 K8s Secret를 생성합니다\n- 볼륨 마운트가 있는 배포 - SecretProviderClass를 볼륨으로 마운트해야 합니다\n- Kubernetes Secret - 파드 내에서 환경 변수로 주입됩니다\n\n<div class=\"content-ad\"></div>\n\n아래 다이어그램은 YAML을 적용할 때 뒷단에서 무슨 일이 벌어지는지 잘 시각화한 것입니다.\n\n![다이어그램](/assets/img/2024-06-19-KubernetesAutoRestartDeploymentswhenK8sConfigMaporSecretisUpdated_1.png)\n\n4. 모든 것이 배포되었는지 확인해보세요.\n\n```js\n# SecretProviderClass 확인\nkubectl get SecretProviderClass aws-secrets-providerclass -o yaml\n\n# 배포 확인\nkubectl get deploy secret-rotation-test-ubuntu-deployment -o yaml\n\n# Pod 확인\nkubectl get po <pod_name> -o yaml\n\n# Secret 가져오기\nkubectl get secret my-test-k8s-secret -o yaml\n```\n\n<div class=\"content-ad\"></div>\n\n# 단계 6: Reloader 설치하기\n\nReloader는 ConfigMap과 Secret의 변경 사항을 감지하고 관련된 DeploymentConfig, Deployment, DaemonSet, StatefulSet 및 Rollout과 함께 Pod의 롤링 업그레이드를 수행할 수 있습니다.\n\n- Reloader Helm Repo 추가\n\n```js\n# Helm Repo 추가\nhelm repo add stakater https://stakater.github.io/stakater-charts\n```\n\n<div class=\"content-ad\"></div>\n\n2. 기본값 가져오기\n\n```js\n# 기본값 가져오기\nhelm show values stakater/reloader > reloader.yaml\n```\n\n3. reloader.yaml 파일 업데이트하기\n\n```js\nreloader:\n  # 리더십 선출을 활성화하려면 true로 설정하여 여러 레플리카를 실행할 수 있습니다.\n  enableHA: true\n  deployment:\n    # 여러 레플리카를 실행하려면 reloader.enableHA = true로 설정합니다.\n    replicas: 2\n```\n\n<div class=\"content-ad\"></div>\n\n4. Helm 차트 설치\n\n```js\n# Helm 차트 설치\nhelm install reloader -f reloader.yaml stakater/reloader -n kube-system\n```\n\n# 단계 7: Secret 업데이트로 테스트하기\n\n- Reloader는 주석에 영향을 받습니다.\n기본 주석 reloader.stakater.com/auto는 주요 메타데이터에 있어야 합니다. 아래 명령을 사용하여 배포에 주석을 추가하세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 배포 주석 추가\nkubectl annotate deployment secret-rotation-test-ubuntu-deployment \"reloader.stakater.com/auto=true\"\n```\n\n또는 다음 블록으로 배포 파일을 편집하고 적용할 수도 있습니다.\n\n```js\nmetadata:\n  annotations:\n    reloader.stakater.com/auto: \"true\"\n```\n\n2. AWS Secret Manager에서 시크릿 업데이트하기\n\n\n<div class=\"content-ad\"></div>\n\n```js\r\naws secretsmanager put-secret-value \\\n      --secret-id my-test-secret \\\n      --secret-string \"{\\\"user\\\":\\\"diegor\\\",\\\"password\\\":\\\"SAMPLE-PASSWORD\\\"}\"\r\n```\n\n3. 한 번 시크릿이 AWS 시크릿 스토어 csi 드라이버에 업데이트되면 K8s 시크릿이 즉시 업데이트됩니다. K8s 시크릿이 업데이트되면 Reloader가 롤아웃을 다시 시작하도록 트리거합니다.\n\n```js\r\n# K8s 시크릿을 확인하세요. 새로운 값이 있어야 합니다.\nkubectl get secret my-test-k8s-secret -o yaml\n\n# Pod를 확인하세요. 몇 초 전에 시작되었어야 합니다.\nkubectl get po\n\n# Reloader 팟의 로그를 확인하세요.\nkubectl logs <reloader-pod-name> -n kube-system\n\n# Pod로 실행 후 새로운 값을 확인하세요.\nkubectl exec -it <pod_name> -- bash\n\n# Pod에 들어간 후 `env` 명령을 실행하세요. Pod에서 사용 가능한 모든 환경 변수가 출력됩니다.\r\n```\n\n# 단계 8: ConfigMap 업데이트를 테스트하세요.\n\n<div class=\"content-ad\"></div>\n\n- \"my-test-cm-manifest.yaml\" 파일을 생성해주세요.\n\n```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-test-k8s-cm\ndata:\n  myvalue: \"Hello World\"\n  drink: coffee\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reloader-poc-ubuntu-deployment\n  labels:\n    app: ubuntu\n  annotations:\n    reloader.stakater.com/auto: \"true\"\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubuntu\n  template:\n    metadata:\n      labels:\n        app: ubuntu\n    spec:\n      containers:\n      - name: ubuntu\n        image: ubuntu\n        command: [\"sleep\", \"123456\"]\n        env:\n        - name: DRINK\n          valueFrom:\n            configMapKeyRef:\n              name: my-test-k8s-cm\n              key: drink\n        - name: MYVALUE\n          valueFrom:\n            configMapKeyRef:\n              name: my-test-k8s-cm\n              key: myvalue\n```\n\n2. 매니페스트 적용\n\n```bash\nkubectl apply -f my-test-cm-manifest.yaml\n```\n\n<div class=\"content-ad\"></div>\n\n3. my-test-cm-manifest.yaml 파일에서 configmap을 업데이트하세요.\n\n```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-test-k8s-cm\ndata:\n  myvalue: \"안녕하세요\"\n  drink: 차\n```\n\n4. 파일을 다시 적용하세요.\n\n```yaml\nkubectl apply -f my-test-cm-manifest.yaml\n```\n\n<div class=\"content-ad\"></div>\n\n5. 확인\n\n```js\n# ConfigMap 확인\nkubectl get cm my-test-k8s-cm -o yaml\n\n# Pod 확인\nkubectl get po\n\n# Pod에 접속하여 새 값 확인\nkubectl exec -it <pod_name> -- bash\n\n# Pod에 들어간 후 `env` 명령어를 실행하면 Pod 내에서 사용 가능한 모든 환경 변수가 출력됩니다\n```\n\n축하합니다!!! secret-store-csi-driver와 reloader를 성공적으로 구성했습니다.\n\n감사합니다!!!\n\n<div class=\"content-ad\"></div>\n\n## 참고 자료:\n\n- [AWS 공식 문서 - CSI 드라이버 통합](https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_csi_driver.html)\n- [Bootlabs 기술 블로그 - AWS Secrets Manager in Kubernetes 시크릿 회전과 리로더](https://blog.bootlabstech.com/aws-secrets-manager-in-kubernetes-secret-rotation-and-reloader)\n- [Secrets Store CSI 드라이버 공식 홈페이지 - 시크릿 자동 회전](https://secrets-store-csi-driver.sigs.k8s.io/topics/secret-auto-rotation)\n- [Secrets Store CSI 드라이버 차트 값 설정 파일](https://github.com/kubernetes-sigs/secrets-store-csi-driver/blob/main/charts/secrets-store-csi-driver/values.yaml)\n- [Reloader GitHub 저장소](https://github.com/stakater/Reloader/tree/master)\n- [Reloader 작동 확인 문서](https://github.com/stakater/Reloader/blob/master/docs/Verify-Reloader-Working.md)\n- [Reloader 작동 방식 문서](https://github.com/stakater/Reloader/blob/master/docs/How-it-works.md)\n\n# 간단히 말하자면 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 계속 참여해 주세요:\n\n<div class=\"content-ad\"></div>\n\n- 작가에게 박수를 보내고 팔로우를 눌러주세요 ️👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼에서 저희를 만나보세요: Stackademic | CoFeed | Venture | Cubed\n- 알고리즘 콘텐츠를 다뤄야 하는 블로그 플랫폼에 지쳤나요? Differ를 시도해보세요\n- PlainEnglish.io에서 더 많은 콘텐츠를 만나보세요","ogImage":{"url":"/assets/img/2024-06-19-KubernetesAutoRestartDeploymentswhenK8sConfigMaporSecretisUpdated_0.png"},"coverImage":"/assets/img/2024-06-19-KubernetesAutoRestartDeploymentswhenK8sConfigMaporSecretisUpdated_0.png","tag":["Tech"],"readingTime":13},{"title":"아마존 EKS의 미래","description":"","date":"2024-06-19 13:10","slug":"2024-06-19-FutureofAmazonEKS","content":"\n\nAWS re:Invent 2023 세션 중 \"Amazon EKS의 미래 (CON203)\"를 Nathan Taber, AWS의 Kubernetes 제품 총괄의 프레젠테이션을 시청해야 했어요. 이 기사에서는 논의된 주요 포인트들을 요약하겠으며, 이해를 돕기 위해 전체 내용을 확인해 보라는 주의문을 담을게요.\n\n# Kubernetes의 중요성\n\nKubernetes는 오케스트레이션을 위한 오픈 소스 기술로, 인기만 끌 뿐만 아니라 엄청난 성공을 거두었어요. Kubernetes의 관리 조직인 Cloud Native Computing Foundation (CNCF)의 설문 조사에 따르면, 지난 해 64%의 기업이 프로덕션에서 Kubernetes를 사용했으며, 추가 25%가 평가 중이거나 시범운영 중이었어요. 이러한 중요한 채용은 Kubernetes가 현대 IT 운영에서 발휘하는 중추적 역할을 반영하고 있어요. 이 기사에서는 이 인기 급증의 이유와 AWS가 Amazon EKS에 투자한 방대한 투자를 탐구해 보겠어요.\n\n## 그렇다면, 왜 사람들은 Kubernetes를 사용할까요?\n\n<div class=\"content-ad\"></div>\n\n- 혁신 주행: 기관이 Kubernetes로 전환하는 주요 이유는 고객을 대신하여 혁신을 이루는 데 있습니다. 빠르게 움직이고 안전하고 신속하게 변화를주는 능력은 혁신에 대단히 중요합니다. Kubernetes는 개발팀을위한 표준 시스템과 표준 세트를 제공하여 조직이 변화를 수용하고 사용자를위한 더 나은 혁신을 이끌어 내도록합니다.\n- 고정 비용 감소: Kubernetes 도입의 또 다른 주요 동기는 고정 비용을 줄이기 위한 욕구입니다. 조직은 정적 자원에서 동적 공유 자원으로 전환하여 컨테이너를 사용하여 복잡한 계약을 분해하고 관리 오버헤드를 줄입니다. 이러한 전환은 비용 절감을 가져오면서 유연성과 확장성을 유지합니다.\n- 전체 조직 활성화: Kubernetes는 응용 프로그램을 조정하기 위한 API 표준을 제공하여 다양한 환경에 대한 공통 기반을 제공합니다. AWS 클라우드의 서로 다른 지역, 온프레미스 또는 다른 클라우드 제공 업체에서 실행 중이든 상관없이 Kubernetes를 사용하면 조직이 모든 배포 위치에서 일관되게 최상의 방법, 거버넌스 컨트롤, 보안 조치, 모니터링 및 비용 관리를 수립할 수 있습니다. 이 표준화는 조직이 인수, 합병 또는 멀티 클라우드 설정과 같은 시나리오에서도 복잡성을 관리할 수 있도록합니다.\n- 미래를 대비하고 위험 감소: Kubernetes는 즉각적인 운영 요구 사항뿐만 아니라 미래를 대비하고 위험을 감소하는 데도 도움을 줍니다. Kubernetes에서 제공하는 표준은 장기 개발 노력을 지원하고 기술 부채를 완화합니다. 이는 CEO 및 CTO뿐만 아니라 엔지니어 및 개발팀에게도 중요한 문제입니다.\n\nAWS의 Kubernetes 투자: 5주년을 축하하는 Amazon EKS는 AWS의 Kubernetes에 대한 중요한 투자를 대표합니다. AWS는 EKS를 강력한 플랫폼으로 만들기 위해 성실히 노력하고 성능, 규모, 신뢰성 및 가용성에서 뛰어납니다. CNCF 및 AWS 고객의 데이터에 따르면, AWS에서実行되는 Kubernetes 워크로드가 다른 모든 플랫폼보다 많이 있으며, 매주 수십억 개의 EC2 인스턴스 시간이 EKS에서 실행됩니다.\n\n## EKS에서 고객들이 무엇을하고 있을까요?\n\n고객들은 EKS를 다양한 목적으로 활용하고 있습니다. 활동 범위는 레거시 .NET 및 Java 응용 프로그램을 클라우드로 이전하고 데이터 처리 작업을 실행하며 실시간 백엔드를 구축하고 웹 프론트 엔드를 개발하는 등 다양합니다.\n\n<div class=\"content-ad\"></div>\n\n요즘에는 EKS와 Kubernetes를 활용한 기계 학습과 AI의 중요성이 크게 늘어났습니다. 생성적 AI와 로봇공학과 같은 다양한 분야를 다루며, 특히 저는 자율 주행 차량에 특히 흥미를 가지고 있습니다. EKS에서 중요한 자율 주행 차량 훈련 활동이 진행 중이라는 점을 강조하고 싶습니다. 기계 학습, kube ray, Spark, kube flow와 같은 도구를 활용하여 미래 기술을 개발 중인 기업들을 관찰하는 것은 현재의 환경에서 주목할 만하고 흥미로운 측면입니다.\n\n## Kubernetes 운영 체제\n\n![이미지](/assets/img/2024-06-19-FutureofAmazonEKS_0.png)\n\n인프라 조사에서 Kubernetes가 핵심적인 역할을 하는 복잡한 계층을 관찰하며, 다양한 인프라 요소를 조율하여 원활하게 통합하는 모습을 보게 됩니다. 고객들은 Kubernetes 위에 배포, 관측, 거버넌스, 트래픽 제어, 보안 제어를 추가하여 적극 참여하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n놀랍게도 CNCF 내에서 599개의 프로젝트가 Kubernetes 내부 또는 옆에서 실행됩니다. 이 중 173개 프로젝트는 CNCF로부터 직접 지도 받는 개방형 지배를 갖고 있습니다. Kubernetes 위에 이러한 방대한 플랫폼 레이어를 관리하는 것은 상당한 일이죠.\n\n게다가 AWS는 자사의 서비스가 Kubernetes로의 기능 제공을 원활하게 하는 통합을 개발하기 위해 적극적으로 노력하고 있습니다. 또한, 통합 개발자 플랫폼(IDP)이 Kubernetes에서 응용 프로그램을 패키징하고 실행하며, 데이터 처리 작업을 조정하고, 기계 학습 워크플로를 관리하는 데 중요해집니다. 이 다양한 레이어를 통해 탐색이 진행됨에 따라 고객 경험 향상과 포장, 컨테이너, 실행 생태계 내에서 프로세스를 정교화하는 데 초점이 맞춰집니다.\n\n## AWS Kubernetes의 목표\n\n- 차별화되지 않은 무거운 작업 제거:\n\n<div class=\"content-ad\"></div>\n\n- AWS에서 Kubernetes를 관리하는 데 연관된 막거운 무거운 작업의 부담에서 해방된 고객들.\n- 운영 복잡성이 처리되어 사용자들이 핵심 역량에 초점을 맞추고 루틴적인 작업 관리에 관여하는 대신에 방향을 전환할 수 있게 합니다.\n\n2. 작업 단순화 및 액세스 부여:\n\n- AWS에서 Kubernetes 경험을 단순화하여 사용자들에게 간소화되고 사용자 친화적인 환경을 제공하는 것을 목표로 합니다.\n- 사용자들에게 클라우드의 규모, 안정성, 보안에 대한 완전한 액세스 권한이 부여되며, 운영 환경이 원활하고 효율적으로 관리되도록 보장합니다.\n\n3. 오픈 소스 표준의 계속적인 유지:\n\n<div class=\"content-ad\"></div>\n\n- 쿠버네티스의 오픈 소스 표준을 계속해서 유지보수합니다.\n- 이는 CNCF 내 599개 프로젝트와의 호환성을 보장하며 빠른 혁신을 가능하게 하고 커뮤니티 및 파트너의 새로운 개발에 접근하며 커뮤니티 표준의 계속된 향상을 도와줍니다.\n\n## AWS가 어떻게 Kubernetes를 지원하나요\n\nAWS 생태계 내에서 Kubernetes에 대한 포괄적인 지원에 참여하고 있는 Amazon EKS는 관리 버킷에서 중요한 요소로 두드러집니다. Kubernetes에 대한 AWS의 약속의 중심 역할을 하는 EKS는 고객에 견고하고 효율적인 Kubernetes 관리 솔루션을 제공하는 의지를 보여줍니다. EKS 외에도 AWS는 Kubernetes 배포의 개발, Kubernetes와 AWS 구성 요소 간의 연결을 용이하게 해주는 다양한 방식을 제공하며 상류 개발에 적극적으로 참여함으로써 포용합니다.\n\nAWS 팀은 CNCF의 보안 협의회 및 Kubernetes 프로젝트에 기여함으로써 안전에 대한 약속을 확장하고 견고한 안전 조치를 보장합니다. 더불어 AWS는 공동 비용 절감 및 공유 리소스의 가용성 향상에 기여하는 커뮤니티 프로젝트에 금융적 지원을 제공함으로써 중요한 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 어디서나 Kubernetes 실행하기\n\n![Future of Amazon EKS](/assets/img/2024-06-19-FutureofAmazonEKS_1.png)\n\nEKS를 중심으로, AWS에서의 Kubernetes 제공은 다양한 환경으로 확장되었습니다. EKS는 우리의 Distro 및 EKS Anywhere에 걸쳐 퍼져 있으며 온-프레미스 Kubernetes 배포를 위한 툴 체인을 포함합니다. 전통적인 AWS 지역을 넘어, EKS는 Snow, Outposts, 로컬 존 및 파장까지 다양한 환경에 확장되었습니다. 이 방대한 커버리지는 사용자가 AWS 인프라 또는 다양한 온-프레미스 시나리오에서 Kubernetes를 실행하더라도 일관되고 성능 좋은 Kubernetes 경험을 쉽게 얻을 수 있도록 합니다. AWS의 목표는 사용자에게 유연한 옵션을 제공하여 선택한 배포 환경에 관계없이 일관되고 신뢰할 수 있는 Kubernetes 경험을 보장하는 것입니다.\n\n# EKS 5주년\n\n<div class=\"content-ad\"></div>\n\n지난 5년 동안 이 제품에 투자된 상당한 개발을 되돌아보면, 2018년 EKS 발표 이후 222번 이상의 다양한 런칭이 진행되었다는 사실을 강조해야 합니다. 이러한 런칭은 가격 인하와 규정 준수 조치부터 새로운 프로젝트 시작, 클러스터 생성 시간 가속화, 새로운 인스턴스 및 지역 지원 추가, 중요한 기능 도입까지 다양한 향상을 포함하고 있습니다. \n\n노바 라일라툴 리즈키아","ogImage":{"url":"/assets/img/2024-06-19-FutureofAmazonEKS_0.png"},"coverImage":"/assets/img/2024-06-19-FutureofAmazonEKS_0.png","tag":["Tech"],"readingTime":5},{"title":"쿠버네티스 x509 인증서가 만료되었거나 아직 유효하지 않음 오류","description":"","date":"2024-06-19 13:09","slug":"2024-06-19-Kubernetesx509certificatehasexpiredorisnotyetvaliderror","content":"\n\n\n<img src=\"/assets/img/2024-06-19-Kubernetesx509certificatehasexpiredorisnotyetvaliderror_0.png\" />\n\n만약 잘 작동되던 쿠버네티스 환경에서 갑자기 \"x509: certificate has expired or is not yet valid\" 오류가 발생한다면 어떻게 하시겠어요?\n\n오늘 나에게 일어난 것과 똑같이요. 저도 이 오류를 경험했어요. 왜 나타나는지도 모르는데, 뭔가 조치를 취하지 않은 상태에서 발생한 문제였죠!\n\n저는 인증서 만료 문제라고 알고 있었지만, 문제를 어떻게 해결할지 알려고 많은 시간을 보냈어요. 그래서 이 주제에 대해 짧은 기사를 써서 이런 문제를 겪는 다른 분들에게 도움이 되길 바라는 마음으로 준비했어요.\n\n\n<div class=\"content-ad\"></div>\n\nKubernetes 문서에 따르면 kubeadm으로 생성된 클라이언트 인증서는 1년 후에 만료됩니다.\n\n그래서 제가 먼저 한 일은:\n\n- 인증서 세부 정보 확인하기\n\n```js\nkubeadm certs check-expiration\n```\n\n<div class=\"content-ad\"></div>\n\n2. 그런 다음 모든 인증서를 갱신했어요\n\n```js\nkubeadm certs renew all\n```\n\n3. 마지막으로 kubelet을 다시 시작했어요\n\n```js\nsudo systemctl restart kubelet\n```\n\n<div class=\"content-ad\"></div>\n\n하지만, 아무것도 바뀌지 않았네요! 계속해서 같은 오류가 발생하고 있습니다.\n\n여기서 뭔가 빠진 게 있는 것 같아요.\n\n이 문제를 조사하는 데 시간을 많이 쏟았는데, ~/.kube/config 파일을 살펴보던 중 예전 클라이언트 인증서 항목을 사용하고 있다는 것을 깨달았어요.\n\n그리고 kubeadm으로 클러스터를 구축할 때 /etc/kubernetes/admin.conf을 /.kube/config 파일로 복사해야 했다는 것을 기억했네요.\n\n<div class=\"content-ad\"></div>\n\n인증서를 갱신하면 /etc/kubernetes/admin.conf 파일이 업데이트되므로, 당연히 내 /.kube/config 파일도 그에 맞게 업데이트해야 했어요.\n\n다음 명령어를 실행하여 문제를 해결했어요.\n\n```js\nsudo cp -i /etc/kubernetes/admin.conf ~/.kube/config\nsudo chown $(id -u):$(id -g) ~/.kube/config\n```\n\n쿠버네티스 문서에 따르면:","ogImage":{"url":"/assets/img/2024-06-19-Kubernetesx509certificatehasexpiredorisnotyetvaliderror_0.png"},"coverImage":"/assets/img/2024-06-19-Kubernetesx509certificatehasexpiredorisnotyetvaliderror_0.png","tag":["Tech"],"readingTime":2},{"title":"DNS 성능을 모니터링하기 위해 알아야 할 모든 것","description":"","date":"2024-06-19 13:07","slug":"2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance","content":"\n\n\n![CoreDNS Monitoring](/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_0.png)\n\n# 📚 소개:\n\nDNS 집중 워크로드를 실행할 때 종종 DNS 쓰로틀링에 의한 간헐적인 CoreDNS 실패가 발생할 수 있습니다. 이러한 문제는 애플리케이션에 중대한 영향을 미칠 수 있습니다.\n\n이러한 중단은 서비스의 신뢰성과 성능에 영향을 미칠 수 있으므로 모니터링 솔루션이 필수적입니다.\n\n\n<div class=\"content-ad\"></div>\n\nAWS는 모니터링 목적으로 통합할 수 있는 오픈 소스 도구인 CloudWatch, Fluentd 및 Grafana를 제공합니다. 이 도구들은 CoreDNS를 모니터링하는 데 사용할 수 있습니다.\n\n# Kubernetes DNS 소개:\n\nKubernetes는 클러스터 내에서 서비스 검색에 DNS를 의존합니다. 팟에서 실행되는 응용 프로그램이 서로 통신해야 할 때, 그들은 주로 IP 주소가 아닌 도메인 이름을 사용하여 서비스를 참조합니다.\n\n이 때 Kubernetes DNS가 필요합니다. 이는 도메인 이름이 올바른 IP 주소로 해석되도록 보장하여 팟과 서비스가 통신할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\nKubernetes에서는 각 파드마다 일시적인 IP 주소가 할당됩니다. 그러나 이러한 IP 주소는 동적이며 시간이 지남에 따라 변경될 수 있어, 애플리케이션이 이를 추적하기 어렵습니다.\n\nKubernetes는 이러한 도전에 대응하기 위해 파드와 서비스에 완전히 정규화된 도메인 이름(FQDNs)을 할당합니다.\n\nKubernetes의 기본 DNS 제공자인 CoreDNS는 클러스터 내에서 DNS 쿼리를 처리하는 역할을 담당합니다. 그는 이러한 FQDN을 해당 IP 주소로 매핑하여 파드와 서비스 간의 통신을 가능하게 합니다.\n\n# 왜 DNS 문제가 흔할까요:\n\n<div class=\"content-ad\"></div>\n\n네트워크 문제 해결 중 공통적으로 발생하는 번거로움의 원인 중 하나인 DNS 문제! DNS는 사람이 읽기 쉬운 도메인 이름을 기계가 이해할 수 있는 IP 주소로 변환하는 데 큰 역할을 합니다.\n\n그러나 DNS 문제는 설정 오류, 네트워크 문제 또는 서버 장애와 같은 여러 요인으로 발생할 수 있습니다. 도메인 이름을 올바르게 해석하지 못할 때 애플리케이션이 외부 서비스에 연결 문제를 경험하거나 액세스에 실패할 수 있습니다.\n\n# 쿠버네티스의 CoreDNS:\n\nCoreDNS는 쿠버네티스 클러스터 내에서 DNS 서비스를 제공하는 데 중요한 역할을 합니다. Kubernetes v1.13 이후 기본 DNS 제공자로 사용되어 온 CoreDNS는 DNS 이름 대신 IP 주소 대신 DNS 이름을 사용하여 클라이언트가 서비스에 액세스할 수 있도록 함으로써 클러스터 네트워킹을 간소화합니다. 그는 도메인 이름 요청을 해결하고 클러스터 내에서 서비스 검색을 용이하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 코어디엔에스의 작동 방식:\n\n코어디엔에스는 쿠버네티스 클러스터 내에서 DNS 요청의 리졸버 및 포워더로 작동합니다. 파드가 다른 서비스와 통신해야 할 때, 대상 서비스의 도메인 이름을 지정하여 DNS 쿼리를 코어디엔에스에 보냅니다. 그럼 코어디엔에스는 내부 레코드를 사용하여 도메인 이름을 해당 IP 주소로 매핑하여 이 쿼리를 해결합니다.\n\n코어디엔에스가 권한이 없는 외부 도메인 이름에 대해서는, 이를 공개 리졸버나 상위 DNS 서버로 전달하여 해결합니다.\n\n성능을 향상시키고 대기 시간을 줄이기 위해 코어디엔에스는 자주 액세스하는 도메인 이름에 대한 DNS 응답을 캐시할 수 있습니다. 이 캐싱 메커니즘은 DNS 쿼리의 응답 속도를 향상시키고 상위 DNS 서버의 부하를 줄입니다.\n\n<div class=\"content-ad\"></div>\n\nCoreDNS는 이 기능을 모듈식 아키텍처와 확장 가능한 플러그인 시스템을 통해 구현하며, 운영자가 독자적인 요구 사항에 따라 DNS 해상도를 사용자 정의하고 최적화할 수 있게 합니다.\n\n# Amazon EKS에서의 CoreDNS 쓰로틀링 완화 방법:\n\nAmazon EKS 클러스터에서 CoreDNS와 DNS 쓰로틀링 문제는 식별하고 해결하기 어려울 수 있습니다. 많은 사용자가 CoreDNS 로그와 메트릭을 모니터링하는 데 주력하지만, 엘라스틱 네트워크 인터페이스(ENI) 수준에서 강제되는 초당 1024개 패킷(PPS)의 하드 제한을 자주 간과합니다. 이 한계가 쓰로틀링 문제로 이어질 수 있는 방식을 이해하려면 Kubernetes 파드의 전형적인 DNS 해상도 흐름에 대한 통찰력이 필요합니다.\n\nKubernetes 환경에서는 파드가 통신을 가능하게 하기 위해 내부 및 외부 서비스의 도메인 이름을 해상해야 합니다. 이 해상도 프로세스는 DNS 쿼리를 워커 노드의 ENI를 통해 라우팅하는 것을 포함하며, 특히 외부 엔드포인트를 해상하는 경우입니다. 내부 엔드포인트의 경우에도 쿼리하는 파드와 동일한 위치에 CoreDNS 파드가 없으면 DNS 패킷이 여전히 워커 노드의 ENI를 통해 이동합니다.\n\n<div class=\"content-ad\"></div>\n\n갑자기 DNS 쿼리가 급증하여 PPS가 하드 제한값 1024에 접근하는 상황이 발생할 수 있습니다. 이러한 상황은 DNS 쓰로틀링을 유발할 수 있으며, 이로 인해 영향을 받는 작업 노드에서 실행 중인 모든 마이크로서비스에 영향을 미칠 수 있습니다. 유감스럽게도, 이러한 문제 해결은 주로 CoreDNS pod에 초점을 맞추는 ENI 메트릭보다 어려울 수 있습니다.\n\nEKS 클러스터에서 DNS 쓰로틀링 문제를 완화하기 위해서는 ENI 수준에서 발생하는 패킷 손실을 지속적으로 모니터링하는 것이 중요합니다. 이 모니터링을 통해 잠재적인 중단을 조기에 감지하고 예방할 수 있습니다. 이 블로그 포스트에서는 네트워크 성능 메트릭을 활용하여 DNS 쓰로틀링 문제를 효과적으로 식별하는 솔루션을 소개합니다.\n\n## 해결책: 🎉\n\n작업 노드에서 DNS 쓰로틀링 문제를 식별하는 간단한 방법은 Elastic Network Adapter (ENA) 드라이버에서 제공하는 linklocal_allowance_exceeded 메트릭 및 다른 메트릭을 캡처하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n`linklocal_allowance_exceeded`은 로컬 프록시 서비스로의 트래픽 PPS가 네트워크 인터페이스의 최대를 초과하여 드롭된 패킷 수입니다. 이는 DNS 서비스, 인스턴스 메타데이터 서비스 및 Amazon 시간 동기화 서비스로의 트래픽에 영향을 줍니다.\n\n이 이벤트를 실시간으로 추적하는 대신, 우리는 이 메트릭을 Amazon Managed Service for Prometheus로 스트리밍하고 Amazon Managed Grafana에서 시각화할 수도 있습니다.\n\n![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_1.png)\n\n# 실전: AWS EKS에서 CoreDNS 메트릭 수집 및 시각화:\n\n<div class=\"content-ad\"></div>\n\nCoreDNS 프로메테우스 플러그인은 OpenMetrics 형식으로 메트릭을 노출하며, 이는 프로메테우스 형식에서 발전한 텍스트 기반 표준입니다. Kubernetes 클러스터에서는 플러그인이 기본적으로 활성화되어 있어 클러스터를 시작하는 즉시 많은 중요한 메트릭을 모니터링할 수 있습니다.\n\n기본 설정에서 프로메테우스 플러그인은 각 CoreDNS 팟의 포트 9153에 있는 /metrics 엔드포인트에 메트릭을 기록합니다.\n\nAmazon Managed Service for Prometheus 워크스페이스와 Managed Service for Grafana를 생성하세요:\n\n이 단계에서는 Amazon Managed Service for Prometheus 및 Managed Service for Grafana를 위한 워크스페이스를 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n이 파일 구성은 다음을 생성합니다:\n\n- AMP 작업 공간\n- AMP 경보 관리자 정의.\n\nmain.tf:\n\n```js\nmodule \"prometheus\" {\n  source = \"terraform-aws-modules/managed-service-prometheus/aws\"\n\n  workspace_alias = \"demo-coredns\"\n\n  alert_manager_definition = <<-EOT\n  alertmanager_config: |\n    route:\n      receiver: 'default'\n    receivers:\n      - name: 'default'\n  EOT\n\n  rule_group_namespaces = {}\n}\n```\n\n<div class=\"content-ad\"></div>\n\nversions.tf:\n\n```js\nterraform {\n  required_version = \">= 1.3\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 5.32\"\n    }\n  }\n}\n```\n\n테라폼을 실행하려면 다음을 실행해야 합니다:\n\n```js\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\n<div class=\"content-ad\"></div>\n\n아래 구성 파일은 다음과 같은 것을 생성합니다:\n\n- 기본 Grafana 작업 공간 (모듈에서 제공하는 기본값 사용)\n\nmain.tf:\n\n```js\nprovider \"aws\" {\n  region = local.region\n}\n\ndata \"aws_availability_zones\" \"available\" {}\n\nlocals {\n  region      = \"eu-west-1\"\n  name        = \"amg-ex-${replace(basename(path.cwd), \"_\", \"-\")}\"\n  description = \"AWS Managed Grafana service for ${local.name}\"\n\n  vpc_cidr = \"10.0.0.0/16\"\n  azs      = slice(data.aws_availability_zones.available.names, 0, 3)\n}\n\n################################################################################\n# Managed Grafana Module\n################################################################################\n\nmodule \"managed_grafana\" {\n  source = \"terraform-aws-modules/managed-service-grafana/aws\"\n\n  # Workspace\n  name                      = local.name\n  associate_license         = false\n  description               = local.description\n  account_access_type       = \"CURRENT_ACCOUNT\"\n  authentication_providers  = [\"AWS_SSO\"]\n  permission_type           = \"SERVICE_MANAGED\"\n  data_sources              = [\"CLOUDWATCH\", \"PROMETHEUS\", \"XRAY\"]\n  notification_destinations = [\"SNS\"]\n  stack_set_name            = local.name\n  grafana_version           = \"9.4\"\n\n  configuration = jsonencode({\n    unifiedAlerting = {\n      enabled = true\n    },\n    plugins = {\n      pluginAdminEnabled = false\n    }\n  })\n\n  # vpc configuration\n  vpc_configuration = {\n    subnet_ids = module.vpc.private_subnets\n  }\n  security_group_rules = {\n    egress_postgresql = {\n      description = \"Allow egress to PostgreSQL\"\n      from_port   = 5432\n      to_port     = 5432\n      protocol    = \"tcp\"\n      cidr_blocks = module.vpc.private_subnets_cidr_blocks\n    }\n  }\n\n  # Workspace API keys\n  workspace_api_keys = {\n    viewer = {\n      key_name        = \"viewer\"\n      key_role        = \"VIEWER\"\n      seconds_to_live = 3600\n    }\n    editor = {\n      key_name        = \"editor\"\n      key_role        = \"EDITOR\"\n      seconds_to_live = 3600\n    }\n    admin = {\n      key_name        = \"admin\"\n      key_role        = \"ADMIN\"\n      seconds_to_live = 3600\n    }\n  }\n\n  # Workspace IAM role\n  create_iam_role                = true\n  iam_role_name                  = local.name\n  use_iam_role_name_prefix       = true\n  iam_role_description           = local.description\n  iam_role_path                  = \"/grafana/\"\n  iam_role_force_detach_policies = true\n  iam_role_max_session_duration  = 7200\n  iam_role_tags                  = { role = true }\n\n\n  tags = local.tags\n}\n\nmodule \"managed_grafana_default\" {\n  source = \"terraform-aws-modules/managed-service-grafana/aws\"\n\n  name              = \"${local.name}-default\"\n  associate_license = false\n\n  tags = local.tags\n}\n\nmodule \"managed_grafana_disabled\" {\n  source = \"terraform-aws-modules/managed-service-grafana/aws\"\n\n  name   = local.name\n  create = false\n}\n\n################################################################################\n# Supporting Resources\n################################################################################\n\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~> 5.0\"\n\n  name = local.name\n  cidr = local.vpc_cidr\n\n  azs             = local.azs\n  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]\n  public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 48)]\n\n  enable_nat_gateway = false \n  single_nat_gateway = true\n\n  tags = local.tags\n}\n```\n\n<div class=\"content-ad\"></div>\n\n\"versions.tf\" 파일입니다:\n\n```js\nterraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 5.0\"\n    }\n  }\n}\n```\n\n이 코드를 실행하려면 다음을 실행해야 합니다:\n\n```js\n$ terraform init\n$ terraform plan\n$ terraform apply\n```\n\n<div class=\"content-ad\"></div>\n\n프로메테우스 ethtool 익스포터 배치:\n\nethtool은 워커 노드의 이더넷 장치에 대한 정보를 구성하고 수집하는 리눅스 도구입니다. 우리는 ethtool의 출력을 사용하여 패킷 손실을 감지하고 이를 프로메테우스 ethtool 익스포터 유틸리티를 사용하여 프로메테우스 형식으로 변환할 것입니다.\n\n배포에는 ethtool에서 정보를 가져 와서 프로메테우스 형식으로 게시하는 Python 스크립트가 포함되어 있습니다.\n\n```js\nkubectl apply -f https://raw.githubusercontent.com/Showmax/prometheus-ethtool-exporter/master/deploy/k8s-daemonset.yaml\n```\n\n<div class=\"content-ad\"></div>\n\n이제 ADOT 수집기를 배포하고 ADOT 수집기를 구성하여 Amazon Managed Service for Prometheus로 메트릭을 수집할 것입니다.\n\n우리는 Amazon EKS 애드온을 사용하여 ADOT 오퍼레이터를 CoreDNS 모니터링을 위해 메트릭 \"linklocal_allowance_exceeded\"를 Amazon Managed Service for Prometheus로 보내게 될 것입니다.\n\nIAM 역할과 Amazon EKS 서비스 계정을 생성하세요.\n\n<div class=\"content-ad\"></div>\n\nADOT 수집기를 Kubernetes 서비스 계정 \"adot-collector\"의 신원으로 배포할 예정입니다. 서비스 계정에 IAM 역할(IRSA)을 연결하여 Kubernetes 서비스 계정에 AmazonPrometheusRemoteWriteAccess 역할을 할당함으로써 해당 서비스 계정을 활용하는 모든 파드가 Amazon Managed Service for Prometheus에 메트릭을 수집하는 데 필요한 IAM 권한을 부여할 수 있습니다.\n\n스크립트를 실행하려면 kubectl 및 eksctl CLI 도구가 필요합니다. 이 도구들은 Amazon EKS 클러스터에 액세스할 수 있도록 구성되어 있어야 합니다.\n\n```js\neksctl create iamserviceaccount \\\n--name adot-collector \\\n--namespace default \\\n--region eu-west-1\\\n--cluster coredns-monitoring-demo\\\n--attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\\n--approve \\\n--override-existing-serviceaccounts\n```\n\nADOT 애드온 설치하기:\n\n<div class=\"content-ad\"></div>\n\n여러분은 다음 명령어를 사용하여 Amazon EKS의 다른 버전에 활성화된 애드온 목록을 확인할 수 있습니다:\n\n클러스터 버전에서 지원하는 ADOT 버전을 확인하십시오.\n\n```js\naws eks describe-addon-versions --addon-name adot --kubernetes-version 1.28 \\\n  --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text\n```\n\n다음 명령어를 실행하여 ADOT 애드온을 설치하십시오. 위 단계에서 기술된 것에 따라 --addon-version 플래그를 교체하십시오.\n\n<div class=\"content-ad\"></div>\n\n```js\naws eks create-addon --addon-name adot --addon-version v0.66.0-eksbuild.1 --cluster-name coredns-monitoring-demo\n```\n\n다음 명령어를 사용하여 ADOT 애드온이 준비되었는지 확인하세요.\n\n```js\nkubectl get po -n opentelemetry-operator-system\n```\n\n다음 절차는 배포를 모드 값으로 사용하는 예제 YAML 파일을 사용합니다. 이는 기본 모드이며 ADOT Collector를 독립 애플리케이션과 유사하게 배포합니다. 이 구성은 샘플 애플리케이션으로부터 OTLP 메트릭을 수신하고 클러스터의 pod에서 스크래핑된 Amazon Managed Service for Prometheus 메트릭을 수신합니다.\n\n<div class=\"content-ad\"></div>\n\n\n\ncurl -o collector-config-amp.yaml https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-amp.yaml\n\n\ncollector-config-amp.yaml 파일에서 다음을 본인의 값으로 바꿔주세요: * mode: deployment * serviceAccount: adot-collector * endpoint: \"\" * region: \"\" * name: adot-collector\n\n\nkubectl apply -f collector-config-amp.yaml\n\n\nadot collector가 배포되면 메트릭이 Amazon Prometheus에 성공적으로 저장됩니다.\n\n\n<div class=\"content-ad\"></div>\n\nAmazon Managed Grafana에서 ethtool 메트릭을 시각화해보세요:\n\nAmazon Managed Grafana 콘솔 내에서 Prometheus 워크스페이스를 데이터 소스로 구성합니다.\n\n이제 Amazon Managed Grafana에서 메트릭을 살펴봅시다: \"탐색\" 버튼을 클릭한 후 ethtool을 검색하세요:\n\n![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_2.png)\n\n<div class=\"content-ad\"></div>\n\n링크로컬 할당 초과 메트릭을 사용하여 대시보드를 만들어 봅시다. 쿼리는 다음과 같습니다.\n\n```js\nrate(node_net_ethtool{device=\"eth0\",type=\"linklocal_allowance_exceeded\"} [30s])\n```\n\n![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_3.png)\n\n값이 0이기 때문에 드랍된 패킷이 없다는 것을 확인할 수 있습니다. Amazon Managed Service for Prometheus의 경고 관리자에서 알림을 구성하여 알림을 보낼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론:\n\n이 글에서는 AWS Distro for OpenTelemetry (ADOT), Amazon Managed Service for Prometheus 및 Amazon Managed Grafana를 사용하여 CoreDNS 쓰로틀링 문제를 모니터링하고 경고를 생성하는 방법을 보여드렸습니다. CoreDNS 메트릭을 모니터링함으로써 고객은 패킷 손실을 사전에 감지하고 예방 조치를 취할 수 있습니다.\n\n다음에 또 만나요 🇵🇸 🎉\n\n![이미지](/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_4.png)\n\n<div class=\"content-ad\"></div>\n\n읽어 주셔서 감사합니다!! 🙌🏻😁📃, 다음 블로그에서 만나요.🤘🇵🇸\n\n🚀 끝까지 함께해 줘서 감사합니다. 이 블로그에 관한 질문/피드백이 있으면 언제든지 연락해 주세요:\n\n♻️ 🇵🇸LinkedIn: https://www.linkedin.com/in/rajhi-saif/\n\n♻️🇵🇸 Twitter : https://twitter.com/rajhisaifeddine\n\n<div class=\"content-ad\"></div>\n\n끝! ✌🏻\n\n# 🔰 계속 배우고!! 계속 공유해요!! 🔰\n\n# 참고:\n\n[https://aws.amazon.com/blogs/mt/monitoring-coredns-for-dns-throttling-issues-using-aws-open-source-monitoring-services/](https://aws.amazon.com/blogs/mt/monitoring-coredns-for-dns-throttling-issues-using-aws-open-source-monitoring-services/)","ogImage":{"url":"/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_0.png"},"coverImage":"/assets/img/2024-06-19-EverythingyouneedtoknowaboutmonitoringCoreDNSforDNSperformance_0.png","tag":["Tech"],"readingTime":13},{"title":"Kubernetes 보안 마스터하기 - 내 Admission Controllers 여행","description":"","date":"2024-06-19 13:06","slug":"2024-06-19-MasteringKubernetesSecurityMyJourneyWithAdmissionControllers","content":"\n\nKubernetes는 Admission Controllers라는 확장 포인트를 포함하고 있습니다. 이들은 Kubernetes 클러스터의 문지기 역할을 하며 들어오고 나가는 모든 자원 요청을 감독합니다. 단순한 감시뿐만 아니라 이들 컨트롤러는 정교한 필터 역할을 합니다.\n\n그들은 조직의 정책을 집행하고 규정 준수를 보장하며 요청을 미리 정의된 표준에 맞게 수정할 수 있습니다.\n\n이 기사는 Kubernetes 보안 시리즈의 네 번째로 Admission Controllers에 중점을 둡니다.\n\n인증된 Kubernetes 보안 (CKS) 자격증 획득을 향해 진로를 나아가면서, CKS 커리큘럼의 \"마이크로서비스 취약점 최소화\" 섹션에서 Admission Controllers의 중요성을 탐구하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 어드미션 컨트롤러란 무엇인가요?\n\n어드미션 컨트롤러는 쿠버네티스 생태계에서 중요한 역할을 하는데, 쿠버네티스 API 서버의 출입구 역할을 합니다.\n\n어드미션 컨트롤러는 요청이 인증되고 권한이 부여된 후에 호출되며, 해당 객체가 쿠버네티스 클러스터에 지속되기 전에 작동합니다. 그들의 주요 기능은 요청을 가로채고 처리하여 클러스터의 운영 무결성과 보안을 유지하는 것입니다.\n\n쿠버네티스에서 어드미션 컨트롤러는 기본적으로 클러스터의 사용 방법을 지배하고 강제화하는 플러그인입니다. 이들은 쿠버네티스 API 서버로의 요청(예: 리소스 생성, 수정 또는 삭제)을 미리 정의된 규칙과 정책에 맞춰평가합니다.\n\n<div class=\"content-ad\"></div>\n\n## 두 가지 유형의 입장 컨트롤러: 변형 및 확인\n\n입장 컨트롤러는 변형과 확인 두 가지 주요 유형으로 분류될 수 있습니다.\n\n변형 입장 컨트롤러는 그들이 수락하는 객체들을 수정할 수 있습니다. 이는 객체가 특정 규칙을 준수하거나 추가 메타데이터로 객체를 향상시킬 때 Kubernetes API 서버에 의해 처리되기 전에 요청 내용을 변경할 수 있음을 의미합니다.\n\n예를 들어, 변형 입장 컨트롤러는 모니터링 에이전트가 모든 작업 부하에 존재함을 보장하기 위해 사이드카 컨테이너를 자동으로 주입할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n한편, 유효성 검사 입합기는 객체를 수정하지 않습니다. 대신 요청이 모든 필요한 기준을 충족하는지 확인합니다. 예를 들어, 배포가 이미지의 최신 버전을 사용하는지 보장할 수 있습니다.\n\n만약 요청이 이러한 검사를 통과하지 못하면, 유효성 검사 입합기는 작업을 거부하고 객체가 생성, 수정 또는 삭제되지 않습니다.\n\n이 프로세스는 조직 정책을 강제하는 데 중요하며 클러스터에 적용되는 규정 준수 및 안전한 구성만이 보장됩니다.\n\n특정 요구 사항에 맞는 사용자 정의 유효성 검사 입합기를 만들 수 있지만, Kubernetes 클러스터에는 이미 내장된 유효성 검사 입합기 스위트가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래 섹션에서는 미리 구성된 컨트롤러의 역할과 기능을 탐색할 것입니다.\n\n## Kubernetes 내장 Admission 컨트롤러\n\n기본 Kubernetes 설치에는 자체 내장 Admission 컨트롤러 모음이 자동으로 포함되어 활성화됩니다. 대부분의 관리자와 사용자에게는 이러한 컨트롤러를 만지는 일이 거의 없고, 기본적으로 활성화된 것을 비활성화할 필요도 거의 없을 것입니다.\n\n그러나 클러스터의 동작을 사용자 정의하거나 활성화된 Admission 컨트롤러를 확인해야 하는 경우, Kubernetes는 제어 플레인에서 직접 수행할 수 있는 간단한 방법을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n현재 활성화된 입장 컨트롤러 목록을 확인하려면 제어 플레인 터미널에서 다음 명령을 실행하세요:\n\n```js\nkube-apiserver -h | grep enable-admission-plugins\n```\n\n기본적으로 설치된 주요 입장 컨트롤러와 쿠버네티스 생태계에 대한 기여에 대해 간략히 살펴보겠습니다. 입장 컨트롤러는 쿠버네티스 버전에 따라 다양하며, 이것들은 v1.29에서 내장된 것들입니다.\n\n- NamespaceLifecycle — 삭제 중인 네임스페이스에 대한 요청을 차단하고 예약된 이름과 일치하는 경우 새로운 네임스페이스를 생성할 수 없도록 보장합니다. 네임스페이스별 리소스의 무결성을 유지하는 데 중요합니다.\n- LimitRanger — LimitRange 개체로 지정된 제약 조건을 네임스페이스 내의 Pod, Container 및 PersistentVolumeClaim 리소스에 적용합니다. 관리자가 정의한 제한을 초과하지 않도록하여 자원 사용을 효율적으로 할당하고 낭용을 방지합니다.\n- ServiceAccount — 명시적 ServiceAccount가 지정되지 않은 Pod에 자동으로 기본 서비스 계정을 첨부합니다. 권한을 관리하고 적절한 수준의 액세스로 Pod가 Kubernetes API에 안전하게 액세스할 수 있도록 중요합니다.\n- PersistentVolumeClaimResize — 기존 PersistentVolumeClaim (PVC)의 크기를 다시 조정할 수 있게 합니다. 이 기능을 통해 응용 프로그램 요구 사항이 변경될 때 저장소 리소스를 조정하는 작업이 간편해집니다.\n- PodSecurity — 사전 정의된 Pod 보안 설정을 강화하는 Pod Security Standards를 적용합니다. 지정된 보안 요구 사항을 충족하지 않는 Pod의 생성을 방지하여 클러스터 내 보안 취약성 위험을 크게 줄입니다.\n\n그 외에도 다음과 같은 입장 컨트롤러가 있습니다:\n\n- MutatingAdmissionWebhook 및 ValidatingAdmissionWebhook — 외부 서비스에 의해 강제된 사용자 정의 입장 정책을 허용하는 웹훅입니다. 동적 입장 제어에 대한 섹션에서 논의할 예정입니다.\n- ResourceQuota — 네임스페이스 내 리소스 할당량 제한을 적용하여 CPU, 메모리, 저장소 및 Pod, 서비스 등의 개수를 다룹니다. 클러스터 리소스의 공정한 사용을 장려하고 모든 서비스 및 사용자 간에 공정한 사용을 촉진합니다.\n- Priority — PriorityClass 이름을 기반으로 Pod의 스케줄링 우선 순위를 결정합니다. 특정 애플리케이션의 중요성을 기준으로 Pod 스케줄링의 우선 순위를 설정하여 중요한 애플리케이션이 최적으로 실행될 수 있도록 보장합니다.\n- RuntimeClass — 컨테이너 런타임 구성의 선택을 지원합니다. Pod에 대해 서로 다른 컨테이너 런타임을 허용하여 런타임별 기능 및 최적화를 간소화합니다.\n- DefaultStorageClass, DefaultIngressClass 및 DefaultTolerationSeconds — 명시적으로 지정되지 않은 경우 Pod에 대해 저장소 클래스, 인그레스 클래스 및 허용 시간의 기본값을 자동으로 설정합니다. 구성을 간소화하고 기본 정책이 클러스터 전체에 일관되게 적용되도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 쿠버네티스의 동적 입장 제어: 클러스터 보안 및 관리 향상\n\n동적 입장 제어는 사용자 정의 정책을 강요하고 MutatingAdmissionWebhook 및 ValidatingAdmissionWebhook 두 가지 중요한 구성 요소를 통해 클러스터 관리를 간소화하는 강력한 메커니즘입니다.\n\n## 동적 입장 제어 컨트롤러 이해\n\n동적 입장 제어 컨트롤러에는 MutatingAdmissionWebhook 및 ValidatingAdmissionWebhook 두 가지 종류가 있습니다. 이 컨트롤러는 외부 서비스에서 정의된 사용자 정책에 따라 쿠버네티스 API 서버에 대한 요청을 허용하거나 거부하는 게이트키퍼 역할을 합니다.\n\n<div class=\"content-ad\"></div>\n\n## MutatingAdmissionWebhook: 변환자\n\nMutatingAdmissionWebhook는 변환하는 어드미션 컨트롤러이며, 이전 섹션에서 본 것처럼 쿠버네티스 API 서버가 처리하기 전에 들어오는 요청을 수정하거나 변환하는 데 사용됩니다. 이 능력은 다음과 같은 시나리오에 유용합니다:\n\n- 사이드카 컨테이너 주입: 팟에 보조 컨테이너를 자동으로 추가하여 로깅, 모니터링 또는 네트워크 트래픽 제어에 사용할 수 있습니다.\n- 구성 변경: 조직 표준을 준수하도록 포드 사양을 수정하여 레이블이나 환경 변수를 추가하는 것과 같은 작업을 수행할 수 있습니다.\n\n## ValidatingAdmissionWebhook: 게이트키퍼\n\n<div class=\"content-ad\"></div>\n\n한편, ValidatingAdmissionWebhook은 미리 정의된 규칙에 대한 요청을 검증하여 계속 진행하기 전에 확인하는 데 중점을 둔 유효성 검사 입장 컨트롤러입니다. 다음과 같은 중요한 역할을 합니다:\n\n- 사용자 지정 리소스 할당량 강제 적용: 리소스 생성이 조직 정책에서 설정한 한도를 초과하지 않도록 보장합니다.\n- 사용자 지정 보안 정책 확인: 구성이 보안 표준을 준수하는지 확인하여 컨테이너가 루트 사용자가 아닌 사용자로 실행되도록 합니다.\n\n# 웹훅 Admission 컨트롤러 구현\n\n사용자 정의 Admission 컨트롤러를 통합하거나 개발하는 두 가지 방법이 있습니다. 전통적인 방법은 Go와 같은 프로그래밍 언어를 사용하여 사용자 정의 Admission 컨트롤러를 생성하는 것인데, 이는 강력한 제어를 제공하지만 심층적인 Kubernetes 생태계 지식이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n대신, 쿠버네티스는 MutatingAdmissionWebhook 및 ValidatingAdmissionWebhook이라는 웹훅 어드미션 컨트롤러 덕분에 더 접근성이 좋은 옵션을 제공합니다. 이를 통해 개발자는 쿠버네티스 API 서버가 웹훅을 통해 통신할 수 있는 REST API 서비스를 구현하여 사용자 정의 어드미션 로직을 도입할 수 있습니다.\n\n## 배포 유연성\n\n웹훅을 호스팅하는 REST API 서비스는 쿠버네티스 클러스터 내부나 외부 중 어디에나 배포할 수 있습니다. 이러한 유연성은 다양한 배포 시나리오와 아키텍처 디자인을 지원합니다.\n\n## 언어 중립성\n\n<div class=\"content-ad\"></div>\n\n웹훅 기반 컨트롤러의 중요한 장점 중 하나는 언어에 구애받지 않는다는 점입니다. 개발자들은 Node.js, Java, C# 또는 기타 선호하는 프로그래밍 언어로 REST API 서비스를 구현할 수 있어 팀의 전문지식에 기반한 더 넓은 채택과 사용자 정의를 유도할 수 있습니다.\n\n## 운영 메커니즘\n\n두 종류의 웹훅은 JSON 형식의 직렬화된 AdmissionReview 객체와 상호작용하여 작동합니다. ValidatingAdmissionWebhook은 객체를 사용하여 허용/거부 결정을 내릴 수 있어야 합니다.\n\n동시에 MutatingAdmissionWebhook은 요청 페이로드를 변경하여 Kubernetes 리소스의 동작을 동적으로 강화하거나 수정하는 다재다능한 메커니즘을 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nGitHub에서 웹훅 구현을 보여주는 다양한 예제를 찾을 수 있어요. 예를 들면, 이겪은 Python을 사용합니다. [https://github.com/garethr/kubernetes-webhook-examples](https://github.com/garethr/kubernetes-webhook-examples)\n\n# Admission Controllers와 CKS 시험 준비\n\nCKS (Certified Kubernetes Security Specialist) 자격증 시험을 통과하기 위해 나의 여정 중, 마이크로서비스 취약점을 최소화하는 막대 아래에 있는 Admission Controllers 섹션에 도달했어요.\n\n아직 시험을 보지는 않았지만, 커스텀 Admission Controller를 제로부터 만들거나 검증을 위한 웹훅의 코딩 세부 사항에 대해 물어보지는 않을 거라고 생각해요.\n\n<div class=\"content-ad\"></div>\n\n그 대신, 나는 더 가능성이 높은 도전 과제에 대비하고 있어요: 어떤 입학 컨트롤러가 켜져 있는지 끄는지 알아내고 이러한 설정을 어떻게 조정하는지 알아내야 해요. 이것은 CKS가 추구하는 것과 더 일치하는데, 실제 Kubernetes 보안에서 요구되는 실용적이고 실무적인 기술에 부합합니다.\n\n# 마무리: Kubernetes의 문지기들\n\nCKS 시험 준비를 시작하기 전에, 입학 컨트롤러가 존재하는 것조차 몰랐어요. 지금은 몇 개의 입학 컨트롤러를 사용하여 배포에 레이블과 같은 기본적인 것들을 추가했어요.\n\n나의 공부를 통해, 다양한 작업을 위해 입학 컨트롤러를 활용하는 많은 도구와 구현을 발견했어요. 예를 들어, 특정 이미지의 최신 버전만 사용되도록 하는 것과 같이 컨테이너 이미지 스캔을 입학 컨트롤러와 통합하는 것이 가능해요.\n\n<div class=\"content-ad\"></div>\n\n더 나아가 몇 가지 도구는 입학 컨트롤러의 기초를 기반으로하여, 특정 보안 정책을 구현하는 과정을 간소화하는 계층을 추가합니다. 사용자 정의 정책 설명 언어를 사용하여 구체적인 보안 정책을 구현하는 과정을 단순화하는 도구들이 있습니다.\n\n그 중 하나가 OPA 또는 Open Policy Agent입니다. 이는 CKS 시험의 요구 사항 중 하나인 것으로 알고 있습니다. 다음 글에서 OPA에 대해 자세히 다룰 계획입니다.\n\n열심히 공부하세요!","ogImage":{"url":"/assets/img/2024-06-19-MasteringKubernetesSecurityMyJourneyWithAdmissionControllers_0.png"},"coverImage":"/assets/img/2024-06-19-MasteringKubernetesSecurityMyJourneyWithAdmissionControllers_0.png","tag":["Tech"],"readingTime":7},{"title":"온프레미스 쿠버네티스 대 관리형 쿠버네티스","description":"","date":"2024-06-19 13:04","slug":"2024-06-19-On-PremisesKubernetesVsManagedKubernetes","content":"\n\n쿠버네티스는 컨테이너화된 응용 프로그램을 관리하는 강력한 오케스트레이션 도구로, 다양한 방식으로 배포할 수 있습니다. 가장 흔한 두 가지 방법은 온프레미스 쿠버네티스와 관리형 쿠버네티스입니다. 이 옵션들을 일상 생활에서의 간단한 비유를 사용하여 설명하고, 서로 다른 점을 이해하고 어떤 것이 당신의 상황에 가장 적합한지 결정하는 데 도움이 되도록 해보겠습니다.\n\n![온프레미스 쿠버네티스 대 관리형 쿠버네티스](/assets/img/2024-06-19-On-PremisesKubernetesVsManagedKubernetes_0.png)\n\n온프레미스 쿠버네티스: 직접 관리하는 방식\n\n온프레미스 쿠버네티스는 자동차를 소유하는 것과 같습니다. 자동차를 소유하면서 할 수 있는 것들:\n\n<div class=\"content-ad\"></div>\n\n1. 구매 및 설정: 차량을 구매하며, 이는 비용이 많이 들 수 있으며 모델, 색상 및 기능 선택과 같은 모든 설정에 책임이 있습니다.\n\n2. 유지 보수: 오일 교환부터 브레이크 수리까지 모든 유지 보수를 당신이 처리해야 합니다. 이는 차량 관리 방법을 알거나 이를 담당할 전문가를 고용해야 한다는 뜻입니다.\n\n3. 통제: 차량을 완전히 통제합니다. 운전 시간과 장소를 결정하며 마음껏 사용할 수 있습니다.\n\n4. 비용: 초기 비용과 계속되는 유지 보수 비용이 있지만 매달 대여료는 없습니다.\n\n<div class=\"content-ad\"></div>\n\n동일한 방식으로, 온프레미스 Kubernetes는 다음을 의미합니다:\n\n- 인프라 소유권: Kubernetes가 실행되는 서버와 하드웨어를 소유하고 있습니다. 이 인프라를 구매, 설정 및 유지보수해야 합니다.\n\n- 완전한 제어: Kubernetes 환경을 완전히 제어할 수 있습니다. 특정한 요구 사항에 맞게 확장하여 사용할 수 있습니다.\n\n- 유지보수 책임: 모든 업데이트, 패치 및 시스템 모니터링에 대한 책임이 있습니다. 이를 위해 적절한 전문 지식을 가진 팀이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n• 비용 고려 사항: 하드웨어 및 소프트웨어에 상당한 초기 비용이 들지만, 클라우드 서비스와 관련된 반복 비용을 피할 수 있습니다.\n\n## 관리형 Kubernetes: 차를 빌리는 것\n\n관리형 Kubernetes는 차를 빌리는 것과 비슷합니다:\n\n1. 쉬운 접근: 렌터카 회사에서 차를 선택하고 준비된 차량을 제공받습니다. 구매 프로세스를 걱정할 필요가 없습니다.\n\n<div class=\"content-ad\"></div>\n\n2. 유지보수가 필요 없음: 렌탈 회사가 모든 유지보수와 수리를 처리합니다. 차량이 고장나면 다른 차량을 제공해줍니다.\n\n3. 편리함: 필요할 때에만 차량을 렌탈할 수 있어 장기간의 약정이나 유지보수 걱정 없이 이용할 수 있습니다.\n\n4. 반복 비용: 렌탈 비용을 지불하면 차량 이용과 유지보수 서비스가 모두 포함됩니다.\n\n## 관리형 쿠버네티스는 비슷한 방식으로 작동합니다:\n\n<div class=\"content-ad\"></div>\n\n- 서비스 제공업체: Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), 또는 Azure Kubernetes Service (AKS)와 같은 클라우드 제공업체가 인프라를 관리합니다.\n\n- 사용 편의성: 제공업체가 설정, 유지 관리 및 업데이트를 처리하므로 시작하고 운영하기 쉬워집니다.\n\n- 유지보수 없음: 클라우드 제공업체가 모든 업데이트, 보안 패치 및 모니터링을 처리하여 팀에 부담을 줄여줍니다.\n\n- 재발생 비용: 사용량에 따라 서비스를 지불하므로 인프라를 소유하는 것보다 예측 가능하고 확장 가능한 경우가 많습니다.\n\n<div class=\"content-ad\"></div>\n\n## 어떤 것이 당신에게 가장 적합할까요?\n\n온프레미스와 관리형 쿠버네티스 중 어떤 것을 선택할지는 당신의 특정 필요와 상황에 따라 다릅니다. 몇 가지 시나리오를 살펴보겠습니다:\n\n온프레미스 쿠버네티스가 가장 적합한 경우:\n1. 완벽한 통제가 필요한 경우: 규제 요건, 데이터 소유권 문제 또는 특정 맞춤화 요구사항으로 인해 인프라에 대한 완벽한 통제가 필요한 경우.\n\n<div class=\"content-ad\"></div>\n\n2. 기존 인프라: 온프레미스 하드웨어에 상당한 투자를 이미 했으며 효율적으로 활용하고 싶습니다.\n\n3. 비용 관리: 특히 클라우드 서비스와 관련된 반복 비용 대신 하드웨어에 선순위 투자를 선호합니다.\n\n관리형 Kubernetes는 다음 상황에 가장 적합합니다:\n\n- 확장성: 수요에 따라 운영을 유연하게 확장 또는 축소할 수 있는 유연성이 필요합니다. 특히 초기에는 빠르게 성장하거나 변동하는 워크로드를 경험하는 스타트업 및 기업에 유리합니다.\n- 전문 지식: Kubernetes 환경을 효과적으로 관리하는 데 필요한 내부 전문 지식이 부족합니다. 관리형 서비스는 Kubernetes 운영의 복잡성을 다루는 전문가 팀에 접근할 수 있도록 합니다.\n- 속도: 온프레미스 인프라를 설정하고 유지하는 데 연관된 지연 없이 가능한 빨리 응용 프로그램을 가동하고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n## 결정 요소들\n\n온프레미스와 관리형 쿠버네티스 사이를 결정할 때 고려해야 할 몇 가지 추가 요소가 있습니다:\n\n- 규정 준수와 보안: 귀하의 산업에 따라 규정 요구사항이 데이터를 어디에서 어떻게 관리해야 하는지를 결정할 수 있습니다. 클라우드 제공업체는 높은 수준의 보안을 제공하고 종종 다양한 규정을 준수하지만 특정 데이터는 특정 상황에서 내부에 보관해야 할 수도 있습니다.\n- 장기 비용: 관리형 쿠버네티스는 장기적으로 더 비싸지만, 그 대가는 책임을 줄이고 잠재적으로 낮은 운영 리스크가 따릅니다. 그에 반해, 온프레미스 쿠버네티스는 처음에는 비용이 더 들지만 지속적인 운영 비용이 낮을 수 있습니다.\n- 혁신과 업그레이드: 클라우드 제공업체들은 지속적으로 서비스를 업데이트하여 최신 기능과 보안 향상을 제공합니다. 온프레미스 쿠버네티스를 사용할 경우 귀하의 팀은 이러한 업데이트를 수동으로 관리해야 하며, 이는 새로운 기능에 접근하는 데 시간이 걸릴 수 있습니다.\n\n여기까지입니다...\n차를 구매할지 렌트할지 선택하는 것은 귀하의 재정적 및 기술적 요구사항 및 상황에 달려 있습니다.\n\n<div class=\"content-ad\"></div>\n\n위 문서를 읽어 주셔서 감사합니다 🙏 \n더 많은 유용한 콘텐츠를 보시려면 제 블로그를 팔로우해 주세요. 놓치지 않으려면 구독도 잊지 말아 주세요.","ogImage":{"url":"/assets/img/2024-06-19-On-PremisesKubernetesVsManagedKubernetes_0.png"},"coverImage":"/assets/img/2024-06-19-On-PremisesKubernetesVsManagedKubernetes_0.png","tag":["Tech"],"readingTime":4},{"title":"어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요","description":"","date":"2024-06-19 13:03","slug":"2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR","content":"\n\n전 eBPF에 상당히 오랫동안 관여해 왔습니다. 우리 팀장이 Azure CNI에서 Cilium CNI로 모든 클러스터를 라이브 이전하는 것을 제안했을 때, 기회에 바로 뛰어들었어요. 이 일은 내가 지금까지 맡았던 가장 힘든 일 중 하나였지만, 그 시간 동안 즐거웠어요.\n\n하지만, 그 이야기는 다음에 하기로 해요. 이 기사의 목표는 제 개인적인 k8s.af 이야기 중 하나를 해설하여, 머리카락을 몇 일 절약할 수 있는 누군가를 돕는 것입니다.\n\n![이미지](/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png)\n\n## 왜 Cilium을 선택했는가?\n\n<div class=\"content-ad\"></div>\n\nCilium을 선택한 우리의 주요 목표는 다음과 같습니다:\n\n- 더 나은 네트워크 격리: 일부 클러스터에서는 고객의 작업 부하를 실행하고 있기 때문에 출발 트래픽을 효과적으로 공유하고 제어해야 했습니다.\n- WireGuard를 사용한 투명한 암호화: 공유 클러스터에서는 제로 트러스트 접근 방식을 채택하고자 했습니다.\n- 관찰 가능성: Cilium은 다양한 관찰 기능을 갖추고 있어 추가 계측없이 Kubernetes 작업 부하를 모니터링할 수 있습니다.\n- 서비스 메시 기능: Cilium은 사이드카를 필요로하지 않고 다시 시도 및 회로 차단과 같은 서비스 메쉬 기능을 제공할 수 있습니다.\n- 효율적이고 가벼운 네트워크 스택: 하드웨어 비용을 낮추면서 더 나은 성능을 원하신다면 저희를 선택해 주세요!\n- 클러스터 매시 망: 우리는 인프라를 미래에 대비하기 위해 준비하고 싶었습니다.\n\n이주 후 모든 것이 원활했습니다 (대부분...). Cilium을 기반으로 개발한 기능을 출시하기 시작했고 고객들로부터 좋은 피드백을 받았습니다.\n\n그리고 모든게 좋았는데...\n\n<div class=\"content-ad\"></div>\n\n# 사건\n\n일반적인 월요일 아침이었고, 매일 릴리스 주기의 일환으로 SRE 팀은 코어 서비스의 최신 업데이트를 스테이징 환경으로 프로모션했습니다.\n\n그러나 프로모션 파이프라인이 완료되자마자 우리의 업타임 모니터링 솔루션이 스테이징 환경에 접근할 수 없다는 이벤트를 트리거하면서 발생했습니다. SRE 팀은 즉시 문제의 근본 원인을 조사하기 시작했습니다.\n\n더 깊이 파고들기 전에, 빠른 다이어그램을 사용하여 우리의 네트워크 아키텍처(간소화된 버전)를 설명해 드리겠습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_1.png\" />\n\n두 개의 클러스터가 있습니다. 하나는 외부에 공개되어 있고, 다른 하나는 비공개입니다. 외부 클러스터는 방화벽을 통해 노출되어 있습니다. 비공개 클러스터의 일부 서비스는 로드 밸런서를 통해 외부 클러스터와 통신합니다.\n\n초기 디버깅 후, SRE 팀은 문제가 방화벽과 클러스터 1의 인그레스 서비스 간의 연결에 있는 것으로 결론 내렸습니다. 클러스터 1 내의 모든 서비스가 실행되고 클러스터 1의 로드 밸런서를 향해 요청을 보내고 있기 때문에 클러스터 2의 pod들이 작동 중이었습니다.\n\nWireshark와 Hubble을 통해, 방화벽에서 전송된 \"SYN\" 패킷이 서비스에 도달했지만 서비스로부터 해당하는 \"ACK\" 패킷이 전송되지 않았음을 확인했습니다.\n\n<div class=\"content-ad\"></div>\n\n문제를 해결하기 위한 몇 차례의 무산된 시도 끝에 SRE 팀은 프로덕션으로의 릴리스를 승인했습니다. 중요한 수정 사항을 가능한 빨리 릴리즈해야 했기 때문입니다. 이 결정의 근거는 애플리케이션 수준의 변경이 인프라를 손상시킬 수 없으며, 이 일은 격리된 사건이었습니다.\n\n그러나 릴리스가 프로덕션으로 승급되자마자 프로덕션 로드 밸런서도 응답을 중단했습니다. SRE 팀은 신속하게 변경 사항을 롤백하여 프로덕션에서 문제를 해결했습니다. 놀랍게도 스테이징 클러스터에서 변경 사항을 롤백해도 문제가 해결되지 않았습니다.\n\n## 재앙이 계속됩니다\n\n스테이징에서의 문제가 여전히 해결되지 않아 전문 네트워크 전문가와 함께 전투실에 호출되었습니다.\n\n<div class=\"content-ad\"></div>\n\n지금쯤 SRE 팀은 광범위한 실험을 통해 많은 데이터를 수집했습니다. 우리 VPC 내의 다른 서브넷에 VM을 배포하는 실험을 해보았는데, 로컬 노드 풀 서브넷부터 다른 클러스터에 속한 원격 서브넷, 로드 밸런서 서브넷까지 다양한 곳에 시도해봤습니다. 모든 것이 예상대로 작동되었는데, 방화벽을 통해 통과하는 트래픽에서 문제가 발생했습니다.\n\n제가 노력에 합류하면서, 그들이 수집한 모든 데이터를 철저히 검토했고, 여러 종류의 워크로드를 배포하고 Hubble과 Wireshark 로그를 분석하며 더 많은 테스트를 진행했습니다. 루트 원인을 밝힐 수 있는 단서나 누락된 부분을 찾기 위해 노력했습니다.\n\nAzure 네트워크 엔지니어가 합류하면서, SRE 팀은 그들이 지금까지 한 모든 단계에 대해 설명했습니다. 수집한 데이터를 분석한 후, 엔지니어는 방화벽 서브넷 내에 VM을 배포하고 문제가 있는 Kubernetes 클러스터로 TCP 연결을 시도하라는 제안을 했습니다.\n\n<div class=\"content-ad\"></div>\n\n저희 SRE 팀은 방화벽 서브넷 내에 임시 VM을 빠르게 설정하고 로드 밸런서 IP로 telnet을 시도해 보았어요. 이 테스트 중에 우리가 직면한 동일한 문제를 관찰했는데, telnet 연결이 초기화되지 않았어요. 그래서 그들은 방화벽 서브넷과 로드 밸런서 서브넷 또는 노드 풀 서브넷 간 네트워크 피어링에 문제가 있는지 조사하기로 결정했어요.\n\n호기심에 저는 다른 SRE 멤버에게 서버에 다시 ping을 시도하도록 요청하고 Hubble 로그를 모니터링했어요. 놀랍게도, telnet이 시작된 순간에 SYN 패킷이 수신되었지만 \"ACK\"은 전달되지 않았어요.\n\n이 행동은 이상하게 보였기 때문에 임시 VM에서 포트를 열도록 요청하고 쿠버네티스 클러스터에서 해당 VM으로 ping을 시도해 보았어요. 하지만 응답이 없었고, 임시 VM에 Wireshark를 설치한 후에도 들어오는 SYN 패킷을 볼 수 없었어요. 흥미로운 점은 방화벽 서브넷을 제외한 다른 목적지로 ping을 시도하면 문제가 없었다는 점이었어요.\n\n의아해하며, 그들에게 노드 풀 서브넷에 생성한 임시 VM을 사용하여 동일한 테스트를 수행해 보라고 요청했더니, 드디어 작동했어요.\n\n<div class=\"content-ad\"></div>\n\n# 루트 원인\n\n이러한 동작이 이상하다고 생각해, 주요 사고 대응 팀 구성원들의 작업을 방해하고 있는 피어링을 점검 중이었던 멤버들에게 얘기했어요. 문제가 인그레스가 아니라 이그레스에 문제가 있을 수 있다는 것을 알려줬죠. 이로써 AKS-관리 노드 내의 라우팅 테이블 문제일 수도 있다는 느낌을 받았어요.\n\n그래서 저희 SRE 팀은 쿠버네티스 클러스터에서 노드 중 하나로 SSH를 통해 연결하고 `ip route` 명령을 실행했어요. 이 명령을 실행하면 Cilium이 교차 노드 통신을 가능하게하기 위해 추가한 몇 가지 라우팅 규칙이 표시됐어요.\n\n![Image](/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_3.png)\n\n<div class=\"content-ad\"></div>\n\n## Cilium 노드 라우팅\n\n고수준 개요에서 Cilium을 클러스터 범위 IPAM 모드에서 실행할 때, Cilium에게 가상 IP를 파드에 할당하도록 지시하기 위해 CIDR 범위를 제공해야 합니다. 기본적으로 이 CIDR 범위는 10.0.0.0/8입니다.\n\n새 노드가 클러스터에 가입하면, Cilium은 해당 노드에게 주어진 CIDR 블록에서 고유한 서브넷을 할당합니다. 노드의 모든 파드는 이 할당된 서브넷 범위에서 IP 주소를 받습니다.\n\n예를 들어, CIDR 범위인 10.0.0.0/8을 사용한다면:\n\n<div class=\"content-ad\"></div>\n\n- 노드 A는 서브넷 10.1.0.0/16을 받을 수 있습니다.\n- 노드 B는 서브넷 10.4.0.0/16을 받을 수 있습니다.\n\n노드 간 통신을 원활하게 하기 위해 Cilium은 IP 경로를 설정하여 트래픽이 노드 간에 올바르게 전달되도록 합니다. 노드 A에 있는 IP가 10.1.5.13인 팟이 노드 B에 있는 IP가 10.4.63.38인 팟과 통신하려고 할 때, 데이터 패킷은 노드 A의 네트워크 인터페이스로 전송됩니다. 그 후 IP 라우팅 테이블을 기반으로 패킷은 노드 B로 라우팅되며, 이는 노드 B가 10.4.0.0/16 서브넷을 소유하기 때문입니다.\n\n## 잠자는 용\n\n보통은 정상적으로 작동합니다. 그러나 유감스럽게도, 노드에 할당된 서브넷 중 하나가 방화벽의 서브넷 범위와 겹치는 문제가 발생했습니다. 이로 인해 SYN 패킷이 팟에 성공적으로 도달하지만, 팟이 응답을 시도할 때 요청이 노드의 네트워크 인터페이스로 전달되는 상황이 발생했습니다.\n\n<div class=\"content-ad\"></div>\n\n그 때, 노드의 IP 라우팅 규칙 때문에 패킷이 다른 노드로 라우팅되었습니다. 이는 방화벽의 VM IP 주소가 두 번째 노드의 서브넷 범위 내에 속했기 때문에 발생했습니다. 그러나 두 번째 노드에는 방화벽의 VM과 정확히 일치하는 IP 주소를 가진 pod가 없었기 때문에 패킷이 소멸하여 사라졌습니다.\n\n이 가설을 확인하기 위해 SRE 팀은 충돌하는 노드에 대해 `kubectl delete node`을 실행했고, 그 노드가 제거되자마자 방화벽을 통한 외부 연결이 다시 작동하기 시작했습니다.\n\n하지만 왜 Cilium을 배포한 후 거의 8개월이 지난 후에 이 문제가 발생했을까요? 이 모든 것은 자동 스케일링으로 귀결되었습니다. 관찰한 바에 따르면, 특정 CIDR 범위가 노드에 할당되면 해당 노드가 제거되더라도 재사용되지 않았습니다. 따라서 Cilium 운영자는 우리가 할당한 대규모 CIDR 범위를 하나씩 소비하면서 점진적으로 이동하고 있었고, 마침내 방화벽의 CIDR 범위에 다다랐습니다.\n\n그 판명날인 첫 월요일 아침, SRE 팀이 개발 환경에서 스테이징으로 릴리스를 승격시키자마자 노드 스케일업을 트리거하여 충돌하는 CIDR 범위가 있는 노드가 생성되었고, 이로 인해 전체 통신 경로가 다운되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 문제 수정\n\n스테이징에서 발생한 이슈를 해결하기 위해, 우리는 clusterPoolIPv4PodCIDRList를 기존 내부 서브넷과 충돌하지 않는 CIDR 범위로 업데이트해 보았습니다. 헬름 업그레이드를 실행한 후에도 아무 변화가 없었습니다. 그래서 노드 스케일 업을 트리거하고 다행히도 - 새로 생성된 노드가 새로운 CIDR 범위의 서브넷으로 생성되었습니다.\n\n저는 두 개의 DaemonSet을 사용하여 교차 노드 통신을 테스트하기 위해 만든 빠른 워크로드를 실행하여 두 개의 CIDR 범위가 아무 문제없이 작동하는 것을 확인했습니다. 그 후, SRE 팀은 기존 노드를 안전하게 비우고 제거하고 나쁜 CIDR 범위를 가진 모든 노드가 완전히 제거될 때까지 새로운 노드 풀을 확장하는 스크립트를 신속하게 작성했습니다. 그 스크립트를 실행하여 스테이징 클러스터를 완전히 복구했습니다.\n\n추가 테스트를 실행한 후에 우리의 프로덕션 클러스터도 동일한 문제를 겪었기 때문에, Cilium 문서에서 반대하고 있던 것에도 불구하고 clusterPoolIPv4PodCIDRList를 업데이트하여 문제를 영구적으로 해결하기로 결정했습니다. 이에 이해 관계자들로부터 동의를 받고 마이그레이션을 실행했습니다.\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n다양한 테스트를 거친 후에도, 거의 2000개의 구성 가능한 값이 있는 Cilium과 같은 복잡한 시스템은 여전히 잘못된 구성을 빠뜨릴 수 있어 예상치 못한 실패로 이어질 수 있습니다.\n\n이 사건을 통해, 네트워크 문제를 체계적으로 해결하고 클라우드 추상화에 의해 제공된 낮은 수준의 네트워킹 인프라와 기술을 이해하는 중요성을 깨달았습니다. 이 문제에 대해 협업한 후, 매우 어려운 경험이었지만 소중한 학습 기회를 제공했다는 것에 대해 모두 동의했습니다.","ogImage":{"url":"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png"},"coverImage":"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png","tag":["Tech"],"readingTime":7},{"title":"나의 쿠버네티스 클러스터에 MongoDB No-SQL 데이터베이스를 추가한 경험","description":"","date":"2024-06-19 13:00","slug":"2024-06-19-MyexperienceaddingaMongoDBNo-SQLdatabasetomyKubernetescluster","content":"\n\n## SQL과 No-SQL 데이터베이스 사이를 선택하는 방법에 대해 읽었다면, Kubernetes 클러스터에 No-SQL MongoDB 데이터베이스를 추가할 수 있는지 궁금할 것입니다. 이 글에서는 그것을 어떻게 수행했는지 설명하고 Spring Boot 애플리케이션과 함께 사용하는 방법에 대해 알려드리겠습니다.\n\n![이미지](/assets/img/2024-06-19-MyexperienceaddingaMongoDBNo-SQLdatabasetomyKubernetescluster_0.png)\n\n# 시작하기\n\n일반적으로 Kubernetes 서비스를 개발할 때는, 개발을 위해 로컬 Kind Kubernetes 클러스터에서 시작합니다. Kind를 설정하는 방법에 대해 이전에 썼었고, 이 글에 관련된 GitHub 저장소에는 이를 수행하는 데 필요한 구성 파일이 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n클론하기 위해 저장소를 다음과 같이 복제할 수 있어요:\n\n```js\ngit clone git@github.com:MartinHodges/aquarium-with-mongo-db.git\n```\n\n# 왜 MongoDB를 사용해야 하나요?\n\n이전 기사에서 SQL 대 No-SQL 결정에 대해 다뤄 보았어요. 여러분이 이 글을 읽고 계신다면 No-SQL을 선택하겠다고 결정하신 거겠죠.\n\n<div class=\"content-ad\"></div>\n\n일단 그 결정이 내렸다면, 이제 No-SQL 데이터베이스를 어떤 것을 선택할지가 문제가 됩니다. MongoDB는 가장 가까운 경쟁상대보다 2배 더 높은 시장 점유율을 보유하고 있습니다. 그것은 매우 정교하며 커뮤니티 버전과 엔터프라이즈 버전 둘 다 가지고 있습니다. 전형적으로 가장 많이 사용되는 No-SQL 데이터베이스입니다.\n\n다른 데이터베이스와의 기술적인 비교는 이 기사의 범위를 벗어나지만, MongoDB가 인기 있는 이유와 일하도록 충분히 할 수 있는 사실에 기반하여 이 기사에서는 MongoDB를 선택했습니다!\n\n# MongoDB 설치\n\nKubernetes 클러스터에 MongoDB를 설치하는 방법은 다른 응용프로그램과 유사하게 operator를 사용하여 수행됩니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-MyexperienceaddingaMongoDBNo-SQLdatabasetomyKubernetescluster_1.png\" />\n\n쿠버네티스 오퍼레이터는 당신을 대신하여 응용 프로그램을 관리합니다. 응용 프로그램의 라이프사이클을 설치하고 관리하며 모니터링하고 필요한 조치를 취할 수 있습니다.\n\n데이터베이스의 경우 데이터베이스 클러스터를 생성하거나 확장하거나 백업하는 등의 작업을 수행할 수 있습니다. 일반적으로 오퍼레이터는 그 자체의 '쿠버네티스 구성 언어'를 제공하는 사용자 정의 리소스 정의 (CRD)를 설치하기에 의존합니다. 이는 클러스터에 사용자 정의 리소스를 추가하기 위한 요청을 감지하고 당신을 대신하여 작동합니다.\n\n## 개발용 쿠버네티스 클러스터 생성\n\n<div class=\"content-ad\"></div>\n\nKind를 설치했다고 가정하면, 다음 구성을 사용하여 Kind 클러스터를 만들 수 있습니다:\n\nkind/kind-config.yml\n\n```js\napiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n  extraPortMappings:\n  # apis\n  - containerPort: 30080\n    hostPort: 30080\n- role: worker\n- role: worker\n- role: worker\n```\n\n이렇게 하면 1개의 컨트롤러 및 3개의 워커로 구성된 4개 노드 클러스터가 생성됩니다. 또한 개발 머신의 포트 30080을 사용할 수 있습니다. 이를 사용하여 로컬 Kubernetes 클러스터를 생성할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```shell\nkind create cluster --config kind/kind-config.yml\n```\n\n## 오퍼레이터 설치\n\nHelm을 사용하여 커뮤니티 지원 오퍼레이터를 설치할 수 있습니다.\n\n먼저 다음과 같이 로컬 리포지토리에 Helm 링크를 추가하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nhelm repo add mongodb https://mongodb.github.io/helm-charts\n```\n\n아래 명령어로 이 리포지토리가 추가한 차트를 확인할 수 있어요:\n\n```js\nhelm search repo mongo\n```\n\n리스트에서 커뮤니티 오퍼레이터를 확인할 수 있을 거에요. 이것을 사용할 거에요.\n\n<div class=\"content-ad\"></div>\n\n저희는 오퍼레이터와 데이터베이스를 별도의 네임스페이스로 mongo라는 이름으로 분리해서 배치할 겁니다. 다음과 같이 생성해보겠습니다:\n\n```js\nkubectl create namespace mongo\n```\n\n이제 다음 명령으로 오퍼레이터를 설치할 수 있어요:\n\n```js\nhelm install community-operator mongodb/community-operator -n mongo\n```\n\n<div class=\"content-ad\"></div>\n\n다음 명령어를 사용하여 준비 상태가 1/1로 Running인지 확인할 수 있어요:\n\n```sh\nkubectl get pods -n mongo\n```\n\n이제 운영자가 작동 중인 것을 볼 수 있습니다. 설치된 CRD는 다음을 통해 확인할 수 있어요:\n\n```sh\nkubectl get crds\nkubectl describe crd mongodbcommunity.mongodbcommunity.mongodb.com \n```\n\n<div class=\"content-ad\"></div>\n\n이제 MongoDB 클러스터를 생성할 준비가 되었습니다.\n\n## 클러스터 생성\n\n오퍼레이터가 설치되었으므로 MongoDB 데이터베이스를 생성하는 요청을 대기 중입니다. 우리는 오퍼레이터에 의해 로드된 CRD를 사용하여 쿠버네티스 클러스터에 MongoDB 매니페스트를 적용하여 요청을 할 수 있습니다.\n\n이를 하기 전에 데이터베이스 사용자의 비밀번호를 쿠버네티스 시크릿으로 설정해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 비밀을 생성하세요 (‘…’를 선택한 비밀번호로 교체하세요):\n\n```js\nkubectl create secret generic my-user-password -n mongo --from-literal=\"password=<당신의 비밀번호>\"\n```\n\n다음 명령어로 확인할 수 있어요:\n\n```js\nkubectl get secrets -n mongo my-user-password -o jsonpath={.data.password} | base64 -d; echo\n```\n\n<div class=\"content-ad\"></div>\n\n모든 쿠버네티스 시크릿은 base64로 인코드되어 있기 때문에 비밀번호를 디코딩하는 데 base64 -d를 사용하는 것을 알 수 있습니다. 우리가 --from-literal을 사용하였기 때문에 create secret 명령어에 의해 비밀번호가 자동으로 base64로 인코드되었습니다.\n\n이제 비밀번호가 준비되었으니, 이 비밀번호를 사용하는 관리자 사용자가 있는 MonogoDB 클러스터와 데이터베이스를 생성할 수 있습니다.\n\n매니페스트 파일을 생성해 보세요:\n\nk8s/my-mongo-db.yml\n\n<div class=\"content-ad\"></div>\n\n```yaml\napiVersion: mongodbcommunity.mongodb.com/v1\nkind: MongoDBCommunity\nmetadata:\n  name: my-mongo-db\n  namespace: mongo\nspec:\n  members: 3\n  type: ReplicaSet\n  version: \"7.0.11\"\n  security:\n    authentication:\n      modes: [\"SCRAM\"]\n  users:\n    - name: my-user\n      db: admin\n      passwordSecretRef: # a reference to the secret that will be used to generate the user's password\n        name: my-user-password\n        key: password\n      roles:\n        - name: clusterAdmin\n          db: admin\n        - name: userAdminAnyDatabase\n          db: admin\n      scramCredentialsSecretName: my-user-scram\n  additionalMongodConfig:\n    storage.wiredTiger.engineConfig.journalCompressor: zlib\n```\n\n이제 다음과 같이 적용할 수 있습니다:\n\n```bash\nkubectl apply -f k8s/my-mongo-db.yml \n```\n\n그리고 진행 상황을 다음과 같이 확인할 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n```js\nkubectl get pods -n mongo\n```\n\n3개의 인스턴스가 생성될 때까지 기다리고 있어요. 제 MacBook Pro(M2 Max Apple 실리콘)에서 4노드 Kind 클러스터를 사용하면, 모든 3개의 인스턴스를 시작하는 데 약 5분 정도 걸렸어요.\n\n시작되고 나면, 다음 명령어로 서비스가 정상적으로 작동하는지 확인할 수 있어요:\n\n```js\nkubectl get svc -n mongo\n```\n\n<div class=\"content-ad\"></div>\n\n이렇게 하면:\n\n```js\nNAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE\nmy-mongo-db-svc   ClusterIP   None         <none>        27017/TCP   6m\n```\n\n## 데이터베이스 테스트\n\n우리 애플리케이션에서는 쿠버네티스 내부에서 직접 데이터베이스에 연결할 것입니다. 데이터베이스의 서비스를 이용해 DNS 이름으로 연결하려고 하지만, 테스트 목적으로는 로컬 개발 머신에서 연결하고 싶습니다.\n\n<div class=\"content-ad\"></div>\n\n처음으로 이를 시도할 때 로컬 개발 머신으로 MonogoDB 파드 중 하나를 포워딩하기 위해 포트 포워딩을 사용했고, 어떤 변경을 시도했을 때 다음과 같은 오류 메시지를 받았습니다:\n\n```js\nMongoServerError[NotWriteablePrimary]: not primary\n```\n\n이것은 포트 포워딩한 파드가 클러스터의 주 파드가 아니기 때문에 발생한 문제입니다. 보조 파드는 읽기 전용 복사본이기 때문에 모든 쓰기 작업은 주 파드를 통해 이루어져야 합니다.\n\n이 문제를 피하려면 주 파드에 연결해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n어떤 노드가 기본 노드인지 알고 싶다면 다음 노드 중 하나의 로그를 조사하면 됩니다:\n\n```js\nkubectl logs my-mongo-db-0 -n mongo -c mongod | grep \"\\\"primary\\\":\"\n```\n\n만약 결과가 없다면, 기본 노드에 도달한 것입니다.\n\n만약 결과를 얻는다면, 몇 줄만 출력될 수 있지만, 그것들은 매우 길고 읽기 어려울 수 있습니다. JSON pretty printer 같은 것(jq와 같은)을 가지고 있다면 다음을 사용할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nkubectl logs my-mongo-db-0 -n mongo -c mongod | grep \"\\\"primary\\\":\" | jq\n```\n\n그러면 다음과 같은 줄을 볼 수 있습니다:\n\n```js\n...\n\"primary\": \"my-mongo-db-1.my-mongo-db-svc.mongo.svc.cluster.local:27017\",\n...\n```\n\n여기에 연결해야 하는 pod의 이름이 나옵니다 (제 경우: my-mongo-db-1). 이제 해당 pod를 포트 포워드할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nkubectl port-forward my-mongo-db-1 -n mongo 27017:27017\n```\n\n이 포트 포워딩이 설정되면 데이터베이스에 연결해야 합니다. MongoDB Compass 클라이언트를 사용할 수 있습니다. 해당 클라이언트는 https://www.mongodb.com/try/download/compass 에서 다운로드할 수 있습니다.\n\n설치 후 데이터베이스에 연결할 수 있어야 합니다. 연결 문자열(mongodb://localhost:27017)이 제안됩니다만, 몇 가지 설정을 변경해야합니다.\n\n고급 연결 옵션을 클릭하고 직접 연결을 클릭하십시오 (이 설정을 변경하지 않으면 내부 쿠버네티스 주소를 사용하려고 시도하여 찾을 수 없는 주소가 발생합니다).\n\n<div class=\"content-ad\"></div>\n\n인증 탭을 클릭해주세요. 사용자 이름/비밀번호를 선택하고 이전에 선택한 사용자 이름(my-user)과 비밀번호를 입력해주세요. Admin을 데이터베이스로 추가하고 SCRAM-SHA-256 인증 메커니즘을 선택해주세요 (필요하다면 아래로 스크롤).\n\n저장 및 연결을 클릭하고 연결 이름을 지정한 후, 데이터베이스에 연결된 Compass 콘솔이 표시됩니다.\n\n클러스터 내에서 admin, config 및 local 데이터베이스가 생성된 것을 확인하실 수 있습니다.\n\n여기까지 오셨다면, MongoDB 클러스터가 정상적으로 실행 중임을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n# 애플리케이션 사용자 생성\n\n우리의 MongoDB에 연결할 모든 애플리케이션이 우리가 생성한 my-user를 사용할 수 있을 것이라고 생각할 수 있습니다. 하지만, 이 사용자는 실제로 데이터베이스 유지 관리를 위한 것이기 때문에 그렇지 않습니다.\n\n애플리케이션이 데이터베이스 클러스터를 사용할 수 있도록하려면 데이터베이스와 해당 데이터에 액세스할 사용자를 생성해야 합니다.\n\nCompass 창의 맨 아래에 `_MONGOSH` 프롬프트가 나타납니다. 이를 클릭하여 명령줄에 액세스할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 다음과 같이 사용자를 생성할 것입니다:\n\n```js\nuse aquarium\ndb.createUser( { user: \"my-app-user\",\n              pwd: \"<password>\",\n              roles: [ {db: \"aquarium\", role: \"dbOwner\"} ] } )\n```\n\n알아둬야 할 몇 가지 사항이 있습니다. 첫번째로, 생성되기 전에 존재하지 않는 데이터베이스(aquarium)로 전환합니다. 이는 사용하기 전에 아무 것도 정의할 필요가 없다는 원칙에 부합합니다. 데이터베이스 및 모든 컬렉션은 문서를 추가할 때 처음 생성됩니다.\n\n두번째는 새 데이터베이스에 할당된 역할입니다. MongoDB에는 사용자에게 부여할 수 있는 소수의 기본 역할이 있습니다. 이 경우 dbOwner 역할은 사용자가 데이터베이스를 읽고 쓰고 관리할 수 있도록 합니다. 실제 운영에서는 사용자 권한을 적절히 제한해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n...\nok: 1,\n...\n```\n\n사용자를 확인하기 위해 새 Compass 연결을 열어보세요. 이는 메뉴를 통해 할 수 있습니다. 혹은 MacOS에서는 Cmd N을 누르세요. 창이 열릴 때까지 몇 초가 걸릴 수 있는데, 아무런 표시가 없으므로 한 번만 누르세요!\n\n새 연결 창이 나타나면, 이전에 저장한 연결을 복제하는 것이 더 쉽다고 생각합니다(연결 옆의 ... 메뉴를 사용하세요).\n\n<div class=\"content-ad\"></div>\n\n사용자 이름과 비밀번호를 변경해주세요. 또한 Authentication Database를 aquarium으로 변경해주세요. 그런 다음 연결하세요.\n\n이제 새로운 aquarium 데이터베이스를 확인할 수 있어야 합니다. \"fishes\"라는 collection을 생성해보면서 테스트해 볼 수 있습니다. 데이터베이스에 문서 형태로 데이터를 추가할 수 있습니다.\n\n```js\n{\n  \"_id\": 123,\n  \"fish\": \"Guppy\"\n}\n```\n\n이 시점에서 Spring Boot 애플리케이션과 함께 사용할 준비가 된 MongoDB가 준비되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 스프링 부트 애플리케이션 만들기\n\n간단한 데이터베이스 지원 예제를 만들 때는 제가 제일 먼저 수족관 애플리케이션을 사용합니다. REST API를 사용하여 물고기와 수족관을 만들고 관리할 수 있습니다. 그런 다음 물고기를 여러분의 수족관 중 하나에 추가할 수 있습니다.\n\n## 코드\n\n저는 코드를 여기에 포함하려는 의도는 없지만 관련된 GitHub 저장소에서 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 종속성\n\nSpring Boot 애플리케이션을 시작하는 것은 항상 https://start.spring.io/에서 Spring Initializr를 사용하는 것이 더 쉽습니다. 사용 방법을 알고 있다고 가정합니다.\n\n이 프로젝트에서 Spring Web과 Spring Data MongoDB를 종속성으로 추가하고 프로젝트를 생성합니다.\n\n## 패키지 구조\n\n<div class=\"content-ad\"></div>\n\n내가 만드는 애플리케이션에 따라, 패키지 구조를 구성하는 데 컴포넌트 유형(예: 컨트롤러, 서비스 및 리포지토리)에 기반을 둘 수도 있고, 비즈니스 도메인에 기반을 둘 수도 있습니다.\n\n물고기와 수조 두 가지 비즈니스 도메인만 있는 작은 애플리케이션인 경우, 이 프로젝트를 이러한 도메인을 기반으로 해서 다음과 같이 만들 것입니다:\n\n```js\nfishes\n  FishController\n  FishService\n  FishRepository\nfishtanks\n  FishTankController\n  FishTankService\n  FishTankRepository\n```\n\n보시다시피, 컨트롤러, 서비스 및 리포지토리 레이어를 사용하여 표준 계층 구조를 따르고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## API 엔드포인트\n\n이 컨트롤러들은 각각의 API에 대해 생성, 조회, 업데이트 및 삭제 (CRUD) 엔드포인트를 제공합니다.\n\n## 엔티티 및 문서\n\n만약 JPA와 Postgres와 같은 SQL 데이터베이스에 익숙하다면, 엔티티와 리포지토리로 익숙할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nNo-SQL 데이터베이스에서는 테이블이 컬렉션으로 대체되고, 테이블 내의 행은 문서로 대체됩니다.\n\n이는 No-SQL 데이터베이스를 위한 리포지토리가 SQL 데이터베이스와는 조금 다르다는 것을 의미합니다.\n\nNo-SQL 데이터베이스는 어떤 구조든 다룰 수 있기 때문에, 엔티티(또는 문서)는 간단한 Plain Old Java Objects (POJOs)가 됩니다. 이는 우리 예시 애플리케이션에서 다음과 같이 엔티티를 생성할 수 있다는 것을 의미합니다:\n\naquarium/fishes/Fish.java\n\n<div class=\"content-ad\"></div>\n\n```js\r\n...\n\n@Setter\n@Getter\n@Document(\"fishes\")\n@NoArgsConstructor\npublic class Fish {\n\n  @Id\n  public UUID id;\n\n  public String type;\n\n  public Fish(String type) {\n      this.id = UUID.randomUUID();\n      this.type = type;\n  }\n  ...\n}\n```\n\n친구야, 여기 몇 가지 주의할 점이 있어요:\n\n- @Entity를 정의하는 대신 컬렉션의 이름을 사용하는 @Document를 정의하고 있어요.\n- 자체 UUID Id를 관리할 수 있도록 @mongoId 대신에 (필수는 아니지만 MongoDB가 제공하지 않은 경우 MongoDB로 제공할 수 있기 때문에) @Id를 사용하고 있어요.\n- Lombok(예: @Getter)을 사용하여 보일러플레이트 코드 일부를 제거하는 것을 좋아해요.\n\n이제 비슷한 방식으로 물고기 수조를 만들 수 있어요:\n\n<div class=\"content-ad\"></div>\n\n수족관/fishtanks/FishTank.java\n\n```java\n@Setter\n@Getter\n@Document(\"fish tanks\")\n@NoArgsConstructor\npublic class FishTank {\n\n    @Id\n    public UUID id;\n\n    public String name;\n\n    public FishTank(String name) {\n        this.id = UUID.randomUUID();\n        this.name = name;\n    }\n\n    @Override\n    public String toString() {\n        return String.format(\n                \"FishTank[id=%s, type='%s']\",\n                id.toString(), name);\n    }\n}\n```\n\n## Repositories\n\n자, 이제 우리의 문서들이 준비되었어요. 이제 이들에 어떻게 접근할까요?\n\n<div class=\"content-ad\"></div>\n\n제가 보여드릴 것은 우리 저장소의 변경 사항입니다. 물고기 저장소를 예로 들어보겠습니다:\n\n```js\n...\npublic interface FishRepository extends MongoRepository<Fish, UUID> {\n\n    public List<Fish> findAll();\n\n    public Optional<Fish> findFirstById(UUID id);\n\n    public Optional<Fish> findFirstByType(String type);\n}\n...\n```\n\n이것이 SQL 데이터베이스에서 찾을 수 있는 Repository 유형과 거의 동일하다는 것을 알 수 있습니다. 유일한 차이점은 인터페이스가 CrudRepository가 아닌 MongoRespository를 확장한다는 것뿐입니다.\n\n한 대 다 및 다른 매핑 주제는 다른 기사로 미루겠습니다. 그래서 현재로서는 물고기와 어항을 생성하고 관리할 수 있을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n## 어플리케이션 속성\n\n데이터베이스와 작업할 때는 어플리케이션이 어떻게 연결해야 하는지를 알려줘야 합니다. 우리는 SQL 데이터베이스와 마찬가지로 어플리케이션 속성을 통해 이를 수행합니다.\n\n나는 Spring Boot 속성 파일에 YAML 파일을 사용하는 것을 선호하며, 내 구성은 다음과 같이 보입니다 (나의 값으로 ` ` 필드를 교체해주시기 바랍니다):\n\nresources/application.yml\n\n<div class=\"content-ad\"></div>\n\n```yaml\nspring:\n  application:\n    name: aquarium-with-mongo-db\n\n  data:\n    mongodb:\n      host: localhost\n      port: 27017\n      database: aquarium\n      username: my-app-user\n      password: <password>\r\n```\n\n나중에 프로필에 대해 이야기할 때 다시 돌아올게요.\n\n## 컨트롤러 및 서비스\n\n이제 SQL 데이터베이스와 마찬가지로 컨트롤러와 서비스를 추가할 수 있습니다. GitHub 저장소에서 이용 가능하므로 별도로 제시하지 않겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 애플리케이션 테스트\n\n코드를 완성하거나(또는 제 저장소를 복제)하여 IDE 내에서 애플리케이션을 실행하십시오. 여전히 주 서버로 포트 포워딩 중인 경우, 애플리케이션이 시작되어야 합니다.\n\n그런 다음 다음 curl 명령을 사용하여 테스트할 수 있습니다:\n\n```js\ncurl localhost:8080/api/v1/fishes -H \"Content-Type: application/json\" -d '{\"type\": \"guppy2\"}' \ncurl localhost:8080/api/v1/fish-tanks -H \"Content-Type: application/json\" -d '{\"name\": \"big one\"}' \ncurl localhost:8080/api/v1/fishes\ncurl localhost:8080/api/v1/fish-tanks\n```\n\n<div class=\"content-ad\"></div>\n\n이제 컴퍼스 클라이언트로 이동하여 아쿠아리움 데이터베이스를 새로 고침하면 fishes 및 fish tanks 두 개의 컬렉션이 표시됩니다. 이러한 컬렉션 내에는 만든 fishes 및 fish tanks가 표시됩니다.\n\n# 최종 단계\n\n이 시점에서 저희는 쿠버네티스 클러스터에서 실행 중인 MongoDB에 연결된 Spring Boot 애플리케이션을 갖추었습니다. 이제 해야 할 마지막 단계, 즉 Spring Boot 애플리케이션을 쿠버네티스 클러스터에 로드하는 것이 남았습니다.\n\n이를 위해 다음을 수행해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n- 팻 JAR 파일을 생성합니다 (모든 종속성이 포함됨)\n- 해당 JAR에서 Docker 이미지를 생성합니다\n- 이미지를 Docker 저장소에 업로드합니다\n- 배포 매니페스트 파일을 생성합니다\n- 배포 매니페스트를 Kubernetes 클러스터에 적용합니다\n\n제가 Kind를 사용하고 있기 때문에, 3단계를 간단한 로드 단계로 대체할 수 있습니다. 이렇게 하면 Docker 저장소를 사용할 필요가 없습니다.\n\n## 프로필\n\nJAR 파일을 생성하기 전에 Spring Boot 프로필 두 개를 생성하는 것이 유용합니다. 이를 통해 애플리케이션을 연결된 모드 (지금까지 한 것처럼) 및 Kubernetes 클러스터 내에서 실행할 수 있습니다. Spring Boot 프로필 두 개를 생성하겠습니다:\n\n<div class=\"content-ad\"></div>\n\n- `connected` ... 클러스터 외부에서 실행 중일 때 사용되는 모드\n- `local-cluster` ... 클러스터 내부에서 실행 중일 때 사용되는 모드\n\n현재 실행 중인 모드는 첫 번째입니다. 이는 우리가 간단히 application.yml(또는 application.properties) 파일을 application-connected.yml로 복사할 수 있다는 것을 의미합니다. 그런 다음 JVM 명령줄에 다음 JVM 인수를 추가할 수 있습니다:\n\n```js\n-Dspring.profiles.active=connected\n```\n\n로컬 클러스터 파일에 대해서도 동일한 작업을 수행하지만 이번에는 변경이 필요합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\r\n...\n  data:\n    mongodb:\n      host: my-mongo-db-svc.mongo.svc.cluster.local\n      port: 27017\n...\r\n```\n\nDNS 이름을 사용하여 올바른 팟에 연결할 수 있습니다. 팟에서 DNS 검색 규칙이 설정되어 있어 my-mongo-db-svc.mongo.svc와 같은 이름 일부를 생략할 수 있습니다. 이를 통해 다른 클러스터로 배포하고도 응용 프로그램이 작동할 수 있습니다.\n\n## 이미지 생성\n\n이제 이미지를 만드는 방법을 살펴보겠습니다. GitHub에 있는 프로젝트가 Gradle 프로젝트이므로 루트 프로젝트 폴더에서 다음과 같이 JAR 파일을 생성할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ngradle build \n```\n\ngradle.build에 아래 내용이 추가되었음을 유의해주세요. 이는 manifest가 주 애플리케이션 파일을 가리키도록 합니다:\n\ngradle.build\n\n```js\njar {\n    manifest {\n        attributes \"Main-Class\": \"com.requillion_solutions.aquarium.AquariumWithMongoDbApplication\"\n    }\n}\n```\n\n<div class=\"content-ad\"></div>\n\n이렇게 하면 jar 파일이 생성됩니다: build/libs/aquarium-with-mongo-db-0.0.1-SNAPSHOT.jar.\n\n도커 이미지를 만들기 위해서는 도커 파일이 필요합니다. 아래 내용대로 만들어보세요:\n\nDockerfile\n\n```js\nFROM openjdk:17.0.2-slim-buster\nRUN addgroup --system spring && useradd --system spring -g spring\nUSER spring:spring\nARG JAR_FILE=build/libs/*.jar\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]\nEXPOSE 8080\n```\n\n<div class=\"content-ad\"></div>\n\n이는 Java 17 기반 이미지를 시작으로 합니다 (이것은 롬복과의 문제를 피하기 위해 필요합니다) 그리고 새 사용자 (spring)를 추가하여 루트로 실행하지 않도록 합니다. 그런 다음 JAR 파일이 이미지로 복사되고 응용 프로그램을 실행하는 엔트리포인트가 생성됩니다.\n\n다음 명령어로 도커 이미지를 생성하세요:\n\n```bash\ndocker build -t aquarium .\n```\n\n그리고 만약 Kind를 사용 중이라면, 다음 명령어로 직접 Kubernetes 클러스터에 로드하세요:\n\n<div class=\"content-ad\"></div>\n\n```js\nkind load docker-image aquarium\n```\n\n이 작업이 완료되면 클러스터에서 실행하기 위한 배포 매니페스트를 생성할 준비가 되었습니다.\n\n## 배포 매니페스트\n\n이제 쿠버네티스 클러스터에 도커 이미지를 로드했으므로 배포 매니페스트를 사용하여 배포할 수 있습니다. 다음 파일을 만들어주세요:\n\n<div class=\"content-ad\"></div>\n\nk8s/deployment.yml\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aquarium\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aquarium\n  template:\n    metadata:\n      labels:\n        app: aquarium\n    spec:\n      containers:\n      - name: aquarium\n        image: aquarium\n        imagePullPolicy: IfNotPresent\n        ports:\n          - containerPort: 8080\n        env:\n          - name: SPRING_PROFILES_ACTIVE\n            value: local-cluster\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: aquarium\n  namespace: default\nspec:\n  selector:\n    app: aquarium\n  type: NodePort\n  ports:\n    - port: 8080\n      targetPort: 8080\n      nodePort: 30080\n```\n\n알아두어야 할 사항이 몇 가지 있어요:\n\n- 어플리케이션이 default 네임스페이스에 배포되었어요 (네임스페이스가 지정되지 않으면 사용되는 곳이죠)\n- 레플리카는 1개뿐이에요\n- 이미지는 이전에 불러왔으므로, 이미지가 없을 때만 불러와요\n- 프로필은 local-cluster로 설정돼요\n- 서비스가 생성되어 어플리케이션의 포트 8080을 개발 머신의 포트 30080으로 매핑돼요\n\n<div class=\"content-ad\"></div>\n\n이제 다음과 같이 배포할 수 있습니다:\n\n```js\nkubectl apply -f k8s/deployment.yml\n```\n\n시작이 성공적으로 이루어졌는지 확인해보세요:\n\n```js\nkubectl get pods\n```\n\n<div class=\"content-ad\"></div>\n\n한 번이 배포되면 API는 이전에 사용한 것과 동일한 curl 명령으로 테스트할 수 있습니다. 단, 포트를 30080으로 변경해 주세요.\n\n```js\ncurl localhost:30080/api/v1/fishes -H \"Content-Type: application/json\" -d '{\"type\": \"guppy2\"}' \ncurl localhost:30080/api/v1/fish-tanks -H \"Content-Type: application/json\" -d '{\"name\": \"big one\"}' \ncurl localhost:30080/api/v1/fishes\ncurl localhost:30080/api/v1/fish-tanks\n```\n\nCompass UI에서 새 문서를 확인할 수도 있습니다 (포트 포워드가 여전히 유지되는지 확인해 주세요).\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n이 기사는 Kind Kubernetes 클러스터로 MongoDB를 설치하고 Spring Boot 애플리케이션과 통합하는 과정에 대해 다루었습니다.\n\n이 연습은 꽤 간단하지만 그냥 어떻게 하는지 보여주는 것뿐입니다. 실제로는 보안, 백업 및 장애 조치에 작업이 필요할 것입니다.\n\n다른 기사에서는 문서간의 관계를 어떻게 관리할 수 있는지도 보여드릴 예정입니다.\n\n이 연습을 통해 No-SQL 데이터베이스가 Kubernetes와 Spring Boot와 간단하게 사용될 수 있다는 것을 보여줬으면 좋겠습니다.\n\n<div class=\"content-ad\"></div>\n\n이 글을 즐겁게 읽으셨기를 바라며, 새로운 것을 배우며 기술을 향상시켰기를 바랍니다. 작은 거라도 새로운 지식을 얻었다면 좋겠네요.\n\n이 글이 유익하게 느껴진다면, 박수 한 번 부탁드립니다. 그렇게 하면 미래에 어떤 글을 써야 하는지 파악할 수 있고, 다음 글을 결정하는 데 도움이 됩니다. 개선 사항이나 제안 사항이 있다면 메모나 답글로 추가해 주세요.","ogImage":{"url":"/assets/img/2024-06-19-MyexperienceaddingaMongoDBNo-SQLdatabasetomyKubernetescluster_0.png"},"coverImage":"/assets/img/2024-06-19-MyexperienceaddingaMongoDBNo-SQLdatabasetomyKubernetescluster_0.png","tag":["Tech"],"readingTime":18},{"title":"Argo CD v212 릴리스 후보판","description":"","date":"2024-06-19 12:59","slug":"2024-06-19-ArgoCDv212ReleaseCandidate","content":"\n\n저희가 기쁜 마음으로 Argo CD v2.12 릴리스 후보판이 공개되었다는 소식을 전해드립니다! 이번 릴리스에는 30개 이상의 새로운 기능, 70여 개의 버그 수정, 그리고 60개의 문서 업데이트가 포함되어 있어요.\n\n곧바로 릴리스 후보판을 테스트하고 마주한 어떤 버그나 문제에 대한 피드백을 보내주시면 감사하겠습니다. 이는 여러분이 의견을 전할 수 있고 Argo CD가 더 나아지도록 도와줄 수 있는 큰 기회입니다.\n\n# Multi-source application advancements\n\n<div class=\"content-ad\"></div>\n\n여러 소스에서 Argo CD 애플리케이션을 생성하는 것은 오랜 시간 동안 가장 요청이 많았던 Argo CD 기능 중 하나였습니다. 이 기능을 통해 여러 위치(예: 공개 Helm 차트 및 로컬 값 파일)에서 정보를 그룹화하여 단일 Argo CD 애플리케이션을 형성할 수 있습니다. 여러 소스를 정의하는 초기 지원은 이미 Argo CD 버전 2.6에 추가되었고, CLI를 위한 지원은 2.11에 추가되었습니다. UI는 여전히 애플리케이션이 단일 소스를 가지고 있다고 가정하고 롤백과 같은 특정 CLI 기능은 여러 소스를 가진 애플리케이션에 대해 아직 지원되지 않았습니다.\n\nArgo CD 버전 2.12에서는 여러 소스 애플리케이션의 롤백이 이제 Argo CD UI 및 CLI에서 모두 가능합니다.\n\n롤백 기능 외에도, 애플리케이션 세부 정보 페이지에 새로운 \"소스\" 탭이 추가되어 사용자가 애플리케이션의 소스를 관리(보기 및 편집)할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-ArgoCDv212ReleaseCandidate_1.png\" />\n\n<div class=\"content-ad\"></div>\n\nKeith Chong (Red Hat)과 Jorge Turrado에게 이러한 기능들을 구현해 줘서 감사합니다.\n\n# 프로젝트별 저장소 자격증명 개선사항\n\n현재 Argo CD API에서는 동일한 URL을 공유하는 여러 저장소 자격증명을 허용하지 않습니다. 저장소 자격증명이 argocd 네임스페이스에 직접 추가된 경우, argocd-server는 오류를 반환하지 않지만 이 작업은 작동하지 않습니다. URL과 일치하는 첫 번째 시크릿이 반환되며 순서도 정의되어 있지 않기 때문입니다. \n\nArgo CD 버전 2.12부터는 여러 앱 프로젝트가 동일한 URL을 가진 별도의 저장소 자격증명을 가질 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n안녕하세요 Blake Pettersson님(Akuity)! \n\n# Kubernetes 이벤트에 레이블 추가하기\n\nArgo CD 버전 2.12에서 사용자들은 Argo CD에서 생성된 k8s 이벤트에 애플리케이션 레이블을 노출할 수 있게 될 것입니다. `resource.includeEventLabelKeys`에서 정의된 특정 레이블 키를 가진 애플리케이션에 대해 생성된 이벤트에 대응하는 레이블이 이벤트에 첨부될 것입니다. 이 연결은 이러한 레이블을 사용하는 애플리케이션을 기반으로 이벤트를 필터링하거나 처리하는 것을 더 간단하게 만들어 줍니다.\n\n이 기능을 구현해준 Siddhesh Ghadi(Red Hat)님에게 감사드립니다.\n\n<div class=\"content-ad\"></div>\n\n# 일관성있는 샤딩 알고리즘\n\nArgo CD는 다른 Argo CD 애플리케이션 컨트롤러에 대한 샤딩 적용 기능을 제공합니다. 이를 통해 특정 클러스터를 특정 컨트롤러에 할당하여 부하를 다양한 샤드로 분산시킬 수 있습니다.\n\n기존의 샤딩 알고리즘인 레거시 방법과 라운드 로빈 알고리즘을 포함한 기존 방식은 최적의 부하 분산 유지와 불필요한 클러스터-샤드 할당 변경을 최소화하는 데 제한 사항이 있었습니다.\n\nArgo CD 버전 2.12부터 새로운 샤딩 알고리즘인 \"일관적 해싱(consistent-hashing)\"이 소개되었으며, 이는 클러스터-샤드 할당 변경을 줄이고 리소스 이용률을 최적화합니다.\n\n<div class=\"content-ad\"></div>\n\n이번 특징을 구현한 Akram Ben Aissi (Red Hat)에게 감사드립니다.\n\n# 기타 주목할만한 변경 사항\n\n릴리스에 추가된 몇 가지 새로운 변경 사항은 다음과 같습니다.\n\n- 시멘틱 버전 태그 해결을 위한 git 클라이언트 업데이트 (Stone Payments의 Pablo Aguilar가 수행)\n- Application Set Git Generator가 이제 GPG 서명 확인을 지원합니다 (Red Hat의 Ishita Sequeira가 수행)\n- ls-remote 요청 실패 지표 추가 (Jack-R-lantern이 수행)\n- 새로운 주석 argocd.argoproj.io/sync-options: Force=true 추가 (CyberAgent, Inc.의 Kota Kimura가 수행)\n- gRPC 메시지 크기를 환경 변수로 설정하는 지원 추가 (Codefresh의 Pavel Kostohrys가 수행)\n- 삭제 팝업에서 종속 리소스 목록 표시 (Intuit의 Alexandre Gaudreault가 수행)\n- old tracking label applications.argoproj.io/app-name에 대한 지원 제거 (Akuity의 Soumya Ghosh Dastidar가 수행)\n- Argo CD CLI에 대한 fish 쉘 완성 지원 추가 (Sn0rt가 수행)\n- 로컬로 존재하는 체크아웃할 커밋이 있는 경우 git fetch 호출 건너뛰기 (Shady Rafehi가 수행)\n\n<div class=\"content-ad\"></div>\n\n# 새 릴리스는 어디에서 받을 수 있나요?\n\n더 자세한 내용과 설치 지침은 릴리스 노트와 업그레이드 지침을 확인해주세요. 릴리스 후보를 시도하고 피드백을 공유해주세요. Argo 커뮤니티의 모든 기여자와 사용자들께 기여, 피드백 및 릴리스 테스트에서 도와준 점에 크게 감사드립니다!","ogImage":{"url":"/assets/img/2024-06-19-ArgoCDv212ReleaseCandidate_0.png"},"coverImage":"/assets/img/2024-06-19-ArgoCDv212ReleaseCandidate_0.png","tag":["Tech"],"readingTime":3},{"title":"도커에서 자체 서명 인증서를 사용한 NGINX","description":"","date":"2024-06-19 12:57","slug":"2024-06-19-NGINXwithSelf-SignedCertificateonDocker","content":"\n\n<img src=\"/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_0.png\" />\n\n저희 코드를 작업하는 동안 HTTPS에서 작업이 잘되는지 또는 더 중요한 것은 HTTPS에서 작동하는 방식을 빠르게 확인해야할 때가 많습니다. 온라인에서는 CSR(Certificate Sign Request)를 생성하고 해당 CSR을 자체로 서명하고 웹 서버의 구성을 수동으로 수정하여 해당 인증서를 사용하도록 만드는 방법을 보여주는 가이드가 많이 있습니다.\n\n이 기사에서는 어떤 것도 생성하거나 수동으로 편집하지 않고도 도커를 사용하여 자체 서명된 인증서가 있는 NGINX 컨테이너를 빠르게 실행하는 완전 자동화된 프로세스를 제시하겠습니다!\n\n# 보안 주의사항과 경고\n\n<div class=\"content-ad\"></div>\n\n- 자체 서명된 인증서는... 당신만 신뢰할 수 있습니다. 생산 환경에서 데이터를 제공하는 수단으로 사용할 수 없습니다. 그런 경우에는 적절한 인증서를 사용하세요.\n- 이 글에서 제시된 HTTPS로 콘텐츠를 제공하게끔 NGINX 구성은 작업을 수행하기 위한 최소한의 것입니다. 본격적인 TLS가 적용된 프로덕션 NGINX를 수정하려면 공식 가이드를 참고하세요.\n\n공개 키 암호화를 처음 시작하는 경우, 도움이 될 수 있는 소개 기사를 작성했습니다.\n\n# 설계 디자인\n\n빌드는 2단계 Docker 빌드로 설계되었습니다.\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_1.png\" />\n\n첫 번째 단계에서 Alpine Linux 이미지를 사용합니다. Alpine의 패키지 관리자인 APK를 사용하여 OpenSSL을 설치합니다. 다음 단계에서 OpenSSL을 사용하여 셀프 서명 인증서와 관련 개인 키를 생성합니다.\n\n두 번째 단계에서 NGINX 이미지를 사용합니다. 빌드는 이전 단계에서 생성된 인증서와 개인 키를 포함하도록 이미지를 수정하고 HTTPS를 활성화하기 위한 간단한 NGINX 구성을 작성합니다.\n\n# Dockerfile\n\n\n<div class=\"content-ad\"></div>\n\n```dockerfile\n# syntax=docker/dockerfile:1\n# 이 Dockerfile을 빌드하려면 Docker BuildKit가 활성화되어 있어야 합니다.\n\n# 사용할 Alpine 및 NGINX 버전을 정의합니다.\nARG ALPINE_VERSION=3.17.3\nARG NGINX_VERSION=1.23.4\n\n# OpenSSL을 사용하기 위해 Alpine 기반 이미지를 준비합니다.\nFROM alpine:${ALPINE_VERSION} as alpine\nARG DOMAIN_NAME=localhost\nARG DAYS_VALID=30\n\nRUN apk add --no-cache openssl\nRUN echo \"${DAYS_VALID}일 동안 유효한 ${DOMAIN_NAME} 도메인을 위한 자체 서명 인증서를 생성합니다.\" && \\\n    openssl \\\n    req -x509 \\\n    -nodes \\\n    -subj \"/CN=${DOMAIN_NAME}\" \\\n    -addext \"subjectAltName=DNS:${DOMAIN_NAME}\" \\\n    -days ${DAYS_VALID} \\\n    -newkey rsa:2048 -keyout /tmp/self-signed.key \\\n    -out /tmp/self-signed.crt\n\n# 위에서 생성한 인증서를 사용하여 NGINX 기반 이미지를 준비합니다.\nFROM nginx:${NGINX_VERSION} as nginx\nCOPY --from=alpine /tmp/self-signed.key /etc/ssl/private\nCOPY --from=alpine /tmp/self-signed.crt /etc/ssl/certs\nCOPY <<EOF /etc/nginx/conf.d/default.conf\nserver {\n    listen 80;\n    listen [::]:80;\n    listen 443 ssl;\n    listen [::]:443 ssl;\n    ssl_certificate /etc/ssl/certs/self-signed.crt;\n    ssl_certificate_key /etc/ssl/private/self-signed.key;\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n}\nEOF\n```\n\n👉 위의 Dockerfile을 시도하기 전에 Docker BuildKit가 활성화되어 있는지 확인해주세요. BuildKit는 레거시 빌더를 대체하는 개선된 백엔드로, Docker 데스크톱 및 Docker Engine 버전 23.0부터 사용자들에게 기본 빌더로 제공됩니다.\n\n## Stage 1: 인증서 생성\n\n인증서 및 개인 키를 생성하기 위해 OpenSSL을 사용하며 필요한 모든 정보를 인수로 전달하여 대화형 모드가 아닌 모드에서 명령을 실행합니다. 다음과 같은 Docker ARG를 지정하여 이 단계를 자신의 요구에 맞게 조정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- DOMAIN_NAME: 이는 인증서가 유효한 도메인입니다. 이는 자체 서명된 인증서이므로 여기에 지정한 도메인 이름은 중요한 역할을 하지 않지만, 해당 인증서를 사용해야 하는 응용 프로그램이 있는 경우, 액세스할 리소스의 도메인 이름과 일치하도록 변경해야 할 수 있습니다. 빌드에 사용된 기본 도메인은 localhost입니다.\r\n- DAYS_VALID: 인증서가 유효한 일수입니다. 테스트를 완료할 수 있도록 충분히 큰 숫자를 사용하십시오. 빌드에 사용된 기본 유효 기간은 30일입니다.\r\n\r\n## 단계 2: 수정된 NGINX 이미지 생성\r\n\r\n이 단계에서는 이전 단계에서 생성된 인증서와 개인 키를 가져와 새로 생성된 이미지로 복사합니다. 또한 HTTPS를 활성화하기 위해 간단한 NGINX 구성을 생성하기 위해 heredoc를 사용합니다.\r\n\r\n직접 이미지 상에 다른 구성을 사용하려는 경우, heredoc를 자체 콘텐츠로 대체하거나 결과 이미지를 확장하여 자체 이미지에 추가할 수 있습니다. 이미지를 자체 구성 파일로 확장하는 경우, 해당 파일은 다음 위치에 배치해야 합니다.\n/etc/nginx/conf.d/default.conf.\n\n<div class=\"content-ad\"></div>\n\n# 이미지 빌드하기\n\n알파인 이미지와 NGINX 이미지는 매우 작기 때문에 빌드 속도가 정말 빠릅니다. 다음으로 빌드를 시작하세요:\n\n```bash\ndocker build . -t nginx-self-signed\n```\n\n![이미지](/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_2.png)\n\n<div class=\"content-ad\"></div>\n\n# 이미지 실행하기\n\n이미지를 실행할 때, Docker Engine이 실행 중인 컴퓨터에서 80번 포트를 HTTP용, 그리고 443번 포트를 HTTPS용으로 사용할 수 있도록 해 주세요. 아래 명령어를 사용하여 컨테이너를 시작할 수 있습니다:\n\n\ndocker run -p 80:80 -p 443:443 nginx-self-signed\n\n\n![NGINXwithSelf-SignedCertificateonDocker_3](/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_3.png)\n\n<div class=\"content-ad\"></div>\n\n# 이미지 테스트 중\n\n신뢰할 수 있는 curl을 사용해서 몇 가지 테스트를 해봅시다. 만약 도커 엔진이 로컬 호스트에서 실행되지 않는다면 localhost를 적절한 주소로 바꿔주셔야 합니다.\n\n## HTTP 접근\n\ncurl localhost\n\n<div class=\"content-ad\"></div>\n\n\n![Screenshot](/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_4.png)\n\n여기에 볼 건 별거 없어요. HTTP 접근은 예상대로 작동합니다.\n\n## HTTPS 접근\n\ncurl https://localhost\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_5.png\" />\n\n여기서 NGINX가 HTTPS 요청에 응답했지만 curl은 다음과 같은 오류로 처리를 거부했습니다:\n\ncurl: (60) SSL certificate problem: self signed certificate\n\n이것은 HTTPS를 위한 기본 TLS를 설정하는 데 사용된 자체 서명된 인증서가 컴퓨터에 의해 신뢰되지 않기 때문입니다. 그렇다면 어떻게 해야 할까요? 여기에는 몇 가지 다른 옵션이 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 자체 서명된 인증서를 OS의 신뢰/인증서 저장소에 가져올 수 있습니다.\n- curl에 보안 인증서를 수락하도록 지시할 수 있습니다.\n\n보안 인증서를 수락하도록 curl에 요청해 봅시다:\n\ncurl https://localhost --insecure\n\n![이미지](/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_6.png)\n\n<div class=\"content-ad\"></div>\n\ncurl은 이제 기초 리소스의 내용을 즐겁게 출력합니다.\n\n마찬가지로, HTTPS URL을 인터넷 브라우저로 열어보려고 하면 보안 경고가 표시됩니다. 실제 경고 메시지와 진행 방법은 각각 다른 인터넷 브라우저마다 다를 수 있습니다. Chrome에서는 다음과 같이 표시됩니다:\n\n![Chrome에서의 보안 경고](/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_7.png)\n\nChrome에서 '고급' 버튼을 클릭한 다음 'localhost로 진행(안전하지 않음)' 옵션을 클릭할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_8.png\" />\n\n# 추가 콘텐츠\n\n가기 전에, 여기서 생성한 이미지를 사용하여 여러분 자신의 콘텐츠와 함께 사용할 수 있는 몇 가지 추가 팁이 있습니다.\n\n## 1. 여러분 자신의 콘텐츠 제공\n\n<div class=\"content-ad\"></div>\n\n기본 NGINX 환영 페이지를 제공하는 것은 누구에게나 큰 가치가 없을 것 같아요. 자신의 콘텐츠로 사용자 정의 NGINX 이미지를 가리킬 수 있도록 로컬 폴더를 컨테이너에 마운트하면 됩니다:\n\n```js\ndocker run \\\n  -p 80:80 -p 443:443 \\\n  -v {YOUR-PATH}:/usr/share/nginx/html \\\nnginx-self-signed\n```\n\n## 2. 자신의 콘텐츠에 대한 역방향 프록시 구성\n\n컨테이너 내부에 콘텐츠를 마운트하고 싶지 않을 경우, NGINX를 구성하여 이미 실행 중인 다른 서버에 대한 역방향 프록시로 사용할 수 있습니다. 다음 코드 조각은 이러한 구성을 설정하는 데 도움이 되나요. 그러나 역방향 프록시는 아래의 간단한 예제보다 더 복잡한데, 실제로 프록시하는 내용에 따라 다양한 문제가 발생할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\nNGINX 설정 파일의 상단에 upstream 서버를 정의해주세요:\n\n```js\nupstream api-gateway {\n  server http://some-server:80;\n}\n```\n\nNGINX 설정 파일의 server 블록을 다음과 같이 업그레이드해주세요:\n\n```js\nlocation /api/ {\n    proxy_pass                http://api-gateway;\n    proxy_redirect            off;\n    proxy_set_header          X-Real-IP $remote_addr;\n    proxy_set_header          X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header          X-NginX-Proxy true;\n    proxy_ssl_session_reuse   off;\n    proxy_set_header Host     $http_host;\n    proxy_cache_bypass        $http_upgrade;\n}\n```\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n이 글에서는 자체 서명 인증서를 사용하는 NGINX 도커 컨테이너를 빠르게 설정하는 방법을 소개했습니다. 컴퓨터에 OpenSSL을 설치할 필요가 없으며 인증서를 생성하기 위해 openssl 명령을 실행할 필요가 없습니다. 모든 작업이 Docker 빌드의 일부로 실행됩니다.\n\n또한 결과물인 NGINX 이미지에 자신의 콘텐츠를 통합하는 두 가지 예제를 제공했습니다. 컨테이너 내에서 콘텐츠를 마운트하거나 이미 실행 중인 다른 서버로 역방향 프록시하는 방법 등이 있습니다.\n\n이 글을 읽어주셔서 감사합니다. 다음 글에서 다시 뵙기를 기대합니다.\n\n<div class=\"content-ad\"></div>\n\n🔔 새 이야기를 발행할 때마다 알림을 받고 싶으세요? 제 콘텐츠는 항상 제가 본 것이나 일했던 것을 바탕으로 한 실용적인 기술 팁과 소프트웨어 엔지니어링 조언을 제공합니다:\nhttps://nmichas.medium.com/subscribe\n\n🚀 아직 Medium 회원이 아니신가요? 커피 한 잔 가격으로 매월 제 이야기에 액세스할 수 있습니다 (그리고 Medium의 수천 명의 다른 작가들의 이야기에도):\nhttps://medium.com/@nmichas/membership","ogImage":{"url":"/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_0.png"},"coverImage":"/assets/img/2024-06-19-NGINXwithSelf-SignedCertificateonDocker_0.png","tag":["Tech"],"readingTime":8}],"page":"19","totalPageCount":98,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}