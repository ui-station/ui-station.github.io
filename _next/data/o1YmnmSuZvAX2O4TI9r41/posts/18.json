{"pageProps":{"posts":[{"title":"테라폼 인터뷰 질문들","description":"","date":"2024-06-19 13:37","slug":"2024-06-19-TerraformInterviewQuestions","content":"\n\n먼저, 아래 Markdown 형식으로 표 태그를 변경하실 수 있습니다.\n\n\n![image](/assets/img/2024-06-19-TerraformInterviewQuestions_0.png)\n\nQ1: 테라폼을 사용하여 ec2 인스턴스를 만들었다고 가정해보겠습니다. 그리고 생성 후, 상태 파일에서 해당 항목을 제거했습니다. 그럼, terraform apply를 실행하면 어떻게 될까요?\n\n- 상태 파일에서 항목을 제거했기 때문에 terraform은 더 이상 해당 리소스를 관리하지 않게 됩니다. 따라서, 다음 apply에서는 새로운 리소스를 생성하게 됩니다.\n\nQ2: Terraform에서 상태 파일(State file)이란 무엇인가요?\n\n\n<div class=\"content-ad\"></div>\n\n- 상태 파일은 Terraform이 배포한 모든 인프라를 추적하는 파일입니다.\n\nQ3: 테라폼 상태 파일을 저장하는 가장 좋은 방법은 무엇인가요?\n\n- 상태 파일을 저장하는 가장 좋은 방법은 S3 또는 GitLab 관리 테라폼 상태와 같은 원격 백엔드에 유지하는 것입니다. 이렇게 하면 여러 사람이 동일한 코드 자원에 작업할 때 자원 중복이 발생하지 않습니다.\n\nQ4: 테라폼 상태 잠금이란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n- 테라폼 코드를 작성할 때 terraform plan, apply 또는 destroy를 실행하면 테라폼이 파괴적인 작업을 방지하기 위해 상태 파일을 잠근다.\n\nQ5: 테라폼 백엔드란 무엇인가요?\n\n- 백엔드는 테라폼이 상태 데이터 파일을 저장하는 위치를 정의합니다. 테라폼은 관리하는 리소스를 추적하기 위해 지속적인 상태 데이터를 사용합니다.\n\nQ6: 테라폼에서 플러그인이란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n- 플러그인은 HCL 코드를 API 호출로 변환하고 해당 공급업체(AWS, GCP)에 요청을 보내는 역할을 합니다.\n\nQ7: 널 리소스(null resource)란 무엇인가요?\n\n- 널(null)이라는 접두어가 붙어 있는 것을 보니 이 리소스는 클라우드 인프라에 존재하지 않을 것을 의미합니다.\n- Terraform의 null_resource는 다음과 같은 시나리오에서 사용할 수 있습니다 -\n- 쉘 명령 실행\n- 로컬 프로비저너 및 원격 프로비저너와 함께 사용할 수 있습니다.\n- Terraform 모듈, Terraform count, Terraform 데이터 소스, 로컬 변수와 함께 사용할 수 있습니다.\n- 출력 블록에도 사용할 수 있습니다.\n\nQ8: 프로비저너의 종류는 무엇이 있나요?\n\n<div class=\"content-ad\"></div>\n\n- 원격 exec: 원격 서버에서 Terraform을 사용하여 명령 실행\n- 로컬 exec: 로컬 시스템에서 Terraform을 사용하여 명령 실행\n\nQ9: Terraform 모듈의 사용 목적은 무엇인가요?\n\n- 한 번만 terraform 모듈을 생성하고 필요할 때마다 재사용할 수 있습니다.\n- 코드를 표준화하려고\n- 코드 중복을 줄이려고\n- 모듈을 버전별로 관리할 수 있습니다.\n\nQ10: Terraform으로 EC2와 VPC를 생성했는데 불행하게도 tfstate 파일이 삭제되었습니다. 복구할 수 있나요? (파일은 S3나 Dynamo DB에 있지 않고 로컬 머신에만 있음)\n\n<div class=\"content-ad\"></div>\n\n- Terraform import 명령을 사용하여 Terraform에 의해 생성된 리소스를 가져올 수 있습니다. 그러면 해당 리소스가 상태 파일에 저장됩니다.\n\nQ11: 만약 VPC, EC2, 보안 그룹, 액세스 키, 서브넷과 같이 여러 모듈을 생성했다면, Terraform은 어떻게 첫 번째로 배포할 리소스를 알 수 있을까요?\n\n- Terraform은 코드 내의 리소스 참조를 기반으로 종속성 그래프를 자동으로 파악합니다. 리소스간의 관계를 이해하고, 이 정보를 사용하여 리소스를 생성 또는 수정해야 할 순서를 결정합니다.\n- depends_on 키워드를 사용하여 명시적인 종속성을 정의할 수 있습니다.\n\nQ12: 어떻게 로직을 변경하지 않고 특정 리소스를 삭제/파괴할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n- taint과 destroy 명령어 사용하기\n- 해당 리소스를 taint하려면 terraform taint RESOURCE_TYPE.RESOURCE_NAME 명령어를 사용해야 합니다.\n- 리소스를 taint한 후, tainted된 리소스를 제거하기 위해 terraform destroy -target=RESOURCE_TYPE.RESOURCE_NAME 명령어를 실행할 수 있습니다.\n\nQ13: 테라폼에서 리소스 이름을 변경하는 방법은 무엇인가요?\n\n- 테라폼 mv 명령어를 사용하여 리소스 이름을 변경할 수 있습니다.\n\nQ14: 테라폼을 사용하여 EC2 인스턴스를 생성했는데, 누군가 수동으로 변경을 했다면, 다음에 Terraform plan을 실행하면 무엇이 발생하나요?\n\n<div class=\"content-ad\"></div>\n\n- Terraform 상태가 일치하지 않으면 Terraform이 EC2 인스턴스를 원하는 상태로 수정합니다. 즉, .tf 파일에 정의한 것과 일치하게 됩니다.\n\nQ15: Terraform에서 locals와 variables의 차이점은 무엇인가요?\n\n- 변수는 variables.tf 파일에 정의되거나 variables 키워드를 사용하여 정의됩니다. 변수는 덮어쓸 수 있지만 로컬 변수는 덮어쓸 수 없습니다.\n- 따라서 변수를 덮어쓰지 못하게 제한하려면 locals을 사용해야 합니다.","ogImage":{"url":"/assets/img/2024-06-19-TerraformInterviewQuestions_0.png"},"coverImage":"/assets/img/2024-06-19-TerraformInterviewQuestions_0.png","tag":["Tech"],"readingTime":3},{"title":"테라폼 안전하고 고가용성이 높으며 고장 허용성이 있는 클라우드 인프라 배포하기","description":"","date":"2024-06-19 13:35","slug":"2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures","content":"\n\n## \"구름의 공포\"\n\n![Image](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_0.png)\n\n# 소개\n\n강력하고 신뢰할 수 있는 클라우드 인프라는 기업이 고객에게 원활한 서비스를 제공하는 데 중요합니다. 고가용성과 내결함성은 이러한 인프라의 중요한 구성 요소입니다. 이를 달성하기 위해 많은 조직이 Terraform과 같은 인프라 자동화 도구를 활용하여 클라우드 자원을 배포하고 관리합니다.\n\n<div class=\"content-ad\"></div>\n\n이 글에서는 Terraform을 사용하여 AWS에 높은 가용성과 장애 허용성을 갖춘 클라우드 환경을 배포하는 방법을 살펴보겠습니다. 사용자 정의 VPC의 프라이빗 서브넷에 걸쳐 두 가용 영역을 포함하는 Auto Scaling 그룹 (ASG)을 포함한 환경을 만들어봅니다. 또한 ASG를 퍼블릭 서브넷에 놓인 애플리케이션 로드 밸런서 (ALB)로 프론트 엔드로 사용하고, 적절한 게이트웨이와 라우트 테이블 구성에 대해 살펴봅니다.\n\n결과적으로, Terraform을 활용하여 안정적이고 확장 가능한 클라우드 인프라를 구축하는 방법에 대한 깊은 이해를 갖게 될 것입니다. 이를 통해 고트래픽을 처리하고 각 구성요소의 개별 실패에도 가동 시간을 유지할 수 있는 클라우드 인프라를 구축할 수 있게 됩니다.\n\n자, 함께 알아보겠습니다!\n\n# 배경\n\n<div class=\"content-ad\"></div>\n\n## 테라폼\n\n이전에 작성한 기사 \"밤을 통해 테라포밍\"에서는 테라폼에 대해 잘 설명하고, 그 이점 및 테라폼의 가장 기본적인 개념 중 일부를 소개했어요.\n\n오늘의 데모를 위해 신속하게 준비하려면 해당 기사로 이동하여 소개 및 배경 그리고 기본 테라폼 명령어 섹션을 읽어보는 걸 권장합니다.\n\n## 아마존 EC2 오토 스케일링 그룹 (ASG)\n\n<div class=\"content-ad\"></div>\n\nAWS에서 제공하는 Amazon EC2 Auto Scaling 그룹은 수요에 따라 그룹 내 EC2 인스턴스의 수를 자동으로 조정해주는 서비스입니다.\n\n이를 통해 워크로드를 처리할 수 있는 원하는 인스턴스 수를 유지하고, 필요에 따라 인스턴스를 자동으로 추가하거나 제거할 수 있습니다. 이를 통해 애플리케이션은 수요에 따라 신축성 있게 확장 또는 축소할 수 있으며, 필요한 인스턴스만 실행하여 비용을 최적화할 수 있습니다.\n\nEC2 Auto Scaling 그룹은 다양한 스케일링 정책, 런치 구성, 인스턴스 유형을 구성하여 효율적이고 안정적인 애플리케이션 성능을 보장할 수 있습니다.\n\n## 애플리케이션 부하 분산기 (ALB)\n\n<div class=\"content-ad\"></div>\n\nAWS의 Application Load Balancer(ALB)는 애플리케이션 수준 콘텐츠를 기반으로 EC2 인스턴스와 같은 여러 대상에 대한 들어오는 트래픽을 분산시킬 수 있습니다.\n\nALB는 고급 라우팅 규칙을 기반으로 다양한 대상으로 트래픽을 지능적으로 라우팅할 수 있습니다. 이러한 라우팅 규칙에는 경로 기반 라우팅, 호스트 기반 라우팅, HTTP 헤더 기반 라우팅이 포함됩니다.\n\nALB는 SSL/TLS 오프로딩, 연결 드레이닝, 스티키 세션과 같은 다양한 기능을 제공하며, AWS의 자동 확장 및 탄력성 기능과 함께 사용하여 최적의 성능과 확장성을 보장할 수 있습니다.\n\n# 필수 사항\n\n<div class=\"content-ad\"></div>\n\n- Terraform 개념과 명령어에 대한 기본 지식 및 이해도\n- 기본적인 리눅스 명령 줄 지식\n- IAM 사용자 및 관리 권한이 있는 AWS 계정\n- 사용할 준비가 된 AWS Cloud9 IDE\n- AWS CLI가 설치되어 Cloud9 IDE에 구성됨\n- Amazon EC2 키페어\n\n# 사용 사례\n\n전자 상거래 회사인 REX TECH Corp는 휴일 시즌 동안 트래픽 증가를 처리해야 합니다. 회사는 웹 사이트가 고객에게 항상 응답 가능하고 이용 가능하도록 하기를 원합니다.\n\n당신의 매니저가 AWS에 클라우드 인프라를 배포하여 가용성과 고장 허용성을 보장할 것을 지시했습니다. 당신은 고객이 항상 웹 사이트에 접속할 수 있도록 하기 위해 트래픽에 따라 EC2 Auto Scaling 그룹(ASG)을 배포하기 위해 Terraform을 활용하기로 결정했습니다. 이 그룹은 개인 서브넷에 배포되며 공개 서브넷의 응용 프로그램 로드 밸런서(ALB)에 의해 선전되며, 트래픽에 따라 자동으로 확장 또는 축소되어 웹 사이트가 항상 고객에게 응답 가능하도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 목표\n\n- 두 개의 공용 서브넷, 두 개의 사설 서브넷이 있는 사용자 정의 가상 사설망(VPC)을 생성합니다.\n- 공용 서브넷에 NAT 게이트웨이를 활용하고, 인터넷 게이트웨이를 사용하여 외부 인터넷 트래픽을 제공합니다.\n- 공용 라우팅 테이블과 사설 라우팅 테이블을 구성합니다.\n- 공용 서브넷에 ALB를 시작합니다.\n- 사설 서브넷에 Auto Scaling 그룹을 시작합니다.\n- ALB의 공용 DNS를 출력하고 해당 URL을 사용하여 웹 서버에 도달 가능한지 확인합니다.\n\n# 단계 0: IDE 환경에서 최신 Terraform 버전 설치\n\nAWS Cloud9은 Terraform이 사전 설치되어 있지만, 최신 버전으로 업그레이드하여 해당 기능과 기능을 완전히 활용하는 것이 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n클라우드9 IDE에 최신 Terraform 버전을 설치하려면 다음 명령어를 차례대로 실행해주세요 —\n\n```js\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\nsudo yum -y install terraform\n```\n\n클라우드9 터미널에서 \"terraform version\"을 실행하여 최신 버전이 정상적으로 설치되었는지 확인해주세요 —\n\n![이미지](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_1.png)\n\n<div class=\"content-ad\"></div>\n\n이해를 높이고 데모를 더 효과적으로 따르기 위해 아래 링크를 클릭하여 GitHub에서 프로젝트 저장소를 클론해 주세요. 계속 진행하면서 필요한 대로 파일을 편집해도 괜찮아요.\n\n이제 Step 1로 이동해봅시다. - 커스텀 VPC 생성 및 구성.\n\n# Step 1: 커스텀 AWS VPC 생성 및 구성\n\n우리의 초기 목표는 AWS 환경 내에서 모든 리소스를 배포할 수 있는 커스텀 VPC를 구축하는 것입니다. 이를 위해 논리적으로 격리된 CIDR 블록을 정의하고 적합한 서브넷 및 해당 가용 영역(AZ)을 신중히 선택하여 배포해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n아래 테라폼 파일 코드를 검토해 봅시다. 사용자 정의 VPC를 생성하고 구성하는 코드입니다.\n\n## 코드 설명\n\n여기서는 지정된 CIDR 블록과 인스턴스 테넌시를 사용하여 AWS VPC를 생성합니다. 그런 다음 네 개의 서브넷을 생성합니다. 두 개는 공개 서브넷이고 두 개는 사설 서브넷입니다. 각 서브넷은 지정된 CIDR 블록을 가지며 앞서 생성된 VPC와 연결되어 지정된 가용 영역에 배포되도록 정의되어 있습니다.\n\n공개 서브넷은 map_public_ip_on_launch 속성이 true로 설정되어 있어 해당 서브넷에 배포된 인스턴스에는 자동으로 공개 IP 주소가 할당됩니다. 사설 서브넷은 해당 속성이 false로 설정되어 있어 해당 서브넷에 배포된 인스턴스는 NAT 게이트웨이를 통해 인터넷에서 접속할 수 없습니다. 각 서브넷에는 태그를 사용하여 이름이 지정되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이제 사용자 정의 VPC를 생성하고 구성했으니, 2단계로 넘어가서 인터넷 게이트웨이와 NAT 게이트웨이를 생성해 봅시다.\n\n# 2단계: 공용 서브넷에 인터넷 게이트웨이 및 NAT 게이트웨이 생성\n\nVPC에서 리소스를 배포할 때, 인터넷에 안전하고 효율적으로 연결되도록하는 것이 중요합니다. 이를 위해 공용 서브넷에 인터넷 게이트웨이와 NAT 게이트웨이를 생성하는 방법 중 하나입니다.\n\n인터넷 게이트웨이는 VPC 구성요소로서 VPC 내의 인스턴스와 인터넷 간의 통신을 허용합니다. 한편, NAT 게이트웨이는 사설 서브넷에 있는 인스턴스가 인터넷이나 다른 AWS 서비스에 연결할 수 있도록하며, 외부 트래픽에 대한 추가 보안도 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n아래에서 Terraform 파일을 만들고 구성하여 인터넷 게이트웨이와 NAT 게이트웨이를 생성하는 코드를 살펴보겠습니다 —\n\n## 코드 설명\n\n먼저, 인터넷 게이트웨이를 생성하고 사용자 정의 VPC에 연결합니다. 또한 Elastic IP 주소와 NAT 게이트웨이를 생성하고 Elastic IP와 공용 서브넷에 연결합니다. 인터넷 게이트웨이가 NAT 게이트웨이를 생성하기 전에 먼저 생성되도록 합니다.\n\n좋아요! 이제 인터넷 게이트웨이와 NAT 게이트웨이가 생성되었으니, 다음은 — 3단계로 넘어가 공용 및 사설 라우트 테이블을 구성하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 단계 3: 공용 라우트 테이블 및 사설 라우트 테이블 구성\n\n이 시점에서, 우리는 맞춤형 VPC와 게이트웨이를 성공적으로 생성했습니다. 그러나, 지금 직면한 문제는 VPC에 배포된 서비스 및 다른 구성 요소들 간의 통신을 관리하는 것입니다. 이곳에서 VPC 내에서의 네트워크 트래픽 흐름을 구성하는 것이 중요해집니다. 이를 위해, 서브넷 간의 네트워크 트래픽을 안내하기 위해 라우팅 테이블을 설정하는 것이 효과적입니다.\n\n아래의 코드를 검토하여 각각의 라우트 테이블을 생성하고 구성하는 방법을 확인해보겠습니다 —\n\n## 코드 설명\n\n<div class=\"content-ad\"></div>\n\n여기에서는 두 개의 라우팅 테이블을 생성 중입니다 — 공개 라우팅 테이블과 사설 라우팅 테이블. 공개 라우팅 테이블은 인터넷 게이트웨이로의 기본 경로를 갖고 있으며, 사설 라우팅 테이블은 NAT 게이트웨이로의 기본 경로를 갖고 있습니다.\n\n공개 라우팅 테이블은 두 개의 공개 서브넷과 연결되어 있고, 사설 라우팅 테이블은 두 개의 사설 서브넷과 연결되어 있습니다. 사설 리소스 연결은 사설 라우팅 테이블이 생성된 후에 수립됩니다. 이 라우팅 테이블에는 태그를 사용하여 이름이 지정되어 있습니다.\n\n이제, Step 4로 계속해 봅시다! 공개 서브넷에서 새 ALB를 실행합니다.\n\n# Step 4: 공개 서브넷에 ALB를 실행합니다.\n\n<div class=\"content-ad\"></div>\n\n사용자의 ALB를 활성화하기 위한 과정을 살펴보겠습니다. 사용자 정의 VPC의 공용 서브넷에서 ALB를 생성하는 것은 여러 단계를 거칩니다. 이 단계에는 보안 그룹 생성, 대상 그룹, 리스너 및 라우팅 규칙 설정이 포함됩니다.\n\n아래의 Terraform 코드를 검토해보세요. 이 코드는 ALB와 연결할 보안 그룹을 생성합니다.\n\n## 코드 설명\n\n이 Terraform 코드에서는 ALB와 함께 사용할 보안 그룹을 생성합니다. 보안 그룹은 이름과 설명과 함께 정의되며, 사용자 정의 VPC와 연관됩니다.\n\n<div class=\"content-ad\"></div>\n\nIngress 규칙을 통해 특정 CIDR 블록에서 HTTP 및 SSH 트래픽을 허용하고, 모든 트래픽을 허용하는 egress 규칙도 있습니다. 또한 보안 그룹에 쉽게 식별할 수 있도록 이름을 태그합니다.\n\n이제 ALB를 생성하고 구성하는 코드를 검토할 수 있습니다 —\n\n## 코드 설명\n\n여기서 ALB 및 해당 리소스를 생성합니다. 먼저 ALB를 생성하고, 사용할 이름, 서브넷 및 보안 그룹을 지정합니다.\n\n<div class=\"content-ad\"></div>\n\nALB가 트래픽을 전달할 타겟 그룹을 생성하여 이름, 포트, 프로토콜 및 헬스 체크 구성을 지정합니다.\n\n마지막으로 ALB를 위해 리스너를 생성하여 수신할 포트와 프로토콜을 지정하고, 기본 작업을 이전에 생성한 타겟 그룹으로의 트래픽 전달로 설정합니다. 이렇게 하면 리스너 규칙과 타겟 그룹의 구성에 따라 적절한 리소스로 트래픽이 전달됩니다.\n\n이제 다음 단계로 계속해 보죠. Terraform 구성 파일을 검토하여 사용자 정의 VPC를 생성하고 해당 적절한 서브넷을 구성하며, 인터넷 및 NAT 게이트웨이, 그리고 공용 및 비공용 라우팅 테이블을 설정했습니다. ALB의 구성이 이제 완료되어 준비되었으므로, Step 5로 이동할 수 있습니다 — 비공용 서브넷에서 ASG를 시작합니다.\n\n# Step 5: 비공용 서브넷에서 자동 스케일링 그룹 시작하기\n\n<div class=\"content-ad\"></div>\n\n오토 스케일링 그룹(ASG)은 일반적으로 공용 서브넷에 배포되지만, 민감한 데이터를 다루거나 규제 요구 사항을 준수해야 하는 경우와 같이 사설 서브넷을 선호하는 경우가 많이 있습니다.\n\n아래 코드를 살펴보면, 두 가용 영역(AZ)에 걸쳐 EC2 ASG를 사설 서브넷에서 시작하는 단계를 설명합니다 —\n\n## 코드 설명\n\n우선, ASG 런치 템플릿용 보안 그룹을 생성하고, 이를 이전에 생성한 사용자 정의 VPC와 연결해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n보안 그룹은 ALB의 보안 그룹에서 엄격히 HTTP 및 SSH 프로토콜에 대한 인바운드 트래픽을 허용합니다. 이렇게 함으로써 ASG에 대한 액세스는 ALB에서만 가능하게 되어 보안이 제공됩니다. 보안 그룹은 상태를 유지하므로 정의된 변수에서 지정된 CIDR 블록으로 어디서든지 아웃바운드 트래픽을 허용합니다.\n\n## Bash 스크립트\n\nASG를 생성하기 위해 우리가 준비할 것은 ASG의 런치 템플릿에 통합될 bash 스크립트를 먼저 작성하는 것입니다. 이 스크립트는 EC2 인스턴스의 사용자 데이터 역할을 하여 Apache 웹 서비스의 설치 및 시작, 그리고 맞춤식 웹 페이지의 생성을 자동화합니다.\n\n아래의 bash 스크립트를 살펴봅시다 —\n\n<div class=\"content-ad\"></div>\n\n## 코드 설명\n\n이 스크립트에서는 먼저 모든 yum 패키지 저장소를 업데이트한 다음 yum을 사용하여 Apache Web Server 패키지를 설치합니다. 그런 다음 Apache 서비스를 시작하고 부팅 시 자동으로 시작되도록 활성화합니다.\n\n또한 이 스크립트는 흔히 사용되는 소프트웨어 패키지를 쉽게 설치할 수 있게 하는 Extra Packages for Enterprise Linux (EPEL) 저장소를 설치합니다.\n\n또한 스크립트는 인스턴스를 고부하로 테스트하는 데 사용할 수 있는 stress 패키지를 설치합니다. 마지막으로 스크립트는 기본 index.html 파일에 REX TECH에 오신 것을 환영합니다! 메시지가 포함된 사용자 정의 웹페이지를 Apache 문서 루트 디렉토리 /var/www/html/에 추가합니다.\n\n<div class=\"content-ad\"></div>\n\n## ASG 생성 및 구성\n\n우리의 ASG 보안 그룹 및 Bash 스크립트가 생성되었으므로 이제 ASG를 만들고 구성할 차례입니다 —\n\n## 코드 설명\n\nASG를 생성할 때 최소, 최대 및 원하는 용량을 설정해야 합니다. 또한 ASG는 서브넷 ID로 식별되는 두 개의 사설 서브넷에서 시작됩니다.\n\n<div class=\"content-ad\"></div>\n\n런치 템플릿(Launch Template)은 Amazon Machine Image (AMI), 인스턴스 유형 및 키파일과 같은 구성과 함께 생성됩니다. 여기에는 이전에 생성한 Bash 스크립트를 base64로 인코딩한 사용자 데이터 파일로 지정합니다. 이는 Apache 웹 서버를 설치하고 시작하고 사용자 정의 웹 페이지를 생성하는 스크립트를 실행합니다.\n\n마지막으로 ASG를 ALB의 대상 그룹에 자동 확장 첨부를 사용하여 연결합니다. 이를 통해 ASG가 들어오는 트래픽에 대응하여 인스턴스 간 부하를 자동으로 균형 잡아 최적의 성능과 가용성을 보장합니다.\n\n이제 6단계로 넘어가서 output.tf 파일을 활용하여 ALB의 퍼블릭 DNS를 얻어봅시다.\n\n# Step 6: output.tf 파일을 활용하여 ALB의 퍼블릭 DNS를 얻기\n\n<div class=\"content-ad\"></div>\n\nALB를 만든 후에는 ALB의 공개 DNS에 직접 액세스하는 것이 귀찮을 수 있습니다. 다행히도, 테라폼에서 output.tf 파일을 생성하여 ALB의 공개 DNS를 즉시 얻을 수 있는 쉬운 해결책을 제공할 수 있습니다. 이를 통해 들어오는 트래픽이 올바르게 타깃 그룹 내의 인스턴스로 경로 지정되도록 보장할 수 있습니다.\n\n아래 output.tf 파일을 확인해 봅시다 —\n\n## 코드 설명\n\nOutputs를 사용하면 테라폼 인프라에서 생성된 리소스의 값들을 볼 수 있습니다. 여기서 우리는 테라폼 인프라를 위해 여러가지 output을 생성합니다. 각 output은 다른 리소스를 참조합니다.\n\n<div class=\"content-ad\"></div>\n\n저희의 출력물에는 VPC의 ID, 퍼블릭 및 프라이빗 서브넷의 ID, 인터넷 게이트웨이와 ALB의 DNS 이름이 포함됩니다. 이러한 출력물은 인프라에 대한 정보를 제공하기 위해 사용될 것입니다.\n\n이제, 7단계로 넘어갑시다 — 리소스 인자의 동적 값에 대한 변수 생성.\n\n# Step 7: 리소스 인자의 동적 값에 대한 변수 생성\n\nTerraform을 사용하여 인프라스트럭처를 생성할 때 리소스 인자에 동적 값들을 사용하는 것이 일반적입니다. 이러한 동적 값들은 사용자 입력, 환경 변수 또는 다른 리소스의 출력 값에 기반할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n이러한 동적 값들을 관리하기 위해 변수를 사용하는 것이 중요합니다. 변수를 사용하면 배포 사항 전반에 걸쳐 일관성을 유지하고, 코드를 읽고 유지하기 쉽게 만들며, 코드의 재사용성을 가능케 합니다.\n\n아래 코드에서 정의한 많은 변수들을 살펴봅시다 —\n\n## 코드 설명\n\n이곳에서는 이 Terraform 프로젝트에서 동적 값들을 사용할 수 있게 해주는 일련의 변수들을 정의합니다.\n\n\n<div class=\"content-ad\"></div>\n\n각 변수에는 이름, 기본값 및 유형이 있습니다. 기본값은 Terraform 코드를 실행할 때 특정 값이 지정되지 않은 경우 변수가 설정되는 값을 나타냅니다. 유형은 변수의 예상 데이터 유형을 지정합니다.\n\n예를 들어, aws_region 변수는 기본값으로 us-east-1을 가지고 있으며 문자열 유형을 갖습니다. 이는 문자열 값을 예상한다는 것을 의미합니다. 마찬가지로 auto-assign-ip 변수는 기본값으로 true를 가지고 있으며 불리언(bool) 유형을 갖습니다. 이는 불리언 값을 예상한다는 것을 의미합니다.\n\n일부 변수는 인프라의 네트워킹 구성 요소와 관련되어 있으며 VPC, 공용 서브넷 및 사설 서브넷의 이름과 같은 값을 지정합니다.\n\n변수를 정의함으로써 주요 구성 파일을 수정하지 않고 값이 쉽게 변경될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n알겠어요! 이제 우리가 모든 Terraform 구성 파일들을 검토했으니, Step 8로 넘어갈게요 — 정의된 인프라를 배포하기 위해 Terraform 워크플로우를 실행해봅시다.\n\n# Step 8: Terraform 워크플로우 실행해서 초기화, 유효성 검사, 계획 및 적용하기\n\nCloud9 터미널에서 필요한 공급자를 초기화하려면 Cloud9 터미널에서 다음 명령을 실행해주세요 —\n\n```js\nterraform init\n```\n\n<div class=\"content-ad\"></div>\n\n초기화 프로세스가 완료되면 아래와 같이 성공적인 프롬프트가 표시됩니다.\n\n![이미지](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_2.png)\n\n이제 다음 명령을 실행하여 코드에 구문 오류가 없는지 확인해보겠습니다 —\n\n```js\nterraform validate\n```\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 유효한 것으로 확인되는 성공 메시지를 생성하는 명령어여야 합니다.\n\n![image](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_3.png)\n\n이제 다음 명령어를 실행하여 Terraform이 적용할 모든 수정 사항 목록을 생성해 봅시다. —\n\n```js\nterraform plan\n```\n\n<div class=\"content-ad\"></div>\n\n테라폼이 기대하는 인프라 리소스에 적용할 변경 목록을 표시해야 합니다. \"+\" 기호는 추가될 내용을 나타내고, \"-\" 기호는 제거될 내용을 나타냅니다.\n\n![변경 목록](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_4.png)\n\n이제 이 인프라를 배포해 봅시다! 다음 명령을 실행하여 변경 사항을 적용하고 리소스를 배포하세요.\n\n참고 - 이 명령을 실행한 후에 변경 사항에 동의하기 위해 \"yes\"를 입력해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nterraform apply\n```\n\n테라폼은 인프라에 대한 모든 변경 사항을 적용하는 프로세스를 시작할 것입니다. 배포 프로세스가 완료될 때까지 잠시 기다려주시기 바랍니다.\n\n![이미지](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_5.png)\n\n# 성공!\n\n<div class=\"content-ad\"></div>\n\n지금은 추가된, 수정된 및 삭제된 리소스의 총 수를 명시하고 \"적용 완료\"라는 메시지로 프로세스를 마무리해야 합니다. 몇 가지 리소스 출력과 함께 해야 합니다.\n\n브라우저에서 웹 페이지에 액세스하려면 ALB의 DNS URL을 복사하여 저장하세요.\n\n\n![ALB DNS URL](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_6.png)\n\n\n이제 리소스가 생성되었는지 확인하기 위해 관리 콘솔에서 검토해봅시다.\n\n<div class=\"content-ad\"></div>\n\n# 단계 9: ASG 및 대상 그룹의 건강 상태에서 EC2 인스턴스 생성 확인\n\nAWS 관리 콘솔에서 EC2 대시보드로 이동하여 ASG에서 시작된 두 대 서버가 있는지 확인하세요.\n\n![이미지](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_7.png)\n\n또한 왼쪽 창으로 이동하여 대상 그룹을 선택하고 대상 그룹을 확인하고, 하단으로 스크롤하여 인스턴스의 건강 상태가 정상인지 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_8.png\" />\n\n좋아요! ASG가 예상대로 EC2 인스턴스를 생성했고, 모든 대상 그룹의 상태가 정상적으로 표시되고 있는 것을 성공적으로 확인했습니다. 이제 ALB를 통해 ASG의 웹 서버에 액세스할 수 있는지 확인해 봅시다.\n\n# 단계 10: 브라우저에서 URL을 사용하여 웹 서버에 도달 가능한지 확인하기\n\n설정한 브라우저를 열고 ALB의 DNS URL을 브라우저에 붙여넣어 주세요.\n\n<div class=\"content-ad\"></div>\n\n**알림** - ALB에 연결하려면 \"http://\" 프로토콜을 사용해야 합니다. \"https://\"는 사용하지 마세요.\n\n![사진](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_9.png)\n\n# 축하합니다!\n\n\"The Terror In The Clouds\"를 성공적으로 완료했어요. Terraform을 사용하여 신뢰할 수 있고 확장 가능한 클라우드 인프라를 구축하는 방법을 배웠습니다. 이 인프라는 높은 트래픽을 다룰 수 있으며 개별 구성 요소의 장애가 발생해도 가동 시간을 유지할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 정리하기\n\n## 인프라 파괴\n\n아래 명령어를 실행하여 이전에 Terraform으로 프로비저닝한 모든 리소스를 삭제/제거/해제하세요 —\n\n```js\nterraform destroy\n```\n\n<div class=\"content-ad\"></div>\n\n\"작업이 완료될 때까지 기다려주세요. 마침내 '파괴가 완료되었습니다'라는 메시지와 함께 파괴된 리소스의 양이 표시됩니다.\n\n지금까지 읽어 주셔서 감사합니다! 유익했기를 바랍니다.\n\n![TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_10.png)\n\nIfeanyi Otuonye는 클라우드/데브옵스 엔지니어로 클라우드 기술과 데브옵스 문화에 광신적인 엔지니어입니다. 배우려는 열정으로 움직이며 협업 환경에서 번영합니다. 정보 기술과 프로젝트 관리의 배경을 가지고 있으며 프로 선수의 삶을 균형 있게 유지하고 있습니다. 2021년 말부터 클라우드/데브옵스 엔지니어로 나아가기 위해 자기 학습을 시작했으며, 최근에는 Level Up In Tech 프로그램에 참여하였습니다!\"\n\n<div class=\"content-ad\"></div>\n\n# Master DynamoDB\n\n지금 'TOWARDSAWS' 코드를 사용하여 DynamoDB Book을 35% 할인된 가격으로 구매하세요.\n\n![DynamoDB Book](/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_11.png)","ogImage":{"url":"/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_0.png"},"coverImage":"/assets/img/2024-06-19-TerraformDeployingSecureHighlyAvailableandFault-TolerantCloudInfrastructures_0.png","tag":["Tech"],"readingTime":14},{"title":"OCI에서 Active Directory 게임을 실행하는 방법  1부","description":"","date":"2024-06-19 13:32","slug":"2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1","content":"\n제가 정말 즐겁게 즐겼던 프로젝트 중 하나는 GOAD입니다. 게임 오브 스로운즈 팬으로서 사이버 지식을 시험하고 동시에 즐길 수 있는 수단이었습니다. 만약 여러분이 스로운즈의 팬이라면, 이 Windows/AD/SCCM 랩은 여러분을 위한 것입니다.\n\n처음에는 중첩 가상화 서버를 만들려고 했는데, 테라폼과 앤서블 지식이 거의 없어서 어려움을 겪었습니다. 하지만 저는 자동화로 나아가기로 결정했습니다. 일상적인 작업에서 시간을 절약하고, 올바르게 구현된다면 인간 에러를 줄일 수 있기 때문에 자동화가 많은 도움이 된다는 점에 동의합니다.\n\n`Game of Active Directory (GOAD)` 프로젝트는 현실적이고 실용적인 경험을 통해 사이버 보안 기술을 향상시키는 포괄적인 랩 환경입니다. Oracle Cloud Infrastructure (OCI)에서 호스팅되며, 다양한 OCI 서비스와 통합하여 현실 세계의 보안 시나리오를 모의할 수 있습니다.\n\nhttps://github.com/adibirzu/GOAD/blob/main/docs/install_with_oci.md\n\n<div class=\"content-ad\"></div>\n\n랩 환경으로서 VCN 보안 목록에서 엄격한 ACL을 구현하지는 않았어요. 그러나 모든 포트에서 192.168.0.0/16에서의 액세스는 허용했어요.\n\n1- 우리는 OCI의 Windows Server/Ubuntu 이미지를 사용할 거에요. 이 선택은 PAYG 요금제를 사용해. Windows Server 평가판을 사용할 계획이라면, 자체 이미지를 빌드하고 테라폼 스크립트를 사용자 정의 이미지 OCID로 설정해야 해요:\n\nOracle Cloud Infrastructure 이미지\n\n2- 테라폼 스크립트와 PowerShell 스크립트는 ad →GOAD →providers →OCI 하위에 추가됐어요.\n\n<div class=\"content-ad\"></div>\n\n![Screenshot](/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_0.png)\n\n```js\n./goad.sh -t check -l GOAD -p oci -m local\n```\n\n3- 서로 다른 공급업체에서 사용자들이 알고 있는 동일한 워크플로를 제공하기 위해 goad.sh 스크립트에 제공자로 oci를 추가했습니다.\n\n![Screenshot](/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_1.png)\n\n<div class=\"content-ad\"></div>\n\nOCI에서 우분투의 기본 사용자는 ubuntu이므로 goad.sh의 모든 참조는 ubuntu 사용자를 사용할 것입니다:\n\n![2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_2](/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_2.png)\n\n4. 스크립트 아래에, 저는 설치 후 우분투 서버의 선행 조건을 설치하기 위해 setup_oci.sh 스크립트를 만들었습니다.\n\n![2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_3](/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_3.png)\n\n<div class=\"content-ad\"></div>\n\n5- 오라클 클라우드 인프라에서 랩을 프로비저닝하는 방법을 설명하는 docs/install_with_oci.md를 업데이트했습니다.\n\n6- ad →GOAD →providers →oci →ssh_keys →ubuntu-jumpox.pem에 개인 키를 배치하여 Ubuntu 인스턴스에 연결을 테스트했습니다.\n\n!!!! 이 키를 매우 주의해서 다루고, git에 공개하지 마세요.\n\n![이미지](/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_4.png)\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 마크다운 형식으로 변경하였습니다.\n\n<div class=\"content-ad\"></div>\n\n9- ad → GOAD → providers → oci → terraform 폴더에 Terraform/PowerShell 파일을 만들어주세요:\n\n- windows_cloud_init.ps1\n\n```powershell\n# 변수\n$adminUsername = “ansible”\n$adminPassword = ConvertTo-SecureString “YourSecurePassword123!” -AsPlainText -Force\n\n# ansible 사용자 생성\nNew-LocalUser $adminUsername -Password $adminPassword -FullName $adminUsername -Description “Ansible admin user”\nAdd-LocalGroupMember -Group “Administrators” -Member $adminUsername\n\n# WinRM 활성화\nwinrm quickconfig -q\nwinrm set winrm/config/service/auth @{Basic=”true”}\nwinrm set winrm/config/service @{AllowUnencrypted=”true”}\nwinrm set winrm/config/service @{EnableCompatibilityHttpsListener=”true”}\nwinrm set winrm/config/service @{EnableCompatibilityHttpListener=”true”}\n$cert = New-SelfSignedCertificate -DnsName $(hostname) -CertStoreLocation Cert:\\LocalMachine\\My\nwinrm create winrm/config/Listener?Address=*+Transport=HTTPS @{Hostname=$(hostname); CertificateThumbprint=$($cert.Thumbprint)}\nSet-Service -Name winrm -StartupType Automatic\nStart-Service -Name winrm\n\n# WinRM을 위한 기본 인증 및 암호화 트래픽 활성화\nSet-Item -Path WSMan:\\localhost\\Service\\Auth\\Basic -Value $true\nSet-Item -Path WSMan:\\localhost\\Service\\AllowUnencrypted -Value $true\n\n# WinRM 방화벽 예외 설정\nNew-NetFirewallRule -Name \"WinRM-HTTP\" -DisplayName \"WinRM (HTTP-In)\" -Protocol TCP -LocalPort 5985 -Action Allow -Enabled True\nNew-NetFirewallRule -Name \"WinRM-HTTPS\" -DisplayName \"WinRM (HTTPS-In)\" -Protocol TCP -LocalPort 5986 -Action Allow -Enabled True\n\n# TLS 1.2 프로토콜 설정\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n\n# NuGet 공급자 설치 및 PowerShellGet 업데이트\nInstall-PackageProvider -Name NuGet -Force -Confirm:$false\nUpdate-Module -Name PowerShellGet -Force -AllowClobber -Confirm:$false\n```\n\n- profile.tf\n\n<div class=\"content-ad\"></div>\n\n```js\nprovider \"oci\" {\n  tenancy_ocid      = var.tenancy_ocid\n  user_ocid         = var.user_ocid\n  fingerprint       = var.fingerprint\n  private_key_path  = var.private_key_path\n  region            = var.region\n}\n```\n\nvariables.tf\n\n```js\nvariable \"tenancy_ocid\" {\n  description = \"The OCID of your tenancy.\"\n  type        = string\n}\n\nvariable \"user_ocid\" {\n  description = \"The OCID of the user calling the API.\"\n  type        = string\n}\n\nvariable \"fingerprint\" {\n  description = \"The fingerprint of the API key.\"\n  type        = string\n}\n\nvariable \"private_key_path\" {\n  description = \"The path to the private key.\"\n  type        = string\n  default     = \"/Users/abirzu/.ssh/newpemkey.pem\"\n}\n\nvariable \"region\" {\n  description = \"The region to use.\"\n  type        = string\n  default     = \"eu-frankfurt-1\"\n}\n\nvariable \"compartment_ocid\" {\n  description = \"The OCID of the compartment to use.\"\n  type        = string\n}\n\nvariable \"availability_domain\" {\n  description = \"The availability domain to use.\"\n  type        = string\n  default     = \"NoEK:EU-FRANKFURT-1-AD-1\"\n}\n\nvariable \"shape\" {\n  description = \"The shape of the instance to be created.\"\n  type        = string\n  default     = \"VM.Standard.E5.Flex\"\n}\n\nvariable \"ocpus\" {\n  description = \"The number of OCPUs to allocate.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"memory_in_gbs\" {\n  description = \"The amount of memory in GBs.\"\n  type        = number\n  default     = 12\n}\n\nvariable \"ssh_authorized_keys\" {\n  description = \"The public key for SSH access to the instances.\"\n  type        = string\n}\n\nvariable \"image_ocid\" {\n  description = \"The OCID of the image to use.\"\n  type        = string\n}\n\nvariable \"windows2016_image_ocid\" {\n  description = \"The OCID of the Windows Server 2016 image.\"\n  type        = string\n}\n\nvariable \"windows2019_image_ocid\" {\n  description = \"The OCID of the Windows Server 2019 image.\"\n  type        = string\n}\n```\n\nnetwork.tf\n\n```js\n\n```\n\n<div class=\"content-ad\"></div>\n\n```js\nresource “oci_core_vcn” “generated_oci_core_vcn” {\n cidr_block = “192.168.0.0/16”\n compartment_id = var.compartment_ocid\n display_name = “goad-virtual-network”\n dns_label = “goadvcn”\n}\nresource “oci_core_subnet” “public_subnet” {\n cidr_block = “192.168.57.0/24”\n compartment_id = var.compartment_ocid\n display_name = “public-subnet”\n dns_label = “publicsubnet”\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n route_table_id = oci_core_route_table.public_route_table.id\n}\nresource “oci_core_subnet” “private_subnet” {\n cidr_block = “192.168.56.0/24”\n compartment_id = var.compartment_ocid\n display_name = “private-subnet”\n dns_label = “privatesubnet”\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n route_table_id = oci_core_route_table.private_route_table.id\n prohibit_internet_ingress = true\n prohibit_public_ip_on_vnic = true\n security_list_ids = [oci_core_security_list.winrm_rdp_security_list.id]\n}\nresource “oci_core_internet_gateway” “generated_oci_core_internet_gateway” {\n compartment_id = var.compartment_ocid\n display_name = “Internet Gateway goad-virtual-network”\n enabled = true\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n}\nresource “oci_core_nat_gateway” “generated_oci_core_nat_gateway” {\n compartment_id = var.compartment_ocid\n display_name = “NAT Gateway goad-virtual-network”\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n}\nresource “oci_core_route_table” “public_route_table” {\n compartment_id = var.compartment_ocid\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n display_name = “public-route-table”\nroute_rules {\n destination = “0.0.0.0/0”\n destination_type = “CIDR_BLOCK”\n network_entity_id = oci_core_internet_gateway.generated_oci_core_internet_gateway.id\n }\n}\nresource “oci_core_route_table” “private_route_table” {\n compartment_id = var.compartment_ocid\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n display_name = “private-route-table”\nroute_rules {\n destination = “0.0.0.0/0”\n destination_type = “CIDR_BLOCK”\n network_entity_id = oci_core_nat_gateway.generated_oci_core_nat_gateway.id\n }\n}\n  options {\n    type                  = \"DomainNameServer\"\n    server_type           = \"CustomDnsServer\"\n    custom_dns_servers    = [\"192.168.56.10\",\"8.8.8.8\"]\n  }\n options {\n        type = \"SearchDomain\"\n        search_domain_names = [ \"sevenkingdoms.local\" ]\n    }\n}\nresource “oci_core_security_list” “winrm_rdp_security_list” {\n compartment_id = var.compartment_ocid\n vcn_id = oci_core_vcn.generated_oci_core_vcn.id\n display_name = “winrm_rdp_security_list”\negress_security_rules {\n protocol = “all”\n destination = “0.0.0.0/0”\n stateless = false\n }\ningress_security_rules {\n protocol = “all”\n source = “192.168.0.0/16”\n stateless = false\n }\n}\n```\n\n```js\nresource “oci_core_default_dhcp_options” “default_dhcp_options” {\n manage_default_resource_id = oci_core_vcn.generated_oci_core_vcn.default_dhcp_options_id\noptions {\n type = “DomainNameServer”\n server_type = “CustomDnsServer”\n custom_dns_servers = [“192.168.56.10”]\n search_domain_names = [“sevenkingdoms.local”]\n }\n```\n\nThe DHCP configuration part is essential, as if it’s not properly configured, it will result in the failure of the ansible jobs. If the terraform variable will not add the search domain, you need to do this manually:\n\nGo to the VCN → DHCP Options:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_7.png\" />\n\n화면 오른쪽 상단에 있는 3 점을 클릭한 후 편집을 선택하세요. 여기에서 외부 DNS 서버를 추가하고 사용자 정의 검색 도메인인 Seven Kingdoms를 추가할 수 있습니다.\n\n<img src=\"/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_8.png\" />\n\n이 부분에서 DNS 문제를 해결해야합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\noptions {\ntype = \"도메인 네임 서버\"\nserver_type = \"사용자 정의 DNS 서버\"\ncustom_dns_servers = [\"192.168.56.10\",\"8.8.8.8\"]\n}\noptions {\ntype = \"검색 도메인\"\nsearch_domain_names = [\"seven kingdoms.local\"]\n}\n```\n\njumpbox.tf\n\n```js\nresource \"oci_core_instance\" \"jumpbox\" {\n availability_domain = var.availability_domain\n compartment_id = var.compartment_ocid\n display_name = \"jumpbox\"\n shape = var.shape\nshape_config {\n baseline_ocpu_utilization = \"BASELINE_1_1\"\n memory_in_gbs = var.memory_in_gbs\n ocpus = var.ocpus\n }\nsource_details {\n source_id = var.image_ocid\n source_type = \"이미지\"\n }\n#이미지 OCID’S https://docs.oracle.com/en-us/iaas/images/image/bd616d0a-fae4-490e-bd31-a9406095b844/\n create_vnic_details {\n assign_ipv6ip = false\n assign_private_dns_record = true\n assign_public_ip = true\n subnet_id = oci_core_subnet.public_subnet.id\n }\nmetadata = {\n ssh_authorized_keys = var.ssh_authorized_keys\n }\nagent_config {\n is_management_disabled = false\n is_monitoring_disabled = false\nplugins_config {\n desired_state = \"사용 안 함\"\n name = \"취약점 스캐닝\"\n }\n plugins_config {\n desired_state = \"사용 안 함\"\n name = \"관리 에이전트\"\n }\n plugins_config {\n desired_state = \"사용함\"\n name = \"사용자 지정 로그 모니터링\"\n }\n plugins_config {\n desired_state = \"사용 안 함\"\n name = \"컴퓨팅 RDMA GPU 모니터링\"\n }\n plugins_config {\n desired_state = \"사용함\"\n name = \"컴퓨팅 인스턴스 모니터링\"\n }\n plugins_config {\n desired_state = \"사용 안 함\"\n name = \"컴퓨팅 HPC RDMA 자동 구성\"\n }\n plugins_config {\n desired_state = \"사용 안 함\"\n name = \"컴퓨팅 HPC RDMA 인증\"\n }\n plugins_config {\n desired_state = \"사용함\"\n name = \"클라우드 가드 워크로드 보호\"\n }\n plugins_config {\n desired_state = \"사용 안 함\"\n name = \"블록 볼륨 관리\"\n }\n plugins_config {\n desired_state = \"사용 안 함\"\n name = \"바스천\"\n }\n }\navailability_config {\n is_live_migration_preferred = true\n recovery_action = \"인스턴스 복원\"\n }\nplatform_config {\n is_symmetric_multi_threading_enabled = true\n type = \"AMD_VM\"\n }\ninstance_options {\n are_legacy_imds_endpoints_disabled = false\n }\n}\n```\n\nwindowsvm.tf\n\n<div class=\"content-ad\"></div>\n\n```js\n리소스 \"oci_core_instance\" \"windows_instance\" {\n for_each = {\n kingslanding = {\n name = \"kingslanding\"\n private_ip_address = \"192.168.56.10\"\n admin_username = \"ansible\"\n admin_password = \"8dCT-DJjgScp\"\n image_ocid = var.windows2019_image_ocid\n }\n winterfell = {\n name = \"winterfell\"\n private_ip_address = \"192.168.56.11\"\n admin_username = \"ansible\"\n admin_password = \"NgtI75cKV+Pu\"\n image_ocid = var.windows2019_image_ocid\n }\n castelblack = {\n name = \"castelblack\"\n private_ip_address = \"192.168.56.22\"\n admin_username = \"ansible\"\n admin_password = \"NgtI75cKV+Pu\"\n image_ocid = var.windows2019_image_ocid\n }\n meereen = {\n name = \"meereen\"\n private_ip_address = \"192.168.56.12\"\n admin_username = \"ansible\"\n admin_password = \"Ufe-bVXSx9rk\"\n image_ocid = var.windows2016_image_ocid\n }\n braavos = {\n name = \"braavos\"\n private_ip_address = \"192.168.56.23\"\n admin_username = \"ansible\"\n admin_password = \"978i2pF43UJ-\"\n image_ocid = var.windows2016_image_ocid\n }\n }\navailability_domain = var.availability_domain\n compartment_id = var.compartment_ocid\n display_name = each.value.name\n shape = \"VM.Standard.E5.Flex\"\nshape_config {\n ocpus = 2\n memory_in_gbs = 32\n }\nsource_details {\n source_id = each.value.image_ocid\n source_type = \"image\"\n }\ncreate_vnic_details {\n assign_ipv6ip = false\n assign_private_dns_record = true\n assign_public_ip = false\n subnet_id = oci_core_subnet.private_subnet.id\n hostname_label = each.value.name\n private_ip = each.value.private_ip_address\n }\nmetadata = {\n user_data = base64encode(file(\"${path.module}/windows_cloud_init.ps1\"))\n admin_password = each.value.admin_password\n }\n}\n```\n\noutputs.tf\n\n```js\noutput \"ubuntu_jumpbox_ip\" {\n value = oci_core_instance.jumpbox.public_ip\n}\noutput \"windows_instance_opc_passwords\" {\n value = { for k, v in oci_core_instance.windows_instance : k => v.metadata.admin_password }\n sensitive = true\n}\n```\n\n프로비저닝 중 발생한 일반적인 오류입니다.\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 Markdown 형식에 맞게 표 태그를 변경하십시오.\n\n![이미지](/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_9.png)\n\n모든 TF가 생성된 후, GOAD git 저장소가 정상적으로 동기화된 경우 다음을 실행해야 합니다:\n\nabirzu@abirzu-mac GOAD % ./goad.sh -t destroy -l GOAD -p oci -m local\n\n모든 서버가 가동되고 작동 중인 것을 확인할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_10.png\" />\n\n축하합니다! 환경이 작동 중에 있습니다.\n\n# 서버\n\n이 랩은 실제로 다섯 개의 가상 머신으로 구성되어 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- kingslanding: DC01는 Windows Server 2019에서 실행 중입니다(기본으로 windefender가 활성화됨)\n- winterfell: DC02는 Windows Server 2019에서 실행 중입니다(기본으로 windefender가 활성화됨)\n- castelblack: SRV02는 Windows Server 2019에서 실행 중입니다(windefender가 기본적으로 비활성화됨)\n- meereen: DC03은 Windows Server 2016에서 실행 중입니다(기본으로 windefender가 활성화됨)\n- braavos: SRV03은 Windows Server 2016에서 실행 중입니다(기본으로 windefender가 활성화됨)\n\n도메인: north.sevenkingdoms.local\n\n- winterfell: DC01\n- castelblack: SRV02: MSSQL / IIS\n\n도메인: sevenkingdoms.local\n\n<div class=\"content-ad\"></div>\n\n- kingslanding: DC02\n- castelrock: SRV01 (자원 부족으로 비활성화됨)\n\n# 도메인: essos.local\n\n- braavos: DC03\n- meeren: SRV03: MSSQL / ADCS\n\n인터넷에서 몇 가지 가이드를 따르거나 자신만의 방법을 찾아 시작할 수 있습니다!\n\n<div class=\"content-ad\"></div>\n\nAD | Mayfly (mayfly277.github.io)\n\nSolving Game of Active Directory (GOAD) by Orange Cyberdefense Part-1 | by n00🔑 | Medium\n\n제1부에서는 OCI에서 GOAD 랩을 만드는 방법을 소개했고, 다음 부분에서는 다음에 초점을 맞출 것입니다:\n\nOCI 통합:\n\n<div class=\"content-ad\"></div>\n\n1. OCI Management Agent:\n\n- 설명: 클라우드 작업을 자동화하고 모니터링합니다.\n- 통합: GOAD 환경 내 자원의 배포 및 관리를 간소화합니다.\n\n2. Sysmon 및 로깅 분석:\n\n<div class=\"content-ad\"></div>\n\n• 설명: Sysmon은 시스템 활동을 기록하여 침입 탐지를 지원합니다.\n\n• 통합: Sysmon 이벤트를 수집하고 OCI Logging Analytics로 전송하여 고급 로그 분석 및 시각화를 지원합니다. 자세한 내용은 여기에서 확인하세요.\n\n3. Arkime 통합:\n\n• 설명: Arkime은 오픈 소스 패킷 캡처 및 검색 도구입니다.\n\n<div class=\"content-ad\"></div>\n\n• 통합: 효율적인 데이터 색인 및 검색을 위해 OCI OpenSearch를 활용합니다. 자세한 내용은 여기에서 확인할 수 있습니다.\n\n4. 클라우드 가드 인스턴스 보안:\n\n• 설명: 클라우드 가드는 포괄적인 클라우드 보안 관리를 제공합니다.\n\n• 통합: 로깅 분석을 활용하여 인스턴스 보안을 강화하며 실시간 위협 감지 및 대응이 가능합니다. 더 많은 정보는 여기에서 확인할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_0.png"},"coverImage":"/assets/img/2024-06-19-HowtorunGameofActiveDirectoryinOCIPart1_0.png","tag":["Tech"],"readingTime":16},{"title":"의존성 관리자 Dependabot GitHub 및 Terraform 버전 관리","description":"","date":"2024-06-19 13:30","slug":"2024-06-19-DependabotGitHubandTerraformversionsmanagement","content":"\n\n![image](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_0.png)\n\n프로젝트가 성장함에 따라 언젠가는 패키지, 모듈 및 차트 버전을 업그레이드해야 할 필요성이 생길 것입니다.\n\n물론 수동으로 할 수도 있지만, 어느 정도까지만 가능합니다. 결국에는 물리적으로 모든 업데이트 사항을 추적하고 업데이트하는 것이 불가능해질 수 있습니다.\n\n이와 같은 프로세스를 자동화하는 다양한 솔루션이 있지만, 가장 일반적으로 사용되는 것은 Renovate와 Dependabot입니다.\n\n<div class=\"content-ad\"></div>\n\n우크라옵스 슬랙 투표 결과에 따르면, Renovate가 훨씬 많은 투표를 받았으며 실제로 Dependabot보다 더 많은 작업을 수행할 수 있습니다.\n\n반면, Dependabot는 이미 GitHub 저장소에서 사용 가능하며 모든 가격 요금제에서 이용 가능합니다. 그러니까, GitHub를 사용하는 경우, Dependabot을 설정하려면 구성 파일을 추가하기만 하면 됩니다. 앞으로 봤을 때, Renovate를 설정하는 것이 더 쉬우나, 다음 게시물에서 더 자세히 다루겠습니다 — Renovate: GitHub 및 Helm 차트 버전 관리.\n\n실제로, Dependabot를 거의 모든 플랫폼에서 사용할 수 있습니다 — GitHub, Github 엔터프라이즈, Azure DevOps, GitLab, BitBucket 및 AWS CodeCommit 등. Dependabot를 실행하는 방법을 확인하려면 How to run Dependabot을 참조하세요.\n\n그러나 — 이게 제게는 큰 놀램이었습니다 — Dependabot은 Helm 차트와 함께 작동하지 않습니다. 그러나 Terraform에서 작동하며 이미 일부 파이썬 코드 저장소에서 사용 가능합니다. 그러니 먼저 그것에 대해 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n다시 미래를 내다보면, 저는 Renovate를 훨씬 더 좋아했고, 이후에는 Renovate를 사용할 것입니다.\n\n# Dependabot가 작동하는 방식\n\n다음은 그 방식입니다:\n\n- 저장소에 Dependabot 구성 파일을 만듭니다.\n- 파일 내에서 무엇을 정확히 확인해야 하는지 설명합니다 — pip 라이브러리, Terraform 모듈 등을\n- 특히 관심 있는 것을 설명합니다 — 보안 업데이트 또는 버전 업데이트\n- 업데이트를 발견하면 — Dependabot은 Pull Request를 만들어 해당 업데이트에 대한 상세 정보를 추가합니다\n- …\n- 수익 창출!\n\n<div class=\"content-ad\"></div>\n\n오늘은 무엇을 할까요?\n\n- 우리는 모니터링을 위한 GitHub 저장소를 갖고 있어요\n- 거기에 Terraform 코드가 있고\n- Dependabot을 사용하여 버전 확인 및 PR 생성을 구성할 거예요\n\n문서 — Dependabot Quick Start Guide, dependabot.yml 파일 구성 옵션.\n\n지원되는 저장소 및 생태계 — Dependabot이 지원하는 시스템을 확인하세요.\n\n<div class=\"content-ad\"></div>\n\n# Dependabot과 Terraform\n\nTerraform의 맥락에서 Dependabot으로 모니터링할 수 있는 것은 프로바이더 및 모듈의 버전입니다.\n\n예를 들어, 제공자 버전이 설정된 versions.tf와 여러 모듈을 사용하는 lambda.tf와 같은 두 파일이 있습니다. 모듈에는 terraform-aws-modules/security-group/aws, terraform-aws-modules/lambda/aws 등이 있습니다:\n\n![이미지](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_1.png)\n\n<div class=\"content-ad\"></div>\n\n이제 Dependabot가 이들에서 버전을 모니터링하도록 시작하려면 디렉토리 .github를 생성하고 그 안에 dependabot.yml 파일을 만듭니다:\n\n![이미지](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_2.png)\n\n파일에 다음과 같이 매개변수를 설정합니다:\n\n```js\nversion: 2\nupdates:\n  - package-ecosystem: \"terraform\"\n    directory: \"/terraform\"\n    schedule:\n      interval: \"daily\"\n      time: \"09:00\"\n      timezone: \"Europe/Kyiv\"\n    assignees:\n      - arseny-zinchenko\n    reviewers:\n      - arseny-zinchenko\n    open-pull-requests-limit: 10\n```\n\n<div class=\"content-ad\"></div>\n\n보편적으로 매개변수 이름에서 모든 것이 명확해요:\n\n- package-ecosystem: 이 설정이 Terraform을 위한 것이므로, 우리는 이것을 지정합니다\n- directory: Terraform 파일은 리포지토리 루트의 terraform 디렉토리에 있습니다\n- schedule: 체크 일정 - 먼저 dependabot.yml 파일을 추가하면 체크가 즉시 시작되며, 나중에 수동으로 실행할 수 있습니다\n- assignees and reviewers: 나를 위해 즉시 PR을 만들어주세요\n- open-pull-requests-limit: 기본적으로 Dependabot은 최대 5개의 PR을 엽니다. 이 매개변수로 증가시킬 수 있습니다\n\n리포지토리에 푸쉬하고 상태를 확인해보세요.\n\n리포지토리에서 Insights ` Dependency graph ` Dependabot로 이동하여 체크가 시작되었음을 확인하세요:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_3.png)\n\nIn a minute, we’ll have open Pull Requests:\n\n![image](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_4.png)\n\n![image](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n동시에, Dependabot은 업데이트에 대한 세부 정보를 주석으로 추가합니다 — 릴리스 노트, 변경 내역 등:\n\n![이미지](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_6.png)\n\n그러나 이 모든 것이 어디서는 안 될 수도 있습니다.\n\n예를 들어, Lambda 모듈의 업데이트는 세부 정보 없이 생성되었습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_7.png)\n\nBut Renovate does it much better.\n\n# Dependabot, and GitHub Secrets\n\nAnother nuance is the GitHub Secrets that are available to Dependabot.\n\n\n<div class=\"content-ad\"></div>\n\n프로젝트의 terraform 디렉토리에 변경 내용이 있는 PR이 올라오면, 우리는 GitHub Actions Workflow를 실행합니다. 해당 Workflow에서는 Terraform의 체크를 수행합니다 (GitHub Actions: Terraform deployments with a review of planned changes 참조).\n\n이 Workflow는 전용 저장소에 위치해 있으며, 접근하기 위해 GitHub 배포용 키가 GitHub Actions Secrets를 통해 호출하는 Workflow로 전달됩니다.\n\n그러나 Dependabot에서 시작된 GitHub Actions 작업에서 다음 단계가 실패했습니다:\n\n![Dependabot Error](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_8.png)\n\n<div class=\"content-ad\"></div>\n\n워크플로 자체는 모든 비밀을 secrets: inherit:를 통해 전달합니다.\n\n```js\n...\njobs:\n  terraform-test:\n    # Reusable Workflow 파일 호출\n    uses: ORG_NAME/atlas-github-actions/.github/workflows/call-terraform-check-and-plan.yml@master\n    with:\n      aws-iam-role: ${ vars.AWS_IAM_ROLE }\n      aws-env: ${ vars.AWS_ENV }\n      pr-num: ${ github.event.pull_request.number }\n      environment: ops\n      slack-channel: '#cicd-devops'      \n    secrets:\n      inherit\n```\n\n그러나 Dependabot에서는 이러한 비밀 정보를 Actions의 secrets 및 변수가 아닌 Actions의 secrets와 변수인 Dependabot에 별도로 설정해야 합니다:\n\n<img src=\"/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_9.png\" />\n\n<div class=\"content-ad\"></div>\n\n새로운 비밀을 추가했고, 이제 확인이 작동합니다:\n\n![Dependabot, and private registries/repositories](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_10.png)\n\n# Dependabot 및 개인 레지스트리/저장소\n\n다른 것들 중에, 우리는 개인 저장소에 저장된 자체 Terraform 모듈을 가지고 있어요.\n\n<div class=\"content-ad\"></div>\n\n의존성 보트에 액세스할 때 \"의존성 보트가 ORG_NAME/atlas-tf-modules에 액세스할 수 없습니다\"라는 오류가 발생합니다:\n\n![image](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_11.png)\n\n첫 번째 옵션은 dependabot.yml 파일에서이 저장소 또는 다른 레지스트리를 명시적으로 추가하는 것입니다. - 개인 레지스트리 구성 항목 참조.\n\n두 번째 옵션은 단순히 '액세스 권한 부여'를 클릭하는 것입니다. 이렇게 하면 조직의 모든 저장소에 대해 해당 저장소의 액세스가 열립니다.\n\n<div class=\"content-ad\"></div>\n\n해당 테이블 태그를 마크다운 형식으로 변경하거나 수동으로 변경하세요. 조직 설정으로 이동한 후 `보안 코드` 및 `전역 설정`으로 이동하여 `사적 저장소에 Dependabot 액세스 권한 부여` 섹션에서 원하는 저장소에 대한 액세스를 추가하세요:\n\n![Dependabot GitHub and Terraform versions management](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_12.png)\n\n# Dependabot 및 수동 실행\n\n접근 권한을 추가했다면, 다시 저장소로 돌아가 Insights -> Dependency graph -> Dependabot으로 이동한 후 업데이트 확인을 클릭하세요:\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 작동 중입니다:\n\n![Image 1](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_13.png)\n\n![Image 2](/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_14.png)\n\n일반적으로 그게 전부에요. 이제 우리는 모든 저장소를 직접 관리하지 않아도 Terraform 업데이트를 받게 될 거예요.\n\n<div class=\"content-ad\"></div>\n\n한번 더 말해도, Renovate가 정말로 더 좋아. Renovate: GitHub을 보고, Helm Charts 버전 관리를 확인해봐.\n\n원문은 RTFM: Linux, DevOps, 그리고 시스템 관리에서 공개되었습니다.","ogImage":{"url":"/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_0.png"},"coverImage":"/assets/img/2024-06-19-DependabotGitHubandTerraformversionsmanagement_0.png","tag":["Tech"],"readingTime":7},{"title":"Azure Storage Account에 개인 엔드포인트와 Terraform을 사용하여 컨테이너 문제 해결하는 방법","description":"","date":"2024-06-19 13:28","slug":"2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform","content":"\n<img src=\"/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_0.png\" />\n\n오늘은 Terraform으로 Azure Storage Account와 Private Endpoint를 배포할 때 발생하는 일반적인 오류에 대해 이야기해보겠습니다.\n\n저희가 스토리지 계정 컨테이너를 추가하려고 시도하면, 다음과 같은 오류가 발생합니다:\n\n# 1. 우리의 시나리오\n\n<div class=\"content-ad\"></div>\n\n- 테라폼이 실행 중인 가상 머신이 있습니다.\n\n- 리소스 그룹 \"kopicloud-core-dev-we-rg\"\n- 가상 네트워크 \"kopicloud-core-dev-we-vnet\"\n- 서브넷 \"kopicloud-core-dev-we-subnet\"\n\n2. 기존의 \"privatelink.blob.core.windows.net\" 프라이빗 DNS 영역이 있습니다.\n\n- 리소스 그룹 \"kopicloud-core-dev-we-dns-rg\"\n\n<div class=\"content-ad\"></div>\n\n3. 새로운 Azure Storage Account와 프라이빗 엔드포인트를 배포할 것입니다.\n\n- 리소스 그룹 \"kopicloud-storage-dev-we-rg\"\n- 가상 네트워크 \"kopicloud-storage-dev-we-vnet\"\n- 서브넷 \"kopicloud-storage-dev-we-endpoint-subnet\"\n\n# 2. Azure Storage Account와 프라이빗 엔드포인트를 배포하는 Terraform 코드\n\n이 코드에 대해 설명은 이 이야기에서 하지 않겠습니다. 더 자세한 내용은 \"Terraform을 사용한 Azure Storage Account의 프라이빗 엔드포인트\" 이야기를 확인해주세요.\n\n<div class=\"content-ad\"></div>\n\n아래는 \"network-variables.tf\" 파일입니다:\n\n```js\nvariable \"network-vnet-cidr\" {\n  type        = string\n  description = \"네트워크 VNET의 CIDR\"\n}\n\nvariable \"network-endpoint-subnet-cidr\" {\n  type        = string\n  description = \"네트워크 서브넷의 CIDR\"\n}\n```\n\n아래는 \"network.tf\" 파일입니다:\n\n```js\n# 네트워크를 위한 리소스 그룹 생성\nresource \"azurerm_resource_group\" \"network-rg\" {\n  name     = \"kopicloud-storage-dev-we-rg\"\n  location = var.location\n}\n\n# 네트워크 VNET 생성\nresource \"azurerm_virtual_network\" \"network-vnet\" {\n  name                = \"kopicloud-storage-dev-we-vnet\"\n  address_space       = [var.network-vnet-cidr]\n  resource_group_name = azurerm_resource_group.network-rg.name\n  location            = azurerm_resource_group.network-rg.location\n}\n\n# Endpoint 서브넷 생성\nresource \"azurerm_subnet\" \"endpoint-subnet\" {\n  name                 = \"kopicloud-storage-dev-we-endpoint-subnet\"\n  address_prefixes     = [var.network-endpoint-subnet-cidr]\n  virtual_network_name = azurerm_virtual_network.network-vnet.name\n  resource_group_name  = azurerm_resource_group.network-rg.name\n\n  private_endpoint_network_policies_enabled = true\n\n  service_endpoints = [\"Microsoft.Storage\"]\n}\n```\n\n<div class=\"content-ad\"></div>\n\n\"storage-account.tf\" 파일:\n\n```js\n# 기존 Private DNS Zone 참조\ndata \"azurerm_private_dns_zone\" \"dns-zone\" {\n  name                = \"privatelink.blob.core.windows.net\"\n  resource_group_name = \"kopicloud-core-dev-we-dns-rg\"\n}\n\n# Private DNS Zone 네트워크 링크 생성\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"network_link\" {\n  name                  = \"kopicloud-storage-dev-we-vnet-link\"\n  resource_group_name = data.azurerm_private_dns_zone.dns-zone.resource_group_name\n  private_dns_zone_name = data.azurerm_private_dns_zone.dns-zone.name\n  virtual_network_id    = azurerm_virtual_network.network-vnet.id\n}\n\n# 스토리지 계정 생성\nresource \"azurerm_storage_account\" \"storage\" {\n  name                = \"kopicloudstoragedevwesta\"\n  resource_group_name = azurerm_resource_group.network-rg.name\n  location            = azurerm_resource_group.network-rg.location\n\n  account_kind             = \"StorageV2\"\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n}\n\n# 프라이빗 엔드포인트 생성\nresource \"azurerm_private_endpoint\" \"endpoint\" {\n  name                = \"kopicloudstoragedevwesta-pe\"\n  resource_group_name = azurerm_resource_group.network-rg.name\n  location            = azurerm_resource_group.network-rg.location\n  subnet_id           = azurerm_subnet.endpoint-subnet.id\n\n  private_service_connection {\n    name                           = \"kopicloudstoragedevwesta-psc\"\n    private_connection_resource_id = azurerm_storage_account.storage.id\n    is_manual_connection           = false\n    subresource_names              = [\"blob\"]\n  }\n}\n\n# DNS A 레코드 생성\nresource \"azurerm_private_dns_a_record\" \"dns_a\" {\n  name                = \"kopicloudstoragedevwesta\"\n  resource_group_name = data.azurerm_private_dns_zone.dns-zone.resource_group_name\n  zone_name = data.azurerm_private_dns_zone.dns-zone.name\n  ttl                 = 300\n  records             = [azurerm_private_endpoint.endpoint.private_service_connection.0.private_ip_address]\n}\n```\n\n# 3. 공개 액세스 해제\n\n기본적으로 이 코드는 공개 액세스가 있는 스토리지 계정을 생성합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_1.png)\n\n공개 액세스를 종료해야만 Private Endpoint를 구현하는 의미가 있습니다.\n\n따라서 \"azurerm_storage_account\" 리소스를 수정하여 public_network_access_enabled = false 라인을 추가하겠습니다.\n\n```js\n// 스토리지 계정 생성\nresource \"azurerm_storage_account\" \"storage\" {\n  name                = \"${lower(replace(var.company,\" \",\"-\"))}${var.app_name}${var.environment}${var.shortlocation}sta\"\n  resource_group_name = azurerm_resource_group.network-rg.name\n  location            = azurerm_resource_group.network-rg.location\n\n  account_kind             = \"StorageV2\"\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n\n  public_network_access_enabled = false\n}\n```\n\n<div class=\"content-ad\"></div>\n\n\"Terraform Apply\" 명령을 실행하여 공개 액세스가 비활성화되었음을 확인했습니다.\n\n![이미지](/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_2.png)\n\n# 4. 스토리지 계정 컨테이너 생성\n\n이제 다음 코드로 스토리지 계정 컨테이너를 생성할 차례입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n// 외부 저장소 계정용 Azure Storage 컨테이너 생성\nresource \"azurerm_storage_container\" \"external\" {\n  name                  = \"container\"\n  storage_account_name  = azurerm_storage_account.storage.name\n  container_access_type = \"private\"\n}\n```\n\n\"Terraform Apply\" 명령을 실행했는데 오류가 발생했습니다.\n\n# 4. 특정 네트워크에서 액세스 권한 활성화\n\n따라서 이 문제를 해결하기 위해 특정 네트워크에서의 공개 액세스를 허용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n그럼 \"azurerm_storage_account\" 리소스를 수정하여 public_network_access_enabled = true 라인을 추가하겠습니다.\n\n```js\n// 저장소 계정 생성\nresource \"azurerm_storage_account\" \"storage\" {\n  name                = \"${lower(replace(var.company,\" \",\"-\"))}${var.app_name}${var.environment}${var.shortlocation}sta\"\n  resource_group_name = azurerm_resource_group.network-rg.name\n  location            = azurerm_resource_group.network-rg.location\n  account_kind             = \"StorageV2\"\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n\n  public_network_access_enabled = true\n}\n```\n\n또한 지정된 네트워크에서 액세스 허용하도록 \"Azure Storage Account Network Rules\" 리소스를 생성해야 합니다; 이 경우 Storage Account Subnet에서 액세스를 허용할 것입니다.\n\n```js\n# Azure Storage Account Network Rules 생성\nresource \"azurerm_storage_account_network_rules\" \"rules\" {\n  storage_account_id = azurerm_storage_account.storage.id\n\n  default_action = \"Deny\"\n  virtual_network_subnet_ids = [ azurerm_subnet.endpoint-subnet.id ]\n  bypass         = [\"Metrics\", \"Logging\", \"AzureServices\"]\n}\n```\n\n<div class=\"content-ad\"></div>\n\n“Terraform Apply” 명령을 실행했을 때 다시 403 오류가 발생했어요.\n\n# 5. Azure Storage Account 네트워크 규칙 업데이트\n\n가시성을 위해 규칙을 변경하여 VM이 Terraform을 실행할 때 서브넷에서의 트래픽을 허용하도록 할게요. 우리는 데이터를 사용하여 서브넷 ID를 가져올 거에요.\n\n```js\n# 핵심 서브넷을 참조합니다\ndata \"azurerm_subnet\" \"core\" {\n  name                 = \"kopicloud-core-dev-we-subnet\"\n  virtual_network_name = \"kopicloud-core-dev-we-vnet\"\n  resource_group_name  = \"kopicloud-core-dev-we-rg\"\n}\n\n# Azure Storage Account 네트워크 규칙 생성\nresource \"azurerm_storage_account_network_rules\" \"rules\" {\n  storage_account_id = azurerm_storage_account.storage.id\n\n  default_action = \"Deny\"\n  virtual_network_subnet_ids = [ azurerm_subnet.endpoint-subnet.id, data.azurerm_subnet.core.id ]\n  bypass         = [\"Metrics\", \"Logging\", \"AzureServices\"]\n}\n```\n\n<div class=\"content-ad\"></div>\n\n\"Terraform Apply\" 명령을 실행했는데 다시 403 오류가 발생했어요.\n\n포털을 살펴보니 규칙이 구현되지 않았다는 것을 알았어요. 컨테이너가 이전에 실패했기 때문이에요.\n\n<img src=\"/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_3.png\"/>\n\n# 6. 코어와 스토리지 가상 네트워크 간 피어링 생성하기\n\n<div class=\"content-ad\"></div>\n\n그 후에, 어쩌면 두 가상 네트워크 사이에 피어링이 필요할지도 모른다고 생각했어요.\n\n그래서 \"peering.tf\" 파일을 만들고 이 코드를 추가했어요:\n\n```js\nlocals {\n  core_rg_name = \"kopicloud-core-dev-we-rg\"\n  core_vnet_name = \"kopicloud-core-dev-we-vnet\"\n}\n\n// Core VNET에 대한 참조\ndata \"azurerm_virtual_network\" \"core\" {\n  name                = local.core_vnet_name\n  resource_group_name = local.core_rg_name\n}\n\n// Core에서 스토리지로 피어링 생성\nresource \"azurerm_virtual_network_peering\" \"core-to-storage\" {\n  name                      = \"Core-to-Storage\"\n  resource_group_name       = local.core_rg_name\n  virtual_network_name      = local.core_vnet_name\n  remote_virtual_network_id = azurerm_virtual_network.network-vnet.id\n}\n\n// 스토리지에서 Core로 피어링 생성\nresource \"azurerm_virtual_network_peering\" \"storage-to-core\" {\n  name                      = \"Storage-to-Core\"\n  resource_group_name       = azurerm_resource_group.network-rg.name\n  virtual_network_name      = azurerm_virtual_network.network-vnet.name\n  remote_virtual_network_id = data.azurerm_virtual_network.core.id\n}\n```\n\n그리고 \"Terraform Apply\" 명령어를 실행했더니, 마침내 우리 코드가 잘 실행되고 있어요!\n\n<div class=\"content-ad\"></div>\n\n최종 네트워크 구성은 다음과 같아야 합니다:\n\n![이미지](/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_4.png)\n\n그리고 이것이 전부에요. 만약 이 이야기를 좋아하셨다면 👏을 눌러 서포트를 보여주세요. 읽어 주셔서 감사합니다!\n","ogImage":{"url":"/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_0.png"},"coverImage":"/assets/img/2024-06-19-HowtoTroubleshootanAzureStorageAccountwithaPrivateEndpointandaContainerwithTerraform_0.png","tag":["Tech"],"readingTime":10},{"title":"2024년에 사용할 수 있는 가장 유용한 Terraform 도구들","description":"","date":"2024-06-19 13:26","slug":"2024-06-19-MostUsefulTerraformToolstoUsein2024","content":"\n\n개발자로서 2년정도 경력이 있고, 많은 도구들이 갑자기 등장하는 것을 보았어요. 요즘 제품을 쉽게 만들기 때문만이 아니라, 전체 Terraform 개발 라이프사이클 내에서 해결해야 할 많은 문제들이 있기 때문이에요. 그리고 제품의 각 미세 카테고리에서 오픈 소스, 무료, 프리미엄, 또는 맞춤 요금제와 같이 수천 가지 선택지가 있는 걸 잊지 마세요.\n\n그래서, 여기 귀하는 위한 모든 것을 간단히 해주기 위해 발견하고 철저히 테스트한 최고의 해결책을 소개합니다:\n\n# 전통적인 데브옵스 엔지니어가 사용하는 도구\n\n![Most Useful Terraform Tools to Use in 2024](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_0.png)\n\n<div class=\"content-ad\"></div>\n\n먼저 할 일은 Infrastructure as Code (IaC)의 기본 형태로 돌아가서 엔지니어들이 일을 어떻게 하는지와 전체 라이프사이클이 얼마나 어려운지를 이해하는 것이었습니다.\n\n## 인프라 다이어그램:\n\n인프라 다이어그램은 종종 화이트보드에 그려졌습니다. Microsoft Visio나 Draw.io가 최고의 다이어그램 솔루션이었지만, 이 다이어그램들은 그저 다이어그램일 뿐이었고 종종 오래되어 있었습니다.\n\n## 코드 정의:\n\n<div class=\"content-ad\"></div>\n\n공학자들은 CLI(Command Line Interface)를 사용하여 코드를 정의했어요.\n\n## 버전 관리 및 배포:\n\n오픈 소스 인프라스트럭처 코드(IaC) 도구인 terraform과 같은 도구들이 인프라 버전 관리에 인기를 끌게 되었어요. 지금은 OpenTofu와 같은 것이 등장해서, 좀 다른 것 같아요. 모든 것은 Git을 통해 배포되었어요.\n\n## 인프라스트럭처 관리:\n\n<div class=\"content-ad\"></div>\n\n전체 인프라는 클라우드 제공 업체 콘솔을 통해 관리되었습니다. 클라우드 제공 업체 중심의 자원 관리가 일반적이었습니다.\n\n## 모니터링:\n\nDatadog와 같은 도구가 등장하기 전에는 홈메이드 제품이 초기에 인프라 모니터링에 사용되었습니다.\n\n## 보안 도구:\n\n<div class=\"content-ad\"></div>\n\n오픈 소스 보안 도구들은 여전히 널리 사용되고 있어요. Terrascan, Checkov, 그리고 OPA (Open Policy Agent)와 같은 도구들은 여전히 중요합니다.\n\n# Terraform을 위한 도구들\n\n![이미지](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_1.png)\n\n## TFLint: 코드 품질\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_2.png\" />\n\nTFLint는 Terraform을 위한 강력한 오픈 소스 린트 도구로 솟아납니다. 그 실력은 구문 오류부터 리소스 명명 규칙 및 사용되지 않는 변수까지 다양한 문제를 식별하는 데 있습니다. TFLint는 여러분의 도구 상자의 필수품으로, Terraform 코드가 높은 품질 기준을 준수하도록 보장합니다.\n\n## Terrascan: 보안 및 규정 준수\n\n<img src=\"/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_3.png\" />\n\n\n<div class=\"content-ad\"></div>\n\nTerrascan은 보안 및 규정 준수에 초점을 맞춘 정적 코드 분석기로 무대에 올라섭니다. 전통적인 린트 이상으로 나아가 구성 오류, 보안이 취약한 리소스 설정 및 정책 위반을 식별합니다. 보안이 최우선 사항인 만큼 Terrascan은 Terraform 인프라를 견고하게 만들어 주는 경계를 지킵니다.\n\n## Checkov: Hashicorp Sentinel 대체안\n\n![이미지](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_4.png)\n\nCheckov은 인프라 코드 파일을 세심하게 검사하는 보안 스캐너로 등장합니다. Checkov의 역할은 Terraform 코드를 면밀히 검사하여 보안 취약성과 정책 위반 사항을 발견하는 것입니다. 사이버 보안이 중요시되는 시대에, Checkov은 감시병으로 작용하여 귀하의 인프라가 견고하고 규정을 준수할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\n## 인프라코스트: 클라우드 비용\n\n![이미지](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_5.png)\n\n테라폼 배포 비용을 예측하는 것은 효과적인 예산 편성에 중요합니다. 인프라코스트는 오픈 소스 툴로, 정확한 비용 추정을 제공하여 이러한 요구를 지원합니다. 다양한 클라우드 공급업체와 온프레미스 인프라를 수용함으로써 인프라코스트는 인프라 요구 사항을 절충하지 않고 예산 제약 내에서 유지할 수 있도록 합니다.\n\n## 브레인보드: 테라폼 GUI\n\n<div class=\"content-ad\"></div>\n\n\n![Brainboard](https://miro.medium.com/v2/resize:fit:1400/1*F9cAb0NT-xSvFhnEXx9SiQ.gif)\n\nBrainboard은 클라우드 인프라 관리를 위한 최첨단 플랫폼으로 나타납니다. 그 강점은 복잡한 클라우드 아키텍처를 디자인, 배포 및 관리하는 직관적인 시각적 인터페이스를 제공하는 데 있습니다. 프로세스를 단순화하고 협업을 강화함으로써, Brainboard는 클라우드 인프라를 효율적이고 확장 가능하게 만들어 현대의 데브옵스 팀에게 꼭 필요한 도구가 됩니다.\n\n## Terragrunt: Module Management\n\n![Terragrunt](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_6.png)\n\n\n<div class=\"content-ad\"></div>\n\nTerragrunt은 Terraform 모듈의 조정 및 배포를 관리하는 도구입니다. 모듈 처리 기능 이상으로, Terragrunt은 Terraform 테스트를 용이하게하고 전체 개발 수명주기를 최적화하는 데 능숙합니다.\n\n## Terradozer: 사용되지 않는 인프라를 위한\n\n![Terradozer](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_7.png)\n\nTerradozer는 강력한 정리 도구로 등장하여 Terraform 상태 파일의 모든 리소스를 파괴하는 것을 목적으로 합니다. 이 기능은 사용되지 않는 인프라를 정리하고 효율적인 환경을 조성하는 데 매우 소중합니다.\n\n<div class=\"content-ad\"></div>\n\n## Eagle-Eye: 종속성 시각화\n\n![이미지](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_8.png)\n\n이글 아이(Eagle-Eye)란 복잡한 Terraform 구성을 탐색하는 것이 더 쉬워집니다. AWS를 위한 이 강력한 시각화 도구는 리소스 간 종속성에 대한 상세한 그래픽 표현을 생성하여 디버깅 및 복잡한 인프라 구성 이해에 중요한 통찰을 제공합니다.\n\n## DiagramGPT: D2 다이어그램\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_9.png)\n\nEraser.io의 DiagramGPT는 자연어 프롬프트를 사용하여 다이어그램을 만드는 혁신적인 도구로 빛을 발합니다. 고급 AI 기술을 활용하여 텍스트 설명에서 자세하고 정확한 다이어그램을 생성하는 과정을 간단화합니다. 이 도구는 생산성과 명확성을 향상시켜 전문가들이 문서 작성과 시각화 작업을 간소화하는 데 꼭 필요한 자산입니다.\n\n하나의 도구로는 모든 요구 사항이 충족되지 않지만, 특정 요구 사항마다 완벽한 도구가 있습니다. 현명하게 선택하고 도구 상자에 새로운 도구를 추가해야 하는지 알려주세요.","ogImage":{"url":"/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_0.png"},"coverImage":"/assets/img/2024-06-19-MostUsefulTerraformToolstoUsein2024_0.png","tag":["Tech"],"readingTime":5},{"title":"테라폼을 이용한 AWS 인프라 배포와 앤서블을 이용한 구성 설정","description":"","date":"2024-06-19 13:24","slug":"2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible","content":"\n# 소개:\n\n현재의 동적 클라우드 컴퓨팅 환경에서는 인프라 프로비저닝을 자동화하는 것이 확장성, 신뢰성, 및 비용 효율성을 원하는 기관들에게 필수적입니다. Terraform은 오픈 소스 인프라 코드 도구로, 선언적 구성 파일을 사용하여 인프라를 정의하고 관리할 수 있도록 팀에게 권한을 부여합니다. 이 블로그에서는 Terraform이 AWS 인프라 구성 요소의 배포를 체계적이고 효율적으로 간소화하는 방법을 탐색해보겠습니다.\n\n# 프로젝트 개요:\n\n우리 Terraform 프로젝트는 가상 사설 클라우드(VPC), 보안 그룹, Amazon Machine Image(AMI), Elastic Block Store(EBS) 볼륨, 및 EC2 인스턴스로 구성된 AWS 인프라 스택의 배포를 자동화하는 데 초점을 맞춥니다. Terraform의 모듈식이자 반복 가능한 구성을 활용하여 환경 간 일관성과 신뢰성을 보장하고, 수동 개입과 인적 오류를 최소화합니다.\n\n<div class=\"content-ad\"></div>\n\n# 상세 단계:\n\n## 1. AWS CLI 설치하기:\n\n- 먼저 로컬 컴퓨터에 AWS CLI를 설치해야 합니다.\n- 다음 링크를 따라 AWS CLI를 설치하세요:\n\n![AWS CLI 설치 링크](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_0.png)\n\n<div class=\"content-ad\"></div>\n\n- AWS CLI를 이곳에서 다운로드하여 계정에 구성하세요.\n- 또는 Linux 버전을 선택하여 설치할 수도 있습니다.\n- 이를 위해 계정에 IAM 사용자를 만들어야 합니다.\n\n![이미지1](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_1.png)\n\n![이미지2](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_2.png)\n\n- 현재는 관리자 액세스를 제공하지만 좋은 실천 방법은 아닙니다.\n\n<div class=\"content-ad\"></div>\n\n![2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_3.png](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_3.png)\n\n![2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_4.png](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_4.png)\n\n- 보안 자격 증명에서 \"액세스 키 생성\" 옵션을 클릭하여 액세스 키 및 비밀 키를 생성하고 둘 다 복사합니다.\n\n![2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_5.png](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_5.png)\n\n<div class=\"content-ad\"></div>\n\n- Access Key ID에 액세스 키를 붙여 넣고, Secret Access Key ID에 비밀 키를 입력해주세요. 그리고 설정하고 싶은 기본 지역도 지정해주세요.\n\n## 2. Terraform 초기화하기.\n\n- main.tf 파일을 생성하고, 제공자(provider) 구성을 거기에 입력해주세요.\n\n```js\n# main.tf\n\n# Provider configuration\nprovider \"aws\" {\n  region = \"us-west-1\"\n}\n```\n\n<div class=\"content-ad\"></div>\n\n- 그럼 아래 명령어를 입력해주세요.\n\n```js\nterraform.exe init\n```\n\n## 3. VPC 생성:\n\n우리는 VPC 구성을 정의합니다. CIDR 블록을 포함하여 네트워크 구조의 기반을 마련합니다. 이는 AWS 환경 내에서 격리되고 안전한 통신을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n- 이 프로젝트 전체에 대해 동일한 main.tf 파일을 사용할 것입니다.\n\n```js\n# main.tf\n\nresource \"aws_vpc\" \"vpc\"{\n        cidr_block = \"192.168.0.0/16\"\n\n        tags = {\n                Name = \"Terraform_VPC\"\n        }\n}\n```\n\n## 4. 서브넷 생성:\n\n- 이제 서브넷 구성을 정의합니다. 이 구성에는 공용 서브넷과 사설 서브넷이 모두 포함됩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# main.tf\n\n변수 \"aws_azs\"를 정의합니다. 여기에는 public 서브넷의 가용 영역이 저장됩니다. 그리고 public 서브넷의 CIDR 블록을 저장하는 다른 변수를 만듭니다.\n\n그런 다음 각 가용 영역에 2개의 public 서브넷을 만듭니다.\n- length 함수는 주어진 목록, 맵 또는 문자열의 길이를 결정합니다. 목록이나 맵이 주어지면 해당 컬렉션의 요소 수가 결과로 나옵니다.\n- element() 함수는 목록에서 특정 인덱스의 요소를 검색합니다. 사용 사례: 목록에서 특정 요소에 액세스하는 것은 해당 인덱스를 기준으로 목록에서 특정 리소스나 매개변수를 선택하려고 할 때 유용합니다.\n- count.index 객체는 count 내의 현재 인스턴스의 인덱스를 나타냅니다. 인덱스는 0부터 시작하며, count가 4인 리소스가 있으면 count.index 객체는 0, 1, 2 및 3이 됩니다.\n- 그런 다음 private 서브넷을 만드는 동일한 단계를 반복합니다.\n\n## 5. Internet-Gateway 및 Route-table 생성:\n\n이제 public 서브넷에 인터넷에 연결할 Internet-Gateway를 만들 것입니다. 이를 위해 라우팅 테이블도 생성하여 경로를 만들어야 합니다.\n```\n\n<div class=\"content-ad\"></div>\n\n```js\n# main.tf\n\nresource \"aws_internet_gateway\" \"igw\" {\n        vpc_id = aws_vpc.vpc.id\n        tags = {\n                Name = \"terrform-vpc-igw\"\n        }\n}\n\nresource \"aws_route_table\" \"second-rt\" {\n        vpc_id = aws_vpc.vpc.id\n        route {\n                cidr_block = \"0.0.0.0/0\"\n                gateway_id = aws_internet_gateway.igw.id\n        }\n\n        tags = {\n                Name = \"Public-route-table\"\n        }\n}\n\nresource \"aws_route_table_association\" \"public-subnets-asso\" {\n        count = length(var.public_subnet_cidrs)\n        subnet_id = element(aws_subnet.public_subnets[*].id, count.index)\n        route_table_id = aws_route_table.second-rt.id\n}\n```\n\n- 여기서, 먼저 VPC 내에서 인터넷 게이트웨이를 생성합니다.\n- 그런 다음 인터넷 게이트웨이를 통해 안전한 인터넷 연결을 가능하게 하는 라우트를 생성하는 라우트 테이블을 만듭니다.\n- 그런 다음 이 라우트 테이블과 공용 서브넷을 연결해야합니다.\n\n## 6. 보안 그룹 구성:\n\nEC2 인스턴스로의 들어오고 나가는 트래픽을 제어하기 위해 보안 그룹 규칙을 지정합니다. 이렇게 하면 사전 정의된 규칙 세트에 따라 액세스를 제한하여 네트워크 보안이 강화되며 잠재적인 보안 취약점을 완화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nresource \"aws_security_group\" \"sg\" {\n    name        = \"terraform_sg\"\n    description = \"This security group is for terraform practice\"\n    vpc_id      = aws_vpc.vpc.id\n\n    tags = {\n        Name = \"terraform_vg\"\n    }\n}\n\nresource \"aws_vpc_security_group_ingress_rule\" \"sg_in_rule\" {\n    security_group_id = aws_security_group.sg.id\n    cidr_ipv4        = \"0.0.0.0/0\"\n    from_port        = 80\n    ip_protocol      = \"tcp\"\n    to_port          = 80\n}\n\nresource \"aws_vpc_security_group_ingress_rule\" \"sg_in_rule2\" {\n    security_group_id = aws_security_group.sg.id\n    cidr_ipv4        = \"0.0.0.0/0\"\n    from_port        = 22\n    ip_protocol      = \"tcp\"\n    to_port          = 22\n}\n\nresource \"aws_vpc_security_group_egress_rule\" \"sg_eg_rule\" {\n    security_group_id = aws_security_group.sg.id\n    cidr_ipv4         = \"0.0.0.0/0\"\n    ip_protocol       = \"-1\" # semantically equivalent to all ports\n}\n```\n\n- 먼저 VPC 내에서 보안 그룹을 만듭니다.\n- 그런 다음 보안 그룹에 대한 인바운드 규칙을 정의하여 클라이언트가 포트 80 및 22에 도달할 수 있도록 허용합니다.\n- 그런 다음 아무 포트에서 어디로든 연결을 허용하는 보안 그룹에 대한 아웃바운드 규칙을 정의합니다.\n\n## 7. 데이터 소스를 활용한 AMI 구성:\n\nTerraform의 데이터 소스를 활용하여 지정된 필터에 따라 기존 AWS AMI에 대한 정보를 가져올 수 있습니다. 이러한 필터는 지역, 운영 체제 및 아키텍처와 같은 미리 정의된 것으로 구성됩니다. 이는 EC2 인스턴스에 가장 적합한 AMI를 동적으로 선택함으로써 배포 간의 호환성 및 일관성을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndata \"aws_ami\" \"rhel9\" {\n        most_recent = true\n\n        owners = [\"309956199498\"] // Red Hat의 계정 ID.\n\n        filter {\n                name   = \"architecture\"\n                values = [\"x86_64\"]\n        }\n\n        filter {\n                name   = \"root-device-type\"\n                values = [\"ebs\"]\n        }\n\n        filter {\n                name   = \"virtualization-type\"\n                values = [\"hvm\"]\n        }\n\n        filter {\n                name   = \"name\"\n                values = [\"RHEL-9.*\"]\n        }\n}\n```\n\n- 데이터 소스는 외부 시스템이나 기존 리소스에서 정보를 조회하고 해당 정보를 Terraform 구성에 통합하는 데 사용됩니다.\n\n## 8. EC2 인스턴스 프로비저닝:\n\n인스턴스 유형, 키페어, 보안 그룹을 포함한 EC2 인스턴스 구성을 정의합니다. Terraform은 VPC 내에서 EC2 인스턴스를 프로비저닝하여 지정된 구성을 준수하면서 연결성과 리소스 격리를 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n```json\n파일 이름 : main.tf\n```\n\n## 3. EC2 인스턴스 생성 :\n\n이 코드는 Terraform을 사용하여 AWS EC2 인스턴스를 생성하는 예시입니다. 우리는 AMI ID, 인스턴스 유형, 키 이름, 서브넷 ID, 보안 그룹 ID 등을 정의하고 있습니다. 이를 통해 인프라스트럭처 스택에 EC2 인스턴스를 통합할 수 있습니다. 태그를 지정하여 리소스를 식별할 수도 있습니다.\n\n## 테이블\n\n| 제목                | 설명                            |\n| ------------------- | ------------------------------- |\n| AMI                 | data.aws_ami.rhel9.id           |\n| 유형                | \"t2.micro\"                      |\n| 키 이름             | \"IAM_California\"                |\n| 서브넷 ID           | aws_subnet.public_subnets[0].id |\n| VPC 보안 그룹 ID    | aws_security_group.sg.id        |\n| 퍼블릭 IP 주소 연결 | true                            |\n\n태그 :\n\n<div class=\"content-ad\"></div>\n\n- 이제 새로운 EBS 볼륨을 생성했습니다. 중요한 점은 ec2 인스턴스와 ebs 볼륨이 동일한 가용 영역에 있어야 한다는 것입니다. 그렇지 않으면 서로 연결할 수 없습니다.\n\n## 10. EBS 볼륨 연결\n\n이제 다음 단계는 새로 생성한 ebs 볼륨을 ec2 인스턴스에 연결하는 것입니다.\n\n```js\nresource \"aws_volume_attachment\" \"ebs_attach\" {\n        device_name = \"/dev/xvdb\"\n        volume_id = aws_ebs_volume.EBS.id\n        instance_id = aws_instance.ec2-1.id\n}\n```\n\n<div class=\"content-ad\"></div>\n\n## 11. 공용 IP 주소 출력:\n\n마지막으로, 테라폼의 출력 기능을 사용하여 프로비저닝된 EC2 인스턴스의 공용 IP 주소를 표시합니다. 이는 관리자와 최종 사용자 모두가 인스턴스에 쉽게 액세스하고 관리할 수 있도록 합니다.\n\n```js\noutput \"ec2_instance_ip\" {\n        value = aws_instance.ec2-1.public_ip\n}\n```\n\n## 12. 테라폼 파일 적용하기\n\n<div class=\"content-ad\"></div>\n\n지금은 여태까지 AWS 인프라를 만든 main.tf 파일을 적용할 것입니다.\n\n```js\nterraform.exe apply\n```\n\n![AWS Infrastructure Deployment](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_6.png)\n\n- 먼저 전체 계획을 알려줍니다. 그렇지 않으면 계획을 보는 별도의 명령어가 있습니다. 즉, terraform.exe plan\n- 계획을 알려준 후에는 앞으로 진행하고 AWS 클라우드에 적용할 것인지 물어봅니다.\n\n<div class=\"content-ad\"></div>\n\n![AWS Infrastructure Deployment with Terraform and Configuration with Ansible 7](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_7.png)\n\n![AWS Infrastructure Deployment with Terraform and Configuration with Ansible 8](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_8.png)\n\n# Let’s Go on our AWS Console to verify this deployment.\n\n## VPC ARCHITECTURE:\n\n<div class=\"content-ad\"></div>\n\n![AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_9](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_9.png)\n\n![AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_10](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_10.png)\n\n## SECURITY GROUP :\n\n![AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_11](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_11.png)\n\n<div class=\"content-ad\"></div>\n\n## EC2-INSTANCE :\n\n![EC2-INSTANCE](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_12.png)\n\n# Ansible Configuration Management:\n\n인프라가 프로비저닝된 후에는 프로비저닝된 인스턴스 내에서 Ansible을 사용하여 구성 관리 작업으로 신속하게 전환합니다. Ansible은 우리에게 idempotent playbooks 및 모듈을 사용하여 복잡한 구성 작업을 자동화할 수 있는 기능을 제공하여 효율적이고 확장 가능한 인프라 관리를 가능하게 합니다.\n\n<div class=\"content-ad\"></div>\n\n저희 프로젝트는 Terraform과 Ansible을 완벽하게 통합하여 일관된 배포 파이프라인을 구축합니다. Terraform은 인프라 스택의 기반을 마련하는 데 도움을 주고, Ansible은 프로비전된 인스턴스를 구성하여 그들이 그 의도한 목적을 위해 완전히 기능하고 최적화되도록 보장합니다.\n\n## EC2-인스턴스 구성을 위한 Ansible의 실용적인 단계:\n\n- ANSIBLE 인벤토리:\n\n```js\nvim / etc / ansible / hosts;\n```\n\n<div class=\"content-ad\"></div>\n\n\\<img src=\"/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_13.png\" />\n\n2. REQUIRED MODULES 설치하기.\n\n```js\nansible-galaxy collections install community.general\nansible-galaxy collections install posix\n```\n\n3. ANSIBLE-PLAYBOOK:\n\n<div class=\"content-ad\"></div>\n\n```js\nvim <file-name>.yml\n```\n\n![AWS Infrastructure Deployment with Terraform and Configuration with Ansible](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_14.png)\n\n![AWS Infrastructure Deployment with Terraform and Configuration with Ansible](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_15.png)\n\n![AWS Infrastructure Deployment with Terraform and Configuration with Ansible](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_16.png)\n\n<div class=\"content-ad\"></div>\n\n```yaml\n- hosts: all\n  become: yes\n  tasks:\n    - name: \"파티션 생성\"\n      community.general.parted:\n        device: \"/dev/xvdb\"\n        name: \"GPT\"\n        number: 1\n        part_end: \"1GiB\"\n        fs_type: \"ext4\"\n        state: present\n        label: \"gpt\"\n        unit: GiB\n\n    - name: \"중요 명령어 실행\"\n      command:\n        cmd: \"udevadm settle\"\n      register: cmd\n\n    - debug:\n        var: cmd\n\n    - command:\n        cmd: \"lsblk\"\n      register: cmd2\n\n    - debug:\n        var: cmd2\n\n    - name: \"파티션 포맷\"\n      community.general.filesystem:\n        fstype: ext4\n        dev: \"/dev/xvdb1\"\n\n    - name: \"웹 서버 설치\"\n      package:\n        name: \"httpd\"\n        state: present\n\n    - name: \"마운트된 볼륨과 연결\"\n      ansible.posix.mount:\n        path: \"/var/www/html\"\n        src: \"/dev/xvdb1\"\n        state: mounted\n        fstype: ext4\n\n    - name: \"데몬 다시로드\"\n      command:\n        cmd: \"systemctl daemon-reload\"\n\n    - command:\n        cmd: \"lsblk\"\n      register: cmd3\n\n    - debug:\n        var: cmd3\n\n    - name: \"인덱스 파일을 웹 서버로 복사\"\n      ansible.builtin.copy:\n        src: \"index.html\"\n        dest: \"/var/www/html/index.html\"\n\n    - name: \"서버 재시작\"\n      service:\n        name: \"httpd\"\n        state: \"started\"\n```\n\n## 이 Playbook은 다음을 수행할 수 있습니다:\n\n- 우리가 연결한 볼륨인 /dev/xvdb에 파티션 생성.\n- ext4 유형으로 새로 생성된 파티션 포맷.\n- 시스템에 아파치 웹 서버 설치.\n- 아파치 루트 문서인 /var/www/html에 파티션을 마운트.\n- 로컬 시스템의 인덱스 파일을 대상 시스템의 루트 문서에 복사.\n- 아파치 서비스 시작.\n\n## ANSIBLE PLAYBOOK 실행하기\n\n<div class=\"content-ad\"></div>\n\n```js\nansible-playbook <file-name>.yml\n```\n\n![Image](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_17.png)\n\n![Image](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_18.png)\n\n## 웹 서버가 성공적으로 시작되었는지 확인해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n![AWS Infrastructure Deployment with Terraform and Configuration with Ansible](/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_19.png)\n\n# 결론:\n\n요약하면, Terraform과 Ansible의 통합은 AWS 인프라 자동화에서 강력한 패러다임 변화를 나타냅니다. 인프라 프로비저닝에 Terraform을 활용하고 구성 관리에는 Ansible을 활용함으로써, 조직은 클라우드 배포에서 전례 없는 민첩성, 확장성 및 신뢰성을 달성할 수 있습니다. 이 통합 접근 방식을 통해 팀은 DevOps 성숙도로 나아가는 여정을 가속화하고 클라우드 자동화의 모든 잠재력을 발휘할 수 있습니다.\n\n오늘 Terraform과 Ansible의 힘을 받아 AWS 인프라 배포 및 구성 워크플로를 혁신하세요! 🚀🔧\n","ogImage":{"url":"/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_0.png"},"coverImage":"/assets/img/2024-06-19-AWSInfrastructureDeploymentwithTerraformandConfigurationwithAnsible_0.png","tag":["Tech"],"readingTime":13},{"title":"랜딩 존 배포 Google Cloud 채택 시리즈","description":"","date":"2024-06-19 13:19","slug":"2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries","content":"\n\n구글 클라우드 채택 및 이전 시리즈에 다시 오신 것을 환영합니다. 이 글이 좀 늦게 나온 점 죄송합니다. 기술적인 문제가 있어서요! (나중에 더 설명하겠죠.)\n\n이전에는 핵심 LZ 팀을 구성하는 방법, 필요한 지원을 받는 방법, 실행해야 할 워크샵, 그리고 설계를 기록하고 문서화하는 방법을 다뤘습니다.\n\n오늘은 재미있는 부분에 진입합니다! 이제 실제로 LZ를 배포해 봅시다! \n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_0.png)\n\n<div class=\"content-ad\"></div>\n\n# LZ를 배포하는 네 가지 방법\n\nGoogle 클라우드 랜딩 존을 배포하는 네 가지 접근 방법이 있습니다. 이러한 방법들은 다음과 같습니다:\n\n- “클릭하고 배포” — Google 클라우드 콘솔의 안내에 따라 클릭 단계별 프로세스를 따르는 방식으로, Google 클라우드 Foundation 설정 체크리스트에서 지원합니다. 버튼을 누르면 실제 배포가 수행됩니다.\n- “클릭하고 다운로드하고 배포” — 여기서는 콘솔에서 UI 기반 프로세스를 계속 따릅니다. 그러나 콘솔에서 배포를 실행하는 대신, 프로세스에 의해 생성된 Terraform 구성을 다운로드합니다. 이를 통해 배포를 별도로 저장, 조정 및 실행할 수 있습니다. 또한 배포를 취소할 수도 있습니다!\n- Cloud Foundation Fabric FAST — 사전에 Fabric 참조 블루프린트 세트를 사전 집계함으로써 구축된 기업용 랜딩 존 블루프린트 시범 구현입니다. 이는 Terraform 기반의 솔루션으로 처음부터 GCP LZ를 부트스트래핑하고 빌드하는 것입니다.\n- 직접 Terraform 구축 — 가능하긴 합니다. 그러나 추천하지는 않습니다. Google은 처음부터 FAST를 구축하고 다양한 기업들로부터 많은 피드백을 모았습니다. 그래서 이 옵션에 대해 더 이상 다루지 않겠습니다.\n\n# 사전 요구사항\n\n<div class=\"content-ad\"></div>\n\n어떤 방식을 선택하든, LZ 설계 단계를 완료하는 것이 중요합니다. 해야 할 결정 사항이 많이 있습니다.\n\n# 1 — 클릭 옵스 및 배포\n\n이 방법은 Google Cloud 콘솔을 통해 진행하는 클릭 단위 안내 프로세스를 사용합니다.\n\n## 누구를 위한 것인가?\n\n<div class=\"content-ad\"></div>\n\n이는 매우 작은 조직 및 매우 작은 Google Cloud 플랫폼 팀을 대상으로하며, Terraform 기술이 거의나 전혀 없는 경우가 많습니다.\n\n## 장점\n\n- 전반적인 프로세스가 매우 빠릅니다. Google 조직 및 랜딩 존을 구성하고 배포하는 데 1시간 또는 2시간이면 충분합니다.\n- Terraform 기술이 필요하지 않습니다. 전체 프로세스를 Google Cloud 콘솔을 통해 실행할 수 있습니다.\n- 제한된 구성 옵션을 제공하여 간단하고 쉽게 따르기 쉽습니다.\n\n## 단점\n\n<div class=\"content-ad\"></div>\n\n- 최종 LZ의 제한된 구성 가능성.\n- 반복성 없음.\n- 자동화된 CI/CD 파이프라인, 테넌트 팩토리 또는 프로젝트 팩토리 생성 불가.\n\n## 단계 요약\n\n- 조직 설정 — 조직 생성; 조직에 연결된 클라우드 ID 테넌트 설정; 도메인 확인; 및 수퍼 관리자 계정 설정.\n- 클라우드 ID의 사용자 및 그룹 구성 — GCDS를 사용한 동기화 포함; 필요한 경우 SSO 설정; 관리자 그룹 설정 (예: 조직, 결제, 네트워크, 보안, 로깅).\n- IAM을 사용한 관리적 액세스 설정 — 이전에 설정한 그룹에 사용자를 매핑; Google Cloud IAM 역할을 그룹에 할당.\n- 결제 설정 — 결제 계정; 예산 및 결제 경고; 결제 내보내기.\n- 리소스 계층 구조 및 액세스 생성 — 폴더 및 공통 프로젝트를 포함한 초기 리소스 계층 구조.\n- BigQuery로의 로깅 중앙화.\n- 네트워킹 — 공유 VPC 배포; 하이브리드 연결 구성; 초기 방화벽 구성; 출발 라우트 및 NAT.\n- 하이브리드 연결\n- 모니터링 — 중앙 집중식 클라우드 모니터링 구성.\n- 보안 — 조직 정책 구성; SCC 대시보드 활성화.\n- 지원 — 기본 또는 프리미엄 지원과 같은 선호하는 지원 옵션 선택.\n\n실제 배포 단계는 설정의 8단계 이후에만 발생합니다. 이 지점까지 모든 것은 적용할 구성입니다.\n\n<div class=\"content-ad\"></div>\n\n세부 단계를 자세히 살펴보기 전에 먼저 Google Cloud 설정 체크리스트를 시작해 보세요. 이 체크리스트는 프로세스를 완료하는 데 단계별 지침을 제공합니다. 체크리스트의 각 번호가 매겨진 항목은 더 많은 세부 정보를 보여줍니다.\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_1.png)\n\n## 1 — 조직 설정\n\n우선 Google Cloud Identity를 설정해야 하며(이미 Cloud Identity 또는 Google Workspace 고객이 아닌 경우), 그런 다음 Cloud Identity 계정을 Google Cloud 조직에 연결해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n아직 구글 클라우드 아이덴티티 고객이 아니라면, 클라우드 아이덴티티 가입 페이지를 열어 진행할 것입니다. 클라우드 아이덴티티에는 무료 및 프리미엄 버전이 있지만, 이 프로세스에서는 무료 티어를 사용하는 방법을 안내해 드릴게요.\n\n여기 유용한 정보가 있어요! 저와 같이 이미 클라우드 아이덴티티 계정을 보유하고 구글 클라우드 조직을 만든 경우를 상상해봅시다. 하지만 이 프로세스를 실험하기 위해 새로운 클라우드 아이덴티티 계정과 별도의 조직을 만들고 싶을 수 있습니다. 그렇다면 서브도메인을 사용하여 그것을 할 수 있어요! 예를 들어, 저는 이미 구글 클라우드 조직으로 just2good.co.uk 도메인을 사용하고 있습니다. 그러나 데모 목적으로 gcp-demos.just2good.co.uk 서브도메인을 방금 만들었어요.\n\n클라우드 아이덴티티 가입은 다음과 같이 보이는 화면에서 시작됩니다:\n\n![클라우드 아이덴티티 가입 페이지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_2.png)\n\n<div class=\"content-ad\"></div>\n\n그럼 비즈니스 도메인 이름을 지정해야 합니다. 이것은 우리의 Google Cloud 조직 이름이 될 것입니다.\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_3.png)\n\n그런 다음 도메인 이름을 입력합니다:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_4.png)\n\n<div class=\"content-ad\"></div>\n\n이제 슈퍼 관리자 계정 세부 정보를 입력해야 합니다. 이 이메일 주소는 Google Cloud Identity의 슈퍼-관리자가 될 것입니다. Google Cloud Identity Admin 콘솔에 로그인하는 데 사용할 이메일 주소입니다. (Google Cloud 콘솔과는 구별되어야 합니다.) 방금 전에 지정한 비즈니스 도메인과 관련된 이메일이어야 합니다. 예를 들어: mydomain.com의 Bob이 슈퍼 관리자라면, super-bob@mydomain.com과 같은 주소를 사용하는 것이 좋습니다.\n\n참고: 이는 강력한 계정으로, Google Cloud 조직과 관련된 이메일 주소나 그룹과 완전히 분리되어 있습니다.\n\nCloud Identity Admin 콘솔에 로그인하십시오:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_5.png)\n\n<div class=\"content-ad\"></div>\n\n이제 도메인 이름을 확인해야 합니다.\n\n![도메인 확인](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_6.png)\n\n도메인 확인 프로세스는 빠르고 쉽습니다. 일반적으로 구글에서 제공하는 값을 사용하여 DNS TXT 레코드를 생성하여 이 작업을 수행합니다. Admin Console에서 프로세스를 안내해줍니다.\n\n도메인이 확인되면 Google Cloud 조직 리소스가 자동으로 생성됩니다. 이는 Google Cloud의 최상위 조직입니다.\n\n<div class=\"content-ad\"></div>\n\n어드민 콘솔에서는 사용자를 만드는 것을 제안합니다. 그러나 Cloud Identity 어드민 콘솔이 아닌 Google Cloud 콘솔에서 진행하려고 합니다. 그래서 \"Google Cloud 콘솔에서 설정\" 링크를 클릭해주세요.\n\n![Google Cloud 콘솔에서 설정](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_7.png)\n\n이제 Google Cloud 콘솔에서 Cloud Foundation 설정 체크리스트를 계속 진행할 수 있습니다.\n\n![Google Cloud 콘솔에서 설정](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_8.png)\n\n<div class=\"content-ad\"></div>\n\n그러나 현재 이 단계에서 - Cloud Identity 관리자 콘솔에서 - 제안드리는 것이 있습니다:\n\n- 하나 또는 두 개의 추가 수퍼 관리자 계정을 추가하는 것. (만일 유일한 수퍼 관리자가 거대한 구덩이로 떨어진다면 어떻게 될까요?)\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_9.png)\n\n- 수퍼 관리자를 위해 이중 인증(MFA) 설정.\n- 계정 복구 설정.\n- 암호 만료를 포함한 암호 정책 정의.\n\n<div class=\"content-ad\"></div>\n\n1단계가 완료되었습니다!\n\n## 2 — 클라우드 아이덴티티에서 사용자 및 그룹 설정\n\n여기서 사용자 그룹을 생성하고 이러한 그룹에 구성원을 추가합니다. 체크리스트의 이 단계에서는 클라우드 설정의 모든 단계에 참여할 사용자를 추가하는 것이 좋습니다.\n\n구글 클라우드 설정으로 돌아가 Step 2로 이동할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_10.png\" />\n\n클라우드 ID 관리자 콘솔을 통해 슈퍼 관리자가 관리 그룹을 프로비저닝하고 초기 사용자를 설정할 수 있습니다. 다음과 같은 방법으로 할 수 있습니다:\n\n- 클라우드 ID 관리자 콘솔에서 그룹(및 사용자)을 수동으로 추가하는 것.\n- 기존 LDAP 또는 Active Directory (AD) 시스템에서 ID를 복제하기 위해 Google Cloud Directory Sync (GCDS)를 설정하는 것. 이 무료 도구는 사용자 및 그룹을 Google Cloud로 단방향 동기화합니다. 기존 LDAP/AD 시스템이 여전히 원본 소스로 유지됩니다. 시리즈 이전 기사에서 더 자세히 설명하겠습니다.\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_11.png\" />\n\n<div class=\"content-ad\"></div>\n\n- 그러나 가장 쉬운 방법은 Google Cloud Setup의 \"모든 그룹 생성\" 버튼을 사용하여 그룹을 만드는 것입니다. 이렇게 하면 클라우드 ID에서 권장하는 그룹이 자동으로 할당됩니다.\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_12.png)\n\n이 단계를 완료하면 그룹이 할당되어 Google 관리자 콘솔에서 볼 수 있습니다.\n\n이제이 그룹에 사용자(멤버)를 추가할 수 있습니다. \"Google 관리자 콘솔로 이동\"을 클릭하세요:\n\n<div class=\"content-ad\"></div>\n\n\n![그룹에 사용자 추가](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_13.png)\n\n그런 다음 이 그룹에 사용자를 추가하세요. 몇 가지 샘플 사용자를 만들어 두었습니다:\n\n![샘플 사용자](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_14.png)\n\n이 사용자들은 자동 이메일을 받게 됩니다. 물론... 유효한 메일함이 있는 경우에만요! (이와 같은 데모용으로, 일반적으로 *@my-domain 전달 규칙을 설정해 유효한 이메일 계정으로 전달하곤 합니다.)\n\n\n<div class=\"content-ad\"></div>\n\n다음으로, 해당 그룹에 그들을 추가하겠습니다:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_15.png)\n\n이제, 구글 클라우드 콘솔로 돌아가주세요. 그룹에 멤버가 추가된 것을 확인할 수 있을 거에요:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_16.png)\n\n<div class=\"content-ad\"></div>\n\n이제 \"관리 액세스로 계속\"을 클릭할 수 있습니다.\n\n## 3 - 관리 액세스\n\n다음으로, 이전 단계에서 생성한 각 그룹에 Google Cloud IAM 역할을 할당합니다. 이는 클라우드 ID 그룹에 적절한 Google Cloud 권한을 부여합니다.\n\n구글 클라우드 설정은 이전에 생성한 각 그룹에 할당될 역할을 제안할 것입니다. \"저장 및 액세스 권한 부여\"를 클릭하여 진행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_17.png)\n\n## 4 — 청구\n\n이 단계에서는 청구 계정을 생성하고(이미 보유하고 있지 않은 경우) 새 Google Cloud 조직과 연결합니다. 예산 설정, 청구 알림 및 청구 내보내기를 선택적으로 구성할 수도 있습니다.\n\n청구 계정은 소비한 모든 Google Cloud 자원을 지불하는 데 사용됩니다. 자원 소비(및 비용)은 프로젝트 수준에서 누적되며, 각 프로젝트는 청구 계정과 연결됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n아래는 해당 데모를 위해 사용할 기본 청구 계정 유형인 온라인(\"셀프 서비스\") 유형으로 계속하겠습니다. 자격을 갖춘 조직은 나중에 청구서 청구 계정으로 변경할 수 있습니다.\n\n\"온라인 청구 계정\"을 선택한 후 \"계속\"을 클릭한 다음 \"청구 계정 생성\"을 클릭하세요:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_19.png)\n\n<div class=\"content-ad\"></div>\n\n청구 계정 설정을 완료하려면 결제 카드 정보를 입력해야 합니다. 걱정하지 마세요. 아직 요금이 청구되지는 않을 거에요. 참고로 Google Cloud 초기 설정에는 300달러의 무료 크레딧이 제공됩니다.\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_20.png)\n\n\"내 청구 계정\"을 클릭하면 예산 및 청구 알림을 설정할 수 있습니다. (나중에도 언제든지 설정할 수 있어요.)\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_21.png)\n\n<div class=\"content-ad\"></div>\n\n예산 및 알림을 클릭한 후 예산 생성을 선택하세요. 예산 이름을 지정하고 시간 범위를 설정하고 적용할 프로젝트를 선택하세요 (\"모두\" 프로젝트가 기본 설정입니다).\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_22.png)\n\n다음으로 경고를 발생시킬 임계값과 경고를 전송할 위치를 지정합니다. 이 예에서는 청구 관리자 그룹에 이메일을 보낼 것이지만, Pub/Sub 주제에 쓰는 것과 같이 더 복잡한 작업도 수행할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_23.png)\n\n<div class=\"content-ad\"></div>\n\n## 5 - 리소스 계층구조와 액세스\n\n이제 이른바 초기 설계 결정을 활용하기 시작하는 단계입니다. 여기서는 조직 리소스 계층구조를 설정합니다.\n\n이 단계에서는 계층구조 단계에서 여러 프로젝트를 생성하기 때문에 청구 계정과 연결된 프로젝트 할당량을 증가 요청해야 할 수도 있습니다. 클라우드 설정 단계에서 할당량 요청 방법을 안내해줍니다. 할당량 요청을 수행하는 계정이 유효한 이메일 주소를 가지고 있는지 확인해주세요!\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_24.png)\n\n<div class=\"content-ad\"></div>\n\n한동안 이 기사를 완성하는 데 방해가 된 것은 할당량 요청 단계였어요. Google은 2영업일 이내에 요청에 대응해야 한다고 했지만 작성 시점에서 할당량 상승을 처리하는 구글 프로세스가 조금 문제가 있었네요. 그래서 수동 개입이 필요했죠! 구글 친구들이 이 문제가 곧 해결될 것이라고 말해 주었어요.\n\n그럼 'Cloud Setup'으로 돌아가볼게요… 우리는 네 가지 프리셋 계층 청사진 중 하나를 선택할 수 있어요.\n\n![Image](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_25.png)\n\n목록에서 계층을 선택하면 콘솔에서 생성된 계층이 어떻게 보일지 미리 보여 줍니다. 함께 비교해 볼까요…\n\n<div class=\"content-ad\"></div>\n\n간단하고 환경 중심의 계층 구조입니다:\n\n이 계층 구조는 매우 소규모 조직을 운영할 때 좋습니다. 아마 개발자 소수만 있을 것입니다.\n\n```js\nOrg/\n├── Common/\n│   ├── vpc-host-prod 📦\n│   ├── vpc-host-nonprod 📦\n│   ├── logging 📦\n│   ├── monitoring-prod 📦\n│   ├── monitoring-nonprod 📦\n│   └── monitoring-dev 📦\n├── Prod/\n│   ├── app1-prod-svc 📦\n│   └── app2-prod-svc 📦\n├── Nonprod/\n│   ├── app1-nonprod-svc 📦\n│   └── app2-nonprod-svc 📦\n└── Dev/\n```\n\n- 프로젝트는 📦 아이콘으로 표시됩니다. 다른 항목들은 폴더입니다.\n- 이 계층 구조는 공통(Common) 폴더와 Prod, Nonprod, Dev와 같은 세 가지 환경을 가장 상위 수준으로 구성합니다.\n- 여기서 Prod와 Nonprod에서 두 개의 공유 VPC 디자인을 구현하고 있음에 주목하세요. Common에는 각각의 호스트 프로젝트가 있습니다.\n- 이 디자인은 환경 당 메트릭 범위를 갖는 모니터링 디자인을 구현합니다. 각 메트릭 범위를 호스팅할 프로젝트가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n간단하고 팀 중심적인 등급체계:\n\n```js\nOrg/\n├── Common/\n│   ├── vpc-host-prod 📦\n│   ├── vpc-host-nonprod 📦\n│   ├── logging 📦\n│   ├── monitoring-prod 📦\n│   ├── monitoring-nonprod 📦\n│   └── monitoring-dev 📦\n├── team-huey/\n│   ├── Prod/\n│   │   └── huey-prod-svc 📦\n│   ├── Nonprod/\n│   │   └── huey-nonprod-svc 📦\n│   └── Dev/\n└── team-dewey/\n    ├── Prod/\n    │   └── dewey-prod-svc 📦\n    ├── Nonprod/\n    │   └── dewey-nonprod-svc 📦\n    └── Dev/\n```\n\n- 여기서 최상위 분류는 환경이 아닌 팀에 따라 구성됩니다.\n- 각 팀은 그런 다음 세 환경을 위한 폴더로 나뉩니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n조직/\n├── 공통/\n│   ├── vpc-host-prod 📦\n│   ├── vpc-host-nonprod 📦\n│   ├── 로깅 📦\n│   ├── 모니터링-운영 📦\n│   ├── 모니터링-비운영 📦\n│   └── 모니터링-개발 📦\n├── 운영/\n│   ├── 리테일뱅킹/\n│   │   ├── 휴이/\n│   │   │   └── 운영/\n│   │   │       └── rb-huey-prod-svc 📦\n│   │   └── 듀이/\n│   │       └── 운영/\n│   │           └── rb-dewey-prod-svc 📦\n│   ├── 자산관리/\n│   └── 모기지/\n├── 비운영/\n│   ├── 리테일뱅킹/\n│   │   ├── 휴이/\n│   │   │   └── 비운영/\n│   │   │       └── rb-huey-nonprod-svc 📦\n│   │   └── 듀이/\n│   │       └── 비운영/\n│   │           └── rb-dewey-nonprod-svc 📦\n│   ├── 자산관리/\n│   └── 모기지/\n└── 개발/\n    ├── 리테일뱅킹\n    ├── 자산관리\n    └── 모기지\r\n```\n\n- 상위 수준에서 각 환경을 기준으로 구성되었지만, 각 환경은 각각의 사업부로 나누어집니다.\n- 각 사업부에는 여러 팀이 포함되어 있습니다.\n\n사업부 중심 계층 구조:\n\n```js\n조직/\n├── 공통/\n│   ├── vpc-host-prod 📦\n│   ├── vpc-host-nonprod 📦\n│   ├── 로깅 📦\n│   ├── 모니터링-운영 📦\n│   ├── 모니터링-비운영 📦\n│   └── 모니터링-개발 📦\n├── 리테일뱅킹/\n│   ├── 휴이/\n│   │   ├── 운영/\n│   │   │   └── rb-huey-prod-svc 📦\n│   │   ├── 비운영/\n│   │   │   └── rb-huey-nonprod-svc 📦\n│   │   └── 개발/\n│   └── 듀이/\n│       ├── 운영/\n│       │   └── rb-dewey-prod-svc 📦\n│       ├── 비운영/\n│       │   └── rb-dewey-nonprod-svc 📦\n│       └── 개발/\n├── 자산관리/\n│   ├── 휴이/\n│   │   ├── 운영/\n│   │   ├── 비운영/\n│   │   └── 개발/\n│   └── 듀이/\n│       ├── 운영/\n│       ├── 비운영/\n│       └── 개발/\n└── 모기지/\n    ├── 휴이/\n    │   ├── 운영/\n    │   ├── 비운영/\n    │   └── 개발/\n    └── 듀이/\n        ├── 운영/\n        ├── 비운영/\n        └── 개발/\r\n```\n\n<div class=\"content-ad\"></div>\n\n- 여기서는 비즈니스 단위, 상위 레벨의 팀, 환경을 카테고리화합니다.\n\n우리가 선택한 계층 구조에 관계없이 다음을 구성할 수 있다는 점에 유의하세요:\n\n- 비즈니스 단위의 수 및 이름. (본 데모에서는 내 조직이 금융 기관 / 은행이라고 가정하고, 소매 은행, 자산 관리 및 모기지를 위한 최상위 비즈니스 단위를 만들었습니다.)\n- 팀의 수와 이름.\n- 세 개의 환경의 이름.\n- 공유 VPC 호스트 프로젝트와 연결된 서비스 프로젝트의 이름.\n- 추가 사용자 정의 프로젝트.\n\n상당한 규모의 조직의 경우 환경 중심의 계층 구조 (환경 → 비즈니스 단위 → 팀)을 선호하는 편이며, 이 데모에서 이 계층 구조를 사용하려고 합니다.\n\n<div class=\"content-ad\"></div>\n\n이제 우리는 계층 구조의 폴더와 프로젝트에 IAM 역할을 적용해야 합니다. Google Cloud 설정은 각 그룹에 추가해야 할 역할을 권장합니다. 그러나 이번에는 조직 수준에서만 적용하는 것이 아니라 리소스 계층에 적용합니다. IAM 정책은 계층 구조를 따라 상속되며 권한이 누적됩니다. 따라서 유효한 액세스는 각 수준에서 상속된 정책과 가장 낮은 수준의 정책을 합한 것입니다.\n\nGoogle Cloud 설정 권장 사항은 다음과 같을 것입니다:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_26.png)\n\n\"드래프트 구성 확인\"을 클릭해주세요.\n\n<div class=\"content-ad\"></div>\n\n## 6 - 중앙 집중식 로깅\n\n여기서 클라우드 설정을 통해 중앙 집중식 로깅을 설정할 수 있습니다.\n\n![Cloud Setup Image](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_27.png)\n\n시작 로깅 구성을 클릭하세요:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_28.png\" />\n\n먼저, 모든 감사 로그의 집계 로깅을 중앙 집중식 로깅 버킷에 설정합니다. 이 버킷은 이전에 생성한 로깅 프로젝트에 저장됩니다. (여기에 문서화한 최상의 모범 사례대로입니다.)\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_29.png\" />\n\n또한 BigQuery로 로그 라우팅 및 아카이브 로깅(예: 규정 준수 목적)을 더 저렴한 GCS 버킷으로 라우팅할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래와 같이 Markdown 형식으로 테이블 태그를 변경해주세요.\n\n\nGo ahead and “Confirm draft configuration.”\n\n![image](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_30.png)\n\n## 7 — VPC Networks\n\nIn this step, we set up a pair of shared virtual private cloud (VPC) networks, as per the dual shared VPC pattern: one in prod, and one in non-prod.\n\n\n<div class=\"content-ad\"></div>\n\n마크다운 형식의 이미지 테그를 변경하세요.\n\n![](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_31.png)\n\nGoogle Cloud 설정은 각 VPC마다 쌍으로 서브넷을 구성해야 합니다. 이것이 최소 요구사항이며, 추가 서브넷을 이 단계에서 구성할 수도 있습니다.\n\n![](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_32.png)\n\n추천드리는 것은:\n\n<div class=\"content-ad\"></div>\n\n- 한 지역에서 첫 번째 프로드 Subnet을 생성하고, 다른 지역에서 두 번째 프로드 Subnet을 생성하세요. 이렇게 함으로써 지역적인 Redundancy가 필요한 이중 지역 아키텍처를 배포할 수 있습니다. 또한 제가 여기서 설명한 대로 99.99% SLA를 갖는 HA Hybrid Connectivity를 구성할 수 있습니다.\n- 비 프로드에도 비슷한 두 개의 Subnet을 구성하세요. 참고: 비 프로드 VPC에는 프로드 VPC에 선택한 IP CIDR 범위와 동일한 것을 사용할 수 있습니다. 이는 IP가 특정 VPC 내에서만 고유해야 한다는 점 때문입니다. 그러나 이로 인해 프로드 Subnet을 비프로드 Subnet에 피어링할 수 없게 되지만, 이 분리를 강제하고 싶을 수도 있습니다.\n- 각 Subnet에서 개인 Google 액세스를 활성화하세요. 이를 통해 외부 IP 주소가 없는 VM도 Google API 및 서비스의 공용 IP 주소에 액세스할 수 있습니다.\n- 각 Subnet에서 Cloud NAT를 활성화하세요. 이를 통해 다음에 대한 인터넷 외부 연결이 가능해집니다: 외부 IP 주소가 없는 VM, Private GKE 클러스터, 서버리스 VPC 액세스를 통한 Cloud Run, 서버리스 VPC 액세스를 통한 Cloud Functions.\n- 권장 VPC 방화벽 규칙을 구성된 상태로 유지하세요. 기본적으로 Google은 VPC 내의 어느 곳에서나 ICMP를 허용하는 방화벽 규칙을 적용하고, SSH 또는 RDP는 클라우드 ID 기반 프록시 (IAP) 범위(35.235.240.0/20)에서만 허용합니다.\n- 기본적으로 설정은 방화벽 규칙 로깅을 활성화합니다. 그러나 이는 비용이 많이 발생할 수 있습니다. 특정 규칙에 대해서만 로깅을 활성화하는 것을 고려해보세요.\n- 기본적으로 설정은 VPC 플로우 로그를 활성화합니다. 이는 VPC 내 VM(포함하여 GKE 노드)에 의해 송수신된 네트워크 흐름의 샘플을 기록합니다. 네트워크 분석 및 포렌식에 유용합니다. 그러나 다시 말씀드리지만, 비용이 많이 발생할 수 있습니다. 제 추천은 로그를 켜둔 채로 완전하게 설정한 후, 플로우 로그를 조절하여 로깅 양을 제한하는 것입니다. (나중에 FinOps 권장사항에서 이를 다룰 예정입니다.)\n\n여기가 제 구성입니다. (요금을 최대한 저렴하게 유지하기 위해 로깅은 꺼져 있습니다. 이 데모의 목적을 위해)\n\n`![image](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_33.png)`\n\n“서비스 프로젝트로 계속하기”를 클릭하세요. 이렇게 하면 우리가 이전에 구성한 프로젝트를 서비스 프로젝트로 연결할 수 있는 화면으로 이동합니다. 이를 통해 이러한 프로젝트가 공유 VPC를 사용할 수 있도록 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n마지막으로, \"확인된 초안 설정\"을 클릭하세요.\n\n## 8—하이브리드 연결\n\n그다음, 클라우드 설정에서는 하이브리드 연결을 설정합니다. 현재 이 설정 작업은 미리보기 단계에 있습니다. IPsec VPN을 사용하여 하이브리드 연결을 구성할 수 있습니다.\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_34.png)\n\n<div class=\"content-ad\"></div>\n\n시작하면 다음과 같은 화면을 볼 수 있습니다:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_35.png)\n\n이 데모의 목적으로 하이브리드 연결 설정은 하지 않겠습니다.\n\n## 배포 또는 다운로드\n\n<div class=\"content-ad\"></div>\n\n이제 중요한 선택이 있습니다. 지금까지 구성한 모든 것을 적용할 수 있는 콘솔에서 직접 배포할 수 있습니다.\n\n또는 구성한 Terraform을 다운로드할 수도 있습니다. Terraform을 다운로드하면...\n\n# 2 — “ClickOps, 다운로드하고 배포하기”\n\n여기서 위에서 한 모든 단계는 동일합니다. 그러나 콘솔에서 배포하는 대신에 Terraform을 다운로드합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_36.png\" />\n\n## 누구를 위한것인가요?\n\n이것은 중소 규모 조직을 위한 것입니다. 이 조직은:\n\n- Terraform을 사용하여 지속적인 LZ 및 Google Cloud 인프라 자원을 관리하려고 합니다. (그리고 이것은 항상 좋은 생각입니다!)\n- 기본 클릭 옵션 설정에 추가적인 사용자 정의 및 구성을 추가할 수 있기를 원합니다.\n- Fabric FAST와 같이 더 정교한 엔터프라이즈 LZ 배포로 진행하기에 충분히 강력한 플랫폼 팀이 필요하지는 않습니다.\n\n<div class=\"content-ad\"></div>\n\n## 장점\n\n- Cloud Console의 \"Click-Ops\" 프로세스로 생성된 Terraform 구성은 제가 설명한대로 가능한 한 쉽게 하루 안에 완료할 수 있습니다.\n- 원하는대로 Terraform을 조정할 수 있습니다.\n- Terraform은 소스 제어(예: GitHub)에 배치해야 합니다.\n- Terraform 구성을 협업하여 작업할 수 있습니다.\n- 향후 Terraform 구성에 변경 사항이 있으면 해당 변경 사항을 적용할 수 있습니다.\n- 전체 LZ를 삭제하려면 한 줄로 몇 초만에 삭제할 수 있습니다.\n\n## 단점\n\n- Terraform 기술이 약간 필요합니다.\n- 자동화된 CI/CD 파이프라인, 테넌트 팩토리 또는 프로젝트 팩토리를 생성하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n## Terraform 설정 파일 다운로드하기\n\n시작해 봅시다!\n\n먼저, \"Terraform으로 다운로드\"를 클릭하세요. 이제 테라폼 상태를 저장할 버킷의 지역을 선택해 주세요. 실제로는 아직 버킷을 생성하지 않고, 나중에 다운로드할 backend.tf 파일에 고유한 버킷 식별자가 생성됩니다.\n\n그런 다음, 테라폼 구성 파일을 다운로드하세요. 이 파일은 terraform.tar.gz로 로컬 기기에 다운로드됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 테라폼을 클라우드 셸에 업로드하세요\n\n다운로드한 후, 테라폼을 실행하는 방법은 두 가지가 있습니다:\n\n- Google Cloud Shell에서 실행.\n- Cloud SDK가 설치된 장치에서 실행.\n\n이 데모를 위해 간단하게 클라우드 셸을 사용하겠습니다. 테라폼으로 기본 구성을 배포하는 데 필요한 모든 것이 미리 설치되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n클라우드 셸에 인증하고 Terraform 구성을 위한 폴더를 만들어봅시다:\n\n```js\n# 인증하기\ngcloud auth list\n\n# Terraform 구성을 위한 폴더 생성\nmkdir tf-foundation-setup\ncd $_\n```\n\n이제 콘솔을 통해 Terraform 구성 파일(.gz 파일)을 업로드한 다음, 이전에 만든 폴더에 파일을 추출해보세요:\n\n```js\n# 방금 업로드한 파일 추출\ntar -xzvf ../terraform.tar.gz\n```\n\n<div class=\"content-ad\"></div>\n\n우리 폴더에 이 파일들이 있습니다:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_37.png)\n\n## 테라폼 상태를 저장할 시드 프로젝트 생성\n\n```sh\n# 테라폼 상태를 저장할 시드 프로젝트 생성\nSUFFIX=$RANDOM\nPROJECT_ID=시드-프로젝트-$SUFFIX\ngcloud projects create $PROJECT_ID\ngcloud config set project ${PROJECT_ID}\n\n# 요금 청구 계정 연결\ngcloud billing projects link $PROJECT_ID --billing-account <당신의_청구_계정_ID>\n\n# 테라폼으로 배포할 필요가 있는 API 활성화\ngcloud services enable cloudresourcemanager.googleapis.com\ngcloud services enable iam.googleapis.com\ngcloud services enable serviceusage.googleapis.com\ngcloud services enable cloudbilling.googleapis.com\ngcloud services enable cloudidentity.googleapis.com\ngcloud services enable orgpolicy.googleapis.com\n```\n\n<div class=\"content-ad\"></div>\n\n저희 조직에서 현재 가지고 있는 프로젝트를 간단히 살펴봅시다:\n\n![프로젝트 이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_38.png)\n\n방금 만들어 놓은 씨드 프로젝트 seed-project-28844를 확인할 수 있습니다.\n\n이제 백엔드.tf 파일에서 상태 유지에 사용될 버킷을 살펴보겠습니다. 실제로 제 버킷 이름을 더 의미 있는 것으로 변경했습니다. 또한 전 세계적으로 고유한 이름이어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_39.png\" />\n\n이 버킷을 사용하기 전에 만들어야 합니다:\n\n```js\ngsutil mb gs://tfstate-28844\n```\n\n다음을 통해 만들어졌는지 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_40.png\" />\n\n## GitHub에 저장하기\n\n이제 테라폼 구성을 GitHub에 저장하는 것이 좋을 시기입니다. 또는 Google Cloud Repos에 저장할 수도 있습니다. GitHub의 개인 저장소에 저장하는 과정을 다음과 같이 따를 수 있습니다:\n\n```js\n# 이전에 생성한 tf-foundation-setup 폴더에 있다고 가정합니다\n\n# Cloud Shell에서 git을 설정합니다. 이전에 설정하지 않았다면\ngit config --global user.email \"bob@wherever.com\"\ngit config --global user.name \"Bob\"\n\n# 로컬 git 저장소를 생성합니다.\n# 진행하기 전에 .gitignore 파일이 생성되어 있는지 확인해주세요.\n# .terraform 디렉토리 및 로컬 state, plans 등을 무시하도록 설정되어 있어야 합니다.\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n\n# GitHub 명령줄 도구를 인증합니다.\n# 이미 Cloud Shell에 설치되어 있습니다.\ngh auth login\n\n# 이제 gh cli를 사용하여 GitHub에 원격 개인 저장소를 생성합니다.\ngh repo create gcp-demos-foundation-setup --private --source=.\ngit push -u origin master\n```\n\n<div class=\"content-ad\"></div>\n\n좋아요. 이제 우리 코드는 안전하게 추적되어 있고 팀원들에게 제공됩니다.\n\n![image](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_41.png)\n\n## 권한이 있는지 확인하기\n\n조직 관리자 그룹이 시드 프로젝트에서 적절한 역할을 부여받았는지 확인해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\ngcloud 조직 add-iam-policy-binding $ORG_ID \\\n  --member=\"group:gcp-organization-admins@gcp-demos.just2good.co.uk\" \\\n  --role=\"roles/storage.admin\"\n\ngcloud 조직 add-iam-policy-binding $ORG_ID \\\n  --member=\"group:gcp-organization-admins@gcp-demos.just2good.co.uk\" \\\n  --role=\"roles/compute.xpnAdmin\"\n\ngcloud 프로젝트 add-iam-policy-binding $PROJECT_ID \\\n  --member=\"group:gcp-organization-admins@gcp-demos.just2good.co.uk\" \\\n  --role=\"roles/serviceusage.serviceUsageConsumer\"\n```\n\n## 테라폼\n\n마지막으로, 테라포밍할 준비가 됐어요!!\n\n```js\n# 테라폼 초기화\nterraform init\n```\n\n<div class=\"content-ad\"></div>\n\n다음은 명령의 출력입니다.\n\n\n![2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_42](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_42.png)\n\n\nGCS 버킷의 콘솔 뷰를 새로고침하면 이제 버킷에 Terraform 상태 파일이 생성된 것을 확인할 수 있습니다:\n\n\n![2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_43](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_43.png)\n\n\n<div class=\"content-ad\"></div>\n\n지금까지 잘 진행되고 있어요.\n\n```shell\n# 테라폼 계획을 작성하고 확인합니다\nterraform plan -out=plan.out\n```\n\n결과는 다음과 같습니다:\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_44.png)\n\n<div class=\"content-ad\"></div>\n\n그리고 이제 진실의 순간입니다. 마침내 저희의 랜딩 존을 배포할 수 있습니다!\n\n```js\n# 계획을 적용하십시오!\nterraform apply plan.out\n```\n\n몇 분이 소요됩니다. 그리고 성공했습니다! 만들어진 폴더와 프로젝트를 확인해보세요:\n\n![랜딩 존 배포 이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_45.png)\n\n<div class=\"content-ad\"></div>\n\n이제, 만든 모든 것을 파괴하고 싶다면, 이렇게 하면 됩니다:\n\n```js\nterraform destroy\n```\n\n![이미지](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_46.png)\n\n한 가지 주의할 점: 클라우드 설정에서의 Terraform 구성에는 여러 하드코딩된 프로젝트 ID가 포함되어 있습니다. 위에서 설명한대로 LZ를 파괴하면 이러한 프로젝트 ID가 즉시 해제되지 않습니다. 결과적으로, 단순히 Terraform 구성을 다시 적용할 수 없습니다. 다시 적용하려면 프로젝트 ID를 변경해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 3 — 클라우드 기반 패브릭 FAST\n\n## 누구를 위한가요?\n\n구성 가능성이 높고, Terraform 기반 LZ를 사용하여 권한 분리, 다중 테넌시 및 즉시 사용 가능한 GitOps 및 CI/CD 파이프라인을 허용하는 대규모 조직을 위한 솔루션입니다.\n\n## 방법은?\n\n<div class=\"content-ad\"></div>\n\n실은, 이 주제에 대해 이전에 상세히 다룬 바 있습니다. 그래서 여기서 다시 다루지 않겠습니다.\n\n# 요약\n\n그럼 이 정도로 마무리 지어보겠습니다! 마침내, 착륙 구역 주제에 대한 이 섹션의 기사를 마쳤습니다. 이전에 다룬 내용은 다음과 같습니다:\n\n- LZ 디자인\n- LZ \"기술 온보딩\"을 위한 LZ 코어 팀 구성\n\n<div class=\"content-ad\"></div>\n\n이 기사에는 LZ를 실제로 배포하는 방법을 안내했습니다.\n\n이제 우리는 작동 중인 기업용 구글 클라우드 플랫폼을 갖추었고, 워크로드를 배포할 준비가 되었습니다!\n\n다음 글에서 만나요.\n\n# 가기 전에\n\n<div class=\"content-ad\"></div>\n\n- 관심 있는 사람들에게 공유해주세요. 그들에게 도움이 될 수도 있고, 저에게 큰 도움이 됩니다!\n- 박수를 좀 주세요! 여러 번 박수 치는 거, 알고 계시죠?\n- 💬 댓글을 자유롭게 남겨주세요.\n- 내 콘텐츠를 놓치지 않으려면 팔로우하고 구독해주세요. 프로필 페이지로 이동하여 다음 아이콘을 클릭하세요:\n\n![Google Cloud](/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_47.png)\n\n# 링크\n\n- 구글 클라우드의 랜딩 존: 무엇인가, 왜 필요한가, 어떻게 만드는가\n- 구글 클라우드 콘솔: 클라우드 설정\n- 구글 클라우드 설정 체크리스트\n- 클라우드 아이덴티티 개요\n- 구글 프라이빗 액세스\n- 구글 클라우드 아이덴티티-인증 프록시 (IAP)\n- 콘솔에서 다운로드한 테라폼으로 기본 구조 배포\n- 테라폼 및 클라우드 기초 패브릭 FAST로 구글 클라우드 랜딩 존\n- 구글 클라우드 아키텍처 프레임워크\n- 엔터프라이즈 기본 토대\n\n<div class=\"content-ad\"></div>\n\n# 시리즈 탐색\n\n- 시리즈 전체 내용 및 구조\n- 이전: 랜딩 존 기술 입사 - \"어떻게\" \n- 다음: 클라우드 소비자 및 테넌트를 활성화하기","ogImage":{"url":"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_0.png"},"coverImage":"/assets/img/2024-06-19-LandingZoneDeploymentGoogleCloudAdoptionSeries_0.png","tag":["Tech"],"readingTime":23},{"title":"AKS 패칭 자동화 및 Terraform과 Logic Apps를 사용하여 Slack 알림 받기 파트 1","description":"","date":"2024-06-19 13:15","slug":"2024-06-19-AutomateAKSPatchingandGetSlackNotificationswithTerraformandLogicAppsPart1","content":"\n\nAKS 완전 자동 패칭 소개: 최신 패치와 업데이트로 Azure Kubernetes 서비스 (AKS) 클러스터를 최신 상태로 유지하는 것은 보안, 안정성 및 성능을 유지하는 데 중요합니다. 패치되지 않은 클러스터는 알려진 보안 취약점, 버그 및 성능 문제에 취약해지며, 다운타임, 데이터 침해 및 기타 비용 소모적인 결과로 이어질 수 있습니다.\n\n그러나 AKS 클러스터를 수동으로 패칭하는 것은 특히 여러 클러스터가 있는 대규모 환경에서 시간이 많이 소요되고 오류가 발생하기 쉬운 과정일 수 있습니다. 수동으로 패칭하려면 유지 보수 창을 조정하고 패칭 프로세스를 모니터링하며 모든 클러스터가 중요한 작업 부하를 방해하지 않고 성공적으로 업데이트되도록 보장해야 합니다.\n\n![AKS Patching](/assets/img/2024-06-19-AutomateAKSPatchingandGetSlackNotificationswithTerraformandLogicAppsPart1_0.png)\n\nTerraform 및 Azure의 유지 보수 창 기능을 활용하여 AKS 패칭 프로세스를 간편하게 진행하고 클러스터가 최신 상태로 안정하게 유지되도록 할 수 있습니다. 설정 작업이나 다운타임을 최소화하면서 클러스터를 업데이트할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 사전 준비 사항 (초보자용)\n\nAKS 패치 자동화를 위해 Terraform을 사용하여 시작하려면 다음 사전 준비 사항(기본 사항)이 필요합니다:\n\n- Azure 구독: Azure 구독이 활성화되어 있어야 합니다. Azure 클라우드 플랫폼 내에서 리소스를 생성하고 관리할 수 있습니다.\n- Terraform 설치: Terraform은 선언적 구성 파일을 사용하여 클라우드 리소스를 정의하고 제공할 수 있도록 해주는 오픈 소스 인프라 코드(IaC) 도구입니다. 로컬 머신이나 빌드 서버에 Terraform이 설치되어 있어야 합니다.\n- Azure CLI 설치 및 인증: Azure CLI는 Azure 서비스와 상호 작용할 수 있는 명령줄 인터페이스입니다. Azure CLI를 설치하고 Azure 구독과 인증해야 합니다. 이를 통해 Terraform이 Azure와 인증하고 필요한 작업을 수행할 수 있습니다.\n\n이러한 사전 준비 사항을 갖추었다면, Terraform 코드를 구성하여 AKS 패치 자동화를 진행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# Azure Provider 구성하기 (초보자용)\n\n우선, Terraform을 위해 Azure Provider를 구성해야 합니다. 이를 통해 Terraform에게 어떤 공급자(provider)를 사용할지 알려주고 선택적 기능을 활성화합니다.\n\n```js\nprovider \"azurerm\" {\n  features {}\n}\n```\n\nfeatures 블록을 사용하여 공급자의 동작을 사용하거나 비활성화하여 사용자 정의할 수 있습니다. 이 경우 비워 두었지만 필요에 따라 여기에 기능 플래그를 추가할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAzure Provider를 구성함으로써 Terraform은 Azure 리소스를 사용할 때 Azure Resource Manager(azurerm) 제공자를 사용하도록 알고 있습니다. 이 제공자를 통해 Terraform은 Azure 구독 내에서 리소스를 생성, 관리 및 업데이트할 수 있습니다.\n\n# 리소스 그룹 생성 (초보자용)\n\n다음으로 AKS 클러스터와 관련 리소스를 포함하는 Azure 리소스 그룹을 생성합니다. 우리는 리소스 그룹의 이름과 위치를 지정합니다.\n\n```js\nresource \"azurerm_resource_group\" \"example\" {\n  name     = \"example-resources\"\n  location = \"West Europe\"\n}\n```\n\n<div class=\"content-ad\"></div>\n\n이 블록에서는 azurerm_resource_group 리소스를 사용하여 새로운 Azure 리소스 그룹을 정의합니다. \"name\" 인수는 리소스 그룹의 이름을 \"example-resources\"로 지정합니다. \"location\" 인수는 리소스 그룹이 생성될 Azure 지역을 설정하며, 이 경우 \"West Europe\"입니다.\n\n전용 리소그룹을 생성함으로써 AKS 클러스터 배포에 관련된 모든 리소스를 논리적으로 그룹화하고 관리할 수 있습니다. 이는 깨끗하고 조직적인 Azure 환경을 유지하고 이 프로젝트와 관련된 리소스를 관리하고 모니터링하기 쉽게 만들어줍니다.\n\n# AKS 클러스터 정의\n\n이제, azurerm_kubernetes_cluster 리소스를 사용하여 AKS 클러스터를 정의해 봅시다. 클러스터 이름, 위치, 리소스 그룹, DNS 접두사, 기본 노드 풀 및 시스템 자동 할당 관리 ID와 같은 필요한 구성을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\nresource \"azurerm_kubernetes_cluster\" \"example\" {\n  name                = \"example-aks1\"\n  location            = azurerm_resource_group.example.location\n  resource_group_name = azurerm_resource_group.example.name\n  dns_prefix          = \"exampleaks1\"\n  default_node_pool {\n    name       = \"default\"\n    node_count = 1\n    vm_size    = \"Standard_D2_v2\"\n  }\n  identity {\n    type = \"SystemAssigned\"\n  }\n  tags = {\n    Environment = \"Production\"\n  }\n  # ... (maintenance window configuration)\n}\n```\n\n이 블록에서는 AKS 클러스터의 이름을 example-aks1로 지정했습니다. 위치와 resource_group_name 속성은 이전에 만든 리소스 그룹을 참조하여 클러스터가 원하는 위치와 리소스 그룹에 배포되도록 합니다.\n\ndns_prefix는 클러스터에 대한 고유한 DNS 접두어로, 클러스터의 완전한 도메인 이름을 만들기 위해 필요합니다.\n\ndefault_node_pool 블록은 클러스터의 기본 노드 풀 구성을 정의합니다. 노드 풀의 이름을 default로 설정하고, 단일 노드를 지정하고(노드 수 = 1), Standard_D2_v2 가상 머신 크기를 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n아이덴티티 블록은 AKS 클러스터에 시스템 지정 관리되는 아이덴티티를 사용하려는 것을 나타냅니다. 이 관리되는 아이덴티티는 클러스터가 Azure Container Registry와 같은 다른 Azure 리소스에 액세스할 수 있도록 사용할 수 있습니다.\n\n마지막으로 우리는 클러스터 리소스에 키가 \"환경\"이고 값이 \"Production\"인 태그를 추가합니다.\n\n# 유지보수 창구 관리\n\nAKS 패치를 자동화하기 위해 AKS 클러스터 리소스 내에서 maintenance_window_auto_upgrade 및 maintenance_window_node_os 블록을 활용할 것입니다. 이러한 블록을 사용하여 자동 업그레이드 및 노드 OS 업그레이드를 위한 주기, 간격, 월별 일, 지속 시간, 시작 시간, UTC 오프셋 및 시작 날짜를 정의할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n우리는 var.automatic_channel_upgrade 및 var.node_os_channel_upgrade 변수의 값에 따라 조건에 따라 유지 관리 창 구성을 동적 블록을 사용하여 포함합니다.\n\nmaintenance_window_auto_upgrade 블록은 Kubernetes 클러스터 자체의 자동 업그레이드 설정을 구성하는 데 사용됩니다. 여기에서는 업그레이드 채널 (패치, 빠른, 노드 이미지 또는 안정), 업그레이드 빈도 (주간, 월간 등), 업그레이드할 달의 날짜, 유지 관리 창의 기간, 시작 시간, UTC 오프셋 및 시작 날짜를 지정할 수 있습니다.\n\n```js\ndynamic \"maintenance_window_auto_upgrade\" {\n  for_each = var.automatic_channel_upgrade != \"none\" ? [1] : []\n  content {\n    frequency     = var.auto_upgrade_frequency\n    interval      = var.auto_upgrade_interval\n    day_of_month  = var.auto_upgrade_day_of_month\n    duration      = var.auto_upgrade_duration\n    start_time    = var.auto_upgrade_start_time\n    utc_offset    = var.auto_upgrade_utc_offset\n    start_date    = var.auto_upgrade_start_date\n  }\n}\n```\n\n마찬가지로, maintenance_window_node_os 블록은 노드 OS 이미지의 업그레이드 설정을 구성하는 데 사용됩니다. 업그레이드 채널 (Unmanaged, SecurityPatch, NodeImage 또는 None), 빈도, 달의 날짜, 지속 시간, 시작 시간, UTC 오프셋 및 시작 날짜를 지정할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```json\ndynamic \"maintenance_window_node_os\" {\n  for_each = var.node_os_channel_upgrade != \"None\" ? [1] : []\n  content {\n    frequency     = var.node_os_upgrade_frequency\n    interval      = var.node_os_upgrade_interval\n    day_of_month  = var.node_os_upgrade_day_of_month\n    duration      = var.node_os_upgrade_duration\n    start_time    = var.node_os_upgrade_start_time\n    utc_offset    = var.node_os_upgrade_utc_offset\n    start_date    = var.node_os_upgrade_start_date\n  }\n}\n```\n\n이 유지 보수 창을 구성함으로써, 우리는 지정된 일정 및 환경 설정에 따라 AKS 클러스터 및 노드 OS 이미지가 자동으로 패치되고 업데이트되도록 보장할 수 있습니다. 이를 통해 수동 개입이 줄어들고 전체적인 보안과 안전성이 향상됩니다.\n\n# 변수 정의\n\nTerraform 구성을 더 유연하고 재사용 가능하게 만들기 위해 유지 보수 창 설정에 대한 변수를 정의합니다. 이러한 변수는 업그레이드 채널, 빈도, 기간, 시작 시간 및 기타 매개변수를 사용자 정의할 수 있도록 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n다음과 같이 변수를 정의합니다:\n\n```js\n변수 \"automatic_channel_upgrade\" {\n    유형        = 문자열\n    설명 = \"이 Kubernetes 클러스터의 업그레이드 채널입니다. patch, rapid, node-image, stable 중에서 선택할 수 있습니다. 이 필드를 생략하면 해당 값은 없음으로 설정됩니다.\"\n    기본값     = \"rapid\"\n유효성 검사 {\n        조건     = (var.automatic_channel_upgrade == null ? true : contains([\"patch\", \"rapid\", \"node-image\", \"stable\"], var.automatic_channel_upgrade))\n        오류 메시지 = \"반드시 'patch', 'rapid', 'node-image', 'stable' 또는 null이어야 합니다. 기본값: null.\"\n    }\n}\n\n변수 \"node_os_channel_upgrade\" {\n    유형        = string\n    설명 = \"이 Kubernetes 클러스터 노드의 OS 이미지의 업그레이드 채널입니다. Unmanaged, SecurityPatch, NodeImage, None 중에서 선택할 수 있습니다.\"\n    기본값     = \"Unmanaged\"\n}\n```\n\n이러한 변수들은 Kubernetes 클러스터 및 노드 OS 이미지의 업그레이드 채널을 제어합니다. automatic_channel_upgrade 변수를 사용하면 클러스터의 업그레이드 채널을 지정할 수 있으며, patch, rapid, node-image, stable과 같은 옵션이 있습니다. node_os_channel_upgrade 변수는 노드 OS 이미지의 업그레이드 채널을 제어하며, Unmanaged, SecurityPatch, NodeImage, None과 같은 옵션을 갖습니다.\n\n또한 유지 관리 창 설정을 구성하는 변수들을 정의합니다:\n\n<div class=\"content-ad\"></div>\n\n```js\r\nvariable \"auto_upgrade_frequency\" {\r\n    description = \"유지 관리 빈도입니다. 가능한 옵션은 주간, 절대월간 및 상대월간입니다.\"\r\n    default     = \"AbsoluteMonthly\"\r\n}\r\n\r\nvariable \"auto_upgrade_day_of_month\" {\r\n    description = \"유지 보수 실행을 위한 달의 날짜입니다. 절대월간 빈도와 함께 필요합니다. 0부터 31 사이의 값(포함)입니다.\"\r\n    default     = \"9\"\r\n}\r\nvariable \"auto_upgrade_duration\" {\r\n    description = \"유지 관리가 실행되는 창의 기간입니다.\"\r\n    default     = \"4\"\r\n}\r\nvariable \"auto_upgrade_start_time\" {\r\n    description = \"유지 보수가 시작되는 시간입니다. utc_offset에 따라 결정된 시간대를 기준으로합니다. 형식은 HH:mm입니다.\"\r\n    default     = \"12:33\"\r\n}\r\nvariable \"auto_upgrade_utc_offset\" {\r\n    description = \"클러스터 유지 관리를 위한 시간대를 결정하는데 사용됩니다.\"\r\n    default     = \"+00:00\"\r\n}\r\nvariable \"auto_upgrade_start_date\" {\r\n    description = \"유지 보수 창이 효력을 발생하는 날짜입니다.\"\r\n    default     = \"2024-05-09T00:00:00Z\"\r\n}\r\nvariable \"auto_upgrade_interval\" {\r\n    description = \"유지 보수 실행 간격입니다. 주간 또는 월간 기반으로이 간격이 지정됩니다.\"\r\n    default     = \"1\"\r\n}\r\n```\r\n\r\n이러한 변수들은 Kubernetes 클러스터의 유지 관리 창에 대한 여러 측면을 제어합니다. 주기(주간, 월간), 달의 날짜, 기간, 시작 시간, UTC 오프셋, 시작 날짜 및 간격과 같은 것이 포함됩니다.\r\n\r\n노드 OS 유지 관리 창에 대해 유사한 변수가 정의되어 있습니다:\r\n\r\n```js\r\nvariable \"node_os_upgrade_frequency\" {...}\r\nvariable \"node_os_upgrade_day_of_month\" {...}\r\nvariable \"node_os_upgrade_duration\" {...}\r\nvariable \"node_os_upgrade_start_time\" {...}\r\nvariable \"node_os_upgrade_utc_offset\" {...}\r\nvariable \"node_os_upgrade_start_date\" {...}\r\nvariable \"node_os_upgrade_interval\" {...}\r\n```\n\n<div class=\"content-ad\"></div>\n\n이러한 변수를 정의함으로써, 쿠버네티스 클러스터와 노드 OS 이미지의 유지 보수 창 설정을 쉽게 사용자 정의하고 조정할 수 있습니다. 이는 우리의 테라폼 구성을 다양한 환경과 요구 사항에 걸쳐 더 유연하고 재사용 가능하게 만들어 줍니다.\n\n# 출력 설정 구성\n\nAKS 클러스터에 대한 중요 정보를 검색하기 위해 출력 블록을 정의합니다. 이러한 출력을 사용하면 클라이언트 인증서 및 원시 쿠버네티스 구성에 액세스할 수 있습니다.\n\n```js\noutput \"client_certificate\" {\n  value     = azurerm_kubernetes_cluster.example.kube_config[0].client_certificate\n  sensitive = true\n}\n\noutput \"kube_config\" {\n  value     = azurerm_kubernetes_cluster.example.kube_config_raw\n  sensitive = true\n}\n```\n\n<div class=\"content-ad\"></div>\n\nclient_certificate 출력은 Kubernetes 클러스터와 인증하는 데 필요한 클라이언트 인증서를 제공합니다. Terraform이 sensitive = true로 표시함으로써, 해당 값이 계획 출력에 표시되지 않거나 상태 파일에 저장되지 않도록 보장합니다.\n\nkube_config_raw 출력에는 클러스터 엔드포인트, 클라이언트 인증서 및 클러스터에 연결하는 데 필요한 기타 정보가 포함되어 있습니다. 이 출력도 민감한 데이터를 보호하기 위해 sensitive로 표시됩니다.\n\n이러한 출력을 정의함으로써, AKS 클러스터의 클라이언트 인증서 및 Kubernetes 구성에 쉽게 액세스할 수 있습니다. 이를 통해 컨테이너 애플리케이션을 안전하게 연결하고 관리할 수 있습니다.\n\n# 경보 및 작업 그룹 설정하기\n\n<div class=\"content-ad\"></div>\n\nAKS 클러스터의 생성 또는 업데이트 작업을 모니터링하기 위해, azurerm_monitor_activity_log_alert 리소스를 사용하여 Azure Monitor Activity Log Alert를 설정했습니다. 이 경고는 AKS 클러스터의 생성 또는 업데이트 작업이 시작될 때 트리거되어 진행 중인 변경 사항을 추적할 수 있도록 합니다.\n\n```js\nresource \"azurerm_monitor_activity_log_alert\" \"aks-cluster-activity-log-alert\" {\n  name                = \"${azurerm_resource_group.example.name}-aks-activity-Started-alert\"\n  resource_group_name = azurerm_resource_group.example.name\n  scopes              = [azurerm_kubernetes_cluster.example.id]\n  description         = \"이 경고는 시작된 Azure Kubernetes Service (AKS) 클러스터의 생성 또는 업데이트 작업을 모니터링합니다. AKS 클러스터의 생성 또는 업데이트 작업이 시작될 때 트리거되어 진행 중인 변경 사항을 추적할 수 있습니다.\"\n  enabled             = true\n  criteria {\n    category           = \"Administrative\"\n    level              = \"Informational\"\n    operation_name     = \"Microsoft.ContainerService/managedClusters/write\"\n    status             = \"Started\"\n    resource_provider  = \"Create or update managed cluster\"\n   }\n  action {\n    action_group_id = azurerm_monitor_action_group.aks-activity-log-action-Started.id\n  }\n}\n```\n\n또한, azurerm_monitor_action_group 리소스를 사용하여 Azure Monitor Action Group을 생성했습니다. 이 액션 그룹은 활동 로그 경고와 연결되어 경고가 활성화될 때 Logic App을 트리거합니다.\n\n```js\nresource \"azurerm_monitor_action_group\" \"aks-activity-log-action-Started\" {\n  name                = \"${azurerm_resource_group.example.name}-aks-activity-log-action-Started\"\n  resource_group_name = azurerm_resource_group.example.name\n  short_name          = \"akslog\"\n  logic_app_receiver {\n    name                    = \"logicappaction-patch-start\"\n    resource_id             = \"\"\n    callback_url            = azurerm_logic_app_trigger_http_request.logicapp-started-trigger.callback_url\n    use_common_alert_schema = true\n  }\n}\n```\n\n<div class=\"content-ad\"></div>\n\n이러한 경고 및 액션 그룹을 설정함으로써 AKS 클러스터를 효과적으로 모니터링하여 생성 또는 업데이트 작업이 발생할 때 필요에 따라 추가 작업이나 알림을 수행하는 로직 앱을 트리거할 수 있습니다.\n\n# 결론\n\n이 블로그 포스트에서는 Terraform과 Azure의 유지 보수 창 기능을 사용하여 AKS 패치를 자동화하는 방법에 대해 살펴보았습니다. AKS 클러스터를 정의하고 유지 보수 창을 구성함으로써 최신 패치와 업데이트를 반영하여 클러스터가 항상 최신 상태를 유지할 수 있습니다. 이 접근 방식은 패치 프로세스를 간소화하고 수동 노력을 줄이며 Kubernetes 환경의 전반적인 보안과 안정성을 향상시킵니다.\n\n이 자동화 프로세스에 관련된 주요 단계는 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- Azure Provider를 구성하고 리소스 그룹을 생성합니다.\n- 필요한 구성, 노드 풀 및 관리 식별자와 함께 AKS 클러스터를 정의합니다.\n- 자동 업그레이드 및 노드 OS 업그레이드 일정을 예약하는 maintenance_window_auto_upgrade 및 maintenance_window_node_os 블록을 활용합니다.\n- 업그레이드 채널, 빈도, 기간, 시작 시간 및 기타 매개변수를 사용자 정의하는 변수를 정의합니다.\n- AKS 클러스터에 관한 중요한 정보를 검색하기 위해 outputs를 구성합니다.\n- AKS 클러스터 생성 또는 업데이트 작업을 모니터링하고 추적하기 위해 Azure Monitor Activity Log Alerts 및 Action Groups를 설정합니다.\n\n이 블로그 시리즈의 다음 부분에서는 Logic Apps를 사용하여 자동화된 Slack 메시지를 보내 AKS 패치 프로세스의 상태에 대해 알림을 받는 내용을 살펴볼 것입니다. 더 많은 자동화 소식을 기대해 주세요!","ogImage":{"url":"/assets/img/2024-06-19-AutomateAKSPatchingandGetSlackNotificationswithTerraformandLogicAppsPart1_0.png"},"coverImage":"/assets/img/2024-06-19-AutomateAKSPatchingandGetSlackNotificationswithTerraformandLogicAppsPart1_0.png","tag":["Tech"],"readingTime":12},{"title":"Kubernetes 클러스터에서 노드 회전 자동화 머신 이미지 업데이트 최적화하기","description":"","date":"2024-06-19 13:13","slug":"2024-06-19-StreamliningMachineImageUpdatesAutomatingNodeRotationinKubernetesClusters","content":"\n\n<img src=\"/assets/img/2024-06-19-StreamliningMachineImageUpdatesAutomatingNodeRotationinKubernetesClusters_0.png\" />\n\n# 문제\n\nSage AI 인프라 팀은 모든 환경에서 우리의 서비스 및 시스템의 유지 보수와 안정성을 담당합니다. 때로는 현재 머신 이미지에 보안 취약점이 발견된 경우와 같이 상대적으로 짧은 시간 내에 Kubernetes 클러스터의 노드를 새로운 머신 이미지로 업데이트해야 할 때도 있습니다. 우리는 Amazon 클라우드를 사용하기 때문에 작업하는 것은 Amazon Machine Images (AMIs)이지만, 문제와 해결책은 어떤 클라우드 환경의 Kubernetes 클러스터에도 적용 가능합니다.\n\n우리의 경우, 새로운 AMI ID를 적용할 것이지만, 그런 다음 노드가 새 AMI를 적용할 수 있도록 시간 내에 회전되도록 보장해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n과거에는 오래된 AMI를 감지하여 해당 노드를 드레인하고 있는 사용자 정의 쉘 스크립트를 사용했습니다(해당 노드에서 실행 중인 파드는 제어된 방식으로 종료되어야 하며, 해당 서비스에 중단이 발생하지 않도록해야 합니다). 이 방법을 사용하면 몇 가지 단점이 있었습니다. 첫째, 누군가가 수동으로 스크립트를 호출하고 모니터링해야 했습니다. 둘째, 클러스터에 액세스하기 위해 인증 프록시를 사용하고 있으며, 해당 프록시가 있는 노드를 드레인하면 스크립트가 서버 연결을 잃고 오류가 발생할 수 있습니다. 때로는 이러한 일이 연이어 발생할 수도 있었습니다. 당연히 노드를 회전시키는 작업은 해당 작업을 담당하는 엔지니어에게 상당한 시간, 주의 및 수동 노력이 필요했습니다.\n\n# 솔루션 설계\n\n우리는 클러스터에서 AMI를 업데이트하는 엔지니어들에게 주는 부담을 크게 줄일 수 있는 솔루션을 찾기로 결정했습니다. 오래된 AMI가 있는 노드를 자동으로 회전시킬 수 있는 도구가 필요했다는 것을 알았지만, 이 요구 사항은 넓은 범위를 요구합니다. 조금 더 세부적으로 이를 분석해보면 프로젝트에 대한 몇 가지 요구 사항이 있었습니다;\n\n<div class=\"content-ad\"></div>\n\n- 우리는 쉽게 운영할 수 있는 도구를 원했습니다. 가장 이상적인 경우에는 감독이 필요하지 않은 것이 좋습니다. 엔지니어의 시간은 귀중하며 딱딱한 도구를 감시하는 데 그 시간을 낭비하는 것은 별 의미가 없습니다.\n- 필요한 경우에는 간편하게 유지 및 업데이트할 수 있는 도구를 원했습니다. 우리는 많은 서드파티 도구를 사용하고 주기적으로 모두를 업그레이드해야 할 때가 있습니다. 그 중 일부는 다른 것들보다 업데이트하기가 훨씬 어려웠습니다.\n- 우리는 쿠버네티스 클러스터에서 실행 중인 워크로드에 대한 설정한 모니터링 및 가시성 프로세스를 사용할 수 있는 솔루션을 원했습니다.\n\n## 우리의 솔루션\n\n우리는 시스템을 두 단계로 작동하도록 설계하기로 결정했습니다. 교체해야 할 노드를 확인하고 해당 노드의 워크로드를 소진하는 것입니다.\n\n간단히 말해서, 노드를 소진한다는 것은 쿠버네티스가 해당 노드에 대해 어떠한 새로운 워크로드도 시도하지 않도록 태그를 달아놓고, 해당 노드의 프로세스와 상태가 해체되고, 새로운 노드가 대신 생성될 때까지 대기하는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n이 작업을 하는 데 몇 가지 이유가 있었습니다.\n- 이를 통해 두 가지 구성 요소를 독립적으로 발전 및 유지할 수 있습니다.\n- 구성 요소에는 각각 다른 수명 주기 기대치가 있으며, 노드 선택 구성 요소는 선택 기준 또는 기본 인프라 변경에 따라 변경될 필요가 있습니다. 반면, 노드를 비우는 구성 요소는 비교적 안정적인 상태로 유지됩니다.\n- 노드 선택 구성 요소는 주로 AWS API와 상호 작용하고, 노드 비우기는 Kubernetes 컨트롤러입니다.\n\n두 구성 요소가 별도로 개발되는 또 다른 이유는 필요에 따라 서로 다른 언어로 작성되어 있으며, 선택 노드를 선택하는 구성 요소는 Bash 및 AWS CLI와 같은 성숙한 도구 및 jq와 같은 JSON 조작 도구 사용의 용이성과 간결성이 더 높을 것입니다. 반면에 우리는 솔루션을 Go 생태계에서 구현하기로 선택했습니다. 왜냐하면 Kubernetes 컨트롤러를 작성하는 데 라이브러리 및 문서 지원이 많기 때문에 노드를 비우는 구성 요소를 Go로 작성하는 것이 유리했기 때문입니다.\n\n다음으로, 부분 간 통신 방식을 결정해야 했습니다. 일반적으로 노드에 작업을 표시하는 방법은 레이블을 지정하는 것입니다. 그러나 이 경우에는 taint를 사용하기로 결정했습니다. 장점은 taint를 한 번 설정하면 해당 노드에는 pod를 실행할 수 없으며, 추방된 pod가 해당 노드에 실행되는 것을 방지합니다. 따라서 노드 선택 작업은 노드에 taint를 설정하고, 비우기 작업은 노드 선택 작업에 의해 tainted된 노드를 식별하고 해당 pod를 비웁니다. 그런 다음 비워진 노드의 후속 종료는 클러스터 자동 확장기에 의해 처리됩니다.\n\n<div class=\"content-ad\"></div>\n\n# 거부된 방식\n\nAWS Lambda를 기반으로 한 노드 드레이너 구현체가 여러 가지 있습니다(예: https://github.com/aws-samples/amazon-k8s-node-drainer) 하지만, 우리는 클러스터 내에서 실행되도록 설계된 도구가 필요했습니다. 이는 이미 우리가 갖고 있는 모니터링 기능을 사용하기 위함입니다.\n\nAWS Auto Scaling 그룹 노드 새로 고침 프로세스를 기반으로 한 노드 드레이닝을 구현한 프로젝트들이 이미 존재하며, https://github.com/rebuy-de/node-drainer가 대표적인 예입니다. 이 프로젝트를 고려해 보았지만, 해당 프로젝트는 2023년 3월 이후에 어떠한 업데이트도 릴리스하지 않았기 때문에 채택하는 것이 리스크가 될 수 있습니다. 만약 이 프로젝트나 관련 종속성에 심각한 취약점이 발견된다면, 업데이트된 이미지를 얻을 수 없게 될 수 있으며, 우리 자신의 포크를 유지보수해야 할 수도 있습니다.\n\n# 구현\n\n<div class=\"content-ad\"></div>\n\n해결 방법의 두 번째 부분은 nodes를 감시하고 지정된 draining taint가 설정되면 node의 포드를 제거하기 시작하는 Kubernetes 컨트롤러로 구현되었습니다. 포드가 떠날 때까지 노드를 제거하는 작업은 타임 아웃 될 때까지 계속 됩니다 (가끔 포드가 떠나고 싶어하지 않을 수도 있어요) 또는 타임아웃 시간이 경과할 때까지입니다. 일정 시간이 지나면 그 노드를 다시 시도합니다. 프로그램은 항상 노드 제거를 직렬화하고, 노드를 처리한 후 클러스터에 보류 중인 포드가 없을 때까지 기다린 후 계속합니다. 이는 클러스터의 용량 문제를 최소화하기 위해 수행됩니다 - 모든 포드가 제거를 위해 tainted되어 있으면 새로운 노드가 생성될 때까지 새로운 포드를 시작할 수 없습니다.\n\n![이미지](/assets/img/2024-06-19-StreamliningMachineImageUpdatesAutomatingNodeRotationinKubernetesClusters_1.png)\n\n# 첫 번째 오픈 소스 기여물로 발표 결정\n\nSage AI 팀은 우리가 내부적으로 개발한 프로젝트들을 확인하여 일반 커뮤니티에 혜택을 줄 수 있다고 판단한 프로젝트들을 식별하기로 결정했습니다. 우리 인프라 팀은 MLOps 분야에서 많은 훌륭한 작업을 수행했고, 위에서 설명한 워크플로우의 자동화가 오픈 소스 커뮤니티에 공개되었다는 것을 자랑스럽게 발표합니다!\n\n<div class=\"content-ad\"></div>\n\n우리의 방식은 내부에서 이미 1년 넘게 사용되어 왔고, 이제 코드는 여기에서 찾을 수 있습니다; https://github.com/sageailabs/ektopistis .\n\n관심이 있으시면 자유롭게 살펴보고, 유용하다고 판단되면 귀하의 환경에서 사용하거나 피드백이나 코드로 기여해 주시기 바랍니다!\n\n프로젝트 README의 설치 섹션의 지침을 따라 Helm을 사용하여 클러스터에 설치할 수 있습니다. 노드 선택 구성 요소의 의미론은 임의적이며, 원하는 선택 기준과 태깅 의미론에 기반하여 구성할 수 있습니다. 예를 들어, AWS 오토 스케일링 그룹에서 시작된 모든 클러스터 노드를 표시하고 설정이 ASG의 론칭 템플릿과 일치하지 않는 스크립트가 있습니다.\n\n우리는 우리의 도구를 공유함으로써 더 나은 해결책으로 이어질 것이라고 믿습니다. 귀하의 피드백과 기여를 기대하며, 즐거운 협업을 기대하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 계획된 미래 작업에 대한 참고사항\n\n저희 노드 드레이너 구현은 중요한 작업 부하의 업데이트를 방해하지 않기 위해 클러스터에 보류 중인 파드가 있을 때 작업을 중지합니다. 그러나 이에는 중요한 하다고 할 수 있는 단점이 있습니다. 클러스터가 클수록 활동이 많을수록 파드의 순환율이 높아집니다. 이로 인해 보류 중인 파드가 계속 발생하여 노드 드레이너의 현재 버전이 멈춰있는 장기간이 발생할 수 있습니다. 저희는 이 대기 정책을 개선하여 프로세스를 가속화하는 계획이 있습니다.","ogImage":{"url":"/assets/img/2024-06-19-StreamliningMachineImageUpdatesAutomatingNodeRotationinKubernetesClusters_0.png"},"coverImage":"/assets/img/2024-06-19-StreamliningMachineImageUpdatesAutomatingNodeRotationinKubernetesClusters_0.png","tag":["Tech"],"readingTime":5}],"page":"18","totalPageCount":98,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}