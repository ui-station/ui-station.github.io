{"pageProps":{"posts":[{"title":"하위 도메인 탈취 기술 마스터하기","description":"","date":"2024-05-18 20:59","slug":"2024-05-18-MasteringSubdomainTakeovers","content":"\n\n여기 저는 서브도메인 탈취를 통해 바운티를 받은 방법을 설명할게요 -\n\n서브도메인 탈취는 의도된 소유자가 더 이상 사용하지 않는 서브도메인(예: mail.example.com과 같은 대형 웹사이트의 일부)에서 발생하는 취약점입니다. 이는 다양한 이유로 발생할 수 있습니다. 예를 들면:\n\n- 포기된 서비스: 서브도메인이 더 이상 제공되지 않는 서비스에 사용되었을 수 있어 비활성 상태가 될 수 있습니다.\n- 잘못된 구성: 도메인 또는 서브도메인 관리 중의 실수로 인해 청구되지 않은 서브도메인이 만들어질 수 있습니다.\n\n바운티를 받는 방법 -\n\n<div class=\"content-ad\"></div>\n\n**단계 1:** 먼저 대상 도메인을 선택하고 잠재적인 서브도메인을 식별하는 프로세스를 시작하세요. Subfinder와 같은 도구를 사용하여 ' -sc 404 ' 플래그와 결합하여 소유되지 않은 서브도메인을 정확하게 파악하세요. 잠재적인 인수 시 필수적인 것입니다. 실행할 초기 명령은 다음과 같습니다:\n\n```js\nsubfinder -d example.com | httpx -sc 404 | tee list.txt\n```\n\n**단계 2:** 404 응답 코드로 표시된 각 서브도메인을 수동으로 검토하세요. 제공된 단서 또는 정보에 주의를 기울이며, 특히 소유되지 않은 S3 버킷이나 다른 관련 세부 정보의 표시에 유의하세요. 또한 'dig' 명령을 사용하여 CNAME (Canonical Name) 레코드를 조사하세요. 예를 들어, dig 명령을 활용하여 CNAME을 검색하여 원본 서브도메인이 어디로 이동되는지 확인할 수 있습니다. 이 단계는 성공적인 도메인 인수에 중요합니다.\n\n```js\ndig mail.example.com\n```\n\n<div class=\"content-ad\"></div>\n\n\n![MasteringSubdomainTakeovers](/assets/img/2024-05-18-MasteringSubdomainTakeovers_0.png)\n\n**단계 3:** 이제 이 GitHub 저장소를 확인하세요. 거기에 있는 테이블에는 취약한 cname 목록이 표시되어 있습니다. 확인할 수 있습니다. cname이 취약한지 확인할 수 있습니다.\n\n더 간단한 방법은 Nuclie를 사용하는 것입니다. \"takeover\"라는 템플릿이 있어 도메인을 빼앗을 수 있는지 확인하는 데 도움이 됩니다. 그러나 자동화 도구는 실수를 할 수 있으므로 때로는 수동으로 두 번 확인하는 것이 좋습니다. Nuclie를 사용하려면 다음 명령어를 입력하세요:\n\n```js\nnuclei -l subdomain_results.txt -t <nuclei_template_path> -o results.txt\n```\n\n<div class=\"content-ad\"></div>\n\nNuclei Template1, Nuclei Template2\n\n만약 인수 가능성이 있다면, 그를 달성하기 위한 다양한 전략을 탐색해보세요. 고려할 몇 가지 방법을 설명해 드리겠습니다.\n\n- unbouncepages.com — 이 문제는 많은 웹사이트에서 상당히 일반적이며 쉽게 보상을 받을 수 있습니다. 구독 가치 약 $100~$150이 필요하기 때문에 번금을 청구할 필요가 없습니다. 그냥 신고하고 고가 때문에 실제 인수가 어려울 수 있다는 사실을 컨셉 증명서에 언급하면 됩니다. 그것만으로 번금을 받을 수 있고, 아마도 첫 번째 번금일 수도 있습니다.\n\n![이미지](/assets/img/2024-05-18-MasteringSubdomainTakeovers_1.png)\n\n<div class=\"content-ad\"></div>\n\n보고서를 제출하는 것이 버그 바운티 프로그램에 중요하다는 것을 명심해주세요. 그래서 올바른 용어를 알아 사용하여 좋은 바운티를 받을 기회를 극대화하는 것이 중요합니다.\n\n- 요청하지 않은 s3 버킷들 —\n\"NoSuchBuckets\"와 같은 메시지가 표시되는 하위 도메인을 찾으면 큰 성과입니다. AWS에 로그인한 후 동일한 이름의 버킷을 생성하고 동일한 지역에 있는지 확인하세요. \"공개 액세스 차단\" 옵션을 선택 해제하는 것을 잊지 마세요. 그런 다음 버킷을 요청하고 HTML 파일을 업로드하여 표시할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- Azure — CNAME이 “.cloudapp.net” 또는 “.azurewebsites.net”로 끝난다면 취약합니다. Microsoft Azure로 이동하여 로그인하세요.\n\nCase I — .cloudapp.net\n\n단계 1: Azure 포털로 이동합니다.\n\n단계 2: 메뉴에서 “Cloud Services (classic)”를 선택합니다.\n\n<div class=\"content-ad\"></div>\n\n**단계 3:** \"추가\" 버튼을 클릭하여 새 클라우드 서비스를 생성합니다.\n\n**단계 4:** 서비스 이름, 구독, 리소스 그룹, 위치 등 필요한 세부 정보를 입력합니다.\n\n**단계 5:** 적절한 구성 옵션과 배포 모델을 선택합니다.\n\n![이미지](/assets/img/2024-05-18-MasteringSubdomainTakeovers_4.png)\n\n<div class=\"content-ad\"></div>\n\nCase II — .azurewebsites.net\n\n단계 1: Azure 포털에 액세스합니다.\n\n단계 2: \"앱 서비스\"로 이동합니다.\n\n단계 3: \"새 웹 앱 만들기\"를 클릭합니다.\n\n<div class=\"content-ad\"></div>\n\n### 단계 4: 필요한 세부 정보를 제공하세요. 이름, 구독, 리소스 그룹 및 지역을 입력해주세요. 교체하려는 경우 기존 이름과 일치하도록 합니다.\n\n### 단계 5: 웹 앱이 생성되면 대시보드로 이동하세요.\n\n### 단계 6: 배포 옵션에 액세스하고 FTP, Git 또는 Azure 파이프라인과 같은 선호하는 배포 방법을 선택하세요.\n\n### 단계 7: 배포 패키지를 업로드하거나 연결한 후 배포 프로세스를 시작하세요.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-18-MasteringSubdomainTakeovers_5.png)\n\n![이미지](/assets/img/2024-05-18-MasteringSubdomainTakeovers_6.png)\n\n서브도메인을 소유하고 관련 보상을 받을 수 있는 다양한 방법이 있습니다. 한 가지 제안은 Google이나 이전 보고서에서 관련 내용을 검색하여 대체 CNAME 레코드를 탐색하는 것입니다. 이를 통해 가치 있는 정보를 얻을 수 있을 수도 있습니다.","ogImage":{"url":"/assets/img/2024-05-18-MasteringSubdomainTakeovers_0.png"},"coverImage":"/assets/img/2024-05-18-MasteringSubdomainTakeovers_0.png","tag":["Tech"],"readingTime":4},{"title":"할아버지는 은행을 믿지 않았다 이제, 디지털 뱅킹이 사람들에게 똑같은 감정을 일으킨다","description":"","date":"2024-05-18 20:57","slug":"2024-05-18-MyGrandfatherDidntTrustBanksNowDigitalBankingMakesPeopleFeeltheSameWay","content":"\n\n![image](/assets/img/2024-05-18-MyGrandfatherDidntTrustBanksNowDigitalBankingMakesPeopleFeeltheSameWay_0.png)\n\n본 이야기에는 금융 관련 조언이 포함되어 있지 않으며, 제휴 링크도 없습니다.\n\n저는 애플에서 여러 해 일하면서 애플 스토어에서도 근무했었고, 고객들이 장치를 어떻게 사용해야 하는지에 대해 매일 수십 개의 질문을 받았습니다. 심지어 수업도 가르쳤어요. 온라인 뱅킹에 대한 이야기가 많이 나왔는데, 시간이 지남에 따라 도움이 되는 몇 가지 제안을 알아가면서 공유하고 싶다고 생각했습니다. 할아버지 이야기부터 시작하겠지만, 이는 노인들에 관한 이야기가 아닙니다. 각 세대가 이 주제에 어려움을 겪는 모습을 직접 목격했습니다.\n\n## 할아버지는 옛 방식으로은 뱅킹을 했습니다.\n\n<div class=\"content-ad\"></div>\n\n은행에 간 적이 있던 유일한 경우는 월급을 현금화하기 위한 것이었습니다. 그는 그 돈을 모두 매트리스 아래에 넣어두었습니다. 청구해야 할 청구서가 있으면, 그는 사업장으로 가서 현금을 전달했습니다. 차를 포함한 물건을 살 때도 현금으로 지불했습니다. 돈이 없으면 사지 않았습니다.\n\n그는 한 가지 양보를 했는데, 바로 정부에 세금을 지불해야 할 때였습니다. 수표부도 없었기 때문에 돈 지급을 위해 서부연합에 가곤했습니다.\n\n부모님은 수표부와 은행 계좌를 가졌지만, 청구서를 지불하는 방식은 별 차이가 없었습니다. 은행 계좌가 있어서 숨길 돈은 없었지만, 그들 또한 여전히 모두에게 현금으로 지불하길 원했습니다. 어머니와 함께 은행에 가는 모습이 기억나는데, 거기서 현금으로 지불할 수표를 썼습니다. 그 때는 ATM이 없었습니다.\n\n오늘은 매우 달라졌습니다. 내 지갑에 현금이 있던 마지막 시간을 기억할 수가 없어요. 모두 카드나 앱을 사용하거나, 시장 앞에서 과자를 판매하는 걸스카우트조차도 그렇습니다.\n\n<div class=\"content-ad\"></div>\n\n현금을 사용하는 유일한 시간은 복권을 사는 드문 경우일 때뿐이에요. 복권 관계자들은 똑똑해요. 카드는 받지 않아요. 그 대신, 장보는 돈을 내고 현금을 받아오면, 바로 매장 앞의 복권 판매기로 가요.\n\n우리는 또한 온라인으로 청구서를 지불해요. 모두가 손쉽게 당신의 돈을 받을 수 있는 지불 포털을 갖고 있어요. 당신의 은행을 연결하고 몇 초 안에 지불하세요.\n\n내 할아버지가 느낀 대로 사람들은 아직 은행을 신뢰하지 않고, 무엇보다 디지털 뱅킹을 신뢰하지 않아요. 해킹, 사기, 신분 도용의 이야기를 듣고 나서 그런 부분에 대해서는 관련이 없다 생각해요. 그리고 온라인전용 은행의 증가가 상황을 해결하지 못하게 만들어요.\n\n일부 사람들은 수표와 현금이 여전히 물건을 지불하는 더 나은 방법이라고 믿어요. 그들이 틀렸다는 게 너무나 안타까운게요, 상황이 많이 바뀌었거든요.\n\n<div class=\"content-ad\"></div>\n\n## 아직 수표를 사용 중이라면 문제가 있을 수 있어요.\n\n이제 거의 모든 곳에서는 수표를 받지 않아요. 은행조차 수표 처리 비용이 더 많이 소요되기 때문에 가맹점들이 수표 수령을 자제하도록 권고하고 있어요. 은행들은 상인들이 카드를 받아들이도록 하기 위해 수표 수수료를 올리기도 했어요.\n\n수표는 여러분이 생각하는 것만큼 안전하지 않아요. 여러분의 수표는 해커가 계정을 털기 위해 필요한 모든 정보를 제공해요. 수표는 계정 번호, 은행 라우팅 번호, 이름, 주소를 나타내거든요. 주소가 없더라도 쉽게 온라인에서 찾을 수 있어요.\n\n수표장도 훔치면 일이 더 간단해져요. 모바일 뱅킹 앱을 통해 앞뒤를 촬영하여 수표를 입금할 수 있죠. 도용범이 여러분의 서명을 맞추지 못해도 문제가 되지 않아요. 도둑은 단지 돈을 다른 계정으로 넣어 출금할 수 있는 만큼만 잠시 보관하면 돼요.\n\n<div class=\"content-ad\"></div>\n\n나는 할아버지처럼 현금만 사용해서 벗어날 수 있을까 궁금했지만, 수표보다 문제가 더 많았어. 우리 모두는 할아버지가 돈을 숨겨 둔 곳을 잘 알고 있었으며, 몇 장을 몰래 꺼내도 괜찮겠지만, 그가 얼마나 많은 돈을 가졌는지 정확히 알지 못할까 두려워했어. 할아버지는 가끔씩 현금을 옮긴다고 주장했지만, 믿는 사람은 아무도 없었어.\n\n이건 결국 누군가가 그의 집에 침입하면 그를 금방 털 수 있다는 뜻이었어. 또한 그가 종종 주머니에 많은 양의 현금을 들고 다니며, 좋은 동네에 살지 않는다는 뜻이기도 했어.\n\n좋은 곳을 찾아 숨길 수도 있겠지만, 문제는 이런 게 있다: 강도들이 너를 속였다는 거야. 책 사이에 지폐를 넣는 건 아마추어 수준이야. 화장실 물조절기나 냉장고 안에 숨기는 것도 마찬가지야.\n\n대마초 흡연자들이 도와줄 수도 있을 거야. 그들은 가짜 비어 있는 캔, 병, 그리고 다른 가정용품을 사용해 자신의 판매품을 감췄다. \"비밀을 폭로했다\"고 나에게 화내지 말아 줘. 이건 도둑질 기초 수업이니까.\n\n<div class=\"content-ad\"></div>\n\n## 이제, 디지턈 뱅킹에 대해 이야기해 봐요.\n\n디지턈 뱅킹의 장점에 대해서 언급해 드릴까요? 편의성은 디지턈 뱅킹에서 큰 장점 중 하나에요. 예를 들어, 더 이상 지점에 방문하지 않아도 되며, ATM을 이용할 필요도 없어요.\n\n수표를 입금하거나 어디에서나 송금하는 것도 편해요. Venmo, Paypal, Zelle과 같은 앱을 통해 간편하게 송금할 수도 있어요. 이들을 이용해 도둑이 당신을 노릴 수 있다고 이야기했지만, 도와줄 수 있는 안전장치가 있어요.\n\n청구서를 예약할 수도 있어요. 만기일에 맞춰 청구서를 스케줄링하여 한 번에 모두 지불한 뒤 잊어버릴 수도 있어요. 자동 결제도 도와줄 거예요. 이제 무언가를 지불했는지 기억해야 하는 수고는 없어요.\n\n<div class=\"content-ad\"></div>\n\n많은 사람들이 월급을 직접 예금으로 받지만, 여러분은 자동으로 월급을 다른 계좌에 나눠 넣을 수 있다는 사실을 알고 계셨나요?\n\n디지털 뱅킹을 통해 금융 기관이 뭔가 잘못되었거나 의심스러운 점이 있을 경우 알려주고, 잔액을 신속하고 쉽게 확인하며 이체된 결제 내역을 확인할 수 있습니다.\n\n## 나는 디지턈 화폐에 대한 두 가지 주요 이유로 사람들이 거리낌을 느끼는 것을 알았어요: 보안과 복잡성.\n\n이전에 언급한 바와 같이, 이는 기술에 어려움을 겪는 노인들과 관련이 없습니다. 자녀들도 자신의 비밀번호를 기억하거나 이중 인증을 사용하려는 점에서 탁월하지 않습니다. 젊은 사람들이 기술에 둘러싸여 자라고 있기 때문에 이점이 있다고 해서 유리한 것은 아닙니다고 진술했어요. 그들도 결국 여러분과 마찬가지로 소비자일 뿐이에요.\n\n<div class=\"content-ad\"></div>\n\n당신의 신원이 도용되는 건 대개 당신이 실수를 한 결과입니다. 아마도 자기가 너무 기술에 능숙하다고 생각해서 사기에 속지 않을 거라 생각할 수도 있지만, 돈을 훔치는 방법들은 점점 더 정교해지고 있는 중이에요. 전 세계적인 거대 기업의 부사장인 친구가 있었는데, 최근에 6,000달러를 잃었던 사기 사건을 해결하는 데 도와줬어요. 그의 문제는 뭘까요? 약한 암호였어요.\n\n그는 혼자가 아니에요. 보안 소프트웨어 업체인 노튼에 따르면 확인된 침입 중 80% 이상이 약한 암호 때문에 발생했습니다. 노튼은 또한 사고 후 60%의 사람들이 암호를 강화하는 대신 처음부터 견고하게 만들지 않았다고 말하고 있어요.\n\n저는 애플 스토어에서 아이폰 설정을 돕기도 했죠. 비록 비추한 것이라고 제안했음에도, 많은 사람들이 6자리 대신 네 자리 숫자로 폰을 잠금해 해제하려 선택하고는 1234나 1111과 같은 암호를 사용했어요. 그 후 폰이 그 암호가 진정으로 원하는 암호인지 묻는데, \"쉽게 알아맞힐 수 있다\"고 알려줬지만 여전히 모두가 \"예\"를 눌렀어요.\n\n여러 사이트에서 동일한 암호를 재사용하나요? 노튼에 따르면 암호를 재사용하는 사람들 때문에 사고가 발생한다고 하네요. 이 통계는 우리 중 91%가 암호를 재사용하는 것은 보안 위험이라는 걸 알고 있다는 점에서 두드러지네요!\n\n<div class=\"content-ad\"></div>\n\n애완동물의 이름이나 생일을 비밀번호로 사용하는 것도 좋지 않은 생각이에요. 이것 역시 사기꾼들에게는 너무 쉬운 방법이죠.\n\n생활 속에서 기억해야 할 것들이 많고, 다른 것들을 쉽게 기억하는데 왜 비밀번호 하나가 어려울까요? 아마도 \"잠깐만요. 비밀번호가 몇 개, 아니면 수백 개일지도 몰라!\"라고 말하실지도 모르겠네요.\n\n여기에서 1Password나 LastPass 같은 비밀번호 관리자가 필요합니다. 각 사이트마다 복잡한 비밀번호를 만들고, 앱이 그걸 대신 기억해줍니다. 앱을 열려면 하나의 비밀번호만 기억하면 되죠. 이것이 누군가가 찾을 수 있도록 패스워드를 책에 적거나 이전 패스워드를 섞어 써 놓은 종이 조각에 메모하는 것보다 훨씬 나아요.\n\n비밀번호 관리자가 없더라도 기기들이 비밀번호를 저장할 것인지 묻는 경우가 있을 거에요. 이는 괜찮은 일이에요. 왜냐하면 로그인 정보가 강력하고, 이중 요소 인증을 사용하고 있으니까 말이죠? 제가 디지털 자산을 보호하는 실용적인 방법들을 마지막에 다룰게요.\n\n<div class=\"content-ad\"></div>\n\n## 이제, 복잡성에 대해 이야기해 봅시다.\n\n이미 디지턈 생활의 가장 어려운 부분인 비밀번호에 대해 다뤘어요. 다른 어려운 부분은 모든 것을 사용하는 법을 배우는 것이에요. 휴대폰을 해제하면 어디에나 버튼이 많아서 당황스러울 수 있어요. 이런 현상은 당신이 그 모든 버튼이 하는 일을 알아야 한다고 생각하기 때문에 발생합니다.\n\n하지만 사실은, 알 필요가 없어요. 예를 들어보죠. 집 주변에 리모컨이 많이 떨어져 있을 것 같아요. 하나 들어서 보고, 그 모든 버튼을 다 알고 있는지 스스로 물어보세요. 제안해 보는데, 여러분은 알지 못할 것이라고 확신해요. 이것이 제가 말하려는 바에요. 당신은 자신이 좋아하는 쇼를 보는 데 충분한 지식만 알고 있으면 돼요.\n\n앱과 기기도 마찬가지에요. 중요한 일을 해내기 위한 충분한 지식만 알면 돼요. 나이가 무엇이든지, 새로운 일을 배우고 있는데, 그럼 왜 휴대폰 앱을 배우는 것이 다른 거라 생각하는 건지요?\n\n<div class=\"content-ad\"></div>\n\n## 결론: 스스로를 보호할 수 있는 방법은 많이 있습니다.\n\n몇 달마다 비밀번호를 변경하십시오. 이중 인증을 활성화하십시오. 이것은 코드가 확인을 위해 다른 장치 중 하나로 보내지는 곳입니다. 알아요, 알아요, 로그인하려면 장치가 서로 가까이 있어야 하는 것이 귀찮은 일이지만, 자신의 돈을 아껴보세요.\n\n계정에 활동이 있을 때 이메일, 문자 및 푸시 알림을 설정하여 알림을 받으세요. 전화로 누군가에게 은행 정보를 제공하지 마십시오. 아무도 그 정보가 필요하지 않습니다. 누군가가 카드 번호를 확인하라고 요청하면, 그들이 보유한 것을 읽어보고 일치하는지 확인하라고 요청하십시오.\n\n금액이나 빚을 기프트 카드로 지불하지 마십시오. 예를 들어, IRS가 왜 기프트 카드로 지불하길 원할까요? 안타깝게도 이에 속은 친구가 있습니다. 애플 스토어에서 우리는 사기당하는 사람들을 돕기위해 기프트 카드가 어떻게 사용될지 물어보는 교육을 받았습니다.\n\n<div class=\"content-ad\"></div>\n\n공항, 커피숍, 도서관 및 호텔에서 사용할 때 공공 와이파이를 사용해서는 절대 은행 업무를 하거나 물건을 구매하지 마세요. 이메일이나 문자 메시지에서 은행 링크를 클릭하거나 전화번호를 걸지 마세요.\n\n요즘은 돈을 다루는 방식에는 긍정적인 면과 부정적인 면이 있습니다. 이러한 방식을 조합하는 방법이 분명히 있습니다. 중요한 것은 어떻게 작동하는지 알지 않고 어떤 방식을 무시하지 않는 것입니다. 제게는 정보에 기반한 결정을 내리는 것이 목표입니다.","ogImage":{"url":"/assets/img/2024-05-18-MyGrandfatherDidntTrustBanksNowDigitalBankingMakesPeopleFeeltheSameWay_0.png"},"coverImage":"/assets/img/2024-05-18-MyGrandfatherDidntTrustBanksNowDigitalBankingMakesPeopleFeeltheSameWay_0.png","tag":["Tech"],"readingTime":6},{"title":"탈취 가능한 사이버 보안 취약점 약 1,000여 개가 MITRE, NIST에서 놓칠 수도 있지만 중국 또는 러시아는 놓치지 않았을 수도 있어요","description":"","date":"2024-05-18 20:56","slug":"2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt","content":"\n\n# 우리는 누구인가요?\n\nA.R.P. Syndicate은 고객이 대상, 취약점 및 위협에 대한 정보를 집중 및 탐색하는 데 도움을 주는 글로벌 사이버보안 인텔리전스 및 연구 회사입니다.\n\n![](/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png)\n\n# Exploit Observer의 VEDAS란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\nVEDAS는 취약점 및 악용 데이터 집계 시스템의 약자입니다. Exploit Observer의 기술이며, 우수한 취약성 + 악용 크롤링 및 상관 기능으로 유명합니다.\n\n이 인텔리전스는 현재 초기 단계이며 실험 단계이기 때문에 거짓 긍정으로 조작될 수 있습니다. 그러나 모든 AI 시스템과 마찬가지로 시간이 지남에 따라 진화하고 있으며, 우리는 그 미래에 대해 매우 희망적입니다.\n\n### 왜 거짓 긍정을 출력하는 시스템을 믿어야 할까요?\n\n우리의 주장은 소중하게만 믿어지면 안 됩니다. 이와 관련하여, 어떤 자동 보안 시스템의 재앙적인 실패는 수백 개의 거짓 긍정 (1형 오류)이 아닌 몇 개의 거짓 부정 (2형 오류) 때문에 발생합니다.\n\n<div class=\"content-ad\"></div>\n\n사이버 보안 주변의 자동화 시스템 대부분은 잘못된 긍정 사례를 제거하는 데 너무 집중하고 있습니다. 이 행동은 결과적으로 잘못된 부정 사례의 수가 증가하게 됩니다. 우리의 목표는 항상 잘못된 긍정 사례보다 잘못된 부정 사례를 제거하는 것입니다.\n\n항상 독립적인 연구자들이 저희의 작업을 테스트, 확인하고 비평하도록 권장합니다. 어떠한 불일치 사항이라도 https://github.com/ARPSyndicate/puncia/issues에서 보고할 수 있습니다.\n\n# MITRE, NIST & CNA 파트너들이 놓칠 수도 있는 취약점을 찾아보세요\n\n취약점 목록은 Exploit Observer의 API를 통해 접근할 수 있습니다 —\n\n<div class=\"content-ad\"></div>\n\n- CVE가 없는 BDU: https://api.exploit.observer/russia/noncve\n- CVE가 없는 CNVD / CNNVD: https://api.exploit.observer/china/noncve\n\n해당 VEDAS 클러스터의 URL 목록을 반환합니다. VEDAS 클러스터는 CVE 식별자와는 달리 단일 또는 일정한 취약성에 매핑되지 않는 VEDAS 식별자로 표시됩니다. 이 클러스터는 자체 조절되며 더 많은 데이터가 집계될 때 매우 발전할 가능성이 높습니다.\n\n# CVE가 무엇이든 놓치지 않는 방법은 무엇인가요?\n\nCVE 생태계에는 가치 있는 정보를 생성하는 370개 이상의 CNA 파트너가 하루 24시간 열심히 입력을 생성합니다. CVE 데이터베이스에서는 매일 많은 변화가 일어나고 시간이 지날수록 더욱 개선됩니다.\n\n<div class=\"content-ad\"></div>\n\n그럼에도 불구하고, 그것은 우리에게도 여전히 수수께끼인데, 실제 영향이 있는 많은 취약점들이 공개적으로 접근 가능하지만 CVE가 할당되지 않은 것이 많습니다.\n\n# 중국과 러시아는 훨씬 놓쳤습니다. 누구나 그들보다 더 우려해야 하는 이유가 있을까요?\n\nCVE 생태계는 실제로 개선되고 있습니다. 하지만 아직 최상이 아닙니다! 우리는 중국과 러시아가 아무것도 놓치지 않는다고 믿습니다. CVE가 놓친 것들을 커버하기 위해 더 많은 보호를 제공하기 위해서 요구됩니다.\n\n게다가, CNITSEC & FSTEC가 좁은 초점을 가지고 있음에도 불구하고, 그들은 국내 및 전 세계 취약성 생태계에 대한 이해력, 통제 및 커버리지가 외부 세계에 공개하는 만큼 크게 가지고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n또한 글로벌 보안 팀들은 여전히 CNVD/CNNVD/BDU보다 CVE에 더 의존하고 있습니다. 따라서, 각 능력 있는 보안 팀들에게는 우려스러운 문제이며, 이 문제를 해결하기 위해 상업적 및 무료 취약점 및 공격 데이터베이스를 최대한 통합할 필요가 있습니다.\n\n# Exploit Observer가 이러한 문제 해결에 어떻게 도움이 될까요?\n\nExploit Observer는 인터넷에서의 취약점/공격 데이터를 집계하고 해석합니다. 결과적으로, 이는 세계 최대의 공격 및 취약점 인텔리전스 데이터베이스로 발전하였으며 모든 사용자에게 무료로 제공됩니다.\n\n![이미지](/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_1.png)\n\n<div class=\"content-ad\"></div>\n\n어떤 보안팀도 이것을 사용하여 새로운 취약성과 악용을 모니터링할 뿐만 아니라 패치 및 악용을 우선순위로 정하는 데 쉽게 활용할 수 있습니다.\n\n사실, 우리의 평가 알고리즘은 CVSS, EPSS 및 Vulners AI Score와 같은 표준과 완전히 독립적입니다. 우리의 기준은 심각도를 고려하지 않습니다. 고려하는 유일한 요소는 \"최소한의 지식으로 상대방이 특정 취약성을 얼마나 쉽게 악용할 수 있는가?\" 를 양적으로 표현하려고 하는 것입니다.\n\n우리의 모든 작업은 현재 굉장히 실험적이며 맹목적으로 의존해서는 안 됩니다!\n\n더 궁금한 점이 있다면 언제든지 문의해 주세요.","ogImage":{"url":"/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png"},"coverImage":"/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png","tag":["Tech"],"readingTime":3},{"title":"중간 과정 프롬프트를 A부터 Z까지 최상위 사진 용어로 변환하기","description":"","date":"2024-05-18 20:52","slug":"2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ","content":"\n\n사진 효과와 기술에 대한 A-Z 안내서에 오신 것을 환영합니다. 이것은 미드저니 작품을 향상시키기 위해 디자인된 가이드입니다. \"추상\"부터 \"줌 버스트\"까지, 이 기사는 다양한 시각적 표현의 팔레트에 대해 심층적으로 다룹니다.\n\n경험 많은 사진 작가이든 호기심 많은 초보자이든, 이 용어들은 여러분의 프롬프트를 풍부하게 만들어 주며 창의력을 넓히게 도와줄 것입니다. 이 변형적 기술을 함께 탐험하며, 디지털 예술 작품에서 새로운 차원을 열어보세요. 이미지에 혁신과 풍미를 부여할 준비가 되셨나요?\n\n# 추상\n\n추상 사진은 전통적인 현실주의와는 다르게 색상, 모양 및 패턴에 집중하여 종종 현실 세계의 대상과 거의 비슷하지 않은 이미지를 만드는 흥미로운 작품을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n형태, 선, 질감과 같은 주요 구성 요소를 강조하여 주제를 전형적인 맥락에서 추상적으로 처리하고 관행적인 지각에 도전하는 방식으로 제시합니다.\n\n극단적인 근접 촬영, 선택적 초점, 동적 흐림, 그리고 빛과 그림자의 상호 작용과 같은 기술이 이러한 효과를 달성하는 데 활용됩니다. 이러한 결과물은 그림이나 그래픽 아트와 유사하며 주관적 해석을 초대하며 종종 심금을 울리거나 사진 주제의 문자 그대로의 외관을 초월하는 깊은 감정이나 아이디어를 떠올리도록 합니다.\n\n추상 사진은 우리에게 예상치 못한 것을 표현하도록 장려하며, 질감 구조, 시각적 관점, 척도, 셔터 속도를 활용하여 생각을 자극하는 추상적 예술 작품을 만들게 합니다.\n\n[이미지 첨부](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png)\n\n<div class=\"content-ad\"></div>\n\n\n![image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_1.png)\n![image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_2.png)\n![image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_3.png)\n\n# Bokeh\n\n\n<div class=\"content-ad\"></div>\n\n볼케는 이미지의 초점을 맞추지 않은 부분에서 생산된 흐릿한 미학적인 효과로 유명한 사진 효과입니다. \"흐릿함\"이나 \"안개\"를 의미하는 일본어 단어에서 비롯되었으며, 보케는 일반적으로 사진 속에서 꿈틀거리는 분위기를 만듭니다.\n\n보케의 품질은 주로 카메라 렌즈 설계와 조리개 모양에 의해 결정됩니다. 둥근 조리개 블레이드를 가진 렌즈는 초점을 맞추지 않은 영역에서 더 매력적이고 부드럽고 둥근 흐림을 만들어냅니다.\n\n보케는 주로 초점 깊이가 얇은 초상화에서 인기가 많습니다. 이는 주제물을 부드럽게 흐린 배경에 두드러지게 만들어서 주제물을 두드러지게 하는 효과를 더해줍니다.\n\n<img src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n\n![This is an image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_5.png)\n\n![This is an image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_6.png)\n\n![This is an image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_7.png)\n\n# Contre jour\n\n\n<div class=\"content-ad\"></div>\n\nContre jour, 또는 역광은 카메라를 빛 원본 쪽으로 향하게 함으로써 대상을 그림자에 둔 채 실루엣 효과를 만들어내는 사진 촬영 기술입니다. 이 기법은 형태와 선을 강조하며 장면에 드라마나 신비로움을 불어넣어 빛과 어둠 사이의 높은 대비를 만들어냅니다.\n\n이 기술은 또한 대상 주변에서 헤일로 또는 테두리 조명 효과를 생성합니다. 초과 노출을 방지하기 위해 정확한 노출 제어가 필요하기 때문에 도전적이지만, 역광을 효과적으로 사용하면 화려하고 예술적인 결과물을 얻을 수 있습니다.\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_8.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_9.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_10.png) \n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_11.png) \n\n# Dutch Angle\n\n더치 앵글(Dutch Angle)은 카메라를 일부러 한 쪽으로 기울여 수평 선이 경사진 상태가 되도록 하는 기술입니다. 이 접근 방식은 혼란, 불안, 또는 역동성을 일으키며, 이는 스릴러와 호러와 같은 장르에서 심리적 긴장을 전달하는 데 좋아지게 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n보는 사람의 균형과 정상적인 시각을 단연 방해하여, 네덜란드 기욤은 객관적이거나 변형된 현실을 제공하여 종종 심리적 불안을 표현하거나 이미지에 독특한 스타일적 매력을 더합니다.\n\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_12.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_13.png)\n\n![image3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_14.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_15.png)\n\n# Edge Darkening\n\nEdge darkening, 또는 바이네팅(vignetting),는 이미지의 모서리가 중앙보다 어두워 보이는 효과입니다. 이는 렌즈의 특성으로 자연스럽게 발생할 수 있으며 종종 엣지에서 센서나 필름에 도달하는 빛이 적어지는 결과를 초래합니다.\n\n이 효과는 사진의 중요한 요소에 관전자의 주의를 끌기 위해 활용되며 자연스러운 프레임을 제공하고 이미지에 깊이나 드라마를 추가합니다. 때때로 결함으로 간주되기도 하지만, 의도적으로 적용할 때 바이네팅은 사진의 예술적 영향을 크게 향상시킬 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_16.png)\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_17.png)\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_18.png)\n![Image 4](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_19.png)\n\n\n<div class=\"content-ad\"></div>\n\n# Fisheye\n\n피시아이 효과는 특수 초광각 렌즈를 사용하여 달성되며, 화려하고 확장된 스타일의 사진을 제공합니다. 비주얼 왜곡이 심각하게 나타나는 피시아이 렌즈는 최대 180도까지의 매우 넓은 시야를 포괄하며 광활한 공간을 한 장면으로 압축합니다. 이로 인해 외곽에 곡선 형태의 선이 나오며 중앙의 물체가 확대되거나 강조될 수 있습니다. 피시아이 사진은 재미있고 초현실적인 요소로 인해 경치, 스포츠 및 창의적인 초상화에 대한 독특한 시각을 제공합니다.\n\n![이미지 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_20.png)\n\n![이미지 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_21.png)\n\n<div class=\"content-ad\"></div>\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_22.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_23.png)\n\n# Grain\n\n사진에서의 \"grain\"은 이미지에 시각적 질감이나 얼룩을 부여하는 효과로, 고속 필름에서 볼 수 있는 곡물 모양의 질감을 상기시킵니다. 이 질감은 필름 유화액에서 금속 은 입자의 분포로부터 발생하며, 높은 ISO 필름의 큰 입자가 더욱 뚜렷한 곡물을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n디지털 사진에서는 센서가 일반적으로 노이즈를 생성하지 않지만, 어두운 조건이나 높은 ISO 설정에서 비슷한 효과가 발생할 수 있습니다. 이는 이미지에 촉감 품질을 더해 줍니다.\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_24.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_25.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_26.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_27.png)\n\n# High Key\n\nHigh key photography is distinguished by its light, airy feel, with an emphasis on bright tones and minimal dark shadows. Achieved through careful lighting, exposure settings, and post-processing, this style features soft lighting and high exposures to wash out midtones and shadows, creating images dominated by whites and light colors.\n\nHigh key photography is often associated with conveying a positive, cheerful mood and is widely used in fashion, beauty, and portrait photography for its clean, minimalist aesthetic.\n\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_28.png)\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_29.png)\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_30.png)\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_31.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 적외선\n\n적외선 사진술은 적외선 필터나 개조된 카메라와 같은 전문 장비를 사용하여 맨눈으로는 보이지 않는 적외선 스펙트럼의 빛을 캡처합니다.\n\n이 기술은 독특하고 종종 초현실적인 이미지를 만들어냅니다. 살아있는 녹색 식물은 강한 적외선 광선 반사로 인해 밝은 흰색 또는 분홍색으로 나타나며, 하늘은 어두워져 구름과 식물과의 대비를 향상시킵니다.\n\n이는 풍경에 초월적이고 거의 초자연적인 퀄리티를 부여하여 익숙한 장면을 신비롭고 신탁스럽게 만들어냅니다. 처음에는 과학 및 군사 응용 분야로 개발되었지만, 적외선 사진술은 독특한 시각적 매력 때문에 예술적인 분야에서도 자리를 잡았습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_32.png)\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_33.png)\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_34.png)\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_35.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 자누스 효과\n\n로마 신 자누스를 영감으로 한 자누스 효과는 두 얼굴을 가진 신의 모습으로 유명한데, 사진에서는 하나의 이미지 내에서 두 가지 대조되는 요소를 캡처하거나 대표합니다.\n\n이 기술은 종종 오래된 것과 새로운 것, 밝음 대 어둠, 또는 성장과 부패와 같은 주제를 탐구하며, 대상의 이중적 성격을 강조하고 시청자들에게 주변 환경의 복잡성을 숙고하도록 장려합니다.\n\n![이미지](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_36.png)\n\n<div class=\"content-ad\"></div>\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_37.png)\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_38.png)\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_39.png)\n\n# 키를리안 효과\n\n<div class=\"content-ad\"></div>\n\n키를리안 효과는 1939년에 그를 발견한 세물론 키를리안의 이름에서 따왔어요. 이것은 사진 과정을 포함하는데, 이는 물체의 에너지 필드 또는 \"오라\"를 포착한다고 알려져 있어요. 특히 잎이나 손과 같은 유기물 아이템에 관한 거든요.\n\n이 기술은 고전압 원원에 연결된 사진판을 사용하는데요; 코로나 방전을 통해 형성된 결과 이미지는 물체의 에너제틱한 특성을 보여준다고 합니다.\n\n![사진](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_40.png)\n\n![사진](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_41.png)\n\n<div class=\"content-ad\"></div>\n\n![TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_42](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_42.png)\n\n![TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_43](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_43.png)\n\n# 롱 익스포저\n\n롱 익스포저 사진술은 카메라의 셔터가 일반적으로 열린 시간보다 길게 열려 빛이 더 많이 들어오도록 한 기술로, 단일 이미지 내에서 시간의 흐름을 캡처하는 것을 가능하게 합니다. 이 방법은 밤 풍경, 광선 뒤끝, 물의 흐름, 천체 사건과 같이 독특하고 종종 초현실적인 시각 효과를 만들어냅니다.\n\n<div class=\"content-ad\"></div>\n\n정적 및 동적 요소가 혼재된 시나리오에서는 롱 노출이 부드러운 빛 흔적이나 물 흐름을 포착하는 동시에 정지된 물체의 선명함을 유지할 수 있습니다. 보통 삼각대와 원격 셔터 릴리스를 사용하여 안정성을 보장하는 롱 노출 사진술은 사진에서 시간을 시각적 요소로 탐구할 수 있도록 해줍니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_44.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_45.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_46.png)\n\n<div class=\"content-ad\"></div>\n\n\\[![이미지](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_47.png)\\]\n\n# 멀티소닉 효과\n\n멀티소닉 사진 촬영은 연속해서 빠르게 번갈아가며 발광하는 플래시 펄스를 사용하여 한 장의 복합 이미지 내에서 여러 각도에서 움직임을 얼립니다. 이 기술은 다양한 움직임 단계를 캡처하여 동적인 장면의 독특한 시각과 포괄적인 관점을 제공합니다.\n\n\\[![이미지](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_48.png)\\]\n\n<div class=\"content-ad\"></div>\n\n\n<img src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_49.png\" />\n\n<img src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_50.png\" />\n\n<img src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_51.png\" />\n\n# Night Vision\n\n\n<div class=\"content-ad\"></div>\n\n밤 시야 사진술은 특징적인 녹색 단색으로 특징 지어지는 야간 시야 장비로 보는 효과를 모방하여 저조명 조건에서 시계바늘을 뚜렷하게 함으로써 시야를 높이는 기술입니다. 이 기술은 종종 대기 효과에 사용되거나 야간 활동 또는 감시와 관련된 주제적 문맥에서 사용됩니다.\n\n![Night vision photography](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_52.png)\n\n![Night vision photography](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_53.png)\n\n![Night vision photography](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_54.png)\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_55.png)\n\n# Overlay\n\n사진 및 디지털 이미징에서의 오버레이 기술은 한 이미지를 다른 이미지 위에 쌓아 두어 두 이미지의 요소를 합쳐 하나의 복합 이미지를 형성하는 것을 의미합니다. 이 방법은 서로 다른 시각적 구성 요소를 창의적이고 예술적으로 혼합하여 사진 작품에서 현실과 상상력을 융합하는 것을 가능하게 합니다.\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_56.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_57.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_58.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_59.png)\n\n# Pinhole Effect\n\n\n<div class=\"content-ad\"></div>\n\n핀홀 효과는 핀홀 카메라와 관련이 있으며 망원현 대신 작은 구멍을 통해 이미지를 캡처하여 모든 장면 요소가 동등하게 초점을 맞추는 무한한 깊이의 영역으로 이어집니다.\n\n이 효과는 핀홀 가장자리에서 발생하는 빛의 굴절로 인해 독특한 부드러움을 가진 이미지를 만들어 냅니다. 핀홀 사진술은 긴 노출 시간과 독특한 시각적 스타일로 알려져 있으며, 렌즈 기반 사진술에서 예상되는 선명함과 대비를 제공합니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_60.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_61.png)\n\n<div class=\"content-ad\"></div>\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_62.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_63.png)\n\n# 퀘이사르 버스트\n\n퀘이사르 버스트는 사후 처리에서 방사형, 오염된, 별 모양의 필터 효과를 추가하여 은구멍 중심에 있는 거대한 블랙홀인 퀘이사르의 광도가 높고 동적인 모습을 시뮬레이션합니다. 이 효과는 사진에 화려하고 우주적인 퀄리티를 부여하여 우주와 공상과학 주제를 향상하는 데 이상적입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_64.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_65.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_66.png)\n\n![Image 4](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_67.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 롤링 셔터 효과\n\n롤링 셔터 효과는 이미지 센서 라인의 순차적 노출로 인한 왜곡으로, 빠른 이동 중 발생하는 기울어짐이나 스며림과 같은 변형을 초래합니다. 이 현상은 전체 이미지를 동시에 캡처하는 글로벌 셔터와 대조적이며, 빠르게 움직이는 상황이나 카메라 자체가 이미지를 캡처하는 동안 빠르게 이동할 때 특히 두드러집니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_68.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_69.png)\n\n<div class=\"content-ad\"></div>\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_70.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_71.png)\n\n# 셀렉티브 컬러링\n\n셀렉티브 컬러링은 주로 흑백 이미지 내에서 특정 주제물을 컬러로 분리하여 시선을 집중시키고 사진의 특정 부분을 강조하는 기술입니다. 이 기술은 주요 요소에 빛을 발해 이야기의 영향력을 향상시키며, 이미지의 컬러부분과 흑백 부분 간의 획기적인 대조를 만들어냅니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_72.png)\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_73.png)\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_74.png)\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_75.png)\n\n\n<div class=\"content-ad\"></div>\n\n# 텍스처 오버레이\n\n텍스처 오버레이는 디지털 텍스처를 사진 위에 겹쳐서 모양을 변경하거나 향상시키는 작업입니다. 사진 작가들은 예쁜 또는 현실적인 텍스처(종이, 천, 녹슨 것 등)를 편집 소프트웨어를 사용해 이미지에 도입하여, 그것들을 혼합하여 사진 속에 심도나 테마적 공감을 만들어냅니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_76.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_77.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_78.png)\n\n# Unicolor\n\n“Unicolor” refers to the use of a single color in an image, often employed to create a uniform or thematic visual impact. When applied correctly, unicolor can evoke specific moods or artistic expressions, enhancing the conceptual unity of the image.\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_79.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_80.png)\n\n![image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_81.png)\n\n![image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_82.png)\n\n![image 4](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_83.png)\n\n\n<div class=\"content-ad\"></div>\n\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_84.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_85.png)\n\n![image3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_86.png)\n\n# Voronoi Map\n\n\n<div class=\"content-ad\"></div>\n\n보로노이 맵은 이미지를 일부 지점과의 근접성에 기초하여 셀들의 모자이크로 분해합니다. 각 셀은 원본 사진의 일부를 포함하며, 이 효과는 조각난 느낌과 패턴이 있는 외관을 만들어내어 유리창 같은 예술 작품을 연상시키며 촬영된 장면에 대한 색다른 관점을 제공합니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_87.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_88.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_89.png)\n\n<div class=\"content-ad\"></div>\n\n\n![Wet Plate Look](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_90.png)\n\nThe wet plate look replicates the style of early 19th-century wet plate photography, where a glass or metal plate coated with a light-sensitive emulsion was exposed while still wet. This method produced images with a distinctive soft focus and subtle imperfections like light leaks, offering a nostalgic and evocative quality reminiscent of historical photographs.\n\n![Wet Plate Look](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_91.png)\n\n\n<div class=\"content-ad\"></div>\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_92.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_93.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_94.png)\n\n# Xylography Effect\n\n<div class=\"content-ad\"></div>\n\nXylography, 또는 목판 인쇄는 목재 표면에 디자인을 새기고 잉크를 바르며 종이나 천에 옮기기 위해 누르는 것을 포함합니다. 이 기술은 굵은 선과 질감 있는 외관으로 특징지어지며 전통 인쇄 기술의 본질을 담아 독특한 예술적 효과를 낳습니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_95.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_96.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_97.png)\n\n<div class=\"content-ad\"></div>\n\n\n# 요코쵸 효과\n\n요코쵸 효과는 간단한 구조와 비네팅, 흐린 가장자리, 빛 누출과 같은 독특한 이미지 특성으로 알려진 빈티지 일본 장난감 카메라의 미학을 흉내냅니다. 이 효과는 꿈풍경적인 시각적 스타일을 만들어내며 종종 향수와 예술적 불완전함의 느낌을 담고 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n![이미지 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_100.png)\n\n![이미지 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_101.png)\n\n![이미지 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_102.png)\n\n# 줌 터보 효과\n\n<div class=\"content-ad\"></div>\n\n줌 버스트 효과는 조리개 렌즈의 초점 길이를 조정하여 얻어지며, 사진 속에서 움직임을 느끼게 합니다. 이 기술은 정지된 요소들이 프레임의 중심쪽으로 빠르게 다가오거나 멀어지는 것처럼 보이게 하며, 촬영된 장면에 에너지와 확장된 느낌을 더합니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_103.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_104.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_105.png)\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_106.png)\n\n# 결론: 당신의 창의적인 나침반\n\n우리가 사진 효과의 A부터 Z까지의 여정을 마무리하는 동안, 각 기술이 당신을 자극하여 당신의 Midjourney 프롬프트의 경계를 넓히도록 합시다. 강렬한 \"줌 버스트\"에서 신비로운 \"적외선\"까지 모든 것을 실험하여 이미지의 감정과 영향력을 깊게 하세요.\n\n기억하세요, 사진은 시각만큼이나 시각적인 것입니다. 이 도구들을 사용하여 보통 것을 환상적인 것으로 바꾸고, 창의적인 풍경의 끝없는 가능성을 탐험하시기 바랍니다. 계속해서 창작하고, 꿈꾸고, 당신의 예술을 빛나게 해보세요!\n\n<div class=\"content-ad\"></div>\n\n--- by 公众号: 小祸碎碎念\n\n💡Want a deeper dive? My Midjourney collection awaits you.\n\n## Loved the article?\n\nIf yes, then:\n\n<div class=\"content-ad\"></div>\n\n- 댓글 남기기\n- 업데이트 팔로우하기\n- 무료 이메일 알림","ogImage":{"url":"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png"},"coverImage":"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png","tag":["Tech"],"readingTime":21},{"title":"인공지능 회사들에 속지 않기 위해 예술가들이 작품에 함정을 설치하고 있어요","description":"","date":"2024-05-18 20:46","slug":"2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork","content":"\n\n![이미지](/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png)\n\n포스트된 Dall-E의 출현 이후에 Midjourney나 Stable Diffusion과 같은 다른 생성 이미지 처리 알고리즘들이 나오기까지 오랜 시간이 걸리지는 않았습니다. 그러나 큰 문제가 드러났습니다: 이러한 알고리즘을 만든 기업들이 이미지에 설명을 달고 라벨을 붙인 거대한 이미지 컬렉션을 축적하고, 그러고는 이를 바탕으로 알고리즘을 훈련시켰다는 것입니다.\n\n이러한 거대한 이미지 컬렉션은 어디서 얻었을까요? 대부분은 이미지 저장소 등의 웹 사이트를 스크랩핑함으로써 입수했습니다. Getty Images가 Stable Diffusion에 대한 소송을 제기함으로써 그들의 이미지 출처가 명백하게 드러났습니다. 많은 경우, 알고리즘은 그것을 이미지의 다른 부분으로 해석하여 그들의 워터마크의 왜곡 버전이 포함된 이미지를 생성하기도 했습니다.\n\n법적 문제는 분명합니다: 웹 상에 공개된 정보는 스크랩핑 대상이 될 수 있다는 것을 수년간 말해 왔습니다. 어떤 목적으로든 웹 페이지에 가서 그 내용을 모두 복사하는 권리를 달성하는 다양한 법적 선례들이 있습니다. 복잡성 때문에 해당 사례가 몇 년간 계속되어 대법원에 이르기도 하겠지만, 이 중요한 사안에 대해 저작물이 알고리즘 훈련에 사용된 작가들은 자신들의 작품이 쉽게 모방될 수 있다는 것을 보거나 누군가가 그들의 스타일을 시뮬레이션하여 새로운 이미지를 만들어낼 수 있다는 것을 보게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n법원이 도움을 많이 제공하지 않을 것이라는 점을 인정하면, 일부 예술가들은 자신들의 작품을 보호하기 위해 보비 트랩을 설치하고 있습니다. 이들은 소프트웨어로 처리된 이미지를 생성하여 알고리즘이 혼란스럽게 만드는 무시무시한 변경을 가하고 있습니다. 이 과정은 사람들의 얼굴을 사진이나 동영상에서 무시무시하게 수정해서 얼굴 인식 알고리즘을 방지하는 방식과 동일합니다. Atropa belladonna 식물(환각을 일으키는 식물)을 기리며 명명된 Nightshade라는 알고리즘은 수정된 사진을 게시할 수 있게 허용하며, 이러한 수정된 사진은 알고리즘이 실제 내용과 다른 설명을 생성하도록 합니다. 이 때문에 알고리즘이 혼란스러워져 결과물로 요청된 것과 다른 이미지를 제공합니다.\n\n결과적으로, 이는 아직도 기능을 수행하는 이미지를 “독”으로 오염시킨 것과 같습니다. 여전히 그림을 보고 예쁘게 고르고 예술가들이 설정한 조건에 따라 선택할 수 있지만, 알고리즘에 흡입되면 “환각”을 유발합니다. “독”이 가미된 이미지가 많을수록 알고리즘은 더 예측할 수 없어지며, 기업들은 알고리즘 교육에 사용하는 콘텐츠를 모니터링하기 위한 메커니즘을 구축하여 비용이 크게 증가합니다.\n\n이것은 이러한 유형의 도구를 만드는 기업에게 경고의 신호이며, 그들이 경고받은 문제들 중 많은 것을 설명합니다: 알고리즘을 쓰레기로 먹이면 쓰레기가 생성됩니다. 많은 경우, 투자자들에게 정당화하기 위해 결과를 너무 빨리 제공해야하는 기업들이 있는데, 이 때문에 어중간한 정보를 사용하게 되어 어떤 교육의 기초가 되어서는 안 되는 상황이 됩니다. 따라서 그들의 알고리즘은 덜 신뢰할 수 있게 됩니다. 기본적으로, “쓰레기를 입력하면 쓰레기를 출력한다.” 대부분의 교육 과정에서 마찬가지로 서둘러서 움직이는 것은 좋은 생각이 아닙니다.\n\n실무적으로, 예술가들은 자신들의 작품에 대해 마음대로 조작할 수 있습니다. 이전까지 회사가 아카이브 전체를 긁어 모든 내용으로 알고리즘을 교육시킬 수 있는 방법을 방해할 수 없다고 믿어져 왔던 것과 마찬가지로 말입니다. 아무것도 절대적으로 확정되어 있는 게 없으며, 일부 예술가들이 보여준 것처럼 특히 저작권을 관리하는 사람들이라면, 이미지가 알고리즘 교육용으로 사용될 때 적절한 보상을 받을 수 있도록 어떠한 협상이 이뤄져야 할 것으로 보입니다.\n\n<div class=\"content-ad\"></div>\n\n이 공간을 주시해주세요. \n\n(스페인어로는 여기에)","ogImage":{"url":"/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png"},"coverImage":"/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png","tag":["Tech"],"readingTime":3},{"title":"안정적인 확산에 대한 설명","description":"","date":"2024-05-18 20:45","slug":"2024-05-18-StableDiffusionExplained","content":"\n\n## Stable 확산이 어떻게 작동합니까? 텍스트에서 이미지 생성 기술 설명\n\n![이미지](/assets/img/2024-05-18-StableDiffusionExplained_0.png)\n\n크기가 큰 텍스트에서 이미지 모델은 텍스트 프롬프트로부터 이미지의 고품질 합성을 가능케하여 높은 성공을 거뒀습니다. 확산 모델은 텍스트에서 이미지 생성 작업에 적용되어 최첨단 이미지 생성 결과를 얻는 데 도움이 될 수 있습니다.\n\nStable 확산 모델은 이미지 생성을 위해 최첨단 결과를 달성했습니다. Stable 확산은 특정 유형의 확산 모델인 Latent 확산 모델에 기초합니다. 이 모델은 CompVis, LMU 및 RunwayML의 연구원 및 엔지니어들에 의해 제안된 'Latent 확산 모델을 사용한 고해상도 이미지 합성'에 기반합니다. 이 모델은 LAION-5B 데이터베이스의 하위 집합에서 512x512 이미지로 초기에 교육되었습니다.\n\n<div class=\"content-ad\"></div>\n\n이는 주로 CLIP와 같은 사전 훈련된 언어 모델을 사용하여 텍스트 입력을 잠재 벡터로 인코딩함으로써 달성됩니다. 확산 모델은 텍스트로부터 이미지 데이터를 생성하는 데 최첨단 결과를 달성할 수 있습니다. 그러나 잡음 제거 프로세스는 고해상도 이미지를 생성할 때 매우 느리며 많은 메모리를 소비합니다. 따라서 이러한 모델을 훈련하고 추론에 사용하는 것이 어려운 과제입니다.\n\n이에 따라 잠재 확산은 실제 픽셀 공간 대신 낮은 차원의 잠재 공간에서 확산 프로세스를 적용함으로써 메모리와 계산 시간을 줄일 수 있습니다. 잠재 확산에서 모델은 이미지의 잠재(압축) 표현을 생성하도록 훈련됩니다.\n\n확산 모델의 훈련\n\nStable Diffusion은 수십억 장의 이미지로 훈련된 대규모 텍스트에서 이미지로 확산 모델입니다. 이미지 확산 모델은 이미지를 잡음 제거하여 출력 이미지를 생성하는 방법을 배웁니다. Stable Diffusion은 훈련 데이터에서 인코딩된 잠재 이미지를 입력으로 사용합니다. 또한 주어진 이미지 z0에 대해, 확산 알고리즘은 이미지에 점진적으로 잡음을 추가하여 소음이 섞인 이미지 zt를 생성합니다. 여기서 t는 잡음이 추가된 횟수를 나타냅니다. t가 충분히 크면 이미지는 순수한 잡음에 가까워집니다. 시간 단계 t, 텍스트 프롬프트, 이미지 확산 알고리즘 등의 입력 집합이 주어진 경우, 확산 알고리즘은 노이즈를 예측하는 네트워크를 학습합니다.\n\n<div class=\"content-ad\"></div>\n\n잠재 확산에는 주로 세 가지 주요 구성요소가 있습니다:\n\n- 자동 인코더 (VAE).\n- U-Net.\n- 텍스트 인코더, 예를 들어 CLIP의 텍스트 인코더.\n\n1. 자동 인코더 (VAE)\n\nVAE 모델은 인코더와 디코더 두 부분으로 구성됩니다. 잠재 확산 학습 중에 인코더는 512*512*3 이미지를 순방향 확산 과정을 위한 사이즈가 64*64*4인 낮은 차원의 잠재 표현으로 변환합니다. 우리는 이미지의 이러한 작은 인코딩된 버전을 잠재라고 부릅니다. 학습 각 단계에서 이러한 잠재에 더 많은 잡음을 적용합니다. 이미지의 인코딩된 잠재 표현은 U-Net 모델의 입력으로 작용합니다.\n\n<div class=\"content-ad\"></div>\n\n여기서는 (3, 512, 512) 모양의 이미지를 (4, 64, 64) 모양의 잠재 이미지로 변환하여 메모리를 48배 더 적게 사용합니다. 이는 픽셀 공간 확산 모델과 비교했을 때 메모리 및 계산 요구 사항을 줄여줍니다. 따라서 16GB Colab GPU에서도 512 × 512 이미지를 매우 빠르게 생성할 수 있습니다.\n\n디코더는 잠재 표현을 이미지로 다시 변환합니다. 역확산 과정에서 생성된 노이즈 제거된 잠재 이미지를 VAE 디코더를 사용하여 이미지로 변환합니다.\n\n추론 중에는 노이즈가 제거된 이미지를 실제 이미지로 변환하기 위해 VAE 디코더만 필요합니다.\n\n```python\nfrom torchvision import transforms as tfms\nfrom diffusers import AutoencoderKL\n\n# Decode the latent representation into image space using the autoencoder model.\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Move to the GPU\nvae = vae.to(torch_device)\n\n# Convert PIL image to latents\n\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (size: 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n```\n\n<div class=\"content-ad\"></div>\n\n2. UNet\n\nU-Net은 노이즈가 있는 잠재 변수의 더 많은 이미지 표현을 예측합니다. 여기서 노이즈가 있는 잠재 변수가 Unet의 입력으로 작용하며 UNet의 출력은 잠재 변수의 노이즈입니다. 이를 사용하여 우리는 노이즈를 노이즈가 있는 잠재 변수에서 뺌으로써 실제 잠재 변수를 얻을 수 있습니다.\n\nUnet은 노이즈가 있는 잠재 변수(x)를 입력으로 사용하고 노이즈를 예측합니다. 우리는 또한 타임스텝(t)과 텍스트 임베딩을 가이드로 사용하는 조건부 모델을 사용합니다.\n\n따라서, 모델은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom diffusers import UNet2DConditionModel\n\n# \"CompVis/stable-diffusion-v1-4\" 모델을 사용하여 UNet 모델을 불러옵니다.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n# GPU로 모델을 이동합니다.\nunet = unet.to(torch_device)\n# 노이즈 예측\nnoise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n```\n\n해당 모델은 기본적으로 UNet 모델을 사용하며, 인코더(12블록), 가운데 블록 및 스킵 연결 디코더(12블록)로 구성되어 있습니다. 이 25개 블록 중 8개 블록은 다운샘플링 또는 업샘플링 컨볼루션 레이어이고, 17개 블록은 각각 네 개의 ResNet 레이어와 두 개의 Vision Transformer(ViT)를 포함하는 주요 블록입니다. 여기서 인코더는 이미지 표현을 낮은 해상도 이미지 표현으로 압축하고, 디코더는 낮은 해상도 이미지 표현을 원래의 더 높은 해상도 이미지 표현으로 해석합니다. 이로써 노이즈가 적은 이미지를 생성합니다.\n\n3. 텍스트-인코더\n\n텍스트-인코더는 입력 프롬프트를 임베딩 공간으로 변환하여 U-Net에 입력으로 제공합니다. Unet을 노이즈 제거 프로세스에 학습시킬 때 노이즈가 많은 latents를 안내하는 역할을 합니다. 텍스트-인코더는 일반적으로 간단한 트랜스포머 기반 인코더로, 입력 토큰 시퀀스를 잠재적인 텍스트-임베딩 시퀀스로 매핑합니다. Stable Diffusion은 새로운 텍스트 인코더를 학습하지 않고 이미 학습된 텍스트 인코더인 CLIP을 사용합니다. 텍스트 인코더는 입력 텍스트에 해당하는 임베딩을 생성합니다.\n\n\n<div class=\"content-ad\"></div>\n\n토큰화\n\n```js\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# 토크나이저 및 텍스트 인코더를로드하여 텍스트를 토큰화하고 인코딩합니다.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# GPU로 이동\ntext_encoder = text_encoder.to(torch_device)\n\nprompt = 'An astronaut riding a horse'\n# 텍스트를 토큰 시퀀스로 변환합니다.\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n```\n\nEmbedding 결과\n\n```js\n# 토큰에서 출력 임베딩 가져오기\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nprint('모양:', output_embeddings.shape)\n```   \n\n<div class=\"content-ad\"></div>\n\n다 모아보면, 모델은 추론 과정 중에 다음과 같이 작동합니다:\n\n![image](/assets/img/2024-05-18-StableDiffusionExplained_1.png)\n\n스케줄러\n\n위에서 언급된 3가지 외에도 이미지에 노이즈를 추가하고 모델을 사용하여 노이즈를 예측하는 데 사용되는 스케줄러가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\ndiffusers에서 LMSDiscreteScheduler를 가져와주세요\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n```\n\n위 코드는 모델을 훈련하는 데 사용되는 스케줄러를 설정합니다. 작은 단계 수에 대해 스케줄러를 설정하려면 다음과 같이 스케줄러를 설정하세요:\n\n```js\n# 샘플링 단계 수를 설정하세요:\nscheduler.set_timesteps(15)\n```\n\nStable Diffusion과 같은 Latent Diffusion Model은 다양한 창의적인 응용 프로그램을 가능하게 합니다:\n  \n\n\n<div class=\"content-ad\"></div>\n\n- 텍스트에서 이미지로 변환\n- 이미지에서 이미지 생성 - 시작점을 기반으로 새 이미지를 생성하거나 수정\n- 이미지 업스케일링 - 작은 이미지를 큰 이미지로 확대\n- 인페인팅 - 이미지의 특정 영역을 마스킹하여 해당 영역에 새로운 디테일을 생성하는 것\n\n잠재 확산 모델은 훈련 비용과 추론을 줄여주어 대량의 고해상도 이미지 합성을 대중화 할 수 있는 잠재력이 있습니다.\n\n다음 블로그에서는 새로운 개념이나 작업을 학습하는데 안정적인 확산을 세밀하게 조정하는 텍스트 역전법에 대해 이야기할 예정입니다.\n\n참고:\n\n<div class=\"content-ad\"></div>\n\n- 롬바흐, R., 블랫만, A., 로렌츠, D., 에셀, P., & 오머, B. (2022). 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전과 패턴 인식 컨퍼런스 논문집 (pp. 10684–10695).\n- 장, L., & 아그라와라, M. (2023). 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543.\n- [Hugging Face Diffusers 문서](https://huggingface.co/docs/diffusers/index)","ogImage":{"url":"/assets/img/2024-05-18-StableDiffusionExplained_0.png"},"coverImage":"/assets/img/2024-05-18-StableDiffusionExplained_0.png","tag":["Tech"],"readingTime":6},{"title":"인간의 시선 AI가 창의력에 미치는 영향 탐구","description":"","date":"2024-05-18 20:44","slug":"2024-05-18-TheHumanLensExploringAIsImpactonCreativity","content":"\n\n<img src=\"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png\" />\n\n“블랭크 슬레이트”에서 스티븐 핑커는 현대 미술을 비판하며 생성적 AI 예술의 폭발과 공감합니다. 현대 미술과 AI 창작물은 우리가 본성적으로 아름다운 것에서 멀어지는 것으로 보입니다. 핑커는 이러한 정체성에 대해 이렇게 얘기합니다: “우리는 아름다움, 감정, 그리고 기술에서 멀어지고 있다.” 어떤 작품들은 특정 대상을 위해 설계되어 완전히 이해하려면 광범위한 맥락이 필요합니다.\n\n알고리즘의 결과가 '예술'로 불릴 때, 이는 예술가에게 모욕이 됩니까? AI는 확실히 기존의 정의에 도전을 제기합니다. 그래도, 이러한 정의를 정제하려는 노력은 새로운 것은 아닙니다. 작가들은 작품을 더 나아가도록 자극하는 이 존재에 직면하며, 일부는 익숙한 것을 버리려는 저항을 합니다. 이러한 변화는 우리에게 뻔한 서사와 기술적 공식을 넘어서도록 이끕니다. 이는 이미 확립된 구조에 익숙한 사람들에게는 불편한 일입니다.\n\n# 모두에게 붓을 전달\n\n<div class=\"content-ad\"></div>\n\n생성적 AI는 예술 창작을 민주화하여, 현대 예술에서 핑커가 비판한 엘리트주의를 흔드는 역할을 합니다. 이제 전통적인 재능의 부재로 인해 창의적인 영혼이 억압받을 필요가 없습니다. 스킬이 항상 신체 민첩성에 결합되지 않는다는 점에 주목한 피커는 이를 예상한 것입니다. 이것이 바로 AI 예술이 구현하는 것입니다. \n\n# 새로운 예술적 도전\n\n현대 예술과 AI 예술은 소수만이 다스릴 수 있는 언어처럼 느껴질 수 있습니다. 사상을 유발하고, 더 깊은 수준의 대화를 유발할 수 있는 작품에 대해 끌리는 것이 있습니다. 그러나 기계가 이러한 대화를 예술을 통해 제공할 수 있을까요? AI 생성은 이를 요구하며, 동시에 핑커가 일부 예술적 영역에서 지적한 속세적인 태도를 강조합니다.\n\n# 디지털 시대의 예술 재정의\n\n<div class=\"content-ad\"></div>\n\n핑커는 특정 예술 형식의 사라짐을 애도하지만, AI는 예술 표현의 완전히 새로운 시대를 열어줍니다. 이 발전은 우리에게 예술을 기술과 아방가르드 아이디어를 넘어서 바라보게 만듭니다. 창의적 영감의 원천이 중요할까요? 기계가 '아티스트'가 될 수 있을까요, 아니면 인간의 지시의 연장으로 존재할까요? 핑커는 마음을 진화의 산물로 보고, AI 아트는 자연적이고 인공적인 창의성을 결합하여 예술이 무엇을 '예술'로 만드는지에 대한 신념적 질문을 던집니다.\n\n# 브러시가 스스로 들고 있는 경우\n\n알고리즘에 의해 휘둘리는 '브러시'로 인해 '인간의 손길'이 어떤 역할을 하는 걸까요? 모든 예술가는 자신의 선택한 매체에서 이 질문에 답하려 노력합니다. AI와 상호작용하는 모든 사람이 '아티스트'가 되는 것은 사실이 아니며, 같은 이치는 캔버스, 악기 또는 기타 전통 도구를 사용하는 사람들에게도 동일합니다.\n\n# 보이지 않는 것을 느낄 수 있을 때\n\n<div class=\"content-ad\"></div>\n\n핑커는 현대 예술이 감정적 연결이 부족할 수 있다고 주장하지만, AI가 이 논쟁의 불을 지피고 있다고 합니다. 기계는 우리를 움직이는 작품을 만들어내지만, 인간의 영혼이 그것이 발생하는 시점을 결정합니다. 동굴 벽화부터 아티스트들은 형식과 내용을 동기부여하고 깊은 감정을 일으키기 위해 노력했습니다. 지금은 우리를 강요하여 감정의 근원을 재정의하도록 이끕니다. 예술이 본질적으로 공유된 경험인 경우, 인간이 만든 것인지 기계가 만든 것인지 상관이 있을까요? 예술 경험은 관객 안에서 독특하게 이루어집니다.\n\n# 무한한 캔버스의 세계\n\n문화적 참조를 끊임없이 섞을 수 있는 생성적 AI는 예술적 일률성에 대한 핑커의 우려에 도전하며, 그가 예술적 관문을 비판하는 것에 울린다고 합니다. 내재적인 제약이 적어져서 전례 없이 다양한 표현을 허용합니다. 이는 문화적 도용부터 잠재적 편견까지 자체적인 복잡성을 야기하나, 일부 예술계의 지각되는 엘리트주의에 대응합니다.\n\n# 새로운 윤리적 풍경을 탐색하기\n\n<div class=\"content-ad\"></div>\n\nAI 예술은 피커가 자주 탐구하는 사회적 갈등을 반영하여 윤리적 문제의 폭풍우를 드러냅니다. 저작권부터 진정성에 대한 우려, 그리고 '영감' 자체의 본질까지, 생성적 AI는 우리에게 현존하는 도덕적 틀을 재고하도록 강요합니다.\n\n생성적 AI는 아름다운 이미지와 윤리적 고민의 폭풍우를 만들어내지만, 피커의 작품 전반에 걸친 통찰은 이 분야에서 놀랍게도 여전히 유의미합니다. 기술과 예술의 본질을 재검토하고 인간의 의도 이상의 감정적 창조력을 사색함으로써, AI 예술은 잠재력과 도발을 제공합니다.\n\n## 이 이야기가 마음에 드셨다면, 다음도 좋아하실 수 있습니다:","ogImage":{"url":"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png"},"coverImage":"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png","tag":["Tech"],"readingTime":3},{"title":"스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법","description":"","date":"2024-05-18 20:43","slug":"2024-05-18-HowStylarAIcanmakeyourdoodlesintoart","content":"\n\n저는 디자이너와 예술가로 훈련을 받았고, 그림 그리는 것을 좋아했어요. 하지만 안타깝게도 그 시절은 지나갔어요. 신경병증 때문에 손을 제어하는 게 어렵게 되었거든요. 그래서 생성 모델 인공지능이 도움을 준 거예요.\n\n어떤 식으로든지 인공지능이 제게 예술적인 비전을 표현하는 데 도움이 돼요. 제 손 대신 단어로요.\n\nStylar AI를 시도해본 적이 있을 때, 나 자신이 적어도 일부분은 다시 창작에 돌아갈 수 있는 잠재력이 있다는 것에 기쁨을 느꼈어요.\n\nStylar AI는 포토샵과 비슷한 레이어, 배경 및 객체 제거 도구, 얼굴 교체 및 수정 도구, 생성 채우기와 확장 기능 등 독특한 기능이 있는 강력한 도구에요.\n\n<div class=\"content-ad\"></div>\n\n한 가지 기능 중 하나는 이미지 대 이미지 생성입니다. 다른 생성적 AI 도구에도 있지만, Stylar AI 버전은 가장 간단한 입력과 작업할 수 있는 놀라운 능력이 있어서 뛰어난 고품질 출력물을 생성할 수 있습니다.\n\nStylar AI 홈페이지에는 간단한 손으로 그린 도형이 어떻게 인상적인 로고로 만들어지는지 보여주는 비디오가 심지어 있는데요.\n\n그것이 저에게 영감을 주어 이미지 대 이미지 도구를 사용해 몇 개의 선으로 그린 그림들을 시도해 보았습니다. 첫 번째 그림은 아르누보 양식의 여성 그림이었습니다.\n\n이 결과물들이 정말 마음에 들었고, 놀라울 정도로 Stylar가 선택한 스타일을 적용하는 데 전혀 문제가 없었고 단지 흑백 선으로 이미지를 안내하여 구성했을 뿐이었습니다.\n\n<div class=\"content-ad\"></div>\n\n다음 시도는 해변 오두막을 그린 것이었습니다. 결과는 다시 한 번 인상적이었습니다.\n\n나는 비교적 간단한 입력 이미지가 탁월한 결과를 만들 수 있는 것을 확신했습니다. 그래서 다음 시도는 꽃무늬의 추상 만달라 스타일 그림이었습니다.\n\n다양한 스타일이 이 간단한 입력 이미지를 수정하는 방법이 놀라울 정도입니다. 이것은 추상 벽 장식품, 스크린 세이버, 배경 및 기타 이미지 생성용 새로운 스타일을 만드는 무한한 기회를 제공합니다.\n\n가장 최근 예시는 나무 그림에 대한 간단한 예술적인 그림입니다.\n\n<div class=\"content-ad\"></div>\n\n다시, 훌륭한 결과네요. 이 중에 있는 그림들을 벽에 걸어두는 것이 수치스럽지 않을 거에요.\n\n그리고 여기가 관건이에요: 이 그림 중 일부는 프롬프트만 있으면 만들기가 굉장히 어려울 거에요.\n\n좋아요, 지금까지 사용한 것은 여전히 숙련된 손에 의해 만들어진 정적 이미지였어요. 그래서 다음 실험은 심지어 더 간단한 선들을 기반으로 한 것이었어요. 이는 큰 예술 능력이나 훈련이 필요하지 않았답니다.\n\n내 자신이 빠져들고 있었어요. 간단한 그림을 계속 만들고, 스타일러 AI에 입력한 뒤 멋진 이미지들의 산을 만들어내고 있었어요.\n\n<div class=\"content-ad\"></div>\n\n네, 손이 잘못됐더라도 다시 만들 수 있었어요. 간단한 선 그림은 정확도나 통제력이 많이 필요하지 않아요. 제 시각과 아이디어의 간단한 낙서가 작동했어요.\n\n당신에게도 도움이 될 거예요. 놀라운 건 이 간단한 스케치가 콘텐츠와 구성을 이끌어 줄 수 있다는 거예요. 스타일은 모든 것을 다 다루죠.\n\n이 그림들을 만들기 위해 프롬프트 창에 단어를 입력할 필요가 없었어요. 그냥 스케치와 선택한 스타일뿐이었어요.\n\n그리고 만약 마음에 드는 스타일이나 원하는 스타일을 찾지 못한다면, Stylar AI에서 손쉽게 여러분만의 스타일을 만들 수 있어요. 이미지를 업로드하기만 하면, 다음에 원할 때 적용하기 쉽도록 여러분의 사용자 정의 스타일 사이에 저장돼요.\n\n<div class=\"content-ad\"></div>\n\n이 기사의 표지 그림은 스케치와 이미지 대 이미지 도구를 사용해서 만들어졌어요. 이 기술은 아름다운 사실적인 이미지, 일러스트레이션, 스티커, 티셔츠 아트, 그리고 떠오르는 다른 모든 것들을 만들어낼 수 있어요.\n\nStylar AI는 다른 방법으로나 어플로는 어렵게 만들 수 없는 이미지를 만들어 낼 수 있어요.\n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png"},"coverImage":"/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png","tag":["Tech"],"readingTime":3},{"title":"The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드","description":"","date":"2024-05-18 20:40","slug":"2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences","content":"\n\n\n![NLP vs LLM](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png)\n\nNLP 및 LLM 기술은 대규모로 사람 언어를 분석하고 생성하는 데 중요합니다. 그들의 증가하는 보급으로, LLM 대 NLP를 구별하는 것은 점점 더 중요해지고 있습니다.\n\nNLP는 인간 언어를 이해, 조작 및 생성하기 위한 일련의 알고리즘을 포함합니다. 1950년대에 처음으로 등장한 이후, NLP는 텍스트 관계를 분석하는 데 진화했습니다. 이는 품사 태깅, 명명된 개체 인식 및 감성 분석 방법을 사용합니다.\n\nOpenAI의 ChatGPT가 보여주는 것처럼, LLM은 깊은 학습을 활용하여 방대한 텍스트 세트로 학습합니다. 인간과 유사한 텍스트를 모방할 수 있지만, 언어의 뉘앙스를 이해하는 것은 제한됩니다. NLP가 언어 분석에 중점을 둔 반면, LLM은 주로 텍스트를 생성합니다.\n\n\n<div class=\"content-ad\"></div>\n\n제안서를 소개해 드릴게요. NLP와 LLMs의 간결하면서 포괄적인 비교를 제공합니다. 이 두 기술의 복잡성을 탐구하고, 다양한 응용 분야를 알아보며, 도전 과제를 살펴볼 것입니다.\n\n# NLP의 독특한 특징 탐구\n\nNLP는 기계가 인간 언어를 의미 있는 방식으로 이해하고 상호 작용하는 데 도움을 줍니다. 스펠 체크, 자동 교정부터 챗봇과 음성 비서에 이르기까지 다양한 응용 분야에 사용될 수 있습니다.\n\nNLP는 인간 언어 생성을 가능하게 하는 알고리즘을 만드는 것입니다. 이것은 디지털 시스템과 인간 간의 소통 간극을 줄입니다. 이 기술은 산업 전반에 걸쳐 증진된 데이터 분석과 통찰력을 위한 길을 엽니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_1.png\" />\n\n## NLP에서 꼭 알아야 할 기술: 파싱부터 자연어 생성까지\n\n자연어 처리는 컴퓨터가 인간의 언어를 생성할 수 있도록 다양한 프로세스에 의존합니다:\n\n- 파싱. 이 기술은 문장을 문법적 요소로 분해합니다. 기계에게 언어 구조를 간소화해 주며 품사, 문장 구분 및 구문적 연결을 인식하는 데 도움이 됩니다.\n- 의미 분석. 단순한 단어 식별을 넘어 단어의 의미와 관계를 파악하는 과정입니다. 텍스트의 맥락, 관용구 및 유머를 해석하는 데 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_2.png)\n\n- 음성 인식. 구어를 쓰인 텍스트로 변환하여 음성을 읽을 수 있는 형식으로 전환하게 됩니다.\n- 자연어 생성. 음성 인식과는 반대로, NLG는 컴퓨터 데이터에 기반하여 인간의 글쓰기를 모방한 텍스트를 제공합니다. 보고서 작성, 요약, 메시지 작성 등을 포함한 응용분야가 있습니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_3.png)\n\n- 감성 분석. 소셜 미디어 모니터링과 브랜드 평판 관리에 자주 사용됩니다. 글의 감정 톤을 평가하고 고객 피드백 및 시장 동향을 분석합니다. \n\n\n<div class=\"content-ad\"></div>\n\n![NLP vs LLMA: Comprehensive Guide to Understanding Key Differences 4](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_4.png)\n\n- 기계 번역. 한 언어에서 다른 언어로 텍스트나 음성을 변환하는 기능을 제공합니다.\n- 명명된 엔터티 인식. 텍스트에서 중요한 정보를 감지하고 분류합니다. 예를 들어 개인, 장소, 조직의 이름 등을 인식합니다.\n\n![NLP vs LLMA: Comprehensive Guide to Understanding Key Differences 5](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_5.png)\n\n- 텍스트 분류 및 분류. 텍스트에 레이블을 할당하여 방대한 데이터 양을 효율적으로 정리하고 관리할 수 있습니다. 이는 문서, 이메일, 그리고 온라인 콘텐츠를 구성하는 데 유용합니다.\n\n<div class=\"content-ad\"></div>\n\n## NLP Applications: Enhancing Communication and Analysis\n\n![NLP Applications](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_6.png)\n\nNLP의 응용 프로그램은 광범위하며 다양한 섹터에 영향을 미칩니다:\n\n- 텍스트 분석. 대규모 텍스트 데이터 세트를 분석하여 중요한 통찰을 얻습니다. 시장 조사 및 소셜 미디어 감시에 유용할 수 있습니다.\n- 음성 인식. 음성으로 된 지시를 이해하고 실행하는 데 사용되는 음성 활성화 장치 및 응용 프로그램을 구동합니다. 이 기술은 가상 어시스턴트 및 필기 도구의 기반을 제공합니다.\n- 감성 분석. 감정적인 맥락을 분석합니다. 대중 의견을 모니터링하고 시장 조사를 수행하는 데 중요합니다.\n- 기계 번역. 언어 장벽을 깨뜨려 텍스트나 음성을 번역하여 국제적인 의사 소통을 촉진합니다.\n- 콘텐츠 추천. NLP를 사용하여 사용자 선호도와 콘텐츠 특성에 기반한 콘텐츠 제안을 맞춤화합니다. 또한 온라인 스트리밍 플랫폼과 온라인 쇼핑에서 경험을 향상시킵니다.\n\n<div class=\"content-ad\"></div>\n\n## NLP에서의 과제: 제한 사항을 극복하며\n\n진전은 있지만 NLP는 여러 가지 장애물을 극복해야 합니다. 이러한 문제를 해결한다면 NLP의 정확성과 기술 통합을 높일 수 있습니다:\n\n- 문맥적 이해. 언어의 미묘한 차이(비꼬거나 관용적인 표현)를 이해하는 것은 여전히 어려움을 겪고, 오해를 일으킬 수 있습니다.\n- 언어 다양성. 문법과 구문이 각각 다른 여러 언어와 방언의 수가 많은 것은 상당한 난관으로 작용합니다.\n- 언어의 모호성. 인간의 언어적 모호성은 NLP 시스템의 해석을 복잡하게 만들 수 있습니다.\n- 데이터 품질과 이용 가능성. NLP 시스템의 성능은 훈련 데이터의 품질과 양에 좌우됩니다. 이 데이터의 편향은 왜곡된 결과를 초래할 수 있습니다.\n- 계산 리소스. 고급 앱에 대한 상당한 계산 능력 수요는 그 개발과 배포를 제한합니다.\n- 실시간 처리. 동시 번역 및 고객 서비스와 같은 응용에 대한 실시간 처리는 기술적인 도전을 제시합니다.\n\n# 대형 언어 모델의 능력 탐색\n\n<div class=\"content-ad\"></div>\n\n대형 언어 모델은 언어 작업에 포괄적인 접근 방식을 제공합니다. 기존의 자연어 처리 시스템 이상으로 유창성과 적응성을 나타냅니다. LLM은 생성적 AI를 위해 정교한 기술 스택을 활용하여:\n\n- 일관성 있고 맥락에 적합한 텍스트 생성\n- 의미 있는 대화를 진행\n- 질문에 대한 답변 제공\n- 인간의 글쓰기와 유사한 콘텐츠 생성\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_7.png)\n\n## LLM의 차별화된 특징\n\n<div class=\"content-ad\"></div>\n\nLLMs는 그들을 돋보이게 하는 몇 가지 핵심 속성으로 특징 지어집니다:\n\n- 광범위한 훈련 데이터. LLMs는 다양한 텍스트 소스에서의 방대한 데이터 세트로 훈련됩니다. 이 접근 방식은 다양한 언어 스타일과 형식을 생성할 수 있게 합니다.\n- 적응성. 언어 모델은 특정 작업마다 특별한 훈련이 필요 없이 여러 가지 언어 작업에 대응할 수 있습니다. LLMs는 자동 콘텐츠 생성과 고급 챗봇 기능에 매우 유연하게 대처할 수 있습니다.\n- 문맥적 이해. LLMs는 문맥과 관련된 텍스트를 생성하여 텍스트 단락 사이에 일관성을 유지합니다.\n- 지속적 학습. LLMs는 새로운 데이터에 노출되면 언어 능력을 개선하고 확장할 수 있습니다. 그들은 계속해서 신조어와 용어에 적응하고 있습니다.\n\n## LLMs 뒤의 핵심 기술들\n\n대형 언어 모델의 효과성은 그들의 기술적 기반이에 근간을 두고 있습니다:\n\n<div class=\"content-ad\"></div>\n\n- 딥 러닝. LLMs는 여러 층의 신경망을 사용하여 자율적으로 학습하고 결정을 내릴 수 있습니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_8.png)\n\n- 트랜스포머 아키텍처. 이러한 모델은 순차적 데이터를 처리하기 위해 설계되어 있으며, 문장에서 다음 단어의 정확한 예측을 가능하게 합니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_9.png)\n\n<div class=\"content-ad\"></div>\n\n- 자기주의 메커니즘. LLMs는 각 단어의 중요성을 평가함으로써 관련성 높은 응답을 생성할 수 있습니다.\n- 확장성. 점진적으로 큰 데이터셋으로 LLMs를 학습시킴으로써 그들의 능력을 향상시킬 수 있습니다.\n\n## LLMs의 실용적인 응용\n\nLLMs는 다음과 같은 다양한 분야에서 응용됩니다:\n\n- 콘텐츠 생성. 기사와 보고서 작성부터 시를 창작하는 데 이르기까지.\n- 고객 서비스. 챗봇을 통해 효율적이고 정확한 자동응답을 제공합니다. 예를 들어, ChatGPT 플러그인 개발은 서비스 중심 분야에서 사용자 경험을 향상시킬 수 있습니다.\n- 언어 번역. 언어적 미묘함에 대한 심층적 이해로 LLMs는 전 세계적인 의사소통을 용이하게 할 수 있습니다.\n- 교육 도구. 개인 맞춤형 학습 자료 생성, 숙제 채점, 방대한 텍스트 요약 등을 보조합니다.\n- 의료 분야. 환자 상호작용, 정보 관리, 의료 문서 분석을 지원합니다.\n\n<div class=\"content-ad\"></div>\n\n## LLMs의 도전과 윤리적 고려 사항\n\n고급 기능을 가진 LLMs는 신중한 고려가 필요한 제한과 윤리적 딜레마에 직면하고 있습니다:\n\n- 편견과 공정성. LLMs는 기존 데이터에서 학습하기 때문에 공정성과 대표성에 대한 우려가 제기됩니다.\n- 정확성과 신뢰성. 출력물은 때로는 사실적인 정확성보다는 데이터 패턴을 반영할 수 있습니다. 이로 인해 부정확성이나 비논리적인 응답이 발생할 수 있습니다.\n- 진정한 이해 부족. LLMs는 이해를 시뮬레이션하지만 진정한 이해가 부족합니다. 이는 복잡한 상황에서의 오류나 부적절한 출력물로 이어질 수 있습니다.\n- 데이터 프라이버시. 잠재적으로 민감한 데이터를 처리하는 것은 엄격한 데이터 거버넌스의 중요성을 강조합니다.\n- 에너지 소비. NLP처럼, 필요한 중요한 계산 리소스는 환경 및 자원 할당에 관한 우려를 제기합니다.\n\n# Comparative Analysis: NLP vs LLM\n\n<div class=\"content-ad\"></div>\n\nNLP과 LLM은 언어를 통해 인간-컴퓨터 상호작용을 향상시키는 데 중요한 역할을 합니다. 둘은 공통 목표를 공유하지만 방법론, 능력 및 적용 영역에서 몇 가지 차이가 있습니다. NLP와 LLM의 성능, 확장성, 정확성 및 다양한 분야에서의 유용성에 초점을 맞춰보겠습니다.\n\n## 성능 메트릭\n\nNLP: 구문 분석 및 entity 인식과 같은 전문 작업에서 높은 정확도를 보여줍니다.\n\nLLM: 인간과 비슷한 텍스트 생성 및 다양한 언어 작업을 처리하는 데 뛰어납니다.\n\n<div class=\"content-ad\"></div>\n\n## 확장성 및 효율성\n\nNLP: 낮은 계산 요구 사항으로 구체적인 작업을 실행하는 데 더 효율적입니다.\n\nLLM: 다양한 작업을 수행하는 데 매우 확장 가능하며, 더 많은 계산 리소스를 필요로하지만 능숙합니다.\n\n## 정확성 및 신뢰성\n\n<div class=\"content-ad\"></div>\n\n**NLP**: 전문 분야에서 높은 정확도와 신뢰성을 보입니다. 문맥의 풍부한 이해를 요구하는 작업에서는 도전을 겪을 수 있습니다.\n\n**LLM**: 일관된 언어 출력을 생성하는 데 신뢰성을 보입니다. 그러나 훈련 데이터에 영향을 받아 부정확하거나 편향된 내용을 생성할 수도 있습니다.\n\n## 건강 관리에서의 활용성\n\n**NLP**: 의료 기록 처리, 관련 환자 정보 추출 및 예측 진단을 가능하게 하는 데 활용됩니다.\n\n<div class=\"content-ad\"></div>\n\nLLM: 환자 상호 작용을 용이하게 하고 정보를 전파하며 일반 의학적 조언을 제공함.\n\n## 금융 분야에서의 유용성\n\nNLP: 감성 분석, 위험 평가, 그리고 고객 서비스 향상에 활용됨. 특히 은행 분야에서 생성 형태 AI를 통한 금융언어 처리에 능하다.\n\nLLM: 금융 보고서 작성, 시장 분석 수행, 그리고 고객 서비스 상호작용 자동화에 유용함.\n\n<div class=\"content-ad\"></div>\n\n## 전자 상거래에서의 사용성\n\nNLP: 챗봇을 통한 고객 경험 향상, 맞춤 추천 및 고객 피드백 분석을 통해 결과를 개선합니다.\n\nLLM: 콘텐츠 생성, 대규모 고객 상호 작용 관리 및 디지털 마케팅의 측면을 자동화하는 데 도움이 됩니다.\n\n# NLP와 LLM 통합을 통한 AI 강화\n\n<div class=\"content-ad\"></div>\n\nNLP와 LLM의 통합은 고급 언어 처리 시스템 개발에서 중대한 발전을 이룬 것입니다. 이 협력은 NLP의 정확한 능력과 LLM의 광범위한 문맥 지식을 결합합니다. 이는 업계 전반에서 AI 응용 프로그램의 효율성과 효과성을 크게 향상시킬 수 있습니다.\n\n## NLP와 대형 언어 모델 통합의 상호 혜택\n\nNLP를 LLM 기술과 통합하는 것은 여러 가지 핵심 이점을 제공합니다:\n\n- 향상된 정확도와 문맥 이해. NLP의 특정 처리 능력을 LLM의 광범위한 문맥 이해와 결합함으로써 언어 작업을 실행할 때 정확도와 관련성을 높일 수 있습니다.\n- 자원 최적화. NLP의 특정 작업 처리 효율성이 LLM의 자원 집약적인 성격을 보완합니다. 이는 확장 가능한 솔루션과 컴퓨팅 자원의 더 나은 할당을 이끌어냅니다.\n- 증가하는 유연성과 적응성. 이러한 기술의 결합은 AI 응용 프로그램의 유연성과 적응성을 향상시킵니다. 그들은 진화하는 요구 사항에 더 민첩하게 대응할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 현실 세계 통합 성공 사례\n\nNLP와 LLM의 협력 잠재력은 다양한 성공적인 응용 프로그램을 통해 입증되었습니다. 이 협력이 AI 응용 프로그램을 혁신화하는 방식을 살펴보겠습니다:\n\n- 의료 분야. IBM 왓슨은 NLP와 LLM을 사용하여 방대한 의료 데이터를 해석합니다. NLP가 구체적 정보 추출에서의 정확성을 발휘하면서 LLM은 보다 넓은 맥락을 이해하는 능력을 결합합니다. 회사는 통찰력 있는 진단 및 치료 권장을 위해 이를 활용합니다.\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_10.png)\n\n<div class=\"content-ad\"></div>\n\n- 금융분야. Bloomberg과 존스홉킨스 대학의 협력 노력을 통해 탄생한 BloombergGPT. 이 모델은 다양한 금융 업무에서 뛰어난 성과를 내기 위해 방대한 데이터셋으로 훈련되었습니다. 이 모델은 연구 확장, 정보 추출, 의사결정 조율, 편향 식별 및 리스크 관리에 도움을 줍니다.\n- 전자상거래분야. Amazon Comprehend는 이 통합을 활용하여 고객 상호작용, 리뷰 및 지원 문의를 분석합니다. 이를 통해 기업은 고객 행위와 선호도를 보다 깊게 이해할 수 있습니다. 이는 제품 검색, 추천, 고객 지원 및 전반적인 만족도 향상으로 이어질 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_11.png)\n\n## NLP 및 LLM 협력의 미래 예측\n\nNLP와 대규모 언어 모델의 지속적인 통합은 새로운 능력과 응용 프로그램을 개방할 것으로 예상됩니다. 확실히 이는 AI 기술과 상호작용하는 방식에 영향을 미칠 것입니다:\n\n<div class=\"content-ad\"></div>\n\n- AI 어시스턴트 업그레이드. 미래 AI 어시스턴트는 복잡한 인간 상호작용에 대한 높은 이해도와 반응성을 보일 것으로 예상됩니다. 이는 NLP와 LLM의 통합 덕분에 실현될 것입니다.\n- 자동 컨텐츠 생성 혁신. NLP의 언어 규칙과 LLM의 창의적 능력을 결합하면 더 정교한 컨텐츠 생성 도구가 제공될 것입니다.\n- 로봇의 언어 이해 업그레이드. 이러한 시너지는 로봇의 언어 처리 능력을 크게 향상시킬 수 있습니다. 이는 보다 자연스럽고 효과적인 인간-로봇 상호작용을 이끌어낼 수 있을 것입니다.\n\n# 결론\n\nNLP 대 LLMs는 각각 인간 언어 처리에 대한 독특한 접근 방식을 가지고 있습니다 — NLP는 구체적인 알고리즘 모델링에 초점을 맞추고 LLMs는 대규모 사전 훈련을 통해 포괄적인 능력을 제공합니다 — 그러나 그들은 서로를 잘 보와합니다. 이들의 통합은 더 풍부한 AI 상호작용, 심층적인 산업 통합, 지속적인 AI 윤리 및 기술 발전을 약속합니다. 이러한 기술들의 책임있는 개발과 적용은 매우 중요합니다.\n\n우리가 미래를 바라보며, LLM과 NLP의 교차점은 새로운 AI 기반 솔루션의 시대를 열 것으로 기대됩니다. NLP와 LLM의 잠재력을 탐색하고자 하는 기관들을 위해, Softermii는 이러한 기술을 효과적으로 활용하기 위한 전문 지식과 지원을 제공합니다. 당사 팀에 연락하시어 혁신적이고 윤리적인 AI 애플리케이션을 위한 길을 열어보세요.","ogImage":{"url":"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png"},"coverImage":"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png","tag":["Tech"],"readingTime":9},{"title":"클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기","description":"","date":"2024-05-18 20:37","slug":"2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost","content":"\n\n## Anthropic이 Karpathy의 비디오 요약 도전에 대한 해결책 재현\n\n이 문서를 작성하는데 출발점은 Andrej Karpathy가 LLM 토큰화에 관한 2시간 13분 비디오 강의를 게시한 직후에 X에서 게시한 글입니다. 이 강의를 책 장/chapter 또는 블로그 글로 자동으로 변환하는 작업에 대한 해결책이 필요한 도전을 받았습니다.\n\n![Image](/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png)\n\n그 후에 Anthropic의 Emmanuel Ameisen과 동료들이 특히 Anthropic의 최신 모델인 Claude 3을 통해 이 작업을 수행할 것을 제안하는 것으로 보이는 해결책이 게시되었습니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_1.png)\n\n사소한 문제와 일관성 부족이 있었지만, 이 방법은 꽤 효율적인 것으로 보였습니다. 결과적으로 생긴 블로그 포스트는 원본 비디오에서 다룬 대부분의 요소와 관련 스크린샷 및 코드 예제를 포함했습니다.\n\n이 작업을 재현하는 데 얼마나 쉽고 비싼지 궁금해졌습니다. 결과적으로, 처음에 기대했던 것보다 더 복잡한 과정이었습니다. 프롬프트는 공유되었으나 코드는 그렇지 않았습니다.\n\n본 문서는 내 구현 방식을 공유하고, 각 단계를 자세히 설명하며, 주요 어려움에 대해 논의합니다. 코드 및 데이터는 이 Github 저장소에서 확인할 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n요약:\n\n- 비디오를 블로그 포스트/책 챕터로 변환하는 것은 대형 멀티모달 모델(LMMs)의 또 다른 매력적인 활용 사례이며, 비디오 콘텐츠를 텍스트 형식으로 제공하여 읽기 쉽고 빠르게 살펴보고 검색할 수 있게 만듭니다.\n- 그러나 LMMs를 기반으로 한 텍스트 변환은 다양한 부정확성과 불일치가 포함될 수 있어 철저한 검토와 교정이 필요합니다. 다른 어려움은 결과의 재현 불가능성과 효과적인 프롬프트 식별에 관려된 것입니다.\n- Claude 3 Opus와 같은 LMM을 활용하여 비디오를 텍스트 형식으로 변환하는 것은 저렴하지 않습니다. 본 문서에서 제시된 솔루션은 이 블로그 포스트로 이 비디오를 변환하는 데 약 4달러의 비용을 소요했습니다.\n\n# 워크플로 개요 및 기술적 제약사항\n\nClaude 3 Opus는 Anthropic에서 제공하는 최신이자 가장 성능이 뛰어난 대형 멀티모달 모델(LMM)입니다. 이 모델은 3월 4일에 발표되었으며, claude.ai 웹 인터페이스 또는 API를 통해 액세스할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n모델은 최대 200K 토큰의 텍스트 또는 이미지를 입력으로 받아들일 수 있고, 최대 4K 토큰의 텍스트를 출력할 수 있습니다. 이것이 정확히 무엇을 의미하는지 조금 더 구체적으로 분석해 보겠습니다:\n\n- 출력의 4K 토큰: 한 토큰이 대략 3/4 단어라는 경험 법칙을 고려하면, 4K 토큰은 대략 3K 단어에 해당합니다. 페이지 당 대략 500단어를 가정한다면, 클로드는 최대 6페이지의 텍스트를 출력할 수 있습니다.\n- 입력의 200K 토큰: 동일한 통계를 따르면, 이는 15만 단어(약 300 페이지)에 해당합니다. 초당 대략 2~3단어의 발화 속도를 전제하면, 약 20시간의 오디오 트랜스크립트를 소화할 수 있으며, 이는 상당히 많은 양입니다. 반면, 1280*720 픽셀(비디오 HD) 해상도의 이미지를 인코딩하는 데에는 약 1.25K 토큰이 필요합니다. 따라서 한 번에 이론상으로는 150여 장의 이미지를 입력으로 제공할 수 있습니다. 실제로는, 토큰 사용량과는 무관하게, 현재 Anthropi API는 입력 이미지 수를 20장으로 제한하고 있음을 참고해야 합니다.\n\n따라서, 주요 제약 사항은 입력으로 제공할 수 있는 이미지의 제한된 수와 모델이 생성할 수 있는 페이지 수의 제한에 있습니다. 해결책은 비디오를 챕터로 분할하여, 각각이 LMM에 의해 별도로 처리되게 하는 것입니다. 결과물은 그 후에 결합되어 최종 문서를 생성합니다.\n\n아래 다이어그램은 워크플로우의 주요 단계를 요약하고 있습니다:\n\n<div class=\"content-ad\"></div>\n\nAmeisen & Co는 YouTube 비디오 설명에 제시된 장을 기준으로 비디오를 분할했습니다(총 24장). 다른 전략으로는 LLM과 같은 주제 분할 도구를 활용하여 대본을 주요 부분으로 분할하는 방법이 있습니다. 몇 분 간격의 장을 목표로 설정하는 것이 좋으며, 이를 통해 명령과 대본에 함께 들어갈 10~20개의 스크린샷을 포함할 수 있습니다.\n\n마지막으로 처리 비용을 예상해 봅시다. Claude 3 Opus의 토큰 사용 비용은 입력 토큰당 15달러이며, 출력 토큰당 75달러입니다.\n\n5분 간격의 장을 가정하면, 2시간짜리 비디오는 24개의 장을 제공하는데, 각 장은 평균적으로 다음을 필요로 합니다:\n\n- 13,000개의 입력 토큰(텍스트 토큰 1,000개 및 1.2K 토큰/이미지의 10개)\n- 1,000개의 출력 토큰(2페이지)\n\n<div class=\"content-ad\"></div>\n\n총 입력 토큰은 약 13*2≈300천 개이며, 출력 토큰은 1천 * 24 = 24천 개입니다. 백만 토큰당 비용을 곱하면 입력 비용이 15*0.3=4.5달러, 출력 비용이 75*0.024=1.8달러가 됩니다.\n\n따라서 2시간 비디오에서 게시물을 생성하는 총 비용은 대략 5에서 10달러 정도입니다. 최적화 전략을 사용하여 어떤 스크린샷을 포함할지 신중하게 선택하고 입력 비용을 줄일 수 있습니다.\n\n# 구현\n\n이제 우리의 구현으로 넘어가 봅시다. 이는 우리의 워크플로우에서 제시한 네 가지 주요 단계를 따릅니다.\n\n<div class=\"content-ad\"></div>\n\n- 비디오를 다운로드하고 텍스트를 획득합니다.\n- 텍스트와 스크린샷을 정렬한 챕터로 분할합니다.\n- 챕터의 LMM 처리를 수행합니다.\n- LMM 출력을 결합하고 블로그 글을 작성합니다.\n\n명확성을 위해 각 단계에 대해 가장 직관적인 구현 방법을 제시합니다. 동반 노트북에는 때로는 데이터를 처리하는 고급 방법을 사용한 추가 코드가 포함될 수 있습니다.\n\n## 비디오를 다운로드하고 오디오 텍스트를 가져옵니다\n\nYouTube에 동영상이 있는 경우, 먼저 pytube 라이브러리를 사용하여 비디오를 다운로드합니다. 나중에 블로그 글을 생성하기 위해 비디오 프레임이 필요하므로 오디오 스트림만이 아닌 전체 비디오를 다운로드합니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nimport pytube\n\n# Andrej Karpathy: GPT Tokenizer 만들기 – https://www.youtube.com/watch?v=zduSFxRajkE\nyoutube_video_id = \"zduSFxRajkE\"\ndef download_youtube_video(video_id, output_path):\n    \"\"\"\n    YouTube 비디오를 다운로드하고 output_path에 저장한 후 비디오 ID를 파일 이름으로 반환합니다.\n    \"\"\"\n    # 비디오 ID로 YouTube 객체 생성\n    youtube = pytube.YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n    # 가장 높은 해상도의 비디오 스트림 가져오기\n    stream = youtube.streams.get_highest_resolution()\n    # 비디오 다운로드\n    video_path = stream.download(output_path=output_path, filename=video_id+\".mp4\")\n    return video_path\n# 330MB 비디오에 대해 약 20초 소요\nvideo_path = download_youtube_video(youtube_video_id, DATA_DIR)\n```\n\n대부분의 Youtube 비디오에는 대본이 이미 제공되어 있습니다. youtube_transcript_api와 같은 라이브러리를 사용하여 Youtube 비디오 ID를 제공함으로써 단순히 대본을 얻을 수 있습니다.\n\n```python\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ntranscript = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n```\n\n2시간 13분의 오디오 스트림 전체가 3422개의 세그먼트로 대본화되었습니다.\n\n<div class=\"content-ad\"></div>\n\n\n```bash\nlen(transcript)\n3422\n\ntranscript[0:4]\n[{'text': \"hi everyone so in this video I'd like us\",  'start': 0.04,  'duration': 4.04}, {'text': 'to cover the process of tokenization in',  'start': 2.04,  'duration': 4.4}, {'text': 'large language models now you see here',  'start': 4.08,  'duration': 4.2}, {'text': \"that I have a set face and that's\",  'start': 6.44,  'duration': 3.88}]\n```\n\n만약 트랜스크립트를 사용할 수 없다면, 오디오 스트림을 음성 인식 모델을 사용하여 텍스트로 변환해야 합니다. 🤗 Open ASR Leaderboard는 성능이 우수한 모델을 찾을 수 있는 좋은 장소입니다. 동반 노트북에 위스퍼 모델과 효율적인 faster-whisper 구현을 사용하여 트랜스크립트를 가져오는 코드를 제공합니다. 이 프로세스는 Google Colab의 T4에서 약 25분이 걸리며(RTX 4090에서는 12분) 완료됩니다.\n\n# 텍스트와 스크린샷이 정렬된 장이로 나누기\n\n장은 수동으로 식별하거나 YouTube가 제공하는 자동 비디오 장 도구와 같은 도구를 사용하여 식별할 수 있습니다. 예제 비디오의 경우, 비디오 설명에 개요된 24개의 장을 복사하여 Python chapters_list 객체에 저장했습니다. 아래에 설명된 것과 같이요.\n\n<div class=\"content-ad\"></div>\n\n```json\nchapters_list = [\n{'timestamp': 0, 'topic': 'Tokenization을 이해하기 위한 소개 및 동기 부여'},\n {'timestamp': 262, 'topic': 'GPT-2에서 토큰화를 위해 바이트 수준 인코딩을 소개한 논문 소개'},\n {'timestamp': 933, 'topic': '유니코드, UTF-8 인코딩 및 어휘 크기'}\n...\n]\n```\n\n그런 다음이 단계의 핵심은 챕터의 시작/끝 타임스탬프에 따라 텍스트와 스크린샷을 추출하는 것입니다. 이것은 chop_up_in_chapters 함수에 의해 구현되며 각 챕터마다 챕터의 시작 및 끝 타임스탬프를 식별하고 트랜스크립트에서 해당 텍스트를 추출하여 비디오에서 스크린샷을 추출합니다.\n\n스크린샷 추출 전략은 각 주어진 챕터에 대해 최대 10장의 스크린샷을 균일하게 샘플링하여 추출하되, 스크린샷 간에 최소 한 분이 경과하도록합니다.\n\n추출된 텍스트 및 스크린샷은 별도의 폴더에 저장됩니다(챕터 번호를 이름으로 사용).\n\n<div class=\"content-ad\"></div>\n\n```python\ndef chop_up_in_chapters(chapters_list, video_path, transcript, timestamps_screenshots_list_seconds=None):\n    \"\"\"\n    비디오를 장(chapter)으로 나눕니다. 비디오 장(chapter) 목록을 기준으로 합니다.\n    \"\"\"\n    n_chapters=len(chapters_list)-1\n    print(f\"장 수: {n_chapters}\")\n    # 타임스탬프와 주제에 대해 반복합니다.\n    for current_chapter in range(n_chapters):\n        output_dir=CHAPTERS_DIR+\"/\"+str(current_chapter)\n         # 해당 출력 디렉토리가 없으면 생성합니다.\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        # 현재와 다음 타임스탬프를 가져옵니다.\n        current_chunk_start_time=chapters_list[current_chapter]['timestamp']\n        current_chunk_end_time=chapters_list[current_chapter+1]['timestamp']-1\n        print(f\"장 {current_chapter}; 시작: {current_chunk_start_time}, 끝: {current_chunk_end_time}\")\n        # 현재 장에 대한 텍스트 및 프레임을 추출합니다.\n        get_text_chapter(transcript, current_chunk_start_time, current_chunk_end_time, output_dir)\n        \n        if timestamps_screenshots_list_seconds is not None:\n            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir,timestamps_screenshots_list_seconds[current_chapter])\n        else:\n            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir)\n```\n\n# 대규모 다중모달 모델(LLM) 처리\n\n이것은 핵심 단계입니다. 각 장마다 오디오 대본과 선택한 스크린샷이 LMM에 제공되어 이 입력 데이터를 교과서에 포함할 수 있는 출력으로 변환하는 것이 목표입니다.\n\n이 단계의 핵심 요소는 우리가 다음과 같이 설계한 LLM 프롬프트입니다:\n\n\n<div class=\"content-ad\"></div>\n\n```js\nprompt_instructions = \"\"\"\n<instructions>\n동영상의 이미지를 다른 타임 스탬프로 제공했으며, <transcript> 안에 오디오 대본도 포함되어 있습니다.\n대본은 인공지능 음성 인식 도구에 의해 생성되었으며 일부 오류/불일치가 있을 수 있습니다.\n귀하의 작업은 대본을 마크다운 블로그 포스트로 변환하는 것입니다.\n이 대본은 소음이 많습니다. 다음 가이드라인을 사용하여 블로그 장을 위한 마크다운 형식으로 다시 작성하십시오:\n- 유효한 마크다운 출력\n- 적절한 곳에 섹션 제목 및 다른 서식 삽입\n- 대사 블록의 일부만 제공되어 주요 주제만 포함하세요. 소개나 결론 단락은 포함하지 마세요. 대사에서 논의된 주요 주제만 포함해주세요.\n- 이미지, 텍스트, 코드, 강조 및 페이지 레이아웃 및 여백을 일반적인 블로그 게시물 또는 교과서와 같이 보이도록 스타일링\n- 말투적인 속성을 제거하십시오\n- 중복 정보가 있는 경우 반복되는 정보는 한 번만 제시해주세요\n- 대화식 내용을 유지하되 대화의 구조를 따를 수 있도록 제목을 포함하세요\n- 각 대본에는 너무 많은 이미지가 포함되어 있으므로 출력에는 가장 중요한 1-2개의 이미지만 포함해주세요\n- 대사와 관련된 일부를 시각화하는 데 도움이 되는 이미지를 선택해주세요\n- 이미지를 선택할 때는 대사에서 설명한 것과 관련된 완전한 코드를 표시하는 이미지를 선호해주세요\n- 이미지가 대본의 일부를 설명하는 데 도움이 될 경우 포함해주세요\n- 이미지를 포함하려면, <img src=\"xxxxx.jpg\"/> 태그를 삽입하며, 여기서 xxxxx는 이미지 데이터 위에 삽입된 정확한 이미지 타임스탬프로 대체되어야 합니다\n- 불필요한 정보를 추가하지 마세요. 대본이나 이미지에서 언급된 사항만 포함해주세요\n최종 출력물은 교과서에 포함하기 적합해야 합니다.\n</instructions>\n\"\"\"\n```\n\n우리는 주로 Ameisen의 안내를 재사용했습니다. 다음과 같은 수정을 가했습니다.\n\n- LMM 출력을 결합하기 위해 출력 형식을 HTML에서 마크다운으로 변경하여 더 직관적으로 만들었습니다 (그리고 마크다운 형식은 블로그 게시물에 시각적으로 잘 어울립니다).\n- 출력 형식이 마크다운으로 정의되었음에도 불구하고 시각적인 요소와 작성 스타일 이미지를 제거했습니다. 이들이 더 유용한 정보를 추가하지 않는다는 결론에 이른 후였습니다.\n- 프롬프트에서 일부 서식을 Anthropic의 가이드라인에 더 잘 따르도록 변경했습니다. 특히, 앞부분에 있는 스크린샷을 이동시키고 지침을 XML 태그로 래핑했습니다.\n\n프롬프트는 장의 스크린샷 및 대본 앞에 오고 있습니다. 우리는 JPG 스크린샷을 Anthropic의 비전 API에 적합한 형식으로 변환하기 위해 `get_screenshots_as_messages` 도우미 함수를 정의했습니다. 이 함수는 모든 스크린샷을 반복하여 각각에 대한 두 가지 메시지를 설명합니다: 스크린샷의 타임스탬프를 지정하는 텍스트 메시지와 그것의 base64로 인코딩된 표현을 포함하는 이미지 메시지입니다. 나중에 하이퍼링크가 추가된 최종 문서에서 원본 비디오로 이동할 수 있게 해주는 타임스탬프가 포함된 텍스트 메시지입니다.\n\n\n<div class=\"content-ad\"></div>\n\n```js\ndef get_screenshots_as_messages(screenshots):\n  screenshots_as_messages = []\n  for i in range(len(screenshots)):\n    screenshots_as_messages.extend([\n    {\n    \"type\": \"text\",\n    \"text\": f\"The timestamp for the following image is {Path(screenshots[i]).stem}.\"\n    },\n    {\n    \"type\": \"image\",\n    \"source\": {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": base64.b64encode(open(screenshots[i], \"rb\").read()).decode(\"utf-8\"),\n      }\n    }])\n  return screenshots_as_messages\n```\n\n우리는 스크린샷, 대본 및 지침을 모아서 함께 가져오는 또 다른 도우미 함수인 get_prompt_as_messages를 정의했습니다. 이 함수는 추가로 Claude의 출력을 미리 채워 마크다운 제목(\"#\")으로 답변을 시작하도록 만듭니다.\n\n```js\ndef get_prompt_as_messages(chapter_id):\n    folder_path=CHAPTERS_DIR+'/'+str(chapter_id)\n    with open(folder_path+'/transcript.txt', \"r\") as f:\n        transcript = f.read()\n    screenshots=sorted(glob.glob(folder_path+'/*.jpg'))\n    \n    screenshots_as_messages=get_screenshots_as_messages(screenshots)\n    prompt_as_messages = [\n        {\n            \"role\": \"user\",\n            \"content\": screenshots_as_messages+\n            [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"<transcript>\\n{transcript}\\n</transcript>\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt_instructions\n                }\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"#\"\n                }\n            ]\n        }\n    ]\n    return prompt_as_messages\n```\n\n그게 다야!\n\n\n<div class=\"content-ad\"></div>\n\n모든 챕터는 클로드를 반복적으로 호출하여 처리한 후 결과를 해당 챕터 폴더에 있는 마크다운 파일로 작성할 수 있습니다.\n\n```js\n# 챕터 목록을 반복하여 처리\nfor chapter in range(len(chapters_list)-1): \n  # 현재 챕터에 대한 프롬프트 생성 (스크린샷, 대본 및 지침이 포함된 메시지 목록).\n    prompt_generate_markdown = get_prompt_as_messages(chapter)\n    # 프롬프트를 사용하여 메시지 생성하기\n    message = client.messages.create(\n        model=\"claude-3-opus-20240229\",\n        system=\"당신은 마크다운 블로그 글쓰기 전문가입니다.\",\n        temperature=0,\n        max_tokens=4000,\n        messages=prompt_generate_markdown\n    )\n    # 응답에서 생성된 마크다운 콘텐츠 추출\n    answer = message.content[0].text\n    markdown = \"#\"+answer  # 마크다운 콘텐츠 앞에 헤더 태그 추가\n    \n    # 현재 챕터에 해당하는 마크다운 파일 경로 정의\n    markdown_file = CHAPTERS_DIR + '/' + str(chapter) + '/markdown.md'\n    # 생성된 마크다운 콘텐츠를 파일에 작성\n    with open(markdown_file, \"w\") as f:\n        f.write(markdown)\n```\n\n아래는 Anthropic의 사용 로그 중 마지막 일곱 챕터 처리에 대한 부분을 보고, 처리 시간 및 입력 및 출력 토큰 수의 변동을 감지할 수 있습니다.\n\n<img src=\"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n가장 긴 장은 마지막에서 두 번째 장이었습니다 (여기에서 장을 확인하세요), 1시 51분부터 2시 10분까지 총 17689 토큰을 처리하는 데 거의 1분이 걸렸습니다. 전체적으로 비디오 및 24 장을 처리하는 데 약 10분이 소요되었고, 18만 토큰의 입력 및 1만 5천 토큰의 출력이 사용되었습니다. 이 과정은 약 4달러의 비용이 소요되었습니다.\n\n# LMM 출력을 결합하여 최종 블로그 포스트 생성\n\n워크플로우의 최종 단계는 주로 두 가지 작업으로 구성됩니다. 먼저 다양한 마크다운 출력을 병합합니다. 그 다음으로는 장 제목 및 이미지에 하이퍼링크를 추가합니다. 이를 통해 최종 마크다운 파일을 해당 타임스탬프에서 원본 YouTube 비디오에 연결할 수 있습니다.\n\n```js\nmerged_markdown=\"\"\n\n# 장 폴더를 반복하여 마크다운 파일 병합\nfor chapter in range(len(chapters_list)-1):\n    markdown_file=CHAPTERS_DIR+'/'+str(chapter)+'/markdown.md'\n    with open(markdown_file, \"r\") as f:\n        markdown = f.readlines()\n    # 각 장 제목에 해당하는 비디오의 링크를 추가\n    url_chapter = f\"https://www.youtube.com/watch?v={youtube_video_id}&t={chapters_list[chapter]['timestamp']}s\"\n    markdown[0] = f\"# [{chapter+1}) {markdown[0][2:].strip()}]({url_chapter})\"\n    markdown = '\\n'.join(markdown)\n    merged_markdown+=\"\\n\"+markdown\n# 병합된 마크다운에서 src 속성의 타임스탬프가 포함된 모든 <img> 태그를 찾아 해당 비디오의 링크를 추가\ntimestamps_screenshots = re.findall(r'<img src=\"(\\d+)\\.jpg\"/>', merged_markdown)\ntimestamps_screenshots = [timestamp for timestamp in timestamps_screenshots]\n# 각 이미지에 해당하는 타임스탬프에서 올바른 비디오 링크를 추가\nfor timestamp in timestamps_screenshots:\n    video_link = f'<a href=\"https://www.youtube.com/watch?v={youtube_video_id}&t={int(timestamp)}s\">비디오 링크</a>'\n    merged_markdown = merged_markdown.replace(f'<img src=\"{timestamp}.jpg\"/>', f'<img src=\"{timestamp}.jpg\"/>\\n\\n{video_link}')\n# 병합된 마크다운에서 이미지를 기반으로한 프레임을 추출하고 merge 폴더에 저장\nget_frames_chapter(video_path, None, None, MERGE_DIR, timestamps_screenshots=timestamps_screenshots)\n# 병합된 마크다운을 markdown 블로그포스트.md 파일로 저장\nmarkdown_file=MERGE_DIR+'/blogpost.md'\nwith open(markdown_file, \"w\") as f:\n        f.write(merged_markdown)\n```\n\n<div class=\"content-ad\"></div>\n\n결합된 마크다운 파일은 MERGE_DIR 폴더에 모든 선택한 JPG 스크린샷과 함께 'markdown.md'로 저장됩니다 (최종 출력).\n\n# 토의\n\n결과적으로 포스트는 원래 비디오의 대부분의 내용을 성공적으로 보존하여, Ameisen 및 그의 동료가 설명한 것과 유사한 품질을 달성했습니다. 또한, 의미 있는 스크린샷 및 코드 조각을 정확하게 식별하여 오디오 대본의 이해를 돕습니다. 그러나, 텍스트 변환이 정확하지 않은 부분을 비롯해 일부 결함이 있습니다.\n\n정확하지 않거나 모순된 부분을 해결하기 위해 철저한 편집과 교정이 여전히 필요합니다. (Ameisen의 작업에 발견된 것과 유사한) 문제의 예로는 예를 들어, \"hello world\" 토큰을 2가 아닌 300으로 잘못 세는 오류, \"tokenization\"의 첫 번째 토큰을 잘못 번호 매기는 오류, 공백을 토큰으로 오도록 잘못 인식하는 등이 있습니다 (블로그 포스트의 2장 참조). 이러한 부정확성 외에도, 이 방법론은 효과적인 프롬프트를 만드는 복잡성, 결과의 재현 불가능성 및 LMM 운영에 따른 비용과 같은 다른 어려움을 야기합니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 이러한 단점에도 불구하고 비디오를 접근 가능하고 쉽게 탐색할 수 있는 텍스트 블로그 포스트로 변환하는 것은 대형 다중 모달 모델의 가치 있는 응용 프로그램입니다. 특히 경쟁하는 LMM(대형 다중 모달 모델)인 GPT4-V, Gemini Pro Vision 및 오픈 소스 대규모 월드 모델의 비디오/이미지 이해 능력과 비교는 내일의 블로그 포스트 주제가 될 것입니다.\n\n# 유용한 링크\n\n- 동반자 Github 저장소\n- Karpathy의 도전과 Ameisen 및 동료의 저장소\n- 토큰화에 대한 비디오 튜토리얼 : https://www.youtube.com/watch?v=zduSFxRajkE 및 손으로 쓴 튜토리얼 요약 : https://github.com/karpathy/minbpe/blob/master/lecture.md\n- Claude 3 — Vision 문서\n- Misbah Syed의 다른 접근 방식\n\n참고: 별도로 표시되지 않는 한, 모든 이미지는 작성자가 제공한 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 게시물을 즐겼나요? 생각을 공유하거나 박수를 보내거나 LinkedIn에서 저와 연락하세요.","ogImage":{"url":"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png"},"coverImage":"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png","tag":["Tech"],"readingTime":15}],"page":"74","totalPageCount":98,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true}