{"pageProps":{"post":{"title":"YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기","description":"","date":"2024-05-18 18:03","slug":"2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer","content":"\n\n그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.\n\n![YoutubeDataPipeline](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png)\n\n## 사용 사례: —\n\n상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?\n\n<div class=\"content-ad\"></div>\n\n## 제안된 해결책: —\n\n위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:\n\n— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.\n\n— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- 데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.\n\n이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.\n\n## 구현\n\n구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n도커 데스크톱을 설치해보세요 — https://www.docker.com/get-started/\n\n머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.\n\n```js\ndocker --version\nDocker version 20.10.23, build 7155243\n```\n\n최신 apache/airflow 이미지를 받아보세요\n\n<div class=\"content-ad\"></div>\n\n```sh\n도커 pull apache/airflow\n```\n\n다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.\n\n```sh\n도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone\n```\n\nAWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기\n\n<div class=\"content-ad\"></div>\n\nconfig\n\n```json\n[default]\nregion = ap-south-1\n```\n\ncredentials\n\n```json\n[default]\naws_access_key_id = AKIB******AXPCMO\naws_secret_access_key = 4D7HkaIBsqu***********+0AT2a8j\n```\n\n<div class=\"content-ad\"></div>\n\n지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.\n\n도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.\n\n![이미지](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png)\n\n웹 브라우저를 열고 `http://localhost:8080` 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.\n\n<div class=\"content-ad\"></div>\n\n`aws_write_utility.py` 파일을 만들 때 평소처럼 진행하시면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\nimport boto3\nimport json\nimport uuid\n\ndef write_json_to_s3(json_data, bucket_name, key_name):\n\n    # S3 클라이언트 초기화\n    s3 = boto3.client('s3')\n    \n    # JSON 데이터를 바이트로 변환\n    json_bytes = json.dumps(json_data).encode('utf-8')\n    \n    # JSON 데이터를 S3에 쓰기\n    s3.put_object(Bucket=bucket_name, Key=key_name, Body=json_bytes)\n\n\ndef generate_uuid():\n    \"\"\"UUID와 유사한 문자열 생성.\"\"\"\n    return str(uuid.uuid4())\n```\n\nyoutube_comments.py\n\n```js\n# -*- coding: utf-8 -*-\n\n# youtube.commentThreads.list를 위한 샘플 Python 코드\n# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:\n# https://developers.google.com/explorer-help/code-samples#python\n\nimport os\n\nimport googleapiclient.discovery\nimport aws_write_utility\nfrom aws_write_utility import write_json_to_s3\n\ndef start_process():\n    # 로컬에서 실행 시 OAuthlib의 HTTPS 확인 비활성화\n    # 제품 환경에서는 이 옵션을 활성화하지 마세요.\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    DEVELOPER_KEY = \"AIzaS*****************PiwBdaP_IE\"\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey=DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=\"snippet,replies\",\n        videoId=\"r_K*****PKU\"\n    )\n    response = request.execute()\n\n    process_comments(response)\n\n\ndef process_comments(response_items):\n\n    # 예시 S3 버킷 및 키 이름\n    bucket_name = 'youtube-comments-analysis'\n    key_name = 'data/{}.json'.format(aws_write_utility.generate_uuid())\n\n    comments = []\n    for comment in response_items['items']:\n        author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n        comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n        publish_time = comment['snippet']['topLevelComment']['snippet']['publishedAt']\n        comment_info = {'author': author, 'comment': comment_text, 'published_at': publish_time}\n        comments.append(comment_info)\n    print(f'총 {len(comments)}개의 댓글 처리 완료.')\n    write_json_to_s3(comments, bucket_name, key_name)\n```\n\nyoutube_dag.py\n\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom youtube_comments import start_process\n\n# 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 5, 16),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DAG 객체 생성\ndag = DAG(\n    'youtube_python_operator_dag',\n    default_args=default_args,\n    description='Python 함수를 호출하는 간단한 DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n# PythonOperator 작업 생성\npython_task = PythonOperator(\n    task_id='my_python_task',\n    python_callable=start_process,\n    dag=dag,\n)\n\n# 작업 간 의존성 정의\npython_task\n\n# DAG 등록\ndag\n```\n\n위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.\n\n좋아요!!\n\n이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.","ogImage":{"url":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png"},"coverImage":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png","tag":["Tech"],"readingTime":6},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.</p>\n<p><img src=\"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png\" alt=\"YoutubeDataPipeline\"></p>\n<h2>사용 사례: —</h2>\n<p>상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?</p>\n<h2>제안된 해결책: —</h2>\n<p>위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:</p>\n<p>— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.</p>\n<p>— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.</p>\n<ul>\n<li>데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.</li>\n</ul>\n<p>이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.</p>\n<h2>구현</h2>\n<p>구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.</p>\n<p>도커 데스크톱을 설치해보세요 — <a href=\"https://www.docker.com/get-started/\" rel=\"nofollow\" target=\"_blank\">https://www.docker.com/get-started/</a></p>\n<p>머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.</p>\n<pre><code class=\"hljs language-js\">docker --version\n<span class=\"hljs-title class_\">Docker</span> version <span class=\"hljs-number\">20.10</span><span class=\"hljs-number\">.23</span>, build <span class=\"hljs-number\">7155243</span>\n</code></pre>\n<p>최신 apache/airflow 이미지를 받아보세요</p>\n<pre><code class=\"hljs language-sh\">도커 pull apache/airflow\n</code></pre>\n<p>다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.</p>\n<pre><code class=\"hljs language-sh\">도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone\n</code></pre>\n<p>AWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기</p>\n<p>config</p>\n<pre><code class=\"hljs language-json\"><span class=\"hljs-punctuation\">[</span>default<span class=\"hljs-punctuation\">]</span>\nregion = ap-south<span class=\"hljs-number\">-1</span>\n</code></pre>\n<p>credentials</p>\n<pre><code class=\"hljs language-json\"><span class=\"hljs-punctuation\">[</span>default<span class=\"hljs-punctuation\">]</span>\naws_access_key_id = AKIB******AXPCMO\naws_secret_access_key = <span class=\"hljs-number\">4</span>D7HkaIBsqu***********+<span class=\"hljs-number\">0</span>AT2a8j\n</code></pre>\n<p>지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.</p>\n<p>도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.</p>\n<p><img src=\"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png\" alt=\"이미지\"></p>\n<p>웹 브라우저를 열고 <code>http://localhost:8080</code> 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.</p>\n<p><code>aws_write_utility.py</code> 파일을 만들 때 평소처럼 진행하시면 됩니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> boto3\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> uuid\n\ndef <span class=\"hljs-title function_\">write_json_to_s3</span>(json_data, bucket_name, key_name):\n\n    # <span class=\"hljs-variable constant_\">S3</span> 클라이언트 초기화\n    s3 = boto3.<span class=\"hljs-title function_\">client</span>(<span class=\"hljs-string\">'s3'</span>)\n    \n    # <span class=\"hljs-title class_\">JSON</span> 데이터를 바이트로 변환\n    json_bytes = json.<span class=\"hljs-title function_\">dumps</span>(json_data).<span class=\"hljs-title function_\">encode</span>(<span class=\"hljs-string\">'utf-8'</span>)\n    \n    # <span class=\"hljs-title class_\">JSON</span> 데이터를 <span class=\"hljs-variable constant_\">S3</span>에 쓰기\n    s3.<span class=\"hljs-title function_\">put_object</span>(<span class=\"hljs-title class_\">Bucket</span>=bucket_name, <span class=\"hljs-title class_\">Key</span>=key_name, <span class=\"hljs-title class_\">Body</span>=json_bytes)\n\n\ndef <span class=\"hljs-title function_\">generate_uuid</span>():\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"UUID와 유사한 문자열 생성.\"</span><span class=\"hljs-string\">\"\"</span>\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">str</span>(uuid.<span class=\"hljs-title function_\">uuid4</span>())\n</code></pre>\n<p>youtube_comments.py</p>\n<pre><code class=\"hljs language-js\"># -*- <span class=\"hljs-attr\">coding</span>: utf-<span class=\"hljs-number\">8</span> -*-\n\n# youtube.<span class=\"hljs-property\">commentThreads</span>.<span class=\"hljs-property\">list</span>를 위한 샘플 <span class=\"hljs-title class_\">Python</span> 코드\n# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:\n# <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//developers.google.com/explorer-help/code-samples#python</span>\n\n<span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-keyword\">import</span> googleapiclient.<span class=\"hljs-property\">discovery</span>\n<span class=\"hljs-keyword\">import</span> aws_write_utility\n<span class=\"hljs-keyword\">from</span> aws_write_utility <span class=\"hljs-keyword\">import</span> write_json_to_s3\n\ndef <span class=\"hljs-title function_\">start_process</span>():\n    # 로컬에서 실행 시 <span class=\"hljs-title class_\">OAuthlib</span>의 <span class=\"hljs-variable constant_\">HTTPS</span> 확인 비활성화\n    # 제품 환경에서는 이 옵션을 활성화하지 마세요.\n    os.<span class=\"hljs-property\">environ</span>[<span class=\"hljs-string\">\"OAUTHLIB_INSECURE_TRANSPORT\"</span>] = <span class=\"hljs-string\">\"1\"</span>\n\n    api_service_name = <span class=\"hljs-string\">\"youtube\"</span>\n    api_version = <span class=\"hljs-string\">\"v3\"</span>\n    <span class=\"hljs-variable constant_\">DEVELOPER_KEY</span> = <span class=\"hljs-string\">\"AIzaS*****************PiwBdaP_IE\"</span>\n\n    youtube = googleapiclient.<span class=\"hljs-property\">discovery</span>.<span class=\"hljs-title function_\">build</span>(\n        api_service_name, api_version, developerKey=<span class=\"hljs-variable constant_\">DEVELOPER_KEY</span>)\n\n    request = youtube.<span class=\"hljs-title function_\">commentThreads</span>().<span class=\"hljs-title function_\">list</span>(\n        part=<span class=\"hljs-string\">\"snippet,replies\"</span>,\n        videoId=<span class=\"hljs-string\">\"r_K*****PKU\"</span>\n    )\n    response = request.<span class=\"hljs-title function_\">execute</span>()\n\n    <span class=\"hljs-title function_\">process_comments</span>(response)\n\n\ndef <span class=\"hljs-title function_\">process_comments</span>(response_items):\n\n    # 예시 <span class=\"hljs-variable constant_\">S3</span> 버킷 및 키 이름\n    bucket_name = <span class=\"hljs-string\">'youtube-comments-analysis'</span>\n    key_name = <span class=\"hljs-string\">'data/{}.json'</span>.<span class=\"hljs-title function_\">format</span>(aws_write_utility.<span class=\"hljs-title function_\">generate_uuid</span>())\n\n    comments = []\n    <span class=\"hljs-keyword\">for</span> comment <span class=\"hljs-keyword\">in</span> response_items[<span class=\"hljs-string\">'items'</span>]:\n        author = comment[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'topLevelComment'</span>][<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'authorDisplayName'</span>]\n        comment_text = comment[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'topLevelComment'</span>][<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'textOriginal'</span>]\n        publish_time = comment[<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'topLevelComment'</span>][<span class=\"hljs-string\">'snippet'</span>][<span class=\"hljs-string\">'publishedAt'</span>]\n        comment_info = {<span class=\"hljs-string\">'author'</span>: author, <span class=\"hljs-string\">'comment'</span>: comment_text, <span class=\"hljs-string\">'published_at'</span>: publish_time}\n        comments.<span class=\"hljs-title function_\">append</span>(comment_info)\n    <span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">'총 {len(comments)}개의 댓글 처리 완료.'</span>)\n    <span class=\"hljs-title function_\">write_json_to_s3</span>(comments, bucket_name, key_name)\n</code></pre>\n<p>youtube_dag.py</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> datetime <span class=\"hljs-keyword\">import</span> datetime, timedelta\n<span class=\"hljs-keyword\">from</span> airflow <span class=\"hljs-keyword\">import</span> DAG\n<span class=\"hljs-keyword\">from</span> airflow.operators.python_operator <span class=\"hljs-keyword\">import</span> PythonOperator\n<span class=\"hljs-keyword\">from</span> youtube_comments <span class=\"hljs-keyword\">import</span> start_process\n\n<span class=\"hljs-comment\"># 기본 인수 정의</span>\ndefault_args = {\n    <span class=\"hljs-string\">'owner'</span>: <span class=\"hljs-string\">'airflow'</span>,\n    <span class=\"hljs-string\">'depends_on_past'</span>: <span class=\"hljs-literal\">False</span>,\n    <span class=\"hljs-string\">'start_date'</span>: datetime(<span class=\"hljs-number\">2024</span>, <span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">16</span>),\n    <span class=\"hljs-string\">'email_on_failure'</span>: <span class=\"hljs-literal\">False</span>,\n    <span class=\"hljs-string\">'email_on_retry'</span>: <span class=\"hljs-literal\">False</span>,\n    <span class=\"hljs-string\">'retries'</span>: <span class=\"hljs-number\">1</span>,\n    <span class=\"hljs-string\">'retry_delay'</span>: timedelta(minutes=<span class=\"hljs-number\">5</span>),\n}\n\n<span class=\"hljs-comment\"># DAG 객체 생성</span>\ndag = DAG(\n    <span class=\"hljs-string\">'youtube_python_operator_dag'</span>,\n    default_args=default_args,\n    description=<span class=\"hljs-string\">'Python 함수를 호출하는 간단한 DAG'</span>,\n    schedule_interval=timedelta(days=<span class=\"hljs-number\">1</span>),\n)\n\n<span class=\"hljs-comment\"># PythonOperator 작업 생성</span>\npython_task = PythonOperator(\n    task_id=<span class=\"hljs-string\">'my_python_task'</span>,\n    python_callable=start_process,\n    dag=dag,\n)\n\n<span class=\"hljs-comment\"># 작업 간 의존성 정의</span>\npython_task\n\n<span class=\"hljs-comment\"># DAG 등록</span>\ndag\n</code></pre>\n<p>위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.</p>\n<p>좋아요!!</p>\n<p>이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.</p>\n<p>이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}