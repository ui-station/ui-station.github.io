{"pageProps":{"post":{"title":"Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기","description":"","date":"2024-06-19 12:53","slug":"2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker","content":"\n\n## 생성적 AI 시리즈\n\n이 블로그는 생성적 AI에 관한 지속적인 시리즈이며 이전 블로그의 연장선입니다. 이 블로그 시리즈에서는 Ollama를 탐색하고 도커를 사용하여 분산 아키텍처에 배포할 수 있는 응용 프로그램을 구축할 것입니다.\n\nOllama는 강력한 언어 모델을 손쉽게 컴퓨터에서 실행할 수 있도록 도와주는 프레임워크입니다. Ollama 소개에 대해서는 2024년 2월에 A B Vijay Kumar에 의해 작성된 [Ollama — Brings runtime to serve LLMs everywhere. | by A B Vijay Kumar | Feb, 2024 | Medium](링크)를 참조해주세요. 이 블로그에서는 langchain 애플리케이션을 구축하고 도커에 배포할 것입니다.\n\n## Ollama를 위한 Langchain 챗봇 애플리케이션\n\n<div class=\"content-ad\"></div>\n\nLangshan을 사용하여 챗봇 애플리케이션을 개발해 보겠습니다. Python 애플리케이션에서 모델에 액세스하기 위해 간단한 Streamlit 챗봇 애플리케이션을 만들 것입니다. 이 Python 애플리케이션을 컨테이너에 배포하고 다른 컨테이너에서 Ollama를 사용할 것입니다. Docker-compose를 사용하여 인프라를 구축할 것입니다. 만약 Docker 또는 docker-compose를 사용하는 방법을 모르신다면, 계속 진행하기 전에 인터넷에서 몇 가지 자습서를 참고해 주세요.\n\n아래 그림은 컨테이너 간 상호 작용과 접근하는 포트를 보여주는 아키텍처를 보여줍니다.\n\n![아키텍처 그림](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png)\n\n컨테이너를 2개 생성할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- Ollama 컨테이너는 모델을 저장하고 로드하기 위해 호스트 볼륨을 사용합니다 (/root/.ollama는 로컬 ./data/ollama로 매핑됩니다). Ollama 컨테이너는 내부적으로 11434로 매핑된 외부 포트인 11434에서 수신 대기합니다.\n- Streamlit 챗봇 애플리케이션은 내부적으로 8501로 매핑된 외부 포트인 8501에서 수신 대기합니다.\n코딩을 시작하기 전에 Python 가상 환경을 설정하겠습니다.\n\n```shell\npython3 -m venv ./ollama-langchain-venv\nsource ./ollama-langchain-venv/bin/activate\n```\n\n다음은 streamlit 애플리케이션의 소스 코드입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_1.png\" />\n\n제가 이전 블로그에서 작성한 것과 매우 유사한 소스 코드입니다. 이 코드가 어떻게 작동하는지에 대한 자세한 내용은 다른 블로그인 \"Retrieval Augmented Generation(RAG) - LlamaIndex를 사용한 문서용 챗봇\"을 참조하실 수 있어요. 주요 차이점은 Ollama를 사용하고 Ollama Langchain 라이브러리를 통해 모델을 호출한다는 점이에요 (이는 langchain_community의 일부입니다).\n\nrequirements.txt에서 종속성을 정의해 봅시다.\n\n<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_2.png\" />\n\n<div class=\"content-ad\"></div>\n\n이제 Streamlit 애플리케이션의 도커 이미지를 빌드하기 위한 Dockerfile을 정의해 보겠습니다.\n\n![image](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_3.png)\n\n우리는 베이스 이미지로 python 도커 이미지를 사용하고, /app이라는 작업 디렉토리를 생성합니다. 그런 다음 애플리케이션 파일을 해당 디렉토리로 복사하고, pip를 사용하여 모든 종속성을 설치합니다. 그 후에 포트 8501을 노출하고 Streamlit 애플리케이션을 시작합니다.\n\n아래에 표시된 대로 docker build 명령을 사용하여 도커 이미지를 빌드할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n![docker_image_4](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_4.png)\n\n도커 이미지가 빌드되었는지 확인할 수 있어야 합니다. 다음에 표시된 것처럼 `docker images` 명령어를 사용하세요.\n\n![docker_image_5](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_5.png)\n\n이제 스트림릿 애플리케이션 및 올라마 컨테이너의 네트워크를 정의하는 docker-compose 구성 파일을 만들어보겠습니다. 이렇게 하면 두 애플리케이션이 상호 작용할 수 있습니다. 또한 위의 그림에서 보듯이 다양한 포트 구성을 정의할 것입니다. 올라마에 대해서는 모델들이 영구적으로 유지되도록 볼륨 매핑도 수행할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_6.png\" />\n\n도커 컴포즈 업 명령을 실행하면 어플리케이션을 실행할 수 있습니다. 도커 컴포즈 업을 실행하면 아래 스크린샷에 표시된 대로 두 컨테이너가 모두 실행되고 있는 것을 확인할 수 있어야 합니다.\n\n<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_7.png\" />\n\n아래 스크린샷에 표시된 대로 도커 컴포즈 ps 명령을 실행하여 컨테이너가 실행 중인 것을 확인할 수 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\n![image1](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_8.png)\n\n올라마가 실행 중인지 확인하려면 아래 스크린샷에 표시된 대로 http://localhost:11434을 호출해야 합니다.\n\n![image2](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_9.png)\n\n이제 아래에 표시된 대로 docker exec 명령을 사용하여 도커 컨테이너에 로그인하여 필요한 모델을 다운로드해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\ndocker exec -it ollama-langchain-ollama-container-1 ollama run phi\n```\n\n우리는 모델 phi를 사용 중이므로 해당 모델을 가져와서 실행하여 테스트 중입니다. 아래 스크린샷을 참고하세요. phi 모델이 다운로드되고 실행을 시작할 것입니다 (-it 플래그를 사용하므로 샘플 프롬프트로 상호작용 및 테스트가 가능해집니다).\n\n<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_10.png\" />\n\n로컬 폴더 ./data/ollama에서 다운로드된 모델 파일 및 매니페스트를 확인할 수 있어야 합니다. 이 폴더는 내부적으로 컨테이너에 매핑되어 있으며(Ollama가 다운로드된 모델을 제공하기 위해 찾는 위치인 /root/.ollama에 매핑됨)\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 변경한 테이블입니다.\n\n\n| `<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_11.png\" />` |\n|---|\n| Lets now run access our streamlit application by opening [http://localhost:8501](http://localhost:8501) on the browser. The following screenshot shows the interface |\n\n| `<img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_12.png\" />` |\n|---|\n| Lets try to run a prompt “generate a story about dog called bozo”. You shud be able to see the console logs reflecting the API calls, that are coming from our Streamlit application, as shown below |\n\n\n<div class=\"content-ad\"></div>\n\n아래 스크린샷에서 확인할 수 있듯이, 내가 보낸 프롬프트에 대한 응답을 받았어요.\n\n도커 컴포즈 다운을 호출하여 배포를 중지할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n다음 스크린샷은 결과를 보여줍니다.\n\n![output](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_15.png)\n\n여기 있습니다. 이 블로그를 준비하면서 랭체인과 함께 Ollama가 작동하고 Docker-Compose를 사용하여 Docker에 배포하는 것이 정말 즐거웠어요.\n\n도움이 되었기를 바랍니다. 더 많은 실험으로 돌아오겠습니다. 그 동안 즐기고 코딩하세요!!! 곧 다시 만나요!!!\n\n<div class=\"content-ad\"></div>\n\n여기서 전체 소스 코드에 액세스할 수 있습니다. abvijaykumar/ollama-langchain (github.com)\n\n참고 자료\n\n- Ollama\n- Docker Compose 개요 | Docker 문서\n- Docker 문서\n- Ollama — LLM을 어디서나 제공하는 런타임을 제공합니다. | 작성자: A B Vijay Kumar | 2024년 2월 | Medium","ogImage":{"url":"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png"},"coverImage":"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png","tag":["Tech"],"readingTime":6},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h2>생성적 AI 시리즈</h2>\n<p>이 블로그는 생성적 AI에 관한 지속적인 시리즈이며 이전 블로그의 연장선입니다. 이 블로그 시리즈에서는 Ollama를 탐색하고 도커를 사용하여 분산 아키텍처에 배포할 수 있는 응용 프로그램을 구축할 것입니다.</p>\n<p>Ollama는 강력한 언어 모델을 손쉽게 컴퓨터에서 실행할 수 있도록 도와주는 프레임워크입니다. Ollama 소개에 대해서는 2024년 2월에 A B Vijay Kumar에 의해 작성된 <a href=\"%EB%A7%81%ED%81%AC\">Ollama — Brings runtime to serve LLMs everywhere. | by A B Vijay Kumar | Feb, 2024 | Medium</a>를 참조해주세요. 이 블로그에서는 langchain 애플리케이션을 구축하고 도커에 배포할 것입니다.</p>\n<h2>Ollama를 위한 Langchain 챗봇 애플리케이션</h2>\n<p>Langshan을 사용하여 챗봇 애플리케이션을 개발해 보겠습니다. Python 애플리케이션에서 모델에 액세스하기 위해 간단한 Streamlit 챗봇 애플리케이션을 만들 것입니다. 이 Python 애플리케이션을 컨테이너에 배포하고 다른 컨테이너에서 Ollama를 사용할 것입니다. Docker-compose를 사용하여 인프라를 구축할 것입니다. 만약 Docker 또는 docker-compose를 사용하는 방법을 모르신다면, 계속 진행하기 전에 인터넷에서 몇 가지 자습서를 참고해 주세요.</p>\n<p>아래 그림은 컨테이너 간 상호 작용과 접근하는 포트를 보여주는 아키텍처를 보여줍니다.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png\" alt=\"아키텍처 그림\"></p>\n<p>컨테이너를 2개 생성할 것입니다.</p>\n<ul>\n<li>Ollama 컨테이너는 모델을 저장하고 로드하기 위해 호스트 볼륨을 사용합니다 (/root/.ollama는 로컬 ./data/ollama로 매핑됩니다). Ollama 컨테이너는 내부적으로 11434로 매핑된 외부 포트인 11434에서 수신 대기합니다.</li>\n<li>Streamlit 챗봇 애플리케이션은 내부적으로 8501로 매핑된 외부 포트인 8501에서 수신 대기합니다.\n코딩을 시작하기 전에 Python 가상 환경을 설정하겠습니다.</li>\n</ul>\n<pre><code class=\"hljs language-shell\">python3 -m venv ./ollama-langchain-venv\nsource ./ollama-langchain-venv/bin/activate\n</code></pre>\n<p>다음은 streamlit 애플리케이션의 소스 코드입니다.</p>\n<p>제가 이전 블로그에서 작성한 것과 매우 유사한 소스 코드입니다. 이 코드가 어떻게 작동하는지에 대한 자세한 내용은 다른 블로그인 \"Retrieval Augmented Generation(RAG) - LlamaIndex를 사용한 문서용 챗봇\"을 참조하실 수 있어요. 주요 차이점은 Ollama를 사용하고 Ollama Langchain 라이브러리를 통해 모델을 호출한다는 점이에요 (이는 langchain_community의 일부입니다).</p>\n<p>requirements.txt에서 종속성을 정의해 봅시다.</p>\n<p>이제 Streamlit 애플리케이션의 도커 이미지를 빌드하기 위한 Dockerfile을 정의해 보겠습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_3.png\" alt=\"image\"></p>\n<p>우리는 베이스 이미지로 python 도커 이미지를 사용하고, /app이라는 작업 디렉토리를 생성합니다. 그런 다음 애플리케이션 파일을 해당 디렉토리로 복사하고, pip를 사용하여 모든 종속성을 설치합니다. 그 후에 포트 8501을 노출하고 Streamlit 애플리케이션을 시작합니다.</p>\n<p>아래에 표시된 대로 docker build 명령을 사용하여 도커 이미지를 빌드할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_4.png\" alt=\"docker_image_4\"></p>\n<p>도커 이미지가 빌드되었는지 확인할 수 있어야 합니다. 다음에 표시된 것처럼 <code>docker images</code> 명령어를 사용하세요.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_5.png\" alt=\"docker_image_5\"></p>\n<p>이제 스트림릿 애플리케이션 및 올라마 컨테이너의 네트워크를 정의하는 docker-compose 구성 파일을 만들어보겠습니다. 이렇게 하면 두 애플리케이션이 상호 작용할 수 있습니다. 또한 위의 그림에서 보듯이 다양한 포트 구성을 정의할 것입니다. 올라마에 대해서는 모델들이 영구적으로 유지되도록 볼륨 매핑도 수행할 것입니다.</p>\n<p>도커 컴포즈 업 명령을 실행하면 어플리케이션을 실행할 수 있습니다. 도커 컴포즈 업을 실행하면 아래 스크린샷에 표시된 대로 두 컨테이너가 모두 실행되고 있는 것을 확인할 수 있어야 합니다.</p>\n<p>아래 스크린샷에 표시된 대로 도커 컴포즈 ps 명령을 실행하여 컨테이너가 실행 중인 것을 확인할 수 있어야 합니다.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_8.png\" alt=\"image1\"></p>\n<p>올라마가 실행 중인지 확인하려면 아래 스크린샷에 표시된 대로 <a href=\"http://localhost:11434%EC%9D%84\" rel=\"nofollow\" target=\"_blank\">http://localhost:11434을</a> 호출해야 합니다.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_9.png\" alt=\"image2\"></p>\n<p>이제 아래에 표시된 대로 docker exec 명령을 사용하여 도커 컨테이너에 로그인하여 필요한 모델을 다운로드해 봅시다.</p>\n<pre><code class=\"hljs language-js\">docker exec -it ollama-langchain-ollama-container-<span class=\"hljs-number\">1</span> ollama run phi\n</code></pre>\n<p>우리는 모델 phi를 사용 중이므로 해당 모델을 가져와서 실행하여 테스트 중입니다. 아래 스크린샷을 참고하세요. phi 모델이 다운로드되고 실행을 시작할 것입니다 (-it 플래그를 사용하므로 샘플 프롬프트로 상호작용 및 테스트가 가능해집니다).</p>\n<p>로컬 폴더 ./data/ollama에서 다운로드된 모델 파일 및 매니페스트를 확인할 수 있어야 합니다. 이 폴더는 내부적으로 컨테이너에 매핑되어 있으며(Ollama가 다운로드된 모델을 제공하기 위해 찾는 위치인 /root/.ollama에 매핑됨)</p>\n<p>아래는 Markdown 형식으로 변경한 테이블입니다.</p>\n<table>\n<thead>\n<tr>\n<th><code>&#x3C;img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_11.png\" /></code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Lets now run access our streamlit application by opening <a href=\"http://localhost:8501\" rel=\"nofollow\" target=\"_blank\">http://localhost:8501</a> on the browser. The following screenshot shows the interface</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th><code>&#x3C;img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_12.png\" /></code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Lets try to run a prompt “generate a story about dog called bozo”. You shud be able to see the console logs reflecting the API calls, that are coming from our Streamlit application, as shown below</td>\n</tr>\n</tbody>\n</table>\n<p>아래 스크린샷에서 확인할 수 있듯이, 내가 보낸 프롬프트에 대한 응답을 받았어요.</p>\n<p>도커 컴포즈 다운을 호출하여 배포를 중지할 수 있어요.</p>\n<p>다음 스크린샷은 결과를 보여줍니다.</p>\n<p><img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_15.png\" alt=\"output\"></p>\n<p>여기 있습니다. 이 블로그를 준비하면서 랭체인과 함께 Ollama가 작동하고 Docker-Compose를 사용하여 Docker에 배포하는 것이 정말 즐거웠어요.</p>\n<p>도움이 되었기를 바랍니다. 더 많은 실험으로 돌아오겠습니다. 그 동안 즐기고 코딩하세요!!! 곧 다시 만나요!!!</p>\n<p>여기서 전체 소스 코드에 액세스할 수 있습니다. abvijaykumar/ollama-langchain (github.com)</p>\n<p>참고 자료</p>\n<ul>\n<li>Ollama</li>\n<li>Docker Compose 개요 | Docker 문서</li>\n<li>Docker 문서</li>\n<li>Ollama — LLM을 어디서나 제공하는 런타임을 제공합니다. | 작성자: A B Vijay Kumar | 2024년 2월 | Medium</li>\n</ul>\n</body>\n</html>\n"},"__N_SSG":true}