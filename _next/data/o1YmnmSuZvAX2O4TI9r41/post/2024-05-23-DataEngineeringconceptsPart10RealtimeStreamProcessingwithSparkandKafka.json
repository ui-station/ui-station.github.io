{"pageProps":{"post":{"title":"데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리","description":"","date":"2024-05-23 14:05","slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka","content":"\n이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\n\n목차:\n\n1. 스트림 처리란\n2. 카프카의 특징\n3. 카프카 구성\n4. 카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\n5. 스파크 구조화 스트리밍 API\n6. 데이타브릭스 델타 레이크\n7. 실전 프로젝트\n\n이전 데이터 보안 부분으로 이동하는 링크입니다:\n\n## 스트림 처리란?\n\n<div class=\"content-ad\"></div>\n\n일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\n\n![데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png)\n\n스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\n\n카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 카프카 특징\n\n- 견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\n- 유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\n- 확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\n\n  - 파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\n  - 수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\n\n## 카프카 구성\n\n<div class=\"content-ad\"></div>\n\n카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\n\n- 파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\n- 복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\n- 유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\n- 자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\n- ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\n\n## 다른 카프카 서비스\n\nKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\" />\n\nksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\" />\n\n스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png)\n\n## Spark Structured Streaming API\n\nSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png)\n\n<div class=\"content-ad\"></div>\n\n따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\n\n또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png)\n\n## Databricks Delta Lake\n\n<div class=\"content-ad\"></div>\n\nDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\n\nDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\n\n스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 실시간 스트리밍 아키텍처\n\n카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\n\n문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\n\n- 단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\n- 단계 2: 토픽 생성\n- 단계 3: 클러스터 API 키 쌍 생성\n\n위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\n\n- 단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png)\n\nUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\n\n- Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n- 단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\n\n- 단계 7: Kafka 토픽에서 스트림 읽기\n\n```js\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Environmental Reporting\").getOrCreate()\n\nkafkaDF = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"abcd.us-west4.gcp.confluent.cloud:9092\") \\\n    .option(\"subscribe\", \"illegal_dumping\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\" password=\"\";\"\"\") \\\n    .load()\n\nprocessedDF = kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\ndisplay(processedDF)\n```\n\n- 단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pyspark.sql.functions as F\nfrom  pyspark.sql.functions import col, struct, to_json\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\njson_schema = StructType(\n  [\n    StructField(\"incident_type\", StringType(), nullable = False),\n    StructField(\"location\", StringType(), nullable = False),\n    StructField(\"description\", StringType(), nullable = True),\n    StructField(\"contact\", StringType(), nullable = True)\n  ]\n)\n\n# Using Spark SQL to write queries on the streaming data in processedDF\n\nquery = processedDF.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n      .select(F.col(\"value.incident_type\"),F.col(\"value.location\"))\ndisplay(query)\n```\n\nWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\n\nDefine a UDF(User Defined Function) to find out the region from the location:\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the regions_to_states dictionary\nregions_to_states = {\n    'South': ['West Virginia', 'District of Columbia', 'Maryland', 'Virginia',\n              'Kentucky', 'Tennessee', 'North Carolina', 'Mississippi',\n              'Arkansas', 'Louisiana', 'Alabama', 'Georgia', 'South Carolina',\n              'Florida', 'Delaware'],\n    'Southwest': ['Arizona', 'New Mexico', 'Oklahoma', 'Texas'],\n    'West': ['Washington', 'Oregon', 'California', 'Nevada', 'Idaho', 'Montana',\n             'Wyoming', 'Utah', 'Colorado', 'Alaska', 'Hawaii'],\n    'Midwest': ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota',\n                'Iowa', 'Missouri', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana',\n                'Ohio'],\n    'Northeast': ['Maine', 'Vermont', 'New York', 'New Hampshire', 'Massachusetts',\n                  'Rhode Island', 'Connecticut', 'New Jersey', 'Pennsylvania']\n}\n\n#from geotext import GeoText\nfrom geopy.geocoders import Nominatim\n\n# Define a function to extract state names from location text\ndef extract_state(location_text):\n    geolocator = Nominatim(user_agent=\"my_application\")\n    location = geolocator.geocode(location_text)\n    #print(location)\n    #print(type(location.raw))\n    if location:\n        state = location.raw['display_name'].split(',')[-2]\n        return state\n    else:\n        return \"Unknown\"\n\n# Create a UDF to map states to regions\n@udf(StringType())\ndef map_state_to_region(location):\n    state = extract_state(location).strip()\n    for region, states in regions_to_states.items():\n        if state in states:\n            return region\n    return \"Unknown\"  # Return \"Unknown\" for states not found in the dictionary\n\n# Apply the UDF to map states to regions\ndf_with_region = query.withColumn(\"region\", map_state_to_region(query[\"location\"]))\n\ndisplay(df_with_region)\n```\n\n\n<div class=\"content-ad\"></div>\n\n- 단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# VADER 감정 분석기 초기화\nanalyzer = SentimentIntensityAnalyzer()\n\n# Description 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef analyze_sentiment(description):\n    # VADER에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.polarity_scores(description)['neg']\n\n    # 감정 점수를 기반으로 심각도 분류\n    if sentiment_score >= 0.4:\n        return \"High\"\n    elif sentiment_score >= 0.2 and sentiment_score < 0.4:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# 감정 분석을 위한 UDF 생성\nsentiment_udf = udf(analyze_sentiment, StringType())\n\n# 처리된 DataFrame(processedDF)의 description 열에 UDF 적용\n# 실제 DataFrame 및 열 이름으로 \"processedDF\" 및 \"description_column\"을 대체합니다.\nprocessedDF_with_severity = query.withColumn(\"severity\", sentiment_udf(\"description\"))\n\n# 추가된 심각도 열이 있는 DataFrame 표시\ndisplay(processedDF_with_severity)\n```\n\n환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\n\n데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\n\n<div class=\"content-ad\"></div>\n\n![Real-time Stream Processing](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png)\n\n위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\n\n또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\n\n```js\n# 스트리밍 DataFrame을 임시 뷰로 등록\nprocessedDF_with_severity.createOrReplaceTempView(\"incident_reports\")\n\n# 집계를 위한 SQL 쿼리 정의\ntotal_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\"\"\n\nseverity_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\"\"\n\n# 집계 수행\ntotal_incidents_df = spark.sql(total_incidents_query)\nseverity_incidents_df = spark.sql(severity_incidents_query)\n\n```\n\n<div class=\"content-ad\"></div>\n\n\n\n![Data Engineering Concepts](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png)\n\nSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\n\n1. 비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\n2. 전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\n\n- 단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\n\n```js\n# Delta Lake에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \"`result_delta_table`\"\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \"/FileStore/tables/checkpoints\"\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.writeStream\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_location)\\\n  .trigger(processingTime='10 seconds')\\ # 10초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .format(\"delta\")\\\n  .toTable(delta_table_path)\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\n\n![Delta Table as Dataframe](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png)\n\n또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\n\n![Query Delta Table with SQL](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png)\n\n<div class=\"content-ad\"></div>\n\n읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\n\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png","tag":["Tech"],"readingTime":13},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.</p>\n<p>목차:</p>\n<ol>\n<li>스트림 처리란</li>\n<li>카프카의 특징</li>\n<li>카프카 구성</li>\n<li>카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리</li>\n<li>스파크 구조화 스트리밍 API</li>\n<li>데이타브릭스 델타 레이크</li>\n<li>실전 프로젝트</li>\n</ol>\n<p>이전 데이터 보안 부분으로 이동하는 링크입니다:</p>\n<h2>스트림 처리란?</h2>\n<p>일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png\" alt=\"데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리\"></p>\n<p>스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.</p>\n<p>카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.</p>\n<h2>카프카 특징</h2>\n<ul>\n<li>\n<p>견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.</p>\n</li>\n<li>\n<p>유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.</p>\n</li>\n<li>\n<p>확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:</p>\n<ul>\n<li>파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.</li>\n<li>수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.</li>\n</ul>\n</li>\n</ul>\n<h2>카프카 구성</h2>\n<p>카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.</p>\n<ul>\n<li>파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.</li>\n<li>복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.</li>\n<li>유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.</li>\n<li>자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.</li>\n<li>ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.</li>\n</ul>\n<h2>다른 카프카 서비스</h2>\n<p>Kafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.</p>\n<p>ksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.</p>\n<p>스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png\" alt=\"이미지\"></p>\n<h2>Spark Structured Streaming API</h2>\n<p>Spark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png\" alt=\"이미지\"></p>\n<p>따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.</p>\n<p>또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png\" alt=\"이미지\"></p>\n<h2>Databricks Delta Lake</h2>\n<p>Databricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.</p>\n<p>Delta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.</p>\n<p>스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.</p>\n<h2>실시간 스트리밍 아키텍처</h2>\n<p>카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.</p>\n<p>문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.</p>\n<p>이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:</p>\n<ul>\n<li>단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성</li>\n<li>단계 2: 토픽 생성</li>\n<li>단계 3: 클러스터 API 키 쌍 생성</li>\n</ul>\n<p>위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-</p>\n<ul>\n<li>단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.</li>\n</ul>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png\" alt=\"Image\"></p>\n<p>Use the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:</p>\n<ul>\n<li>Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated</li>\n</ul>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png\" alt=\"Image\"></p>\n<ul>\n<li>\n<p>단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성</p>\n</li>\n<li>\n<p>단계 7: Kafka 토픽에서 스트림 읽기</p>\n</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\n\nspark = <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span>.<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">\"Environmental Reporting\"</span>).<span class=\"hljs-title function_\">getOrCreate</span>()\n\nkafkaDF = spark \\\n    .<span class=\"hljs-property\">readStream</span> \\\n    .<span class=\"hljs-title function_\">format</span>(<span class=\"hljs-string\">\"kafka\"</span>) \\\n    .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"kafka.bootstrap.servers\"</span>, <span class=\"hljs-string\">\"abcd.us-west4.gcp.confluent.cloud:9092\"</span>) \\\n    .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"subscribe\"</span>, <span class=\"hljs-string\">\"illegal_dumping\"</span>) \\\n    .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"startingOffsets\"</span>, <span class=\"hljs-string\">\"earliest\"</span>) \\\n    .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"kafka.security.protocol\"</span>,<span class=\"hljs-string\">\"SASL_SSL\"</span>) \\\n    .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"kafka.sasl.mechanism\"</span>, <span class=\"hljs-string\">\"PLAIN\"</span>) \\\n    .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"kafka.sasl.jaas.config\"</span>, <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"</span><span class=\"hljs-string\">\" password=\"</span><span class=\"hljs-string\">\";\"</span><span class=\"hljs-string\">\"\"</span>) \\\n    .<span class=\"hljs-title function_\">load</span>()\n\nprocessedDF = kafkaDF.<span class=\"hljs-title function_\">selectExpr</span>(<span class=\"hljs-string\">\"CAST(key AS STRING)\"</span>, <span class=\"hljs-string\">\"CAST(value AS STRING)\"</span>)\n\n<span class=\"hljs-title function_\">display</span>(processedDF)\n</code></pre>\n<ul>\n<li>단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">as</span> F\n<span class=\"hljs-keyword\">from</span>  pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> col, struct, to_json\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StructField</span>, <span class=\"hljs-title class_\">StructType</span>, <span class=\"hljs-title class_\">StringType</span>, <span class=\"hljs-title class_\">MapType</span>\n\njson_schema = <span class=\"hljs-title class_\">StructType</span>(\n  [\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"incident_type\"</span>, <span class=\"hljs-title class_\">StringType</span>(), nullable = <span class=\"hljs-title class_\">False</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"location\"</span>, <span class=\"hljs-title class_\">StringType</span>(), nullable = <span class=\"hljs-title class_\">False</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"description\"</span>, <span class=\"hljs-title class_\">StringType</span>(), nullable = <span class=\"hljs-title class_\">True</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"contact\"</span>, <span class=\"hljs-title class_\">StringType</span>(), nullable = <span class=\"hljs-title class_\">True</span>)\n  ]\n)\n\n# <span class=\"hljs-title class_\">Using</span> <span class=\"hljs-title class_\">Spark</span> <span class=\"hljs-variable constant_\">SQL</span> to write queries on the streaming data <span class=\"hljs-keyword\">in</span> processedDF\n\nquery = processedDF.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">'value'</span>, F.<span class=\"hljs-title function_\">from_json</span>(F.<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">'value'</span>).<span class=\"hljs-title function_\">cast</span>(<span class=\"hljs-string\">'string'</span>), json_schema))  \\\n      .<span class=\"hljs-title function_\">select</span>(F.<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"value.incident_type\"</span>),F.<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"value.location\"</span>))\n<span class=\"hljs-title function_\">display</span>(query)\n</code></pre>\n<p>We will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.</p>\n<p>Define a UDF(User Defined Function) to find out the region from the location:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> udf\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StringType</span>\n\n# <span class=\"hljs-title class_\">Define</span> the regions_to_states dictionary\nregions_to_states = {\n    <span class=\"hljs-string\">'South'</span>: [<span class=\"hljs-string\">'West Virginia'</span>, <span class=\"hljs-string\">'District of Columbia'</span>, <span class=\"hljs-string\">'Maryland'</span>, <span class=\"hljs-string\">'Virginia'</span>,\n              <span class=\"hljs-string\">'Kentucky'</span>, <span class=\"hljs-string\">'Tennessee'</span>, <span class=\"hljs-string\">'North Carolina'</span>, <span class=\"hljs-string\">'Mississippi'</span>,\n              <span class=\"hljs-string\">'Arkansas'</span>, <span class=\"hljs-string\">'Louisiana'</span>, <span class=\"hljs-string\">'Alabama'</span>, <span class=\"hljs-string\">'Georgia'</span>, <span class=\"hljs-string\">'South Carolina'</span>,\n              <span class=\"hljs-string\">'Florida'</span>, <span class=\"hljs-string\">'Delaware'</span>],\n    <span class=\"hljs-string\">'Southwest'</span>: [<span class=\"hljs-string\">'Arizona'</span>, <span class=\"hljs-string\">'New Mexico'</span>, <span class=\"hljs-string\">'Oklahoma'</span>, <span class=\"hljs-string\">'Texas'</span>],\n    <span class=\"hljs-string\">'West'</span>: [<span class=\"hljs-string\">'Washington'</span>, <span class=\"hljs-string\">'Oregon'</span>, <span class=\"hljs-string\">'California'</span>, <span class=\"hljs-string\">'Nevada'</span>, <span class=\"hljs-string\">'Idaho'</span>, <span class=\"hljs-string\">'Montana'</span>,\n             <span class=\"hljs-string\">'Wyoming'</span>, <span class=\"hljs-string\">'Utah'</span>, <span class=\"hljs-string\">'Colorado'</span>, <span class=\"hljs-string\">'Alaska'</span>, <span class=\"hljs-string\">'Hawaii'</span>],\n    <span class=\"hljs-string\">'Midwest'</span>: [<span class=\"hljs-string\">'North Dakota'</span>, <span class=\"hljs-string\">'South Dakota'</span>, <span class=\"hljs-string\">'Nebraska'</span>, <span class=\"hljs-string\">'Kansas'</span>, <span class=\"hljs-string\">'Minnesota'</span>,\n                <span class=\"hljs-string\">'Iowa'</span>, <span class=\"hljs-string\">'Missouri'</span>, <span class=\"hljs-string\">'Wisconsin'</span>, <span class=\"hljs-string\">'Illinois'</span>, <span class=\"hljs-string\">'Michigan'</span>, <span class=\"hljs-string\">'Indiana'</span>,\n                <span class=\"hljs-string\">'Ohio'</span>],\n    <span class=\"hljs-string\">'Northeast'</span>: [<span class=\"hljs-string\">'Maine'</span>, <span class=\"hljs-string\">'Vermont'</span>, <span class=\"hljs-string\">'New York'</span>, <span class=\"hljs-string\">'New Hampshire'</span>, <span class=\"hljs-string\">'Massachusetts'</span>,\n                  <span class=\"hljs-string\">'Rhode Island'</span>, <span class=\"hljs-string\">'Connecticut'</span>, <span class=\"hljs-string\">'New Jersey'</span>, <span class=\"hljs-string\">'Pennsylvania'</span>]\n}\n\n#<span class=\"hljs-keyword\">from</span> geotext <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">GeoText</span>\n<span class=\"hljs-keyword\">from</span> geopy.<span class=\"hljs-property\">geocoders</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Nominatim</span>\n\n# <span class=\"hljs-title class_\">Define</span> a <span class=\"hljs-keyword\">function</span> to extract state names <span class=\"hljs-keyword\">from</span> location text\ndef <span class=\"hljs-title function_\">extract_state</span>(location_text):\n    geolocator = <span class=\"hljs-title class_\">Nominatim</span>(user_agent=<span class=\"hljs-string\">\"my_application\"</span>)\n    location = geolocator.<span class=\"hljs-title function_\">geocode</span>(location_text)\n    #<span class=\"hljs-title function_\">print</span>(location)\n    #<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-title function_\">type</span>(location.<span class=\"hljs-property\">raw</span>))\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-attr\">location</span>:\n        state = location.<span class=\"hljs-property\">raw</span>[<span class=\"hljs-string\">'display_name'</span>].<span class=\"hljs-title function_\">split</span>(<span class=\"hljs-string\">','</span>)[-<span class=\"hljs-number\">2</span>]\n        <span class=\"hljs-keyword\">return</span> state\n    <span class=\"hljs-attr\">else</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Unknown\"</span>\n\n# <span class=\"hljs-title class_\">Create</span> a <span class=\"hljs-variable constant_\">UDF</span> to map states to regions\n@<span class=\"hljs-title function_\">udf</span>(<span class=\"hljs-title class_\">StringType</span>())\ndef <span class=\"hljs-title function_\">map_state_to_region</span>(location):\n    state = <span class=\"hljs-title function_\">extract_state</span>(location).<span class=\"hljs-title function_\">strip</span>()\n    <span class=\"hljs-keyword\">for</span> region, states <span class=\"hljs-keyword\">in</span> regions_to_states.<span class=\"hljs-title function_\">items</span>():\n        <span class=\"hljs-keyword\">if</span> state <span class=\"hljs-keyword\">in</span> <span class=\"hljs-attr\">states</span>:\n            <span class=\"hljs-keyword\">return</span> region\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Unknown\"</span>  # <span class=\"hljs-title class_\">Return</span> <span class=\"hljs-string\">\"Unknown\"</span> <span class=\"hljs-keyword\">for</span> states not found <span class=\"hljs-keyword\">in</span> the dictionary\n\n# <span class=\"hljs-title class_\">Apply</span> the <span class=\"hljs-variable constant_\">UDF</span> to map states to regions\ndf_with_region = query.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"region\"</span>, <span class=\"hljs-title function_\">map_state_to_region</span>(query[<span class=\"hljs-string\">\"location\"</span>]))\n\n<span class=\"hljs-title function_\">display</span>(df_with_region)\n</code></pre>\n<ul>\n<li>단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> udf\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StringType</span>\n<span class=\"hljs-keyword\">from</span> vaderSentiment.<span class=\"hljs-property\">vaderSentiment</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SentimentIntensityAnalyzer</span>\n\n# <span class=\"hljs-variable constant_\">VADER</span> 감정 분석기 초기화\nanalyzer = <span class=\"hljs-title class_\">SentimentIntensityAnalyzer</span>()\n\n# <span class=\"hljs-title class_\">Description</span> 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef <span class=\"hljs-title function_\">analyze_sentiment</span>(description):\n    # <span class=\"hljs-variable constant_\">VADER</span>에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.<span class=\"hljs-title function_\">polarity_scores</span>(description)[<span class=\"hljs-string\">'neg'</span>]\n\n    # 감정 점수를 기반으로 심각도 분류\n    <span class=\"hljs-keyword\">if</span> sentiment_score >= <span class=\"hljs-number\">0.4</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"High\"</span>\n    elif sentiment_score >= <span class=\"hljs-number\">0.2</span> and sentiment_score &#x3C; <span class=\"hljs-number\">0.4</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Medium\"</span>\n    <span class=\"hljs-attr\">else</span>:\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"Low\"</span>\n\n# 감정 분석을 위한 <span class=\"hljs-variable constant_\">UDF</span> 생성\nsentiment_udf = <span class=\"hljs-title function_\">udf</span>(analyze_sentiment, <span class=\"hljs-title class_\">StringType</span>())\n\n# 처리된 <span class=\"hljs-title class_\">DataFrame</span>(processedDF)의 description 열에 <span class=\"hljs-variable constant_\">UDF</span> 적용\n# 실제 <span class=\"hljs-title class_\">DataFrame</span> 및 열 이름으로 <span class=\"hljs-string\">\"processedDF\"</span> 및 <span class=\"hljs-string\">\"description_column\"</span>을 대체합니다.\nprocessedDF_with_severity = query.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"severity\"</span>, <span class=\"hljs-title function_\">sentiment_udf</span>(<span class=\"hljs-string\">\"description\"</span>))\n\n# 추가된 심각도 열이 있는 <span class=\"hljs-title class_\">DataFrame</span> 표시\n<span class=\"hljs-title function_\">display</span>(processedDF_with_severity)\n</code></pre>\n<p>환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.</p>\n<p>데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png\" alt=\"Real-time Stream Processing\"></p>\n<p>위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:</p>\n<p>또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:</p>\n<pre><code class=\"hljs language-js\"># 스트리밍 <span class=\"hljs-title class_\">DataFrame</span>을 임시 뷰로 등록\nprocessedDF_with_severity.<span class=\"hljs-title function_\">createOrReplaceTempView</span>(<span class=\"hljs-string\">\"incident_reports\"</span>)\n\n# 집계를 위한 <span class=\"hljs-variable constant_\">SQL</span> 쿼리 정의\ntotal_incidents_query = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"</span><span class=\"hljs-string\">\"\"</span>\n\nseverity_incidents_query = <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"</span><span class=\"hljs-string\">\"\"</span>\n\n# 집계 수행\ntotal_incidents_df = spark.<span class=\"hljs-title function_\">sql</span>(total_incidents_query)\nseverity_incidents_df = spark.<span class=\"hljs-title function_\">sql</span>(severity_incidents_query)\n\n</code></pre>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png\" alt=\"Data Engineering Concepts\"></p>\n<p>Spark 구조화된 스트리밍 분석에 대한 일부 제한 사항:</p>\n<ol>\n<li>비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.</li>\n<li>전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.</li>\n</ol>\n<ul>\n<li>단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다</li>\n</ul>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">Delta</span> <span class=\"hljs-title class_\">Lake</span>에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = <span class=\"hljs-string\">\"`result_delta_table`\"</span>\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = <span class=\"hljs-string\">\"/FileStore/tables/checkpoints\"</span>\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.<span class=\"hljs-property\">writeStream</span>\\\n  .<span class=\"hljs-title function_\">outputMode</span>(<span class=\"hljs-string\">\"append\"</span>)\\\n  .<span class=\"hljs-title function_\">option</span>(<span class=\"hljs-string\">\"checkpointLocation\"</span>, checkpoint_location)\\\n  .<span class=\"hljs-title function_\">trigger</span>(processingTime=<span class=\"hljs-string\">'10 seconds'</span>)\\ # <span class=\"hljs-number\">10</span>초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .<span class=\"hljs-title function_\">format</span>(<span class=\"hljs-string\">\"delta\"</span>)\\\n  .<span class=\"hljs-title function_\">toTable</span>(delta_table_path)\n</code></pre>\n<p>그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png\" alt=\"Delta Table as Dataframe\"></p>\n<p>또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:</p>\n<p><img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png\" alt=\"Query Delta Table with SQL\"></p>\n<p>읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:</p>\n</body>\n</html>\n"},"__N_SSG":true}