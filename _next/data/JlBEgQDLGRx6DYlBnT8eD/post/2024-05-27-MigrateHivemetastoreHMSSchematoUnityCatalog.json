{"pageProps":{"post":{"title":"하이브 메타스토어 HMS 스키마를 유니티 카탈로그로 이관하기","description":"","date":"2024-05-27 17:16","slug":"2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog","content":"\n<img src=\"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png\" />\n\nHMS에서 Unity Catalog로의 이주 이야기에 오신 것을 환영합니다.\n\n본 글에서는 HMS에서 Unity Catalog로의 이주 과정을 공유하고자 합니다. HMS를 Unity Catalog로 마이그레이션하기 위한 여러 도구들이 있음을 알고 있습니다. 특히 현재 시장에서 인기를 끌고 있는 UCX가 있습니다. 아직 UCX를 탐험해보지는 않았지만, 앞으로 UCX를 살펴볼 예정입니다.\nUCX를 사용해보고 싶다면, https://github.com/databrickslabs/ucx 에서 확인하고 그 경험을 공유해주세요.\n\n본 글의 범위\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 외부 테이블을 Unity 카탈로그로 이주합니다.\n\n이 글에서 다루겠습니다.\n\n- 관리형 테이블을 Unity 카탈로그로 이주합니다.\n  https://medium.com/@data_engineering_0216/migrate-managed-table-to-unity-catalog-ab4dbba9d6aa\n- 뷰를 Unity 카탈로그로 이주합니다.\n  https://medium.com/@data_engineering_0216/migrate-views-from-hive-metastore-to-unity-catalog-7aac5ec1da50\n- 메타데이터 기반 권한 관리\n  https://medium.com/@data_engineering_0216/unity-catalog-permissions-f1e6221cbc68\n- https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate\n\n외부 테이블을 Unity 카탈로그로 이주합니다.\n\n준비물\n\n- 메타스토어 또는 카탈로그 관리자 권한\n- Unity 카탈로그가 활성화된 Databricks 워크스페이스\n- Unity 카탈로그가 활성화된 클러스터\n- Databricks 접근 커넥터\n- 마운트 지점과 동등한 스토리지 자격 증명 및 외부 위치(읽기 및 쓰기 권한 필요)\n- 워크스페이스 또는 클러스터 수준에서 기본 카탈로그 설정: 선택 사항\n  클러스터 구성: spark.databricks.sql.initial.catalog.name gold_dv\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n솔루션을 깊이 살펴보겠습니다. 사용자 정의 Python 함수인 migrate_tables_to_unity_catalog을 살펴봅시다. 이 코드는 외부 테이블을 하나의 (hive_metastore) 카탈로그에서 다른 카탈로그(Unity Catalog)로 동기화하는 함수를 정의합니다.\n\n이 함수는 다음과 같은 매개변수를 사용합니다:\n\n- src_ct_name: 원본 카탈로그의 이름.\n- src_databases: 원본 카탈로그에서 가져온 데이터베이스 사전의 목록.\n- dst_ct_name: 대상 카탈로그의 이름.\n- exclude_databases: 마이그레이션에서 제외할 데이터베이스 이름의 목록.\n- full_reset: 전체 리셋을 수행해야 하는지 여부를 나타내는 부울 플래그.\n\n이 함수는 먼저 src_databases 매개변수에서 데이터베이스 이름 목록을 작성합니다. 제외할 데이터베이스가 있는 경우 해당 데이터베이스를 목록에서 필터링합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼, 각 데이터베이스를 처리하는 process_database 내부 함수를 정의합니다. full_reset이 True 인 경우, 대상 카탈로그의 데이터베이스를 삭제하고 새 데이터베이스를 생성한 후 소스 카탈로그에서 대상 카탈로그로 스키마를 동기화합니다. full_reset이 False 인 경우, 새 데이터베이스를 생성하고 스키마를 동기화합니다. full_reset이 제공되지 않은 경우, 재설정 모드를 요청하는 메시지를 출력합니다.\n\nconcurrent.futures.ThreadPoolExecutor를 사용하여 함수는 각 데이터베이스를 병렬로 처리하도록 제출합니다. 모든 futures가 완료되기를 기다리고 처리 중 발생한 예외를 처리합니다.\n\n이 함수를 사용하려면 필요한 매개변수를 제공하고 함수를 호출해야 합니다.\n\n```python\nimport concurrent.futures\n\n\ndef migrate_tables_to_unity_catalog(src_ct_name, src_databases, dst_ct_name, exclude_databases, full_reset):\n    \"\"\"\n    한 카탈로그에서 다른 카탈로그로 관리되는 모든 테이블을 복사합니다.\n\n    매개변수:\n        src_ct_name (str): 원본 카탈로그의 이름.\n        src_databases (list): 원본 카탈로그에서의 데이터베이스 딕셔너리 목록.\n        dst_ct_name (str): 대상 카탈로그의 이름.\n        exclude_databases (list): 마이그레이션에서 제외할 데이터베이스 이름 목록.\n\n    반환:\n        None\n\n    예외:\n        None\n\n    예시:\n        src_ct_name       = \"hive_metastore\"\n        src_databases     = spark.sql(f\"SHOW DATABASES IN {src_ct_name}\").collect()\n        dst_ct_name       = \"uc_dv\"\n        exclude_databases = [\"poc\", \"temp_tbd\", \"default\"]\n        migrate_managed_tables_to_unity_catalog(src_ct_name, src_databases, dst_ct_name, exclude_databases)\n\n    \"\"\"\n\n    list_of_db = []\n    for db in src_databases:\n        dbName = db['databaseName']\n        list_of_db.append(dbName)\n    if exclude_databases:\n        databases = [x for x in list_of_db if x not in exclude_databases]\n    else:\n        databases = list_of_db\n\n    print(databases)\n\n    def process_database(db, full_reset):\n\n        if full_reset == True:\n            drop_db = f\"DROP DATABASE IF EXISTS {dst_ct_name}.{db} CASCADE\"\n            display(spark.sql(drop_db))\n            create_db = f\"CREATE DATABASE IF NOT EXISTS {dst_ct_name}.{db}\"\n            display(spark.sql(create_db))\n            query = f\"SYNC SCHEMA {dst_ct_name}.{db} from {src_ct_name}.{db}\" # SYNC SCHEMA uc_dv.gold from hive_metastore.clean\n\n            print(query)\n            display(spark.sql(query))\n        elif full_reset == False:\n            create_db = f\"CREATE DATABASE IF NOT EXISTS {dst_ct_name}.{db}\"\n            display(spark.sql(create_db))\n            query = f\"SYNC SCHEMA {dst_ct_name}.{db} from {src_ct_name}.{db}\" # SYNC SCHEMA uc_dv.gold from hive_metastore.clean\n\n            print(query)\n            display(spark.sql(query))\n        else:\n            print(\"재설정 모드를 제공해주세요\")\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = [executor.submit(process_database, db, full_reset) for db in databases]\n        # 모든 futures가 완료되기를 기다림\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                # 각 future의 결과를 가져옴\n                result = future.result()\n            except Exception as e:\n                # 발생한 예외 처리\n                pass\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 코드는 다음 작업을 수행합니다:\n\n```js\nsrc_ct_name       = \"hive_metastore\"\nsrc_databases     = spark.sql(f\"SHOW DATABASES IN {src_ct_name}\").collect()\ndst_ct_name       = \"uc_dv\" #spark.sql(\"SELECT current_catalog()\").collect()[0]['current_catalog()']\nexclude_databases = [\"poc\", \"temp_tbd\", \"default\"]\nfull_reset         = False\n\nmigrate_tables_to_unity_catalog(src_ct_name,src_databases,dst_ct_name,exclude_databases,full_reset)\n```\n\n- \"src_ct_name\" 변수를 정의하여 값 \"hive_metastore\"를 할당합니다.\n- Spark를 사용하여 src_ct_name 카탈로그 내의 데이터베이스 목록을 검색하는 SQL 쿼리를 실행하고 결과를 src_databases 변수에 할당합니다.\n- Spark를 사용하여 현재 카탈로그를 검색하는 SQL 쿼리를 실행하고 결과를 dst_ct_name 변수에 할당합니다.\n- 마이그레이션 프로세스에서 제외될 데이터베이스 이름을 포함하는 \"exclude_databases\" 목록을 정의합니다.\n- 값이 False인 부울 변수 \"full_reset\"을 정의합니다.\n- migrate_tables_to_unity_catalog 함수를 src_ct_name, src_databases, dst_ct_name, exclude_databases 및 full_reset 매개변수로 호출합니다. 이 함수는 소스 카탈로그에서 대상 카탈로그로 테이블을 마이그레이션하는 역할을 합니다.\n\n위의 Python 함수는 hive_metastore의 외부 테이블을 unity Catalog로 몇 분 안에 마이그레이션하는 데 유용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 코드를 Databricks 노트북에 복사하여 Adf나 Databricks 일정으로 매일 실행하면 Unity Catalog로 완전히 마이그레이션할 때까지 도움을 줄 수 있습니다. 예를 들어, hive_metastore에 새 테이블을 추가하면 일정이 자동으로 새로운 테이블을 Unity Catalog에 동기화합니다.\n\nUnity Catalog로의 마이그레이션 여정에 도움이 되기를 바라며, 궁금한 점이 있으시면 언제든지 물어보세요.\n","ogImage":{"url":"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png"},"coverImage":"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png","tag":["Tech"],"readingTime":8},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<img src=\"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png\">\n<p>HMS에서 Unity Catalog로의 이주 이야기에 오신 것을 환영합니다.</p>\n<p>본 글에서는 HMS에서 Unity Catalog로의 이주 과정을 공유하고자 합니다. HMS를 Unity Catalog로 마이그레이션하기 위한 여러 도구들이 있음을 알고 있습니다. 특히 현재 시장에서 인기를 끌고 있는 UCX가 있습니다. 아직 UCX를 탐험해보지는 않았지만, 앞으로 UCX를 살펴볼 예정입니다.\nUCX를 사용해보고 싶다면, <a href=\"https://github.com/databrickslabs/ucx\" rel=\"nofollow\" target=\"_blank\">https://github.com/databrickslabs/ucx</a> 에서 확인하고 그 경험을 공유해주세요.</p>\n<p>본 글의 범위</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>외부 테이블을 Unity 카탈로그로 이주합니다.</li>\n</ul>\n<p>이 글에서 다루겠습니다.</p>\n<ul>\n<li>관리형 테이블을 Unity 카탈로그로 이주합니다.\n<a href=\"https://medium.com/@data_engineering_0216/migrate-managed-table-to-unity-catalog-ab4dbba9d6aa\" rel=\"nofollow\" target=\"_blank\">https://medium.com/@data_engineering_0216/migrate-managed-table-to-unity-catalog-ab4dbba9d6aa</a></li>\n<li>뷰를 Unity 카탈로그로 이주합니다.\n<a href=\"https://medium.com/@data_engineering_0216/migrate-views-from-hive-metastore-to-unity-catalog-7aac5ec1da50\" rel=\"nofollow\" target=\"_blank\">https://medium.com/@data_engineering_0216/migrate-views-from-hive-metastore-to-unity-catalog-7aac5ec1da50</a></li>\n<li>메타데이터 기반 권한 관리\n<a href=\"https://medium.com/@data_engineering_0216/unity-catalog-permissions-f1e6221cbc68\" rel=\"nofollow\" target=\"_blank\">https://medium.com/@data_engineering_0216/unity-catalog-permissions-f1e6221cbc68</a></li>\n<li><a href=\"https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate\" rel=\"nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate</a></li>\n</ul>\n<p>외부 테이블을 Unity 카탈로그로 이주합니다.</p>\n<p>준비물</p>\n<ul>\n<li>메타스토어 또는 카탈로그 관리자 권한</li>\n<li>Unity 카탈로그가 활성화된 Databricks 워크스페이스</li>\n<li>Unity 카탈로그가 활성화된 클러스터</li>\n<li>Databricks 접근 커넥터</li>\n<li>마운트 지점과 동등한 스토리지 자격 증명 및 외부 위치(읽기 및 쓰기 권한 필요)</li>\n<li>워크스페이스 또는 클러스터 수준에서 기본 카탈로그 설정: 선택 사항\n클러스터 구성: spark.databricks.sql.initial.catalog.name gold_dv</li>\n</ul>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>솔루션을 깊이 살펴보겠습니다. 사용자 정의 Python 함수인 migrate_tables_to_unity_catalog을 살펴봅시다. 이 코드는 외부 테이블을 하나의 (hive_metastore) 카탈로그에서 다른 카탈로그(Unity Catalog)로 동기화하는 함수를 정의합니다.</p>\n<p>이 함수는 다음과 같은 매개변수를 사용합니다:</p>\n<ul>\n<li>src_ct_name: 원본 카탈로그의 이름.</li>\n<li>src_databases: 원본 카탈로그에서 가져온 데이터베이스 사전의 목록.</li>\n<li>dst_ct_name: 대상 카탈로그의 이름.</li>\n<li>exclude_databases: 마이그레이션에서 제외할 데이터베이스 이름의 목록.</li>\n<li>full_reset: 전체 리셋을 수행해야 하는지 여부를 나타내는 부울 플래그.</li>\n</ul>\n<p>이 함수는 먼저 src_databases 매개변수에서 데이터베이스 이름 목록을 작성합니다. 제외할 데이터베이스가 있는 경우 해당 데이터베이스를 목록에서 필터링합니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>그럼, 각 데이터베이스를 처리하는 process_database 내부 함수를 정의합니다. full_reset이 True 인 경우, 대상 카탈로그의 데이터베이스를 삭제하고 새 데이터베이스를 생성한 후 소스 카탈로그에서 대상 카탈로그로 스키마를 동기화합니다. full_reset이 False 인 경우, 새 데이터베이스를 생성하고 스키마를 동기화합니다. full_reset이 제공되지 않은 경우, 재설정 모드를 요청하는 메시지를 출력합니다.</p>\n<p>concurrent.futures.ThreadPoolExecutor를 사용하여 함수는 각 데이터베이스를 병렬로 처리하도록 제출합니다. 모든 futures가 완료되기를 기다리고 처리 중 발생한 예외를 처리합니다.</p>\n<p>이 함수를 사용하려면 필요한 매개변수를 제공하고 함수를 호출해야 합니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> concurrent.futures\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">migrate_tables_to_unity_catalog</span>(<span class=\"hljs-params\">src_ct_name, src_databases, dst_ct_name, exclude_databases, full_reset</span>):\n    <span class=\"hljs-string\">\"\"\"\n    한 카탈로그에서 다른 카탈로그로 관리되는 모든 테이블을 복사합니다.\n\n    매개변수:\n        src_ct_name (str): 원본 카탈로그의 이름.\n        src_databases (list): 원본 카탈로그에서의 데이터베이스 딕셔너리 목록.\n        dst_ct_name (str): 대상 카탈로그의 이름.\n        exclude_databases (list): 마이그레이션에서 제외할 데이터베이스 이름 목록.\n\n    반환:\n        None\n\n    예외:\n        None\n\n    예시:\n        src_ct_name       = \"hive_metastore\"\n        src_databases     = spark.sql(f\"SHOW DATABASES IN {src_ct_name}\").collect()\n        dst_ct_name       = \"uc_dv\"\n        exclude_databases = [\"poc\", \"temp_tbd\", \"default\"]\n        migrate_managed_tables_to_unity_catalog(src_ct_name, src_databases, dst_ct_name, exclude_databases)\n\n    \"\"\"</span>\n\n    list_of_db = []\n    <span class=\"hljs-keyword\">for</span> db <span class=\"hljs-keyword\">in</span> src_databases:\n        dbName = db[<span class=\"hljs-string\">'databaseName'</span>]\n        list_of_db.append(dbName)\n    <span class=\"hljs-keyword\">if</span> exclude_databases:\n        databases = [x <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> list_of_db <span class=\"hljs-keyword\">if</span> x <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> exclude_databases]\n    <span class=\"hljs-keyword\">else</span>:\n        databases = list_of_db\n\n    <span class=\"hljs-built_in\">print</span>(databases)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">process_database</span>(<span class=\"hljs-params\">db, full_reset</span>):\n\n        <span class=\"hljs-keyword\">if</span> full_reset == <span class=\"hljs-literal\">True</span>:\n            drop_db = <span class=\"hljs-string\">f\"DROP DATABASE IF EXISTS <span class=\"hljs-subst\">{dst_ct_name}</span>.<span class=\"hljs-subst\">{db}</span> CASCADE\"</span>\n            display(spark.sql(drop_db))\n            create_db = <span class=\"hljs-string\">f\"CREATE DATABASE IF NOT EXISTS <span class=\"hljs-subst\">{dst_ct_name}</span>.<span class=\"hljs-subst\">{db}</span>\"</span>\n            display(spark.sql(create_db))\n            query = <span class=\"hljs-string\">f\"SYNC SCHEMA <span class=\"hljs-subst\">{dst_ct_name}</span>.<span class=\"hljs-subst\">{db}</span> from <span class=\"hljs-subst\">{src_ct_name}</span>.<span class=\"hljs-subst\">{db}</span>\"</span> <span class=\"hljs-comment\"># SYNC SCHEMA uc_dv.gold from hive_metastore.clean</span>\n\n            <span class=\"hljs-built_in\">print</span>(query)\n            display(spark.sql(query))\n        <span class=\"hljs-keyword\">elif</span> full_reset == <span class=\"hljs-literal\">False</span>:\n            create_db = <span class=\"hljs-string\">f\"CREATE DATABASE IF NOT EXISTS <span class=\"hljs-subst\">{dst_ct_name}</span>.<span class=\"hljs-subst\">{db}</span>\"</span>\n            display(spark.sql(create_db))\n            query = <span class=\"hljs-string\">f\"SYNC SCHEMA <span class=\"hljs-subst\">{dst_ct_name}</span>.<span class=\"hljs-subst\">{db}</span> from <span class=\"hljs-subst\">{src_ct_name}</span>.<span class=\"hljs-subst\">{db}</span>\"</span> <span class=\"hljs-comment\"># SYNC SCHEMA uc_dv.gold from hive_metastore.clean</span>\n\n            <span class=\"hljs-built_in\">print</span>(query)\n            display(spark.sql(query))\n        <span class=\"hljs-keyword\">else</span>:\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"재설정 모드를 제공해주세요\"</span>)\n\n    <span class=\"hljs-keyword\">with</span> concurrent.futures.ThreadPoolExecutor() <span class=\"hljs-keyword\">as</span> executor:\n        futures = [executor.submit(process_database, db, full_reset) <span class=\"hljs-keyword\">for</span> db <span class=\"hljs-keyword\">in</span> databases]\n        <span class=\"hljs-comment\"># 모든 futures가 완료되기를 기다림</span>\n        <span class=\"hljs-keyword\">for</span> future <span class=\"hljs-keyword\">in</span> concurrent.futures.as_completed(futures):\n            <span class=\"hljs-keyword\">try</span>:\n                <span class=\"hljs-comment\"># 각 future의 결과를 가져옴</span>\n                result = future.result()\n            <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n                <span class=\"hljs-comment\"># 발생한 예외 처리</span>\n                <span class=\"hljs-keyword\">pass</span>\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>아래 코드는 다음 작업을 수행합니다:</p>\n<pre><code class=\"hljs language-js\">src_ct_name       = <span class=\"hljs-string\">\"hive_metastore\"</span>\nsrc_databases     = spark.<span class=\"hljs-title function_\">sql</span>(f<span class=\"hljs-string\">\"SHOW DATABASES IN {src_ct_name}\"</span>).<span class=\"hljs-title function_\">collect</span>()\ndst_ct_name       = <span class=\"hljs-string\">\"uc_dv\"</span> #spark.<span class=\"hljs-title function_\">sql</span>(<span class=\"hljs-string\">\"SELECT current_catalog()\"</span>).<span class=\"hljs-title function_\">collect</span>()[<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'current_catalog()'</span>]\nexclude_databases = [<span class=\"hljs-string\">\"poc\"</span>, <span class=\"hljs-string\">\"temp_tbd\"</span>, <span class=\"hljs-string\">\"default\"</span>]\nfull_reset         = <span class=\"hljs-title class_\">False</span>\n\n<span class=\"hljs-title function_\">migrate_tables_to_unity_catalog</span>(src_ct_name,src_databases,dst_ct_name,exclude_databases,full_reset)\n</code></pre>\n<ul>\n<li>\"src_ct_name\" 변수를 정의하여 값 \"hive_metastore\"를 할당합니다.</li>\n<li>Spark를 사용하여 src_ct_name 카탈로그 내의 데이터베이스 목록을 검색하는 SQL 쿼리를 실행하고 결과를 src_databases 변수에 할당합니다.</li>\n<li>Spark를 사용하여 현재 카탈로그를 검색하는 SQL 쿼리를 실행하고 결과를 dst_ct_name 변수에 할당합니다.</li>\n<li>마이그레이션 프로세스에서 제외될 데이터베이스 이름을 포함하는 \"exclude_databases\" 목록을 정의합니다.</li>\n<li>값이 False인 부울 변수 \"full_reset\"을 정의합니다.</li>\n<li>migrate_tables_to_unity_catalog 함수를 src_ct_name, src_databases, dst_ct_name, exclude_databases 및 full_reset 매개변수로 호출합니다. 이 함수는 소스 카탈로그에서 대상 카탈로그로 테이블을 마이그레이션하는 역할을 합니다.</li>\n</ul>\n<p>위의 Python 함수는 hive_metastore의 외부 테이블을 unity Catalog로 몇 분 안에 마이그레이션하는 데 유용합니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>위 코드를 Databricks 노트북에 복사하여 Adf나 Databricks 일정으로 매일 실행하면 Unity Catalog로 완전히 마이그레이션할 때까지 도움을 줄 수 있습니다. 예를 들어, hive_metastore에 새 테이블을 추가하면 일정이 자동으로 새로운 테이블을 Unity Catalog에 동기화합니다.</p>\n<p>Unity Catalog로의 마이그레이션 여정에 도움이 되기를 바라며, 궁금한 점이 있으시면 언제든지 물어보세요.</p>\n</body>\n</html>\n"},"__N_SSG":true}