{"pageProps":{"posts":[{"title":"Shadowfax 안드로이드 앱 40 더 빠르게 만드는 방법","description":"","date":"2024-06-23 01:14","slug":"2024-06-23-MakingShadowfaxAndroidApp40faster","content":"\n![Image](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_0.png)\n\n# 1. 목표 설정\n\n모바일 앱 성능에는 모든 밀리초가 중요합니다. 앱이 빨리 로드될수록 사용자가 머물 가능성이 높아집니다.\n\n매일 10만 명 이상의 DAU를 보유한 Shadowfax Rider 앱은 앱이 실행되는 데 약 3.5초가 걸리는 문제에 직면했습니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n목표는 다음과 같이이 시간을 단축하는 것이었습니다:\n\n- 90 백분위에는 2 초\n- 중간 사용자에 대해 800ms\n\n## 2. 앱 시작 시간 측정\n\nFirebase에 따르면 앱 시작 시간은 런처에서 앱이 시작되어 첫 번째 액티비티의 `onResume()` 메서드가 호출 될 때까지의 지속 시간입니다. 이 기간은 다음과 같이 logcat에도 보고됩니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기에서 더 많은 정보를 읽을 수 있습니다. Firebase에서 Startup 시간이 진실의 근원이었습니다.\n\n만약 onResume이 호출된 후 어떤 시점에 앱이 완전히 로드된 것으로 간주한다면 (지도가 완전히 그려진 후와 같이), 해당 시점을 시스템 및 Firebase에 Activity.reportFullyDrawn()으로 보고할 수 있습니다.\n\n만약 Perfetto를 사용하고 있다면, 나중에 그에 대해 자세히 설명하겠습니다.\n\n# 3. 자세히 들여다보기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n앱의 시작 시간을 기간별로 분해하기 위해 Firebase 성능 라이브러리의 @trace 어노테이션을 app 클래스의 onCreate() 함수, BaseActivity 및 MainActivity의 onCreate() & onStart()에 추가했어요. 기본적으로 모든 것을 최상위 수준에서 측정하므로 주요 원인을 파악하고 거기서부터 드릴다운합니다.\n\n![이미지](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_1.png)\n\nMain 및 Base 액티비티 이외에도 앱 클래스가 앱 시작 시간의 30%를 차지했기 때문에 2가지 작업을 수행했어요:\n\n# 3.1 라이브러리 및 콘텐츠 제공자 지연 로드 (-10% 시작 시간)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n애플리케이션 클래스는 일반적으로 많은 라이브러리를 초기화합니다. 필요하지 않은 라이브러리는 앱 시작 시 즉시 초기화하는 대신 백그라운드에서 초기화하도록 변경했습니다. Content Provider가 있다면 Startup 라이브러리를 사용하여 그것들을 나중에 로딩할 수도 있습니다.\n\n다음은 몇 가지 SDK를 백그라운드 스레드에서 초기화하는 방법입니다:\n\n```js\nclass MyApp : Application() {\n    @AddTrace(name = \"backgroundInitializationsTrace\")\n    private fun performBackgroundInitializations() {\n        runWithLooper {\n            // 메인 스레드에서 초기화가 필요하지 않은 SDK 초기화\n        }\n    }\n}\n\nobject MyUtils {\n    // 백그라운드 스레드에서 초기화를 위한 유틸리티 함수\n    fun runWithLooper(runnable: Runnable) {\n        val threadHandler = HandlerThread(\"Thread${System.currentTimeMillis()}\")\n        try {\n            threadHandler.start()\n            val handler = Handler(threadHandler.looper)\n            handler.post {\n                runnable.run()\n            }\n        } catch (e: OutOfMemoryError) {\n            firebaseCrashlytics.recordException(e)\n            runWithinMainLooper(runnable)\n        }\n    }\n    // 대체 방법\n    fun runWithinMainLooper(mainThread: Runnable) {\n        val handler = Handler(Looper.getMainLooper())\n        handler.post {\n            mainThread.run()\n        }\n    }\n}\n```\n\n# 3.2 베이스라인 프로필 (-7% 시작 시간)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGoogle은 첫 번째 앱 시작 시간을 개선하기 위해 기본 프로필을 설정하는 것을 권장합니다. 전체 앱 시작 시간이 7% 향상되었음을 확인했습니다. 실제 결과는 다를 수 있지만 꼭 시도해 보세요.\n\n그래서 우리는 좋은 시작을 했지만 더 심층적으로 파헤쳐야 했습니다.\n\n### 4. Perfetto 사용하기\n\n안드로이드 스튜디오에서 시스템 추적을 실행하여 각 함수의 수행 시간을 측정할 수 있으며, 그런 다음 앱을 실행하고 추적을 Perfetto 비주얼라이저에 로드할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAndroid Studio에 내장된 프로파일러를 사용할 수 있지만, Perfetto가 더 나은 탐색 및 세부 정보를 제공합니다.\n\n시스템 추적 방법은 여기에 있지만 앱 실행 시간을 프로파일링하려면 앱을 보통 실행하지 마세요. 다음과 같이 해야 합니다:\n\n- 정확한 결과를 얻기 위해 앱의 릴리스 빌드 변형을 선택하세요. 디버그 빌드 대신\n- Android Studio의 실행 버튼 근처에 있는 3점 메뉴를 클릭하세요\n- 그런 다음 \"오버헤드 낮은 상태로 앱 프로파일링\"을 선택하여 앱을 실행하세요\n- 이제 앱이 완전히 로드될 때까지 대기한 후 녹화를 중지하세요\n- 마지막으로 Profiler의 저장 아이콘을 사용하여 추적을 내보낸 다음 Perfetto Web UI로 가져오세요\n\n<img src=\"/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_2.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n퍼페토에 추적 파일을 로드하면 화면에 수많은 색상이 나타나더라도 놀라지 마세요. 그 모든 것을 다룰 필요는 없습니다.\n\n# 4.1 퍼페토에서 시작 시간 지표 찾기\n\n![이미지](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_3.png)\n\n“startup”을 검색하면 앱 시작 시간의 시각화를 볼 수 있습니다. 초록 막대를 클릭한 후 키보드에서 ‘M’을 누르세요. 이렇게 하면 그 막대의 시작과 끝을 표시할 수 있습니다. 지금은 이에만 집중하고 나머지는 그냥 잡음입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 4.2 원인을 찾아보세요\n\n지금 시작 시간 막대 아래에 패키지 이름을 찾아 확장하세요. 그래프의 표시된 영역에만 집중하고 메인 스레드를 보세요. x축에 각 함수의 지속 시간이 나올 것이며, y축에 중첩된 막대가 있는 경우 중첩된 함수를 의미합니다.\n\n먼저 수평으로 긴 막대부터 살펴보세요. 그 막대가 가장 많이 소요된 시간을 나타낼 것입니다. 예상보다 시간이 더 많이 소요된 함수가 어떤 것인지 확인하고 기록해보세요. 그런 다음 상위 4~5개의 주범을 최적화할 수 있을 것입니다.\n\n![그래프](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n프레임은 16밀리초 내에 렌더링되어야 60fps를 달성할 수 있어요\n\n팁: WASD 키를 사용하여 이동하세요. perfetto UI에 대해 더 읽고 싶다면 여기를 확인해보세요. 문서는 약간 오래되었지만 핵심 원칙은 같아요.\n\n# 5. 토끼굴에 들어가다\n\n긴 지속 시간을 가진 함수들이 있을 수 있지만 결과물이 없을 수도 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저기요, 이 BaseActivity의 onResume() 메서드가 엄청 길어요. 하지만 이 그래프만으로는 무엇이 시간을 차지하는지 알 수 없어요.\n\n마지막 부분에야 전체 시간의 1/4를 차지하는 중첩 함수가 보이네요.\n\n그럼 이제 어떻게 해볼까요?\n\n![이미지](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n트레이싱을 더 추가해야 돼요. 자바 및 코틀린에서 사용 가능한 트레이싱 라이브러리로 쉽게 할 수 있어요. 자세한 내용은 여기를 참조할 수 있지만 코드 일부를 살펴봐요:\n\n```js\nclass MyClass {\n  fun foo(pika: String) {\n    trace(\"MyClass.foo\") {\n    // 기존의 함수 로직이 여기에 있어요...\n    }\n  }\n}\n```\n\n이제 MyClass.foo가 Perfetto에 나타날 거예요. 따라서 디버그 중인 함수 내 모든 중첩 함수에 대한 트레이싱을 추가한 후 트레이스를 다시 기록하고 Perfetto에서 분석해보세요. 각 라이프사이클 함수에 대해 이 과정을 계속 반복하세요.\n\n![이미지](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 6. 우리의 솔루션\n\n이 연습을 통해 매우 명확한 근본 원인과 목표를 얻었습니다. 이제 각 원인을 하나씩 해결할 때가 왔습니다. Perfetto와 SysTrace의 관측을 바탕으로 다음과 같은 다양한 최적화가 실행되었습니다:\n\n## 6.1. 홈 화면 레이아웃 최적화 (-15% 시작 시간)\n\nConstraintLayout으로 뷰를 중첩하지 않고 hidden views 대신 viewstubs를 사용하여 최대 600ms까지 감소하였습니다. 이 접근 방식은 기본적으로 숨겨져 있고 사용자 상호작용 후에만 표시되는 뷰의 불필요한 측정 및 확장을 방지합니다. view stubs에 대해 더 알아보기.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n200~300 밀리초가 걸리는 mapView가 onResume()이 호출된 후에 init으로 이동되었기 때문에 핵심 UI에서 주문 수와 상태를 표시한 후에 로드됩니다.\n\n```js\nprivate fun HomeFragment.lazyLoadMap() {\n  Handler(Looper.getMainLooper()).post {\n    // 이 Runnable은 메인 스레드가 HomeFrag를 확장한 후에만 실행됩니다.\n    initMap()\n  }\n}\n```\n\n# 6.2 MainActivity 최적화하기 (-5% 시작 시간)\n\n반복된 성능 테스트에서 LinearLayout이 Fragment에 대한 컨테이너로만 사용되는 MainActivity에서 ConstraintLayout보다 성능이 더 우수했습니다. 실제로 LinearLayout으로 변경하면 MainActivity가 특히 웜 스타트에서 2배 더 빨리 시작되었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 6.3 MainActivity SDK들의 지연 로딩(-10% 시간)\n\n퍼페토는 MainActivity에서 초기화되는 단일 3rd party SDK가 onCreate() 지속 시간의 70%를 차지하고 있다는 것을 보여주었습니다. 우리는 앱 시작 시 즉시 필요하지 않았기 때문에 백그라운드에서 이를 지연로딩하기 시작했습니다.\n\n## 7. 실제 성능 결과\n\n이러한 솔루션을 여러 릴리스를 통해 출시한 뒤, 시작 시간의 90분위가 3.5초에서 거의 2초 미만으로 점진적으로 개선되어, 놀라운 42%의 감소를 보았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_7.png)\n\n더 많은 병목 현상을 찾고 앱 속도를 높이는 데 노력하고 있습니다. 이를 통해 파트너들이 더 생산적일 수 있도록 돕고 있습니다.\n\n프레펫토를 사용하여 병목 현상을 발견하는 것은이 프로젝트에 있어 중요했으며, 각 문제를 해결함으로써 얼마나 많은 성능 향상을 이끌어낼 수 있는지를 알기에 자신감을 갖게 되었습니다.\n\n앱 시작 시간을 개선하는 데 도움을 준 Burhan & Vishnu에게 감사드립니다.\n","ogImage":{"url":"/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_0.png"},"coverImage":"/assets/img/2024-06-23-MakingShadowfaxAndroidApp40faster_0.png","tag":["Tech"],"readingTime":10},{"title":"Jetpack Compose에서 MVI 패턴을 강력하게 구현하는 방법","description":"","date":"2024-06-23 01:12","slug":"2024-06-23-ArobustMVIimplementationwithJetpackCompose","content":"\n# 왜 이렇게 하고 있는 걸까요?\n\n저는 학습 곡선이 낮고 개발자들이 인식할 수 있는 견고한 아키텍처를 개발하는 데 매진해 왔습니다.\n\n목표는 프로젝트와 무관한 것을 갖는 것이었습니다. 간단히 말해서, 우리는 모든 개발자가 아키텍처에 익숙해져 다른 프로젝트에 보다 쉽게 기여할 수 있도록 하고 싶었습니다. 필요할 때 다른 프로젝트에 쉽게 기여할 수 있어야 한다는 것을 명심하며, 이를 통해 성취도를 더욱 높일 수 있도록 하려 했습니다.\n\n아키텍처를 구축하는 데에는 시간이 걸립니다. 저는 대부분의 시간을 안드로이드 생태계 및 그의 모범 사례에 대해 배우고, 그리고 이전 권장 사항에서 새로운 것으로의 전환하는 것과 같은 작업에 투자했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- DataBinding 대신 ViewBinding 권장\n- XML에서 Jetpack Compose로 전환\n- MVI 모습 기능\n- KMP의 안정적인 릴리스\n\n# 어떻게 해볼까요?\n\n코드 한 줄을 쓰기 전에, 너무 많은 방향으로 헤딩하지 않도록 몇 가지 작업을 해야 합니다:\n\n- 좋은 컵 자신이 좋아하는 음료를 듭니다.\n- MVI가 실제로 무엇인지 조사합니다.\n- 목표 동작을 정의합니다.\n- 실제 구현을 코딩합니다.\n- 실제 예제에서 결과를 테스트합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 연구 단계\n\nMVI가 무엇을 의미하는지 알고 있는 것으로 가정하지만(아니면 지금까지 그것을 피해왔다면), 간단히 모든 사람들을 최신 상태로 알려드리겠습니다.\n\nMVI는 Model-View-Intent의 약어입니다. 이 아키텍처는 MVVM 및 기타 모델들과 함께 MV\\* 패밀리의 일부입니다. 이 아키텍처의 핵심 원칙은 입력 Intents를 받아들이는 상태 머신이며, 기저 UI를 나타내는 View State를 생성합니다. 이 모든 것은 MVI가 예전의 형제 MVVM과 달리 SSoT(Single Source of Truth) 원칙을 준수한다는 것을 의미합니다.\n\n![이미지](/assets/img/2024-06-23-ArobustMVIimplementationwithJetpackCompose_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 대상 동작\n\n먼저, 구현의 기초 역할을 할 화면이 필요합니다. 이번에 사용할 앱은 Now In Android 애플리케이션입니다. (여기에서 찾을 수 있습니다.) 다음과 같이 생겼습니다:\n\n간편함을 위해, 첫 번째 스크린샷인 ForYouScreen에만 집중하겠습니다. 이 화면에는 표시하고 상호작용할 수 있는 다양한 데이터가 있습니다.\n\n이제 MVI의 핵심 개념인 Reducer를 설명해야 합니다! Reducer는 어떤 의미에서 UI와 ViewModel 간의 계약입니다. 이는 두 부분으로 나누어집니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 객체 인터페이스 : State, Event 및 Effect\n- reduce 함수 (수학적 reduce 연산자와 혼동하지 말 것)\n\n## ViewState\n\n이것은 UI의 표현입니다. 이론적으로, Compose 화면이 표시하는 데 필요한 모든 것을 포함해야 합니다. 이것은 화면의 다른 상태를 나타내는 여러 미리 보기를 쉽게 만들 수 있는 장점이 있습니다.\n\n## ViewEvent\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMVI의 핵심입니다. 모든 사용자 상호작용(그리고 조금 더)을 보유합니다. 이 것들은 ViewModel에 의해 상태 변경을 일으키는 데 사용됩니다.\n\n## ViewEffect\n\nViewEvent의 특별한 종류입니다. ViewModel에 의해 UI로 전달됩니다. 네비게이션이나 스낵바/토스트 표시와 같은 작업을 포함합니다.\n\n## reduce 함수\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nreduce 함수는 ViewState와 ViewEvent를 가져와서 새로운 ViewState와 주어진 이벤트에 연결된 필요에 따라 ViewEffect를 생성합니다.\n\n이제 Reducer가 준비되었으니, 모든 ViewModel에 의해 구현될 BaseViewModel을 정의해야 합니다:\n\n## 구현\n\n이제 기본 구조가 설정되었으니, 이제 화면을 위해 실제로 이를 구현할 시간입니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Reducer\n\n먼저, ViewState를 정의하여 몇 가지 이벤트를 쉽게 식별할 수 있습니다.\n\n이제 ViewState가 준비되었으므로 사용자 상호작용을 처리하고 상태를 업데이트하는 ViewEvent를 정의할 수 있습니다.\n\n위 내용은 ViewState와 함께 명확할 것으로 생각됩니다. 그러나 마지막 3개의 이벤트에 대해서는 설명이 필요할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-ArobustMVIimplementationwithJetpackCompose_1.png)\n\n화면의 클릭 가능한 요소에 다음과 같이 연결되어 있습니다:\n\n- UpdateTopicIsFollowed은 초록 테두리의 요소와 관련이 있습니다.\n- UpdateNewsIsSaved은 주황색 테두리의 요소와 관련이 있습니다.\n- UpdateNewsIsViewed는 보라색 테두리의 요소와 관련이 있습니다.\n\n이제 MVI 아키텍처의 마지막 부분, ViewEffect를 정의해야 합니다! 다행히도, 이 화면에서는 가능한 효과가 두 가지뿐이므로 비교적 간단합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로 구현할 reduce 함수가 있습니다. 대부분의 경우, reduce 함수는 매우 간단하여 입력 ViewEvent 데이터를 수정된 ViewState로 매핑합니다.\n\n여기서 중요한 점(특히 마지막 3개의 ViewEvent 경우)은 실제 프로덕션 애플리케이션에서 Reducer에서 직접 표시되는 데이터를 거의 건드리지 않는다는 것입니다.\n\n## ViewModel\n\n이제 우리가 Reducer를 가졌으니, 다음과 같은 ViewModel에서 사용해야 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희 ViewModel을 보시면, 매우 간단한 코드만 포함되어 있습니다. 이는 우리가 사용할 주요 기능이 포함된 BaseViewModel을 사용하기 때문입니다.\n\n## 화면\n\n마지막으로, 이 모든 것을 화면에 연결하여 UI에서 발생할 수 있는 다양한 경우를 처리하는 것이 얼마나 간단한지 확인할 수 있습니다.\n\n# 그게 다입니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금까지 우리가 한 모든 작업으로, SSoT 원칙을 준수하고 UI를 직접 나타내는 ViewState를 활용하는 완전히 정의된 MVI 아키텍처가 생겼습니다!\n\n아마도 여러분 중에는 이미 시도하고 테스트되었으며 작동이 증명된 기존 아키텍처가 있음에도 불구하고 왜 이 모든 R&D를 한 것인지 궁금해할 수도 있습니다. 간단히 말해서, 필요했기 때문입니다. 저는 여러 프로젝트에서 다양한 프로젝트 매니저 및 다른 개발자들과 협업하고 있습니다.\n\n이러한 프로젝트에서 리드 개발자로 있는 것은 종종 프로젝트에 시간을 들여 PRs(또는 GitLab 사용자를 위한 MRs)를 리뷰하는 일을 요구받기 때문입니다. 종종 작은 아키텍처적 오류에 대한 주석을 지적해야 하는데, 해당 아키텍처에 대한 보다 깊은 지식으로 방지할 수 있는 경우가 많습니다.\n\n그래서 제가 일하는 모든 프로젝트와 호환되며 동료들이 알고 인정할 수 있는 아키텍처를 상상하면, 모든 관련 당사자들에게 전면 리뷰 프로세스를 더 빠르고 쉽게 만들어주며 전체적인 개발 속도를 높일 수 있습니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 링크에서 전체 프로젝트를 찾아볼 수 있어요:\n\nMVI에 관한 다양한 훌륭한 기사가 많이 있어요. 관심이 있다면 몇 가지를 소개해 드릴게요:\n","ogImage":{"url":"/assets/img/2024-06-23-ArobustMVIimplementationwithJetpackCompose_0.png"},"coverImage":"/assets/img/2024-06-23-ArobustMVIimplementationwithJetpackCompose_0.png","tag":["Tech"],"readingTime":7},{"title":"비용 절감 클라우드 솔루션 비즈니스를 위한 최고의 선택","description":"","date":"2024-06-23 01:10","slug":"2024-06-23-EscapetheDollarTrapAffordableCloudSolutionsBuiltforBusinesses","content":"\n![Escape the Dollar Trap: Affordable Cloud Solutions Built for Businesses](/assets/img/2024-06-23-EscapetheDollarTrapAffordableCloudSolutionsBuiltforBusinesses_0.png)\n\n클라우드 비용이 치솟고 수익은 줄어들고 있다면 지쳤을 것입니다. 높은 클라우드 비용을 부담하는 것은 기업에게 큰 문제입니다.\n\nHostSpace가 해결책입니다. HostSpace는 아프리카 기업의 독특한 요구와 예산 제약을 충족시키기 위해 특별히 설계된 관리형 클라우드 솔루션 스위트를 개발했습니다. 현지 인프라와 전문성을 활용하여 클라우드의 힘을 누리면서도 시중을 깨지 않게 합니다.\n\n클라우드 컴퓨팅의 비용 상승은 아프리카 전역의 기업에게 주요 고통 요인입니다. 지역 통화로 수익을 올리는 동안 외화로 지불하는 것은 재정 부담만 증대시킵니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n안녕하세요! 여러분의 문제와 예산 제한을 고려한 관리 클라우드 솔루션 스위트를 개발했습니다.\n\n우리의 접근 방식은 다음과 같습니다:\n\n우리는 신뢰할 수 있는 현지 클라우드 인프라 공급업체와 파트너십을 맺어 Google Cloud나 AWS와 같은 글로벌 플레이어들이 제공하는 동일한 도구와 서비스를 저렴한 비용에 제공합니다.\n\n주요 솔루션:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nHostSpace 쿠버네티스 엔진 (HKE)\n저희의 완전히 자동화된 관리형 쿠버네티스 환경으로 인프라를 간단하게 만들어보세요. 어렵지 않고 다루기 쉽게 확장하고, 쿠버네티스 전문가가 아니어도 신속하게 배포하세요.\n\n저희의 HostSpace 쿠버네티스 엔진 (HKE)은 사용하기 더 쉽기만 한 것이 아니라, 더 저렴하기도 합니다. 예를 들어, Nobus Cloud에서 구동되는 HKE의 표준 4 vCPU, 8GB RAM 설정은 주요 클라우드 제공업체와 비교했을 때 월 비용이 최대 70% 더 저렴할 수 있습니다. 이는 높은 성능과 확장성을 가진 쿠버네티스를 무겁지 않은 가격에 이용할 수 있다는 뜻입니다.\n\nHostSpace 컨테이너 솔루션 (HCS)\n\n소수의 클릭으로 컨테이너에 커스텀 애플리케이션 또는 WordPress와 같은 인기 있는 오픈 소스 소프트웨어를 배포하세요. 아이디어를 온라인으로 신속하게 올리는 가장 빠르고 쉬운 방법입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기술적인 문제를 걱정하지 마세요. 저희의 관리형 컨테이너 배포 솔루션은 귀하의 애플리케이션을 손쉽게 실행할 수 있도록 해줍니다. 커스텀 제작되었든 인기 있는 오픈 소스 도구인 WordPress든, 어렵고 복잡한 설정이나 서버 설정과 씨름할 필요가 없습니다. 몇 번의 클릭만으로 아이디어를 실현하고 대중에 도달할 준비가 됩니다.\n\n이는 기업의 빠른 시장 진입을 의미하며, 개발자들은 최고의 능력에 집중하여 혁신적인 솔루션을 만들 수 있는 더 많은 자유를 얻게 됩니다.\n\n아프리카 기업을 위한 게임 체인저인 HostSpace의 이유:\n\n- 막대한 비용 절감: 저희 고객들은 공개 클라우드 제공업체를 사용하는 것과 비교하여 최대 90%의 클라우드 비용을 절약했다고 보고했습니다.\n- 지역 전문지식: 아프리카 기업이 직면한 독특한 도전에 대한 이해와 맞춤형 지원 및 솔루션 제공.\n- 빠른 배포: 몇 분이면 어플리케이션을 가동시킬 수 있습니다.\n- 확장성: 비증식 사업의 성장에 따라 사업 인프라를 쉽게 확장.\n- 신뢰성: 강력한 인프라와 전문가 지원에 의존하여 어플리케이션이 원활하게 실행되도록 확신합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 모든 사람에게 클라우드 컴퓨팅을 접근할 수 있도록 만들기를 믿습니다. 그래서 호스트스페이스의 차이를 직접 경험할 수 있도록 무료 평가판을 제공합니다. 이후에는 저렴한 비용부터 시작하는 요금제가 있습니다...\n\n호스트스페이스를 선택하여 아프리카 기업의 수가 증가하고 있습니다. 당신의 모든 잠재력을 발휘하는 데 도움이 되기를 바랍니다.\n\n클라우드 전략을 혁신할 수 있는 방법을 발견하기 위해 무료 상담 및 데모를 받으려면 오늘 admin@hostspaceng.com으로 연락하십시오. 솔루션에 대해 더 알아보려면 우리의 브로셔를 다운로드해보세요.\n","ogImage":{"url":"/assets/img/2024-06-23-EscapetheDollarTrapAffordableCloudSolutionsBuiltforBusinesses_0.png"},"coverImage":"/assets/img/2024-06-23-EscapetheDollarTrapAffordableCloudSolutionsBuiltforBusinesses_0.png","tag":["Tech"],"readingTime":4},{"title":"Argo Rollouts가 이제 Kubernetes Gateway API 10 버전을 지원합니다","description":"","date":"2024-06-23 01:09","slug":"2024-06-23-ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI","content":"\n안녕하세요! Argo Rollouts, 쿠버네티스를 위한 프로그레시브 딜리버리 컨트롤러,이(가) 새로운 쿠버네티스 게이트웨이 API(다음 세대 인그레스/서비스 메시 표준)를 지원하는 새로운 플러그인을 통해 이제 새로운 기능을 제공합니다.\n\n이제 거의 모든 인그레스, 게이트웨이 또는 서비스 메시를 사용하여 쿠버네티스에서 카나리 배포를 수행할 수 있습니다. 지원되는 공급업체의 전체 목록을 확인하고 자세한 내용을 알아보세요.\n\nArgo Rollouts은 프로그레시브 딜리버리 시나리오(블루/그린 및 카나리 배포)에 중점을 둔 Argo 패밀리 프로젝트 중 하나입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n카나리 배포에서 Argo Rollouts는 애플리케이션의 새 버전으로 트래픽을 점진적으로 이동시킬 수 있는 트래픽 공급자를 선택적으로 사용할 수 있습니다.\n\n![Argo Rollouts](/assets/img/2024-06-23-ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI_1.png)\n\n쿠버네티스를 위한 여러 네트워킹 제품이 있으며, 대부분은 인그레스(클러스터로 들어오는 트래픽) 또는 서비스 메시(클러스터 내에서의 트래픽)의 형태로 제공됩니다.\n\nArgo Rollouts는 이미 여러 인그레스 및 서비스 메시를 내장 지원하고 있습니다. 이러한 도구들의 지원은 Argo Rollouts의 인트리 소스 코드의 일부이며, 새 도구를 도입하는 과정이 불편합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Argo Rollouts 프로젝트 전체의 소스 코드를 확인해보세요.\n- 전체 컨트롤러가 어떻게 작동하는지 이해하세요.\n- 본 Argo Rollouts 릴리즈에 병합될 풀 리퀘스트를 제출하세요.\n- 프로젝트 유지자가 승인/병합하도록 기다리세요.\n- Argo Rollouts의 새로운 릴리즈가 나올 때마다 툴 특화 코드를 유지/추적해야합니다.\n\n사람들이 Argo Rollouts와 함께 사용하고 싶어하는 여러 네트워킹 도구가 있습니다. 이 복잡한 과정으로 컨트롤러가 네이티브 방식으로 이러한 도구를 지원하지 못했습니다.\n\n그러나 최근 두 가지 발전으로 이 과정이 더 이상 필요하지 않아졌습니다.\n\n첫 번째는 Argo Rollouts가 1.5 버전부터 트래픽 플러그인을 지원한다는 점입니다. 플러그인이 어떻게 작동하는지 릴리스 공지에서 설명했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 번째로 소개할 것은 새로운 Kubernetes Gateway API입니다.\n\n# Gateway API — 쿠버네티스의 서비스 매쉬와 인그레스를 위한 통합 표준\n\nGateway API는 쿠버네티스 SIG 프로젝트로, 쿠버네티스의 모든 네트워킹 솔루션을 통합하여 기본 인그레스 API에 누락된 몇 가지 기능을 추가하면서 동시에 모든 서비스 매쉬 솔루션을 통합합니다.\n\nGateway API는 쿠버네티스 인그레스 API의 다음 세대로 생각할 수 있으며, 동시에 (이제는 폐지된) SMI 표준의 다음 세대로도 볼 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n쿠버네티스 게이트웨이 API는 인프라를 정의하는 사람과 인프라를 사용하는 사람 사이에 명확히 구분을 정의했다는 점에서 주목할 만합니다.\n\n![이미지](/assets/img/2024-06-23-ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI_2.png)\n\n동시에, 모든 주요 네트워킹 공급업체들의 일치된 지원을 받고 있습니다. 대부분의 네트워킹 솔루션은 게이트웨이 API에 대한 초기 지원을 추가했거나 해당 API를 따를 것임을 선언했습니다.\n\nAPI 지원자의 상세 목록은 구현 목록에서 확인하실 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Argo Rollouts 및 게이트웨이 API - 젤리와 채피 버터처럼\n\nArgo Rollouts에는 이제 Gateway API를 지원하는 플러그인이 있습니다. 저희는 쿠버네티스 게이트웨이 API의 0.x 버전을 대상으로 한 초기 개발을 시작했으며 테스트된 구현체 목록을 계속 확장하고 있습니다.\n\n이것은 Argo Rollouts를 사용하여 쿠버네티스 게이트웨이 API의 현재 및 향후 구현체와 함께 캐너리 배포에 사용할 수 있음을 의미합니다. 얼마나 멋진가요?\n\n![ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI_3](/assets/img/2024-06-23-ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n쿠버네티스 게이트웨이 API는 2023년에 1.0 버전을 출시했으며, 최신 버전의 플러그인(0.3)은 이제 1.0 사양에 맞춰 컴파일되었습니다.\n\n귀하의 네트워킹 제공 업체로 플러그인을 테스트하시고 궁금한 점이나 문제가 있으면 언제든지 알려주시기 바랍니다.\n\n캐너리 배포를 즐기세요!\n\nUnsplash의 Shivansh Singh 사진을 참고해주세요.\n","ogImage":{"url":"/assets/img/2024-06-23-ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI_0.png"},"coverImage":"/assets/img/2024-06-23-ArgoRolloutsNowSupportsVersion10oftheKubernetesGatewayAPI_0.png","tag":["Tech"],"readingTime":5},{"title":"Kubernetes에서 2년간 Airflow를 운영하며 배운 교훈들","description":"","date":"2024-06-23 01:07","slug":"2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years","content":"\n![이미지](/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_0.png)\n\nApache Airflow은 우리 데이터 플랫폼에서 가장 중요한 구성 요소 중 하나로, 비즈니스 내 다양한 팀들이 사용하고 있습니다. 이는 우리의 모든 데이터 변환, 사기 탐지 메커니즘, 데이터 과학 이니셔티브 및 Teya에서 실행하는 다양한 일상 및 내부 작업을 지원합니다.\n\n전체적인 그림을 보면, 저희는 300개 이상의 운영 DAG와 평균 하루에 5,000개 이상의 작업을 수행하는 중형 규모의 Airflow 배치 시스템을 운영 중입니다. 그래서 저는 우리가 사용자에게 가치를 전달할 수 있는 꽤 규모가 있는 Airflow 배포 시스템을 보유하고 있다고 말할 수 있겠습니다. 8개월 이상 동안 Airflow에서 한 번의 인시던트나 실패 없이 운영되어 왔습니다.\n\n본 게시물을 통해, 확장 가능하고 신뢰할 수 있는 환경을 구축하는 데에 도움이 된 저희 배포의 중요한 측면을 공유하고자 합니다. 혹시 지금 Airflow를 프로덕션 환경에서 시작하는 중이거나 다른 아이디어를 평가하고 사용 사례에 적용하고자 하는 경우 도움이 되기를 바라겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희는 현재 Airflow 구현의 주요 측면에 따라 나눠볼 거예요:\n\n- Executor 선택\n- 디커플링 및 동적 DAG 생성\n- 세밀한 구성 조정\n- 알림, 경보 및 감시\n\n## Executor 선택\n\n여기서는 Kubernetes에서 모든 것을 실행합니다. 그래서 Airflow의 경우도 다를 바 없습니다. 처음에는 Executor 선택이 명백해 보였습니다: 쿠버네티스 Executor를 사용합시다! 런타임 격리, 쿠버네티스를 활용하여 원활한 작업 확장성, 그리고 관리해야 할 구성 요소가 적다는 장점들이 모두 아주 좋아 보였죠. 그렇게 우리의 여정을 시작했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 저희 스택에서 중요한 특징 하나가 있습니다: 대부분의 작업은 가벼운 DBT 증분 변환 작업이며, 아주 소수의 작업만 장기 실행되는 모델(+/- 1시간)입니다.\n\n처음에 직면한 문제는 작업을 시작하는 데 드는 오버헤드였습니다. KubernetesExecutor는 각 작업을 별도의 Pod에서 실행하므로 때로는 Pod를 시작하는 대기 시간이 작업 자체의 실행 시간보다 더 길어지곤 했습니다. 작은 작업이 많아서 Kubernetes 노드의 확장을 기다리느라 계속해서 기다려야 했습니다.\n\n두 번째 문제는 더 많은 고통을 야기한 문제이었는데, 일부 작업(특히 장기 실행되는 작업)이 Pod가 유배되어 예기치 않게 실패하는 일이 있었습니다. 작업이 급격히 증가하면 Pod와 클러스터 노드의 수도 증가하므로 작업이 완료되자마자 시스템이 다시 축소될 준비가되었습니다.\n\n<img src=\"/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_1.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n문제는 Karpenter를 사용하여 k8s 클러스터의 리소스 사용량을 최적화하고 있어서 더욱 악화되었습니다. 따라서 몇 개의 Pods가 완료된 후에 노드의 스케일인이 매우 빠르게 발생했습니다. 이 동작은 해당 노드에 남아 있는 나머지 Pods를 추방하여 다른 노드로 재배포하여 총 노드 수를 줄이고 비용을 절감하는 것입니다.\n\n## CeleryExecutor가 해결책으로 등장\n\n이 모든 상황을 고려하여, 우리는 오래된 신뢰할 수 있는 Celery Executor로 전환하기로 결정했습니다. 이제 고정된 워커 노드를 갖게 되어, 우리의 많고 빠른 작업을 완벽히 처리할 수 있습니다. DBT 작업의 평균 실행 시간이 크게 감소했습니다. 이제는 초기화를 기다릴 필요가 없게 되었기 때문입니다.\n\nAirflow의 공식 최신 helm 차트를 사용함으로써, KEDA 오토스케일러의 도움을 받아 필요에 따라 셀러리 워커의 수를 증가 또는 감소시킬 수 있습니다. 따라서 아이들 상태의 워커에 대해 추가 비용을 지불할 필요가 없게 됩니다. 이는 Airflow의 데이터베이스에서 실행 중이거나 대기 중인 작업의 수를 가져와서 작업자 병행성 구성에 따라 작업자 수를 조정함으로써 작동합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자원을 더 많이 필요로 하는 맞춤형 작업의 경우, KubernetesPodOperator를 사용하여 실행할 수 있는 옵션이 있습니다. 이를 통해 Airflow의 이미지에 설치하지 않고도 특정 종속성에 대한 런타임 분리를 유지하고 각 작업에 개별 자원 요청을 정의할 수 있습니다.\n\n현재는 KubernetesCeleryExecutor의 채택을 고려 중입니다. 이는 작업을 두 개의 별도 대기열인 k8s와 Celery로 예약할 수 있게 해줍니다. 몇몇 작업이 Celery에 더 잘 맞고 다른 일부는 Kubernetes에 더 적합한 경우에 유용할 수 있습니다.\n\n# Decoupling 및 동적 DAG 생성\n\n데이터 엔지니어링 팀뿐만 아니라 각 팀이 자체 DAG를 작성하는 시나리오를 수용하기 위해 DAG에 대한 멀티 레포 접근 방식이 필요했습니다. 그러나 동시에 일관성을 유지하고 지침을 강제할 필요가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## DAGs를 위한 다중 저장소 접근 지원\n\nDAGs는 각각 다른 팀이 소유하는 개별 저장소에서 개발될 수 있으며 여전히 동일한 Airflow 인스턴스에 배치될 수 있습니다. 물론 DAGs를 Airflow 이미지에 포함할 필요 없이 😉\n\n한 사람이 DAG의 한 줄을 수정할 때마다 스케줄러와 워커를 계속 다시 시작하려고 하지 않는 게 좋을 거에요.\n\n<img src=\"/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_2.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n각 DAG는 팀이 소유한 DAG에 대한 경로에 따라 동기화 프로세스에 의해 어딘가의 버킷에 끝나게 됩니다.\n\n이 접근 방식이 작동하려면 CI/CD 가드레일을 강제로 실행해야 합니다. 각 DAG 이름은 해당 DAG를 소유한 팀으로 미리 지정되어야 하므로 DAG ID 충돌을 피할 수 있습니다. 또한 각 DAG에 대해 정적 검사가 수행되어 정확한 소유자 할당과 태그 존재 여부, 가능한 가져오기 오류 등을 확인합니다.\n\n이를 통해 네이티브 Airflow 역할을 사용하여 액세스 제어를 강화하고 각 DAG는 커밋되기 위해 최소 거버넌스 체크리스트를 통과해야 합니다.\n\n## 그런데, Airflow로 DAG를 어떻게 동기화할까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAirflow에 DAG가 반영되려면, 스케줄러, 워커 등을 실행하는 Pod의 로컬 파일 시스템과 버킷 내용을 동기화해야 합니다. 이를 위해 우리는 Objinsync를 사용하고 있습니다. Objinsync는 가벼운 데몬으로 원격 객체 스토어를 로컬 파일 시스템으로 점진적으로 동기화합니다.\n\n우리는 모든 Airflow 구성 요소 Pod에 부착된 부가 컨테이너로 objinsync를 실행하여 자주 동기화 작업을 수행합니다. 그래서 DAG에 대한 새로운 업데이트를 항상 몇 분 내에 확인할 수 있습니다. 여기서 배운 교훈은 objinsync를 이니셜라이저 컨테이너로도 추가하는 것입니다. 이렇게 하면 메인 스케줄러나 워커 컨테이너가 시작되기 전에도 DAG를 동기화할 수 있습니다. 이는 특히 Celery 워커들에 대해 매우 중요했습니다. 노드 회전이나 릴리스로 인해 다시 시작된 후, 때로는 DAG를 아직 가져오지 않은 새로운 워커에 할당된 작업이 발생하여 즉시 실패하는 경우가 있습니다.\n\n<img src=\"/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_3.png\" />\n\n이를 더 효과적으로 처리하는 이상적인 방법은 스케줄러에 부착된 부가 컨테이너로써 objinsync 프로세스를 하나만 실행하고 버킷 내용을 영구 볼륨에 복사하는 것입니다. 그래서 해당 PV가 모든 Airflow 구성 요소에 장착됩니다. 여기에는 DAG가 서로 다른 Airflow 구성 요소 사이에 동기화 문제가 발생하지 않는 장점이 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n죄송하지만, 현재는 클러스터 노드에 대해서는 EBS 볼륨만 지원하고 있어서 이러한 해결책을 아직 적용할 수 없습니다. 서로 다른 노드에 PV를 마운트하려면 ReadWriteMany 액세스 모드가 필요합니다. 현재 이 모드는 AWS EKS에서만 EFS 볼륨 모드를 사용할 때에만 사용할 수 있습니다.\n\n우리의 제한 사항에 대한 해결책으로는 Airflow Pods들을 동일한 노드에 스케줄링하기 위해 nodeSelector를 사용하는 것이 될 수 있습니다. 그러나 우리는 서로 다른 가용 영역의 노드를 사용하여 고가용성 Airflow 배포를 선호했습니다.\n\n## DAG를 동적으로 생성할 때 주의해야 할 점\n\n규모에 맞게 DAG를 생성하려면 DAG 템플릿 및 프로그래밍 방식의 생성을 활용해야 합니다. 더 이상 DAG를 수동으로 모두 작성할 필요가 없습니다 😂\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가장 간단한 방법으로 DAG를 동적으로 생성하는 방법은 단일 파일 방식을 사용하는 것입니다. 루프에서 DAG 객체를 생성하고 globals() 사전에 추가하는 하나의 파일이 있습니다.\n\n이 방법은 처음에는 우리의 DBT 프로젝트를 기반으로 동적 DAG를 생성하기 시작했을 때 매우 직관적이었습니다 (DBT 오케스트레이션에 대한 주제는 별도의 게시물이 필요하며 나중에 이루어질 예정입니다). 그러나 DAG가 정기적으로 스케줄러에 의해 구문 분석되는 경우, 이 방법을 사용할 때 CPU 및 메모리 사용량이 증가하고 스케줄러 루프 시간이 더 오래 걸리는 것을 관측했습니다. 특히 DBT manifest.json을 구문 분석해야 했기 때문에 이 방법은 우리 프로젝트 규모에서 확장 가능하지 않았음이 빨리 드러났습니다.\n\n해결책은 다중 파일 방식을 채택하는 것이었습니다. 이 방식에서는 동적으로 생성하려는 DAG마다 .py 파일을 생성합니다. 이를 통해 DAG 생성 프로세스를 우리의 DBT 프로젝트 저장소로 통합했습니다. 프로젝트는 이제 더 많은 DAG를 생성하는 프로듀서로 변화하여 동적으로 생성된 파일을 DAG 버킷으로 푸시합니다.\n\nAstronomer에는 단일 파일 방법과 다중 파일 방법에 대한 훌륭한 기사가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 세부 구성 조정\n\nCeleryExecutor로 이동한 순간, 우리가 한 문제를 해결했지만 새로운 문제가 발생하기 시작했습니다. 몇 일 후(또는 몇 시간 후)에 에어플로우를 실행하는 동안 일부 Celery 워커가 OOM(메모리 부족) 문제로 죽기 시작했습니다. Pod에 충분한 메모리 자원을 제공했기 때문에 무언가 이상한 것이 있었습니다.\n\n조사 결과, 우리가 본 Celery 워커 자원 사용 차트는 다음과 같습니다.\n\n![차트](/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희 업무는 주로 셀러리 워커에 의해 실행되는 DBT 작업으로 구성되어 있습니다. 이 때 거의 지속적으로 증가하는 메모리 사용량은 우리를 혼란스럽게 했었죠. 우리는 작업 간에 메모리 누수가 있는 것으로 의심하기 시작했습니다.\n\n메모리 누수를 방지하고 작업의 메모리 사용량을 제어하기 위해 두 가지 중요한 셀러리 구성을 세밀하게 조정해야 했습니다: worker_max_tasks_per_child와 worker_max_memory_per_child.\n\n첫 번째 구성은 워커 프로세스가 새 프로세스로 교체되기 전에 실행할 수있는 최대 작업 수를 제어합니다. 먼저 셀러리 워커와 워커 프로세스의 차이를 이해해야 합니다. 한 워커 노드는 여러 워커 프로세스를 생성할 수 있으며 이는 동시성 설정에 의해 제어됩니다. 예를 들어, 동시성을 12로 설정하고 셀러리 워커가 2개 있다면 총 24개의 워커 프로세스가 생깁니다.\n\n따라서 동일한 워커 프로세스 내 작업 간 메모리 누수를 방지하기 위해 때때로 해당 프로세스를 재시작하는 것이 좋습니다. 이 구성이 설정되지 않으면 기본적으로 해당 워커 프로세스는 재시작하지 않습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 번째 설정인 worker_max_memory_per_child은, 단일 워커 프로세스가 실행되기 전에 교체되는 최대 주거 중인 메모리 양을 제어합니다. 이는 기본적으로 메모리 사용량을 제어합니다. 기본 설정은 제한이 없으므로 항상 설정하는 것이 좋습니다.\n\n이 두 구성을 조정함으로써 우리는 메모리 사용량을 제어하고, 두 가지 경우에 워커 프로세스를 재활용하게 됩니다: 만약 최대 작업 수에 도달하거나 최대 주거 중인 메모리양에 도달했을 때입니다. 이러한 구성은 프리포크 풀을 사용하는 경우에만 작동한다는 점을 주의해야 합니다. 자세한 내용은 공식 문서에서 확인할 수 있습니다.\n\nAirflow에서 이를 설정하는 것은 매우 간단합니다. Airflow의 config_templates 폴더에 있는 기본 Celery 구성을 업데이트해야 합니다. 아래와 같이 해주세요:\n\n```js\n# config_templates/custom_celery.py\n\nfrom airflow.config_templates.default_celery import DEFAULT_CELERY_CONFIG\n\nCUSTOM_CELERY_CONFIG = DEFAULT_CELERY_CONFIG.copy()\nCUSTOM_CELERY_CONFIG.update(\n    {\n        \"worker_max_tasks_per_child\": <int>,\n        \"worker_max_memory_per_child\": <int>,\n    }\n)\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그리고 values.yaml에서 이 사용자 정의 구성물을 가리키도록 합니다.\n\n```js\nairflow:\n   config:\n     celery:\n       worker_concurrency: <int>\n       celery_config_options: config_templates.custom_celery.CUSTOM_CELERY_CONFIG\n```\n\n위 구성에 사용하는 구체적인 값은 워커 노드 구성, 메모리 요청/제한 양, 동시성 수준 및 작업이 메모리 집약적인지에 따라 다를 수 있습니다. 따라서 귀하의 특정 설정에 맞게 세밀하게 조정해야합니다.\n\n## 노드 회전을 대비하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nk8s 노드는 고장이 발생하거나 쿠버네티스 클러스터를 관리하는 인프라 팀에 의해 예정된 노드 회전으로 인해 회전할 수 있습니다. 또한, 워커 노드(Pods)는 릴리스 이벤트 발생 시, 구성 변경(예: 환경 변수)이나 베이스 이미지 변경 시 회전합니다. 노드 회전은 당연히 Pods가 종료되는 결과를 가져옵니다.\n\n이러한 사건에 대비하여 우리의 작업이 단순히 종료된 Pod로 인해 실패하지 않도록 준비되어 있어야 합니다. 특히 장시간 실행되는 작업의 경우 특히 아플 수 있습니다. 예를 들어, 2~3시간 동안 작업을 실행 중에 예정된 노드 회전으로 인해 실패한다면 상상할 수 있습니다.\n\n이를 방지하기 위해 개별적인 요구 사항에 따라 Worker Termination Grace Period 구성을 설정하는 것이 중요합니다. 이 구성은 쉘러리 워커가 릴리스 프로세스나 노드 회전에 의해 종료되기 전에 해당 시간(초)까지 기다리도록 만들 것입니다. 이것은 Airflow의 차트 값(values.yaml)에서 쉽게 설정할 수도 있습니다.\n\nairflow:\nworkers:\nterminationGracePeriodSeconds: <int>\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가장 오래 걸리는 작업 완료 시간의 1.5배로 설정하는 것이 좋습니다. 이렇게 하면 모든 작업이 그 기간 내에 완료될 것이므로, 작업자를 원할하게 종료할 수 있습니다.\n\n# 알림, 경보 및 관측\n\n## 회사 알림 통합\n\nAirflow의 가장 일반적인 사용 사례 중 하나는 특정 작업 이벤트, 예를 들어 파일 처리, 클린업 작업 또는 작업 실패 후에 사용자 지정 알림을 보내는 것입니다. Airflow를 사용하는 여러 팀이 있는 환경에서 작업 알림 메커니즘을 통합하는 것이 좋습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테이블 태그를 마크다운 형식으로 변경해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n커스텀 알림은 템플릿화되어 있어서 팀들은 표준 형식으로 Slack에서 정보 메시지를 만드는 데 사용할 수 있습니다. 또 다른 장점은 이 접근 방식을 사용하는 개별 팀이 개별 알림 대상의 비밀을 관리할 필요가 없다는 것입니다.\n\n## 실패에 대해 가장 먼저 알아보세요\n\n고가용성의 최상의 실천 방법과 패턴을 구현하더라도 Airflow는 다양한 이유로 실패할 수 있습니다. 이것이 인프라 수준에서의 관측 가능성, 메트릭 및 경보가 매우 중요한 이유입니다.\n\nKubernetes에서 실행할 때 관심 있는 모든 이벤트에 대해 PrometheusRule을 설정하여 수행할 수 있습니다. 예를 들어 스케줄러 노드의 상태, 사용 가능한 워커 노드 수 또는 스케줄러 루프 시간과 같은 특정 Airflow 메트릭을 모니터링할 수 있습니다. 또한 AlertManager를 실행하면 다양한 대상 (Slack, PagerDuty, Opsgenie 등)으로 경보를 발생시킬 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n또 다른 현명한 일은 Airflow 메트릭을 활용하여 환경의 가시성을 향상시키는 것입니다. 현재 Airflow는 StatsD와 OpenTelemetry로 메트릭을 전송하는 것을 지원합니다. 후자가 더 우선되는데, 왜냐하면 OpenTelemetry가 Logs와 Traces를 지원하는 더 완전한 프레임워크이기 때문입니다. 그러나 현재 Airflow는 아직 OTEL을 통한 로그 및 추적을 지원하지 않습니다. (하지만 미래에 지원할 예정입니다!)\n\n사용하고 싶다면, Kubernetes에서 OTEL Collector 배포를 관리해야 합니다 (공식 헬름 차트는 여기에 있습니다). 공식 Airflow 차트는 statsd와 달리 OTEL Collector를 제공하지 않습니다.\n\n표준 메트릭으로 경보 기능을 크게 향상시킬 수 있습니다. 예를 들어, 대기 중인 작업의 총 수를 사용하여 대기열이 특정 시간 동안 너무 많이 증가할 때 트리거할 경고를 설정할 수 있습니다. 예를 들어 SLA 시간보다 더 긴 대기열은 원하지 않으실 겁니다.\n\n❗️경고: 이 이미지는 미리보기용이므로 이미지가 표시되지 않을 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다른 유용한 지표는 DAG 구문 분석 시간과 스케줄러 루프 시간입니다. 이를 통해 Airflow에 영향을 미치는 문제를 빠르게 식별하고 전체 애플리케이션을 속도조절하는 데 도움을 줄 수 있습니다.\n\n## Airflow 메타데이터 관리에 주의\n\n메타데이터 데이터베이스는 성공적인 Airflow 구현의 중요한 부분이며, 성능을 저하시키거나 Airflow를 다운시킬 수 있습니다.\n\nAirflow 노드 및 위에서 언급한 성능 지표를 모니터링하는 것 외에도 데이터베이스의 건강 지표를 모니터링하는 것이 중요합니다. PostgreSQL 또는 MySQL을 사용하는지에 따라 다르지만, CPU 사용량, 사용 가능한 스토리지, 열린 연결 수 등이 가장 일반적인 지표입니다.😂\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테이블 태그를 마크다운 형식으로 변경하면 더 좋은 방법입니다.\n\n오래되거나 사용하지 않는 메타데이터를 정기적으로 정리하는 것도 좋은 실천 방법 중 하나입니다. job, dag_run, task_instance, log, xcom, sla_miss, dags, task_reschedule, task_fail 등과 같은 테이블이 여기에 포함될 수 있습니다. 이 모든 메타데이터는 Airflow 내에서 계속 쌓이며, 예를 들어 작업 상태를 가져오는 평균 쿼리가 필요 이상으로 오래 걸릴 수 있습니다. 또한 Airflow가 로드되고 탐색하는 것이 너무 느리게 느껴지는 적이 있나요? 메타데이터가 쌓이는 것이 그 원인일 수 있습니다.\n\n다행히도, Airflow에는 그에 대한 네이티브 명령어가 제공됩니다. airflow db clean — 이 명령어는 동작을 구성할 수 있는 선택적 플래그와 함께 제공됩니다. 자세한 내용은 여기에서 확인할 수 있습니다.\n\nKubernetes를 사용하는 경우에는 Airflow 차트에 추가 리소스로 CronJob을 설정하여 지정한 플래그와 함께 주기적으로 airflow db clean 명령어를 실행하도록 하는 것이 좋은 접근 방법입니다. 구현 규모에 따라 이 작업을 매일 또는 매주 실행해야 할 수도 있습니다.\n\n# 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 작성한 텍스트가 Airflow on Kubernetes를 사용하는 여러 팀들이 함께 사용하는 협업 환경에서 여정을 시작하는 데 도움이 될 것을 희망합니다.\n\n여기에 언급되지 않은 많은 구성 요소와 세부 정보들이 성공적인 구현에 기여하고 있습니다. 아직 개선할 부분이 많고 갈 길이 멀지만, 여러분도 경험을 공유하거나 질문이 있으시면 언제든지 연락해주세요. 함께 이야기 나누면 더 좋겠죠 😄\n","ogImage":{"url":"/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_0.png"},"coverImage":"/assets/img/2024-06-23-WhatwelearnedafterrunningAirflowonKubernetesfor2years_0.png","tag":["Tech"],"readingTime":16},{"title":"YAML Lint로 YAML 파일 검증하는 방법 ","description":"","date":"2024-06-23 01:06","slug":"2024-06-23-ValidateyourYAMLFileswithYAMLLint","content":"\n<img src=\"/assets/img/2024-06-23-ValidateyourYAMLFileswithYAMLLint_0.png\" />\n\n쿠버네티스에서 YAML 파일을 다루는 것은 데브옵스 분야에서 꼭 필요한 기술입니다. 쿠버네티스와 함께 작업할 때 중요한 측면 중 하나는 YAML 파일을 만들고 관리하는 것입니다. 이 파일들은 쿠버네티스 구성의 주춧돌이죠.\n\n그리고 YAML이 강력하다고 생각하지만, 매번 문법을 정확하게 맞추는 것이 까다로울 수 있어요.\n\n저는 쿠버네티스 여정에서 앞으로 기억에 남는 양상 중 하나가 YAML 구문 오류를 디버깅하는 데 상당한 시간을 투자했던 것입니다. 그때 이 린팅 도구를 발견했고, 이후로는 저의 작업 흐름에서 없어서는 안 될 부분이 됐습니다. 여러분과 공유하여 디버깅 시간을 절약할 수 있기를 간절히 바랍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nYAML 파일에서 가장 자주 발생하는 문제 중 하나는 들여쓰기가 잘못된 경우입니다..🤯.. 그렇지 않나요?\n\nYAML은 구조를 나타내기 위해 들여쓰기를 사용하며, 심지어 한 칸의 공백이 잘못되면 전체 파일이 깨질 수 있습니다 💔.. (우리에게는 대부분의 시간이 발생합니다 😉)\n\nKubernetes 클러스터에 적용하기 전에 YAML 파일을 유효성 검사하는 것이 중요합니다. 동의하세요..?\n\n## YAML Lint: 세이버\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![YAML Lint](/assets/img/2024-06-23-ValidateyourYAMLFileswithYAMLLint_1.png)\n\n내가 자주 사용하는 도구 중 하나는 YAML Lint입니다. 이 도구는 YAML 파일을 유효성을 검사하여 올바른 구문을 준수하는지 확인하는 데 도움을 줍니다. 오류를 강조하고 자세한 피드백을 제공하여 문제를 파악하고 해결하기 쉽게 만듭니다.\n\n## YAML Lint 사용 방법\n\n간단합니다. 다양한 방법으로 사용할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 온라인 유효성 검사기: YAML Lint와 같은 웹 사이트를 통해 YAML 내용을 붙여넣고 즉시 유효성을 검사할 수 있습니다. (그것만으로 충분히 간단하죠 :) )\n- 명령 줄 도구: CLI를 사용하는 사람을 위해 yamllint와 같은 도구를 설치하고 명령 줄에서 파일을 직접 유효성 검사할 수 있습니다. (예: yamllint file.yaml)\n\nYAML Lint (CLI) 설치:\n\n```js\nsudo apt-get install yamllint\n```\n\n## 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nYAML Lint와 같은 도구들이 정말 생명구조였어요, YAML 파일을 빠르고 효율적으로 검증하고 수정하는 저를 도와주었죠. 여러분은 YAML 파일을 어떻게 확인하시나요..?\n\n아래 댓글로 알려주세요 👇\n\n![이미지](https://miro.medium.com/v2/resize:fit:996/1*q5OVUu9A0bH8GS7wDRYhbQ.gif)\n\n다음에 또 뵙겠습니다 :) — Venila Musunuru\n","ogImage":{"url":"/assets/img/2024-06-23-ValidateyourYAMLFileswithYAMLLint_0.png"},"coverImage":"/assets/img/2024-06-23-ValidateyourYAMLFileswithYAMLLint_0.png","tag":["Tech"],"readingTime":3},{"title":"eBPF와 Go를 사용한 투명 프록시 구현 방법","description":"","date":"2024-06-23 01:04","slug":"2024-06-23-TransparentProxyImplementationusingeBPFandGo","content":"\n## eBPF를 활용한 투명 프록시 구축\n\n투명 프록시는 인라인 프록시로도 알려져 있으며, 클라이언트 요청을 가로채어 수정없이 리디렉션하는 기술입니다. 사용자에게는 보이지 않고 네트워크 설정을 조정할 필요도 없이 작동하므로 사용자들이 그 존재를 알 수 없습니다. 이 기술은 네트워크 관리, 보안 정책 강제, 트래픽 모니터링, 최적화 등에 매우 유용합니다. 투명 프록시는 콘텐츠 필터링, 캐시 가속, 트래픽 제어, 로드 밸런싱 등 다양한 기능을 실행할 수 있습니다. 투명 프록시를 구현하는 데 자주 사용되는 기술로는 TPROXY, NAT 등이 있습니다.\n\n이 블로그 게시물에서는 eBPF를 사용하여 투명 프록시의 구현을 설명하겠습니다. 구체적으로는 ebpf-go 패키지와 함께 Golang을 활용할 것입니다.\n\n<img src=\"/assets/img/2024-06-23-TransparentProxyImplementationusingeBPFandGo_0.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 투명 프록시 구현에서의 eBPF\n\nExtended Berkeley Packet Filter (eBPF)는 Linux 커널에서 샌드박스화된 프로그램을 실행할 수 있는 뛰어난 도구로, 투명 프록시를 구현하는 데 강력한 기능을 제공합니다. 이 능력은 사용자 공간과 커널 공간 간의 컨텍스트 전환 오버헤드 없이 고성능 패킷 처리와 실시간 트래픽 조작이 가능하도록 합니다.\n\neBPF를 사용하여 투명 프록시를 구현하는 과정에는 네트워크 가로채기와 전달의 다른 측면을 담당하는 세 가지 서로 다른 eBPF 프로그램이 포함됩니다:\n\n- 연결 설정 시 주소 교체: 첫 번째 eBPF 프로그램인 cgroup/connect4은 connect 시스템 콜에 연결됩니다. 클라이언트가 대상 서버에 연결을 시도할 때 이 프로그램은 연결 시도를 가로채어 대상의 IP 주소와 포트를 로컬 투명 프록시의 것으로 교체합니다. 이 리디렉션은 클라이언트에게 완전히 투명합니다. 동시에 원래 대상 주소와 포트는 map_socks eBPF 맵 내에 저장되어 있어 다른 eBPF 프로그램이 나중에 이 정보를 참조할 수 있도록 합니다.\n- 연결 후 소스 주소 기록: 두 번째 eBPF 프로그램인 sockops는 프록시와 프록시 간의 연결이 성공적으로 설정된 후에 실행됩니다. 주요 기능은 연결의 소스 주소와 포트를 기록하는 것입니다. 이 정보는 map_socks eBPF 맵의 해당 항목에 업데이트됩니다. 추가적으로, 소스 포트와 소켓의 쿠키 (고유 식별자)는 map_ports eBPF 맵에 매핑되어 모든 필요한 연결 세부 정보가 다른 eBPF 프로그램에서 사용 가능하도록 보장됩니다. 이 단계는 네트워크 연결의 상태를 유지하기 위해 중요합니다.\n- 원래 대상 정보를 기반으로 전달: 세 번째 eBPF 프로그램인 cgroup/getsockopt은 Pipy 프록시가 getsockopt 호출을 사용하여 원래 대상 정보를 조회할 때 활성화됩니다. 이 프로그램은 source port를 사용하여 map_ports에서 원래 소켓의 쿠키를 검색한 다음 map_socks에 저장된 원래 대상 정보에 액세스합니다. 이 정보를 사용하여 원래 대상 서버와의 연결을 설정하고 클라이언트 요청을 전달합니다. 이로써 트래픽이 프록시에서 처리된 후 의도한 대상으로 투명하게 리디렉션되도록 보장됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n세 프로그램 모두 특정 cgroup에 연결되어 있습니다. 우리의 경우에는 루트 cgroup이지만 다른 것일 수도 있습니다. 이는 해당 그룹 내의 프로세스가 지정된 시스템 호출 (syscalls)을 실행할 때에만 활성화됨을 보장합니다. 현재, 우리의 구성은 TCP IPv4 연결만 프록시하고 있지만 다른 프로토콜에 대해 적응 가능합니다.\n\n이론적으로는 여기까지입니다. 이제 코드를 살펴봅시다.\n\n## 사용자 공간 코드\n\n```js\n패키지 main\n\n//go:generate go run github.com/cilium/ebpf/cmd/bpf2go -type Config proxy proxy.c\n\nimport (\n \"fmt\"\n \"io\"\n \"log\"\n \"net\"\n \"os\"\n \"syscall\"\n \"time\"\n \"unsafe\"\n\n \"github.com/cilium/ebpf\"\n \"github.com/cilium/ebpf/link\"\n \"github.com/cilium/ebpf/rlimit\"\n)\n\nconst (\n CGROUP_PATH = \"/sys/fs/cgroup\" // 루트 cgroup 경로\n PROXY_PORT      = 18000 // 프록시 서버가 수신 대기하는 포트\n SO_ORIGINAL_DST = 80 // 원본 대상 주소를 가져오기 위한 소켓 옵션\n)\n\n// SockAddrIn은 SO_ORIGINAL_DST에 의해 \"검색된\" IPv4 sockaddr_in 구조체를 보관하는 구조체입니다.\ntype SockAddrIn struct {\n SinFamily uint16\n SinPort   [2]byte\n SinAddr   [4]byte\n // sockaddr_in의 크기와 일치하도록 패딩 처리\n Pad [8]byte\n}\n\n// getsockopt에 대한 도우미 함수\nfunc getsockopt(s int, level int, optname int, optval unsafe.Pointer, optlen *uint32) (err error) {\n _, _, e := syscall.Syscall6(syscall.SYS_GETSOCKOPT, uintptr(s), uintptr(level), uintptr(optname), uintptr(optval), uintptr(unsafe.Pointer(optlen)), 0)\n if e != 0 {\n  return e\n }\n return\n}\n\n// HTTP 프록시 요청 핸들러\nfunc handleConnection(conn net.Conn) {\n defer conn.Close()\n\n // RawConn을 사용하면 Go에서 기본 소켓 파일 기술자에 대한 저수준 작업을 수행하는 것이 필요합니다.\n // 이를 통해 소켓이 설정한 원본 대상 주소를 getsockopt을 사용하여 가져올 수 있습니다.\n rawConn, err := conn.(*net.TCPConn).SyscallConn()\n if err != nil {\n  log.Printf(\"Raw 연결을 가져오지 못했습니다: %v\", err)\n  return\n }\n\n var originalDst SockAddrIn\n // Control이 nil이 아닌 경우, 네트워크 연결을 만든 후 운영 체제에 바인딩하기 전에 호출됩니다.\n rawConn.Control(func(fd uintptr) {\n  optlen := uint32(unsafe.Sizeof(originalDst))\n  // SO_ORIGINAL_DST 옵션을 사용하여 시스템 호출을 수행하여 원본 대상 주소를 가져옵니다.\n  err = getsockopt(int(fd), syscall.SOL_IP, SO_ORIGINAL_DST, unsafe.Pointer(&originalDst), &optlen)\n  if err != nil {\n   log.Printf(\"getsockopt SO_ORIGINAL_DST 실패: %v\", err)\n  }\n })\n\n targetAddr := net.IPv4(originalDst.SinAddr[0], originalDst.SinAddr[1], originalDst.SinAddr[2], originalDst.SinAddr[3]).String()\n targetPort := (uint16(originalDst.SinPort[0]) << 8) | uint16(originalDst.SinPort[1])\n\n fmt.Printf(\"원본 대상 주소: %s:%d\\n\", targetAddr, targetPort)\n\n // 원본 대상 주소가 프록시에서 접근 가능한지 확인합니다.\n targetConn, err := net.DialTimeout(\"tcp\", fmt.Sprintf(\"%s:%d\", targetAddr, targetPort), 5*time.Second)\n if err != nil {\n  log.Printf(\"원본 대상에 연결할 수 없습니다: %v\", err)\n  return\n }\n defer targetConn.Close()\n\n fmt.Printf(\"%s에서 %s로 연결을 프록시합니다\\n\", conn.RemoteAddr(), targetConn.RemoteAddr())\n\n // 다음 코드는 두 데이터 전송 채널을 생성합니다:\n  // - 클라이언트에서 타겟 서버로 (별도의 고루틴에서 처리).\n  // - 타겟 서버에서 클라이언트로 (주요 고루틴에서 처리).\n go func() {\n  _, err = io.Copy(targetConn, conn)\n  if err != nil {\n   log.Printf(\"타겟으로 데이터 복사 실패: %v\", err)\n  }\n }()\n _, err = io.Copy(conn, targetConn)\n if err != nil {\n  log.Printf(\"타겟으로부터 데이터 복사 실패: %v\", err)\n }\n}\n\nfunc main() {\n // 커널 버전이 5.11 미만인 경우 리소스 제한을 제거합니다.\n if err := rlimit.RemoveMemlock(); err != nil {\n  log.Print(\"Memlock 제거 중 오류 발생:\", err)\n }\n\n // 컴파일된 eBPF ELF를 로드하고 커널로 로드\n // 참고: eBPF 프로그램을 고정시킬 수도 있습니다.\n var objs proxyObjects\n if err := loadProxyObjects(&objs, nil); err != nil {\n   log.Print(\"eBPF 오브젝트 로드 오류:\", err)\n }\n defer objs.Close()\n\n // eBPF 프로그램을 루트 cgroup에 연결\n connect4Link, err := link.AttachCgroup(link.CgroupOptions{\n  Path:    CGROUP_PATH,\n  Attach:  ebpf.AttachCGroupInet4Connect,\n  Program: objs.CgConnect4,\n })\n if err != nil {\n   log.Print(\"CgConnect4 프로그램을 Cgroup에 연결 중 오류 발생:\", err)\n }\n defer connect4Link.Close()\n\n sockopsLink, err := link.AttachCgroup(link.CgroupOptions{\n  Path:    CGROUP_PATH,\n  Attach:  ebpf.AttachCGroupSockOps,\n  Program: objs.CgSockOps,\n })\n if err != nil {\n   log.Print(\"CgSockOps 프로그램을 Cgroup에 연결 중 오류 발생:\", err)\n }\n defer sockopsLink.Close()\n\n sockoptLink, err := link.AttachCgroup(link.CgroupOptions{\n  Path:    CGROUP_PATH,\n  Attach:  ebpf.AttachCGroupGetsockopt,\n  Program: objs.CgSockOpt,\n })\n if err != nil {\n   log.Print(\"CgSockOpt 프로그램을 Cgroup에 연결 중 오류 발생:\", err)\n }\n defer sockoptLink.Close()\n\n // 로컬호스트에 프록시 서버 시작합니다\n // 이 예제에서는 IPv4만을 보여주지만 IPv6에도 동일한 방법을 사용할 수 있습니다.\n proxyAddr := fmt.Sprintf(\"127.0.0.1:%d\", PROXY_PORT)\n listener, err := net.Listen(\"tcp\", proxyAddr)\n if err != nil {\n  log.Fatalf(\"프록시 서버 시작 실패: %v\", err)\n }\n defer listener.Close()\n\n // 프록시 서버 구성으로 proxyMaps 맵 업데이트합니다.\n // 이는 프록시 서버 PID가 필요하기 때문에 필요합니다.\n var key uint32 = 0\n config := proxyConfig{\n  ProxyPort: PROXY_PORT,\n  ProxyPid: uint64(os.Getpid()),\n }\n err = objs.proxyMaps.MapConfig.Update(&key, &config, ebpf.UpdateAny)\n if err != nil {\n  log.Fatalf(\"프록시맵 업데이트 실패: %v\", err)\n }\n\n log.Printf(\"PID %d의 프록시 서버가 %s에서 수신 대기 중\", os.Getpid(), proxyAddr)\n for {\n  conn, err := listener.Accept()\n  if err != nil {\n   log.Printf(\"연결 수락 실패: %v\", err)\n   continue\n  }\n\n  go handleConnection(conn)\n }\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 커널 스페이스 코드\n\n```js\n//go:build ignore\n#include <stddef.h>\n#include <linux/bpf.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/in.h>\n#include <sys/socket.h>\n\n#include \"bpf-builtin.h\"\n#include \"bpf-utils.h\"\n\n#undef bpf_printk\n#define bpf_printk(fmt, ...)                            \\\n({                                                      \\\n        static const char ____fmt[] = fmt;              \\\n        bpf_trace_printk(____fmt, sizeof(____fmt),      \\\n                         ##__VA_ARGS__);                \\\n})\n\n#define MAX_CONNECTIONS 20000\n\nstruct Config {\n  __u16 proxy_port;\n  __u64 proxy_pid;\n};\n\nstruct Socket {\n  __u32 src_addr;\n  __u16 src_port;\n  __u32 dst_addr;\n  __u16 dst_port;\n};\n\nstruct {\n  int (*type)[BPF_MAP_TYPE_ARRAY];\n  int (*max_entries)[1];\n  __u32 *key;\n  struct Config *value;\n} map_config SEC(\".maps\");\n\nstruct {\n  int (*type)[BPF_MAP_TYPE_HASH];\n  int (*max_entries)[MAX_CONNECTIONS];\n  __u64 *key;\n  struct Socket *value;\n} map_socks SEC(\".maps\");\n\nstruct {\n  int (*type)[BPF_MAP_TYPE_HASH];\n  int (*max_entries)[MAX_CONNECTIONS];\n  __u16 *key;\n  __u64 *value;\n} map_ports SEC(\".maps\");\n\n// 이 후킹은 프로세스가 connect() 시스템 호출 시 호출됩니다.\n// 연결을 투명한 프록시로 리디렉션하지만 원래 대상 주소와 포트를 map_socks에 저장합니다.\nSEC(\"cgroup/connect4\")\nint cg_connect4(struct bpf_sock_addr *ctx) {\n  // IPv4 TCP 연결만 전달\n  if (ctx->user_family != AF_INET) return 1;\n  if (ctx->protocol != IPPROTO_TCP) return 1;\n\n  // 프록시 자체를 프록시하지 않도록 함\n  __u32 key = 0;\n  struct Config *conf = bpf_map_lookup_elem(&map_config, &key);\n  if (!conf) return 1;\n  if ((bpf_get_current_pid_tgid() >> 32) == conf->proxy_pid) return 1;\n\n  // 이 필드에는 connect() 시스템 호출에 전달된 IPv4 주소가 포함됩니다.\n  // 즉, 이 소켓 대상 주소 및 포트에 연결\n  __u32 dst_addr = ntohl(ctx->user_ip4);\n  // 이 필드에는 connect() 시스템 호출에 전달된 포트 번호가 포함됩니다.\n  __u16 dst_port = ntohl(ctx->user_port) >> 16;\n  // 대상 소켓의 고유 식별자\n  __u64 cookie = bpf_get_socket_cookie(ctx);\n\n  // 대상 소켓을 쿠키 키 아래 저장\n  struct Socket sock;\n  __builtin_memset(&sock, 0, sizeof(sock));\n  sock.dst_addr = dst_addr;\n  sock.dst_port = dst_port;\n  bpf_map_update_elem(&map_socks, &cookie, &sock, 0);\n\n  // 연결을 프록시로 리디렉션\n  ctx->user_ip4 = htonl(0x7f000001); // 127.0.0.1 == 프록시 IP\n  ctx->user_port = htonl(conf->proxy_port << 16); // 프록시 포트\n\n  bpf_printk(\"클라이언트 연결을 프록시로 리디렉션 중\\n\");\n\n  return 1;\n}\n\n// 해당 cgroup(다시 전송 시간 초과, 연결 설정 등)에 소켓 작업이 발생할 때 호출되는 프로그램입니다.\n// 이는 클라이언트 출처 주소와 포트를 기록하기 위한 것입니다.\nSEC(\"sockops\")\nint cg_sock_ops(struct bpf_sock_ops *ctx) {\n  // IPv4 연결만 전달\n  if (ctx->family != AF_INET) return 0;\n\n  // Active socket with an established connection\n  if (ctx->op == BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB) {\n    __u64 cookie = bpf_get_socket_cookie(ctx);\n\n    // 해당 cookie에 대한 맵에서 소켓 조회\n    // 소켓이 있는 경우 소스 포트와 소켓 매핑 저장\n    struct Socket *sock = bpf_map_lookup_elem(&map_socks, &cookie);\n    if (sock) {\n      __u16 src_port = ctx->local_port;\n      bpf_map_update_elem(&map_ports, &src_port, &cookie, 0);\n    }\n  }\n\n  bpf_printk(\"sockops 후킹 성공\\n\");\n\n  return 0;\n}\n\n// 이는 프록시가 getsockopt SO_ORIGINAL_DST를 통해 원래 대상 정보를 쿼리할 때 트리거됩니다.\n// 이 프로그램은 클라이언트의 소스 포트를 map_ports에서 소켓의 쿠키를 검색하고,\n// 그런 다음 map_socks에서 원래 대상 정보를 가져와 클라이언트 요청을 전달합니다.\nSEC(\"cgroup/getsockopt\")\nint cg_sock_opt(struct bpf_sockopt *ctx) {\n  // SO_ORIGINAL_DST 소켓 옵션은 네트워크 주소 변환(NAT) 및 투명 프록시를 주로 위한 특수화된 옵션입니다.\n  // 전형적인 NAT 또는 투명 프록시 설정에서, 들어오는 패킷은 원래 대상에서 프록시 서버로 리디렉션됩니다.\n  // 패킷을 수신한 프록시 서버는 종종 트래픽을 적절하게 처리하기 위해 원래 대상 주소를 알아야 합니다.\n  // 이것이 SO_ORIGINAL_DST의 역할입니다.\n  if (ctx->optname != SO_ORIGINAL_DST) return 1;\n  // IPv4 TCP 연결만 전달\n  if (ctx->sk->family != AF_INET) return 1;\n  if (ctx->sk->protocol != IPPROTO_TCP) return 1;\n\n  // 클라이언트의 소스 포트 가져오기\n  // 실제로 sk->dst_port인데, SO_ORIGINAL_DST 소켓 옵션으로 getsockopt() 시스템 호출이\n  // 클라이언트의 원래 대상 포트를 가져오기 때문에 클라이언트의 대상 포트를 \"쿼리\"합니다.\n  __u16 src_port = ntohs(ctx->sk->dst_port);\n\n  // 클라이언트 src_port를 사용하여 소켓 쿠키 검색\n  __u64 *cookie = bpf_map_lookup_elem(&map_ports, &src_port);\n  if (!cookie) return 1;\n\n  // 쿠키(소켓 식별자)를 사용하여 map_socks에서 원래 소켓(대상으로 클라이언트 연결)을 가져옵니다.\n  struct Socket *sock = bpf_map_lookup_elem(&map_socks, cookie);\n  if (!sock) return 1;\n\n  struct sockaddr_in *sa = ctx->optval;\n  if ((void*)(sa + 1) > ctx->optval_end) return 1;\n\n  // 원래 대상 대상과의 연결 설정\n  ctx->optlen = sizeof(*sa);\n  sa->sin_family = ctx->sk->family; // 주소 패밀리\n  sa->sin_addr.s_addr = htonl(sock->dst_addr); // 대상 주소\n  sa->sin_port = htons(sock->dst_port); // 대상 포트\n  ctx->retval = 0;\n\n  bpf_printk(\"원래 대상으로 연결 리디렉션 중\\n\");\n\n  return 1;\n}\n\nchar __LICENSE[] SEC(\"license\") = \"GPL\";\n```\n\n자세한 내용은 아래 GitHub 저장소 링크에서 확인하세요:\n\n# 성능 평가\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마무리하며, 저는 호스트 서버의 영향을 평가하기 위해 eBPF 프로그램이 트래픽을 가로챌 때 발생하는 지연 시간과 CPU 부하에 초점을 맞춘 기본 성능 테스트를 실시했습니다. 이 테스트는 1만 개의 요청을 기준으로 평균 지연 시간을 측정하는 것을 포함했습니다.\n\n![이미지](/assets/img/2024-06-23-TransparentProxyImplementationusingeBPFandGo_1.png)\n\n우리의 결과는 eBPF 프로그램이 평균적으로 약 1밀리초의 고정 eBPF 오버헤드를 추가한다는 것을 나타냅니다. 또한 각 후크에 의해 도입된 평균 CPU 부하는 다음과 같습니다: sockops의 경우 0.4%, cgroup/connect4의 경우 0.1%, cgroup/getsockopt의 경우 0.09%입니다. 요컨대, 사실상 아무것도 아닙니다.\n\n이 연구 결과는 eBPF 프로그램에 의한 추가 지연 시간과 CPU 부하와 트래픽 가로채기의 이점 사이의 균형을 다루고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n결론적으로, eBPF를 활용하여 투명 프록시를 구현하는 것은 네트워크 가로채기와 전달에 강력한 해결책을 제시합니다. eBPF의 높은 성능 패킷 처리 및 리눅스 커널 내에서 실시간 트래픽 조작과 같은 기능을 활용함으로써, 투명 프록시는 보안 강화, 트래픽 모니터링 및 최적화를 포함한 다양한 목적으로 네트워크 트래픽을 효율적으로 관리할 수 있습니다. 제공된 예제는 Go와 eBPF의 통합을 보여주며, 서로 다른 eBPF 프로그램이 원활하게 협력하여 투명 프록시 기능을 달성하는 방법을 보여줍니다. 이 접근 방식은 최소한의 오버헤드를 보장할 뿐만 아니라 TCP IPv4 연결을 넘어 추가 프로토콜을 지원하기 위해 프록시 기능을 확장하는 유연성을 제공합니다.\n","ogImage":{"url":"/assets/img/2024-06-23-TransparentProxyImplementationusingeBPFandGo_0.png"},"coverImage":"/assets/img/2024-06-23-TransparentProxyImplementationusingeBPFandGo_0.png","tag":["Tech"],"readingTime":14},{"title":"카프카 클러스터를 쿠버네티스로 이관하는 방법 매끄러운 전환 가이드","description":"","date":"2024-06-23 01:01","slug":"2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes","content":"\n# 배경\n\n저희는 Zendesk에서 자체 Kafka 인프라를 관리합니다. 처음에는 시장에 세련된 관리형 Kafka 서비스가 없었습니다. 그래서 우리는 Chef를 통해 Kafka 인프라를 구축하고 AWS EC2 인스턴스에 배포했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n카프카 클러스터는 두 부분으로 구성됩니다: 카프카 브로커 집합과 주키퍼 노드입니다. 브로커는 카프카 클러스터의 서버로 데이터를 저장하고 관리하며 메시지의 발행과 구독을 처리합니다. 주키퍼 클러스터는 카프카 브로커를 조정하고 관리하는 쿼럼으로, 리더 선출, 클러스터 구성원 관리 및 메타데이터 유지와 같은 작업을 처리합니다.\n\n저희는 VM 기반 인프라에서 Kubernetes (K8s) 기반으로 이동 중입니다. Chef 스택의 지원이 줄어들어 카프카 인프라를 마이그레이션해야 했습니다.\n\n## 원활한 마이그레이션 정의\n\n카프카 클러스터의 원활한 마이그레이션은 데이터 무결성과 일관성을 유지하면서 투명하고 다운 타임이 없는 프로세스를 포함하며, 클라이언트의 최소한의 변경과 수동 개입이 필요한 최소한의 변화를 필요로 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이전 클러스터에서 구성 및 데이터를 복제하고, 클라이언트를 새 클러스터로 전환하며, 운영 연속성과 성능 동등성을 보장하는 것을 포함합니다.\n\n원활한 마이그레이션의 복잡성은 클러스터 규모와 클라이언트 수와 직접적인 관련이 있습니다. 사용 빈도가 낮거나 주로 오프라인 워크로드를 처리하는 클러스터를 마이그레이션하는 것은 비교적 간단할 수 있습니다. 그러나 대규모 실시간 워크로드를 처리하고 상당한 데이터 양을 다루는 클러스터를 마이그레이션하는 것은 훨씬 더 어려운 프로젝트입니다.\n\nZendesk에서는 우리의 Kafka 클러스터가 두 번째 범주에 속합니다. Kafka 클러스터는 상당한 양의 데이터를 관리하며 100개 이상의 서비스에서 사용됩니다.\n\nKafka 클러스터 상태 자세히:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 12개의 Kafka 클러스터\n- 모든 환경을 통틀어 약 100개의 브로커\n- 300TB의 데이터 저장 용량\n- 하루에 300억 건의 메시지\n- 1000개 이상의 토픽\n- 연결된 서비스가 100개 이상\n\n## 주요 도전 과제\n\n이러한 대규모이자 중요한 클러스터를 이주하는 것은 신중히 설계된 디자인이 필요합니다. 주의를 요하는 다수의 세부 사항 중에서, 주요 대응해야 할 도전 과제는 다음과 같습니다:\n\n- 복제 단계 중 데이터 무결성과 클러스터 가용성 보장,\n  클라이언트들에게 원활한 전환 기회를 제공하기 위한 전환 단계 스핑크 요소입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터가 완전히 복제된 후 새 클러스터로 고객이 대규모로 전환될 수 있는 구체적인 전환 시점이 없다는 것을 이해하는 것이 중요합니다. 원활한 이주 요구 사항으로 인해 전환 단계는 지속적인 데이터 복제와 클라이언트 전환을 동시에 수용해야 합니다.\n\n# 이주 디자인\n\n## 일방향 클러스터 간 이주\n\n첫 번째 버전 디자인은 단방향 클러스터 간 이주입니다. 이 방법은 MirrorMaker2를 사용하여 이전 클러스터의 데이터 및 구성을 새 클러스터로 복제합니다. MirrorMaker가 클러스터 간의 초기 데이터 로드를 전송한 후에도 데이터를 동기화 상태로 유지하며, 각 클라이언트가 이전 클러스터와 연결을 끊고 새 클러스터에 연결하는 전환 프로세스를 실행합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_2.png)\n\n문제: 마이그레이션 복잡성이 클라이언트로 이동됩니다.\n\n이 접근 방식은 처음에는 간단하고 견고해 보이지만, Kafka 클라이언트를 관리하는 각 팀에게 마이그레이션의 복잡성을 이동시킵니다. 이는 모든 클라이언트가 자체 커트오버 전략을 개발하고 실행해야 하며, 구현에 따라 복수의 배포가 필요할 수 있습니다.\n\n예를 들어, 단일팀이 한 주제의 프로듀서와 컨슈머를 모두 제어하는 가장 간단한 시나리오에서, 클라이언트는 코드 변경과 다수의 배포를 통해 다음 단계를 거쳐 마이그레이션될 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 모든 소비자를 새 클러스터로 이동합니다.\n- 이전 클러스터로의 생성을 중지합니다.\n- 데이터가 새 클러스터에 완전히 동기화될 때까지 기다립니다.\n- 새 클러스터를 가리키는 생산자를 시작합니다.\n\n주제의 여러 팀이 서로 다른 소비자 그룹을 제어하는 더 복잡한 시나리오에서는 모든 팀이 참여하는 조정된 마이그레이션이 필요합니다. 게다가 소비자가 또 다른 주제의 생성자 역할도 한다면 복잡성이 증가하여 복잡하고 다단계의 마이그레이션 과정으로 이어질 수 있습니다.\n\n부수적인 오버헤드\n\n특정 마이그레이션 디자인에는 몇 가지 부수적인 오버헤드가 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단일 장애 지점: MirrorMaker2 연결 클러스터가 단일 장애 지점입니다. 모든 주제의 소비자가 새 클러스터로 성공적으로 마이그레이션되어 있지만 MirrorMaker2의 실패로 인해 이전 클러스터에서 남은 실시간 데이터 동기화에 실패하는 시나리오를 상상해보세요.\n- 장애 감지 및 장애 조치 작업의 간접성: MirrorMaker2의 실패는 각 팀의 통제 범위를 벗어나지만 클라이언트 장애 조치 프로세스는 완전히 그들에게 의존적이며 이전 버전의 코드 변경이나 재배포를 필요로 합니다.\n- 긴 꼬리 문제: 마이그레이션의 완전한 완료는 가장 느린 클라이언트의 속도에 따라 달려있습니다. 앞서 언급된 복잡성으로 인해 일부 클라이언트는 마이그레이션을 완료하는 데 상당히 오랜 기간이 걸릴 수 있으며 이는 전체 프로젝트 전달 속도를 늦춥니다.\n- 비용: 이 단계에서 브로커에 대한 배로된 컴퓨팅 및 저장소 비용뿐만 아니라 세 배로 증가한 데이터 전송 비용에 직면합니다. 즉, 이전 브로커에서 연결 클러스터로 데이터를 이동하고 새로운 브로커로 다시 옮기는 비용입니다.\n\n## 클러스터 간 마이그레이션은 가능하지만, 더 나은 결과를 추구합니다\n\n이 접근 방식은 클라이언트 관점에서의 원활한 마이그레이션이 아니며 운영 및 비용 대비로 이 규모에서 실용적인 구현이 아닙니다.\n\n## 통합된 Kafka 클러스터 내 브로커 수준의 마이그레이션\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n클러스터 간 이동의 주요 도전 과제는 어느 시점에서든 클라이언트가 다른 클러스터를 가리켜야 한다는 것입니다. 데이터 무결성을 보장하기 위해 커트오버의 타이밍은 각각의 소비자와 생산자에 따라 다릅니다.\n\n이 문제를 해결하기 위해 더 강렬한 질문을 해보는 것은 유용할 수 있습니다: 클라이언트가 클러스터 전환에 대해 무관심할 수 있을까요?\n\n이 기사에서 언급한 대로, 클러스터 이전은 이전 클러스터에서 설정 및 데이터를 새 클러스터로 복제하고 클라이언트를 전환하는 것을 포함합니다.\n\n하지만, 만약 우리가 방정식에서 클라이언트 전환이 필요 없게 한다면 어떻게 될까요? 새로운 클러스터를 구축하는 대신, K8s에서 새로운 브로커 그룹을 구성하고 기존 클러스터에 등록할 수 있습니다. 이렇게 하면 클라이언트가 클러스터를 변경할 필요가 없어집니다. 유일하게 남은 작업은 구체화된 Kafka 클러스터 내에서 이전 브로커에서 새 브로커로 데이터를 전송하는 것뿐입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그렇습니다! 새로운 중개업체가 기존 클러스터에 참여할 예정이기 때문에 구성 이관이 필요하지 않습니다. 필요한 구성은 이미 클러스터 내에 준비되어 있어 프로세스가 간소화됩니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_3.png)\n\n## 중개업체 간 데이터 전송이 클라이언트 스위치오버를 어떻게 달성하나요?\n\n이 프로세스는 카프카 클라이언트 상호작용 메커니즘에 의해 이뤄집니다. 카프카 주제는 일련의 파티션으로 구성되며, 각 파티션에는 여러 개의 레플리카가 있을 수 있습니다. 대부분의 경우, 클라이언트는 리더 레플리카와 상호작용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 브로커에서 다른 브로커로 토픽 데이터를 이동할 때는 기본적으로 한 브로커에서 다른 브로커로 복제본을 전송하는 과정을 포함합니다. 한번 복제본이 완전히 이전되고 리더 복제본으로 선출되면, 클라이언트는 이 변경 사항을 감지하고 새 리더 복제본과 상호 작용하기 시작합니다. 결과적으로, 새로운 브로커와 상호 작용을 시작합니다.\n\n아래 그래프에 설명된 프로세스를 따라, 모든 복제본이 새 브로커에 저장되면 모든 클라이언트가 새 브로커와 상호 작용하며, 이전 브로커는 더 이상 중요한 작업을 수행하지 않는다는 것을 의미합니다.\n\n이 접근 방식이 어떻게 작동하는지 구체적으로 설명해 봅니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단계 1: 동일한 수의 브로커를 K8s에 배포하고 ZooKeeper 하에 동일한 Kafka 클러스터에 등록합니다. 이 단계에서 EC2와 K8s의 브로커는 동일한 클러스터의 멤버입니다. 그러나 EC2 브로커는 여전히 모든 데이터를 보유하고 모든 클라이언트 트래픽을 처리합니다. K8s 브로커는 ZooKeeper와의 하트비트를 유지하는 것 외에는 비활성화 상태입니다.\n- 단계 2: 토픽 파티션 레플리카가 EC2의 브로커에서 K8s로 마이그레이션을 시작합니다. 한 레플리카가 완전히 이동되면 K8s 브로커는 리더 레플리카를 보유하고 있는지에 따라 실제 클라이언트 토픽을 제공하기 시작할 수 있습니다.\n- 단계 3: 모든 토픽 파티션 레플리카가 EC2의 브로커에서 K8s의 브로커로 이전되었습니다. EC2의 브로커는 더 이상 데이터를 보유하지 않고 클라이언트 요청을 처리하지 않습니다.\n- 단계 4: EC2의 브로커를 축소하여 클러스터를 완전히 K8s 브로커로 구성합니다. 이제 마이그레이션이 완료되었습니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_5.png)\n\n위에 언급된 마이그레이션 단계를 따르면 클라이언트 전환이 필요 없이 성공적으로 진행됩니다. 클라이언트는 마이그레이션 전체 과정 동안 일관되게 한 클러스터와 상호 작용하므로, 클라이언트 관점에서 마이그레이션이 완전히 원활해집니다.\n\n# 심층 분석\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 전반적인 개념을 알게 되었으니 구체적인 설정에 대해 다루어 봅시다:\n\n- 네트워크 설정: 중요한 점은 두 가지 다른 브로커 세트를 그룹화하고, 이 하이브리드 카프카 클러스터에서 브로커 검색을 용이하게 하는 것입니다.\n- 이주 구현 및 조정: 새 브로커로 레플리카를 이동하는 구현이 중요하지만, 조정은 더욱 중요합니다. 효율적인 조정은 롤백과 같은 재해 복구 시나리오를 고려해야 합니다.\n- 메트릭과 모니터링: 하이브리드 클러스터의 모니터링을 효과적으로 수행하는 것은 EC2 및 K8s에 있는 브로커가 트래픽을 처리할 때 문제 감지, 디버깅 및 성능 세밀 조정에 중요합니다.\n- 브로커 중단 관리: 고가용성을 위해 서로 다른 가용 영역(AZs)에 있는 여러 브로커가 중단되지 않도록 롤링 브로커를 설정하거나 이주 중에도 방지해야 합니다.\n\n## 브로커 간 및 클라이언트-브로커 통신을 위한 네트워크 설정\n\n브로커 수준의 이주에 대한 중요한 전제 조건은 두 세트의 브로커와 주키퍼 클러스터가 서로에게 비교적 낮은 지연 시간으로 통신할 수 있어야 한다는 것입니다. 게다가 클라이언트는 두 세트의 브로커를 발견하고 낮은 지연 시간으로 통신할 수 있어야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리의 네트워크 아키텍처로 첫 번째 문제가 해결되었습니다. EC2 상의 브로커, Zookeeper 클러스터 및 K8s 클러스터는 서로 다른 서브넷에 배치되었지만, 동일한 AWS 계정 내에 있으며 서브넷은 서로 연결되어 있습니다.\n\n클라이언트가 EC2 및 K8s에 있는 브로커를 발견할 수 있도록 하려면 약간의 작업이 필요합니다. 무엇을 해야 하는지 이해하려면 먼저 클라이언트가 브로커를 어떻게 발견하는지를 파헤쳐야 합니다.\n\n클라이언트-브로커 발견 메커니즘\n\nKafka 클라이언트는 대상 토픽 아래에 있는 파티션의 수와 해당 파티션 복제본이 어떻게 분산되었는지에 따라 올바르게 작동하기 위해 거의 모든 브로커를 알아야 합니다. 전형적인 접근 방식:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 고객은 부트스트랩 브로커 중 하나를 선택하여 어떤 브로커가 어떤 파티션을 담당하는지의 메타데이터를 가져옵니다.\n- 고객은 이러한 브로커들과 연결을 설정하도록 요청합니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_6.png)\n\n따라서 EC2의 브로커에 대해 정의된 리버스 프록시에 추가로, K8s의 브로커에 대한 리버스 프록시도 클라이언트에게 제공되어야 합니다. 모든 브로커가 동일한 클러스터 내에 있으므로, EC2 또는 K8s에 있더라도 어떤 브로커든 클라이언트에게 전체 메타데이터를 제공할 수 있을 것입니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_7.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nHashiCorp의 Consul 서비스는 서비스 검색에 사용됩니다. 새로운 리버스 프록시 서비스의 IP 주소를 기존의 Kafka Consul 서비스에 등록함으로써, 클라이언트는 두 세트의 브로커를 모두 발견할 수 있습니다. K8s에서 각 브로커의 홍보 리스너와 해당 홍보 리스너의 리버스 프록시는 모두 K8s 서비스가 될 수 있습니다.\n\n## 이주 실행 및 오케스트레이션\n\n지금까지 잘 진행되고 있습니다. EC2 또는 K8s에서 상호 연결된 브로커로 Kafka 클러스터를 구성하는 것이 가능합니다. 클라이언트는 모든 브로커에 연결하여 발견할 수 있습니다. 해결해야 할 다음 질문은 복제본이 어떻게 다른 브로커로 옮겨질 수 있는지, 그 방법은 무엇인지 입니다.\n\n복제본 이동의 실행은 Cruise Control에 의해 가능합니다. 이는 대규모로 Kafka 클러스터를 관리하기 위한 풍부한 API를 제공합니다. 복제본을 이동하기 위해 사용할 수 있는 엔드포인트 중 하나는 remove_broker 입니다. 이 엔드포인트는 모든 해당 브로커의 복제본을 대상 브로커로 이동시켜 브로커를 비활성화합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ncurl -X POST \"$CRUISE_CONTROL_SERVICE/remove_broker?brokerid=11&concurrent_partition_movements_per_broker=25&destination_broker_ids=31&replication_throttle=100000000\"\n```\n\n여기에서 우리가 필요한 것이 바로 이거에요! 위 요청은 Cruise Control에게 모든 레플리카를 브로커-11에서 브로커-31로 이동하도록 요청합니다. 동시에 한 번에 이동 가능한 레플리카 수는 최대 25개이며, 총 복제 쓰로틀 속도는 약 100Mb/s로 설정되어 있습니다.\n\n## Orchestration\n\n크루즈 컨트롤 엔드포인트를 수동으로 호출하는 것은 이상적이지 않습니다. 안전하거나 효율적이지 않기 때문이죠. 그래서 우리는 이 상황을 위해 Kafka-Migration이라는 오케스트레이션 시스템을 구축했습니다. 이 시스템은 다음과 같은 역할을 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 마이그레이션 조건 확인: 예를 들어, 복제 이동 호출을 실행하기 전에 클러스터가 건강한 상태인지, 대상 및 목적지 브로커가 동일한 가용 영역(AZ)에 있는지 확인합니다.\n- 다양한 마이그레이션 모드: 시스템에는 다양한 마이그레이션 모드가 있습니다. '특정 브로커' 모드는 특정 브로커 세트에서 데이터를 다른 브로커로 이동할 수 있습니다. 'AZ' 모드는 기존 브로커에서 목적지 브로커로 데이터를 이동하는 것을 용이하게 합니다. '완전한 마이그레이션' 모드는 새로운 브로커 세트로의 완전한 데이터 이동을 관리하며 올바른 복제 할당을 보장하고 AZ별로 이동을 조정합니다. 이러한 모드와 규칙을 설정하여 복제 진행이 안전하고 관리 가능한 범위 내에 있음을 보장합니다. 결과적으로 영향을 최소화하고 이동 중 발생하는 사건에 신속히 대응할 수 있습니다.\n- 세밀한 마이그레이션 제어: 단방향 마이그레이션 뿐만 아니라 중지, 재개 및 되돌리기 작업도 지원합니다. 예를 들어, 트래픽 급증으로 인해 마이그레이션을 일시 중지하여 운영 트래픽에 더 많은 대역폭을 제공한 다음 트래픽이 다시 감소하면 마이그레이션을 재개할 수 있습니다.\n\n![링크 없음](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_8.png)\n\n## 메트릭 및 모니터링\n\n메트릭을 수집하고 상단에 모니터를 구축하는 것은 문제 감지와 클러스터가 마이그레이션 중에 어떻게 수행되는지에 대한 통찰을 얻는 데 중요합니다. Kafka에 대해 수집해야 할 세 가지 다른 메트릭 소스가 있습니다: Kafka 브로커 JMX 메트릭, 브로커 시스템 수준 메트릭 및 AWS EBS 메트릭입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJMX 및 EBS 메트릭은 대체로 EC2 및 K8s 전반에 걸쳐 크게 일관성이 있어서, 마이그레이션 중에 본질적으로 변경되지 않습니다. EC2와 K8s 브로커의 메트릭에서 유일한 차이점은 태그에 있습니다. 이 태그를 기존 모니터 시스템으로 다시 매핑하는 것은 간단합니다.\n\n브로커 시스템 레벨 메트릭은 조금 흥미로운데, EC2는 가상 머신이지만 K8s 파드는 여러 개의 컨테이너 그룹입니다. VM 기반 메트릭은 때로 컨테이너 기반 메트릭에서 찾을 수 없는 경우가 있습니다. 예를 들어 호스트 CPU의 작업 부하를 시간별로 측정하고 시스템이 얼마나 바쁜지를 나타내는 로드 평균 메트릭은 컨테이너 수준에 직접적인 동등물이 없습니다. 아마도 CPU 쓰로틀링 비율 메트릭을 가장 유사한 대리자로 사용할 수 있지만 완전히 같지는 않습니다.\n\n문제 탐지를 위해 고수준 메트릭에 의존하는 것이 시간별로 저수준 메트릭을 살펴보는 것보다 좋습니다. 사실, 이러한 고수준 Kafka 브로커 JMX 메트릭은 리소스 고갈 문제의 좋은 지표를 제공할 수 있습니다: 프로듀서 지연 시간, 네트워크 풀 용량, 요청 처리기 용량 및 복제 지연 시간.\n\n## 브로커 중단 관리\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마이그레이션 중에는 브로커 장애를 주의 깊게 관리해야 합니다. 브로커 장애란 브로커가 정상적으로 작동하지 않거나 사용 불가능해지는 문제나 사건을 의미합니다. 이는 강제적인(예: 하드웨어 고장으로 인한) 또는 자발적인(예: 브로커 롤링 재시작으로 인한) 장애일 수 있습니다.\n\n잘못된 장애 관리는 가용성이 낮아지게 됩니다.\n\n3개의 가용 영역(AZ)에 걸쳐 있는 브로커로 이루어진 Kafka 클러스터의 경우, 대부분의 토픽이 3개의 복제본과 최소 2개의 인-싱크 복제본으로 구성되어 있습니다. 한 가용 영역에서 일부 브로커를 잃는 것은 관리할 수 있는 일입니다. 그러나 서로 다른 두 가용 영역에서 브로커 두 대를 잃는 것은 문제가 발생할 수 있습니다. 왜냐하면 최소 인-싱크 복제본이 2개로 설정되어 있으면 \"오프라인 파티션\" 문제가 발생합니다.\n\n이러한 시나리오가 발생하지 않도록 예방하는 시스템을 만들어야 합니다. 강제적인 장애는 피할 수 없는 것이지만 강제적인 장애 발생 시 자발적인 장애를 최소화할 수 있도록 개선할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n커파 모니터링 서비스를 이용한 Pod 중단 예산\n\nK8s에서 Pod 중단 예산(PDB)은 K8s에 배포된 브로커 Pod를 모니터링하기 위해 설정됩니다. PDB는 오프라인 브로커를 감지하면 해당 중단 예산을 깨어 K8s의 다른 브로커가 내쫓히는 것을 방지합니다.\n\n그러나 PDB로는 EC2 인스턴스의 브로커 상태를 감지할 수 있는 방법이 없습니다. 우리는 Kafka 모니터링 서비스를 추가로 설정하여 EC2의 브로커 상태를 모니터링 범위에 포함시켰습니다. 이 서비스는 Under Replicated Partitions (URP)를 확인하여 카프카 클러스터 전반의 상태를 감시합니다. URP가 감지되면 서비스는 준비 상태로 전환됩니다.\n\nPDB는 Kafka 모니터링 서비스와 K8s에 배포된 브로커 모두를 모니터링하도록 구성되어 있습니다. 이로써 브로커 Pod를 직접 감시하여 K8s의 오프라인 브로커뿐만 아니라 Kafka 모니터링 서비스를 통해 EC2의 오프라인 브로커도 감지할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Image 1](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_9.png)\n\n# Zookeeper Migration\n\n카프카 브로커를 EC2 인스턴스에서 K8s로 성공적으로 이전했습니다. 그러나 Zookeeper 클러스터는 여전히 EC2 인스턴스에서 실행 중입니다.\n\n![Image 2](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_10.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n물론, Zookeeper 클러스터를 Kafka 브로커를 마이그레이션하는 방식과 유사하게 마이그레이션할 수 있습니다. 그러나 더 좋은 아이디어가 있습니다! Kafka 버전 3.6에서 Kraft가 드디어 General Availability에 도달했는데, 이는 Kraft로 구동된 컨트롤러 역할을 하는 브로커를 사용하여 Zookeeper를 대체할 수 있음을 의미합니다. 이미 브로커가 K8s에서 실행 중이기 때문에, 컨트롤러 역할을 하는 새로운 브로커 집합을 쉽게 설정할 수 있습니다.\n\n![Kubernetes를 통한 Kafka 클러스터 마이그레이션](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_11.png)\n\n# 요약\n\n통합된 Kafka 클러스터 내에서 브로커 수준의 마이그레이션 전략을 적용하여, 우리는 모든 Kafka 클러스터를 K8s로 성공적으로 마이그레이션했습니다. 마이그레이션 프로세스는 클라이언트 관점에서 완전히 원활하며, 마이그레이션으로 인한 사건은 전혀 발생하지 않았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nK8s에서 Kafka를 관리하는 것은 EC2보다 훨씬 쉽습니다. K8s가 제공하는 강력하고 확장 가능하며 유연한 플랫폼 덕분에 배포부터 모니터링, 확장, 그리고 자가 치유까지 많은 운영 작업이 자동화됩니다. 잘 정의된 K8s 객체와 Custom Resource Definition(CRD)을 통해 우리는 Kafka 주변에 오퍼레이터를 구축하여 운영 작업을 더욱 자동화할 수 있습니다.\n\n개선된 자동화와 감소된 운영 부담은 우리에게 Kafka 생태계를 더 빠른 속도로 발전시킬 수 있는 기회를 제공할 뿐만 아니라, 팀의 개발 시간을 새로운 개발 작업에 집중할 수 있도록 해줍니다.\n","ogImage":{"url":"/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_0.png"},"coverImage":"/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_0.png","tag":["Tech"],"readingTime":18},{"title":"Challenge 14 바운스 방법 이해 및 구현","description":"","date":"2024-06-23 00:59","slug":"2024-06-23-Challenge14bounce","content":"\n쿠버네티스 서비스: 생각했던 대로 항상 작동하지 않을 수 있어요.\n\n쿠버네티스에서는 서비스가 존재하지 않아요: IP 주소나 포트를 수신하는 프로세스가 없어요. 대신, 쿠버네티스는 고급 네트워크 규칙을 사용하여 로드 밸런서를 모방해요.\n\n기본적으로, 이러한 네트워크 규칙은 Kube-proxy가 관리하는 iptables 규칙을 사용해요.\n\n하지만, Cilium으로 작성된 eBPF를 사용하는 대안이 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"왜 \"표준\" 로드 밸런서를 사용하지 않는 거죠?\n\n이것은 주로 단일 고장 지점의 문제를 극복하기 위해 설계되었습니다. 각 노드에 규칙이 기입되어 있기 때문에, 단일 구성 요소의 고장이 전체 클러스터에 영향을 미칠 가능성이 적습니다.\n\n그러나 이것은 흥미로운(그리고 예상치 못한) 결과를 초래할 수 있습니다.\n\n세 개의 노드와 두 개의 팟이 있는 다음 클러스터를 고려해보세요.\"\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Challenge14bounce_0.png\" />\n\n외부 트래픽에 파드를 노출하는 NodePort 서비스가 있습니다.\n\nNodePort로 요청을 보내면, 해당 트래픽이 동일 노드의 파드 2에 도착할 확률은 얼마인가요?\n\n- 트래픽은 항상 파드 2에 도착합니다\n- 트래픽이 파드 2에 도착할 확률은 50% 입니다\n- 트래픽이 파드 2에 도착할 확률은 33% 입니다\n- 위의 어느 것도 아닙니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 해결책\n\n왜 그런지 이해하려면 Kube-proxy가 노드에 전달 규칙을 설정하는 방식을 살펴봐야 합니다 (예: iptables, ipvs, eBPF 등).\n\nNodePort를 생성하면 클러스터의 각 노드가 30000~32767 범위의 포트를 노출합니다. 30000번 포트로 가정해 보겠습니다.\n\n`노드 IP`:30000으로 curl 요청을 발행하면 무슨 일이 일어날까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nIP 헤더의 대상 IP 주소는 Kubernetes에 서비스가 존재하지 않기 때문에 다시 작성됩니다(그리고 그들은 트래픽을 리디렉션하고 전달하는 규칙의 모음일 뿐입니다).\n\n두 개의 파드 중 하나가 대상으로 선택되고, 트래픽이 해당 팟에 도달할 수 있습니다.\n\n그래서 각 팟은 트래픽을 받을 확률이 50%입니다.\n\n이는 클러스터에 있는 어느 노드에서 클러스터를 받았는지에 관계없이 발생합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만, 토론할 가치가 있는 두 가지 흥미로운 경계 사례가 있습니다:\n\n- 팟이 없는 노드를 만나면 여전히 트래픽이 팟 1 또는 팟 2로 리디렉션됩니다.\n- 팟 2를 포함한 두 번째 노드를 만나면 트래픽이 여전히 다른 팟으로 이동할 수 있습니다.\n\n이 두 경우 모두 현재 노드는 트래픽을 처리하지 않으며, 추가적인 홉이 발생합니다.\n\n이것이 낭비라고 생각할 수 있지만, 이는 타협점입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n특정 테이블 태그를 마크다운 형식으로 변경하시면 추가 hop이 발생하여 포드 간에 우수한 인트라 클러스터 로드 밸런싱을 할 수 있거나 항상 서비스를 구성하여 현재 노드의 포드로 트래픽을 전달하도록 설정할 수 있습니다.\n\n대기시간이 향상되지만 요청을 부분적으로 로드 밸런싱할 수 있기 때문에 다른 포드보다 연결 수가 더 많은 포드를 얻을 수도 있습니다.\n\n이 내용이 마음에 드셨다면 아래 내용도 좋아하실 수 있습니다:\n\n- Learnk8s에서 운영하는 Kubernetes 강좌.\n- 매주 발행하는 Learn Kubernetes Weekly 뉴스레터.\n- 20주 동안 게시한 20가지 Kubernetes 개념 시리즈.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 링크\n\n리소스 할당에 대해 더 자세히 알고 싶다면, 다음 자료를 꼭 확인해보세요:\n\n- [kubernetes service에 핑을 보낼 수 없는 이유](https://medium.com/@danielepolencic/learn-why-you-cant-ping-a-kubernetes-service-dec88b55e1a3)\n- [kubernetes 네트워크 패킷](https://learnk8s.io/kubernetes-network-packets)\n","ogImage":{"url":"/assets/img/2024-06-23-Challenge14bounce_0.png"},"coverImage":"/assets/img/2024-06-23-Challenge14bounce_0.png","tag":["Tech"],"readingTime":4},{"title":"프로젝트 8  Kubernetes에서 삼계층 애플리케이션 배포 하는 방법","description":"","date":"2024-06-23 00:57","slug":"2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes","content":"\n우리 시리즈의 8번째 프로젝트입니다. 계속 따라오세요!\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*ZBUzOwaMkbfPD3iZXxVH1g.gif)\n\n## 3Tier란 무엇인가요?\n\n사실은 시스템을 3부분으로 나누는 것을 말해요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 표현 계층 (티어 1):\n\n  - 웹 사이트를 열 때 보는 것이 바로 표현 계층입니다. 기본적으로 이 계층은 사용자가 직접 상호 작용하는 계층입니다.\n\n- 논리 계층 (티어 2):\n\n  - 이 계층을 뒷단의 두뇌로 상상해보세요. 사용자 인터페이스를 통해 제공하는 정보를 받아 시스템의 규칙에 따라 처리합니다. 예를 들어 쇼핑 웹 사이트의 경우, 이 계층은 물품의 총 가격을 계산하거나 할인을 적용하며 모든 제품이 재고에 있는지 확인합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 데이터 레이어 (티어 3):\n\n- 데이터가 저장되고 검색되는 곳입니다. 시스템의 메모리와 같습니다.\n- 데이터는 데이터베이스, 파일 또는 다른 데이터 저장 시스템에 저장될 수 있습니다.\n- 데이터 레이어는 시스템이 필요로 하는 정보를 관리하고 저장하는 역할을 합니다.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_0.png)\n\n# 완료 단계 →\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Phase 1\n\nEC2 인스턴스, IAM 사용자 및 EC2의 기본 도구 설정하기\n\n# Phase 2\n\n프론트엔드 및 백엔드 이미지 빌드하기\n\n# Phase 3\n\n쿠버네티스\n\n# Phase 4\n\n애플리케이션 로드 밸런서 및 인그레스 설정하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPhase 5 → 모든 것 파괴하기\n\n# Phase 1 → 기본 EC2, IAM 사용자 및 EC2에 기본 도구 설정\n\n## 단계 1. IAM 사용자 생성\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 'iam'을 클릭하세요.\n\n2. 사용자를 클릭하고 사용자 생성을 클릭하세요\n\n   ![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_2.png)\n\n3. 사용자에게 이름을 지정하고 관리 콘솔에 사용자 액세스 제공에 체크하고 IAM 사용자 옵션을 선택하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_3.png)\n\n5. 사용자의 암호를 선택하십시오 → 다음을 클릭하세요.\n\n6. IAM 사용자에 정책을 직접 첨부 → 다음을 클릭하세요.\n\n참고 → 현재는 관리자 액세스를 제공하겠지만 작업 공간에 정책을 첨부할 때 신중하시기 바랍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_4.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_5.png)\n\n사용자를 검토하고 만들기\n\n7. '사용자 만들기'를 클릭하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Project Screenshot 6](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_6.png)\n\n8. If the password file is autogenerated, download it. Otherwise, you can choose whether to download it or not.\n\n![Project Screenshot 7](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_7.png)\n\n9. Click on your IAM user → Security Credentials.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Project Screenshot 8](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_8.png)\n\n10. 스크롤하여 키에 액세스하고 액세스 키를 생성합니다.\n\n![Project Screenshot 9](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_9.png)\n\n11. 옵션 목록에서 aws cli를 선택하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_10.png\" />\n\n12. 다음을 클릭하고 사용자 이름과 암호의 CSV 파일을 다운로드하세요\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_11.png\" />\n\n## 단계 2. 모든 작업을 수행하는 기본 EC2를 시작합니다 →\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- AWS 콘솔을 열고 EC2로 이동한 다음 EC2 시작을 클릭하세요\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_12.png)\n\n2. 연결을 클릭하고 다음 명령을 실행하세요\n\n```bash\nsudo su\napt update\nmkdir 3-tier\ncd 3-tier\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 깃허브에서 코드를 가져오려면 git clone을 사용하세요\n\n```js\ngit clone https://github.com/Aakibgithuber/Three-tier-Application-Deployment-.git\n```\n\n4. ls를 실행하여 레포지토리 안에 무엇이 있는지 확인하세요\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_13.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 3 → aws cli, 도커, kubectl 및 eksctl 설정\n\n## 1. aws cli 구성\n\nAWS CLI (Amazon Web Services Command Line Interface)는 명령어를 사용하여 AWS 서비스와 상호 작용할 수 있는 도구입니다.\n\n- 아래 명령어를 실행하여 aws cli를 설치합니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 먼저 다음 명령어로 AWS CLI를 설치해 주세요.\n\nsnap install aws-cli --classic\n\n2. 이제 AWS를 구성해야 합니다. 아래 명령어를 사용해 구성해 주세요.\n\naws configure\n\n3. 접근 키와 비밀 키를 요청할 것입니다. 이제 앞서 다운로드한 CSV 파일을 열어서 액세스 키와 비밀 키를 복사해 주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_14.png\" />\n\n4. 모든 것을 그대로 유지하고 Enter 키를 누르세요.\n\n지금까지 AWS CLI를 설정하셨습니다. 이제 Docker를 설정하세요.\n\n## 2. Docker 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다음 명령어를 실행해주세요\n\n```js\napt install docker.io\nusermod -aG docker $USER # 사용자 이름으로 변경해주세요. 예: 'ubuntu'\nnewgrp docker\nsudo chmod 777 /var/run/docker.sock\nwhich docker\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_15.png)\n\n## 3. kubectl 설정하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 Kubernetes 클러스터를 관리하고 상호 작용하는 데 사용되는 명령 줄 도구입니다.\n\n- kubectl을 설치하려면 다음 명령을 실행하십시오.\n\n```js\nsnap install kubectl --classic\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_16.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4. eksctl 설정\n\n이것은 Amazon EKS (Elastic Kubernetes Service) 클러스터를 관리하는 데 사용되는 명령줄 도구입니다.\n\n- eksctl 도구를 설치하려면 다음 명령을 실행하십시오\n\n```js\ncurl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\neksctl version\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_17.png\" />\n\n# Phase 2 → 프론트엔드 및 백엔드 이미지 구축\n\n## 단계 1 → Elastic Container Registry (ECR) 설정\n\n도커 이미지를 저장하는 도커허브와 유사합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- AWS 콘솔로 이동해서 ECR을 검색해주세요.\n- 프론트엔드를 위한 저장소를 만들고 가시성 설정을 \"퍼블릭\"으로 설정해주세요.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_18.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_19.png)\n\n3. 백엔드 저장소 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_20.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_21.png)\n\n## 단계 2 → 프론트엔드 설정\n\n- 터미널에서 프론트엔드 디렉토리로 이동하고 ls 명령어를 실행하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_22.png)\n\n2. Amazon ECR 저장소로 이동하고 푸시 명령 보기를 클릭합니다\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_23.png)\n\n3. 위 명령을 하나씩 실행하여 프론트엔드 이미지를 빌드하고 ECR 저장소에 푸시합니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/l0l7e4u1\ndocker build -t 3-tier-frontend .\ndocker tag 3-tier-frontend:latest public.ecr.aws/l0l7e4u1/3-tier-frontend:latest\ndocker push public.ecr.aws/l0l7e4u1/3-tier-frontend:latest\n```\n\n![Project Image 24](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_24.png)\n\n![Project Image 25](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_25.png)\n\n4. 이미지에서 컨테이너를 실행해 보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 이미지 목록에서 이미지 이름을 복사하세요.\n도커 실행 -d -p 3000:3000 3-tier-frontend:latest\n```\n\n![이미지1](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_26.png)\n\n프론트엔드가 설정되었고 애플리케이션이 실행 중입니다. 애플리케이션을 확인하려면 브라우저에서 해당 주소로 이동하세요 →public-ip:3000\n\n![이미지2](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_27.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 3 → 백엔드 설정\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_28.png)\n\n- 이제 백엔드 디렉토리로 이동하여 백엔드를 설정하세요.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_29.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. ECR 리포지토리로 이동하여 백엔드 리포지토리의 푸시 명령을 보기 위해 클릭하세요\n\n![백엔드 리포지토리 푸시 명령](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_30.png)\n\n3. 위 명령을 단계별로 터미널에서 실행해주세요\n\n```js\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/l0l7e4u1\ndocker build -t 3-tier-backend .\ndocker tag 3-tier-backend:latest public.ecr.aws/l0l7e4u1/3-tier-backend:latest\ndocker push public.ecr.aws/l0l7e4u1/3-tier-backend:latest\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_31.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_32.png)\n\n이제 백엔드 이미지가 성공적으로 빌드되었고, 우리가 탄력적인 쿠버네티스 서비스를 생성할 때 사용한 Elastic Container Registry에 푸시되었습니다.\n\n# Phase 3 Kubernetes\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 배포란 무엇인가요?\n\n- 공장을 상상해보세요: 배포는 여러분의 소프트웨어 응용 프로그램의 사본을 생성하고 관리하는 공장으로 생각해보세요.\n- 여러 복제본: 공장이 여러 개의 동일한 항목을 생성할 수 있는 것처럼, Kubernetes의 배포는 응용 프로그램의 여러 사본(복제본)을 생성하고 다룰 수 있습니다.\n- 간편한 업데이트: 응용 프로그램을 변경하거나 업데이트하려면, 배포 시스템이 그것을 원활하게 처리할 수 있습니다. 마치 공장에서 부품을 교체하면서 생산을 멈추지 않는 것처럼요.\n\n2. 서비스란 무엇인가요?\n\n- 수신 데스크를 상상해보세요: Kubernetes의 서비스를 건물의 수신 데스크처럼 생각해보세요.\n- 중앙 연락처: 서비스는 응용 프로그램을 위한 중앙 연락처를 제공합니다. 직접 각 응용 프로그램을 찾는 대신 시스템의 다른 부분들이 서비스와 대화하고, 서비스가 올바른 응용 프로그램을 찾는 방법을 알고 있습니다.\n- 안정적인 주소: 수신 데스크에 일정한 주소가 있는 것처럼, 서비스에도 시스템의 다른 부분들이 응용 프로그램과 통신하기 위해 사용할 수 있는 안정적인 주소가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. Namespace란 무엇인가요?\n\n- 이것은 쿠버네티스 내에서 당신이 어플리케이션을 조직하고 실행할 수 있는 레이블이 붙은 섹션과 같습니다. 각 네임스페이스는 당신의 어플리케이션이 서로 방해하지 않고 자신의 일을 할 수 있는 울타리 치여진 영역과 같습니다.\n- 좀 더 간단하게 말하면, 쿠버네티스에서의 네임스페이스는 다른 프로젝트나 어플리케이션을 분리하고 조직화하며, 쿠버네티스 클러스터의 분주한 환경에서 쉽게 관리할 수 있도록 하는 방법입니다.\n\n## Step 1 → EKS 클러스터 설정하고 네임스페이스 생성하기\n\n- 다음 명령어를 실행하여 EKS 클러스터를 설정하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\neksctl create cluster --name three-tier-cluster --region us-east-1 --node-type t2.medium --nodes-min 2 --nodes-max 2\naws eks update-kubeconfig --region us-east-1 --name three-tier-cluster\nkubectl get nodes\n```\n\n2. 클러스터를 생성하는 데 15~20분이 소요됩니다.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_33.png)\n\n3. AWS 콘솔에서 AWS CloudFormation을 검색하여 EKS 클러스터 생성 중 발생하는 이벤트를 확인하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 명령어를 사용하여 Namesapce를 생성하세요.\n\n```shell\nkubectl create namespace workshop\nkubectl config set-context --current --namespace workshop\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_36.png\" />\n\n## 단계 2→ 프론트엔드를 위한 배포 및 서비스 생성하기\n\n- k8s_manifests 디렉토리로 이동하면 프론트엔드를 위한 배포 및 서비스 파일을 찾을 수 있습니다.\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_37.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. frontend-deployment.yaml 파일을 편집해야 합니다.\n\n3. 바꿔야 할 것은 이미지 이름입니다.\n\n![image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_38.png)\n\n4. 따라서 ECR 레포지토리로 이동하여 프론트엔드 레포지토리를 선택한 후 \"View public listing\"을 클릭하여 이미지 이름을 복사하고 frontend-deployment.yaml 파일에 붙여넣으세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 다음 명령어를 실행하여 프론트엔드용 배포 및 서비스를 생성하세요.\n\n```js\nkubectl apply -f frontend-deployment.yaml\nkubectl apply -f frontend-service.yaml\n```\n\n## 단계 3→ 백엔드용 배포 및 서비스 생성\n\n- 동일한 폴더에 백엔드 배포 및 서비스인 backend-deployment.yaml 및 backend-service.yaml 파일이 있습니다.\n- backend-deployment.yaml 파일을 편집해야 합니다.\n- 변경해야 할 것은 이미지 이름뿐입니다.\n- 따라서 ECR 저장소로 이동하여 백엔드 저장소를 선택한 다음, \"View Public Listing\"을 클릭하여 이미지 이름을 복사한 후 backend-deployment.yaml 파일 안에 붙여넣으세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 다음 명령을 실행하여 배포 및 백엔드 서비스를 생성하십시오.\n\n```js\nkubectl apply -f backend-deployment.yaml\nkubectl apply -f backend-service.yaml\nkubectl get pods -n workshop\n```\n\n이제 우리의 2계층이 준비되었습니다. 즉, 프론트엔드와 백엔드가 설정되었습니다. 이제 세 번째 계층을 설정해 봅시다.\n\n## 단계 4 → 데이터베이스 계층 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 배포, 서비스 및 시크릿 매니페스트를 저장하는 몽고 폴더를 찾아주세요.\n\n2. 아래 명령어를 실행하여 데이터베이스 티어를 설정하세요.\n\n```js\nkubectl apply -f .\nkubectl get all\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_39.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 모든 세 티어가 준비되었습니다. 하지만 이들을 어떻게 액세스할까요? 클러스터로 외부 트래픽을 전송하기 위해 애플리케이션 로드 밸런서를 생성해야 하고, 내부의 3개 티어 사이를 연결하기 위해 인그레스를 만들어야 합니다.\n\n# 단계 4 → 애플리케이션 로드 밸런서와 인그레스 설정\n\n외부 트래픽을 클러스터로 전송하기 위해 애플리케이션 로드 밸런서를 생성하고, 내부의 3개 티어 사이를 연결할 인그레스를 만들어야 합니다.\n\n## 단계 1 → AWS 로드 밸런서 설정; 설치 및 EKS 클러스터에 연결\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 아래 명령은 ALB의 IAM 정책을 가져옵니다.\n\n```js\ncurl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_40.png)\n\n2. 이 명령어는 첫 번째 명령어에서 설정한 `iam_policy.json` 파일을 사용하여 AWS 계정에서 IAM 정책을 생성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\naws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json\n```\n\n![Image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_41.png)\n\n3. 이 명령은 로드 밸런서 정책을 EKS 클러스터에 적용하여 정책에 따라 EKS 클러스터가 로드 밸런서와 함께 작동하도록 합니다.\n\n```js\neksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=three-tier-cluster --approve\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 명령어는 클러스터에 서비스 계정을 생성하고 추가하여 클러스터가 로드 밸런서 서비스와 작업할 수 있도록합니다.\n\n아래 명령어에서 AWS 계정 번호를 꼭 변경해야 합니다. 그렇지 않으면 작동하지 않습니다.\n\n```js\neksctl create iamserviceaccount --cluster=three-tier-cluster --namespace=kube-system --name=aws-load-balancer-controller --role-name AmazonEKSLoadBalancerControllerRole --attach-policy-arn=arn:aws:iam::767397866747:policy/AWSLoadBalancerControllerIAMPolicy --approve --region=us-east-1\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_43.png\" />\n\n## 모든 정책이 첨부되었습니다. 로드 밸런서를 배포해 봅시다.\n\n5. 이를 위해 helm을 설치해야 합니다. Helm은 쿠버네티스를 사용할 때 소프트웨어를 쉽게 운반하고 관리하는 데 도움이 되는 특별한 도구입니다. 쿠버네티스는 애플리케이션을 실행하기 위한 큰 놀이터와 같습니다.\n\n```js\nsudo snap install helm --classic\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n6. 이제 로드 밸런서를 위해 eks 리포지토리에서 미리 작성된 특정 manifest 파일을 추가해야 해요. helm을 사용하여 다음 명령을 실행해주세요.\n\n```js\nhelm repo add eks https://aws.github.io/eks-charts\n```\n\n7. helm을 사용하여 eks 리포지토리를 업데이트해주세요.\n\n```js\nhelm repo update eks\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n8. EKS 클러스터에 로드 밸런서 컨트롤러를 설치해주세요\n\n```js\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=my-cluster --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\nkubectl get deployment -n kube-system aws-load-balancer-controller\n```\n\n![Image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_44.png)\n\n이제 로드 밸런서가 작동되니 내부 라우팅을 위해 인그레스를 설정합시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Step 2 → 내부 라우팅을 위한 Ingress 설정\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_45.png)\n\n- full_stack_lb.yaml 파일을 찾습니다\n\n```js\nkubectl apply -f full_stack_lb.yaml\nkubectl get ing -n workshop\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_46.png\" />\n\n2. 웹 브라우저로 이동하여 위의 DNS 주소를 붙여넣으세요\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_47.png\" />\n\n축하합니다!! 여러분의 애플리케이션이 로드 밸런서 인그레스를 통해 접근 가능합니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 5 단계 → 모두 삭제하기\n\n- 현재 폴더에서 다음을 실행하세요\n\n```js\nkubectl delete -f .\n```\n\n2. 데이터베이스 계층을 삭제하려면 몽고 폴더로 이동하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\nkubectl delete -f .\n```\n\n3. 클러스터 및 클라우드 형성 스택을 삭제하세요.\n\n```yaml\neksctl delete cluster --name three-tier-cluster --region us-east-1\naws cloudformation delete-stack --stack-name eksctl-three-tier-cluster-cluster\n```\n\n4. AWS의 클라우드 형성 콘솔에서 모든 변경 사항을 확인할 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Project Screenshot](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_48.png)\n\n## 지금 모든 것이 삭제되었으니 AWS 요금을 줄여줘서 고마워\n\n## 오늘은 여기까지 프로젝트가 완료되었어요. 만약 여기까지 오셨다면 박수를 치세요. 그리고 LinkedIn에서 저를 팔로우해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제 깃허브 계정을 팔로우해주세요!\n","ogImage":{"url":"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_0.png"},"coverImage":"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_0.png","tag":["Tech"],"readingTime":28}],"page":"25","totalPageCount":120,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}