{"pageProps":{"posts":[{"title":"당신의 안드로이드 앱에 다국어 지원다국어 추가하기","description":"","date":"2024-05-20 17:37","slug":"2024-05-20-AddMultilingualsupportMultipleLanguagestoyourAndroidApp","content":"\n![다국어 지원 이미지](/assets/img/2024-05-20-AddMultilingualsupportMultipleLanguagestoyourAndroidApp_0.png)\n\n여러 언어를 지원하는 것은 애플리케이션을 확장하고 대중에 도달하는 데 중요합니다. 인도의 약 25%와 유럽의 64%의 작업 성인 인구가 다국어를 구사하며 미국도 다국어 구사자가 약 194% 증가했습니다. (출처)\n\n또한, 소비자의 65% 이상이 선호하는 언어로 콘텐츠를 소비하는 것으로 나타나므로 이는 Amazon, WhatsApp, Facebook 등 대부분의 선도적인 애플리케이션에서 이미 제공되는 중요한 기능으로 고려되어야 합니다.\n\n그러니 다국어 세계를 위해 함께 만들어봅시다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. 문자열 리소스를 체계화하세요.\n\n코드에서 하드코딩된 문자열 값을 사용하지 마세요.\n\n올바르게\n\n```js\nText(stringResources(R.string.follow_me));\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nUsing strings.xml, we will have a common place for all our string resources and we can then support multiple languages by adding more strings.xml files.\n\n## 2. Add Multiple Languages\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이미 모든 문자열 리소스를 저장할 수 있는 곳이 있습니다. 이제 모든 문자열 리소스를 다른 지원되는 언어로 번역하기만 하면 됩니다.\n\n```js\n//힌디어 문자열 리소스 파일 예제\n<resources>\n  <string name=\"subscribe_to_sagar_malhotra\">\n    सागर मल्होत्रा की सदस्यता लें\n  </string>\n  <string name=\"language\">हिन्दी</string>\n</resources>\n```\n\n이 작업을 수행하기 위해 \"AndroidLocalize\"라는 Android Studio 플러그인을 사용하고 있습니다.\n\n![이미지](/assets/img/2024-05-20-AddMultilingualsupportMultipleLanguagestoyourAndroidApp_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3. 언어 변경 트리거\n\n어떤 UI를 사용하더라도(여기서는 ExposedDropDownMenu), 애플리케이션의 로캘을 변경했음을 OS에 알리기 위해 onClick 이벤트를 트리거해야 합니다. 이렇게 하면 OS도 특정 언어의 strings.xml 파일로 전환할 수 있습니다.\n\n```js\nonClick = {\n    // 사용자가 선택한 로캘에 따라 앱 로캘 설정\n    AppCompatDelegate.setApplicationLocales(\n        LocaleListCompat.forLanguageTags(\n            \"hi\"// 힌디어를 위한 ISO 코드\n        )\n    )\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n귀하는 해당 언어의 ISO-639 코드를 전달하여 OS에 언어 환경 설정 변경을 알릴 필요가 있습니다.\n\n## 3.1 문제 해결\n\n현재 이 방법은 AppCompatActivity에만 작동하므로 귀하의 애플리케이션에 맞지 않을 수도 있습니다. 특정 Activity에 ComponentActivity를 확장하고 있는지 확인하십시오.\n\n```js\nclass MainActivity : AppCompatActivity() { ... }\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAppCompatActivity를 확장한 후에도, 특정 활동을 위해 지원되는 테마를 변경해야 합니다.\n\n```js\n<style\n  name=\"Theme.MultilingualApp\"\n  parent=\"Theme.AppCompat.Light.NoActionBar\"\n/>\n```\n\n## 4. 로케일 설정 저장\n\nAndroid 12 이하 버전에서는 선택한 언어 환경 값을 수동으로 저장하거나 AndroidX가 로케일 환경을 스스로 처리하도록 AndroidManifest에서 이 구성을 사용해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```bash\n<service//Inside application tag\n    android:name=\"androidx.appcompat.app.AppLocalesMetadataHolderService\"\n    android:enabled=\"false\"\n    android:exported=\"false\">\n    <meta-data\n        android:name=\"autoStoreLocales\"\n        android:value=\"true\" />\n</service>\n```\n\n## 5. Android OS Per-App Language Preferences\n\nIn Android 13 and above, the Android OS also supports changing the Per-App Language preference from system settings.\n\n<img src=\"/assets/img/2024-05-20-AddMultilingualsupportMultipleLanguagestoyourAndroidApp_2.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n귀하의 응용 프로그램이 여러 언어도 지원한다는 것을 운영 체계에 알리려면 AndroidManifest 파일에 필요한 구성을 추가하십시오.\n\n```js\n// application 태그에 다음을 추가하세요\nandroid: localeConfig = \"@xml/locale_config\";\n```\n\nlocal_config.xml 파일에 모든 지원하는 언어를 정의하세요.\n\n```js\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale-config xmlns:android=\"http://schemas.android.com/apk/res/android\">\n    <locale android:name=\"en\" />\n    <locale android:name=\"gu\" />\n    <locale android:name=\"hi\" />\n    <locale android:name=\"ar-AE\" />\n</locale-config>\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 앱에서 시스템 설정에서 언어를 변경하는 것도 지원하게 됩니다.\n\n## 6. 활동 재생성 피하기\n\n앱 언어를 변경할 때 활동이 재생성되는 것을 눈치챌 수 있습니다. 이는 로캘을 변경하는 것도 구성 변경의 한 종류이기 때문에 기본적으로 활동이 구성 변경이 발생할 때마다 재생성됩니다.\n\nAndroidManifest를 사용하여 이 기본 동작을 방지할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n// 특정 활동 태그에 다음을 추가하십시오\nandroid:configChanges=\"layoutDirection|locale\"\n\n## 비디오:\n\n이 기능이 보다 많은 관객에 도움이 되기를 바라며, 더 많은 유용한 콘텐츠를 위해 저를 팔로우하시기 바랍니다.\n","ogImage":{"url":"/assets/img/2024-05-20-AddMultilingualsupportMultipleLanguagestoyourAndroidApp_0.png"},"coverImage":"/assets/img/2024-05-20-AddMultilingualsupportMultipleLanguagestoyourAndroidApp_0.png","tag":["Tech"],"readingTime":7},{"title":"디자인 시스템 워크플로우를 Kelp 플러그인으로 Android Studio에서 강화해보세요 ","description":"","date":"2024-05-20 17:35","slug":"2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio","content":"\n![Kelp Plugin for Android Studio](/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_0.png)\n\n안녕하세요! 안드로이드 개발자 여러분! 저희는 항상 업무 흐름을 최적화하고 생산성을 향상시킬 도구를 찾고 있어요. 그리고 여기에 Kelp이 등장합니다. 강력한 안드로이드 스튜디오 플러그인인 Kelp은 사용자 정의 디자인 시스템을 안드로이드 스튜디오에 원활하게 통합하여 UI 개발을 빠르고 쉽게 만들어주는 다양한 기능을 제공해요.\n\n## 왜 Kelp을 사용해야 할까요? 🤔\n\nKelp은 안드로이드 스튜디오 내에서 직관적이고 자동화된 지원을 통해 사용자 정의 디자인 시스템 사용 시 발생하는 일반적인 문제점을 해결해요. 아이콘, 색상 또는 구성 요소 함수를 다루더라도 Kelp은 가시성과 접근성을 향상시켜 주어요. 그렇게 함으로써 여러분은 뛰어난 앱을 만드는 데 집중할 수 있게 되어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 주요 기능\n\n## 🔧 컴포넌트 기능용 사용자 정의 아이콘\n\nKelp를 통해 디자인 시스템 컴포넌트 기능에 사용자 정의 아이콘을 설정할 수 있습니다. 이 아이콘은 코드 완성 드롭다운에 표시되며, R.drawable 리소스가 표시되는 방식과 유사합니다. 이 시각적 지원을 통해 프로젝트 전반에 걸쳐 컴포넌트를 식별하고 일관되게 사용하는 것이 더 쉬워집니다.\n\n![Kelp 플러그인을 사용하여 디자인 시스템 작업 흐름 향상](/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 🎨 디자인 시스템 아이콘\n\nKelp를 사용하면 디자인 시스템 아이콘을 코드 완성 드롭다운 및 거터에서 네이티브 안드로이드 drawable 리소스처럼 렌더링할 수 있습니다. 이 기능을 통해 쉽게 적절한 아이콘을 찾아 전환 없이 바로 사용할 수 있습니다.\n\n![Kelp Plugin](/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_2.png)\n\n## 🌈 색상 미리보기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKelp는 코드 완성 드롭다운 및 거터에서 색상 미리보기를 통해 디자인 시스템의 색상 팔레트를 살아있게 만듭니다. 이 기능은 기본 R.color 리소스를 반영하여 올바른 색상을 적용하는 프로세스를 간단하게하는 즉각적인 시각 참조를 제공합니다.\n\n![Kelp Plugin](/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_3.png)\n\n## 📱 데모 앱 통합\n\n디자인 시스템 구성 요소를 쇼케이스하는 데모 앱 설치 및 탐색이 이제 더욱 간편해졌습니다. Kelp는 APK 설치를 용이하게 하고 코드에서 직접 특정 구성 요소 페이지를 열 수 있는 의도 작업을 제공하여 개발 프로세스를 향상시킵니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_4.png\" />\n\n## 🖼️ KDoc Image Rendering\n\nKelp의 뛰어난 기능 중 하나는 Android Studio의 현재 제한 사항에도 불구하고 KDoc에서 이미지를 렌더링할 수 있는 능력입니다. 이 기능은 컴포넌트에 대한 내용을 인라인 이미지 참조를 허용하여 문서 작성을 향상시켜주어 더 나은 이해와 활용을 돕습니다.\n\n<img src=\"/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_5.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## ⌨️ 라이브 템플릿\n\n맞춤 설정 가능한 라이브 템플릿으로 코딩 속도를 높이세요. 이러한 템플릿은 사용자의 디자인 시스템 코드에서 자주 사용하는 코드 조각을 포함하도록 맞춤 설정할 수 있으며, 개발 속도를 더 높이기 위해 코드 완성 팝업을 자동으로 트리거합니다.\n\n![image](/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_6.png)\n\n## 🧩 사용자 정의\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n켈프의 진정한 힘은 구성 가능성에 있습니다. config.json 파일을 수정하여 플러그인을 프로젝트의 고유한 요구 사항에 맞게 조정할 수 있습니다. 이 JSON 기반 구성은 함수 접두사부터 색상 및 아이콘 렌더링에 이르기까지 세부적인 사용자 정의를 지원합니다.\n\n게다가, 이것은 파일이기 때문에 Git에 저장하여 팀 전체에서 공유할 수 있어 일관성을 유지하고 각 사용자가 Android Studio의 설정 UI를 수동으로 구성할 필요성을 제거할 수 있습니다.\n\n다음은 주석이 포함된 전체 구성 파일입니다:\n\n```js\n{\n  // 이러한 기능 중 일부를 비활성화하려면\n  // json 파일에 해당 섹션을 포함하지 않기만 하면 됩니다.\n\n  // 디자인 시스템 구성 요소의 기본 아이콘을 사용자 정의 아이콘으로 교체\n  // 코드 완성에서 사용자 정의 아이콘이 표시됨\n  // 사용자 정의 아이콘은\n  // 1. svg여야 함\n  // 2. 크기 - 40x40\n  // 3. 여기에 위치해야 함: /.idea/kelp/dsComponentFunIcon.svg\n  // 4. 선택 사항으로 다크 버전 추가 가능:\n  // /.idea/kelp/dsComponentFunIcon_dark.svg\n  \"componentFunHighlighting\": {\n    // 사용자 정의 아이콘이이 패키지의 모든 함수에 추가됨\n    \"functionFqnPrefix\": \"com.your.designsystem.package.components.\",\n    \"functionSimpleNamePrefix\": \"Ds\" // 선택 사항\n  },\n\n  // 코드 완성 및 구두에 디자인 시스템 색상 표시\n  // 일반적인 안드로이드 자원과 비슷합니다.\n  \"colorPreview\": {\n    \"codeCompletionEnabled\": true,\n    \"gutterEnabled\": true,\n    // 선택 사항, 열거형 클래스에서 색상 토큰\n    \"enumColorTokensEnabled\": true,\n  },\n\n  // 코드 완성 및 구두에 디자인 시스템 아이콘 렌더링\n  // 일반적인 안드로이드 자원과 비슷합니다.\n  \"iconsRendering\": {\n    \"codeCompletionEnabled\": true,\n    \"gutterEnabled\": true,\n    // 아이콘을 반환하고\n    // 이름이 아이콘이라는 많은 속성을 가진 클래스\n    \"containerClassName\": \"com.your.designsystem.package.DsIcons\",\n\n    // 선택 사항: 아이콘이 아닌 속성을 필터링\n    \"propertyNameFilter\": {\n      // 선택 사항:이 접두사가있는 속성만\n      // 아이콘으로 간주\n      \"startsWith\": [\"ic_\"],\n      // 선택 사항:이 접두사가있는 모든 속성은 건너뜀\n      \"doesNotStartWith\": [\"allIconsAsList\", \"otherProperty\"]\n    },\n\n    // 속성 이름을 drawable 자원 이름으로 매핑\n    \"propertyToResourceMapper\": {\n      \"addPrefix\": \"ic_\", // 선택 사항\n      \"convertToSnakeCase\": true // 선택 사항; 예: \"AddAccount\" -> \"add_account\"\n    }\n  },\n\n  // 의도 작업을 통해 데모 앱에서 구성 요소 페이지 열기\n  \"demoApp\": {\n    // 선택 사항: 의도 작업의 사용자 지정 이름\n    \"intentionName\": \"🚀 Open in MY CUSTOM design system demo app\",\n    \"functionFqnPrefix\": \"com.your.designsystem.package.components.\",\n    \"functionSimpleNamePrefix\": \"Ds\", // 선택 사항\n    // 데모 앱의 패키지 이름\n    \"appPackageName\": \"com.your.designsystem.package.demo\",\n    // 데모 앱에서 구성 요소 페이지를 열기 위해 사용되는 딥 링크.\n    // DS_COMPONENT_FQN_DEEPLINK_PLACEHOLDER가로 교체됩니다\n    // 완전히 정규화 된 이름, 예 : com.your.designsystem.package.components.Badge\n    \"componentDeeplink\": \"yourscheme://component/DS_COMPONENT_FQN_DEEPLINK_PLACEHOLDER\",\n\n    // 선택 사항\n    // 데모 앱 (쇼케이스 앱)의 apk 파일을 설치 (미설치되어 있으면)합니다.\n\n    // 데모 앱 apk는 여기에 있어야합니다.\n    // 이 이름 :/.idea/kelp/demoApp-VERSION_NAME.apk\n    // 예 :/.idea/kelp/demoApp-0.12.0.apk\n    // 플러그인은 최신 버전을 얻습니다.\n    // apk 파일 이름에서 (예 : 0.12.0).\n    // 앱이 설치되어 있지 않거나 설치되어 있지만 하위 버전인 경우\n    // 플러그인은 장치에 apk를 설치합니다.\n    \"apkInstallation\": true\n  },\n\n  // IDE에 라이브 템플릿 설치\n  // \"MaterialTheme.colors.\"와 같은\n  // 자주 사용하는 코드 작성에 유용합니다.\n  // 완성 후 코드 완성을 엽니다\n  // $CODE_COMPLETION$ 자리에, 더 많은 노력을 절약합니다.\n  \"liveTemplates\": [\n    {\n      \"abbreviation\": \"dt\",\n      \"text\": \"com.your.designsystem.DsTheme.$CODE_COMPLETION$\",\n      \"description\": \"\\\"DsTheme.\\\" 쓰기\"\n    },\n    {\n      \"abbreviation\": \"dtc\",\n      \"text\": \"com.your.designsystem.DsTheme.colors.$CODE_COMPLETION$\",\n      \"description\": \"\\\"DsTheme.colors\\\" 쓰기\"\n    },\n    {\n      \"abbreviation\": \"dtt\",\n      \"text\": \"com.your.designsystem.DsTheme.typography.$CODE_COMPLETION$\",\n      \"description\": \"\\\"DsTheme.typography\\\" 쓰기\"\n    },\n    {\n      \"abbreviation\": \"dtt\",\n      \"text\": \"com.your.designsystem.DsTheme.icons.$CODE_COMPLETION$\",\n      \"description\": \"\\\"DsTheme.icons\\\" 쓰기\"\n    }\n  ]\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 🛠️ 설치\n\nKelp를 사용하여 시작하려면:\n\n- Android Studio Koala | 2024.1.1 Canary 3 또는 그 이상 버전을 사용하는지 확인합니다.\n- Kelp GitHub 저장소에서 최신 릴리스를 다운로드하여 설정/환경설정에서 수동으로 설치합니다. `플러그인` ⚙️ ` 디스크에서 플러그인 설치`...\n- 원하는 경우, 모든 팀원이 플러그인을 사용하고 있는지 확실하게 하기 위해 externalDependencies.xml 파일을 생성하여 팀에 알립니다.\n- 프로젝트에 Kelp를 구성하기 위해 config.json 파일을 생성하고 필요에 따라 사용자 정의합니다.\n\n## 🚀 개발 효율성을 높이세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKelp은 안드로이드 스튜디오 내에서 디자인 시스템과 상호 작용하는 방식을 변화시킵니다. 시각적 단서와 디자인 리소스에 간편하게 접근할 수 있도록 IDE에 직접 통합함으로써 추측을 제거하고 적절한 구성 요소를 찾는 데 소요되는 시간을 줄입니다.\n\nKelp를 사용하면 일관성을 보장하고 코드 가독성을 향상시키며 궁극적으로 더 나은 제품을 빠르게 제공할 수 있습니다. 지금 Kelp를 개발 워크플로에 통합하고 그 차이를 경험해보세요.\n\n더 자세한 안내와 예제는 공식 Kelp 저장소를 방문해주세요.\n\nKelp의 잠재력에 흥분하신다면 GitHub에서 ⭐️을 부탁드립니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테이블 태그를 Markdown 형식으로 변경해 주세요.\n","ogImage":{"url":"/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_0.png"},"coverImage":"/assets/img/2024-05-20-EnhanceYourDesignSystemWorkflowwithKelpPluginforAndroidStudio_0.png","tag":["Tech"],"readingTime":9},{"title":"매뉴얼에서 선언적 방식으로 빠르게 성장하는 회사에서의 Terraform과 IaC","description":"","date":"2024-05-20 17:31","slug":"2024-05-20-FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany","content":"\n우리의 여정은 수동으로 인프라 리소스를 프로비저닝하고 유지하는 것에서 완전히 선언적 인프라 코드(IaC)로 이동하는 것이었습니다.\n\n작성자: 워커블의 시니어 사이트 신뢰성 엔지니어 테오도어 커키리스, 워커블의 시니어 사이트 신뢰성 엔지니어 콘스탄티노스 루소프로스\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*VLL84MCp0Wo4Ec3zwuoVvA.gif)\n\n# 요약\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 Workable에서의 인프라스트럭처 코드 (IaC) 여정을 논의할 것입니다. 이미 IaC에 익숙하신 분들 중에서 Terraform을 선호하는 도구로 채택한 분들은 이미 코드를 DRY하게, 간단하고 유지 보수가 쉽도록 구성하는 데 오는 도전과 불편함을 이해하고 계실 겁니다. (다름이 아니라 버그나 오타 하나로 생산 리소스가 중대한 문제를 일으킬 수 있습니다.) 동시에 확장 가능하고 유연하며 변화하는 빠르게 성장하는 회사의 요구 사항을 충족하기 위해 확장 가능하고 유연하며 확장 가능하게 만들기 위한 도구로 Terraform을 채택한 고객이 이미 이러한 도전과 불편함을 이해하고 있을 겁니다.\n\n이 기사는 여러 해 동안의 IaC의 발전을 제공하지만, 만약 귀하가 귀사의 IaC를 구조화하는 방법에 대한 제안을 찾는 중이라면 마지막 섹션으로 건너뛰어도 됩니다. 해당 섹션에서는 현재 아키텍처를 설명하며, DRY(반복하지 마세요), 유연하며 우리 팀이 지속적으로 성장하는 인프라를 효과적으로 관리하는 데 효율적이고 확신을 주는 아키텍처로 믿고 있습니다.\n\nWorkable의 엔지니어링 발전 일부를 간단히 소개하기 위해 언급되었지만, 이것은 완전한 여정은 아닙니다. 더 알고 싶으시다면 우리의 엔지니어링 부사장들이 Voxxed Days에서 하는 훌륭한 발표를 시청해 보세요.\n\n2012년 Workable이 시작됐을 때, 아주 소수의 엔지니어 팀은 최초 세대에서 작업했습니다. 이는 매우 적은 인프라 리소스를 요구하는 모놀리스 였습니다. 우리는 주로 PostgreSQL을 주요 지속성 계층으로, Solr를 텍스트 검색, Redis를 분산 캐싱으로 사용했습니다. 코드는 Heroku에 배포되었습니다. Heroku는 통합된 데이터 서비스를 제공하고 현대 어플리케이션을 배포하고 실행하기 위한 강력한 생태계를 제공하는 컨테이너 기반 플랫폼 서비스였습니다. 이를 통해 개발자들은 생산용 용량에서 인프라 관리의 복잡성 없이도 응용 프로그램 로직에 집중할 수 있었습니다.\n\n제품은 연도가 흘러가면서 계속 성장했고, 2016년으로 빨리 앞으로 가면 이미 엔지니어링 팀은 사용자 경험을 향상시키는 보다 다양한 기능을 제공하기 위해 몇 개의 마이크로서비스를 도입했습니다. 그러나 이러한 마이크로서비스는 모놀리스의 스택과 도메인에 잘 들어맞지 않았으며 더 많은 마이크로서비스가 추가될 것이었습니다. 코드는 여전히 Heroku에 배포되었지만 제품은 상당히 커졌습니다. 우리는 이미 외부 제공업체의 서비스 (클라우드 저장소, 데이터베이스 등)를 사용하기 시작했고, 인프라 유지 및 모니터링은 엔지니어링 팀 간의 공동 책임으로 남아 있었습니다. 이때 SRE 팀이 형성되었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 처음에 인프라를 관리하고 어떻게 관리해야 할지 평가하는 동안 모든 것을 수동으로 프로비저닝했어요. 회사로서 성장하던 2017년, 우리는 인프라를 프로비저닝, 구성 및 관리하는 더 효율적인 방법이 필요한 지점에 이르렀어요. 반복적이고 복잡해지며 실수하기 쉬운 수동 작업에서 벗어나 우리의 인프라 구성을 단순화하고 표준화하며 최소한의 노력으로 확장할 수 있는 프로세스가 필요했어요.\n\n이것이 테라폼과 함께 하는 이야기: 우리 회사와 함께 성장하고 변화하는 인프라스트럭처를 정의하는 여정입니다.\n\n# 1G IaC — 테라폼 도입\n\n인프라스트럭처 코드 (IaC) 도구는 설정 파일을 사용하여 인프라를 관리할 수 있게 해줍니다. 이러한 도구들은 자원 구성을 정의하여 버전 관리, 재사용, 공유가 가능하게 함으로써 안전하고 일관되며 반복 가능한 방식으로 인프라를 구축, 수정 및 유지할 수 있도록 도와줍니다. 테라폼은 이러한 도구 중 하나로 HashiCorp에서 개발했어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테라폼은 인간이 읽기 쉬운 구성 파일을 사용하여 리소스와 인프라를 정의할 수 있게 해줍니다. 이는 HashiCorp가 개발한 선언형 언어인 HCL(HashiCorp Configuration Language)을 사용합니다. 선언형이란 인프라를 위한 원하는 최종 상태를 설명하는 것을 의미하며, 잘 정의된 단계별 지침이 필요한 절차적 프로그래밍 언어와는 다릅니다(예: Ansible은 IaC 영역에서 사용됩니다).\n\n또한 테라폼은 인프라의 수명 주기를 관리하도록 허용하여 상태 파일을 유지함으로써 인프라의 원하는 최종 상태가 정의된 구성과 일치하는지 확인하고 변경 사항을 식별하고 적용합니다. 또한 리소스 간의 종속성을 결정하고 올바른 순서로 생성하거나 제거할 수 있습니다.\n\n초기 단계에서 우리의 인프라 요구 사항은 매우 복잡하지 않았으며 단일 제공업체에 한정되었습니다. 결과적으로, 우리의 테라폼 코드의 디렉토리 구조는 소규모에서 중간 복잡성의 인프라에 대한 당시 표준을 따르고 있었습니다:\n\n```js\ninfrastructure\n└── aws\n    ├── production\n    │   ├── eu-west-1\n    │   │   ├── ec2\n    │   │   ├── lambda\n    │   │   └── ...\n    │   ├── global\n    │   │   └── iam\n    │   └── us-east-1\n    │       ├── ec2\n    │       ├── rds\n    │       └── ...\n    └── staging\n        └── ...\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일부 리소스를 관리하는 데는 문제가 없었고 깔끔하며 어떤 리소스가 어디에 설정되었는지 쉽게 추적할 수 있었으며, 프로덕션 및 스테이징 환경을 완전히 격리시킵니다. 그러나 마이크로서비스 아키텍처를 채택하고 개발, 테스트 및 프로덕션용 여러 환경으로 확장하면 다음과 같이 복잡해졌습니다:\n\n- 모듈을 사용하지 않았기 때문에 코드가 DRY(반복이 줄어든 개발) 또는 표준화되지 않았습니다.\n- 동일한 AWS 리소스의 인스턴스가 한 파일에 도입될수록 구성 파일이 더욱 장황해졌습니다:\n  - 업데이트/검토하는 데 더 많은 시간과 노력이 필요했습니다.\n  - Terraform이 계획을 더 많이 수행하고 적용하는 데 시간이 더 오래 걸렸습니다.\n\n![FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany_0.png](/assets/img/2024-05-20-FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany_0.png)\n\n우리는 각각이 있는 상당수의 마이크로서비스의 인프라를 관리해야 했습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다른 리소스 세트가 필요합니다.\n- 약간 다른 구성으로 여러 환경에 배포됩니다.\n\n그리고 비즈니스 로직을 추가하여 조건부로 인프라를 생성하고 구성하는 코드를 넣으세요. 예를 들어:\n\n- 마이크로서비스는 프로덕션을 위해 전용 리소스가 필요하지만 개발 또는 테스트 환경에서는 공유 리소스를 사용할 수 있습니다. 예: 클라우드 저장소\n- 개발 및 테스트 환경을 위한 인프라는 꼭 고가용성이 필요 없습니다.\n\n그러나 디렉터리 구조가 마이크로서비스 아키텍처를 반영하지 않아서 모든 필요한 리소스를 하나의 마이크로서비스에 번들로 제공하지 못했습니다. 이는 모두 동시에 프로비저닝하거나 폐기하는 것이 어려워져서 임시로 사용하지 않는 리소스를 남길 수 있는 가능성이 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n상기 내용 외에도 2018년에는 Heroku를 포기하고 배포 플랫폼으로 Kubernetes를 도입하기로 결정했습니다. Heroku가 우수한 선택이었지만, 회사가 성장하며 새로운 리소스 수요가 증가함에 따라 비용 부담이 커졌습니다. 또한 필요한 유연성과 자세한 모니터링 기능이 부족했습니다. 이 결정으로 Terraform을 사용하여 관리해야 할 인프라의 크기와 복잡성이 상당히 증가했습니다. 그 당시 우리는 간단한 구조가 확장 가능하지 않음을 깨닫고 IaC 아키텍처를 다시 고려해야 한다는 것을 알았습니다.\n\nTerraform은 인프라를 관리하는 데 탁월한 도구이지만, 2018년 당시 제한이 있었습니다. 비즈니스 로직을 구현하는 것이 어려웠고, 설정 파일을 공유하는 것이 간단하지 않았으며, \"환경별 애플리케이션\"을 지원하기 위해 디렉터리 구조를 변경하는 것은 네이티브로 처리할 수 없었습니다. Terraform 모듈을 사용하더라도 각 모듈과 환경마다 상당한 양의 코드를 수동으로 중복해야 했기 때문에 우리의 설정은 DRY 원칙을 따르지 못하며 리소스를 묶어 함께 처리하는 문제는 해결되지 않았습니다.\n\n지금은 CI/CD에 대해 생각하고 있을 수 있습니다. 당신의 의견을 이해합니다만, 아직 그 단계에는 이르지 못했습니다.\n\n목표는 리소스 기반 구조에서 마이크로서비스 기반 구조로 전환하는 것이었지만, Terraform은 IaC를 재설계하는 데 필요한 주요 기능이 부족했습니다. 그때 Terragrunt가 등장했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 2G IaC — Terragrunt으로 구조화된 Terraform 코드\n\nTerragrunt은 Gruntwork에서 개발한 Terraform 래퍼로, 설정을 DRY 유지하고 여러 Terraform 모듈을 사용하며 원격 상태를 관리하는 추가 도구를 제공합니다.\n\nTerragrunt을 사용하면 다음과 같은 작업을 수행할 수 있습니다:\n\n- Terraform 코드를 DRY 상태로 유지\n- 중복된 백엔드 코드 삭제\n- 상위 디렉토리에서 설정 상속\n- 한 번에 여러 모듈 적용\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이를 통해 우리는 원하는 마이크로서비스 블루프린트를 만들고 IaC를 재구성할 수 있게 되었습니다.\n\n이 단계에서는 코드의 디렉토리 구조를 재설계하여 우리의 인프라 구조에 맞추었습니다. 클라우드 공급업체나 서비스 제공자에 중립적인 디자인으로 조직화되어 있으며, 스코프의 명확한 격리를 가지고 있습니다.\n\n- 조직, 예: 스테이징, 프로덕션 등\n- 환경, 예: 테스트, 개발, 프로덕션 등\n- 마이크로서비스\n\nIaC를 재구성하고 보다 일관성 있게 만들기로 결정했기 때문에, 우리는 리소스에 대한 새로운 일관된 명명 규칙을 수립해야 했습니다. 이 명명 규칙은 Terraform 리소스와 실제 클라우드 인프라에 적용될 것이며, 각 환경을 최상위 추상화 수준으로 보고 리소스가 다른 환경 간에 공유되지 않을 것을 고려하여 Terraform 리소스 이름, 변수 이름 및 리소스 태그에 대한 명명 규칙 제안을 마련했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 모듈\n\n명확한 디렉토리 구조와 명명 규칙을 가지고 인프라를 모듈로 분할하여 아키텍처를 기반으로 인프라를 기술하는 것이 목표였습니다. Terraform의 최상의 관행을 따라, 각 마이크로서비스를 위한 리소스를 번들로 제공하고 조건부 프로비저닝 및 구성을 위한 비즈니스 로직을 통합하기 위해 재사용 가능한 모듈을 만들었습니다.\n\n최종적으로, 디렉토리 구조는 다음과 같이 보이게 되었습니다:\n\nmodules/\n├── README.md\n└── organization\n└── infrastructure\n└── environments\n├── gke\n│ ├── firewall.tf\n│ ├── iam.tf\n│ ├── main.tf\n│ ├── nodepools.tf\n│ ├── providers.tf\n│ ├── remote_state.tf\n│ └── variables.tf\n├── microservice1\n│ ├── README.md\n│ ├── iam.tf\n│ ├── mongo.tf\n│ ├── providers.tf\n│ ├── s3.tf\n│ └── variables.tf\n└── microservice2\n├── README.md\n├── cloudfront.tf\n├── iam.tf\n├── iam_policy.json\n├── postgres.tf\n├── providers.tf\n├── remote_state.tf\n├── s3.tf\n├── s3_policy.json\n└── variables.tf\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테라폼 모듈로 전환함으로써 필요한 추상화를 달성할 수 있었습니다. 그러나 각 모듈을 인스턴스화하고 입력 변수에 값을 설정하고 출력 변수를 정의하며, 공급자를 구성하고 원격 상태를 제공하는 코드는 여전히 많은 유지 보수 부담을 야기했습니다.\n\n## 라이브\n\nTerragrunt를 사용하여 코드의 추상화 수준을 추가하고 서로 다른 환경에서 코드의 버전화된, 변경 불가능한 artifact를 제공할 수 있었습니다. 이 도구는 일반적인 테라폼 코드에 존재하는 원격 테라폼 구성을 가져올 수 있으며, 환경 간에 다를 수 있는 값에 대해 입력 값을 요구합니다.\n\n별도의 저장소에서 비슷한 디렉토리 구조를 따라 우리의 모든 환경에 대한 라이브 코드를 정의했습니다. 이제 라이브 코드는 단 3개의 파일로 이루어져 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- [필수] 코드의 소스를 지정하는 Terragrunt .hcl 파일\n- [필수] 리소스를 구성하는 데 필요한 키/값 쌍만 포함해야 하는 Terraform .auto.tfvars 파일\n- [선택 사항] Terraform 비밀 .auto.tfvars 파일은 구성에 비밀을 유지하려는 경우에만 사용됩니다. 비밀을 안전하게 유지하기 위해 git-crypt를 사용하여 git 저장소에서 파일의 투명한 암호화 및 해독을 가능하게 합니다.\n\n이 방법으로 모듈은 조직 및 환경에 구애받지 않으며, 구성 코드는 다른 환경 및 조직 간에 각 마이크로서비스마다 다를 것입니다. 최종적으로 우리의 라이브 구성은 다음과 같았습니다:\n\nlive/\n├── production\n│ ├── org_config.auto.tfvars\n│ ├── production1\n│ │ ├── env_config.auto.tfvars\n│ │ ├── gke\n│ │ │ ├── terragrunt.hcl\n│ │ │ ├── variables.auto.tfvars\n| | | └── secrets.auto.tfvars\n│ │ ├── microservice1\n│ │ │ ├── terragrunt.hcl\n│ │ │ └── variables.auto.tfvars\n│ │ └── microservice2\n│ │ ├── terragrunt.hcl\n│ │ └── variables.auto.tfvars\n│ └── production2\n│ ├── env_config.auto.tfvars\n│ ├── gke\n│ │ ├── terragrunt.hcl\n│ │ ├── variables.auto.tfvars\n| | └── secrets.auto.tfvars\n│ ├── microservice1\n│ │ ├── terragrunt.hcl\n│ │ └── variables.auto.tfvars\n│ └── microservice2\n│ ├── terragrunt.hcl\n│ └── variables.auto.tfvars\n└── staging\n├── dev\n│ ├── env_config.auto.tfvars\n│ ├── gke\n│ │ ├── terragrunt.hcl\n│ │ ├── variables.auto.tfvars\n| | └── secrets.auto.tfvars\n│ ├── microservice1\n│ │ ├── terragrunt.hcl\n│ │ └── variables.auto.tfvars\n│ └── microservice2\n│ ├── terragrunt.hcl\n│ └── variables.auto.tfvars\n├── org_config.auto.tfvars\n└── qa\n├── env_config.auto.tfvars\n├── gke\n│ ├── terragrunt.hcl\n│ ├── variables.auto.tfvars\n| └── secrets.auto.tfvars\n├── microservice1\n│ ├── terragrunt.hcl\n│ └── variables.auto.tfvars\n└── microservice2\n├── terragrunt.hcl\n└── variables.auto.tfvars\n\n그러한 방법으로 코드 측면에서 모든 것이 갖추어졌고 대부분의 경우에 잘 작동했습니다. Terraform은 애플리케이션이 실행될 인프라를 생성하고 관리하는 도구입니다. 리소스와 그 사양을 선언적인 방식으로 정의하고, 의존성을 매핑하고, 모든 것을 구축하고, 심지어 현재 인프라와 원하는 최종 상태 간의 일치를 보장하기 위해 상태를 유지할 것입니다. 그러나, 소프트웨어에는 적용되지 않습니다. Terraform은 리소스 자체를 생성하는 데 사용되지만 실행 중인 소프트웨어를 관리하지는 않습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n쿠버네티스에서 대부분의 워크로드를 실행하고 일부는 가상 머신(VMs)에서 실행하면 많은 클러스터 및 개별 서버를 특정 소프트웨어 관점에서 일정 수준의 사용자 정의로 관리해야 했습니다. 예를 들어, GitOps 파이프라인(Flux)을 구축하고 클러스터의 네트워킹 기능을 확장하는 소프트웨어를 원하거나, VM에서 실행 중인 Redash 및 Airflow와 같은 주로 내부 도구와 같은 서비스를 부트스트랩하는 것이 목표였습니다.\n\n여러 환경을 유지하기 위해서는 소프트웨어를 설치, 구성 및 관리하는 일련의 일관된, 신뢰할 수 있고 안전한 방법이 필요했습니다. 이것이 Ansible이 등장한 곳입니다. 일부 관리 되는 리소스용 Terraform 프로바이더가 그 때에 이미 사용 가능했지만, 팀의 역량 및 기존 Ansible에 대한 친숙함과 함께 특정 사용자 정의 작업, 주로 소프트웨어 설치를 위해 채택하는 결정을 내렸습니다.\n\nAnsible\n\nAnsible은 시스템을 구성하고 소프트웨어를 배포하며 연속 배포나 제로 다운타임 롤링 업데이트와 같은 보다 고급 IT 작업을 조정할 수 있는 IT 자동화 (IaC) 도구입니다. Ansible은 미리 정의된 단계 집합을 실행하고 원하는 최종 상태보다는 자동화 프로세스에 집중합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테라폼과 앤서블은 상호 배타적이지 않지만 둘 다 인프라스트럭처 코드 (IaC)에 사용할 수 있는 도구들입니다. 테라폼은 선언적인 접근 방식을 따라 구성 파일에 기반하여 인프라스트럭처 리소스를 프로비저닝, 수정, 관리 및 파괴하는 데 이상적입니다. 한편, 앤서블은 주로 절차적인 방식을 따르는 구성 관리 도구로, 특정 단계가 특정 순서로 실행되어야 하는 상황에서 리소스를 구성하는 데 뛰어납니다. 예를 들어 소프트웨어 설치/업데이트, 런타임 환경 설정, 시스템 구성 파일 업데이트 등.\n\n테라폼과 앤서블을 결합함으로써 새로운 인프라스트럭처를 구축하고 필요한 하드웨어와 소프트웨어를 구성하는 유연한 워크플로우를 만들었습니다. 우리는 테라폼의 로컬 실행자(provisioner)를 활용하여 모듈 내부에서 앤서블 플레이북을 실행하고, 테라폼 변수를 기반으로 플레이북을 사용자 정의하기 위해 템플릿을 사용했습니다. 이 하이브리드 접근 방식을 통해 워크로드의 의존성에 필요한 클러스터를 빠르게 구성할 수 있었습니다. 게다가, 플레이북이 모듈에 통합되어 있었기 때문에 모든 프로비저닝된 리소스가 동일한 방식으로 구성되도록 일관성을 확보할 수 있었습니다. 또한, 모든 환경에서 일관성을 유지함으로써 유지 관리와 문제 해결을 단순화했습니다… 또는 아니었을지도 모릅니다 :)\n\n모든 것이 준비되어 좋은 시작이었지만, 우리가 작업을 완료할 쯤 다른 영역에서 비롯된 다른 유형의 제약 조건에 부딪히기 시작했습니다.\n\n## 문제\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nWET 코드\n우리의 대부분의 마이크로서비스는 RDS 인스턴스나 S3 버킷과 같은 다양한 종류의 리소스가 필요합니다. 그래서 우리는 리소스 기반에서 마이크로서비스 기반 설계로 전환하기로 결정했습니다. 그러나 동일한 유형의 리소스 정의를 서로 다른 모듈 간에 중복하는 실천은 WET 코드뿐만 아니라 구성에서의 일관성 부족을 만들어내기도 했습니다. 우리 모든 S3 버킷에 대해 데이터 암호화를 강제하는 등 변경 사항을 적용해야 하는 경우에 코드 일관성을 보장하기 위해 상당한 인지적 부담이 도입되었습니다.\n\n하나의 모듈로 모두 통합\n다양한 리소스를 재사용 가능한 모듈로 번들링하고 비즈니스 로직을 포함하는 것이 가야 였습니다(그리고 우리는 여전히 그것이 옳다고 믿습니다). 그러나 서로 다른 팀에서 자주 다른 환경에 대한 요구사항을 갖고 장비 제공 및 동일한 마이크로서비스의 구성을 관리하는 것은 모든 가능한 시나리오를 수용하기 위한 지루한 양의 비즈니스 로직을 유발했습니다.\n\n예를 들어, 클라우드 스토리지와 그에 접근하기 위한 서비스 계정, 포스트그레SQL 데이터베이스 및 CDN이 필요한 단일 마이크로서비스를 고려해 봅시다. 일부 잠재적인 시나리오는 다음과 같습니다:\n\n- QA 팀: 여러 번 파일을 업로드할 필요가 없도록 X 환경에 생성된 공유 버킷을 사용해야 함\n- 개발 팀: 여러 출처를 유지 관리할 필요가 없도록 Y 환경에 생성된 공통 CDN을 사용해야 함\n- 프로덕션 환경: 리소스 분리 및 격리가 필요함\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n앤서블을 실행하는 것이 무서워졌어요\n우리의 인프라 구조 요구 사항이 증가하고 제품 아키텍처가 더 복잡해지면서, 앤서블에 의해 수행된 사용자 정의 및 구성이 점점 관리하기 어려워졌어요. 1000줄 이상의 단일 플레이북과 100개의 앤서블 태스크가 있는 상황에서는 제어하고 변경 사항을 추적하기 어려워졌습니다. 테라폼과 달리, 앤서블은 어떤 태스크가 리소스를 수정할지 여부를 명확하게 제공하지 않았어요. 특히 더 많은 중요한 소프트웨어가 앤서블로 관리되는 상황에서 프로덕션 환경에서 실행해야 할 때 마다 불안함을 가중시키기도 했죠.\n\n그때 우리는 다음과 같은 결정을 내렸어요:\n\n- 가능한 것을 테라폼 프로바이더로 이동시키기로 했어요: 쿠버네티스 및 헬름 오퍼레이터는 이 시점에서 더 성숙해졌으므로, kubectl 명령을 실행하거나 직접 헬름 차트를 설치하는 대신 이들을 사용하기 시작했어요.\n- 쿠버네티스 서비스 설치를 테라폼 밖으로 이동하기로 결정했어요: 가능하고 적절한 경우, 쿠버네티스 서비스의 설치 프로세스를 Flux에 의해 관리되는 GitOps 워크플로로 이동했어요.\n  새로운 환경, 일관성 및 자동화에 대해 궁금해할 수 있어요. 우리의 헬름 / Flux 설정은 별도의 기사가 필요한 지금, 대부분의 기능이 헬름 차트 방향으로 이동되었지만 이 서비스들에 대한 헬름 릴리스는 여전히 테라폼을 통해 관리되며, 테라폼을 통해 이 서비스에 대한 헬름 릴리스를 실행하고 있습니다.\n- 남은 작업을 개별 플레이북으로 분해하기로 결정했어요: 관리를 단순화하고 유연성을 향상시키기 위해 남은 작업을 더 작고 명확한 작업을 수행하는 개별 플레이북으로 분할했기 때문에 이들을 필요에 따라 별도로 실행할 수 있어요.\n\n문서화\n모듈의 복잡성 증가는 이들을 이해, 업데이트 및 새로운 환경에서 리소스 프로비저닝에 사용하기 어렵게 만들었습니다. 기능을 명확히 파악하고 입력 변수를 구성할 필요가 있는 값을, 예상 결과를 파악하기 위한 오퐋이 미비하다는 사실이 점점 명백해졌습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 3G IaC — Terraform의 귀환\n\nTerraform를 v1.x.x로 업그레이드 한 후, 우리는 현재 구조의 한계와 직면한 문제를 평가하기 시작할 좋은 위치에 있었습니다.\n\n지금까지 사용해온 IaC의 마이크로서비스 기반 조직은 필요한 빌딩 블록을 만들고 필요한 비즈니스 로직을 통합하여 비즈니스 요구 사항을 충족하는 데 유익한 것으로 입증되었습니다. 이 방식은 새로운 마이크로서비스를 도입하거나 기존 마이크로서비스를 개선하는 데 용이했습니다. 예를 들어, 만약 마이크로서비스 X가 NoSQL 데이터베이스를 활용해야 한다면, 해당 모듈을 업데이트하여 Mongo 클러스터를 포함시키고 이 변경 사항을 마이크로서비스 X가 배포된 모든 환경에 적용하면 됩니다. 그러나 모든 마이크로서비스의 NoSQL 데이터베이스에 대해 수평적인 변경을 수행해야 하는 경우에는 점점 더 지루해지는 일이었습니다. 회사가 성장함에 따라 코드 기반이 점점 커지고 관리하기 어려워지며, 보다 복잡한 요구 사항을 구현하는 것이 어려워졌기 때문에 모든 위의 문제를 가능한 빨리 해결해야 했습니다.\n\n우리의 현재 구조를 살펴보겠습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany_1.png\" />\n\n## 코어 리소스로서의 자식 모듈\n\n이전 문제를 해결하기 위해 우리는 자식 모듈의 사용을 도입하여 추상화의 추가 층을 더했습니다.\n\n자식 모듈을 개발하기 시작하려면 일관성과 균일성을 보장하기 위한 규칙 세트를 확립해야 했습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Terraform 리소스를 항상 함께 배치해야 하는 번들로 구성합니다. 예를 들어, 파라미터가 있는 PostgreSQL RDS 인스턴스, 서브넷 및 보안 그룹입니다.\n- 인프라 전역에 적용되는 기본 값을 설정하고 강제합니다. 예를 들어, S3 버킷은 공개 액세스 차단, HTTP 요청 거부 및 서버 측 암호화를 사용해야 합니다.\n- 어떠한 비즈니스 로직도 포함해서는 안 됩니다.\n  \"비즈니스 논리\"는 Terraform 리소스를 특정 환경 및 애플리케이션에 맞게 구성하는 방식 및 이러한 리소스를 번들로 묶는 방식을 가리킵니다. 모든 비즈니스 로직은 루트 모듈에 유지되어야 합니다.\n- 하나의 제공업체를 대상으로 한 리소스가 포함됩니다.\n\n하위 모듈에는 동일한 제공업체의 리소스만 포함되어야 합니다. 이것은 이러한 모듈에 따라 가야 할 파일 구조에서 더 잘 보여집니다.\n\n```js\nmodules-terraform/\n├── aiven\n│  └── kafka\n│     ├── README.md\n│     ├── main.tf\n│     ├── outputs.tf\n│     ├── variables.tf\n│     └── versions.tf\n├── aws\n│  └── db_instance\n│     ├── README.md\n│     ├── main.tf\n│     ├── outputs.tf\n│     ├── variables.tf\n│     └── versions.tf\n└── gcp\n   └── storage_bucket\n      ├── README.md\n      ├── main.tf\n      ├── outputs.tf\n      ├── variables.tf\n      └── versions.tf\n```\n\n자식 모듈은 더 포괄적인 자식 모듈을 만들기 위해 연결될 수 있습니다. 예를 들어, AWS RDS 인스턴스에 Postgres DB를 인스턴스화하는 경우, 모든 Postgres 인스턴스에 걸쳐 적용하는 표준 Postgres 구성이 있는 Postgres 자식 모듈을 갖습니다. 그럼에도 불구하고 인기 있는 RDS 구성 (로그 기록, SSL, 휴면 시 암호화 등)과 모든 표준 구성물을 포함하며 Postgres에 특정한 표준 구성 사항을 위해 Postgres 자식 모듈을 활용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래서 새 구조에 하위 모듈이 추가된 것은 다음과 같이 표현될 수 있습니다:\n\n![구조](/assets/img/2024-05-20-FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany_2.png)\n\n자원 번들링 및 비즈니스 로직을 위한 Terragrunt 루트 모듈\n\nTerraform 루트 모듈은 여러 하위 모듈을 인스턴스화하고, 특정 마이크로서비스에 대한 모든 자원을 제공하며, 환경 및 마이크로서비스 요구사항에 따른 \"비즈니스 로직\"을 담당합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예를 들어, 마이크로서비스를 위해 새로운 RDS 데이터베이스를 만들어야 한다고 가정해봅시다:\n\n우리의 요구 사항:\n\n- 각 환경/애플리케이션(비즈니스 로직)에 대한 네이밍 규칙 적용\n  클라우드 인프라의 네이밍 규칙은 다음과 같습니다:\n  `environment_name`-`service_name`-`resource_type`-`random_id`\n- 인프라 전반에 표준 태그 적용 (비즈니스 로직)\n  태그는 항상 이름, 제공자, 팀, 애플리케이션, 환경, 조직을 포함해야 합니다\n- 원격 상태에서 VPC 보안 그룹 사용\\*\\* (비즈니스 로직)\n- 한꺼번에 모든 것 생성 (Child 모듈)\n  RDS PostgreSQL 인스턴스, SSL 적용을 강제하는 매개변수 그룹, 서브넷 그룹, 모든 아웃바운드 트래픽을 허용하는 보안 그룹을 만듭니다.\n  이 경우 모든 것이 한 번에 처리될 것이며, 이 요구 사항은 항상 이러한 리소스를 함께 프로비저닝해야 한다는 것입니다. 심지어 SSL을 강제하지 않는 것을 허용하지 않는 엄격한 보안 요구 사항 같은 유효성 검사도 Child 모듈에서 처리될 것입니다.\n\n```js\n변수 \"db_parameter_group_parameters\" {\n  description = \"적용할 DB 매개변수 맵 목록\"\n  타입        = 목록(맵(문자열))\n  기본값     = []\n\n  유효성 {\n    조건 = alltrue(\n      [\n        매개변수 중에 var.db_parameter_group_parameters :\n        (\n          !포함([\"rds.force_ssl\"], 매개변수[\"이름\"])\n        )\n      ]\n    )\n    에러 메시지 = \"force_ssl을 덮어쓸 수 없습니다.\"\n  }\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리의 새로운 구조를 사용하면 이 작업을 단 40줄의 코드로 손쉽게 수행할 수 있습니다.\n\n```js\nlocals {\n  identifier = (var.microservice_pg_identifier == null\n    ? \"${var.env_name}-${var.microservice_name}-pg-${random_id.id.hex}\"\n    : var.microservice_pg_identifier\n  )\n  db_parameter_group_name = \"${var.env_name}-${var.microservice_name}-postgres-${element(split(\".\", var.microservice_pg_engine_version), 0)}\"\n  db_subnet_group_name    = \"${var.env_name}-${var.microservice_name}-${var.aws_region}-db-subnet\"\n  db_security_group_name  = \"${var.env_name}-${var.microservice_name}-${var.aws_region}-db-sg\"\n\n  tags = {\n    name        = local.identifier\n    provisioner = \"terraform\"\n    team        = var.team_name\n    app         = var.microservice_name\n    env         = var.env_name\n    org         = var.org_name\n  }\n}\n\nresource \"random_id\" \"id\" {\n  byte_length = 2\n}\n\nmodule \"microservice_pg\" {\n  source = \"../../../../modules-terraform/aws/db_instance\"\n\n  identifier        = local.identifier\n  engine_version    = var.microservice_pg_engine_version\n  allocated_storage = var.microservice_pg_allocated_storage\n\n  vpc_security_group_ids = [\n    data.terraform_remote_state.vpc.outputs.postgres_security_group_production_id,\n    data.terraform_remote_state.vpc.outputs.postgres_security_group_staging_id,\n    data.terraform_remote_state.vpc.outputs.postgres_security_group_services_id,\n  ]\n\n  backup_retention_period = var.microservice_pg_backup_retention_period\n\n  db_subnet_group_name       = local.db_subnet_group_name\n  db_subnet_group_subnet_ids = data.terraform_remote_state.vpc.outputs.vpc_id\n\n  db_parameter_group_name   = local.db_parameter_group_name\n  db_parameter_group_family = var.microservice_pg_parameter_group_family\n\n  db_security_group_name   = local.db_security_group_name\n  db_security_group_vpc_id = data.terraform_remote_state.vpc.outputs.vpc_id\n\n  tags = local.tags\n}\n```\n\n실시간 설정\n\n실제 설정에서는 Terragrunt를 사용하여 전역 구성 변수를 설정하고 프로바이더 버전을 생성하며, 인프라 전체에 걸쳐 화이트리스트로 지정된 IP를 조작합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테라폼 프로바이더 버전 관리\n\n테라폼 프로바이더 버전을 관리하기 위해 각 모듈마다 versions.tf 파일을 생성하는 데 Terragrunt를 사용합니다. 루트 Terragrunt 구성에서 생성 블록을 추가하고 로컬 블록에서 현재 사용 중인 프로바이더 버전이 포함된 YAML 파일을 디코딩합니다. 이를 통해 전체 모듈에 걸쳐 프로바이더 버전을 업데이트할 수 있습니다. 특정 모듈이 프로바이더의 다른 버전을 사용하길 원한다면, 해당 모듈의 라이브 구성에서 덮어쓸 수 있습니다.\n\n루트 terragrunt.hcl:\n\n```js\nlocals {\n  provider_version = yamldecode(file(\"provider_versions.yaml\"))\n  [...]\n}\n\ngenerate \"versions\" {\n  path      = \"versions.tf\"\n  if_exists = \"overwrite\"\n  contents  = <<EOF\n  terraform {\n    required_version = \">= 1.0\"\n    required_providers {\n      aws = {\n        source  = \"hashicorp/aws\"\n        version = \"${local.provider_version.aws}\"\n      }\n      [...]\n    }\n  }\nEOF\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nprovider_versions.yaml:\n\n```yaml\naws: \"3.74.1\" # [AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/3.74.1)\ngoogle: \"3.90.1\" # [Google Cloud Provider](https://registry.terraform.io/providers/hashicorp/google/3.90.1)\n[...]\n```\n\nWhat about Ansible?\n\nAs mentioned earlier, we have decided to simplify Ansible and migrate as much as possible to more suitable IaC or GitOps workflows. In our IaC code, we only use Ansible for:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Istio 설치. 우리가 리펙터링을 진행한 당시, Istio의 생산 준비가 된 유일한 설치 방법은 istioctl이었으며, 여전히 istioctl에서 Helm으로 이동하려면 삭제 및 재설치가 필요하므로 우리의 프로덕션 환경에 신중히 계획해야 합니다.\n- VM 구성 및 관리. Ansible, Chef, 그리고 Puppet과 같은 절차적 도구들이 아직도 VM 소프트웨어를 구성하고 유지하는 데 최적의 선택지입니다. Ansible의 장점 중 하나는 클라이언트 측에서 실행될 수 있으므로 VM이 구성 관리 도구에 액세스할 필요가 없다는 것입니다.\n\n# CI\n\n이 내용은 이 글의 범위를 벗어나므로 자세히 다루지 않겠습니다. 하지만 조만간...\n\nIaC CI/CD 여정을 설명할 후속 기사가 곧 공개될 예정입니다.\n","ogImage":{"url":"/assets/img/2024-05-20-FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany_0.png"},"coverImage":"/assets/img/2024-05-20-FrommanualtodeclarativeTerraformandIaCinafastgrowingcompany_0.png","tag":["Tech"],"readingTime":25},{"title":"인프라스트럭처를 코드로 씁니다 새로운 사고방식이 필요합니다","description":"","date":"2024-05-20 17:29","slug":"2024-05-20-InfrastructureasCodeNeedsaRethink","content":"\n## 개발자에게 구름의 복잡성을 되돌리면, 제공 속도가 느리고 비용이 많이 들며 보안이 취약해집니다.\n\n# IaC의 진화\n\n코드를 사용하여 클라우드 컴퓨팅 인프라를 정의하는 Infrastructure as Code (IaC)는 클라우드 환경을 배포하고 관리하는 기본 메커니즘이 되었습니다. 이것은 물론, 그런 이유 때문에 그렇습니다.\n\nIaC가 나오기 전에는 조직이 클라우드 제공업체의 웹 응용 프로그램에서 수동으로 클라우드 자원을 생성했습니다. 이제 클라우드 인프라를 선언적으로 정의하고 버전 관리 시스템에 저장함으로써, 개발자는 애플리케이션 코드가 관리되는 방식과 같이 버전이 지정되고 반복 가능한 인프라 릴리스를 만들 수 있습니다. 클라우드 인프라의 제공 속도, 안정성 및 감사 가능성 측면에서 얻는 가치는 과소평가해서는 안 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 지금까지 있어 온 곳이죠. 이제 클라우드 인프라 관리가 어디로 나아가야 할지 이야기해 보겠습니다.\n\n# 클라우드 복잡성의 직접적 반영\n\n주요 클라우드 공급업체 중립 IaC 도구로는 Terraform과 Pulumi가 있습니다. 생산에서 널리 사용되는 공급업체별 도구로는 CloudFormation과 Azure Arm 템플릿이 있습니다. 참고: Terraform이 압도적으로 가장 널리 사용되고 있기 때문에, 이 기사의 나머지 부분에서는 Terraform과 IaC를 서로 바꿔서 언급할 것입니다.\n\n이 도구들 간의 구문과 구현 세부 사항은 각각 다르지만, 철학적으로 핵심 측면에서는 비슷합니다. 지원되는 클라우드의 인프라 관리를 가능한 한 유연하고 맞춤화하여 제공합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n표면적으로는 정말 멋져보이죠. 문제는 클라우드가 굉장히 복잡하다는 것이고, 이러한 도구들은 이 복잡성을 개발자에게 직접 전달합니다. 예를 들어, Terraform을 사용하여 AWS에서 상태를 가지는 표준 컨테이너화된 애플리케이션을 실행하고 싶다고 가정해봅시다. 이를 위해서는 다음과 같은 작업을 해야 합니다:\n\n- 어떤 서비스를 사용할지 결정해야 합니다 (AWS ECS, AWS Fargate, Containerized Lambdas, EKS, EC2 인스턴스 등).\n- 우리의 컨테이너를 AWS의 ECR 서비스에 저장하고 선택한 컴퓨팅 방법이 액세스할 수 있도록 해야 합니다.\n- 데이터베이스 인스턴스를 선택해야 합니다 (RDS, Aurora, DocumentDB, DynamoDB, Redshift 등).\n- 네트워킹 리소스를 구성하여 컴퓨팅 인스턴스가 안전하게 데이터베이스에 연결할 수 있도록 해야 합니다. 이 과정에는 VPC, 서브넷, 보안 그룹 등이 포함됩니다.\n- 민감한 환경 변수를 AWS Secrets Manager 또는 Parameter Store에 비밀로 저장하고 컴퓨팅 인스턴스가 이에 액세스할 수 있도록 해야 합니다.\n- 위 모든 인프라를 Terraform을 사용하여 정의하고 배포해야 합니다. 우리의 비교적 간단한 앱을 위해 위 요구 사항을 충족하는데는 Terraform의 수백 줄이 필요할 수 있습니다.\n\n와우! 이제 우리 애플리케이션이 배포되었습니다. 하지만 우리가 확인했는지 확인해 보았는지요:\n\n- 우리의 컴퓨팅, 데이터베이스 및 네트워킹 구성에서 보안 취약점이 존재하지 않았나요?\n- 각 리소스 사이에 제한된 엑세스만 부여되었나요?\n- 인프라를 구성하기 전 애플리케이션 비용을 확신하고 추정했나요?\n- 각 클라우드 리소스에 적절한 태그가 지정되었나요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로 고려해야 할 복잡성의 다른 요소 두 가지가 있습니다:\n\n- Terraform 상태 파일을 원격으로 저장하고 있는지 확인해야 합니다. 그렇다면 s3 버킷과 DynamoDB 인스턴스를 생성해야 합니다.\n- 만약 동일한 애플리케이션을 새로운 클라우드 공급업체에 배포하려면, 처음부터 시작하여 해당 새로운 클라우드 공급자의 독특한 특성을 고려해야 합니다.\n\n# Best Practices are Applied Reactively\n\n대부분의 조직이라면 우선 빠르게 애플리케이션을 클라우드로 옮기고, 그런 다음 제3자 도구(오픈 소스 및 상용)를 사용하여 클라우드 인프라를 반응적으로 정리하는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 보안 스캐닝: tfsec, checkov, Wiz, Snyk, Prisma Cloud 등\n- 클라우드 비용 최적화: infracost, 다양한 유료 오퍼링 등\n\n넓은 권한이 이미 부여된 상황에서 최소 권한 액세스를 강제하는 것은 특히 식별하고 해결하기 어렵습니다.\n\n## 가장 세련된 팀들은 내부 모듈을 제공합니다\n\n매우 성숙한 Terraform 관리를 제공하는 조직은 개별 개발자가 자체 인프라 요구 사항에 활용할 수 있는 미리 구성된 Terraform 모듈을 제공합니다. 이 접근 방식은 주로 보안 및 비용 문제를 해결하지만 다음 측면에서는 약점이 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 개발자들이 올바른 모듈을 선택하는 동안 계속해서 인지 부하가 발생합니다.\n- 중요한 추가 투자 없이 여러 클라우드 제공 업체에 확장되지 않습니다.\n- 구축 및 유지가 매우 비싸며 결과적으로 이 전략은 주로 규제 검토 수준이 높은 기관(예: 금융 기관)에서 사용됩니다.\n- 자동으로 최신 릴리스와 일치하는 모듈을 유지하도록 강제하는 것은 크게 해결하지 못한 문제입니다 (내 지식으로는).\n\n조직 외부에는 공개 모듈이 있지만 가장 잘 만들어진 모듈조차도 상당한 복잡성을 나타내며 적절하게 구성하기는 쉽지 않습니다.\n\n## 근본적으로 반응적인\n\nTerraform의 배포 전에 문제를 예방하기 위해 위의 도구와 전략을 적극적으로 도입했다고 하더라도, 운영 모델은 여전히 근본적으로 반응적입니다. Terraform이 거의 모든 리소스를 거의 모든 방법으로 배포할 수 있게 되는 반응으로, 개발 운영/플랫폼/SRE 엔지니어 팀이 복잡한 내부 서비스를 유지보수해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 미래가 어떻게 보일지\n\n인프라스트럭처를 코드로 다루는 새로운 방식은 기존 방식을 깨끗하게 벗어나야 합니다. 모든 클라우드 자원을 모든 사용 사례에 노출시키는 대신 개발자에게 의견이 강한 클라우드 빌딩 블록 세트를 제공하는 방식으로 클라우드 제공 업체의 세부 정보를 추상화할 수 있는 구성 블록을 제공하는 것이 좋겣습니다.\n\n- 클라우드 제공 업체의 세부 정보를 추상화하는 구성 블록: \"네트워크에 컴퓨팅 및 데이터베이스가 필요하다면 AWS에서 실행될 수 있도록 구성해주세요. 배포합니다.\"\n- 고유 방식의 접근 권한 및 보안 모범 사례를 기본적으로 조정 - 이러한 모범 사례를 벗어나 배포할 수있는 옵션이 없어야 합니다.\n- 새로운 클라우드 자원을 배포하기 전 통합 비용 추정.\n- 조금 더 구현 세부사항이지만, 이러한 도구가 \"서버\" 모드에서 실행되어 자동으로 drift를 조정하거나 상태 파일 없이 작동할 수 있다면 Terraform을 대규모로 사용할 때 발생하는 일부 문제를 해결할 수 있을 것입니다.\n\n# 다음에는\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n새로운 솔루션이 등장하여 현재 IaC 도구로는 해결하기 어려웠던 이러한 큰 문제들을 해결할 수 있을지는 알 수 없습니다. Terraform는 IaC 영역에서 너무도 우세해져서 새롭고 흥미로운 접근 방식이라도 Terraform을 기반으로 사용하고 있습니다 (예: Wing).\n\n그렇다고 해서 시간이 흐르면서 새로운 IaC 도구들이 나타나서 유연성 대신 의견을, 선택의 역설 대신 간단함을, 반응형 모범 사례 대신 선제적인 것들을 제시하는 방향으로 나아갈 것인지는 시간이 풀어줄 것입니다.\n\n—\n\n궁금한 점이나 생각이 있으시면 댓글로 알려주세요!\n","ogImage":{"url":"/assets/img/2024-05-20-InfrastructureasCodeNeedsaRethink_0.png"},"coverImage":"/assets/img/2024-05-20-InfrastructureasCodeNeedsaRethink_0.png","tag":["Tech"],"readingTime":6},{"title":"Google Cloud에서 Terraform을 사용하여 인프라 구축하기 - Google Challenge Lab 안내","description":"","date":"2024-05-20 17:28","slug":"2024-05-20-BuildInfrastructureonGoogleCloudwithTerraformGoogleChallengeLabWalkthrough","content":"\n구글 클라우드에서 Terraform으로 인프라를 구축하는 코스의 챌린지 랩 워크스루입니다.\n\n이 랩은 다음을 테스트합니다:\n\n- 기존 인프라를 Terraform 구성으로 가져오는 능력.\n- 직접 Terraform 모듈을 빌드하고 참조하는 것.\n- 원격 백엔드를 구성에 추가하는 것.\n- Terraform 레지스트리에서 모듈을 사용하고 구현하는 것.\n- 인프라를 다시 프로비저닝하고 파괴하고 업데이트하는 것.\n- 생성한 리소스 간의 연결성을 테스트하는 것.\n\n# 챌린지 랩 소개\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGoogle은 구글 클라우드 스킬즈 부스트(이전 명칭: QwikLabs)라는 온라인 학습 플랫폼을 제공합니다. 이 플랫폼에서는 학습 경로에 맞는 교육 과정을 따르거나 특정 제품 또는 솔루션에 대한 교육 과정을 진행할 수 있습니다.\n\n이 플랫폼에서의 학습 경험 중 하나는 퀘스트입니다. 퀘스트에 참여하면 안내형 실습 랩을 수행한 후 도전 랩을 완료합니다. 도전 랩은 목표가 명시되지만 목표를 달성하는 방법에 대한 안내가 거의 없는 다른 랩과는 다릅니다.\n\n제가 가끔 이 도전 랩에 대한 해결 방법을 제공합니다. 목표는 도전 랩을 도와주어서 쉽게 풀어내는 데 도움을 주는 것이 아닙니다! 대신에:\n\n- 랩을 완료하는 데 가장 이상적인 경로로 이끄는 것을 보여드립니다.\n- 랩을 스스로 완료할 수 없는 특정 문제나 장애물을 해결하는 데 도움을 줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도움이 필요한 경우, 도전 과제에 대한 도움을 얻으려면 올바른 곳에 오신 것을 환영합니다. 그러나 먼저 퀘스트를 진행하고 직접 랩을 시도한 후에 계속 읽기를 강력히 권장합니다!\n\n이러한 랩들은 문제 해결에 대해 다양한 방법이 항상 있습니다. 일반적으로 저는 더 반복 가능하고 프로그래밸하게 문제를 해결할 수 있도록 문제를 해결하는 것이 좋습니다. 그러나 물론 클라우드 콘솔도 사용할 수 있습니다.\n\n# 이 랩 개요\n\n이 랩에서는 Terraform을 사용하여 Google Cloud에서 인프라를 생성, 배포 및 관리해야 합니다. 또한 일부 관리되지 않은 인스턴스를 구성에 가져와 수정해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 나의 해결책\n\n우리가 이 도전 과제 동안 사용할 몇 가지 변수를 정의하는 것부터 시작해봅시다. 실제 변수는 랩을 시작할 때 제공될 것입니다.\n\n```js\ngcloud auth list\n\nregion=<지역 입력>\nzone=<존 입력>\nprj=<프로젝트 ID 입력>\n```\n\n## 작업 1 — 구성 파일 생성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 폴더 구조를 만들라고 지시받았어요:\n\n```js\nmain.tf\nvariables.tf\nmodules/\n└── instances\n|   ├── instances.tf\n|   ├── outputs.tf\n|   └── variables.tf\n└── storage\n    ├── storage.tf\n    ├── outputs.tf\n    └── variables.tf\n```\n\n이렇게 만들어볼까요:\n\n```js\n# 루트 디렉토리에 main.tf와 variables.tf 파일 생성\ntouch main.tf variables.tf\n\n# main 디렉토리 및 파일 생성\nmkdir -p modules/instances\nmkdir modules/storage\n\n# 'instances' 모듈 디렉토리에 필요한 파일 생성\ntouch modules/instances/instances.tf\ntouch modules/instances/outputs.tf\ntouch modules/instances/variables.tf\n\n# 'storage' 모듈 디렉토리에 필요한 파일 생성\ntouch modules/storage/storage.tf\ntouch modules/storage/outputs.tf\ntouch modules/storage/variables.tf\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 변수.tf 파일을 업데이트하여 다음 변수들을 포함하도록 합니다:\n\n```js\nvariable \"region\" {\n  description = \"Google Cloud 지역\"\n  type        = string\n  default     = \"실습에서 제공하는 지역\"\n}\n\nvariable \"zone\" {\n  description = \"Google Cloud 존\"\n  type        = string\n  default     = \"실습에서 제공하는 존\"\n}\n\nvariable \"project_id\" {\n  description = \"리소스를 프로비저닝할 프로젝트의 ID\"\n  type        = string\n  default     = \"귀하의 프로젝트 ID\"\n}\n```\n\n루트 모듈 main.tf를 업데이트하여 Google Cloud Provider를 포함하도록 합니다. Terraform Registry에서 항상 확인할 수 있습니다. 제공자 블록에 세 가지 변수를 모두 포함하도록 요청되었습니다.\n\n```js\nterraform {\n  required_providers {\n    google = {\n      version = \"~> 4.0\"\n    }\n  }\n}\n\nprovider \"google\" {\n  project     = var.project_id\n  region      = var.region\n  zone        = var.zone\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 테라폼을 초기화해야 합니다. 따라서 다음 명령을 실행하세요:\n\n```js\nterraform init\n```\n\n## 작업 2 — 인프라 가져오기\n\n여기서의 목표는 지금까지 테라폼 외부에서 프로비저닝된 인프라를 테라폼 제어 아래로 가져오는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테라폼 가져오기 워크플로우를 사용할 거에요:\n\n![이미지](/assets/img/2024-05-20-BuildInfrastructureonGoogleCloudwithTerraformGoogleChallengeLabWalkthrough_0.png)\n\n이것들은 가져오기 단계들이에요:\n\n- 가져올 기존 인프라를 식별하세요.\n- 인프라를 테라폼 상태에 가져오세요.\n- 해당 인프라와 일치하는 테라폼 구성을 작성하세요.\n- 구성이 예상 상태와 인프라와 일치하는지 확인하기 위해 테라폼 계획을 검토하세요.\n- 구성을 적용하여 테라폼 상태를 업데이트하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 가져와야 할 기존 인프라 확인\n\n이미 두 개의 GCE 인스턴스가 생성되었습니다. Cloud 콘솔에서 기존 인스턴스 중 하나인 tf-instance-1을 확인하세요. 우리는 다음을 검색하려고 합니다:\n\n- 네트워크\n- 머신 유형\n- 디스크\n\n다음으로, 우리는 main.tf에 우리의 인스턴스 모듈을 두 번 호출해야 합니다. 이 두 호출은 빈 정의를 포함할 것이므로 가져올 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nmodule \"tf_instance_1\" {\nsource = \"./modules/instances\"\ninstance_name = \"tf-instance-1\"\nzone = var.zone\nregion = var.region\n}\n\nmodule \"tf_instance_2\" {\nsource = \"./modules/instances\"\ninstance_name = \"tf-instance-2\"\nzone = var.zone\nregion = var.region\n}\n\n각 모듈 정의에는 고유한 레이블이 있어야 합니다.\n\n이제 초기화해보세요:\n\n```bash\nterraform init\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 instances.tf에 모듈 구성을 작성합니다. 우리는 포함해야 할 최소한의 구성 요소를 알려받았습니다:\n\n```js\nresource \"google_compute_instance\" \"instance\" {\n  name         = var.instance_name\n  machine_type = \"기존 인스턴스에서 하드 코딩\"\n  zone         = var.zone\n\n  boot_disk {\n    initialize_params {\n      # image = \"debian-cloud/debian-11\"\n      image = \"기존 인스턴스에서 하드 코딩\"\n    }\n  }\n\n  network_interface {\n    # network = \"default\"\n    network = \"기존 인스턴스에서 하드 코딩\"\n    access_config {\n      // 일시적 공용 IP\n    }\n  }\n\n  metadata_startup_script = <<-EOT\n          #!/bin/bash\n      EOT\n  allow_stopping_for_update = true\n}\n```\n\n인스턴스 모듈의 변수를 전달할 수 있도록 variables.tf를 업데이트하십시오. instance_name을 전달할 수 있도록 합니다:\n\n```js\nvariable \"instance_name\" {\n  description = \"인스턴스의 이름.\"\n  type        = string\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Terraform State로 기존 인프라 가져오기\n\n```js\nterraform import module.tf_instance_1.google_compute_instance.instance \\\n  projects/$prj/zones/$zone/instances/tf-instance-1\n\nterraform import module.tf_instance_2.google_compute_instance.instance \\\n  projects/$prj/zones/$zone/instances/tf-instance-2\n\n# 가져오기 확인\nterraform show\n```\n\n다음과 같이 가져와야 합니다:\n\n<img src=\"/assets/img/2024-05-20-BuildInfrastructureonGoogleCloudwithTerraformGoogleChallengeLabWalkthrough_1.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 계획 및 적용\n\n이제 우리는 apply를 실행하여 인스턴스를 그 자리에서 업데이트합니다:\n\n```js\nterraform plan\nterraform apply\n```\n\n## 작업 3 — 원격 백엔드 구성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이건 꽤 쉬워요. 원격 GCS 백엔드에 Terraform 상태를 저장하려고 할 때 실행해야 하는 표준 단계들이에요:\n\n- Terraform을 사용하여 GCS 버킷을 프로비저닝합니다.\n- 새로운 GCS 버킷을 가리키는 백엔드 블록을 추가합니다.\n- Terraform을 다시 초기화하고 로컬 상태 파일에서 원격 백엔드로 상태를 이관합니다.\n\n## GCS Bucket 프로비저닝하기\n\n다음 리소스 정의를 main.tf에 추가하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 코드를 아래와 같이 번역해 드립니다.\n\n```js\n리소스 \"google_storage_bucket\" \"test-bucket-for-state\" {\n  이름        = \"할당 받은 버킷 이름\"\n  위치    = \"US\"\n  uniform_bucket_level_access = true\n\n  force_destroy = true\n}\n```\n\n그리고 다음을 적용하세요:\n\n```js\nterraform apply\n```\n\n## GCS 백엔드 추가하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nmain.tf 파일을 수정하여 백엔드를 terraform 블록에 포함시키세요:\n\n```js\nterraform {\n  backend \"gcs\" {\n    bucket  = \"제공받은 버킷 이름\"\n    prefix  = \"terraform/state\"\n  }\n}\n```\n\n## 상태 이전\n\n로컬 상태 파일에서 GCS 백엔드로 Terraform 상태를 이전하는 부분입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nterraform init -migrate-state\n```\n\n상태를 이전하려는 것을 확인하라는 메시지가 표시됩니다:\n\n<img src=\"/assets/img/2024-05-20-BuildInfrastructureonGoogleCloudwithTerraformGoogleChallengeLabWalkthrough_2.png\" />\n\n## 작업 4 — 인프라 수정 및 업데이트\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 machine_type을 포함하는 variables.tf를 업데이트해야합니다:\n\n```js\nvariable \"machine_type\" {\n  description = \"인스턴스의 머신 유형\"\n  type        = string\n  default     = \"e2-standard-2\"\n}\n```\n\n그런 다음, instance.tf를 수정하여 machine_type 매개변수를 허용할 수 있도록 설정해야합니다:\n\n```js\nresource \"google_compute_instance\" \"instance\" {\n  name         = var.instance_name\n  machine_type = var.machine_type\n  zone         = var.zone\n\n  ...\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로, main.tf를 수정해서 세 번째 인스턴스를 추가해야 합니다. 세 번째 모듈을 호출하여 main.tf에 추가합니다. 이미 기본값을 설정했으므로 machine_type을 전달할 필요가 없습니다.\n\n이제 초기화하고(모듈 인스턴스를 추가했으므로) 적용하세요.\n\n```js\nterraform init\nterraform apply\n```\n\n## 작업 5 — 리소스 파기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이전에 추가했던 인스턴스를 제거해 보겠습니다. main.tf에서 이 모듈 호출을 제거한 다음 다시 적용하십시오:\n\n```js\nterraform init\nterraform apply\n```\n\n## 작업 6 — 레지스트리에서 모듈 사용하기\n\nGoogle Network Module를 사용하려고 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```bash\nmodule \"network\" {\n  source  = \"terraform-google-modules/network/google\"\n  version = \"6.0.0\"\n\n  project_id   = var.project_id\n  network_name = \"Use Supplied VPC Name\"\n  routing_mode = \"GLOBAL\"\n\n  subnets = [\n    {\n      subnet_name           = \"subnet-01\"\n      subnet_ip             = \"10.10.10.0/24\"\n      subnet_region         = var.region\n    },\n    {\n      subnet_name           = \"subnet-02\"\n      subnet_ip             = \"10.10.20.0/24\"\n      subnet_region         = var.region\n    }\n  ]\n}\n```\n\n초기화하고 적용하세요:\n\n```bash\nterraform init\nterraform apply\n```\n\n인스턴스 모듈을 업데이트하여 네트워크 매개변수와 서브넷 매개변수를 사용하도록 하십시오.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nvariables.tf:\n\n```js\nvariable \"network\" {\n  description = \"네트워크\"\n  type        = string\n}\n\nvariable \"subnet\" {\n  description = \"서브넷\"\n  type        = string\n}\n```\n\ninstance.tf:\n\n```js\nnetwork_interface {\n  network = var.network\n  subnetwork = var.subnet\n\n  access_config {\n    // 일시적인 공용 IP\n  }\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼 이렇게 인스턴스를 생성하도록 main.tf를 업데이트하세요:\n\n```js\nmodule \"tf_instance_1\" {\n  source        = \"./modules/instances\"\n  instance_name = \"tf-instance-1\"\n  zone          = var.zone\n  region        = var.region\n\n  network       = module.network.network_name\n  subnet        = \"subnet-01\"\n}\n\nmodule \"tf_instance_2\" {\n  source        = \"./modules/instances\"\n  instance_name = \"tf-instance-2\"\n  zone          = var.zone\n  region        = var.region\n  network       = module.network.network_name\n  subnet        = \"subnet-02\"\n}\n```\n\n```js\nterraform init\nterraform apply\n```\n\n## 작업 7 — 방화벽 추가\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 코드를 Korean으로 번역하였습니다:\n\nmd\nmain.tf을 업데이트 해주세요:\n\n```js\nresource \"google_compute_firewall\" \"default\" {\n  name          = \"tf-firewall\"\n  network       = module.network.network_name\n  direction     = \"INGRESS\"\n  source_ranges = [\"0.0.0.0/0\"]\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"80\"]\n  }\n}\n```\n\n그리고 마지막으로 한 번 더 실행해주세요...\n\n```js\nterraform apply\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼 끝났어요!\n","ogImage":{"url":"/assets/img/2024-05-20-BuildInfrastructureonGoogleCloudwithTerraformGoogleChallengeLabWalkthrough_0.png"},"coverImage":"/assets/img/2024-05-20-BuildInfrastructureonGoogleCloudwithTerraformGoogleChallengeLabWalkthrough_0.png","tag":["Tech"],"readingTime":16},{"title":"뒷면에는 Terraform - 운영에 강력한 조합, 개발자에게 훌륭한 도구","description":"","date":"2024-05-20 17:26","slug":"2024-05-20-BackstageandTerraformAPowerfulCombinationforOpsWonderfulforDevs","content":"\nDevOps/Platform 세계가 끊임없이 진화하는 가운데 많은 기업들이 전통적인 도전에 직면하고 있습니다, 특히 클라우드 네이티브 기술을 널리 채용한 기업들은 더 그렇습니다.\n\n아마 당신도 적나라히 알고 계실 인프라스트럭처 자동화(IaC)의 유명한 시나리오를 만난 적이 있을 것입니다.\n\n코드를 통해 클라우드 리소스를 관리하는 IaC는 오늘날 거의 필수적이 되어갔습니다.\n\n시간이 지남에 따라 이 방식이 스스로 입증되어 오고 있지만, 채택이 증가함에 따라 어느 정도 도전이 팀들의 일상에 영향을 끼치기 시작하는 것은 당연한 일입니다. 아마도 개발팀의 일원으로 그 시나리오를 이해할 수 있을 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n클라우드에서 새로운 리소스를 생성해야 할 때 데브옵스/시스템 신뢰성 엔지니어(SRE) 팀에게 리소스를 생성해 달라는 티켓을 열어야 할 필요가 있다는 사실을 깨닫게 될 겁니다. 이는 모든 리소스가 인프라스트럭처를 코드로만(IaC) 생성하기 때문입니다.\n\n이 작업은 기능의 개발 시간에서 상당한 시간을 차지합니다. 아니면 심지어 리소스 생성을 위한 풀 리퀘스트를 직접 작성할 권한이 있더라도 자신이 리소스를 생성하기 위해 새로운 언어(Terraform)를 배워야 한다는 사실에 맞닥뜨릴 수도 있습니다.\n\n혹시 이제는 플랫폼 엔지니어링이라는 용어를 접하고 개발자 포털의 개념을 마주친 적이 있을 수도 있습니다.\n\n또한 Backstage.io에 대해 들어본 적이 있을 겁니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n(이미 확인하지 않은 경우 다른 게시물을 소개해드릴게요 - 링크).\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*sYL2IGcBPy8XNyPu_4wPQg.gif)\n\n포스트 처음에 언급한 문제를 해결하는 데 도움을 주는 소프트웨어 템플릿이라는 기능을 소개하고 싶어요. 이 기능이 여러분에게도 도움이 될 것 같아요.\n\n여러분이 다양한 필드를 갖는 폼을 만들고 자동으로 Terraform 코드를 생성하는 파이프라인을 실행할 수 있다고 상상해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n개발자들에게는 몇 가지 필드가 있는 페이지일 뿐입니다.\n\n유지보수자에게는 필요한 파일을 생성하는 데 도움이 되는 Jinja/Helm과 매우 유사한 템플릿 메커니즘과 YAML 파일이 필요합니다. 그것이 모두 담긴 페이지입니다. 원하는 모든 정보를 담은 풀 리퀘스트를 오픈합니다.\n\n# 그러면, 어떻게 사용하나요?\n\n전체 비밀은 한 파일에 있습니다. 여기서 포맷에 대해서 말씀드리겠습니다. 이 파일에서 양식과 실행 파이프라인을 모두 구성할 것이며, 이 포맷은 Kubernetes Custom Resource Definition (CRD)과 매우 유사한 형식을 따릅니다. 종류, 메타데이터 및 스펙이 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 파일은 4개의 부분으로 나눠져 있어요.\n\n## 메타데이터\n\n이 부분에는 Backstage의 메인 페이지에서 당신의 템플릿을 보여주는 데 사용되는 정보들이 포함되어 있습니다. 제목, 설명, 책임 있는 팀과 함께 문서 링크 및 템플릿을 분류하는 데 사용되는 태그들을 추가할 수 있어요.\n\n## 매개변수\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n매개변수는 데이터 입력 양식이 어떻게 선언될지를 정하는 곳입니다. 이 양식은 내부적으로 react-jsonschema-form을 사용하므로 간단한 텍스트에서 여러 옵션을 가진 드롭다운까지 필드를 생성할 수 있습니다. 또한 텍스트가 민감한 정보인지 여부와 같은 다른 측면을 사용자가 선택한 값의 가능성과 패턴을 아주 잘 제어할 수 있습니다.\n\n이것은 사용자가 선택할 값의 가능성과 패턴을 매우 잘 제어할 수 있기 때문에 매우 유용합니다.\n\nBackstage 문서 자체에는 사용 예제가 몇 가지 있습니다.\n\n좋은 팁은 템플릿 편집기를 사용하는 것입니다. 이렇게 하면 양식이 어떻게 표시될지에 대한 빠른 피드백을 받을 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 또 Backstage를 위해 외부 정보를 얻기 위해 사용자 정의 입력을 생성할 수도 있습니다. 이는 다른 API에 요청을 보내어 드롭다운에서 이를 표시하는 것을 포함합니다.\n\nBackstage 문서에서 자세한 정보를 볼 수 있습니다:\n\n## actions\n\n액션은 플랫폼 팀이 최고의 작업을 보여줄 수 있는 곳입니다. 👩🏽‍💻✨👨🏽‍💻.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<table>\n  <tr>\n    <th>이미지</th>\n    <th>파일 경로</th>\n  </tr>\n  <tr>\n    <td><img src=\"https://miro.medium.com/v2/resize:fit:1286/1*j8k216skDRrfd5bLDbEz6Q.gif\" /></td>\n    <td>Think of it as creating a pipeline.</td>\n  </tr>\n</table>\n\n파이프라인을 생성하는 것으로 생각해보세요.\n\n우리는 기본 Backstage 작업이나 플러그인으로 설치할 수 있는 것들을 사용하여 여러 단계를 만들 수 있습니다.\n\n각 단계에는 다음과 같은 필드가 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- id: 파일의 다른 부분에서 참조하는 데 사용하는 고유 식별자입니다.\n- name: 단계 실행 중 UI에 표시될 텍스트입니다.\n- action: 사용 중인 작업의 식별자입니다.\n- input: 작업에 전달할 수 있는 변수입니다.\n\nGitHub Actions를 사용해 보셨다면 익숙할 것입니다.\n\nBackstage에 있는 모든 작업을 확인하려면 /create/actions 페이지에서 작업의 이름과 받을 수 있는 입력을 볼 수 있습니다.\n\n또한 이러한 작업에서는 양식에 입력된 값들을 $' parameters.field-id ' 구문을 사용하여 입력으로 사용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예시:\n\n![Backstage and Terraform: A Powerful Combination for Ops, Wonderful for Devs](/assets/img/2024-05-20-BackstageandTerraformAPowerfulCombinationforOpsWonderfulforDevs_0.png)\n\n아웃풋은 단계 실행이 끝나고 사용자에게 표시할 수 있는 링크입니다.\n\n이렇게 하면 사용자가 다음 단계를 따르거나 생성된 결과를 확인할 수 있는 링크를 이미 제공할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n좋은 예시는 생성된 Pull Request에 링크를 추가하는 것입니다.\n\n# 이제, 모든 것을 결합하여 실용적인 예시를 살펴봅시다.\n\n이미 Atlantis와 Terraform을 사용하여 인프라 모노레포를 갖고 있다고 가정하고, SQS 큐를 생성하는 GitHub Pull Request의 자동화를 해봅시다.\n\n이를 통해 어떤 팀이든 DevOps/SRE 팀이 만든 패턴을 따라 독립적으로 Pull Request를 생성할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 파일을 Markdown 형식으로 변환하면 다음과 같습니다:\n\n```yaml\napiVersion: scaffolder.backstage.io/v1beta3\nkind: Template\nmetadata:\n  name: sqs-with-dlq\n  title: Standard AWS SQS\n  description: PR to create AWS SQS Queue\n  tags:\n    - infrastructure\n    - terraform\n    - aws\n    - queue\n  links:\n    - title: Documentation\n      url: https://backstage.io/docs/features/software-templates\n      icon: docs\n    - title: Source\n      url: https://github.com.br/gabriel-dantas98/backstage-scaffolders/blob/main/basic-react-app/create-web-app.yaml\n      icon: github\nspec:\n  owner: cloud_platform\n  type: infra-a-code\n\n  parameters:\n    - title: Inform the SQS initial information\n      required:\n        - queue_name\n      properties:\n        queue_name:\n          title: Queue name\n          type: string\n          description: |\n            The name of the queue\n\n        environment:\n          title: The environment the resource is part of\n          type: string\n          description: Which environment your application will be created in is usually related to the AWS account you will be provisioning your resource to.\n          default: prod\n          enum: [prod, staging, dev]\n\n        create_dlq:\n          title: Create a DLQ with redrive policy?\n          type: boolean\n          description: If you desire to create a DLQ with redrive policy\n          default: true\n          ui:widget: radio\n\n    - title: Information about ownership (using in tags)\n      properties:\n        tribe:\n          title: Tribe\n          type: string\n          description: |\n            Select the Tribe owner of resource, we used as tags in AWS\n          ui:field: OwnerPicker\n          ui:options:\n            allowedKinds:\n              - group\n\n        squad:\n          title: Squad\n          type: string\n          description: |\n            Select the Tribe owner of resource, we used as tags in AWS\n          ui:field: OwnerPicker\n          ui:options:\n            allowedKinds:\n              - group\n\n  steps:\n    - id: template\n      name: Render terraform files\n      action: fetch:template\n      input:\n        targetPath: ./templates/outputs\n        url: ./skeleton\n        values:\n          queue_name: ${ parameters.queue_name }\n          create_dlq: ${ parameters.create_dlq }\n          environment: ${ parameters.environment }\n          squad: ${ parameters.squad }\n          tribe: ${ parameters.tribe }\n\n    - id: show_workspace\n      name: Show workspace files\n      action: debug:log\n      input:\n        listWorkspace: true\n\n    - id: terraform_pr\n      name: Create terraform PR\n      action: publish:github:pull-request\n      input:\n        repoUrl: github.com?owner=gabriel-dantas98&repo=piltover-infrastructure\n        branchName: \"sw-template/sqs/${ parameters.queue_name }\"\n        title: \"🔩 Create ${ parameters.queue_name } AWS SQS\"\n        description: |\n          ## Creating SQS ${ parameters.queue_name }\n\n          This is an initial pull request to create an SQS queue and was created based on the Backstage template.\n\n          If you need to add more parameters, check the official documentation - https://registry.terraform.io/modules/terraform-aws-modules/sqs/aws/latest\n\n          *created by: [Backstage Software Template](https://hextech-portal.gdantas.com.br/create)* 👷‍♂️⚙️👷‍♀️\n        sourcePath: ./templates/outputs\n        targetPath: \"aws/production/sqs/${ parameters.queue_name }\"\n\n    - id: label_pr\n      name: Add labels to PR\n      action: github:issues:label\n      input:\n        repoUrl: github.com?owner=gabriel-dantas98&repo=piltover-infrastructure\n        number: \"${ steps.terraform_pr.output.pullRequestNumber }\"\n        labels:\n          - terraform\n          - created-by-backstage\n          - ${ parameters.environment }\n          - sqs\n\n  output:\n    links:\n      - title: \"Go to pull request :D\"\n        url: ${ steps.terraform_pr .output.remoteUrl }\n        icon: github\n      - title: \"To view more check documentation\"\n        icon: docs\n        url: \"https://registry.terraform.io/modules/terraform-aws-modules/sqs/aws/latest\"\n```\n\n실행한 후 다음과 같은 Pull Request가 생성됩니다:\n\n이로써 Atlantis가 팀의 저장소에 적용되어 있으면 별도의 검토를 거쳐 큐 생성을 적용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n글이 더 막대해지지 않도록 몇 가지 Atlantis 구성에 도움이 될만한 링크들을 분리했어요:\n\n# 받았으면 하는 소중한 팁을 공유하고 싶어요\n\n- 가능한 간단하게 하려고 노력해보세요; 이 양식을 작성하는 사람이 회사에 오래 있든 짧게 있든 모두 사용할 수 있다고 생각해보세요.\n- 설명을 자유롭게 사용하세요; 양식을 작성하는 사람에게 커뮤니케이션 및 맥락을 제공하는 최고의 도구입니다.\n- 가능한 경우 Markdown에 구조 및 흐름 이미지를 사용하세요; 이는 더 많은 자신감과 가시성을 부여합니다. 양식을 사용하는 사람들이 도움이 되는 문서에 대한 링크를 삽입할 수 있는 기회를 잡아보세요.\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1000/1*JMykIYYqaU__zxhOqxGPUg.gif\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n앞으로의 게시물에 대한 제안이 있으면 언제든지 환영합니다! 🚀\n\n여기까지 오신 데 감사드립니다! 떠나실 때:\n\n- 👏을 눌러주세요.\n- gdantas.com.br에서 더 멋진 콘텐츠를 만나보세요. 🚀\n- 스타일리시하게 유지하고 싶다면 https://reserva.ink/deployou에서 저희 티셔츠를 확인해보세요. 👕💻\n","ogImage":{"url":"/assets/img/2024-05-20-BackstageandTerraformAPowerfulCombinationforOpsWonderfulforDevs_0.png"},"coverImage":"/assets/img/2024-05-20-BackstageandTerraformAPowerfulCombinationforOpsWonderfulforDevs_0.png","tag":["Tech"],"readingTime":12},{"title":"2클릭으로 Kubernetes 애플리케이션 배포하기  Azure DevOps, Terraform","description":"","date":"2024-05-20 17:25","slug":"2024-05-20-Deployingkubernetesapplicationswith2-clicksAzureDevOpsTerraform","content":"\n제목을 보고 클릭베이트일 것이라고 생각할 수 있지만, 이 기사를 끝까지 읽어보는 것이 좋습니다. 인프라를 코드로 생성하면 Azure DevOps와 테라폼을 사용하여 k8s 애플리케이션을 배포하는 것이 매우 쉬워질 수 있음을 확인할 수 있습니다.\n\n이 예제에서는 Azure DevOps 파이프라인과 테라폼을 활용하여 Azure에서 실행되는 AKS 클러스터에 yaml 정의를 배포할 것입니다. 이를 위해 세 단계가 필요합니다.\n\n첫 번째 단계는 Azure에서 AKS 클러스터를 생성하는 것입니다. 인프라가 준비되면 Azure DevOps 파이프라인을 AKS 리소스와 바인딩하여 클러스터에 배포할 수 있도록 해야 합니다. 마지막 단계는 배포하고 실행할 애플리케이션의 yaml 정의를 가져와 Azure DevOps 내에서 애플리케이션 배포 프로세스를 실행하는 것입니다.\n\n![이미지](/assets/img/2024-05-20-Deployingkubernetesapplicationswith2-clicksAzureDevOpsTerraform_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n프로젝트 구조는 아래 그림과 같이 구성되어 있습니다.\n\n- 코드 폴더에는 yaml k8s 정의 파일이 포함되어 있습니다.\n- iac_aks는 Azure 내에서 AKS 클러스터를 생성합니다.\n- iac_devops는 필요한 Azure DevOps 리소스(AKS와의 서비스 연결)를 생성합니다.\n- 마지막으로 azure-pipeline 및 application-pipeline은 자동화를 실행하고 작업을 수행할 파이프라인입니다.\n\n예제를 시도해 보려면 먼저 Azure DevOps 내에 변수 그룹을 생성하고 두 가지 값을 저장해야 합니다. 첫 번째 값은 Azure DevOps 리소스를 만들 때 사용할 비밀 개인 액세스 토큰입니다. 두 번째 값은 Azure DevOps 조직의 URL입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 마크다운 형식으로 변경한 코드입니다:\n\n![Deploying Kubernetes applications with 2 clicks Azure DevOps Terraform](/assets/img/2024-05-20-Deployingkubernetesapplicationswith2-clicksAzureDevOpsTerraform_2.png)\n\n위 설정이 완료되면 tfvars 파일을 변경하고, 생성할 리소스에 사용하고자 하는 이름을 추가해야 합니다. 마지막으로 인프라 파이프라인을 클릭 한 번, 애플리케이션 파이프라인을 클릭 한 번으로 배포를 끝내실 수 있습니다.\n\n![Deploying Kubernetes applications with 2 clicks Azure DevOps Terraform](/assets/img/2024-05-20-Deployingkubernetesapplicationswith2-clicksAzureDevOpsTerraform_3.png)\n\n코드는 Github에 호스팅되어 있습니다.\nhttps://github.com/geralexgr/globalazuregreece2024\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nhttps://blog.geralexgr.com에 2024년 5월 19일에 원래 게시되었습니다.\n더 많은 기사를 보려면 내 계정을 팔로우해보세요. Medium의 모든 이야기에 완전 액세스하려면 회원이 되어주세요.\n","ogImage":{"url":"/assets/img/2024-05-20-Deployingkubernetesapplicationswith2-clicksAzureDevOpsTerraform_0.png"},"coverImage":"/assets/img/2024-05-20-Deployingkubernetesapplicationswith2-clicksAzureDevOpsTerraform_0.png","tag":["Tech"],"readingTime":3},{"title":"프로메테우스 부하와 카디널리티 대폭 줄이는 방법 필요한 이스티오 레이블만 사용하기","description":"","date":"2024-05-20 17:24","slug":"2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed","content":"\n![image](/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_0.png)\n\n만약 프로메테우스 작업에 노출되어 있다면, 카디널리티 관리가 중요하다는 것을 들어봤을 것입니다. 이것은 관측 구성의 가장 중요한 측면 중 하나이며 시스템 부하를 크게 증가시킬 수 있습니다. Robust Perception에는 왜 이것이 중요한지에 대해 좋은 설명이 있으므로, 왜 이것이 중요한지에 대해 자세히 다루지는 않겠지만, 가장 비용이 많이 드는 메트릭 중 일부를 다룰 수 있도록 메트릭을 조정하는 한 가지 방법을 설명하겠습니다.\n\n구체적으로 이스티오에 대해 이야기할 것입니다. 현재 우리는 v1.18을 사용 중이므로, 참고하셔야 할 필드 중 일부는 이전 버전에서 (또는 신규한 버전에서) 다르게 보일 수 있음을 주의해 주세요. 이 버전에서 예제를 정확히 찾는 데 어려움을 겪어 예제가 유용하게 될 것을 희망합니다.\n\n# 재미있는 부분으로 넘어가보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우선, 우리가 이야기하는 것은 Istio (그리고 간접적으로 Kubernetes)입니다. 우리가 조정하고 있는 구체적인 구성 요소는 Istio Operator입니다.\n익숙하지 않다면, Istio Operator는 istioOperator 사용자 지정 리소스를 관리하는 Kubernetes 컨트롤러입니다.\n그 결과로 Istio Operator는 클러스터에서 Istio 리소스를 만들고 지속적으로 조정합니다.\n이에는 Gateway 리소스, Envoy Filters, Pilot (istiod) 등이 포함됩니다.\n\nIstio 문서에는 우리가 작업하는 기능을 참조하는데, 사용 방법에 대한 예시를 충분히 제공하지 않는 단 한 줄이 있습니다.\n\n그래서 우리는 설정해야 할 istioOperator 매니페스트가 있습니다. 그리고 metric 라벨을 덮어쓰기 위해 tags_to_remove 옵션을 사용한다는 것을 알고 있습니다.\n\n## 참고: Istio 구성을 업데이트할 때는 항상 실제 운영 환경이 아닌 환경에서 테스트해야 합니다.\n\n메트릭 라벨에 대해 이야기하고 있더라도 의도치 않은 영향이 발생할 수 있습니다.\n예를 들어, 사용자 지정 메트릭을 사용하여 스케일링 동작을 정의하는 경우 전체 클러스터의 스케일링 동작을 망가뜨릴 수 있습니다.\n이것은 한 가지 예시일 뿐이지만, 이것을 염두에 두세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_1.png\" />\n\n이제, 실제로 이 구성 변경이 어떻게 보이는지는 명확하지 않습니다. 그래서 여기에 있습니다. 원하는 구성은 inboundSidecar 및 outboundSidecar 속성 모두에 대해 설정됩니다. istiooperator 매니페스트에서 전체 경로는 다음과 같습니다:\n\nspec.telemetry.v2.prometheus.configOverride.inboundSidecar.metrics\n\nspec.telemetry.v2.prometheus.configOverride.outboundSidecar.metrics\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래와 같이 더 완전한 예시를 살펴보세요 (이 이야기에서 필요한 구성만을 간략화하여 포함하였음을 유의해주세요):\n\n```js\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  namespace: ${istio_namespace}\n  name: ${istio_name}\nspec:\n  values:\n    telemetry:\n      enabled: true\n      v2:\n        prometheus:\n          wasmEnabled: true\n          configOverride:\n            inboundSidecar:\n              metrics:\n              - name: requests_total\n                tags_to_remove:\n                - connection_security_policy\n                - destination_cluster\n                - destination_canonical_revision\n                - destination_canonical_service\n                - destination_principal\n                - destination_version\n                - destination_service_name\n                - destination_service_namespace\n                - destination_workload_namespace\n                - source_canonical_service\n                - source_canonical_revision\n                - source_workload_namespace\n                - source_version\n                - source_cluster\n                - source_principal\n              - name: request_bytes\n                tags_to_remove:\n                - destination_cluster\n                - destination_canonical_revision\n                - destination_canonical_service\n                - destination_principal\n                - destination_version\n                - destination_service_name\n                - destination_service_namespace\n                - destination_workload_namespace\n                - source_canonical_service\n                - source_canonical_revision\n                - source_workload_namespace\n                - source_version\n                - source_cluster\n                - source_principal\n                - service\n                - pod\n```\n\n# 이제 무엇을 해야 할까요?\n\nIstio Operator 구성이 업데이트되었습니다. 멋져요! 하지만… 이제 무엇을 해야 할까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아마도 별도의 운영 환경에 액세스할 수 있고 이 업데이트된 매니페스트를 해당 Kubernetes 클러스터에 적용할 수 있을 것으로 예상됩니다. Istio 오퍼레이터 파드의 로그를 추적하여 클러스터 리소스를 동기화하는 것을 확인할 수 있습니다. 구체적으로, Envoy 필터를 업데이트해야 합니다.\n\n만약 구문 오류가 없다면 로그에 다음과 같은 내용이 표시될 것입니다:\n\n```js\n2024-03-18T17:54:21.303088Z info installer 생성된 매니페스트와 캐시 사이에 다음 오브젝트가 다릅니다:\n - ConfigMap:istio-system:istio\n - EnvoyFilter:istio-system:stats-filter-1.16\n - EnvoyFilter:istio-system:tcp-stats-filter-1.16\n - EnvoyFilter:istio-system:stats-filter-1.17\n - EnvoyFilter:istio-system:tcp-stats-filter-1.17\n - EnvoyFilter:istio-system:stats-filter-1.18\n - EnvoyFilter:istio-system:tcp-stats-filter-1.18\n2024-03-18T17:54:21.303181Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/stats-filter-1.16\n- 제거된 리소스를 가지치기 중\n2024-03-18T17:54:21.333325Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/stats-filter-1.17\n2024-03-18T17:54:21.352880Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/stats-filter-1.18\n2024-03-18T17:54:21.372182Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/tcp-stats-filter-1.16\n2024-03-18T17:54:21.392004Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/tcp-stats-filter-1.17\n2024-03-18T17:54:21.409610Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/tcp-stats-filter-1.18\n2024-03-18T17:54:21.433381Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: ConfigMap/istio-system/istio\n```\n\n전체적으로 너무 조심스러운 면이 있다고 생각되어 istio-system 네임스페이스에 있는 모든 것을 재시작하는 기회를 가졌습니다. 이는 모든 게이트웨이 및 istiod를 포함합니다. 이것이 완전히 필요한지는 모르지만, 제가 한 작업입니다. 변경 사항을 적용한 모든 클러스터에서 트래픽을 제거한 다음 변경을 적용하고 다시 시작했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n변경 사항을 적용하는 데 한 가지 더: 제 네임 스페이스 중 일부에서 메트릭 레이블 업데이트가 즉시 적용되지 않는 것을 발견했습니다. 이스티오 프록시 컨테이너가 실제로 Envoy 필터에 변경 사항을 적용하기 전에 다시 시작해야 한다는 이유로 이해됩니다. 100% 사실인지는 모르지만, 다시 말하지만 — 제 네임 스페이스 중 일부에서 제 관측 결과였습니다. 이것이 이례적으로 느껴지지는 않지만, 변경 사항을 할 때 주의하고 확인하는 것이 좋습니다.\n\n# 작별 인사와 예시\n\nKubernetes 클러스터가 서비스로의 매우 많은 요청을 허용한다면, 카디널리티를 줄이는 데 얻게 될 이득은 상당할 것입니다. 레이블을 제대로 검토하는 데 시간이 걸리지만(prometheusRules, 대시보드 등을 확인), 이 작업은 귀중합니다.\n\n고 카디널리티 레이블을 다음과 같이 줄일 때:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n{\n    \"container\":\"istio-proxy\",\n    \"destination_canonical_revision\":\"<dstUID>\",\n    \"destination_canonical_service\":\"<dstApp>\",\n    \"destination_cluster\":\"Kubernetes\",\n    \"destination_service\":\"<dstApp>.<dstApp>-production.svc.cluster.local\",\n    \"destination_service_name\":\"PassthroughCluster\",\n    \"destination_service_namespace\":\"<dstApp>-production\",\n    \"destination_version\":\"<dstUID>\",\n    \"destination_workload\":\"<dstUID>\",\n    \"destination_workload_namespace\":\"<dstApp>-production\",\n    \"instance\":\"<someIP>\",\n    \"job\":\"<appName>\",\n    \"le\":\"0.5\",\n    \"namespace\":\"<appName>-production\",\n    \"pod\":\"<srcUID>-kkd77\",\n    \"reporter\":\"source\",\n    \"request_protocol\":\"http\",\n    \"response_code\":\"200\",\n    \"response_flags\":\"-\",\n    \"service\":\"<appName>\",\n    \"source_app\":\"<appName>\",\n    \"source_canonical_revision\":\"<srcUID>\",\n    \"source_canonical_service\":\"<appName}\",\n    \"source_cluster\":\"Kubernetes\",\n    \"tag\":\"<srcUID>\"\n}\n```\n\n위를 아래의 형식으로 변경해주세요.\n\n```js\n{\n    \"reporter\": \"destination\",\n    \"source_workload\": \"<srcUID>\",\n    \"source_app\": \"<srcApp>\",\n    \"destination_workload\": \"<dstUID>\",\n    \"destination_app\": \"<dstApp>\",\n    \"destination_service\": \"<dstApp>.<dstApp>-production.svc.cluster.local\",\n    \"request_protocol\": \"http\",\n    \"job\":\"<appName>\",\n    \"response_code\": \"200\",\n    \"grpc_response_status\": \"\",\n    \"response_flags\": \"-\",\n    \"le\": \"1\"\n}\n```\n\nS3 저장 비용이 크게 절감되었고, 단 몇 일 만에 압도적인 성능 향상을 볼 수 있었습니다. 또한 Prometheus 인스턴스에서 리소스 사용량이 줄어들었습니다. 다른 말로 — 요청 당 메트릭에서 불필요한 레이블을 식별하고 제거하는 것은 절대적으로 가치 있는 작업입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 작성한 워크스루가 몇몇 프로메테우스 운영자분들께서 관측성 스택을 최적화하는데 도움이 되길 바라요!\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_0.png","tag":["Tech"],"readingTime":10},{"title":"Kubernetes에서 Jenkins, Docker, Harbor Private Repository 및 ArgoCD를 사용하여 CICD 파이프라인으로 애플리케이션을 빌드하고 배포하는 방법","description":"","date":"2024-05-20 17:22","slug":"2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD","content":"\n오늘날의 소프트웨어 개발 환경에서 고품질 소프트웨어를 효과적으로 생산하기 위해서는 CI/CD 파이프라인이 필수적입니다. 이 글에서는 Jenkins, Docker 이미지, Harbour 프라이빗 저장소, Git 및 ArgoCD를 사용하여 지속적인 통합/배포 파이프라인(CI/CD)을 어떻게 설정하는지 설명합니다. 이를 통해 이미지를 원활하게 생성하고 harbor에 푸시한 다음 쿠버네티스 파드에 자동으로 배포할 수 있습니다.\n\n# 시작하기\n\n일반적인 작업 흐름은 다음과 같습니다.\n\n1. Docker 파일 생성: 먼저 Docker 파일을 만들어야 합니다. 이 파일에서는 DevOps 엔지니어가 애플리케이션에 필요한 환경 및 종속성을 지정합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. 리포지토리에 코드를 커밋합니다: Git과 같은 리포지토리는 소스 코드와 Docker 파일을 저장합니다. 저는 GitHub에 이 리포지토리를 저장할 것입니다.\n\n3. 젠킨스가 코드를 가져옵니다: 우리는 젠킨스 작업을 시작하여 소스 코드 리포지토리에서 가장 최근의 코드를 가져와 빌드 프로세스를 시작할 수 있습니다.\n\n4. 도커 이미지 빌드 및 푸시: 젠킨스는 Docker 파일을 사용하여 Docker 이미지를 빌드합니다. 젠킨스는 완료된 이미지를 Harbour 개인 리포지토리에 푸시합니다. Harbour 개인 리포지토리를 빌드하는 방법은 이 곳에서 확인할 수 있습니다.\n\n5. 배포 리포지토리 업데이트: 젠킨스는 애플리케이션에 필요한 쿠버네티스 배포 구성이 포함된 배포 파일 리포지토리에 변경 사항을 가합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n6. ArgoCD와 동기화: 모든 것이 최신 상태인지 확인하기 위해 ArgoCD라는 Kubernetes 연속 전달 도구는 배포 소스에서 가장 최근 업데이트를 검색합니다.\n\n7. Kubernetes로 배포: 마지막으로, ArgoCD는 모든 기능이 구성된 것을 보장하고, 이러한 수정 사항을 Kubernetes 클러스터와 동기화하여 응용 프로그램을 지정된 pod에 배포합니다.\n\n## 작업 흐름\n\n# 구현\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 필요한 도구\n\n- Kubernetes 클러스터\n- Docker\n- Jenkins\n- Harbor\n- ArgoCD\n- GitHub\n\n## 1. Docker 파일 생성하기\n\n아래에 언급된 Docker 파일을 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 index.html 파일을 사용하고 있습니다.\n\nGitHub 저장소에서 모든 파일을 찾을 수 있습니다: https://github.com/tanmaybhandge/Harbor_CICD_Pipeline.git\n\n## 2. Jenkins 작업 트리거\n\n저희는 Jenkins에서 두 개의 작업을 구성했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n첫 번째 작업(상위 작업)은 최신 소스 코드를 얻기 위해 GitHub 저장소를 복제하는 작업을 포함합니다. 이 코드에서 Docker 이미지가 생성됩니다. 그런 다음 이미지를 빌드하고 Harbor 프라이빗 레지스트리에 로그인한 후 이미지를 푸시하고 상위 작업의 빌드 번호와 태깅합니다.\n\n이미지가 성공적으로 푸시된 후 상위 작업에 의해 하위 작업이 시작됩니다. 하위 작업은 빌드 번호를 매개변수로 받고 해당 빌드 번호가 될 것입니다. 그런 다음 하위 작업은 이미지 버전을 빌드 번호와 동일하게 수정한 다음 쿠버네티스 배포 파일에 변경 사항을 푸시할 것입니다.\n\n상위 작업과 하위 작업이란 무엇을 의미합니까?\n\n한 Jenkins 작업이 다른 작업을 시작할 때, 프로세스를 시작하는 작업을 상위 작업이라고하고, 결과로 시작되는 작업을 하위 작업이라고합니다. 아래 예시에서 Build_Docker_Image_Push_Harbor은 상위 작업이고 push_image_tag_git은 하위 작업입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 작업: 도커 이미지 빌드 및 Harbor로 푸시\n\n파이프라인 작업 \"Build_Docker_Image_Push_Harbor\"을 생성하고 아래 스크립트를 붙여넣으십시오.\n\n이 Jenkins 파이프라인 스크립트는 GitHub 저장소에서 Docker 이미지를 빌드하고, 해당 이미지를 Harbor 저장소로 푸시하며, 다른 Jenkins 작업 \"push_image_tag_git\"을 트리거하여 GitHub에서 배포 파일을 수정하고 변경 사항을 커밋하는 프로세스를 자동화합니다.\n\n```js\npipeline {\n    agent any\n\n    stages {\n        stage('Build') {\n            steps {\n                // GitHub 저장소에서 코드 가져오기\n                git branch: 'main', url: 'https://github.com/tanmaybhandge/Harbor_CICD_Pipeline.git'\n                sh 'docker build -t library/harbor_cicd_v2 .'\n            }\n        }\n        stage('Harbor로 푸시') {\n            environment {\n                DOCKER_CREDENTIALS = credentials('Harbor')\n            }\n            steps {\n                script {\n                    // 인증 정보를 사용하여 Harbor에 로그인\n                    sh \"docker login -u ${DOCKER_CREDENTIALS_USR} -p ${DOCKER_CREDENTIALS_PSW} 10.1.1.1\"\n\n                    // 이미지에 태그 추가\n                    sh 'docker tag library/harbor_cicd_v2 10.1.1.1/library/harbor_cicd:v${BUILD_NUMBER}'\n\n                    // Harbor로 이미지 푸시\n                    sh 'docker push 10.1.1.1/library/harbor_cicd:v${BUILD_NUMBER}'\n                }\n            }\n        }\n        stage('GitHub 푸시 트리거') {\n            steps {\n                build job: 'push_image_tag_git', wait: true, parameters: [string(name: 'Build_Number_Image', value: \"${BUILD_NUMBER}\")]\n            }\n        }\n    }\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n작업 \"Build_Docker_Image_Push_Harbor\"에서는 선택된 체크박스가 없습니다. 위 스크립트를 그대로 Pipeline 스크립트에 붙여넣기만 하면 됩니다.\n\n![이미지](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_0.png)\n\n## 스테이지\n\n각 스테이지에 대한 설명입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nBuild\n\n- Git Checkout: 지정된 URL에서 저장소를 복제하고 'main' 브랜치를 체크아웃합니다.\n- Docker Build: 저장소의 Dockerfile을 사용하여 Docker 이미지를 빌드하고 library/harbor_cicd_v2로 태깅합니다.\n\nHarbor로 푸시\n\n- 환경 변수: Jenkins에 저장된 'Harbor' ID를 사용하여 Docker 자격 증명을 가져옵니다. 이미 Jenkins에서 Harbor 자격 증명을 만들었습니다 (Harbor의 기본 자격 증명: 사용자 admin, 비밀번호 Harbor12345).\n- Docker Login: 검색한 자격 증명을 사용하여 Harbor 레지스트리에 로그인합니다.\n- Docker Tag: 빌드된 Docker 이미지에 Jenkins 빌드 번호를 포함한 새 태그를 추가합니다. 현재 파이프라인의 태그로 빌드 번호를 사용할 것입니다.\n- Docker Push: 태그가 붙은 Docker 이미지를 지정된 Harbor 저장소로 푸시합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGitHub Push를 트리거합니다.\n\n- 또 다른 작업 트리거: push_image_tag_git 작업을 시작합니다.\n- 매개변수 전달: 현재 빌드 번호를 트리거된 작업에 전송합니다.\n\n## 작업: Push_Image_Tag_Git\n\n이 작업은 GitHub에 호스팅된 Kubernetes 배포 YAML 파일을 수정하여 상위 작업에서 푸시된 새 이미지 태그를 포함시킵니다. 이를 위해 다음이 필요합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Parameterized Trigger Plugin\n- GitHub Plugin\n- push access 권한이 있는 GitHub 자격 증명\n\n작업의 구성은 다음과 같습니다.\n\npush_image_tag_git이라는 이름의 Freestyle 프로젝트를 생성합니다.\n\nA. This project is parameterised 옵션이 선택되었는지 확인한 후, 문자열 매개변수를 적절하게 구성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지1](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_1.png)\n\nB. 소스 코드 관리 섹션에서 Git을 선택하고 저장소 URL을 붙여넣은 다음 GitHub에 권한 및 액세스 권한이 있는 적절한 자격 증명을 선택합니다. 빌드할 브랜치 섹션에 브랜치를 지정하세요. 저는 기본 메인 브랜치를 사용합니다.\n\n![이미지2](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_2.png)\n\nC. 빌드 단계에 실행 셸을 추가하고 다음을 붙여넣으세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 스크립트는 Deployment.yaml 파일에서 이미지 태그를 찾아 해당 버전을 Jenkins 빌드의 ($ 'Build_Number_Image')로 바꿉니다. 또한 Git 커밋을 위해 이메일과 이름을 구성하고, 수정된 모든 파일을 Git 스테이징 영역에 추가하고, 설명적인 메시지로 변경 사항을 커밋합니다.\n\nD. 'Post-Build Actions' 섹션에서 '빌드 성공 시에만 푸시'를 선택한 다음 'Branches' 필드 아래에서 브랜치 이름과 대상 원격 이름을 지정합니다. 저는 브랜치 이름을 'main', 대상 원격 이름을 'origin'으로 지정했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_4.png\" />\n\n## 3. 도커 빌드 작업을 트리거합니다.\n\n수동으로 Build_Docker_Image_Push_Harbor를 트리거하면 아래 작업을 수행하고 Push_image_Tag_Git 작업을 트리거합니다. 두 작업은 다음을 수행합니다.\n\nBuild_Docker_Image_Push_Harbor\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 빌드 단계: GitHub 저장소를 복제하고 Docker 이미지를 빌드합니다.\n- Harbor에 푸시하는 단계: Harbor 레지스트리에 로그인하여 이미지에 빌드 번호를 태깅하고 푸시합니다.\n- GitHub 푸시 트리거 단계: 하향 작업인 Push_image_Tag_Git을 트리거하고 빌드 번호를 매개변수로 전달합니다.\n\nPush_image_Tag_Git\n\n- 쉘 실행: Deployment.yaml 파일에서 이미지 태그를 찾아 이전 젠킨스 빌드에서 지정된 버전으로 대체하고, Git 커밋을 위해 이메일 및 이름을 구성하고, 변경된 모든 파일을 Git 스테이징 영역에 추가하여 설명적인 메시지로 변경 사항을 커밋합니다.\n\n여기 Status 출력 결과입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_5.png\" />\n\n도커 이미지 빌드 및 쿠버네티스 배포 간의 동기화를 보장하기 위해 Deployment.yaml 파일에 지정된 버전이 Build_Docker_Image_Push_Harbor 중에 생성된 빌드 번호를 반영하도록 업데이트됩니다.\n\n<img src=\"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_6.png\" />\n\n## 4. 더 새로운 이미지 버전으로 Pods 다시 생성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nArgoCD 애플리케이션을 설정하여 deployment.yaml 파일을 주시하고 Kubernetes 클러스터에서 모든 것이 동기화되고 최신 상태를 유지하도록 하였습니다. 따라서 우리의 deployment.yaml에서 조정이 발생할 때마다 ArgoCD가 바로 개입하여 손을 거치지 않고도 Kubernetes 배포 환경이 동기화되도록 합니다.\n\n다음은 ArgoCD 애플리케이션의 구성 세부 정보입니다:\n\n- 클러스터: https://kubernetes.default.svc\n- 네임스페이스: webapp\n- 저장소 URL: https://github.com/tanmaybhandge/Harbor_CICD_Pipeline.git\n- 경로: Deployment\n\n쿠버네티스 클러스터에 webapp 네임스페이스를 생성해야 할 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_7.png)\n\n![image](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_8.png)\n\n## 5. 포드에 액세스하기\n\nArgoCD 배포가 모두 정상적으로 보인다면, 배포된 애플리케이션을 테스트할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n노드 포트 서비스를 통해 배포를 노출했습니다. 테스트 목적으로 사용할 수 있습니다. 어플리케이션에 액세스하려면 워커 노드의 IP 주소와 서비스 포트를 사용하십시오. 이 설정을 통해 웹페이지에 쉽게 액세스하여 앱의 기능을 테스트하고 유효성을 검사할 수 있습니다.\n\nMarkdown 형식의 테이블입니다.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\nspec:\n  selector:\n    app: webapp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n```\n\n![웹페이지](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_9.png)\n\n여기까지 오신 것을 환영합니다! 즐거운 독해가 되었으면 좋겠습니다. 궁금한 점이 있거나 연락하고 싶다면 언제든지 LinkedIn에서 연락해 주세요. 즐거운 하루 보내세요!\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_0.png","tag":["Tech"],"readingTime":13},{"title":"Kubernetes에서 JVM 애플리케이션 실행하기 java -jar을 넘어서","description":"","date":"2024-05-20 17:17","slug":"2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar","content":"\n## 쿠버네티스로 오케스트레이션된 컨테이너 환경에서 JVM 애플리케이션을 실행하는 중요한 팁 몇 가지를 발견하세요\n\n[이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_0.png)을 확인해보세요.\n\n## 대상 독자\n\n쿠버네티스 상의 컨테이너 환경에서 JVM 애플리케이션을 실행하고 있고 성능 및 비용을 더 잘 최적화할 수 있을 것 같다고 느끼시나요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여러분은 JVM 애플리케이션을 이러한 환경으로 이전하는 것을 고려 중이지만 효율적으로 어떻게 할지 고민하고 계신가요?\n\n쿠버네티스에서 실행되는 JVM 애플리케이션의 지연시간 및 처리량 문제에 고민하고 계신가요?\n\n또는 간단히 쿠버네티스 환경에서 JVM 애플리케이션을 최적화하는 방법에 대해 더 알고 싶으신가요?\n\n그럼, 이 게시물이 여러분을 위한 것입니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 시작하기 전에, 몇 가지 매우 구체적인 시나리오에서는 아래 제공된 모든 팁이 합리적이지 않을 수 있다는 점을 명확히 해야 합니다. 그러나 대부분의 경우에는 쿠버네티스로 조정된 컨테이너화된 환경 내에서 JVM 내에서 실행 중인 서비스가 있는 사용 사례에서 다음 정보가 매우 유용할 것으로 믿습니다.\n\n다음에 제시할 팁을 더 잘 이해하기 위해 몇 가지 기본 개념을 맞추는 것이 중요합니다:\n\n- Java (프로그래밍 언어) 대 Java 플랫폼/JVM: 자바 프로그래밍 언어를 자바 플랫폼과 혼동해서는 안됩니다. 요즘에는 JVM이 자바 이외의 Kotlin, Scala, Groovy, Clojure 등과 같은 여러 프로그래밍 언어를 실행하고 지원할 수 있습니다. 이 글에서는 프로그래밍 언어 자체보다는 실행 환경(런타임)과 관련하여 주로 JVM에 대해 논의할 것입니다.\n\n![image](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그런 면에서 팁으로 넘어가 보겠습니다.\n\n## 1) 에르고노믹스: 영웅과 악당\n\n오늘날, 단순히 java -jar 명령을 실행하여 JVM 애플리케이션을 실행할 때, JVM에는 실행 환경을 기반으로 적절한 구성을 찾으려는 에르고노믹스라는 기능이 있습니다.\n\n처음에는 아주 좋은 것 아닌가? 라고 생각할 수 있습니다. 그러나 이에 대한 답은: 상황에 따라 다릅니다. 이 기능은 세밀한 튜닝에 대해 걱정하지 않고 애플리케이션을 빌드하고 실행하는 데 도움이 될 수 있지만, 대규모 환경에서는 결과가 최적이 아닐 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJVM에서 Ergonomics가 수행하는 일부 자동 구성 조정 사항 중에는 성능 및 자원 소비에 직접 영향을 미칠 수 있는 Garbage Collector 선택 및 Heap 크기와 관련된 것이 있습니다. 함께 조금 더 자세히 살펴보겠습니다.\n\nGarbage Collector 선택\n\nGC의 선택은 두 가지 조건에 기반합니다: JVM에서 사용 가능한 메모리 양과 CPU의 개수입니다.\n\n규칙은 다음과 같이 작동합니다:\nJava 8 이전: CPU 수가 2 이상이고 메모리 양이 1792MB보다 크면 선택된 GC는 ParallelGC가 됩니다. 이 두 조건 중 하나라도 위의 값보다 낮은 경우에는 선택된 GC가 SerialGC가 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJava 9부터: 조건은 사실상 동일하지만 CPU 수가 2 이상이고 메모리 양이 1792MB보다 큰 경우에는 ParallelGC 대신 G1GC가 선택된 GC로 지정됩니다.\n\n![이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_2.png)\n\n힙 크기\n\n일부 예외를 제외하고 대부분의 경우, -Xmx 매개변수나 -XX:MaxRAMPercentage 플래그를 사용하여 원하는 힙 크기를 지정하지 않을 때, Ergonomics는 사용 가능한 메모리의 ¼를 최대 힙 값으로 구성합니다. 예를 들어, 컨테이너의 메모리 제한이 2GB로 설정된 경우, Ergonomics는 최대 힙 크기를 512MB로 구성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n언급된 예외 사항은 다음과 같습니다:\n\n- 컨테이너에 최대 256MB의 사용 가능한 메모리가 있는 경우, 최대 힙 값은 50%가 됩니다.\n- 컨테이너에 256MB에서 512MB 사이의 사용 가능한 메모리가 있는 경우, 최대 힙 값은 약 127MB가 됩니다.\n\n하지만 이 모든 것이 미치는 영향은 무엇일까요?\n\nGC의 자동 선택에 관해, SerialGC는 장기간 일시 중지 시간을 발생시켜 고출력 서버 환경에서 성능이 좋지 않을 수 있다는 점을 염두에 두어야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만, 여기서 궁금증이 생길 수도 있어요: 내 컨테이너가 1000m의 제한을 가지고 있고 결과적으로 JVM은 하나의 CPU만 사용 가능한 상황에서, 멀티 스레딩 리소스를 활용하는 GC를 사용하는 장점은 무엇일까요?\n\n이에 대한 답변은 구체적인 세부 사항에 대해서는 들어가지 않겠지만, 요약하자면, 컨테이너 관점에서 혼란이 있습니다; 많은 사람들이 1000m(1000 밀리코어)을 1 CPU로 오해합니다. 그러나 1000m은 계산 용량 시간을 나타내며, 이는 노드의 모든 사용 가능한 CPU들 사이에서 분산될 수 있는 것입니다.\n\n이러한 혼란이 생기는 이유는 JVM이 1000m을 1개의 사용 가능한 CPU로 해석하기 때문입니다. 1001m는 2개의 CPU로, 2001은 3개로 해석됩니다.\n\n이를 알고 있으면, JVM이 CPU가 1000m으로 제한된 컨테이너에서도 멀티 스레딩 GC를 활용할 수 있습니다. 이를 달성하기 위해, -XX:ActiveProcessorCount 플래그를 사용하여 JVM의 사용 가능한 CPU 수를 강제로 설정할 수 있으며, 이때 값은 1보다 큰 값을 전달합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSerialGC 외에도 1000m 이하로 다른 GC를 사용할 수 있지만, 사용 사례에 따라 CPU 사용 가능성이 낮아 성능이 저하될 수 있음을 염두에 두는 것이 중요합니다. 사용 가능 할당량(CFS 할당량)을 짧게 초과하여 응용 프로그램 쓰로틀링이 발생할 수 있습니다.\n\nJVM이 사용 중인 GC 구현을 확인하려면 kubectl exec를 사용하여 컨테이너에 액세스하고 다음 명령을 실행하십시오:\n\n```js\njava -XX:+PrintFlagsFinal -version | grep “Use*.*GC “\n```\n\n아래 예시와 유사한 출력을 받게 되며, 사용되고 있는 구현이 부울 값으로 표시됩니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_3.png)\n\n출력물에서 볼 수 있는 예제를 통해 사용 중인 구현은 SerialGC이며, Ergonomics를 통해 구성되었습니다.\n\n힙 크기에 대해, 사용 가능한 메모리의 ¼만 사용한다면, 우리는 자원을 낭비할 수 있습니다. 컨테이너는 애플리케이션을 실행하기 위해 구축된 것이며, 나머지 메모리를 소비할 병렬 프로세스는 가지고 있으면 안 됩니다. 그러나 항상 기억해야 할 중요한 것은 JVM이 힙만으로 구성되지 않으며, 힙 외의 다른 구성 요소도 있어서 우리는 이를 비힙으로 부르고 있습니다. 힙과 비힙 뿐만 아니라 '운영 체제'와 관련된 일부 프로세스도 메모리를 소비하나 작은 양이지만 있습니다. 다음 팁에서 살펴보겠습니다.\n\n![이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJVM의 최대 힙 크기를 확인하려면 kubectl exec를 사용하여 컨테이너에 액세스하고 다음 명령을 실행하세요:\n\n```js\njava -XX:+PrintFlagsFinal -version | grep “ MaxHeapSize”\n```\n\n다음과 같이 예제와 비슷한 출력을 받게 될 것입니다:\n\n<img src=\"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_5.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 예시에서 보는 것처럼, MaxHeapSize가 바이트로 표시되었고 Ergonomics를 통해 구성되었습니다.\n\n팁 요약:\n\n- 고유련한 서버 환경에서 SerialGC를 사용하지 않도록 주의하세요. JVM이 1개의 CPU에만 제한되지 않도록합니다. 이를 달성하는 여러 방법이 있습니다. 예를 들어 컨테이너의 CPU 제한을 1001m 이상으로 조정하거나 -XX:ActiveProcessorCount 플래그를 1보다 큰 값으로 사용하는 등입니다.\n- 컨테이너의 사용 가능한 메모리가 1792MB 미만이고 특정 GC 버전을 강제하지 않으면, Ergonomics가 SerialGC를 선택할 수도 있음을 인식하세요.\n- JVM 인수를 통해 원하는 GC 구현을 지정할 수도 있습니다. 권장 사항으로는 4GB 이하의 힙에는 ParallelGC를 사용하고 4GB 이상의 힙에는 G1을 사용합니다. 더 많은 GC 구현이 있지만, 대부분의 사용 사례를 커버해야 합니다. 특정 GC 사용을 강제하는 예시 몇 가지는 다음과 같습니다:\n\n```js\n# Serial GC\njava -XX:+UseSerialGC -jar meuapp.jar\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 병렬 GC\njava -XX:+UseParallelGC -jar meuapp.jar\n# 병렬 G1 GC\njava -XX:+UseG1GC -jar meuapp.jar\n# 병렬 UseShenandoahGC GC\njava -XX:+UseShenandoahGC -jar meuapp.jar\n# 병렬 Z GC\njava -XX:+UseZGC -jar meuapp.jar\n```\n\n- 성능 향상과 애플리케이션 스로틀링을 피하기 위해 CPU 제한이 2000m 미만인 컨테이너를 피하세요. 많은 상황에서, 1000m CPU 제한이 있는 두 개의 컨테이너보다 2000m CPU 제한이 있는 컨테이너에 단일 JVM이 있는 것이 더 유익할 수 있습니다. 때로는 적은 게 더 나은 선택일 수도 있습니다 — 한 번 생각해보세요.\n- 자원 낭비를 피하려면, -Xmx 매개변수나 -XX:MaxRAMPercentage 플래그를 사용하여 JVM 힙 크기를 적절하게 구성하세요. 다음 팁에서 적절한 힙 크기에 대해 더 이야기하겠습니다.\n\n```js\n# 예시\n```\n\n```js\n# MaxRAMPercentage 사용\njava -XX:MaxRAMPercentage=50.0 -jar meuapp.jar\n# xmx 사용\njava -Xmx1g -jar meuapp.jar\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2) 적절한 메모리 크기 설정: 힙 이상의 JVM의 수명\n\n첫 번째 팁을 읽으면 아마도 궁금해 할 것입니다: 리소스 최적화를 위해 컨테이너의 사용 가능한 메모리의 100%에 대한 힙 크기를 구성하는 것은 왜 아닌가요?\n\n그 질문에 대한 답변은 이 팁의 끝에서 알려드리겠습니다. 그 전에 JVM을 구성하는 메모리 영역을 간단히 설명하고 싶습니다.\n\n1번 팁에 언급된 것처럼 힙 이외에 JVM의 메모리 영역에는 힙 이외라고도 불리는 네이티브 메모리 또는 오프-힙 메모리가 포함됩니다. 비-힙 내에서는 메타스페이스, 코드 캐시, 스택 메모리, GC 데이터 등 여러 가지 중요한 구성 요소가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이러한 구성 요소 각각에 대해 자세히 다루지는 않겠습니다. 이 글의 목적은 단순히 힙 이외에도 JVM의 다른 영역이 있고, 이 경우 애플리케이션이 실행되는 컨테이너에서 호스트로부터 메모리를 소비한다는 점을 명확히하는 것입니다.\n\n이제 당신은 생각할 수 있습니다: \"컨테이너의 사용 가능한 메모리를 힙과 비힙으로 나눌 수 없나요?\"\n\n답은 \"아니요\"입니다. 이 컨테이너의 JVM이 실행되는 기본 이미지는 활성 상태를 유지하기 위해 일정량의 메모리를 소비하는 에뮬레이션 운영 체제도 고려해야 합니다.\n\n그래서 이제 초반 질문에 대한 대답은: 컨테이너의 사용 가능한 메모리의 100%에 힙 크기를 구성하는 것이 왜 좋지 않을까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n힙 크기를 사용 가능한 메모리의 100%로 설정하면, 비-힙 및 운영 체제도 컨테이너 메모리를 사용한다는 것을 감안해야 하므로 OOM(메모리 부족)에 의해 컨테이너가 종료될 수 있습니다.\n\n![image](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_6.png)\n\n그러나 새로운 질문이 발생할 수 있습니다: 이 메모리를 힙, 비-힙 및 운영 체제 사이에 어떻게 분배해야 할까요?\n\n문헌에서는 사용 가능한 메모리의 75%를 힙에 구성하는 것이 안전한 값이라고 언급되는 경우를 본 적이 있습니다. 그러나 실무에서는 60% 이상의 값 사용 시 문제가 발생했습니다. 저는 개인적으로 전문 경험을 통해 힙에 50%를 할당하는 것이 안전한 값이라고 생각합니다. 초기에는 매우 보수적으로 보일 수 있지만, 이보다 높은 값 사용 가능성을 배제하지는 않습니다. 그러나 이를 확인하는 것이 중요합니다. 확인을 위해 부하 테스트를 활용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_7.png\" />\n\n힙 크기에 대해 또 하나 중요한 사항은 매우 작은 힙이 가비지 수집기에 과도한 작업을 유발할 수 있어 CPU 사용량이 늘어나고 애플리케이션 성능이 저하될 수 있다는 것을 이해하는 것입니다. 반면에 매우 큰 힙은 애플리케이션 시작 시간에 상당한 영향을 미칠 수 있고 긴 가비지 수집 시간을 유발할 수 있습니다.\n\n팁 요약:\n\n- 컨테이너의 메모리 사용량을 최적화하려면 JVM의 힙 크기를 50%에서 75% 사이로 구성하고 나머지 값은 비힙 및 운영 체제를 위해 예약하세요.\n- 구성된 힙 크기가 애플리케이션에 적합한지 검증하기 위해 부하 테스트를 수행하고 테스트 실행 중 OOM (메모리 부족 오류)이 발생하는지 확인하세요. JVM 메모리 소비를 추적하기 위해 모니터링 도구를 사용하고 Kubernetes API를 통해 Kubernetes Metrics Server를 사용하여 파드 메트릭을 모니터링할 수 있습니다.\n- 애플리케이션 성능이 저하되지 않도록 매우 작은 힙을 피하세요.\n- 애플리케이션 시작 시간에 영향을 미치지 않고 오랜 가비지 수집 시간을 유발하지 않도록 매우 큰 힙을 피하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3) Xms와 Xmx가 동일하다면: 필요한 양만 말해주세요\n\n이 팁을 살펴보기 전에, Xms와 Xmx에 대한 기본 내용을 알아보겠습니다.\n이 두 매개변수는 JVM에게 최소 (Xms) 및 최대 (Xmx) 힙 크기에 대한 정보를 제공하는 데 사용됩니다.\n\n이렇게 작동합니다. Xms는 JVM에 의해 힙에 할당된 초기 메모리 양을 나타내고, Xmx는 할당할 수 있는 최대 양을 나타냅니다. JVM은 Xms에 정의된 값으로 초기 메모리를 할당하고, 프로그램 실행 중에 필요에 따라 이 값은 Xmx에 정의된 값까지 증가할 수 있습니다. JVM이 Xmx를 초과하는 값으로 할당하려고 할 때 OutOfMemoryError가 발생할 수 있습니다.\n\n컨테이너 사용이 오늘날과 같이 널리 사용되기 전에는 JVM에서 실행되는 애플리케이션이 종종 동일한 서버를 공유했습니다. 이러한 시나리오에서는 Xms 매개변수에는 작은 값을 설정하고 Xmx에는 큰 값을 설정하여 리소스 활용 및 동시에 실행 중인 프로세스 간의 공유를 더 잘 목표로했습니다. 따라서 JVM은 필요할 때만 메모리를 할당하고 사용이 끝나면 호스트로 메모리를 반환했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너를 다룰 때 상황이 달라집니다. 대부분의 경우에는 동일한 컨테이너 내에서 실행 중인 다른 관련 병렬 프로세스가 없습니다. 따라서 프로그램 실행 중에 호스트에 동적으로 메모리를 할당하고 반환할 필요가 없습니다. JVM이 메모리 할당 작업을 처리할 필요가 없도록 하려면 Xms의 값이 Xmx의 값과 같도록 구성할 수 있습니다.\n\n팁 요약:\n\n- JVM이 메모리 할당 및 해제 작업을 처리하는 것을 피하려면 Xms의 값이 Xmx의 값과 같도록 설정하세요. 이를 위해 JVM 구성에서 매개변수 -Xms 및 -Xmx를 사용하십시오.\n\n```bash\n# 예시\njava -Xms2g -Xmx2g -jar meuapp.jar\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4) JVM 컨테이너에서 CPU 과부하: 위험한 실천 방법이지만 일부 경우에는 허용됨\n\n이 팁을 진행하기 전에 Kubernetes의 두 가지 개념인 \"Pod 및 컨테이너 자원 관리\"와 \"서비스 품질(QoS)\"를 설명하는 것이 중요합니다. 이러한 개념은 다음에 다루게 될 주제를 더 잘 이해하는 데 도움이 될 것입니다.\n\nPod 및 컨테이너 자원 관리\n\n이 시점에서 CPU 및 메모리에 대한 요청과 제한 개념을 설명하고 싶습니다. 기본적으로 요청은 컨테이너가 실행되는 데 필요한 최소 자원량을 나타내며, 제한은 클러스터 내에서 컨테이너가 소비할 수 있는 최대 자원량을 나타냅니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n메모리 요청 및 제한은 쿠버네티스에서 Pod를 만들 때 사용하는 YAML 파일의 \"resources\" 섹션에서 구성할 수 있습니다.\n\n간단한 예시는 다음과 같습니다:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: container-name\n      image: image-name:latest\n      resources:\n        requests:\n          memory: \"1Gi\"\n          cpu: \"1\"\n        limits:\n          memory: \"2Gi\"\n          cpu: \"2\"\n```\n\n이 시점에서, 저는 팁 1에서 다룬 주제와 관련된 질문을 하고 싶습니다. 거기서는 JVM에 대한 인체공학과 사용 가능한 호스트 리소스에 따라 설정된 기본 구성에 대해 이야기했습니다. 질문은 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너 내에서 사용자 정의 없이 JVM을 실행하고, Ergonomics가 설정을 정의하도록 하는 경우, 컨테이너의 요청 및 제한 값이 YAML 예제에 명시된 값과 같다고 가정하면, JVM이 채택하는 GC 설정 및 최대 힙 크기는 무엇일까요? 컨테이너에 정의된 요청 또는 제한 값이 고려될까요?\n\n이 질문에 대한 답변을 얻기 위해 JVM 버전 17을 사용하고 YAML에서의 구성으로 컨테이너를 실행해보겠습니다. 그런 다음, kubectl exec를 통해 컨테이너에 액세스하여 선택된 GC와 최대 힙 크기를 확인할 것입니다.\n\nJVM이 1GB 메모리와 1 CPU에 대한 요청을 존중한다면, 선택된 GC는 SerialGC이 되고 최대 힙 크기는 256MB (1GB의 ¼)가 될 것입니다.\n\n그러나 JVM이 2GB 메모리와 2 CPU의 제한을 존중한다면, 선택된 GC는 G1GC가 되고 최대 힙 크기는 512MB (2GB의 ¼)가 될 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 실행 후의 파드 및 컨테이너 구성입니다:\n\n![Pod Configuration](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_8.png)\n\nErgonomics에 의해 실행된 JVM 구성 결과는 다음과 같습니다:\n\n![JVM Configuration](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_9.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJVM은 컨테이너의 한계 값을 고려했다는 것이 눈에 띄었습니다. 따라서 선택된 GC는 G1GC이었고, 최대 힙 크기는 512MB로 설정되었는데, 이는 2GB의 ¼에 해당합니다.\n\n이제 리소스 관리 주제에서 잠시 쉬고 두 번째 개념에 대해 이야기해 봅시다.\n\nQoS(Quality of Service)\n\nKubernetes에서의 QoS(Quality of Service)는 요청하고 사용하는 리소스를 기반으로 Pod를 세 가지 범주로 분류하는 방법으로, 이 범주는 각 Pod의 우선 순위를 결정하는 데 사용됩니다. 이러한 분류로는 Guaranteed, Burstable 및 BestEffort가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n각 QoS 범주로 Pod를 분류하는 규칙은 다음과 같은 조건에 의해 정의됩니다:\n\n- 보장된(Guaranteed): 이 수준에서 CPU와 메모리 모두에 대해 요청(request) 및 제한(limit) 값이 지정되어야 합니다. 예시:\n\n```js\n리소스: 요청: 메모리: \"3Gi\";\nCPU: \"2\";\n제한: 메모리: \"3Gi\";\nCPU: \"2\";\n```\n\n- 터프너블(Burstable): 이 수준은 보장된으로 분류되기 위한 규칙을 충족하지 못하는 Pod를 위한 것입니다. Pod의 적어도 한 컨테이너는 메모리 또는 CPU에 대한 요청(request)나 제한(limit)이 있어야 합니다. 예시:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nresources:\nrequests:\nmemory: \"3Gi\"\n\n# Burstable의 또 다른 예:\n\nresources:\nrequests:\nmemory: \"3Gi\"\ncpu: \"1\"\nlimits:\nmemory: \"3Gi\"\ncpu: \"2\"\n\n# 이 예제에서 메모리 요청과 제한은 같지만\n\n# CPU 요청이 제한보다 작으므로,\n\n# 팟은 Burstable로 분류됩니다.\n\n- BestEffort: 컨테이너 내에 CPU 또는 메모리에 대한 요청 또는 제한 구성이 없는 경우 팟은 BestEffort로 분류됩니다. 적어도 하나의 컨테이너에 요청이나 제한 구성이 있는 경우 분류가 Burstable로 변경됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 결국 QoS의 중요성과 수준 범주는 무엇인가요?\n\n클러스터의 노드 중 하나가 과부하 상태거나 자원 부족 상태인 경우 Kubernetes 스케줄러는 QoS 우선 순위를 기반으로 제거할 팟을 선택할 수 있습니다. 이는 다음과 같이 정의됩니다:\n\n- Best-Effort: 이러한 팟은 가장 낮은 우선 순위를 가지며 가장 먼저 제거됩니다. 자원 요청이나 제한이 정의되지 않았기 때문에 클러스터에 영향을 미치지 않고 쉽게 제거할 수 있습니다.\n\n- Burstable: 이러한 팟은 중간 우선 순위를 가지며 최소한의 자원 요청 구성을 가지고 있기 때문에 Best-Effort 팟 이후에 제거됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n보장된: 이러한 팟은 CPU 및 메모리에 대한 요청 및 제한 값이 동일하여 실행에 필요한 충분한 자원이 확보되어 있기 때문에 가장 높은 우선 순위를 갖고 마지막으로 제거됩니다.\n\n좋아요, 이제 위의 두 개념인 \"팟 및 컨테이너 자원 관리\"와 \"QoS(서비스 품질)\"에 대해 논의했으니, 모든 팟을 보장된 상태로 구성하는 것이 가장 좋은 옵션이 될 수 있을지 궁금할 것입니다. 이 수준은 팟에 대한 더 큰 보장과 안정성을 제공하므로, 옳습니까?\n\n그래서 CPU와 메모리에 대한 요청 및 제한을 항상 동일하게 유지하여 모든 팟을 보장된 상태로 구성하는 것은 클러스터 내의 노드가 더 적은 팟을 수용하게 하고, 클러스터 내에서 더 많은 노드가 필요하게 함으로써 환경을 매우 비싸게 만들 수 있습니다. 추가적으로, 이 구성은 자원의 효율적인 활용을 방해할 수 있는 미활용 자원을 남겨 다른 팟과 공유할 수 있는 효과적이지 않은 자원 활용을 초래할 수 있습니다.\n\n반대로, 모든 팟을 보장된 상태로 구성하는 것은 실패와 지연 없이 항상 실행에 필요한 자원을 보장하므로 애플리케이션 안정성을 보장합니다. 게다가 이 구성은 높은 가용성과 성능이 필요한 중요한 애플리케이션에 적합한 옵션이 될 수 있으며, 팟에 최대 우선 순위를 보장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자 그럼 팁으로 넘어가기 전에, 제가 제안하는 것을 정말로 이해하실 수 있도록 하나의 추가적인 개념에 대해 더 이야기를 나누고 싶습니다.\n\n쿠버네티스 클러스터에서 오버부킹(Ovebooking) 개념에 대해 이야기해보겠습니다.\n\n일반적으로 오버부킹은 사용 가능한 것보다 더 많은 리소스를 할당하는 기술로, 할당된 모든 리소스가 동시에 사용되지는 않을 것으로 예상합니다. 예를 들어 항공사에서는 비행기의 총 좌석 수보다 더 많은 좌석을 판매하는 오버부킹을 사용하여 모든 승객이 비행기에 탑승하지는 않을 것을 가정합니다.\n\n쿠버네티스의 맥락에서, 오버부킹은 파드 내의 컨테이너에 할당된 CPU 및 메모리 리소스에 적용될 수 있습니다. 이는 컨테이너에 대한 요청 및 제한에 기반하여 클러스터에 사용 가능한 총 리소스보다 더 많은 리소스를 할당할 수 있는 것을 의미합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예를 들어, 4GB의 메모리와 4개의 CPU가 있는 노드가 있다고 상상해보세요. 이 노드에서 하나의 컨테이너를 가진 두 개의 포드가 각각 다음과 같은 리소스 구성을 갖고 있습니다:\n\n```js\n리소스: 요청: 메모리: \"1Gi\";\nCPU: \"1\";\n제한: 메모리: \"1Gi\";\nCPU: \"3\";\n```\n\nCPU 제한이 3으로 설정되어 있습니다. 두 개의 포드가 실행 중이며 각각 위의 구성을 갖는 한 개의 컨테이너를 고려할 때, 포드 수를 CPU 제한으로 곱하면 결과적으로 6개의 CPU가 되는데, 이는 노드에서 사용 가능한 CPU의 최대 개수를 초과합니다.\n\nKubernetes에서 Overbooking을 다룰 때 중요한 점은 CPU가 \"압축 가능\" 리소스로 간주되지만 메모리는 그렇지 않다는 것입니다. 즉, Kubernetes는 컨테이너가 요청한 양의 CPU를 받도록 보장하고 나머지를 제한할 것입니다. 그러나 컨테이너가 CPU 제한을 초과하기 시작하면 Kubernetes는 해당 컨테이너의 사용을 제한하기 시작할 것이며, 이는 응용 프로그램의 성능 하락으로 이어질 수 있습니다. 단, 해당 컨테이너는 종료되거나 제거되지 않는다는 점을 강조해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nCPU와 달리 메모리는 압축할 수 없는 자원입니다. 이는 노드가 사용 가능한 메모리를 모두 소진하면 Kubernetes가 메모리 공간을 확보하기 위해 어떤 컨테이너를 종료할지 결정해야 한다는 것을 의미합니다.\n\n대부분의 경우 JVM 애플리케이션은 데이터 처리가 많거나 복잡한 계산이 필요한 경우를 제외하고 메모리를 더 많이 요구합니다. 그래서 개발 환경을 효율적으로 관리하려면 다음 팁을 활용해보세요.\n\n클러스터를 효율적으로 활용하기 위해, 메모리에 대해서는 요청 값(request)을 제한 값(limits)과 동일하게 설정하고, CPU에 대해서는 오버북(overbook)해서 요청 값을 제한 값보다 낮게 설정하여 Pod를 Burstable 수준으로 유지하세요. 이렇게 함으로써 CPU 자원을 Pod 간에 공유할 수 있어 클러스터 효율성이 향상될 것입니다.\n\n만약 모든 Pod가 동시에 CPU 제한값을 사용해야 하는 경우(이는 드문 경우입니다), 과도한 CPU 소비를 노드 자동 확장의 트리거로 사용하여 환경의 부하를 균형잡을 수 있습니다. 이러한 시나리오에서 유일한 부작용은 노드를 확장할 때까지 일시적으로 컨테이너가 제한될 수 있다는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 전략은 처음에는 논란이 될 수 있지만, CPU의 가격이 메모리에 비해 비례적으로 높기 때문에 환경 비용을 극적으로 최적화할 수 있습니다.\n\n팁 요약\n\n- 특히 테스트 환경에서 비용을 줄이려면 쿠버네티스 환경에서 JVM 애플리케이션을 실행하는 파드를 Burstable 수준에서 구성하는 것을 고려하십시오. 메모리는 동등한 요청 및 한도를 포함하며 CPU의 경우 요청을 한계보다 낮게 설정하여 오버부킹하세요.\n\n## 5) JVM 및 HPA: 표준 사용해야 할 경우, 메모리보다 CPU를 우선시하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 팁을 진행하기 전에 Horizontal Pod Autoscaler(HPA)에 대해 간단히 이야기하려고 합니다.\n\nHPA는 쿠버네티스의 기능 중 하나로, 팟의 CPU 또는 메모리 사용량에 기반하여 애플리케이션의 복제본 수를 자동으로 조정하는 데 도움을 줍니다. 특정 수요를 충족시키기 위해 복제본 수를 증가하거나 감소시킵니다. 처리 능력이나 메모리에 대한 더 높은 수요가 있을 때 HPA는 복제본 수를 늘립니다. 수요가 줄어들면 HPA는 복제본 수를 감소시킵니다. 이렇게 함으로써 HPA는 피크 수요 기간에도 서비스 가용성을 유지합니다.\n\nJVM 애플리케이션을 다룰 때는 보통 CPU보다 더 많은 메모리를 소비하는 경우가 많습니다. 따라서 HPA를 메모리에 기반으로 스케일링하도록 구성하는 것이 유혹적일 수 있습니다. 그러나, JVM은 쓰레기 수집 및 메모리 할당 프로세스로 인해 메모리 소비에 변동이 발생할 수 있습니다. 이는 메모리를 HPA의 트리거로 사용하여 HPA를 불안정하게 만들어 애플리케이션의 수평 스케일링 프로세스를 방해할 수 있습니다.\n\n이를 알아두면, 기본적으로 쿠버네티스에서는 메모리와 CPU 메트릭을 고려하여 스케일을 조정할 수 있기 때문에, 우리의 유일한 선택은 CPU를 HPA 트리거 구성의 대안으로 고려하는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 CPU를 HPA 트리거로 사용하도록 권장하는 것은 유일한 옵션인 것으로 생각하지 마세요. 실제로 쿠버네티스에서 JVM 애플리케이션을 수평 확장하는 흥미로운 전략이 있습니다. CPU를 메트릭으로 사용하는 것이 관련됩니다. 이에 대해 자세히 알아보겠습니다.\n\n이 기본 아이디어는 다음과 같습니다. 고수요 시나리오에서 JVM은 메모리를 강하게 할당하기 시작하면서 응용 프로그램의 가비지 컬렉터(GC)에 대한 작업이 증가하고 풀 가비지 컬렉션 주기(Full GC)를 포함한 CPU 집약적인 프로세스가 발생합니다. 여기서 HPA 역할이 의미를 갖기 시작합니다. 이 문맥에서 HPA는 응용 프로그램의 수요를 나타내는 지표로 CPU 메트릭을 기반으로 응용 프로그램을 수평 확장하는 데 사용할 수 있습니다.\n\n다음은 이 HPA 구성의 예시입니다:\n\n```js\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nome-do-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nome-do-deployment\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 예제에서 HPA는 배치의 CPU 사용률을 모니터링하고 자동으로 팟 레플리카 수를 조정하여 CPU 사용률을 50%로 유지합니다. 최소 레플리카 수는 1이며, 최대는 5로 설정되어 있어 요구에 따라 응용 프로그램이 수평 확장될 수 있습니다.\n\nHPA는 주기적으로 배치의 CPU 사용률을 모니터링하고, 일정 기간 동안 사용률이 임계값 아래로 유지되면 HPA는 천천히 레플리카 수를 줄이기 시작하여 정의된 최소값에 도달할 때까지 줄입니다. Kubernetes는 기본적으로 스케일 다운 프로세스를 시작하기 전에 5분간 대기합니다.\n\n팁 요약\n\n- JVM(Java Virtual Machine) 애플리케이션의 성능을 향상시키기 위해 Kubernetes의 Horizontal Pod Autoscaler(HPA)에 CPU 메트릭을 기본 구성으로 사용하여 고수요 시나리오에서 애플리케이션 성능을 향상시킬 수 있습니다. 이는 트래픽 급증 시 JVM에 의한 강력한 메모리 할당이 쓰레기 수집기의 작업 부하를 크게 증가시킬 수 있어 전체 가비지 컬렉션 주기(Full GC)와 과도한 CPU 사용량이 발생할 수 있기 때문입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 결론\n\n이 포스트가 너무 길어서 죄송합니다. 제 목적은 포괄적이고 자세한 내용을 제공하는 것이었습니다. 이러한 지식은 오랜 연구와 경력을 통해 얻었으며, 제가 공유한 팁들이 도움이 되었기를 바랍니다.\n\n이것은 단순히 시작에 불과하며, 가능한 제2편에서 이와 같은 팁을 더 공유할 계획입니다.\n\n비판, 제안이 있거나 단순히 내용을 좋아했다면 의견을 남겨 주시기 바랍니다. 함께 공유하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 참고 자료\n\n- Microsoft Learn의 Microsoft for Java Developers\n- Kubernetes의 Kubernetes Documentation\n- Bruno Borges의 YouTube 동영상인 (178) Secrets of Performance Tuning Java on Kubernetes (해당 주제에 대해 본 적이 가장 좋았던 콘텐츠)\n- Google Cloud Blog의 Kubernetes requests vs limits: Why adding them to your Pods and Namespaces matters\n- Java inside docker: What you must know to not FAIL\n- DigitalOcean Kubernetes에서 워크로드를 자동으로 확장하는 방법\n","ogImage":{"url":"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_0.png"},"coverImage":"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_0.png","tag":["Tech"],"readingTime":26}],"page":"89","totalPageCount":120,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}