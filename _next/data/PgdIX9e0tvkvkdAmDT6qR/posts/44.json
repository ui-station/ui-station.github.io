{"pageProps":{"posts":[{"title":"2024년 소프트웨어 개발자를 위한 내가 가장 좋아하는 SQL과 데이터베이스 강좌들","description":"","date":"2024-05-18 18:13","slug":"2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024","content":"\n\n## 소프트웨어 개발자가 SQL 및 데이터베이스 개념을 깊이 학습할 수 있는 최고의 온라인 강좌들입니다.\n\n![이미지](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_0.png)\n\n안녕하세요 여러분, SQL과 데이터베이스를 배우고 최고의 Udemy 강좌를 찾고 있다면, 당신이 올바른 곳에 왔습니다.\n\n이전에는 SQL을 배울 수 있는 최적의 위치와 최고의 무료 SQL 강좌를 공유했었는데요, 그 안에는 Udemy나 Coursera 및 다른 웹사이트의 무료 강좌들이 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 초보자와 중급 개발자를 위한 Udemy의 최고의 SQL 강좌를 소개할 것입니다.\n\nSQL은 오랜 시간 동안 중요한 기술 기술이었지만 데이터 과학 및 데이터 분석의 등장으로 인해 데이터의 중요성이 현재 세상에서 더욱 중요해졌습니다.\n\n요즘 회사들 사이에서 데이터 과학 및 분석 직업은 높은 수요가 있으며 사용자들의 대량 데이터 및 기타 정보를 활용하여 이 데이터에 대한 통찰을 얻고 회사의 성장을 위한 더 나은 결정을 내리는 데 중요한 역할을 합니다. 데이터와 관련된 모든 직업이 SQL 언어를 배우는 것을 필요로 한다는 공통점이 있습니다.\n\nSQL은 회사의 데이터를 저장하기 위한 데이터베이스를 구축하고 데이터베이스와 상호 작용하기 위해 SQL 쿼리라고 불리는 명령을 사용하여 정보를 추출하고 데이터 분석 목적을 위해 필요한 정보만 남기기 위해 필터링하는 사람들을 위한 가장 인기 있는 언어입니다.\n\n<div class=\"content-ad\"></div>\n\n데이터베이스는 테이블의 모음이며, 각 테이블에는 데이터를 보유하는 행(row)과 열(column)이 포함되어 있습니다.\n\n이 언어를 배우는 것은 대부분의 다른 언어보다 쉽습니다. 심지어 Python보다도 쉽죠. SQL을 배우는 데 투자한 시간과 비용은 데이터 관련 분야의 취업을 원하는 학생들에게 좋은 투자입니다. 이는 당신을 경쟁자들보다 우위에 서게 할 겁니다.\n\n온라인에서 수천 개의 SQL 코스가 제공되지만, 당신의 시간과 노력을 가치 있게 만들어주는 코스를 찾는 것은 쉽지 않습니다. 이 글에서는 내 검색 결과에 따라 가장 좋은 코스를 제안하겠습니다.\n\n그런데, 만약 급한 대로 배우려 한다면, Udemy의 '15 Days of SQL: The Complete SQL Masterclass 2024' 코스를 참여하는 것을 제안합니다. 이 Udemy의 새 SQL 코스는 실생활 프로젝트에서 SQL을 딱 15일 만에 가르쳐줍니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_1.png\" />\n\n# 2024년 소프트웨어 개발자를 위한 최고의 SQL 및 데이터베이스 강좌 6선 - Udemy 및 Coursera 온라인 학습\n\n2024년에 온라인으로 배울 수 있는 최고의 Udemy 강좌 목록입니다. 이 강좌들은 SQL을 사용해 본 적은 있지만 깊이 있는 지식으로 습득하고 싶은 초보자 및 중급 개발자들을 위한 적합한 강좌입니다.\n\n## 1. The Complete SQL Bootcamp 2024: 처음부터 전문가까지\n\n<div class=\"content-ad\"></div>\n\nJose Portilla가 만든 이 강좌에는 41만 명 이상의 수강생이 있습니다. 이 데이터베이스나 이 언어에 이전 경험이 없는 초보자를 위한 이 가이드로 SQL 언어 학습 여정을 시작하는 것을 적극 추천합니다.\n\n이 강좌를 통해 SELECT 및 COUNT와 같은 간단한 SQL 명령어를 데이터베이스에 적용하는 방법, 그리고 GROUP BY 문을 사용하는 방법을 배울 수 있습니다. 또한 이 강좌는 PostgreSQL을 기반으로 하며 PostgreSQL 데이터베이스를 사용합니다.\n\n그런 다음 JOIN 명령어를 사용하여 여러 테이블에서 데이터를 검색하는 방법을 배우고 특정 데이터를 추출하기 위한 일부 고급 SQL 명령어를 익힐 수 있습니다. 마지막으로 PostgreSQL 데이터베이스에서 데이터베이스 및 테이블을 생성하는 방법도 배울 수 있습니다.\n\n여기 이 강좌에 가입할 수 있는 링크가 있습니다 - The Complete SQL Bootcamp 2024: 제로부터 히어로까지 변화하기\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_2.png\" />\n\n## 2. The Ultimate MySQL Bootcamp [Udemy Course]\n\n또 한 번 소개할 만한 좋은 강의는 이 최고의 MySQL 부트캠프이다. 이 코스에는 20시간 이상의 비디오 콘텐츠와 26.4만 명의 학생이 참여하고 있다.\n\n먼저 MySQL 데이터베이스의 중요 개념과 해당 환경을 컴퓨터에 설치하는 방법을 이해할 수 있게 될 것이고, 이후 MySQL에서 데이터베이스와 테이블을 생성하는 방법으로 나아갈 것이다.\n\n<div class=\"content-ad\"></div>\n\n그러면 이 데이터베이스에 데이터를 삽입하는 방법과 기타 사항을 배울 것입니다.\n\nSQL 언어에서 CRUD 명령문에 대해 알게 될 것입니다: 생성(Create), 조회(Read), 갱신(Update), 삭제(Delete) 쿼리에 대해 배울 것입니다. 또한 집계 함수에 대해 배우고 논리 연산자의 힘을 탐색할 것입니다.\n\n마지막으로 Node.js와 MySQL 데이터베이스를 사용하여 작은 웹 앱을 만들 것입니다.\n\n이 코스에 참여하기 위한 링크는 여기에 있습니다 — The Ultimate MySQL Bootcamp\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_3.png)\n\n## 3. 데이터 분석 및 비즈니스 인텔리전스를 위한 MySQL\n\n만약 데이터 분석가가 되려고 한다면, 이 강의가 적합할 것입니다. SQL 언어뿐만 아니라 Tableau 소프트웨어와 결합하여 데이터 시각화를 쉽게 할 수 있습니다.\n\n우선 데이터베이스가 어떻게 작동하고 데이터를 저장하는지 이해하고, MySQL을 설치하고 SQL 명령어로 연습을 시작할 것입니다.\n\n<div class=\"content-ad\"></div>\n\nSQL의 기본 명령어인 SELECT, INSERT, UPDATE, DELETE 및 집계 함수와 몇 가지 고급 주제를 학습한 후, 마지막으로 Tableau 소프트웨어와 결합하여 데이터 시각화를 수행할 수 있습니다.\n\n이 강좌에 참여하려면 다음 링크를 클릭하세요 — MySQL for Data Analytics and Business Intelligence\n\n[링크](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_4.png)\n\n## 4. SQL 초보자를 위한강좌\n\n<div class=\"content-ad\"></div>\n\n이 강좌에는 8시간 이상의 비디오 콘텐츠가 포함되어 있으며 MySQL 데이터베이스를 사용하여 시네마 예매 시스템을 만드는 실제 예제를 제공합니다.\n\nSQL 언어를 사용하기 전에 시스템에 MySQL 데이터베이스를 설치하고 주요 및 외래 키, 테이블과 같은 데이터베이스 개념을 이해할 수 있습니다.\n\n이 언어를 사용하여 테이블과 많은 테이블에서 데이터를 선택하고 간단한 SQL 명령을 사용하여 정보를 추출하는 방법을 배우게 됩니다. 데이터베이스 설계 및 데이터베이스 내에서 다양한 관계를 이해하고 시네마 예매 시스템과 같은 프로젝트를 개발합니다.\n\n이 강좌에 가입하려면 여기를 클릭하세요 - SQL for Beginners\n\n<div class=\"content-ad\"></div>\n\n![마크다운](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_5.png)\n\n## 5. 초보를 위한 Microsoft SQL\n\n우리 목록에서 마지막으로 소개하는 이 코스는 마이크로소프트 SQL 서버에서 SQL 언어를 사용하는 방법을 가르쳐 줍니다. 이는 수백만 명의 사용자가 데이터베이스로 사용하고 있는 서비스를 사용하는 데 도움이 될 것입니다.\n\n먼저 간단한 SQL 명령어를 이해하고 적용한 다음, WHERE 절을 사용하여 데이터를 필터링하고 데이터를 정렬하며 여러 테이블에서 데이터를 추출하고 집계 함수를 사용하는 방법을 배울 것입니다.\n\n<div class=\"content-ad\"></div>\n\n이 수업에 참여하려면 다음 링크를 클릭해주세요 — Microsoft SQL for Beginners\n\n![Microsoft SQL for Beginners](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_6.png)\n\n## 6. 데이터 과학을 위한 SQL\n\nUdemy를 좋아하지 않거나 Coursera와 같은 인기 있는 학습 플랫폼에서 최고의 SQL 과정을 찾고 있다면, 이 수업을 확인해보세요.\n\n<div class=\"content-ad\"></div>\n\n데이터 과학을 위한 SQL은 Coursera에서 가장 인기 있는 강좌 중 하나입니다.\n\nSQL의 기본을 마스터하여 데이터 과학자처럼 데이터를 분석할 수 있게 될 것입니다.\n\n이 강좌를 마친 후 여러 종류의 데이터, 문자열과 정수를 사용하고, 기본 및 복잡한 데이터 선택 쿼리를 수행할 수 있으며 SQL의 원리를 이해할 수 있게 될 것입니다.\n\nWomen in Data의 창립자/CEO이자 데이터 과학자인 Sadie St. Lawrence가 이 Coursera 강좌를 가르칩니다.\n\n<div class=\"content-ad\"></div>\n\n이 코스에 가입하려면 링크를 확인해보세요 — SQL For Data Science\n\n![이미지](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_7.png)\n\n그리고, Coursera 코스가 유용하다고 생각되시나요? 전 세계적으로 유명한 기업과 대학에서 만들어졌기 때문에 그렇습니다. Coursera Plus에 가입하는 것을 추천드립니다. 이 구독 플랜은 Coursera의 가장 인기 있는 강좌, 전문 강의, 프로페셔널 인증, 그리고 가이드 프로젝트에 무제한 액세스를 제공해요. 매년 $399이나 월 단위로 $59이 들지만, 돈을 완전히 가치 있게 쓸 수 있을 거라고 생각해요. 왜냐하면 무제한 인증서를 받을 수 있기 때문이거든.\n\n2024년에 SQL과 데이터베이스를 배우기 위한 최고의 Udemy와 Coursera 온라인 강좌는 여기까지에요. 이 목록에는 SQL 기본 사항과 데이터베이스 기초를 배울 수 있는 수업들, 그리고 MySQL, PostgreSQL, 그리고 Microsoft SQL Server와 같은 인기 데이터베이스를 배울 수 있는 온라인 강좌가 포함되어 있어요.\n\n<div class=\"content-ad\"></div>\n\n이것은 SQL을 배우는 데 적합한 데이터베이스로 수업을 듣는다는 것을 의미합니다. 이것은 초보자들의 관점에서 매우 중요합니다.\n\nSQL 언어를 배우는 것은 데이터 과학자나 데이터 분석가와 같은 데이터와 관련된 모든 직업의 중요한 부분입니다.\n\n웹 개발자라도 데이터베이스를 사용하여 이 언어를 배우고 프로페셔널하게 사용해야 합니다. 왜냐하면 이것이 당신의 경쟁자들에게 이점을 줄 것이기 때문입니다.\n\n만약 이러한 강좌들을 좋아하지 않고 연습이 가득한 부트캠프 스타일의 강좌를 찾고 있다면 Andrei Negaoie의 Complete SQL and Databases Bootcamp 강좌가 시작하기에 좋은 강좌입니다. 이 강좌는 주요 SQL 개념을 가르치기 위한 연습과 SQL 쿼리가 가득합니다.\n\n<div class=\"content-ad\"></div>\n\n그리고, 2024년에 SQL을 배우는 가장 좋은 방법을 보여주는 ByteByteGo의 멋진 비디오가 여기 있어요!\n\n탐색해볼 수 있는 다른 SQL 및 개발 코스\n\n- JavaScript를 배우기 위한 10가지 최고의 Udemy 코스\n- Python을 배우기 위한 10가지 최고의 Udemy 코스\n- 2024년을 위한 10가지 최고의 Udemy 코스\n- 풀 스택 웹 개발자로 성장하기 위한 10가지 코스\n- 2024년에 TypeScript를 무료로 배울 수 있는 10가지 코스\n- 초보자를 위한 Angular를 배우기 위한 나의 좋아하는 코스\n- 무료로 Ruby 및 Rails를 배울 수 있는 5가지 코스\n- 2024년 React JS 개발자 로드맵\n- 웹 개발자를 위한 PHP 및 MySQL 학습을 위한 5가지 수업\n- 무료로 블록체인 기술을 배울 수 있는 5가지 코스\n- Oracle 및 Microsoft SQL Server 데이터베이스를 배울 수 있는 5가지 코스\n- 초보자를 위한 10가지 Python 웹 개발 코스\n- 풀 스택 개발자 로드맵\n- Servlet, JSP 및 JDBC를 배울 수 있는 무료 강좌 5개\n- Java 및 DevOps 엔지니어를 위한 Docker 무료 코스 5가지\n- 2024년에 JavaScript를 배울 수 있는 13가지 무료 코스\n- Java에서 RESTful 웹 서비스를 배우기 위한 3권의 책 및 강좌\n- 2024년에 Angular를 배울 수 있는 5가지 무료 코스\n- 풀스택 개발자가 배워야 할 10가지 프레임워크\n\n지금까지 이 기사를 읽어주셔서 감사합니다. 만약 SQL을 배우기 위한 이 최고의 Udemy 코스들이 마음에 든다면, 친구들과 동료들과 공유해주세요. 이 목록에는 Udemy의 최고의 MySQL, PostgreSQL 및 Microsoft SQL Server 코스가 포함되어 있습니다. 질문이나 피드백이 있으시면, 댓글을 남겨주세요.\n\n참고: 만약 SQL 및 데이터베이스에 새로 입문한 분이라면, 여행을 시작할 무료 SQL 코스를 찾고 계시다면, 초보자를 위한 무료 SQL 및 데이터베이스 코스도 확인해보세요. 이 코스들은 Udemy 및 Coursera에서 법적으로 무료로 제공되며 SQL 개념, 데이터베이스 기본 개념, SQL 쿼리 작성 방법 등을 배울 수 있습니다.","ogImage":{"url":"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_0.png"},"coverImage":"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_0.png","tag":["Tech"],"readingTime":8},{"title":"ML, 데이터 팀을 위한 Gen AI","description":"","date":"2024-05-18 18:10","slug":"2024-05-18-MLGenAIfordatateams","content":"\n\n## 고전적인 ML 사용 사례와 Gen AI를 위한 신뢰성 있는 설계 구축\n\nAI와 ML은 대부분의 데이터 팀에게 중요한 주제입니다. 회사들은 AI로 실질적인 영향을 얻고 있으며, 데이터 팀은 이 중심에 있어 자신의 작업을 ROI에 결부시키는 원하는 방법을 얻고 있습니다.\n\n최근 예로, AI가 스웨덴의 '지금 살고 나중에 지불' 핀테크 Klarna를 위해 700명의 정근 연애를 자동화하는 데 도움을 주었습니다. Intercom은 이제 AI 중심의 고객 서비스 플랫폼이 되었으며, 임원들은 Gen AI 사용 사례를 구현하는 데 직접적으로 연관된 OKR을 가지고 있습니다.\n\n이 게시물에서는 데이터 팀에서 일하는 경우 이것이 무슨 의미를 하는지 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 팀에서의 AI 현황\n\nAI는 많이 발전했습니다. 실제로 그렇습니다. 스탠포드 대학의 2024 AI 지수 보고서에 따르면 AI는 이미지 분류, 시각적 추론, 그리고 영어 이해와 같은 여러 벤치마크에서 인간의 성능을 넘어섰다고 합니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_0.png)\n\n<div class=\"content-ad\"></div>\n\n요즘에는 ML 및 AI에 대한 수요가 급증하여 많은 데이터 팀이 업무 우선 순위를 재조정하게 되었습니다. 이는 ML 및 AI에서 데이터 팀의 역할에 대한 질문을 답하지 못한 채 남아 있습니다. 저희 경험상 데이터가 소유한 부분과 엔지니어가 소유한 부분 사이의 경계가 여전히 모호한 상황입니다.\n\ndbt가 최근 수천 명의 데이터 실무자를 대상으로 조사한 결과, 데이터 팀이 AI 및 ML에 참여하는 정도에 대한 정보를 얻을 수 있었습니다.\n\nAI 도입의 신호는 있지만, 대부분의 데이터 팀은 아직 일상적인 업무에 AI를 사용하고 있지 않습니다. 현재 응답자 중 1/3만이 오늘날 AI 모델 훈련을 위한 데이터를 관리하고 있습니다.\n\n![그림](/assets/img/2024-05-18-MLGenAIfordatateams_1.png)\n\n<div class=\"content-ad\"></div>\n\n이것은 곧 변경될 수 있습니다. 55%의 사람들이 곧 AI가 자가 데이터 탐색을 위해 혜택을 누리기를 기대하고 있습니다. \n\n![AI 및 ML use cases](/assets/img/2024-05-18-MLGenAIfordatateams_2.png)\n\n이는 우리가 1,000개 이상의 데이터 팀과 대화한 경험을 반영한 것입니다. 현재의 노력은 주로 데이터 분석을 위한 데이터 준비, 대시보드 유지 및 이해관계자 지원에 집중되어 있지만, AI 및 ML에 투자하고자 하는 욕망이 있습니다.\n\n# AI 및 ML 사용 사례\n\n<div class=\"content-ad\"></div>\n\nML과 AI가 수십 년 동안 존재해왔다는 것을 알아야 합니다. 최신 AI 모델인 Gen AI 모델은 텍스트에서 SQL 코드를 생성하거나 비즈니스 질문에 자동으로 답변하는 것과 같은 첨단 사용 사례에 가장 적합할 수 있지만, 분류 및 회귀 모델과 같은 더 검증된 방법들도 중요한 목적을 가지고 있습니다.\n\n가장 인기 있는 기술들 중 일부는 다음과 같습니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_3.png)\n\n# 고전적인 머신 러닝 사용 사례\n\n<div class=\"content-ad\"></div>\n\n대부분의 팀은 아직 전통적인 머신러닝 방법을 사용하지 않고 있습니다. 예를 들어, 분류, 회귀, 이상 감지와 같은 방법들이 있습니다. 이러한 방법들은 특히, 당신이 예측하고자 하는 명확한 결과 (예: 위험한 고객)와 예측 기능 (예: 가입 국가, 나이, 이전 사기)이 명확한 감독 학습에 유용할 수 있습니다. \n\n이러한 시스템들은 종종 설명하기 쉽고, 각 기능의 상대적 중요성을 추출할 수 있어 이를 통해 이유를 설명하기 쉽습니다. 이로써 이해관계자에게 고위험 고객을 거부하는 결정이 내려진 이유를 설명할 수 있게 됩니다.\n\n아래의 머신러닝 시스템은 고객 위험 점수 모델을 강조하며, 새로 가입한 사용자가 고위험 고객인지 거부해야 할 가능성이 얼마나 높은지를 예측합니다.\n\n![image](/assets/img/2024-05-18-MLGenAIfordatateams_4.png)\n\n다양한 소스에서 수집된 원시 데이터를 활용하여 예측 기능을 구축하며, 이는 데이터 과학자의 전문 지식과 모델이 식별한 예상치 못한 패턴을 결합합니다. 핵심 개념은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n- Sources 및 데이터 마트: 시스템에서 추출된 원시 및 가공되지 않은 데이터로 데이터 과학자가 관련성이 있는 것으로 판단한 것\n- 특성: ML 모델에 공급되는 전처리된 데이터 (예: 대도시의 거리, 나이, 이전 사기)\n- 레이블: 이전 위험한 고객을 기반으로 한 목표 출력 (예/아니오)\n- 트레이닝: 기계 학습 모델에 내부 매개변수나 가중치를 레이블된 예시에 기반하여 조정하여 정확한 예측을 수행할 수 있도록 가르치는 반복적인 프로세스\n- 추론: 트레이닝 단계 이후 새로운, 보이지 않은 데이터에 대해 예측이나 분류를 수행하기 위해 훈련된 기계 학습 모델을 사용하는 것\n\n데이터 팀과의 협업을 통해, 전통적인 ML 작업 흐름의 많은 부분이 데이터 웨어하우스로 이동되어 데이터 소스 및 피처 저장소의 기반이 되는 것을 볼 수 있습니다. 주요 데이터 웨어하우스는 이를 직접 제공하도록 시작했으며(예: BigQuery ML), 미래에는 전체적인 ML 작업 흐름이 데이터 웨어하우스로 완전히 이동할 것을 시사합니다.\n\n전통적인 ML 모델의 성공을 위한 일반적인 도전 과제는 다음과 같습니다:\n\n- 이용 가능한 데이터를 바탕으로 모델이 원하는 결과를 정확하고 적합한 수준으로 예측할 수 있는가\n- 달성된 정확도와 적합도 수준이 비즈니스에 대한 ROI로 충분한가\n- 이 작업을 수행하기 위해 우리가 해야 하는 트레이드 오프는 무엇인가(예: 위험한 고객을 검토하기 위해 더 많은 운영 직원)\n- 모델 유지 및 모니터링에 대한 유지와 모니터링의 비용은 얼마인가\n\n<div class=\"content-ad\"></div>\n\n# 차세대 및 Gen AI 사용 사례\n\n최근 몇 년간 차세대 및 특히 Gen AI 사용 사례에 대한 이야기가 소개되었으며 ChatGPT 3의 효율성으로 유명해졌습니다. 이 분야는 새로운 것이며 비즈니스 ROI가 아직 증명되지 않았지만 잠재력은 매우 큽니다.\n\n아래는 데이터 팀을 위해 본 Gen AI 사용 사례 중에서 가장 인기 있는 몇 가지입니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_5.png)\n\n<div class=\"content-ad\"></div>\n\n오늘의 사용 사례는 크게 두 가지 영역으로 그룹화될 수 있어요.\n\n- 비즈니스 가치 향상 — 고객 지원 챗봇에서 간단한 고객 상호 작용을 자동화하거나 고객 답변을 관련 지식 베이스 기사와 매칭하는 등 비즈니스 프로세스를 자동화하거나 최적화합니다.\n- 데이터 팀 생산성 향상 — 근본적인 데이터 워크플로우를 단순화하여 기술에 능통하지 않은 분석가가 ‘텍스트를 SQL로’ 쓸 수 있도록 하거나 비즈니스 이해자가 제시한 자연어 질문에서 답변을 생성함으로써 비즈니스 이해자의 즉각적인 요청을 줄입니다.\n\n아래는 비즈니스에 관련된 특정 데이터 말뭉치를 기반으로 ChatGPT의 사용자 버전을 설정하는 샘플 아키텍처입니다. 시스템은 두 부분으로 구성됩니다: (1) 도메인 데이터의 데이터 적재 및 (2) 실시간으로 질문에 답변할 수 있도록 데이터를 쿼리합니다.\n\n![image](/assets/img/2024-05-18-MLGenAIfordatateams_6.png)\n\n<div class=\"content-ad\"></div>\n\n예제 정보 검색 시스템 (출처: Langchain)\n\n첫 번째 단계는 문서를 벡터 저장소에 로드하는 것입니다. 이 과정에는 서로 다른 소스에서 데이터를 결합하거나 엔지니어들과 함께 생 데이터를 다루는 것, 그리고 모델이 교육받지 않아도 되는 데이터를 수동으로 제거하는 것(예: 고객 만족도 낮은 지원 응답)이 포함될 수 있습니다.\n\n- 특정 텍스트 말뭉치에서 텍스트로 데이터 소스 로드\n- 전처리하고 텍스트를 작은 조각으로 나누기\n- 단어들의 유사성에 따라 단어의 벡터 공간을 만들기 위해 임베딩 만들기\n- 임베딩을 벡터 저장소에 로드하기\n\n임베딩에 익숙하지 않다면, 단어나 문서의 숫자적 표현이고 이들 사이에 존재하는 의미와 관계를 포착하는 것이다. 아래 코드 스니펫을 실행하면 실제로 무엇인지 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom gensim.models import Word2Vec\n# 문장 말뭉치 정의\ncorpus = [\n    \"the cat sat on the mat\",\n    \"the dog barked loudly\",\n    \"the sun is shining brightly\"\n]\n# 문장 토큰화\ntokenized_corpus = [sentence.split() for sentence in corpus]\n# Word2Vec 모델 학습\nmodel = Word2Vec(sentences=tokenized_corpus, vector_size=3, window=5, min_count=1, sg=0)\n# 단어 임베딩 획득\nword_embeddings = {word: model.wv[word].tolist() for word in model.wv.index_to_key}\n# 단어 임베딩 출력\nfor word, embedding in word_embeddings.items():\n    print(f\"{word}: {embedding}\")\n```\n\n도메인 데이터를 벡터 저장소에 입력한 후, 사전에 학습된 LLM을 세밀하게 조정하여 도메인과 관련된 질문에 답변하는 시스템을 확장할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_7.png)\n\n예시 정보 검색 시스템 (출처: Langchain)\n\n\n<div class=\"content-ad\"></div>\n\n사용자는 채팅과 새로운 질문을 결합하여 후속 질문을 할 수 있습니다.\n위의 임베딩 및 벡터 저장소를 사용하여 유사 문서를 찾을 수 있습니다.\n큰 언어 모델(ChatGPT와 같은)을 사용하여 유사 문서를 활용하여 응답을 생성할 수 있습니다.\n\n다행히도 Meta와 Databricks와 같은 기업들이 교육 및 오픈소스 모델을 제공하고 있으므로 (Huggingface는 현재 1000여 개 이상의 Llama 3 오픈소스 모델을 보유하고 있습니다) 자체 모델을 교육시키기 위해 수백만 달러를 소비할 필요가 없습니다. 대신 기존 모델을 데이터로 세밀하게 조정하세요.\n\n위와 같은 LLM(Large Language Model) 기반 시스템의 효과는 그들에게 주어지는 데이터의 품질에 달려 있습니다. 따라서 데이터 전문가들은 여러 소스에서 가져온 가능한 많은 데이터를 피드하는 것이 장려되며, 이들 소스가 어디에서 오는지 추적하고 데이터가 예상대로 흐르는지 확인하는 것이 최우선 과제여야 합니다.\n\nGen AI 모델의 성공을 위한 전형적인 도전 과제는:\n\n<div class=\"content-ad\"></div>\n\n- 모델을 충분히 훈련할 만한 데이터가 있나요? 개인정보 문제로 사용이 제한되는 데이터가 있나요?\n- 모델이 해석 가능하고 설명 가능해야 하는가요? 예를 들어 고객이나 규제기관을 위해\n- LLM을 훈련하고 세부 조정하는 것에 대한 잠재적 비용은 무엇인가요? 그 혜택이 이 비용을 상회하나요?\n\n# AI와 ML에서 데이터 품질의 중요성\n\n당신의 주요 데이터 전달은 의사 결정에 도움을 주는 BI 대시보드를 위해 무작위 통찰을 제공할 때, 인간이 개입합니다. 인간의 직관과 기대로 인해 데이터 문제나 설명할 수 없는 추세가 종종 발견됩니다 — 그리고 아마도 몇 일 안에 해결됩니다.\n\nML과 AI 시스템은 다릅니다.\n\n<div class=\"content-ad\"></div>\n\nML 시스템이 수백 개 또는 수천 개의 다양한 소스에서 가져온 기능에 의존하는 것은 흔한 일입니다. 간단한 데이터 문제처럼 보일 수 있는 것들 — 누락된 데이터, 중복, 널 값 또는 빈 값, 이상치 — 이들은 비즈니스에 중대한 문제를 일으킬 수 있습니다. 이를 세 가지 다른 방법으로 생각해 볼 수 있습니다.\n\n- 비즈니스 중단 — 모든 사용자 ID가 비어 있는 중대한 오류는 새 사용자 가입 승인 비율이 90% 감소할 수 있습니다. 이러한 유형의 문제는 비용이 많이 들지만 종종 초기에 발견됩니다.\n- 드리프트 또는 '잠재적' 문제 — 이는 고객 분포의 변경이나 특정 세그먼트에 대한 누락된 값을 포함할 수 있으며, 이로 인해 체계적으로 부정확한 예측이 발생할 수 있습니다. 이러한 문제는 발견하기 어려우며, 몇 달 또는 몇 년 동안 지속될 수 있습니다.\n- 체계적인 편향 — Gen AI와 같은 경우, 데이터 수집에 대한 인간의 판단이나 결정으로 편향이 발생할 수 있습니다. 구글의 Gemini 모델에서 발생한 편견과 같이 최근 예들은 이러한 결과가 가져다 줄 수 있는 결과를 강조했습니다.\n\n회귀 모델을 지원하거나 LLM을 위한 새로운 텍스트 말뭉치를 작성 중이더라도, 새로운 모델을 개발하는 연구자가 아닌 한, 업무의 대부분은 데이터 수집 및 전처리에 관련될 것입니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_8.png)\n\n<div class=\"content-ad\"></div>\n\nML 시스템은 모델이 하나의 부분에 불과한 대규모 생태계입니다. — Google on Production ML Systems\n\n일반적으로, 화면 왼쪽에 위치할수록 오류를 모니터링하기 어려울 수 있습니다. 수백 개의 입력 및 원시 소스가 있어서 때로는 데이터 관련 전문가의 통제 영역을 벗어날 수 있으며, 데이터는 수천 가지 방법으로 잘못될 수 있습니다.\n\n![MLGenAIfordatateams_9](/assets/img/2024-05-18-MLGenAIfordatateams_9.png)\n\n모델 성능은 ROC, AUC 및 F1 점수와 같이 잘 알려진 메트릭을 사용하여 간단히 모니터링할 수 있으며, 이러한 메트릭은 모델 성능의 단일 측정 항목을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n상류 데이터 품질 문제의 예시\n\n- 결측 데이터: 데이터셋의 불완전하거나 없는 값은 모델이 일반화하고 정확한 예측을 하는 능력에 영향을 미칠 수 있습니다.\n- 일관성 없는 데이터: 서로 다른 소스 또는 시간에 따라 다양한 형식, 단위 또는 표현으로 인한 데이터 변이는 모델 학습 및 추론 중 혼동과 오류를 유발할 수 있습니다.\n- 이상치: 대부분의 관측치와 유별난 점이 큰 데이터의 이상치 또는 특이치는 모델 학습에 영향을 주고 편향적이거나 부정확한 예측을 유발할 수 있습니다.\n- 중복 레코드: 데이터셋에 중복된 항목이 들어 있는 경우 모델의 학습 과정을 왜곡시킬 수 있으며, 모델이 훈련 데이터에서 성능이 우수하지만 새로운, 보지 못한 데이터에서는 성능이 저하될 수 있습니다.\n\n데이터 이동의 예시\n\n- 계절별 제품 선호도: 계절에 따른 고객 선호도의 변화가 전자 상거래 추천에 영향을 미칩니다.\n- 금융 시장 변동: 경제적 사건으로 인한 시장의 급격한 변동이 주식 가격 예측 모델에 영향을 미치는 것입니다.\n\n<div class=\"content-ad\"></div>\n\nLLM에 대한 텍스트 데이터의 데이터 품질 문제 예시\n\n- 품질이 낮은 입력 데이터: 챗봇은 정확한 과거 사례 해결을 기반으로 작동합니다. 이 데이터의 정확성에 따라 봇의 효과가 결정되며, 잘못된 정보를 배우는 것을 피해야 합니다. 고객 만족도나 해결 점수가 낮은 답변은 모델이 잘못된 정보를 학습했을 수 있다는 신호일 수 있습니다.\n- 오래된 데이터: 의료 상담 봇은 오래된 정보에 의존할 수 있어서 관련성이 적은 권장 사항을 제공할 수 있습니다. 특정 일자 이전에 작성된 연구는 더 이상 목적에 부합하지 않을 수 있음을 나타낼 수 있습니다.\n\n# 신뢰할 수 있는 머신 러닝 및 인공지능 시스템 구축\n\n우리는 데이터 팀이 소프트웨어 엔지니어링과 비교했을 때 신뢰할 수 있는 데이터 시스템을 제공하는 데 신뢰받지 못한다고 믿습니다. 인공지능 파동은 \"쓰레기를 넣으면 쓰레기가 나온다\" 모델과 그 모든 함의를 기하급수적으로 확장하고 있습니다. 모든 기업이 경쟁 우위를 위한 데이터를 활성화하는 새로운 방법을 찾는 압박 속에 있을 때입니다.\n\n<div class=\"content-ad\"></div>\n\n특정 도구와 시스템이 모델 성능을 모니터링하기 위해 사용되지만, 이러한 도구들은 종종 데이터 웨어하우스의 상위 소스와 데이터 변환을 고려하지 않습니다. 데이터 신뢰성 플랫폼은 이를 위해 구축되었습니다.\n\n<img src=\"/assets/img/2024-05-18-MLGenAIfordatateams_10.png\" />\n\n# 안정적인 ML 및 AI 시스템 구축을 위한 다섯 가지 요추\n\n고품질의 제품용 ML 및 AI 시스템을 지원하고 유지하기 위해 데이터 팀은 엔지니어들의 최상의 실천 방법을 채택해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n![img](/assets/img/2024-05-18-MLGenAIfordatateams_11.png)\n\n- 성실한 테스트 — ML 및 AI 시스템에 공급되는 상위 소스 및 출력이 의도적으로 테스트되어야 함 (이상값, 널 값, 분포 변화, 품질)\n- 소유자 관리 — ML 및 AI 시스템은 명확한 소유자가 할당되어 문제를 통지받고 조치를 취하기를 기대해야 함\n- 사건 처리 — 심각한 문제는 명확한 SLA 및 에스컬레이션 경로를 가진 사건으로 취급되어야 함\n- 데이터 제품 마인드셋 — ML 및 AI 시스템으로 공급되는 전체 가치 사슬을 하나의 제품으로 고려해야 함\n- 데이터 품질 메트릭스 — 데이터 팀은 ML 및 AI 시스템의 가동 시간, 오류, SLA 등 핵심 메트릭을 보고할 수 있어야 함\n\n한 축에만 집중하는 것은 드물게 충분하지 않습니다. 명확한 소유권이 없는 채로 테스트에 과도하게 투자하면 문제가 슬립할 수 있습니다. 소유에 투자하지만 의도적으로 사건을 관리하지 않으면 심각한 문제가 너무 오랫동안 해결되지 않을 수 있습니다.\n\n![img](/assets/img/2024-05-18-MLGenAIfordatateams_12.png)\n\n<div class=\"content-ad\"></div>\n\n중요한 점은 다섯 가지 데이터 신뢰성 기둥을 구현하는 데 성공한다고 해도 문제가 발생하지 않는다는 것이 아니라, 단지 미리 발견할 가능성이 더 높아지고 자신감을 키워 고객에게 시간이 지남에 따라 어떻게 개선되고 있는지 전달할 수 있다는 것입니다.\n\n# 요약\n\n현재 데이터 팀 중 33%만이 AI 및 ML 모델을 지원하지만 대부분은 가까운 미래에 지원할 것으로 예상합니다. 이러한 변화는 데이터 팀이 비즈니스 중요 시스템을 지원하고 소프트웨어 엔지니어처럼 더 많이 일해야 한다는 새로운 세계에 적응해야 한다는 것을 의미합니다.\n\n- 데이터 팀에서의 AI 상황 - AI 시스템은 이미지 분류, 시각적 추론 및 영어 이해와 같은 여러 기준에서 성능이 향상되고 있습니다. 현재 데이터 팀 중 33%가 생산 중인 AI 및 ML을 사용하지만 55%의 팀이 예상됩니다.\n- AI 사용 사례 - 분류 및 회귀에서 Gen AI까지 다양한 ML 및 AI 사용 사례가 있습니다. 각 시스템은 도전적인 과제를 제기하지만 \"고전적인 ML\"과 Gen AI 간의 차이는 명백합니다. 우리는 이를 고전적인 고객 위험 예측 모델과 정보 검색 챗봇을 통해 살펴봤습니다.\n- AI 및 ML 시스템의 데이터 품질 - 데이터 품질은 ML 및 AI 프로젝트의 성공에 가장 중요한 위험 중 하나입니다. AI 및 ML 모델이 종종 수백 개의 데이터 소스에 의존하는데, 문제를 수동으로 감지하는 것은 거의 불가능합니다.\n- 믿을 수 있는 데이터를 위한 다섯 가지 단계 - ML 및 AI 시스템을 지원하고 유지하기 위해 데이터 팀은 엔지니어처럼 더 많이 일해야 합니다. 이에는 지속적인 테스트, 명확한 소유권, 사건 관리 프로세스, 데이터 제품 마인드셋 및 가동 시간 및 SLA와 같은 지표에 대한 보고 능력이 포함됩니다.","ogImage":{"url":"/assets/img/2024-05-18-MLGenAIfordatateams_0.png"},"coverImage":"/assets/img/2024-05-18-MLGenAIfordatateams_0.png","tag":["Tech"],"readingTime":11},{"title":"데이터 웨어하우징을 위한 5가지 사이버보안 팁","description":"","date":"2024-05-18 18:08","slug":"2024-05-18-5CybersecurityTipsforDataWarehousing","content":"\n\n<img src=\"/assets/img/2024-05-18-5CybersecurityTipsforDataWarehousing_0.png\" />\n\n데이터 웨어하우징은 대규모 AI 및 기계 학습 애플리케이션을 훨씬 더 관리하기 쉽게 만듭니다. 모든 것을 한 곳에 가지고 있으면 더 빠르고 정확한 분석이 가능해지지만, 동시에 일부 보안 문제를 야기할 수도 있습니다. 이러한 대규모로 통합된 데이터베이스는 사이버 범죄자들에게 유혹이 되는 대상이므로 면밀한 보호가 필요합니다.\n\n조직 간에도 데이터 웨어하우스 자체가 다양하듯이 특정 보안 시스템도 다양합니다. 그럼에도 불구하고 설정과는 상관없이 몇 가지 모범 사례를 도입해야 합니다. 고려해야 할 다섯 가지 주요 사이버 보안 팁을 소개합니다.\n\n# 1. 데이터 익명화 및 암호화\n\n<div class=\"content-ad\"></div>\n\n데이터 창고에서 모든 데이터를 암호화하는 것이 첫 번째 단계입니다. 데이터에 높은 암호화 표준을 적용하면, 해커들이 액세스하더라도 그것이 쓸모없게 만들어질 것입니다. 홀모모르픽 암호화와 같은 새로운 기술은 당신이 데이터를 복호화하기 전에도 암호화된 데이터를 사용할 수 있도록 한 단계 더 나아간 것입니다.\n\n사용하는 데이터 유형에 따라 데이터를 익명화해야 할 수도 있습니다. 이는 개인 식별자를 제거하여 개인 정보 침해를 방지하는 프로세스입니다. 실제 세계의 수치를 합성 데이터로 교체하는 것이 가장 안전한 방법이지만, 데이터가 실제 세계 사람들을 반영해야 하는 경우 역동적 익명화가 좋은 대안입니다.\n\n## 2. 액세스 권한 제한\n\n데이터 창고 사이버 보안의 다음 단계는 사용자의 액세스 권한을 제한하는 것입니다. 이 작업에 접근하는 가장 좋은 방법은 최소 권한 원칙(Least Privilege Principle, PoLP)을 따르는 것입니다.\n\n<div class=\"content-ad\"></div>\n\nPoLP(Principle of Least Privilege)는 작업을 올바르게 수행하기 위해 필요한 것만 액세스할 수 있어야 한다고 주장합니다. 기계 학습 모델과 작업하지 않는 직원은 기계 학습 훈련을 위해 구체적으로 데이터 웨어하우스에 액세스할 수 없어야 합니다. 마찬가지로, 데이터 과학자는 급여 데이터를 볼 수 없어야 합니다.\n\n액세스 권한 제한은 두 가지 주요 이점이 있습니다. 첫째, 주어진 데이터 웨어하우스에 영향을 미칠 수 있는 사람 수를 줄임으로써 발생하는 74%의 데이터 침해와 관련된 인적 오류를 최소화합니다. 둘째, 공격자가 한 계정을 침해하면 측면 이동을 최소화합니다.\n\n# 3. 인증 조치 향상\n\n권한 제한은 신뢰할 수 있는 방법으로 누가 누구인지 판별할 수 있을 때에만 효과가 있음을 기억하세요. 따라서, PoLP를 강력한 인증 조치와 함께 실행해야 합니다. 가장 기본적인 수준에서는 다중 요소 인증(MFA)이 시행되어야 합니다.\n\n<div class=\"content-ad\"></div>\n\nMFA는 여러 방법으로 실행할 수 있지만, 모든 방법이 동일한 수준의 보안을 제공하는 것은 아닙니다. 예를 들어 SMS 기반 인증은 이메일 인증보다 더 안전합니다. 특정 장치에 액세스가 필요하기 때문입니다. 생체 인증은 암호보다 해킹이 더 어려울 수 있지만, 공격자가 생체 데이터에 액세스하면 변경할 수 없으므로, 민감한 창고에는 이상적이지 않을 수 있습니다.\n\n# 4. 데이터 분류 및 조직화\n\n데이터 웨어하우징 보안에서 놓치기 쉬운 하지만 여전히 중요한 단계는 데이터를 분류하는 것입니다. 조직화는 사이버 보안 문제보다는 작업 문제처럼 보일 수 있지만, 중요한 보안적 영향을 많이 미칩니다.\n\n먼저, 볼 수 없는 것은 안전으로 보호할 수 없습니다. 보안 소프트웨어 사용자의 약 60%가 데이터의 40% 미만만 분석한다고 합니다. 이는 중요한 취약점을 놓칠 수 있거나 침해를 인식하지 못할 수 있음을 의미합니다. 조직의 부재는 시각성을 제한하기 때문에 데이터를 분류하여 그룹으로 구성하여 보다 철저한 취약점 분석과 빠른 사고 대응을 가능하게 해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n분류는 엑세스 권한을 정제하는 데도 도움이 됩니다. 데이터를 사용 또는 민감도에 따라 정렬하면 누가 액세스할 수 있는지 결정하고 해당 정책을 시행하는 데 도움이 됩니다. 또한 행동 생체 인식을 구현할 수 있어 이로 인해 평상시에는 액세스할 수 없는 데이터에 접근하는 경우 경고를 받을 수 있습니다.\n\n# 5. 창고를 면밀히 모니터링하십시오\n\n이러한 변경 사항을 시행한 후 데이터 창고를 지속적으로 모니터링해야 합니다. 어떤 방어 기법도 100% 효과적일 수는 없지만, 신속한 대응은 침해 사건 발생 시 피해를 최소화할 것입니다. 새로운 위협에 대응하거나 실시간 보안 사건에 대응할 수 있는 유일한 방법은 지속적인 모니터링을 통해 가능합니다.\n\n인공지능과 자동화는 여기서 꼭 필요합니다. 24시간 수동 모니터링은 많은 보안 인력이 필요합니다. 대부분의 기관에게는 선택사항이 아닙니다. 세계적으로 노동력 수요가 증가하더라도 사이버 보안 직원은 340만 명이 부족합니다. 자동화된 네트워크 모니터링은 실시간 사건 제한 및 부족한 보안 직원을 보완하기 위한 경고를 제공할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 데이터 웨어하우징 보안에 대한 주의가 필요합니다\n\n데이터 웨어하우스는 한 대의 큰 데이터베이스를 보호하는 것이 여러 개의 자원에 분산하는 것보다 쉽기 때문에 보안이 개선됩니다. 그러나 동시에, 그 크기 때문에 눈에 띄는 관심을 끄는 경우가 있습니다, 특히 민감한 정보를 저장하는 경우에는 더욱 그렇습니다.\n\n이러한 위험 요소들을 고려하여 데이터 웨어하우징 사이버 보안은 필수적입니다. 기존의 보안 시스템에 다음 다섯 가지 모범 사례를 통합하여 데이터 웨어하우스를 가능한 한 안전하게 유지하십시오.\n\n최초 게시물: OpenDataScience.com\n\n<div class=\"content-ad\"></div>\n\nOpenDataScience.com에서 데이터 과학 관련 기사를 더 읽어보세요. 초보자부터 고급 수준까지의 튜토리얼과 안내서를 만나보실 수 있습니다! 매주 목요일마다 최신 소식을 받아보고 싶으시다면 여기를 클릭하여 주간 뉴스레터를 구독해보세요. 또한 Ai+ 트레이닝 플랫폼을 통해 언제 어디서든 데이터 과학을 학습할 수 있습니다. ODSC 이벤트에 참석하고 싶으신가요? 다가오는 이벤트에 대해 더 알아보고 싶으시다면 여기를 클릭해주세요.","ogImage":{"url":"/assets/img/2024-05-18-5CybersecurityTipsforDataWarehousing_0.png"},"coverImage":"/assets/img/2024-05-18-5CybersecurityTipsforDataWarehousing_0.png","tag":["Tech"],"readingTime":4},{"title":"Delta 테이블을 REST API를 통해 노출하는 방법","description":"","date":"2024-05-18 18:06","slug":"2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs","content":"\n\n## 델타 테이블을 제공하기 위해 토론 및 테스트된 세 가지 아키텍처\n\n![이미지](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_0.png)\n\n# 1. 소개\n\n메달리온 아키텍처 내의 델타 테이블은 일반적으로 데이터 제품을 생성하는 데 사용됩니다. 이러한 데이터 제품은 데이터 과학, 데이터 분석 및 보고를 위해 사용됩니다. 그러나 데이터 제품을 REST API를 통해 노출하는 것도 일반적인 문제입니다. 이 아이디어는 이러한 API를 더 엄격한 성능 요구 사항을 갖춘 웹 앱에 내장하는 것입니다. 중요한 질문은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n데팔타 테이블에서 데이터를 읽는 것이 웹 애플리케이션에 빠르게 서비스할 수 있을까요?\n솔루션을 확장할 수 있는 컴퓨팅 레이어가 필요할까요?\n엄격한 성능 요구 사항을 충족시키기 위한 스토리지 레이어가 필요할까요?\n\n이러한 질문에 대해 심층적으로 다루기 위해 세 가지 아키텍처가 다음과 같이 평가됩니다: 아키텍처 A — API의 라이브러리, 아키텍처 B — 컴퓨팅 레이어 및 아키텍처 C — 스토리지 레이어. 아래 이미지 참조하세요.\n\n![image](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_1.png)\n\n블로그 글의 나머지 부분에서 세 가지 아키텍처에 대한 설명을 제공하고, 배포 및 테스트를 수행한 후 결과를 도출합니다.\n\n<div class=\"content-ad\"></div>\n\n# 2. 아키텍처 설명\n\n## 2.1 아키텍처 A: DuckDB와 PyArrow를 사용한 API 내 라이브러리\n\n이 아키텍처에서는 API가 직접 델타 테이블에 연결되어 있으며 중간에 계산 레이어가 없습니다. 이는 데이터가 API 자체의 메모리와 계산을 사용하여 분석된다는 것을 의미합니다. 성능을 향상시키기 위해 내장 데이터베이스 DuckDB와 PyArrow의 Python 라이브러리를 사용합니다. 이러한 라이브러리는 API에서 필요한 열만로드되도록 보장합니다.\n\n![이미지](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_2.png)\n\n<div class=\"content-ad\"></div>\n\n이 아키텍처의 장점은 데이터를 중복으로 만들 필요가 없으며 API와 델탔 테이블 사이에 필요한 레이어가 없다는 것입니다. 이는 구성 요소가 적다는 것을 의미합니다.\n\n이 아키텍처의 단점은 확장하기 어렵고 모든 작업을 API의 컴퓨팅 및 메모리에서 처리해야 한다는 것입니다. 특히 많은 양의 데이터를 분석해야 하는 경우에는 특히 도전적입니다. 이는 많은 레코드, 큰 컬럼 또는 많은 동시 요청에서 나올 수 있습니다.\n\n## 2.2 아키텍처 B: Synapse, Databricks 또는 Fabric을 사용하는 컴퓨팅 레이어\n\n이 아키텍처에서 API는 컴퓨팅 레이어에 연결되고 델탔 테이블에 직접 연결되지 않습니다. 이 컴퓨팅 레이어는 델타 테이블에서 데이터를 가져와 데이터를 분석합니다. 컴퓨팅 레이어는 Azure Synapse, Azure Databricks 또는 Microsoft Fabric일 수 있으며 일반적으로 잘 확장됩니다. 데이터는 컴퓨팅 레이어로 중복되지 않지만 컴퓨팅 레이어에서 캐싱을 적용할 수 있습니다. 이 블로그의 남은 부분에서는 Synapse Serverless로 테스트 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_3.png)\n\n이 아키텍처의 장점은 데이터를 중복하여 저장할 필요가 없으며 아키텍처가 잘 확장된다는 것입니다. 또한 대규모 데이터 세트를 처리하는 데 사용할 수 있습니다.\n\n이 아키텍처의 단점은 API와 델타 테이블 사이에 추가적인 레이어가 필요하다는 것입니다. 이는 더 많은 이동 부품을 유지 및 보안해야 한다는 의미입니다.\n\n## 2.3 아키텍처 C: Azure SQL이나 Cosmos DB를 사용한 최적화된 저장 레이어\n\n<div class=\"content-ad\"></div>\n\n이 아키텍처에서 API는 델타 테이블에 직접 연결되지 않고, 델타 테이블이 복제된 다른 저장 계층에 연결됩니다. 다른 저장 계층은 Azure SQL 또는 Cosmos DB일 수 있습니다. 이 저장 계층은 데이터를 빠르게 검색하기 위해 최적화될 수 있습니다. 이 블로그의 나머지 부분에서는 Azure SQL을 사용하여 테스트를 진행합니다.\n\n![이미지](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_4.png)\n\n이 아키텍처의 장점은 저장 계층이 인덱스, 파티셔닝 및 머티얼라이즈드 뷰를 사용하여 데이터를 빠르게 읽을 수 있도록 최적화될 수 있다는 것입니다. 이는 주로 요청-응답 웹 앱 시나리오에서 요구 사항입니다.\n\n이 아키텍처의 단점은 데이터가 중복되어야 하며 API와 델타 테이블 사이에 추가적인 계층이 필요하다는 것입니다. 이는 더 많은 구성 요소를 유지보수하고 보안해야 한다는 의미입니다.\n\n<div class=\"content-ad\"></div>\n\n블로그의 나머지 부분에서 아키텍처를 배포하고 테스트합니다.\n\n# 3. 아키텍처 배포 및 테스트\n\n## 3.1 아키텍처 배포\n\n아키텍처를 배포하기 위해 이전 장에서 논의한 세 가지 솔루션을 배포하는 GitHub 프로젝트가 생성되었습니다. 해당 프로젝트는 아래 링크에서 확인할 수 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nhttps://github.com/rebremer/expose-deltatable-via-restapi\n```\n\n다음은 GitHub 프로젝트를 실행할 때 배포될 내용입니다:\n\n- 표준 테스트 데이터 세트 WideWorldImporterdDW full에서 시작한 델타 테이블. 테스트 데이터 세트는 50백만 건의 레코드와 22개 열로 구성되어 있으며 1개의 큰 설명 열이 있습니다.\n- 모든 아키텍처: API로 작용하는 Azure Function.\n- 아키텍처 B: 컴퓨팅 계층으로 작용하는 Synapse Serverless.\n- 아키텍처 C: 최적화된 저장 계층으로 작용하는 Azure SQL.\n\n배포된 후 테스트를 실행할 수 있습니다. 다음 단락에서 테스트에 대해 설명하겠습니다.```\n\n<div class=\"content-ad\"></div>\n\n## 3.2 테스트 아키텍처\n\n아키텍처를 테스트하기 위해 다양한 유형의 쿼리 및 다른 스케일링을 적용할 것입니다. 다양한 유형의 쿼리는 다음과 같이 설명할 수 있습니다:\n\n- 11개의 작은 열(char, integer, datetime)을 포함하는 20개 레코드를 조회합니다.\n- 각 필드당 500자 이상을 포함하는 큰 설명 열이 포함된 2개 열을 사용하여 20개 레코드를 조회합니다.\n- 그룹별 데이터 집계, having, max, average를 사용한 데이터 집계.\n\n아래에서 쿼리를 설명합니다.\n\n<div class=\"content-ad\"></div>\n\n```sql\n-- 쿼리 1: 대형 텍스트 없이 11개 열의 포인트 조회\nSELECT SaleKey, TaxAmount, CityKey, CustomerKey, BillToCustomerKey, SalespersonKey, DeliveryDateKey, Package \nFROM silver_fact_sale\nWHERE CityKey=41749 and SalespersonKey=40 and CustomerKey=397 and TaxAmount > 20\n-- 쿼리 2: 500자 이상의 Description 열\nSELECT SaleKey, Description \nFROM silver_fact_sale\nWHERE CityKey=41749 and SalespersonKey=40 and CustomerKey=397 and TaxAmount > 20\n-- 쿼리 3: 집계\nSELECT MAX(DeliveryDateKey), CityKey, AVG(TaxAmount)\nFROM silver_fact_sale\nGROUP BY CityKey\nHAVING COUNT(CityKey) > 10\n```\n\n다음과 같이 스케일링이 가능합니다:\n\n- 아키텍처 A의 경우, 데이터 처리는 API 자체에서 수행됩니다. 이는 API의 컴퓨트 및 메모리가 앱 서비스 플랜을 통해 사용된다는 것을 의미합니다. SKU Basic(1코어 및 1.75GB 메모리) 및 SKU P1V3 SKU(2코어, 8GB 메모리)로 테스트될 것입니다. 아키텍처 B 및 C의 경우에는 처리가 다른 곳에서 이루어지기 때문에 이러한 정보는 해당하지 않습니다.\n- 아키텍처 B의 경우, Synapse Serverless가 사용됩니다. 스케일링은 자동으로 이루어집니다.\n- 아키텍처 C의 경우, 표준 티어의 Azure SQL 데이터베이스가 125 DTU로 사용됩니다. CityKey에 인덱스가 없는 상태와 CityKey에 인덱스가 있는 상태에서 테스트될 것입니다.\n\n다음 단락에서 결과가 설명됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n## 3.3 결과\n\n아키텍처를 배포하고 테스트한 후에는 결과를 얻을 수 있습니다. 다음은 결과 요약입니다:\n\n![Results](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_5.png)\n\n아키텍처 A는 SKU B1로 배포할 수 없습니다. 만약 SKU P1V3가 사용된다면, 컬럼 크기가 크지 않다면 결과는 15초 이내에 계산될 수 있습니다. 모든 데이터를 API 앱 서비스 계획에서 분석한다는 점을 유의하십시오. 너무 많은 데이터가로드되면(많은 행, 큰 컬럼 및/또는 많은 동시 요청으로),이 아키텍처는 확장하기 어려울 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아키텍처 B는 Synapse Serverless를 사용하여 10-15초 내에 작동합니다. 계산은 데이터를 가져와 분석하기 위해 자동으로 조정되는 Synapse Serverless에서 이루어집니다. 성능은 세 가지 유형의 쿼리에 대해 일관되게 유지됩니다.\n\n아키텍처 C는 Azure SQL을 사용할 때 인덱스가 생성되면 가장 잘 작동합니다. 조회 쿼리 1과 2의 경우 API는 대략 1초 내에 응답합니다. 쿼리 3은 전체 테이블 스캔이 필요하며 성능은 다른 솔루션과 거의 동일합니다.\n\n# 3. 결론\n\n중재 아키텍처의 Delta 테이블은 일반적으로 데이터 제품을 생성하는 데 사용됩니다. 이러한 데이터 제품은 데이터 과학, 데이터 분석 및 보고서 작성에 사용됩니다. 그러나 일반적으로 Delta 테이블을 REST API를 통해 노출하는 것도 자주 묻는 질문 중 하나입니다. 이 블로그 포스트에서는 이와 같은 장단점을 갖는 세 가지 아키텍처가 설명되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\nArchitecture A: DuckDB 및 PyArrow를 사용하여 API 내 라이브러리를 활용하는 아키텍처입니다.\n이 아키텍처에서는 API가 직접 델타 테이블에 연결되어 중간 계층이 없습니다. 이는 모든 데이터가 메모리에서 분석되고 Azure Function의 연산을 함께 함을 의미합니다.\n\n- 이 아키텍처의 장점은 추가 리소스가 필요하지 않다는 것입니다. 이는 유지 및 보안해야 하는 부분이 적기 때문에 이점으로 작용합니다.\n- 이 아키텍처의 단점은 API 자체에서 모든 데이터를 분석해야 하기 때문에 확장성이 떨어진다는 것입니다. 따라서 소량의 데이터에만 사용해야 합니다.\n\nArchitecture B: Synapse, Databricks 또는 Fabric을 사용한 컴퓨팅 레이어.\n이 아키텍처에서는 API가 컴퓨팅 레이어에 연결됩니다. 이 컴퓨팅 레이어는 델타 테이블에서 데이터를 가져와 분석합니다.\n\n- 이 아키텍처의 장점은 확장성이 좋고 데이터가 중복되지 않습니다. 집계를 수행하며 대량의 데이터를 분석하는 쿼리에 적합합니다.\n- 이 아키텍처의 단점은 조회 쿼리에 일관되게 5초 이내의 응답을 받는 것이 불가능하다는 것입니다. 또한 추가 리소스를 보안 및 유지해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n아키텍처 C: Azure SQL 또는 Cosmos DB를 사용한 최적화된 저장 계층입니다. \n\n이 아키텍처에서는 API가 최적화된 저장 계층에 연결됩니다. 델타 테이블이 미리 이 저장 계층으로 복제되며 데이터를 검색하고 분석하는 데 사용됩니다.\n\n- 이 아키텍처의 장점은 인덱스, 파티셔닝, 머티얼라이즈드 뷰를 사용하여 룩업의 빠른 쿼리를 위해 최적화될 수 있다는 것입니다. 이것은 종종 요청-응답 웹 앱에 필요한 요구사항입니다.\n- 이 아키텍처의 단점은 데이터가 다른 저장 계층으로 중복되어 동기화가 유지되어야 한다는 것입니다. 또한 추가 자원을 보안하고 유지해야 합니다.\n\n안타깝게도, 완벽한 해결책은 없습니다. 이 글은 REST API를 통해 델타 테이블을 노출하는 데 가장 적합한 아키텍처를 선택하는 데 도움을 주기 위한 가이드를 제시했습니다.","ogImage":{"url":"/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_0.png"},"coverImage":"/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_0.png","tag":["Tech"],"readingTime":7},{"title":"Power BI 최적화 차원 모델링에서 서로간키의 필요성","description":"","date":"2024-05-18 18:05","slug":"2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling","content":"\n\n```md\n![Surrogate Key](/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_0.png)\n\n# 문제 설명\n\n최근에 나는 대체키를 조합키 대신 사용하는 것을 연구하는 업무를 맡았습니다. 지금까지 우리 팀은 레코드를 고유하게 나타내고 차원과 사실을 차원 모델에 연결하는 데 조합 키를 사용했습니다. 조합 키는 작업을 수행했지만 Power BI 측에서 쿼리 실행 시간이 만족스럽지 않았습니다. 조인이 오랜 시간이 걸렸는데 그 이유는 조합된 키의 열 크기가 큰 것입니다. Power BI에서는 대규모 관계(키) 열이있을 때 쿼리가 훨씬 느리게 실행됩니다.\n\n내 목표는 Power BI에서 쿼리 실행 시간을 줄일 방법을 찾는 것이었습니다. 차원과 사실을 결합하기 위해 대체 키를 조합 키로 교체하는 것은 제가 자세히 탐색한 대안 중 하나였습니다. 이 기사에서는 대체 키의 정의, 목적 및 구현하는 대안 방법에 대해 안내하겠습니다. 실용적인 예제는 PySpark에서 보여집니다.\n```\n\n<div class=\"content-ad\"></div>\n\n# 대리 키란 무엇인가요?\n\n남들의 말을 바꾸는 대신, 관계형 데이터베이스의 맥락에서 대리 키 및 다른 유형의 키를 다루는 이 글을 참고해보세요.\n\n내가 대리 키를 정의하는 방식은 이러하다. 대리 키는 인공적이며 비즈니스나 현실 세계와 관련이 없으며 어떠한 비즈니스 개념과도 연결되지 않는다. 레코드를 고유하게 식별하는 데 도움이 되는 고유성을 가지고 있으며 대부분 (항상은 아니지만) 정수 형태이다.\n\n아래 직원 테이블에서 직원 ID 열이 대리 키의 예시입니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Surrogate Key](/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_1.png)\n\n서로게이트 키는 여러 가지 이유로 존재합니다. 하나의 명확한 답변은 해당 테이블에 자연 키의 명백한 후보가 없고 레코드를 고유하게 식별하기 위해 (서로게이트 키라고도 함) 가짜 키를 생성해야하는 경우입니다.\n\n두 번째 명백한 이유는 시간의 시험을 견디는 능력입니다. 이 이유는 데이터 웨어하우스 디자인의 맥락에서 더 관련이 있습니다. 기본 키로 자연 키에 의존하는 것은 데이터베이스 수준에서 시간이 지남에 따라 변경 사항에 취약해질 수 있습니다.\n\n구글의 주식 심볼을 예로 들 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n원래 구글 주식의 종류는 GOOG 주식 심볼을 가지고 있었습니다. 2014년 4월, 회사는 GOOGLE 주식 심볼 아래 새로운 주식 종류를 만들었습니다. 데이터베이스 디자인에서 구글 주식을 나타내는 주요 키로 원래 GOOG 심볼을 사용하고 있다면, 이 변경 사항을 반영하기 위해 적절한 수정을 해야 할 것입니다. Kimball (1998)은 이를 정확히 이유로 자연 키를 사용하는 것을 지양해야 한다고 주장했습니다. 그는 트랜잭션 데이터베이스의 자연 키는 \"생산의 지시에 따라 생성, 형식화, 업데이트, 삭제, 재활용 및 재사용\"되기 때문에 트랜잭션 데이터베이스 수준에서 발생하는 변경 사항에 지속적으로 의존해야 한다고 설명했습니다.\n\n대리 키의 세 번째 이유는 쿼리 성능이 더 좋다는 것입니다. 이는 특히 데이터 웨어하우스의 맥락에서 맞닿은 사실입니다. 차원 모델은 차원과 사실 사이에서 많은 조인을 수행해야 한다. 수치 (일반적으로 정수 또는 smallint)인 대리 키를 사용하여 조인하는 것은 문자열로 이루어진 자연 키나 복합 키로 조인하는 것보다 빠릅니다. 이 이유에 대해 더 자세히 알아보려면 여기를 읽어보세요.\n\n# 대리 키 생성의 대체 방법\n\n대리 키 구현에 대한 제 연구로 결과적으로 대리 키를 생성하는 주요 방법은 두 가지라고 결론내렸습니다.\n\n<div class=\"content-ad\"></div>\n\n한 가지 방법은 단조 증가하는 정수를 생성하는 전통적인 함수를 통해 정수를 만드는 것입니다. 이 함수는 테이블의 고유한 값에 대해 정수를 생성합니다.\n\n다른 대안은 암호 해싱 함수를 사용하는 것입니다. 해싱 함수는 데이터 자체에 대해 고유한 값을 생성하며, \"동일한 입력 집합은 항상 동일한 출력 집합을 생성한다\"는 의미입니다(Connors, 2022). 해싱 함수의 몇 가지 예는 crc32, md5, sha1/ sha2 등이 있습니다.\n\n이 두 가지 방법을 PySpark에서 어떻게 구현하는지 살펴봅시다.\n\n## 단조 증가하는 정수\n\n<div class=\"content-ad\"></div>\n\n쇼핑 목록에 제품 목록과 제품이 목록에 추가된 타임스탬프가 있는 테이블이 있다고 상상해보세요.\n\n| Product      | Timestamp            |\n|--------------|----------------------|\n| Product A    | 2024-09-15 10:30:00  |\n| Product B    | 2024-09-15 11:45:00  |\n| Product C    | 2024-09-15 13:20:00  |\n\n제품 차원의 서로 다른 키를 생성하려면 monotonically_increasing_id() 함수를 사용해야 합니다. 이 함수를 적용하기 전에 제품 목록이 고유한지 확인해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n위의 제품 테이블은 바나나와 빵이 2일에 나누어서 입력되어서 두 번씩 나타납니다. 따라서 단계는 제품 열만 선택한 다음, 값을 고유하게 만들고 monotonically_increasing_id() 함수를 적용하는 것입니다.\n\n한 가지 더 고려해야 할 사항은 테이블에 NULL 값이 있는지 여부입니다. 이를 함수를 적용하기 전에 필터링해야 합니다. NULL 값에 대한 대체 키 ID를 부여하고 싶지 않다면 필터링을 해야 합니다.\n\n```js\nfrom pyspark.sql.functions import monotonically_increasing_id, col\n\n#clean\ndf_select=df.select(\n  col('product')\n  )\ndf_distinct=df_select.distinct()\n\n#monotonically_increasing_id 함수를 사용하여 대체 키 생성\ndf_sk=df_distinct.withColumn(\n  \"surrogate_key\",\n  monotonically_increasing_id()\n  )\n```\n\n출력 결과는 다음과 같을 것입니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_3.png)\n\n모노토닉하게 증가하는 정수는 결정론적이 아니며, 동일한 값 목록에 대해 이 기능을 실행해도 항상 동일한 대리 키가 할당되지는 않습니다. 예를 들어, 사과는 항상 ID 1이 할당되는 것은 아닙니다. 대리 키의 유지보수는 따라서 몇 가지 추가 작업이 필요할 수 있습니다.\n\n이를 해결하는 한 가지 방법은 사실보다 차원을 먼저 로드하는 것입니다. 저의 팀에서 구현한 방법은 먼저 대리 키를 사용하여 차원을 생성한 다음 사실에서 차원을 사용하여 대리 키를 조회하는 것입니다. 이는 차원과 사실의 참조 무결성을 보장합니다.\n\n## 해싱 함수\n\n\n<div class=\"content-ad\"></div>\n\n해싱 함수에 대한 내 연구는 그들이 무엇인지 이해하고 대체 키 생성을 위한 전통적인 함수들이 언제 선호되는지를 이해하는 데 국한되어 있었습니다. 따라서 해싱 함수들 간의 차이점이나 그들을 어떻게 구현하는지에 대해 자세히 다루지는 않겠습니다. PySpark에서 구현 측면을 다루는 이 Medium 기사를 발견했습니다.\n\n나는 해싱 함수를 선택하지 않은 이유는 해싱 함수를 사용하여 생성된 대체 키들이 종종 더 긴 문자열 값을 갖기 때문입니다. 나가 해결하려고 하는 문제는 Power BI에서 조인의 쿼리 성능을 개선하는 것이었습니다. 긴 합성 키를 해싱 함수에 의해 생성된 긴 키로 바꾸는 것은 나의 문제에 적합한 해결책이 아니었습니다.\n\n하지만 해싱 함수를 매력적으로 만드는 것은 해싱 함수를 통해 생성된 대체 키들이 상기에서 논의된 단조증가 정수보다 유지하기가 훨씬 더 쉽고 (그리고 덜 복잡)이라는 점입니다. 해싱 함수는 결정론적이기 때문에 동일한 입력 세트는 항상 동일한 출력을 생성합니다. 이상적으로는 차원 및 사실을 병렬로 대체 키를 생성할 수 있습니다.\n\n# 요약\n\n<div class=\"content-ad\"></div>\n\n이 기사에서는 Power BI에서 쿼리 성능을 향상시키는 데 중점을 두며, 차원 모델의 컴포지트 키에 대한 효율적인 대안으로서 대리키의 사용을 탐색했습니다.\n\n비즈니스 의미를 가지지 않는 고유 식별자인 대리키는 데이터 웨어하우스에서 차원과 사실 사이의 관계 관리를 크게 최적화할 수 있습니다.\n\n우리는 이러한 키를 생성하는 다양한 방법을 탐구했습니다. PySpark에서 실용적인 예제를 통해 각 접근 방식의 이점과 고려 사항을 고려하여 데이터 아키텍처 요구에 맞는 올바른 전략을 선택하는 데 도움이 될 수 있도록 안내했습니다.\n\n# 자원\n\n<div class=\"content-ad\"></div>\n\n| Author                    | Title                                                                       | Year | Source           |\n|---------------------------|-----------------------------------------------------------------------------|------|------------------|\n| Ben                       | Database Keys: The Complete Guide (Surrogate, Natural, Composite & More)    | 2022 | Database Star   |\n| Connor, Dave              | Surrogate Keys In dbt: Integers or Hashes?                                   | 2022 | dbt             |\n| Kimball, R.               | Surrogate Keys                                                              | 1998 | Kimball Group   |\n| Stiglich, Peter           | Performance Benefits of Surrogate Keys in Dimensional Models                | -----| EWS Solutions   |\n\n<div class=\"content-ad\"></div>\n\n| 저자 | 제목 | 출판연도 | 출처 |\n|---|---|---|---|\n| Wikstrom, Max | Power BI Data Types In Relationships- Does It Matter? | 2022 | Data, Business Intelligence and Beyond |\n| Zaman, Ahmed Uz | PySpark Hash Functions: A Comprehensive Guide | 2023 | Medium |","ogImage":{"url":"/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_0.png"},"coverImage":"/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_0.png","tag":["Tech"],"readingTime":6},{"title":"YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기","description":"","date":"2024-05-18 18:03","slug":"2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer","content":"\n\n그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.\n\n![YoutubeDataPipeline](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png)\n\n## 사용 사례: —\n\n상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?\n\n<div class=\"content-ad\"></div>\n\n## 제안된 해결책: —\n\n위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:\n\n— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.\n\n— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n- 데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.\n\n이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.\n\n## 구현\n\n구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n도커 데스크톱을 설치해보세요 — https://www.docker.com/get-started/\n\n머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.\n\n```js\ndocker --version\nDocker version 20.10.23, build 7155243\n```\n\n최신 apache/airflow 이미지를 받아보세요\n\n<div class=\"content-ad\"></div>\n\n```sh\n도커 pull apache/airflow\n```\n\n다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.\n\n```sh\n도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone\n```\n\nAWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기\n\n<div class=\"content-ad\"></div>\n\nconfig\n\n```json\n[default]\nregion = ap-south-1\n```\n\ncredentials\n\n```json\n[default]\naws_access_key_id = AKIB******AXPCMO\naws_secret_access_key = 4D7HkaIBsqu***********+0AT2a8j\n```\n\n<div class=\"content-ad\"></div>\n\n지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.\n\n도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.\n\n![이미지](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png)\n\n웹 브라우저를 열고 `http://localhost:8080` 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.\n\n<div class=\"content-ad\"></div>\n\n`aws_write_utility.py` 파일을 만들 때 평소처럼 진행하시면 됩니다.\n\n<div class=\"content-ad\"></div>\n\n\n```js\nimport boto3\nimport json\nimport uuid\n\ndef write_json_to_s3(json_data, bucket_name, key_name):\n\n    # S3 클라이언트 초기화\n    s3 = boto3.client('s3')\n    \n    # JSON 데이터를 바이트로 변환\n    json_bytes = json.dumps(json_data).encode('utf-8')\n    \n    # JSON 데이터를 S3에 쓰기\n    s3.put_object(Bucket=bucket_name, Key=key_name, Body=json_bytes)\n\n\ndef generate_uuid():\n    \"\"\"UUID와 유사한 문자열 생성.\"\"\"\n    return str(uuid.uuid4())\n```\n\nyoutube_comments.py\n\n```js\n# -*- coding: utf-8 -*-\n\n# youtube.commentThreads.list를 위한 샘플 Python 코드\n# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:\n# https://developers.google.com/explorer-help/code-samples#python\n\nimport os\n\nimport googleapiclient.discovery\nimport aws_write_utility\nfrom aws_write_utility import write_json_to_s3\n\ndef start_process():\n    # 로컬에서 실행 시 OAuthlib의 HTTPS 확인 비활성화\n    # 제품 환경에서는 이 옵션을 활성화하지 마세요.\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    DEVELOPER_KEY = \"AIzaS*****************PiwBdaP_IE\"\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey=DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=\"snippet,replies\",\n        videoId=\"r_K*****PKU\"\n    )\n    response = request.execute()\n\n    process_comments(response)\n\n\ndef process_comments(response_items):\n\n    # 예시 S3 버킷 및 키 이름\n    bucket_name = 'youtube-comments-analysis'\n    key_name = 'data/{}.json'.format(aws_write_utility.generate_uuid())\n\n    comments = []\n    for comment in response_items['items']:\n        author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n        comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n        publish_time = comment['snippet']['topLevelComment']['snippet']['publishedAt']\n        comment_info = {'author': author, 'comment': comment_text, 'published_at': publish_time}\n        comments.append(comment_info)\n    print(f'총 {len(comments)}개의 댓글 처리 완료.')\n    write_json_to_s3(comments, bucket_name, key_name)\n```\n\nyoutube_dag.py\n\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom youtube_comments import start_process\n\n# 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 5, 16),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DAG 객체 생성\ndag = DAG(\n    'youtube_python_operator_dag',\n    default_args=default_args,\n    description='Python 함수를 호출하는 간단한 DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n# PythonOperator 작업 생성\npython_task = PythonOperator(\n    task_id='my_python_task',\n    python_callable=start_process,\n    dag=dag,\n)\n\n# 작업 간 의존성 정의\npython_task\n\n# DAG 등록\ndag\n```\n\n위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.\n\n좋아요!!\n\n이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.\n\n\n<div class=\"content-ad\"></div>\n\n이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.","ogImage":{"url":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png"},"coverImage":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png","tag":["Tech"],"readingTime":6},{"title":"AI로 기후 변화를 고칠 수 있을까 데이터 전문가의 견해","description":"","date":"2024-05-18 18:01","slug":"2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd","content":"\n\n![image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_0.png)\n\n기후 변화는 짜증나는 주제입니다. 정치인들은 그에 대해 의미 있는 조치를 취하기에 열의를 보이지 않습니다. 대부분의 사람들은 우리와 같은 힘없는 존재로 느껴지며 어떻게 도울지 모릅니다.\n\n그럼에도 불구하고, 기후 변화는 일어나고 있으며 아마도 가속화되고 있습니다(이 블로그 게시물의 데이터에서 나중에 확인하겠지요). 우리는 매 여름이 지난 여름보다 더 덥다는 세계에 살고 있는 것 같군요.\n\n처음으로 태어난 세대로서 가끔씩 심각하게 생각할 때가 있습니다. 미래 기후 대재앙을 겪게 될 자녀들을 이 세상에 데리고 오는 것이 공정한 일인지에 대해 🤔. 한편, 어떤 사람들은 우리를 (우리로부터) 구해 줄 수 있는 초지능 AI에 기대를 걸어 놓고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제에 대한 명확성을 얻기 위해 기후 변화에 관한 가장 중요한 데이터 지점들을 수집했습니다. 그리고 이 블로그 포스트에서는 데이터 관련 열광적인 사람의 시각을 통해 현재 상황을 함께 공유하겠습니다 📈.\n\n시작해봅시다!\n\n👉 참고: 이 블로그 포스트의 비디오 버전을 시청하거나 제 Youtube 채널에서 데이터 보고서를 확인할 수도 있습니다:\n\n# 펼쳐지는 기후 이야기\n\n<div class=\"content-ad\"></div>\n\n## 온실 효과\n\n2023년은 1850년 이후 기록된 가장 더운 해로, 이는 이례적인 해였습니다. 사실, 일부 데이터 세트가 제안하는 바에 따르면, 연간 평균 기온이 산업화 이전 기준 기간을 1.5°C 이상 초과하는 것은 이번이 처음입니다. 만약 이 용어가 익숙하지 않다면, 산업화 이전 시기는 1880년부터 1900년까지의 기준 기간입니다.\n\n이것이 그냥 이상 현상이거나 엘니뇨가 이 이례적으로 더운 해를 일으켰다고 주장할 수도 있습니다. 하지만 다음 그래프에서 볼 수 있듯이, 지난 4년 동안 평균 기온에서 매우 확고한 상승 추세를 볼 수 있습니다.\n\n![그래프](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_1.png)\n\n<div class=\"content-ad\"></div>\n\n파리협정 아래에서 많은 국가들이 장기적인 지구 온난화를 1.5°C(2.7°F) 이상으로 제한하려는 포부적인 목표를 세웠습니다. 이 목표는 많은 연도에 걸친 기후 상태를 기반으로 하기 때문에 1.5°C를 초과하는 단일 연도는 자동으로 이 목표를 위반한 것으로 간주되지 않습니다. 그러나 이것은 파리협정 목표를 초과하고 온클유어(Codesphere)글로벌 sandbox를 얼마나 가까이 왔는지에 대한 뚜렷한 경고 신호입니다.\n\n인간들이 대기 중에 더 많은 이산화탄소를 방출하면, 기후 온난화가 다음 10년 동안 정기적으로 1.5°C를 초과할 가능성이 높습니다.\n\n아래 그래프를 보면 대기 중 CO2 수준이 과거 백만 년 동안 어떤 시점에서도 점선 위로 올라가지 않았음을 알 수 있습니다. 그리고 빨간색으로 튀어나오는 이 스파이크는 지난 70년을 대표합니다. 나로서는 확실히 그 위에 앉고 싶지 않네요! \n\n![그래프 이미지](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_2.png)\n\n<div class=\"content-ad\"></div>\n\n지난 백 년 동안의 이산화탄소 배출 증가는 대부분 화석 연료 사용과 산업에 기인한다고 볼 수 있습니다. 다시 말해, 인간의 활동입니다.\n\n![Image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_3.png)\n\n기후 변화에 관한 정부간 기후변화협약 합동 심의 보고서에 따르면, 2030년까지 약속된 국가별 기여(NDCs)는 2030년대 초반에 온도가 1.5°C 상승할 것으로 나타났습니다. 만약 그게 사실이라면, 이세기 말까지 온도 상승을 2.0°C 이하로 유지하려고 노력할지도 모릅니다.\n\n기온 상승으로 인해 더위가 심한 지역은 더 덥게, 비가 많이 오는 지역은 더 많이 비가 오게 되며, 극한 기상 현상의 위험성과 강도는 크게 증가할 것으로 예상됩니다.\n\n<div class=\"content-ad\"></div>\n\n1.5°C 온난화 시, 지구 인구의 약 14%가 매 5년마다 적어도 한 번은 심한 폭염에 노출될 것으로 예상됩니다. 그러나 2°C 온난화 시, 해당 비율은 37%까지 늘어납니다. 극한 폭염 또한 보편화될 것으로 예상되며, 2°C 온난화 시에는 일부 국가에서 매년 치명적인 폭염이 발생할 수 있습니다.\n\n2°C를 넘는 온난화는 모든 극단적인 상황을 더욱 심각하게 만들 것으로 예상되며, 빈발하는 허리케인, 가뭄 및 산불이 포함됩니다. 더 많은 생태계가 심한 압력에 시달리게 될 것이며, 일부는 간단히 살아남지 못할 것입니다.\n\n인류에게는 먹을 음식을 생산해내기 어려워질 수도 있다는 의미입니다. 많은 사람들이 이주할 수 있고, 이는 국가를 불안정하게 만들 수 있습니다.\n\n## 녹는 얼음과 높아지는 바다수준\n\n<div class=\"content-ad\"></div>\n\n극단적인 날씨를 한쪽에 두고 또 다른 큰 문제가 있습니다: 극지방의 녹는 얼음입니다. 위 문제는 위성 자료와 지상 관측을 통해 모니터링할 수 있습니다.\n\n21세기 초부터 남극과 그린란드 빙하는 질량이 감소했습니다.\n\n![image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_4.png)\n\n남극과 그린란드 빙하는 지구의 물 중에서 99% 이상을 차지합니다. 이 둘이 완전히 녹는다면, 예상된 바다 수위 상승은 67.4미터 (223피트)에 이를 것으로 추정됩니다.\n\n<div class=\"content-ad\"></div>\n\n그래서, 우리는 멸망했을까요? 기후 종말은 피할 수 없을까요? 인공지능은 어떨까요? 우리를 구할 수 있을까요?\n\n자, 이제 인공지능이 우리가 기후 변화와 싸우는 데 어떻게 도움을 줄 수 있는지에 대해 이야기해 봅시다!\n\n## 인공지능이 기후 변화 대응을 돕는 방법\n\n2023년과 2024년에는 인공지능 개발에서 엄청난 발전이 있었습니다. GPT4, Gemini, 그리고 많은 오픈 소스 언어 모델들이 대중에게 공개되면서 기후 변화 대응을 함께 할 수 있게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\nAI는 아직 새로운 기후 정책을 작성하고 시행할 수는 없습니다만 (아직), 정치인들이 그런 것을 허용하지 않을지도 모르겠군요. 그러면 여러분이 궁금해할 수도 있는데, AI가 실제로 우리가 많은 기후 문제에 대처하는 데 어떻게 도움을 줄 수 있을까요?\n\nAI는 가장 기본적인 수준에서 우리가 무슨 일이 벌어지고 있는지 이해하는 데 도움을 줄 수 있고 문제를 인정하는 데 도움이 될 수 있습니다. 제 연구에서 본 AI의 주요 활용은 다음과 같은 범주에 속합니다:\n \n- 모니터링;\n- 예측;\n- 최적화.\n\n## 온실가스 (GHG)의 실시간 모니터링을 향해\n\n<div class=\"content-ad\"></div>\n\n기후 추적은 인공 지능과 기계 학습을 활용하여 전 세계적으로 온실 가스 배출량을 계산하는 이니셔티브로, 실시간 정확도를 향한 발전을 목표로 합니다. 이는 위치와 원천별 온실 가스 배출량에 대한 강력하고 무료이면서 독립적인 개요를 제공합니다. 저의 고향인 베트남 남부를 살펴보면, 석유 및 가스 분야에서 많은 배출량을 볼 수 있어 놀라지 않습니다. 또한 호치민 시 공항 주변에서 배출량이 집중되어 있는 것을 확인할 수 있습니다. 이 데이터는 Climate Trace 웹사이트에서 다운로드하여 자신의 연구에 활용할 수 있습니다. 정말 놀라운 일이죠!\n\n## 인공 지능이 빙산을 사람보다 10,000배 빨리 맵핑합니다\n\n또한, 연구자들은 위성 이미지에서 남극 빙산의 규모를 빠르고 정확하게 맵핑하고 모니터링하는 데 신경망 모델을 활용해왔습니다. 이는 빙산이 해양으로 녹는 양을 정량화하는 데 중요합니다.\n\n위성 이미지에서는 빙산, 해빙, 구름이 모두 하얗게 보여서 실제 빙산을 식별하기 어려운 경우가 많습니다. 그러나 신경망 모델은 이 작업을 훨씬 더 정확하고 효율적으로 처리합니다.\n\n<div class=\"content-ad\"></div>\n\n## AI를 활용한 쓰레기 재활용\n\nAI도 쓰레기 관리를 더 효율적으로 만들고 있어요. 쓰레기는 메탄의 큰 배출원이며 상당 수의 이산화탄소 배출을 담당하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n물체 감지를 위해 머신 러닝 시스템을 사용하여 한 스타트업이 2022년에 67가지 종류의 쓰레기 범주에서 320억 개의 폐기물 항목을 추적했습니다. 회사는 회수할 수 있는 재료의 평균량으로 86톤을 식별했지만, 이 재료는 폐기물 처리장에 보내지고 있습니다. 전 세계의 대형 슈퍼마켓도 수요를 예측하고 이를 통해 폐기물을 줄이기 위해 AI를 사용하고 있습니다.\n\n## AI가 바다를 청소하고 있습니다\n\n네덜란드의 환경 단체인 The Ocean Cleanup은 바다로부터 플라스틱 오염물을 제거하는 데 도움을 주기 위해 AI와 다른 기술을 사용하고 있습니다.\n\n물체를 감지하는 신경망 알고리즘은 해당 조직이 원격 지역에 있는 해양 폐기물의 상세지도를 작성하는 데 도와주고 있습니다. 이 해양 폐기물은 그 후 수거 및 제거될 수 있으며, 이는 이전의 트롤러 및 항공기를 사용한 청소 방법보다 효율적입니다.\n\n<div class=\"content-ad\"></div>\n\n## 인공지능이 기후 재해를 예측하는 데 도움을 줍니다\n\n인공지능 모델은 온도와 기후 재해를 더 정확하게 예측하는 데 도움이 되었습니다. GraphCast라는 AI 모델은 10일간의 기상 예측을 제공하는 것뿐만 아니라 극단적인 기상 현상에 대한 빠른 경보도 제공합니다. 이 모델은 태풍의 경로를 예측하고 홍수 위험과 관련된 대기 강과 극한 온도의 발생을 예측할 수 있습니다. 이는 생명을 구할 수 있는 잠재력이 있다는 것을 의미합니다.\n\n지금까지 저는 AI가 주로 기계 학습 도구로 사용되어왔다고 느꼈고, AI가 자체적으로 새로운 해결책을 만드는 데 사용되는 것을 보지 못했습니다. 앞으로, 만약 우리가 인공 일반 지능을 갖게 된다면, 이러한 종류의 AI가 더 많은 일을 할 수도 있을 것입니다.\n\n# 어떻게 인공지능이 상황을 악화시키고 있는지\n\n<div class=\"content-ad\"></div>\n\n그러나 몇몇 회의론자들(source 1, source 2)은 AI가 \"행성을 구하는 데 도움이 될 것\"에 너무 낭만적으로 생각해서는 안 된다고 생각합니다. 그들은 인공 지능이 기후 위기 해결에 도움이 될 것이라는 주장이 잘못되었다고 믿습니다.\n\nGPT4와 Gemini 같은 모델은 훈련하고 실행하는 데 엄청난 양의 에너지가 필요합니다.\n\n![image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_5.png)\n\nAI 모델의 탄소 발자국을 정확하게 계산하는 것도 어렵습니다. 왜냐하면 OpenAI나 Google과 같은 기업들은 보통 자신들의 모델에 대한 상세 명세를 게시하지 않기 때문입니다.\n\n<div class=\"content-ad\"></div>\n\n가장 간단한 형태로, AI 모델의 탄소 발자국은 모델을 훈련하는 데 필요한 에너지에 쿼리의 수와 각 쿼리가 필요로 하는 에너지를 더한 것과 같습니다. 이 모든 것은 하드웨어의 에너지 효율성에 의해 곱해질 것입니다.\n\n탄소 발자국 = (전기 에너지 훈련 + 쿼리 수 × 전기 에너지 추론) × CO2e 데이터 센터/KWh,\n\n여기서 CO2e 데이터 센터는 데이터 센터의 CO2 효율성을 의미합니다.\n\n대부분의 기업은 AI 모델을 제공하는 데 (추론 수행) 훈련하는 것보다 훨씬 더 많은 에너지를 사용합니다. 실제로, 에너지의 90%가 제공하는 데 사용된다고 추정됩니다. AI의 전기 수요는 산업과 발걸음을 맞추기 위해 데이터 센터의 2배 증설이 필요하다는 의미입니다.\n\n<div class=\"content-ad\"></div>\n\n미국에서는 AI가 막대한 양의 전력을 필요로 하기 때문에 에너지 수요를 충족하기 위해 여전히 오래된 석탄 발전소가 필요합니다. 정말 무서운 일이죠!\n\nOpenAI와 Google과 같은 기업은 자사 모델의 환경 영향을 일반적으로 공개하지 않습니다. 현재까지는 추측과 예측만 있습니다. 나는 대규모 AI 모델에 대한 환경 영향 보고가 규제되어야 한다고 생각합니다.\n\n# 지난 수십 년간의 추이\n\n최근 몇 년간 화석 연료의 증가 추세가 있는지 확인하기 위해 지난 10년간의 에너지 소비를 더 자세히 살펴보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n풍력 및 태양 에너지와 같은 재생 에너지 생산을 더 싸고 매력적으로 만드는 기술들도 있습니다. 많은 사람들이 친환경적인 솔루션을 선호하면서 전기 자동차가 많은 유럽 국가에서 일상적인 것으로 자리 잡았습니다.\n\n<div class=\"content-ad\"></div>\n\n모든 것이 잘되었다는 것을 의미하는 것은 아니며, AI 모델의 발전이 환경에 눈에 띄는 피해를 일으킨 적이 없다는 것은 아닙니다. 이것은 앞으로 몇 년 동안 더 명백하게 관찰될 수 있을 것으로 예상됩니다.\n\n# 결론\n\n우리가 본 바와 같이, 우리가 조심하지 않으면 AI는 이중날을 수도 있습니다. 현재는 혜택이 비용을 상쇄하는지에 대해 명확한 증거가 충분하지 않습니다.\n\n한편, 데이터는 우리가 아무것도 하지 않으면 기후 변화가 어떻게 진행될지 명백하게 보여줍니다. 그러나 우리는 해결책과 인간의 창의력에 한이 없음을 보았습니다.\n\n<div class=\"content-ad\"></div>\n\n많은 개인, 스타트업, 그리고 기관들이 기후 위험의 다양한 측면을 해결하기 위해 노력하고 있어요.\n\n이것이 저에게 매우 긍정적인 느낌을 줍니다. 세상을 바꾸고 싶다면, 먼저 변화가 가능하다고 믿어야 하며 어떤 문제도 해결하기에는 너무 크지 않다고 생각해야 합니다. 더 많은 젊은 사람들이 영향력 있는 자리로 진출하면서 기후 변화를 우선순위로 삼고 새로운 해결책에 열을 올리고 있어요. 기대를 해봅니다. 수 년 후에는 인공지능이 우리에게 미래 기후를 위한 중요한 한걸음을 내딛도록 도와줄 것이며, 혜택이 실제로 비용을 상회할 것이라고 믿어요.\n\n더 많은 정보를 찾고 싶다면, 아래의 Datalore 노트북에 있는 데이터 보고서를 확인해보세요!\n\n# 데이터 소스\n\n<div class=\"content-ad\"></div>\n\n1. 1850년부터 1900년까지의 전 세계 평균 기온(ºC)\n\n- Met Office: [링크](https://climate.metoffice.cloud/temperature.html#datasets)\n\n2. 지난 800,000년 동안 대기 중 CO2 수준\n\n- 모든 연구: [링크](https://www.ncei.noaa.gov/access/paleo-search/study/17975)\n- 수정된 코어 CO2 데이터 800,000–현재까지(2001): 남극 빙하 코어 수정된 복합 및 개별 코어 CO2 데이터\n- 2002년부터 2023년까지의 기간에는 여기에서 사용 가능한 현대 계기 데이터를 사용해야 합니다: [링크](https://climate.nasa.gov/vital-signs/carbon-dioxide/)\n\n<div class=\"content-ad\"></div>\n\n3. 세계 온실 가스 배출\n\n- [ourworldindata.org/greenhouse-gas-emissions](https://ourworldindata.org/greenhouse-gas-emissions)\n\n4. 국가별 석탄, 석유 및 가스 소비\n\n- [ourworldindata.org/fossil-fuels](https://ourworldindata.org/fossil-fuels)\n- 석탄 소비에 대한 정보: [ourworldindata.org/grapher/coal-consumption-per-capita?tab=chart&country=High-income+countries~Upper-middle-income+countries~Lower-middle-income+countries](https://ourworldindata.org/grapher/coal-consumption-per-capita?tab=chart&country=High-income+countries~Upper-middle-income+countries~Lower-middle-income+countries)\n\n<div class=\"content-ad\"></div>\n\n5. Antarctic 및 Greenland 빙하의 녹는 얼음 시트: 1992-2020 IPCC AR6를 위한 빙하량 균형\n\n- [https://ramadda.data.bas.ac.uk/repository/entry/show?entryid=77b64c55-7166-4a06-9def-2e400398e452](https://ramadda.data.bas.ac.uk/repository/entry/show?entryid=77b64c55-7166-4a06-9def-2e400398e452)\n- [https://www.epa.gov/climate-indicators/climate-change-indicators-ice-sheets](https://www.epa.gov/climate-indicators/climate-change-indicators-ice-sheets)\n\n6. LLMs 훈련 비용\n\n- [https://tinyml.substack.com/p/the-carbon-impact-of-large-language](https://tinyml.substack.com/p/the-carbon-impact-of-large-language)\n- [https://www.statista.com/statistics/1384418/co2-emissions-when-training-llm-models/](https://www.statista.com/statistics/1384418/co2-emissions-when-training-llm-models/)\n\n<div class=\"content-ad\"></div>\n\nhttps://blog.jetbrains.com에 원래 게시되었습니다. 2024년 4월 10일.","ogImage":{"url":"/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_0.png"},"coverImage":"/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_0.png","tag":["Tech"],"readingTime":10},{"title":"위치 기반의 위성 이미지의 시계열을 표시하는 대화형 지도 만들기","description":"","date":"2024-05-18 17:58","slug":"2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery","content":"\n\n\n![image](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_0.png)\n\n# Table of Contents\n\n- 🌟 Introduction\n- 📌 Area of Interest (AOI)\n- 💾 Loading Sentinel-2 Imagery\n- ⏳ Extracting Time Series from Satellite Imagery\n- 🌍 Developing an Interactive Map with Time Series\n- 📄 Conclusion\n- 📚 References\n\n## 🌟 Introduction\n\n\n<div class=\"content-ad\"></div>\n\n한동안 사용자가 특정 위치를 클릭할 때 상호작용 맵에 시계열 데이터가 표시되는 쉽고 직관적인 방법을 찾고 있었습니다. 저는 folium 라이브러리를 탐색하고 위성 이미지에서 추출한 시계열 데이터베이스를 folium과 연결하는 방법을 알아냈습니다. 오늘은 내 방법을 공유하겠습니다.\n\n이 게시물에서는 두 가지 함수를 작성할 것입니다. 첫 번째 함수는 위성 데이터를 다운로드하지 않고 로드하고, 두 번째 함수는 데이터와 타임스탬프를 추출하여 데이터 형식의 시계열을 생성합니다. 그런 다음 AOIs(관심 지역)를 순환하여 두 함수를 실행하고 AOI에 대한 시계열 데이터를 추출할 것입니다. 마지막으로 생성된 시계열 데이터와 folium 라이브러리를 사용하여 이를 상호작용적인 지도에 지리적으로 표시할 것입니다.\n\n이 게시물을 마치면 어떤 변수나 매개변수에 대해 추출된 시계열 데이터를 상호작용적인 맵과 시각적으로 표시할 수 있게 될 것입니다. 예를 들어, 캘리포니아 호수 면적의 시계열을 추출하고 상호작용적 지도에 표시하겠습니다. 그러나 흥미를 갖거나 이러한 조언과 꿀팁을 찾고 있었다면 계속 읽어보세요!\n\n## 📌 관심 지역 (AOI)\n\n<div class=\"content-ad\"></div>\n\n소개에서 언급한 대로 상호작용적 지도에서 어떤 위치의 변수에 대한 시계열 데이터를 표시하려면 다음 단계를 따를 수 있습니다. 이 예시에서는 캘리포니아의 몇 개 호수의 물 픽셀을 계수하고 2024년에 촬영된 모든 Sentinel-2 이미지를 사용하여 표면적을 계산할 것입니다. QGIS에서 그 호수 주변에 폴리곤을 그리고 그것들을 shapefile로 저장했습니다. 관심 영역 (AOI)에 대한 shapefile을 만드는 방법을 배우고 싶다면, Medium의 첫 번째 스토리에서 \"🛠️ QGIS에서 Shapefile 생성\"이라는 섹션을 참조해주세요.\n\n다음은 QGIS에서 호수 주변에 그린 폴리곤의 스냅샷입니다:\n\n![lakes](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_1.png)\n\n## 💾 Sentinel-2 영상 불러오기\n\n<div class=\"content-ad\"></div>\n\n이 섹션의 목표는 다운로드 없이 아카이브된 위성 이미지를 메모리에 로드하는 것입니다. 특정 지역에 대한 오랜 기간에 걸친 위성 이미지 다운로드는 시간이 많이 소요되며 계산 비용이 많이 들며 비효율적일 수 있습니다. 특히 전체 장면에서 작은 영역의 변화를 탐색하려면 문제가 될 수 있습니다.\n\n이러한 문제를 극복하기 위해 12줄의 코드만 사용하여 다운로드 없이 위성 이미지를 로드하는 방법을 보여주는 포스트를 작성했습니다. 해당 포스트에서 확인할 수 있습니다.\n\n이 섹션에서는 해당 포스트에서 제시된 템플릿을 사용하여 함수를 작성할 것입니다. 이 함수를 사용하면 특정 기간에 대한 AOI 위성 이미지를 쉽게 로드할 수 있습니다. 해당 기간이 길든 짧든 상관없이:\n\n```python\nfrom pystac_client import Client\nfrom odc.stac import load\n\ndef search_satellite_images(collection=\"sentinel-2-l2a\",\n                            bbox=[-120.15,38.93,-119.88,39.25],\n                            date=\"2023-01-01/2023-03-12\",\n                            cloud_cover=(0, 10)):\n    \"\"\"\n    Collection, 범위, 날짜 범위 및 구름 덮개를 기반으로 위성 이미지를 검색합니다.\n\n    :param collection: Collection 이름 (기본값: \"sentinel-2-l2a\").\n    :param bbox: 경계 상자 [min_lon, min_lat, max_lon, max_lat] (기본값: Lake Tahoe 지역).\n    :param date: 날짜 범위 \"YYYY-MM-DD/YYYY-MM-DD\" (기본값: \"2023-01-01/2023-12-30\").\n    :param cloud_cover: 구름 덮개 범위를 나타내는 Tuple (최소, 최대) (기본값: (0, 10)).\n    :return: 검색 기준에 따라 로드된 데이터.\n    \"\"\"\n    # 검색 클라이언트 정의\n    client=Client.open(\"https://earth-search.aws.element84.com/v1\")\n    search = client.search(collections=[collection],\n                            bbox=bbox,\n                            datetime=date,\n                            query=[f\"eo:cloud_cover<{cloud_cover[1]}\", f\"eo:cloud_cover>{cloud_cover[0]}\"])\n\n    # 일치하는 항목 수 출력\n    print(f\"발견된 이미지 수: {search.matched()}\")\n\n    data = load(search.items(), bbox=bbox, groupby=\"solar_day\", chunks={})\n\n    print(f\"데이터에서의 날짜 수: {len(data.time)}\")\n\n    return data\n```\n\n<div class=\"content-ad\"></div>\n\n이 함수를 사용하면 위성 이미지 검색을 위한 매개변수를 지정할 수 있어서 다양한 관심 영역 및 시간대에 유연하고 쉽게 사용할 수 있습니다. 수집, 경계 상자, 날짜 범위 및 구름 양 등과 같은 기준에 따라 위성 이미지를 검색합니다. 이는 이미지를 찾기 위해 Earth Search API를 사용하고 일치하는 수를 출력하며 큐브 형식으로 클립 된 이미지를 반환합니다.\n\n## ⏳ 위성 영상에서 시계열 추출\n\n이제 AOI를 위해 클립 된 이미지를로드하는 함수가 있으므로 찾고있는 정보를 추출하는 두 번째 함수를 정의해야합니다. 이미지와 함께 필요한 정보를 추출하고 맵에 표시하는 다음 단계에서 사용할 수 있도록 DataFrame에 넣어 시계열 데이터베이스로 고려할 수 있습니다. 다시 한 번 필요한 데이터를 추출 할 수 있지만, 전체 캘리포니아 호수의 표면적을 볼 수있는 것이 흥미로울 것으로 생각되어 2024 년에 Sentinel-2로 촬영 된 모든 이미지에서 최근 이미지를 포함하여 주요 캘리포니아 호수의 표면적을 볼 수 있습니다.\n\n이를 위해 Sentinel-2 이미지의 씬 분류 레이어에 있는 물 픽셀로 분류 된 픽셀을 추출 할 수 있습니다. 다시 말해, 각 씬에서 물 픽셀 수를 세어야합니다. 픽셀 해상도가 10m x 10m임을 감안하면 수를 100 제곱 미터 (10m x 10m)로 곱하면 각 호수의 표면적이 나올 것입니다. 그러나 여기서는 위성에 의해 촬영 된 이미지가 각 씬에서 전체 호수를 커버하도록해야합니다. 이를 설명하기 위해 1 월 7 일과 1 월 4 일에 촬영 된 이 두 장의 이미지 중 하나에 캡처된 호수 중 하나를 살펴 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n\n<div class=\"content-ad\"></div>\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef count_water_pixels(data,lake_id):\n    \"\"\"\n    각 시점의 Sentinel-2 SCL 데이터에서 물 픽셀 수를 계산합니다.\n\n    :param data: Sentinel-2 SCL 데이터가 포함된 xarray Dataset입니다.\n    :return: 날짜, 물 횟수 및 눈 횟수가 포함된 DataFrame입니다.\n    \"\"\"\n\n    water_counts = []\n    date_labels = []\n    water_area = []\n    coverage_ratio = []\n\n    # 시간 단계 수를 확인합니다.\n    numb_days = len(data.time)\n\n    # 각 시간 단계를 반복합니다.\n    for t in range(numb_days):\n        scl_image = data[[\"scl\"]].isel(time=t).to_array()\n        dt = pd.to_datetime(scl_image.time.values)\n        year = dt.year\n        month = dt.month\n        day = dt.day\n\n        date_string = f\"{year}-{month:02d}-{day:02d}\"\n        print(date_string)\n\n        # 물에 해당하는 픽셀 수를 계산합니다.\n        count_water = np.count_nonzero(scl_image == 6)  # 물\n\n        surface_area = count_water * 10 * 10 / (10 ** 6)\n\n        count_pixels = np.count_nonzero((scl_image == 1) | (scl_image == 2) | (scl_image == 3) | (scl_image == 4) | (\n                    scl_image == 5) | (scl_image == 6) | (scl_image == 7) | (scl_image == 8) | (scl_image == 9) | (\n                                               scl_image == 10) | (scl_image == 11))\n        total_pixels = data.dims['y'] * data.dims['x']\n\n        coverage = count_pixels * 10 * 10 / 1e6\n        lake_area = total_pixels * 10 * 10 / 1e6\n\n        ratio = coverage / lake_area\n\n        print(coverage)\n        print(lake_area)\n        print(ratio)\n\n        if ratio < 0.8:\n            continue\n\n        # 추가\n        water_counts.append(count_water)\n        date_labels.append(date_string)\n        water_area.append(surface_area)\n        coverage_ratio.append(ratio)\n\n    # 날짜 레이블을 pandas datetime 형식으로 변환합니다.\n    datetime_index = pd.to_datetime(date_labels)\n\n    # DataFrame을 구성하기 위한 딕셔너리 생성\n    data_dict = {\n        'Date': datetime_index,\n        'ID': lake_id,\n        'Water Counts': water_counts,\n        'Pixel Counts': count_pixels,\n        'Total Pixels': total_pixels,\n        'Coverage Ratio': coverage_ratio,\n        'Water Surface Area': water_area\n    }\n\n    # DataFrame 생성\n    df = pd.DataFrame(data_dict)\n\n    return df\n```\n\n이 함수는 데이터셋의 각 시간 단계를 반복하여 물 픽셀 수를 계산하고 표면적을 계산하며 커버리지 비율을 계산합니다. 커버리지 비율이 80% 미만이면 시간 단계가 건너뜁니다. 그런 다음 횟수, 날짜, 표면적 및 커버리지 비율을 리스트에 추가하고 해당 값과 물 ID 및 총 픽셀 수가 포함된 DataFrame을 반환합니다.\n\n커버리지 문제와 해결하는 속임수에 대해 자세히 알아보려면 이 포스트의 섹션 (📈 통계 파일에서 대염해 지역의 시계열)을 참조해주세요:\n\n## 🌍 시계열과 함께 상호작용하는 지도 개발하기```\n\n<div class=\"content-ad\"></div>\n\n이 섹션에서는 세 개의 스크립트를 작성할 것입니다. 첫 번째 스크립트는 다각형(AOI)의 바운딩 박스와 중심 좌표를 추출하는 함수입니다. 첫 번째 함수(search_satellite_images)를 실행하려면 바운딩 박스가 필요하며, 맵에 호수를 표시하는 데 중심 좌표가 필요합니다. 다음 코드로 이 작업을 수행할 수 있습니다:\n\n```js\nimport geopandas as gpd\nimport pandas as pd\n\ndef get_centroids_and_bboxes(shapefile_path):\n    \"\"\"\n    shapefile을 처리하여 각 다각형의 ID, 중심점, 바운딩 박스(bbox)를 포함하는 DataFrame을 반환합니다.\n    :param shapefile_path: shapefile의 경로.\n    :return: 각 다각형의 ID, 중심점, 및 bbox가 있는 pandas DataFrame.\n    \"\"\"\n\n    # shapefile 불러오기\n    gdf = gpd.read_file(shapefile_path)\n\n    # EPSG:4326으로 재투영\n    gdf_proj = gdf.to_crs(\"EPSG:4326\")\n\n    centroids = []\n    bboxes = []\n\n    # 각 다각형을 처리하여 중심점과 bbox 얻기\n    for index, row in gdf_proj.iterrows():\n        # 중심점\n        centroid_lat = row.geometry.centroid.y\n        centroid_lon = row.geometry.centroid.x\n        centroids.append((centroid_lat, centroid_lon))\n\n        # 바운딩 박스\n        minx, miny, maxx, maxy = row.geometry.bounds\n        bbox = (minx, miny, maxx, maxy)\n        bboxes.append(bbox)\n\n    # DataFrame 생성\n    df = pd.DataFrame({\n        'ID': gdf_proj.index,\n        'Centroid_Lat': [lat for lat, lon in centroids],\n        'Centroid_Lon': [lon for lat, lon in centroids],\n        'BBox_Min_Lon': [bbox[0] for bbox in bboxes],\n        'BBox_Min_Lat': [bbox[1] for bbox in bboxes],\n        'BBox_Max_Lon': [bbox[2] for bbox in bboxes],\n        'BBox_Max_Lat': [bbox[3] for bbox in bboxes]\n    })\n\n    return df\n\nshapefile_path = \"lakes_boundry.shp\"\nlakes_df = get_centroids_and_bboxes(shapefile_path)\nprint(lakes_df)\n```\n\n위 단계를 따르고 코드를 성공적으로 실행하면, 다음 형식의 다각형에 대한 유사한 DataFrame이 표시될 것입니다:\n\n<img src=\"/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n다음 스크립트는 2024년 센티넬-2에 의해 촬영된 모든 이미지를 호수 위에서 루핑하고 두 번째 함수를 실행하여 커버리지 비율이 80%보다 높은 경우 각 이미지에서 표면적을 계산하며 각 호수의 표면적을 시계열로 보여주는 DataFrame을 보고하는 것을 포함합니다:\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\n\nall_water_pixels_dfs = [] \n\nfor lake_id in lakes_df.ID:\n    print(lake_id)\n    lake_df = lakes_df[lakes_df['ID'] == lake_id]\n\n    if not lake_df.empty:\n        bbox = [lake_df.iloc[0].BBox_Min_Lon, lake_df.iloc[0].BBox_Min_Lat,\n                lake_df.iloc[0].BBox_Max_Lon, lake_df.iloc[0].BBox_Max_Lat]\n\n        data = search_satellite_images(collection=\"sentinel-2-l2a\",\n                                       date=\"2024-01-01/2024-05-14\",\n                                       cloud_cover=(0, 5),\n                                       bbox=bbox)\n        # Pass the lake_id\n        water_pixels_df = count_water_pixels(data, lake_id)\n\n        # Append\n        all_water_pixels_dfs.append(water_pixels_df)\n\n# Concatenate all DataFrames into a single DataFrame\nfinal_df = pd.concat(all_water_pixels_dfs, ignore_index=True)\n```\n\n최종 DataFrame은 이미지 날짜, 물 픽셀 수, 총 픽셀 수, 커버리지 비율 및 표면적을 요약하여 다음과 같이 보여집니다:\n\n![이미지](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_5.png)\n\n\n<div class=\"content-ad\"></div>\n\n거의 다 왔어요!\n\n지도 상에서 시계열을 보기 위해 마지막 한 단계가 남았습니다. 이제 표면적의 시계열 데이터가 있으므로 Folium 라이브러리를 사용하여 두 가지를 표시할 수 있습니다: (1) 지도상의 호수 중심을 지점으로 표시하고 (2) 각 호수를 클릭하면 팝업으로 표면적의 시계열을 보여주는 그래프를 표시합니다. 다음 코드로 이 작업을 수행할 수 있습니다:\n\n```js\nimport folium\nimport plotly.express as px\nimport os\n\n# 시계열 플롯을 그려 HTML로 저장하는 함수\ndef plot_timeseries_for_spot(spot_id, ts_df):\n    df_spot = ts_df[ts_df['ID'] == spot_id]\n    print(df_spot)\n    fig = px.line(df_spot, x='Date', y='Water Surface Area', title=f'Time Series for Lake {spot_id}')\n\n     # X 및 Y 축 레이블 추가\n    fig.update_layout(\n        xaxis_title=\"Date\",\n        yaxis_title=\"Water Surface Area (sq km)\"\n    )\n\n    filepath = f'tmp_{int(spot_id)}.html'\n    fig.write_html(filepath, include_plotlyjs='cdn')\n    return filepath\n\n# 지도 생성\nm = folium.Map(location=[35.5, -119.5], zoom_start=7)\n\n# Plotly 시계열 팝업이 있는 마커 추가\nfor index, row in lakes_df.iterrows():\n    html_path = plot_timeseries_for_spot(row['ID'], final_df)\n    iframe = folium.IFrame(html=open(html_path).read(), width=500, height=300)\n    popup = folium.Popup(iframe, max_width=2650)\n    folium.Marker([row['Centroid_Lat'], row['Centroid_Lon']], popup=popup).add_to(m)\n\nm.save('map_with_timeseries.html')\n\n# 임시 HTML 파일 정리\nfor spot_id in lakes_df['ID']:\n    os.remove(f'tmp_{spot_id}.html')\n```\n\n이 스크립트에서는 함수가 각 호수의 시계열 데이터를 필터링하고, Plotly를 사용하여 라인 플롯을 생성하고, 플롯을 HTML 파일로 저장합니다. 다음으로 Folium을 사용하여 지도를 초기화합니다. 그런 다음 호수 DataFrame을 반복하면서, 각 호수의 중심 좌표에 마커를 추가하고, 각 마커에 팝업을 연결하여 시계열 플롯을 표시합니다. 최종 지도는 HTML 파일로 저장됩니다. 마지막으로, Plotly 플롯에 생성된 임시 HTML 파일을 삭제하여 정리합니다.```\n\n<div class=\"content-ad\"></div>\n\n완료되었습니다!\n\n콘텐츠 폴더에 생성된 HTML 파일을 열면 지도에 표시된 각 호수의 중심 좌표를 볼 수 있습니다.\n\n![지도](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_6.png)\n\n각 호수를 클릭하여 시계열이 표시되는지 확인해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n이 모든 노력 끝에 이렇게 실용적인 지도가 만들어졌네요, 맞나요? :D\n\n## 📄 결론\n\n<div class=\"content-ad\"></div>\n\n거의 매달 새로운 패키지와 라이브러리들이 나와서 데이터를 추출하고 분석하며 표시하고 시각화하는 법을 실용적으로 제공합니다. 그러나 이 분야에서 아직 남아 있는 두 가지 과제가 있습니다. 첫 번째는 데이터를 정확하게 분석하기 위해서는 테라바이트 또는 페타바이트에서 추출된 데이터가 정확한지 확인하기 위해 충분한 경험이 필요합니다. 두 번째는 이러한 라이브러리들을 연결하여 의미 있는 것을 만들어내는 아키텍처를 만드는 것입니다.\n\n이미지 처리에서는 처리된 데이터에서의 간단한 실수가 중대한 오류로 이어질 수 있는 점을 강조해보았습니다. 시각화 부분에서는 Folium, Plotly, 그리고 새로운 이미지를 추출하기 위한 API를 연결하여, 리모트 센싱 관측을 사용하여 다양한 현상을 모니터링하는 유용한 도구를 만들 수 있음을 보여주었습니다. 이 글을 읽는 데 즐거움을 느끼시기를 바라며, 궁금한 사항이 있으시면 언제든지 연락 주세요.\n\n## 📚 참고 자료\n\nhttps://github.com/stac-utils/pystac-client/blob/main/docs/quickstart.rst\n\n<div class=\"content-ad\"></div>\n\nhttps://www.element84.com/earth-search/examples/\n\nSentinel 데이터용 Copernicus Sentinel 데이터 [2024]\n\nCopernicus 서비스 정보용 Copernicus 서비스 정보 [2024]\n\n📱 더 많은 흥미로운 콘텐츠를 제공하는 다른 플랫폼에서 저와 소통하세요! LinkedIn, ResearchGate, Github 및 Twitter.\n\n<div class=\"content-ad\"></div>\n\n이 링크를 통해 확인할 수 있는 관련 게시물이 있습니다:","ogImage":{"url":"/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_0.png"},"coverImage":"/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_0.png","tag":["Tech"],"readingTime":13},{"title":"Excel, Power BI 및 모든 분석 코스를 대체할 것입니다","description":"","date":"2024-05-18 17:57","slug":"2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses","content":"\n\n## 혹은 우리가 속는 법\n\n![image](/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_0.png)\n\n디지털 비즈니스 변화가 단순화와 효율성의 전례 없는 지평을 약속하는 시대에, 우리는 영원히 모든 데이터 분석 문제를 해결할 수 있는 해결책을 지속적으로 찾고 있습니다.\n\n우리가 익숙한 모든 것들보다 무한한 단순함과 우월성을 약속하는 서비스들이 여기에 있습니다. 그러나 우리가 기대하는 훌륭한 슬로건과 가혹한 (아니면 아닐까?) 현실을 비교해 봅시다.\n\n<div class=\"content-ad\"></div>\n\n## 베이스대시\n\n- 약속: 대시보드 조립 속도가 수동보다 100배 빠릅니다. AI 보조 기능이 인사이트를 찾아주죠!\n- 결과: 인터페이스는 괜찮지만 아직 사용자 친화적이지 않습니다. 엑셀 데이터를 끌어올 수는 없지만 SQL 데이터베이스 목록은 매우 많습니다. 대시보드를 자동으로 조립해주지 않고 그래프도 많지 않습니다. 이 도구는 대시보드용이 아니라 데이터를 한 곳에 통합하는 데 사용됩니다. 유용할까요? 아무도 모르겠네요...\n\n<img src=\"/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_1.png\" />\n\n## 플러그인 GPT — VizCritique Pro\n\n<div class=\"content-ad\"></div>\n\n- Promise: 다양한 기준에 따라 차트를 평가하고 점수를 매겨 피드백을 제공합니다. 구체적인 권고사항을 제공합니다.\n- Result: 우리는 많은 오류가 있는 가장 복잡한 3D 히스토그램에서 테스트를 진행했고, 7/10의 점수를 줬는데, 중요한 오류를 찾지 못했습니다. 너무 관대한 것 같아, 친구야!\n\n![이미지](/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_2.png)\n\n## 행\n\n- Promise: 전통적인 스프레드시트를 대체하고, 데이터 분석을 간소화하며 대시보드와 보고서 작성을 자동화하며 프로젝트에서 협업을 간편화합니다. + AI 어시스턴트.\n- Result: 장단점은 쉬운 데이터 통합과 다양한 템플릿이 있습니다. 하지만 솔직히 말해서, 여전히 옛날의 엑셀이 더 쉽고 편리합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Table Image](/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_3.png)\n\n### Julius\n\n- **Promise:** 사용자에게 강력한 AI 데이터 분석을 제공해 데이터를 편리한 인터페이스를 통해 분석하고 시각화할 수 있도록 합니다.\n- **결과:** 약속한 기능을 제공합니다. 빠르게 분석해야 할 필요가 있다면 좋은 도구입니다. 작업 흐름은 데이터 로드 - 인사이트 찾기 요청 - 차트 작성 요청 (또는 제안 목록에서 선택)입니다. 이것은 대시보드가 아니라 그냥 차트입니다.\n- 텍스트로 차트를 편집할 수 있는 멋진 기능이 있어서 제거할 부분이나 사용할 색상을 알려줄 수 있습니다.\n\n### Polymer Search\n\n\n<div class=\"content-ad\"></div>\n\n- 약속: 대시보드 생성 및 데이터 시각화 과정을 간단하게 만들어주기 위해 자동화된 생성 및 직관적인 템플릿 및 AI 기능을 제공합니다.\n- 결과: 와우! 지금까지 본 목록 중에서 가장 흥미로운 것 같아요. 데이터를 제공하면 대시보드를 직접 만들어줘요. 너무 멋진 것은 아니지만 이미 큰 발전이에요. 불필요한 것은 제거하고 필요한 것을 추가할 수 있어요. 차트가 많지 않고 설정이 유연하지 않아요. 하지만 전체적으로 좋은 시작이에요.\n- 흥미로운 기능 중 하나는 차트/카드 옆에 물음표를 클릭하면 프로그램이 차트에 대해 설명하고 제공하는 통찰을 제안하고 예측까지 해줘요!\n\n여기 예시 대시보드 보실 수 있어요 (하지만 이러한 힌트는 편집 모드에서만 볼 수 있어요).\n\n# 도덕\n\n진보와 혁신이 있더라도, 데이터 분석에서 인간의 경험과 지식의 가치는 변하지 않습니다. 현대 기술의 능력은 정말 인상적이지만, 그것들은 마스터의 손에 있는 도구에 불과해요. 얼마나 발전되었더라도, 분석가의 깊은 지식과 기술 없이는 그들의 전체 잠재력을 발휘할 수 없어요.\n\n<div class=\"content-ad\"></div>\n\n사랑하는 여러분, 함께 편안히 쉬지 말고 깃발을 잡고 앞으로 전진하여 새로운 도전과 높은 곳을 향해 빨리 나아가요! 😘\n\nData2Speak 웹사이트를 확인하고 LinkedIn 및 Twitter에서 팔로우하세요!\n\n![이미지](/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_4.png)","ogImage":{"url":"/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_0.png"},"coverImage":"/assets/img/2024-05-18-ThisWillReplaceExcelPowerBIandAllAnalyticsCourses_0.png","tag":["Tech"],"readingTime":3},{"title":"2024 보스턴 마라톤을 숫자로 살펴보기","description":"","date":"2024-05-18 17:54","slug":"2024-05-18-The2024BostonMarathonBytheNumbers","content":"\n\n![Image](/assets/img/2024-05-18-The2024BostonMarathonBytheNumbers_0.png)\n\n이번 주에는 3만 명의 러너들이 세계에서 가장 오래되고 가장 권위있는 마라톤인 보스턴을 위해 모여들었습니다.\n\n이것은 어려운 코스이며, 시카고, 런던 또는 베를린과 같이 초고속 시간으로 알려져 있지는 않습니다. 하지만 최고의 엘리트 러너들을 많이 끌어들입니다.\n\n그리고 그 뒤에는 다수의 러너들이 자격 기준을 충족시켜 입상 자격을 얻은 것으로, 이는 모든 주요 마라톤 중에서 가장 인상적인 집단이라 할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n하지만 4월의 날씨는 예측하기 어려운 경우가 많아요.\n\n올해의 날씨는 최근 기억에 있는 것 중 가장 나쁜 것은 아니었지만, 달리기에는 좋은 날씨는 아니었어요. Hopkinton에서는 높은 50도 초반에서 70% 이상의 습도로 시작되었어요. 완주자들이 보스턴에 도착했을 때는 온도가 높은 60도에서 낮은 70도까지 올랐어요.\n\n덥고 오르막과 겨울 훈련은 잠재적인 재앙의 원인이 될 수 있어요.\n\n이것이 완주자들에게 미친 영향은 무엇이었을까요? 그리고 완주자 데이터를 분석했을 때 어떤 결론들을 얻을 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n자세히 살펴보고 숫자로 알아봅시다. 2024 보스턴 마라톤을 탐색해 보세요.\n\n참고: 이 분석은 BAA에서 제공한 결과를 기반으로 합니다. 경우에 따라 해당 결과를 이전 연도와 결합하여 맥락을 제공했습니다. BAA는 완주자 데이터를 손쉽게 다운로드하고 분석할 수 있도록 제공하는 주요 경주 기관입니다.\n\n# 우승자는 이전 연도와 어떻게 비교되나요?\n\n더 큰 데이터셋으로 들어가기 전에, 더 간단한 질문부터 시작해보죠. 우승자들은 어떻게 한 걸까요?\n\n<div class=\"content-ad\"></div>\n\n남자 부분에서는 Sisay Lemma가 선두를 달렸어. 그는 일찍 앞서 나가서, 처음에는 그가 조금씩 따라잡힐 것으로 생각했어. 나는 나의 일요일 나만의 마라톤 경주 리포트를 쓰는 중이었기 때문에 경주를 보며 조금 멀리 빠져 있었어. 올려다볼 때, 그가 선두를 넓힌 것을 보고 조금 놀랐어.\n\n그 공격적인 움직임으로 그는 보스턴에서 좋은 시간을 기록했어 — 2:06:17. 그것은 Geoffrey Mutai의 2011년 코스 기록을 몇 분 남겨둔 것이지만, 이것은 여전히 올 타임 명단의 10위에 해당하는 좋은 기록이었어. 사실, 2011년 이후로는 그 코스 기록에 더 가까운 선수들은 지난 해에 이겨낸 세 명 뿐이었어.\n\n여자 부분에서는 상황이 훨씬 더 느리게 전개됐어. 전략적인 경주였고, 아주 늦게까지 여러 여성들이 함께 뛰는 큰 그룹이 있었어. 결국, 그들 중 몇 명이 움직임을 보였고 — Hellen Obiri가 최종 마일에서 Sharon Lokedi를 이기고 승리했어.\n\nObiri의 최종 시간 — 2:22:37 —은 Buzunesh Deba의 2014년 코스 기록에서 3분 남짓한 시간이 나왔어. 하지만 이것은 역대 여성 선수들의 상위 20위 명단에 들기에 충분하지 않았어.\n\n<div class=\"content-ad\"></div>\n\n다른 소식으로:\n\n- 40세가 된 엘카나 키벳은 40~44세 남성 그룹에서 새로운 최고 기록을 세웠습니다. 그의 2시 12분 32초는 2017년 압디 압디라만(2시 12분 45초)의 기록을 압도했습니다.\n- 진 다이크스는 75~79세 남성 그룹을 위한 새로운 최고 기록을 세웠습니다. 그는 3시 28분 43초에 완주했는데, 2015년 케이죠 타이바살로(3시 35분 21초)의 기록을 크게 앞섰습니다.\n- 세 번째로 완주한 에드나 키플라갓은 40~44세 여성들을 위한 세 번째 최고 기록을 세웠습니다. 그녀는 이제 그 목록에서 상위 세 개의 자리를 소유하고 있습니다. 사라 홀은 4번째와 5번째 자리를 소유하고 있습니다. 그리고 마지막 것은 이번 월요일에 완주한 것입니다.\n\n그래서 날씨가 그리 좋지 않았음에도 불구하고, 올해 보스턴 마라톤에서 엘리트들은 상당히 잘 해냈습니다.\n\n## 2024년 보스턴 마라톤 참가자들의 나이와 성별\n\n<div class=\"content-ad\"></div>\n\n현장의 나머지 부분으로 넘어가 볼까요? 간단한 질문부터 시작해봅시다.\n\n올해 보스턴 마라톤의 완주 선수들 비율은 어떻고, 이전 연도와 어떻게 비교되나요?\n\n올해는 지원자 수가 기록적으로 많았는데, BAA는 5분 이상의 엄격한 컷오프 시간을 시행해야 했습니다. 이에 대한 이전 분석 결과, 연령대가 높은 러너일수록 예선을 통과할 확률이 더 높고, 그 차이가 더 컸다는 사실을 발견했습니다.\n\n작년에만 더 큰 컷오프가 있었던 2021년을 떠올려봅시다. 그 해에는 참가자 수가 적어 7:47의 컷오프가 적용되었습니다. 2019년에도 4:52의 컷오프가 있었지만, 그 해는 예전 예선 기준을 사용하여, 효과적으로 가장 쉬운 해로 취급되었습니다.\n\n<div class=\"content-ad\"></div>\n\n따라서 그 시간 제한은 경기 참가자 구성에 영향을 미칩니까?\n\n이 시각화에서 각 색깔은 다른 연령 그룹을 나타냅니다. 맨 위의 막대는 남성 참가자의 비율을 나타내고, 아래의 막대는 여성 참가자의 비율을 나타냅니다. 백분율은 해당 연령 및 성별 그룹이 전체 참가자 중 차지하는 비율입니다.\n\n성별 측면에서, 올해의 참가자 구성은 2022년과 2023년과 유사합니다. 참가자 중 여성의 비율은 약 43%이고 남성의 비율은 57%입니다. 보스턴 역사상 가장 균형 잡힌 참가자 구성은 2021년에 있었고, 2019년보다 이번 해는 약간 더 성별 균형이 잡혔습니다.\n\n연령 측면에서, 가장 큰 차이점은 35세 미만의 젊은 남성 참가자와 50대 및 60대의 참가자들 사이에서 나타났습니다. 앞선 연령 그룹은 상당한 감소를 보였고, 후자의 연령 그룹은 상당한 증가를 보였습니다. 이는 깊은 시간 제한이 있기 때문에 이해할 수 있으며, 더 젊은 참가자 수가 줄어들고 더 많은 연장자들이 있었던 유일한 해는 2021년이었습니다(더 깊은 시간 제한과 참가자 수가 적은 연도).\n\n<div class=\"content-ad\"></div>\n\n# 이번 대회 우승자는 어느 주 출신인가요?\n\n올해 대회에 참가한 25,530명 중 17,335명은 미국에서 왔습니다. 여기서 흥미로운 관찰할 점이 있을까요?\n\n러너들의 가장 일반적인 고향 주는 매사추세츠입니다. 이것은 제휴사들과 경로 주변 도시들에게 상당 수의 비브가 제공된다는 가정하에 합리적입니다.\n\n이후로 캘리포니아, 뉴욕, 텍사스가 가장 일반적인 주입니다. 다시 말해, 이것은 그들이 모두 미국에서 가장 인구 많은 주들 중 상위에 있기 때문에 합리적입니다.\n\n<div class=\"content-ad\"></div>\n\n각 주가 과대 또는 과소 대표되는지 더 잘 이해하기 위해, 총 완주자 수를 인구에 비교했습니다. 이는 인구조사국의 2023년 인구 추정자료를 기반으로 하였습니다.\n\n그런 다음, 1백만명 당 완주자 수를 계산하여 아래 시각화로 나타내었습니다. 파란색의 짙은 음영은 해당 주가 다른 주보다 완주자 비율이 높다는 것을 나타냅니다.\n\n뉴 잉글랜드 지역 주들은 모두 매우 잘 대표되어 있습니다. 매사추세츠를 제외하고 다른 지역들은 주변에 있어 뛰기에 더 가깝고 편리한 것이 이러한 결과를 가져온 것으로 생각됩니다.\n\n다른 최상위 주들은 유타와 콜로라도이며, 이 두 주는 강력한 러닝 전통을 갖고 있습니다. 두 주 모두 해발고가로, 바닷물 수준에서 마라톤을 뛴다면 쉽게 예선을 통과할 수 있을 가능성이 있습니다.\n\n<div class=\"content-ad\"></div>\n\n대부분의 다른 잘 대표되는 주들은 해안 지역이나 중서부에 위치하고 있습니다. 보통, 미국 중부 지역에서는 예선자가 적고, 남부에서 더더욱 적다고 합니다.\n\n하지만, 루이지애나나 미시시피에서 일 년 내내 살았다면, 저도 달리기를 즐기지 않았을 것 같아요.\n\n이것은 나중에 또 다른 질문이 될 수도 있겠지만, 이 주들의 상대적인 부유함과 어떤 상관 관계가 있는지 궁금하네요. 달리기는 반드시 비싼 스포츠는 아니지만, 보스턴과 같은 대회에 참가하는 것은 비용이 많이 들어갈 수 있어요. 그래서 부유한 주일수록 보스턴에 참가하기 위해 예선자를 많이 배출하고 참가하는 경우가 더 많을지도 모르겠어요.\n\n어떤 이유 때문인지, 알래스카가 여기서 좀 놀랐어요. 백만 명 당 예선자 수를 기준으로 보면, 그들은 중위권에 위치하고 있어요. 제게 물었으면, 저는 그들이 목록에서 더 아래쪽에 있을 것으로 예상했을 텐데요.\n\n<div class=\"content-ad\"></div>\n\n# 국제 참가자에 대해서는 어떻게 될까요?\n\n하지만 17,000명정도의 완주자가 미국에서 온 것을 고려하면, 다른 8,000명정도는 어디서 왔을까요? 전 세계 각지에서 온 참가자들입니다.\n\n이 그래프는 인구에 크기에 따라 조정되어 있지 않습니다. 따라서 파란색의 진하기는 특정 국가에서 온 참가자의 절대 수를 나타냅니다. 예를 들어 중국은 상당히 많은 참가자를 보유하고 있었지만, 비례적으로는 훨씬 낮을 것입니다.\n\n<div class=\"content-ad\"></div>\n\n북미와 유럽이 가장 많은 러너들을 보유하고 있으며, 남미와 동아시아가 그 뒤를 이었습니다.\n\n이 그래프는 보스턴 마라톤에서 완주한 상위 10개 국가를 2024년과 2018년에 격리하여 나타냅니다. 대부분이 중복되므로 여기에는 12개 국가만 표시되어 있습니다. 전설의 국가 이름을 클릭하여 해당 국가를 숨기고 특정 국가에 집중할 수 있습니다.\n\n모든 국가에서 2021년에 큰 하락을 볼 수 있습니다. 해당 연도에 경주는 10월에 치러지고 참가 인원이 줄어들었으며, COVID로 인해 국제 여행이 훨씬 적어졌습니다.\n\n캐나다와 영국은 꾸준히 1위와 2위를 유지하고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n2018년 이후로, 멕시코와 브라질이 순위를 올라갔어요. 대만도 상당히 크게 증가했는데요 - 2018년 약 100명에서 2024년 약 300명으로 늘었어요.\n\n한편, 중국과 일본은 조금 감소했고, 나머지는 대부분 그대로 유지되었어요.\n\n재미있는 점은, 이 곳 보스턴의 상위 10개 국가 목록이 뉴욕시 마라톤에서 대표하는 상위 10개 국가 목록과 많은 면에서 다르다는 것이에요. 이 차이는 무엇이 이끌었을지 궁금하네요.\n\n이 국제 러너들에 대한 최종 시각화에서 미국이 아닌 다른 국가에 거주하는 완주자 총 수를 보여줘요.\n\n<div class=\"content-ad\"></div>\n\n2020년(레이스가 없었던 해)와 2021년(참가자 수가 크게 감소한 해)에는 분명히 하락이 있었습니다. 그러나 2022년 이후부터 국제 참여가 완전히 회복되었습니다.\n\n실제로 올해는 지난 어떤 해보다도 더 많은 국제 러너들이 참여하고 있습니다. 국제 수요가 증가한 신호인지, 아니면 국제 러너들이 미국 러너들보다 빠른 것인지(그리고 깊은 커트라인 시간 이내에 들어갈 가능성이 더 높은지) 궁금합니다.\n\n어쨌든, 내년 이 트렌드가 어떻게 변할지 보는 것은 흥미롭겠네요.\n\n# 참가자들의 시간이 영향을 받았을까요?\n\n<div class=\"content-ad\"></div>\n\n마지막으로 — 대중들이 경주에서 실제로 어떻게 하셨나요? 평소보다 빨랐나요? 더 느렸나요?\n\n아래 시각화는 두 가지 시간을 보여줍니다. 중위값은 '평균' 완주 시간으로, 다른 러너 중 절반은 앞쪽에 다른 절반은 뒤쪽에 도착한 시간을 의미합니다. 90번째 백분위는 훨씬 빠른 러너로, 동일한 연령 그룹의 다른 러너 중 90%보다 앞에 도착했을 사람을 의미합니다.\n\n드롭다운을 사용하여 남성과 여성을 전환할 수 있습니다. 시각적으로 깔끔하게 유지하기 위해 4개의 최연소 연령 그룹만 포함했지만 (35세 미만, 35-39세, 40-44세, 45-59세), 나이가 많은 러너들 사이에서도 패턴이 동일하다고 확신합니다.\n\n왼쪽을 보면 — 중위값을 살펴보면 — 2023년과 2024년 사이에 큰 점프가 있습니다. 남성 혹은 여성을 보더라도 동일합니다.\n\n<div class=\"content-ad\"></div>\n\n오른쪽으로 가면 언덕이 덜컥합니다. 그러나 보통 남녀 모두, 그리고 모든 연령 그룹에서, 올해의 완주자들 중 90번째 백분위는 작년에 비해 2~3분 느렸습니다.\n\n이제 조금 더 맥락을 살펴보겠습니다.\n\n올해, 모든 지원자는 컷오프로 인해 예선 시간을 최소 5분 이상 달성해야 했습니다. 2022년과 2023년에는 컷오프가 없었습니다. 따라서 전반적으로 이번 대회는 훨씬 더 빨랐습니다.\n\n모든 조건이 동일하다면, 올해의 완주 시간이 지난 두 해보다 빠를 것으로 예상됩니다. 그러나 실제로 그렇지 않았습니다. 이것은 다른 요인(아마도 날씨)이 완주 시간에 상당한 영향을 미쳤다는 명백한 신호입니다.\n\n<div class=\"content-ad\"></div>\n\n느린 러너(중간값과 같은)들이 나중에 출발하고 코스에서 더 많은 시간을 소비하는 것을 감안할 때, 따뜻한 날씨로부터 더 큰 영향을 받을 것이라는 것은 합리적입니다.\n\n# 몇 명의 러너가 다시 예선했나요?\n\n마지막으로 살펴볼 것은, 러너 중 몇 명이 내년 보스턴 마라톤을 위해 자격 시간을 충족하여 다시 예선했는지입니다.\n\n처음에는 거의 모든 사람이 재예선할 것이라고 생각할 수 있지 않나요?\n\n<div class=\"content-ad\"></div>\n\n하지만 보스톤에서 참가 자격 시간을 충족하지 못하는 이유가 몇 가지 있습니다:\n\n- 자격 시간 부적격으로 자석차량을 탄 자석 러너일 수 있습니다.\n- 시간을 늘리기 위해 내리막 마라톤을 이용했을 수도 있습니다.\n- 코스 자체가 힘들기 때문에, 가장자리에서 자격을 얻은 사람이 그 업적을 반복하기 어려울 수 있습니다.\n\n그럼에도 불구하고 많은 러너가 다시 자격을 얻을 것으로 예상됩니다. 이번 해와 지난 두 해를 비교하면, 가장 큰 차이점은 날씨일 것입니다.\n\n위 차트는 각 보스톤 마라톤의 완주자 수를 보여줍니다. 2019년부터 2024년까지, 그들이 보스톤에서 자신의 연령 그룹에 해당하는 자격 시간을 충족했을 때의 수를 보여줍니다. 즉, 다시 자격을 얻은 러너들의 수를 보여주는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n왼쪽의 막대는 예선 시간을 얼마나 넘어섰는지 나타내고, 오른쪽의 막대는 적어도 예선 시간보다 다섯 분 이상 빠른 러너 수를 나타냅니다.\n\n작년에는 13,736명의 선수가 자격 시간을 충족했습니다. 그 전년도에는 10,996명이었습니다. 올해는 9,810명으로 많이 줄었습니다.\n\n올해는 더 빠른 경기장이었을 것으로 가정할 수 있지만, 그 중 다시 자격을 획득한 선수는 훨씬 적었습니다. 날씨의 영향이 없다면, 이번 해는 예선 시간이 없었던 이전 두 해보다 더 많은 자격 획득자가 있기를 기대할 수 있습니다.\n\n2019년과 2021년은 여기에 함께 포함되어 있지만, 이번 해와 직접 비교하기에는 적합하지 않습니다. 2021년에는 참가자 수가 훨씬 적었으므로 더 적은 자격 획득자가 나타날 것으로 예상됩니다. 2019년에는 러너들이 이전 기준에 따라 자격을 획득했으므로, 전체가 (평균적으로) 약간 느렸다고 가정해 볼 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 2024 보스턴 마라톤에 대한 생각이 어땠나요?\n\n올해의 보스턴 마라톤에 참가했나요? 아니면 저처럼 TV로 시청했나요? 올해의 경기에 대한 당신의 반응과 응답을 듣고 싶어요.\n\n저는 경기를 관람할 때 날씨가 큰 영향을 끼치는 것을 생각하지 않았어요. 정상급 선수들에게는 그다지 영향을 미치지 않은 것 같았죠.\n\n하지만 경기 후에 여러 러너들이 오후의 더위를 견뎌낸 경험을 담은 경기 보고서와 감상문을 읽었어요. 여름을 통해 훈련하고 열에 적응한 사람들에게는 그리 나쁘지 않았을 수도 있지만, 겨울을 겨우 지나고 나온 사람들에게는 그랬나 봐요.\n\n<div class=\"content-ad\"></div>\n\n완주 시간 데이터는 이것이 상당히 일반적인 결과였음을 시사하는 것 같습니다.\n\n나는 보스톤으로 재지원하는 러너 수에 대한 마지막 시각적 자료를 포함했습니다. 사람들이 매우 관심을 갖고 있는 주제이기 때문에요. 올해의 절단선 이후에는 보스톤 2025에 대해 어떻게 될지에 대한 다양한 추측이 있습니다.\n\n보스톤 마라톤 자체가 보스톤 대회에 대한 1순위 진출 대회임을 감안하면, 작년보다 대회 참가자 수가 거의 4,000명 줄어든 사실은 (진출을 희망하는 사람들에게) 좋은 시그널입니다. 이는 절삭 시간이 전혀 없을 것을 의미하는 것은 아니지만, 최악의 시나리오 중 일부가 실현될 가능성이 줄어드는 것을 의미합니다.\n\n이 질문에 대한 더 철저한 답변은 데이터를 더 심층적으로 조사해야 하지만, 그 부분은 다른 날로 미루겠습니다.\n\n<div class=\"content-ad\"></div>\n\n만약 당신이 이에 관심이 있다면 — 또는 마라톤에 관한 다른 데이터 중심 이야기에 대해서라도 — 이메일 업데이트를 구독해주세요.\n\n저는 열심히 달리는 사람이자 데이터 열차인입니다. 이번 해의 보스턴 마라톤은 컷오프 시간을 놓쳤기 때문에 참가하지 못했지만, 2025년에 다시 도전할 예정입니다. 저의 소식을 계속 받고 싶다면 다음과 같은 방법이 있습니다:\n\n- 'Running with Rock'을 팔로우하여 제 훈련에 대해 들어보세요.\n- 마라톤 트레이닝 계획을 선택하는 팁을 확인해보세요.\n- Strava에서 저를 따라다니세요.","ogImage":{"url":"/assets/img/2024-05-18-The2024BostonMarathonBytheNumbers_0.png"},"coverImage":"/assets/img/2024-05-18-The2024BostonMarathonBytheNumbers_0.png","tag":["Tech"],"readingTime":9}],"page":"44","totalPageCount":61,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}