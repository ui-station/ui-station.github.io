{"pageProps":{"posts":[{"title":"AWS Lake Formation에서 데이터 필터를 사용하여 Terraform을 통해 여러 계정 간 액세스 활성화하기","description":"","date":"2024-05-18 16:54","slug":"2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform","content":"\n\n# 소개\n\n이전 블로그에서는 AWS Lake Formation과 Terraform을 사용하여 다른 계정 간 데이터 공유를 활성화하는 방법을 탐험했습니다. 이번 포스트에서는 데이터 필터를 사용하여 해당 설정을 더욱 향상시키는 방법에 대해 더 깊이 파고들어보겠습니다. Lake Formation 데이터 필터링을 통해 열 수준, 행 수준 및 셀 수준 보안을 구현할 수 있습니다. 이 블로그에서는 특히 셀 수준 보안을 구현하여 데이터 접근 제어를 미세 조정하는 데 중점을 둘 것입니다.\n\n![이미지](/assets/img/2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform_0.png)\n\n## 데이터 필터 수준\n\n<div class=\"content-ad\"></div>\n\n- 열 수준\n\n열 수준 필터링을 사용하여 데이터 카탈로그 테이블에 대한 권한을 부여하면 사용자가 액세스 권한이 있는 특정 열 및 중첩 열만 볼 수 있습니다. 이를 통해 중첩된 열의 부분 하위 구조에 액세스 권한을 부여하는 보안 정책을 정의할 수 있습니다.\n\n- 행 수준\n\n행 수준 필터링을 사용하여 데이터 카탈로그 테이블에 대한 권한을 부여하면 사용자가 액세스 권한이 있는 데이터의 특정 행만 볼 수 있습니다. 필터링은 하나 이상의 열 값에 기반하며, 중첩 열 구조도 행 필터 식에 포함할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n- 셀 수준\n\n셀 수준 보안은 행 및 열 필터링을 결합하여 매우 유연한 권한 모델을 제공합니다.\n\n# 소스 계정에 데이터 필터 만들기\n\n이미 이전 블로그에서 자세히 설명한 Source Account에서 Lake Formation 설정을 완료했다고 가정하면, 이제 데이터 필터를 만드는 작업을 진행할 수 있습니다. IIoT 측정을 사용한 예제를 사용해보겠습니다. 여러 사이트에 퍼져있는 장비가 있고 특정 IAM 역할에 특정 사이트 및 열 액세스를 부여해야 하는 경우를 가정해보겠습니다. 이를 Terraform을 사용하여 어떻게 구현할지 알아보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n아래 예시에서:\n\n- 로컬 구성 정의: filter_config 변수는 액세스가 필요한 사이트, 열 및 IAM 역할을 나열합니다.\n- AWS 계정 ID 검색: aws_caller_identity 데이터 원본은 현재 AWS 계정 ID를 가져옵니다.\n- 데이터 셀 필터 생성: aws_lakeformation_data_cells_filter 자원은 filter_config를 반복하여 각 IAM 역할에 필요한 필터를 생성합니다.\n\n이 설정을 통해 특정 IAM 역할이 정의된 사이트 및 열에만 액세스할 수 있도록 하여 보안과 데이터 관리를 강화할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 대상 계정과 공유 카탈로그\n\n이제 데이터 필터를 생성했으니, 카탈로그를 공유하는 동안 활용해봅시다. 아래 코드 스니펫에서는 데이터베이스와 테이블을 대상 계정과 공유할 것입니다. 테이블을 공유할 때는 이전 단계에서 생성한 데이터 필터를 함께 포함할 것입니다.\n\n![이미지](/assets/img/2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform_2.png)\n\n이 스니펫에서는:\n\n<div class=\"content-ad\"></div>\n\n- 데이터베이스 권한 공유: aws_lakeformation_permissions 리소스는 IIoTDataLake 데이터베이스를 대상 계정과 공유하여 DESCRIBE 권한을 부여합니다.\n- 테이블 권한 공유: 비슷하게, 해당 리소스는 측정 테이블을 대상 계정과 공유하며 SELECT 권한을 부여합니다. 이전에 생성한 데이터 필터도 포함되어 대상 계정이 정의된 기준에 따라 필터링된 데이터에만 액세스할 수 있도록 보장합니다.\n\n이 설정을 통해 카탈로그에서 대상 계정과 특정 데이터를 안전하게 공유하여 규정 준수 및 데이터 무결성을 보장할 수 있습니다.\n\n# 대상 계정에서 액세스용 리소스 링크 생성\n\n대상 계정에 카탈로그와 테이블을 데이터 필터와 함께 공유한 후, 공유된 카탈로그 데이터에 액세스하기 위해 대상 계정으로 이동하여 리소스 링크를 설정합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform_3.png)\n\nIn this setup:\n\n- **Retrieve Source Account ID:** The `aws_caller_identity` data source fetches the AWS account ID of the source account.\n- **Create Resource Link:** The `aws_glue_catalog_database` resource establishes a database resource link named `IIoTDataLake-Target` in the target account. It links to the `IIoTDataLake` database in the source account, enabling access to the shared catalog data.\n\nBy creating this resource link, you enable seamless access to the shared data catalog from the target account, facilitating data utilization and analysis across accounts while maintaining security and compliance measures.\n\n\n<div class=\"content-ad\"></div>\n\n# IAM 역할에 권한 부여\n\n이제 우리가 리소스 링크를 생성했으니, 리소스 링크 및 공유 카탈로그에 액세스를 부여할 수 있습니다. 이 단계 이후에 IAM 역할은 소스 계정에서 공유된 필터링된 데이터에 액세스할 수 있게 될 것입니다.\n\n![Image](/assets/img/2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform_4.png)\n\n이 구성에서:\n\n<div class=\"content-ad\"></div>\n\n- 데이터베이스 권한 부여: aws_lakeformation_permissions 리소스는 IIoTDataLake-Target 데이터베이스의 IAM 역할에 DESCRIBE 권한을 부여합니다. 이를 통해 역할은 데이터베이스 구조와 메타데이터를 설명할 수 있습니다.\n- 테이블 권한 부여: 마찬가지로, 해당 리소스는 공유 카탈로그의 measurements 테이블에 대한 SELECT 권한을 IAM 역할에 부여합니다. 이를 통해 역할은 테이블에서 데이터를 선택하고 읽을 수 있습니다.\n\n이러한 권한이 부여된 상태에서 IAM 역할은 이제 소스 계정에서 공유된 필터링된 데이터에 액세스할 수 있어 타겟 계정 내에서 매끄러운 데이터 분석과 활용이 가능해졌습니다.\n\n# 결론\n\n이 블로그에서는 AWS Lake Formation과 Terraform을 사용하여 계정간 데이터 공유의 복잡성에 대해 살펴보았습니다. 데이터 필터를 구현하고 리소스 링크를 설정함으로써, 공유 데이터에 안전한 액세스를 보장하면서 권한을 세밀하게 제어할 수 있었습니다. 이러한 간소화된 접근 방식은 계정간 협업적인 데이터 분석을 용이하게하며, 팀이 효과적으로 통찰력을 얻을 수 있도록 도와주면서 데이터 보안 및 규정 준수 기준을 유지합니다.\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n데이터 필터링 및 셀 레벨 보안 - AWS Lake Formation (amazon.com)","ogImage":{"url":"/assets/img/2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform_0.png"},"coverImage":"/assets/img/2024-05-18-EnablingCross-AccountAccessforAWSLakeFormationwithDataFiltersUsingTerraform_0.png","tag":["Tech"],"readingTime":4},{"title":"GKE  Gemma  Ollama 유연한 LLM 배포를 위한 파워 트리오 ","description":"","date":"2024-05-18 16:52","slug":"2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment","content":"\n\n오늘은 구글 젬마를 중점으로 다양한 LLM 배포의 복잡성을 탐색하겠습니다. 선택한 플랫폼은  Ollama 프레임워크로부터 가치 있는 지원을 받는 GKE일 것입니다. 이 이정표를 달성하기 위한 여정은 Open WebUI에 의해 용이하게 이끌어질 것입니다. 이는 원래 OpenAI ChatGPT 프롬프트 인터페이스와 놀라운 유사성을 가지고 있어 사용자 경험을 원활하고 직관적으로 만들어줍니다.\n\n세부적인 내용에 대해 들어가기 전에, 가장 중요한 부분에 대해 이야기해 봅시다. 왜 먼저 이 방향으로 나아가는 것일까요? 제게는 이 이유가 명백하게 보이며, 여러 가지 강력한 요소로 요약될 수 있습니다:\n\n- 비용 효율성: 공개 클라우드 인프라에서 LLM을 운영할 경우, 특히 예산 제약으로 집중적인 작은 조직이나 연구 기관에게 더 경제적인 솔루션을 제공할 수 있습니다. 그러나 Vertex AI Studio 및 OpenAI Developer Platform과 같은 플랫폼이 이미 경제적이고 완전히 플래시된 관리 서비스를 제공한다는 조건부 혜택을 강조하는 것이 중요합니다. 또한 Vertex AI는 모델의 수명 주기와 가시성을 관리할 것입니다. 이를 염두에 두세요.\n- 맞춤화와 유연성: Ollama는 맞춤화, 유연성 및 오픈 소스 원칙을 핵심으로 만들어졌습니다. 클라우드 제공 업체의 모델 레지스트리를 통해 사용 가능한 포괄적인 모델 옵션에도 불구하고, 원하는 특정 모델이 즉시 사용 가능하지 않은 시나리오가 발생할 수 있습니다. 이때 Ollama가 해결책을 제공합니다.\n- 환경 간 이식성: Ollama의 디자인은 클라우드 및 플랫폼에 중립적이며, Docker를 수용하는 모든 공개 또는 개인 플랫폼에 배포할 자유를 제공합니다. 이는 Vertex AI 및 SageMaker와 같은 강력한 솔루션과 대조적입니다. 이 솔루션들은 그들 각자의 클라우드 환경과 결합되어 있습니다. Docker와 Kubernetes가 시장 전체를 섭렵한 이유가 있습니다. 이것은 x86에 대해서도 마찬가지입니다.\n- 개인 정보 보호 및 데이터 제어: 완전히 오픈 소스 모델인 LLaVA 및 젬마 같은 것을 활용하려는 사람들을 위해 완전히 개인적인 프레임워크 내에서 데이터 개인 정보 보호와 배포 환경에 대한 완전한 제어를 보장할 수 있는 최적의 방법을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n## GKE 플랫폼\n\n이 실험에서 나의 GKE 플랫폼 설정은 효율성과 성능을 우선시했습니다:\n\n- GKE 1.27 (정기 채널): 호환성 및 최신 구글 쿠버네티스 엔진 기능에 대한 액세스를 보장합니다.\n- Container-Optimized OS: 보다 빠른 워크로드 배포를 위해 노드 시작 시간을 줄입니다 (이전 글에서 자세히 읽을 수 있습니다).\n- g2-standard-4 노드 풀 (NVIDIA L4 GPU): GPU 및 CPU 리소스의 강력한 조합으로, 머신 러닝 작업에 이상적입니다. 벤치마크 결과가 장점을 보여줄 것입니다.\n- 관리되는 NVIDIA GPU 드라이버: GKE에 드라이버를 직접 통합하여 설정 프로세스를 간소화하며, 클러스터가 준비되면 즉시 사용할 수 있도록 합니다.\n\n<div class=\"content-ad\"></div>\n\nNVIDIA의 L4 GPU는 원시 사양과 결과에서 강력한 처리 능력을 발휘하며, 계산 집중적인 ML 작업에 대한 강력한 처리 능력을 제공합니다:\n\n- 7680 Shader Processors, 240 TMUs, 80 ROPs, 60 RT cores, 240 Tensor Cores.\n- 24GB GDDR6 메모리, 300GB/s 대역폭.\n- 485 teraFLOPs (FP8 처리량).\n\nG2 Machine Series는 Intel Cascade Lake 기반의 플랫폼으로, GPU를 보완하고 그것에 충분한 처리를 제공하기 위한 탁월한 솔루션을 제공합니다.\n\nG2는 Spot VM을 지원합니다: 중지를 허용할 수 있는 ML 작업에 대해 대규모 비용 절감(약 67% 할인)을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n## 올라마 및 오픈 웹UI 배포하기 (이전 올라마 웹UI)\n\nK8s 생태계의 성숙함으로 인해 배포 프로세스가 간소화되었으며, 이제는 주로 helm install 및 kubectl apply 명령을 실행하는 문제입니다. 올라마의 경우, 배포는 GitHub에서 제공되는 커뮤니티 주도의 Helm Chart를 활용하며, 설정을 안내하는 표준 values.yaml 파일이 제공됩니다:\n\n```js\nollama:\n  gpu:\n    enabled: true\n    type: 'nvidia'\n    number: 1\n  models:\n    - gemma:7b\n    - llava:13b\n    - llama2:7b\npersistentVolume:\n  enabled: true\n  size: 100Gi\n  storageClass: \"premium-rwo\"\n```\n\n한편, Open WebUI를 배포하는 경우에는 공식 차트와 커뮤니티에서 제공하는 Kustomize 템플릿을 사용하여 이 구현에 더 적합한 접근방식을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\nOpen WebUI은 Ollama 배포를 위한 매니페스트를 제공하지만, 저는 Helm Chart의 기능 풍부함을 선호했습니다. 배포 후에는 GCP 로드 밸런서의 IP 주소와 포트 8080으로 이동하여 Open WebUI 로그인 화면에 액세스할 수 있어야합니다.\n\n![이미지](/assets/img/2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment_1.png)\n\nollama 네임스페이스에서 간단한 확인 작업을 수행하면 모든 시스템이 운영 중인지 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment_2.png)\n\n<div class=\"content-ad\"></div>\n\n한 번 경험해보자! 왜 하늘은 파란색일까요?\n\n![이미지](https://miro.medium.com/v2/resize:fit:1388/1*0CM7W3rKWJbuMEGsAGkC5Q.gif)\n\n이것은 실시간 영상입니다 — NVIDIA L4의 Gemma 7B가 빠른 속도로 결과를 제공합니다! 직접 시도해 보고 싶나요? Ollama에서 모델을 배포하는 것은 아주 간단합니다. 단순히 ollama run gemma:7b를 사용하면 됩니다.\n\n## GPU 대 CPU — 속도의 문제\n\n<div class=\"content-ad\"></div>\n\n이제 플랫폼이 준비되어 있는데, 난 좋은 벤치마크 세션을 참여할 수 없다는 걸 알고 있지 😉. 다른 모델들에 걸쳐 두 종류의 벤치마크를 실행했어:\n\n- 고전적인 왜 하늘은 파랗게 보일까? 질문: Gemma 2B 및 7B, 그리고 LLaMA v1.6 7B와 13B에게 질문을 했지. 멀티모달과 유니모달 LLM을 테스트해 봐야 하거든!\n- 사진 속에 뭔가 있니? LLaMA v1.6 7B와 13B를 대상으로: 여기서 이미지 분석에 초점을 맞췄어.\n\n걱정 마, 내가 완전한 LLM 격렬한 대결을 벌일 생각은 없어 — 그건 전혀 다른 문제고, 내 이해 범위를 넘어서지. 내 목표는 다른 머신 유형이 속도와 반응성에 미치는 영향을 추적하는 거였어.\n\n<div class=\"content-ad\"></div>\n\nGPU 가속없이는 추론 성능이 주로 원시 CPU 성능과 메모리 대역폭에 의존했습니다. 당연히, 저는 CPU나 메모리 제한 없이 Ollama를 배포했고, CPU 활용률을 확인했습니다. 그러나 추론 작업은 종종 메모리 대역폭 가용성과 메모리 아키텍처에 의해 병목 현상을 겪을 수 있습니다.\n\n첫 번째 그래프는 몇 가지 주요 지표를 보여줍니다:\n\n- 총 소요 시간: 모델이 입력을 처리하고 응답을 생성하는 데 걸리는 시간입니다.\n- 응답 토큰/초: 모델이 출력을 생산하는 속도를 측정한 지표입니다.\n- 월간 비용: 선택한 구성을 한 달 동안 실행하는 데의 재정적 영향을 보여줍니다.\n\n![그래프 이미지](/assets/img/2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment_4.png)\n\n<div class=\"content-ad\"></div>\n\n여기에는 많은 것이 풀어야 할 문제가 있지만, 경고로 시작하고 싶습니다: 여러분이 지금 볼 성능 숫자는 특정 시나리오에 대한 것으로만 나타나는 것에 주목하세요. LLM의 세계는 매우 넓고 빠르기 때문에, 현재 이 그림조차 며칠 안에 완전히 관련이 없어질 수도 있습니다. 약간 다른 시나리오에서도 그렇습니다.\n\nGPU 우세성:\n\n- GPU는 CPU보다 대폭으로 낮은 대기 시간을 제공합니다(초당 더 많은 토큰). 심지어 $12,000/월에 180개의 전용 CPU 코어도 경쟁할 수 없습니다.\n- NVIDIA L4는 이전 T4보다 15% 더 빠른 속도를 제공하지만, 비용은 78% 증가했습니다. 지속 사용 할인이 반영되었습니다.\n- A100은 번개처럼 빠르지만, L4보다 약 세 배 더 빠르며, 높은 가격과 교육에 중점을 둔 점 때문에 대부분의 추론 작업에는 지나친 것으로 보입니다. 그럼에도 불구하고 3.6초 미만에 답변하는 데 성공했습니다 🤯.\n\nCPU 고민:\n\n<div class=\"content-ad\"></div>\n\n- 작은 CPU는 명백히 느리고 놀랍도록 비싸다.\n- 비용이 비슷한 CPU들도 (c3-highcpu-22 / c3d-highcpu-16) 처리량 면에서 L4 및 T4보다 뒤처진다.\n- 가장 큰 CPU들 (c3-standard-176 / c3d-standard-360)은 그들의 엄청난 비용에 비해 성능이 나쁘다.\n- C3는 나쁘게 확장되는데, 이것은 ollama/llama.cpp, 내 설정 또는 C3 인스턴스와 그들의 vNUMA 토폴로지 부족과 관련된 잠재적인 문제일 수 있다. 하지만, 가격 때문에 그것은 의미가 없어진다.\n\n이제 이미지 인식 프롬프트를 살펴보자. 이번에 선택한 모델은 13B 파라미터를 가진 LLaVA v1.6이다.\n\n![이미지](/assets/img/2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment_5.png)\n\nGPU의 성능 우위는 여기서도 유효하며, CPU는 이 도메인에서 단순히 경쟁할 수 없음을 보여준다. 흥미로운 점은 c3-standard-176이 마침내 c3-highcpu-22를 능가했는데, 이것은 C3나 내 설정에 버그가 있음을 의심하는 것을 해소시켰다.\n\n<div class=\"content-ad\"></div>\n\n전통에 따라 모든 결과는 다음 Google 시트에서 공개적으로 이용 가능합니다:\n\nOllama에 대해 몇 가지 포인트를 논의하기 전에, 이 환경에서 사용된 정확한 SHA와 태그를 공유하고 싶습니다. AI 세계가 빠르게 변화하고 있어, 제 작업을 재현하려는 누군가는 길을 나가면 얼마든지 다른 환경을 발견할 수 있습니다:\n\n- ollama v0.1.29;\n- Gemma 2B SHA b50d6c999e59\n- Gemma 7B SHA 430ed3535049\n- LLaVA v1.6 7B SHA 8dd30f6b0cb1\n- LLaVA v1.6 13B SHA 0d0eb4d7f485\n\n그리고 벤치마크가 실행된 방식에 대해:\n\n<div class=\"content-ad\"></div>\n\n```js\ncurl http://localhost:8080/api/generate -d \\\n'{\n  \"model\": \"gemma:7b\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false,\n  \"options\": {\"seed\": 100}\n}'\n```\n\n```js\ncurl http://localhost:8080/api/generate -d \\\n'{\n  \"model\": \"llava:13b\",\n  \"prompt\":\"What is in this picture?\",\n  \"images\": [\"iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMH...\n```\n\n결과를 기록하는 동안 사용된 설정은 다음과 같습니다:\n\n- Ollama API와의 직접 통신\n- 스트리밍 비활성화\n- 모든 프롬프트에 동일한 시드값 사용함.\n\n<div class=\"content-ad\"></div>\n\n## 올라마의 현재 한계: 더 깊게 파헤쳐보기\n\n올라마 프로젝트가 빠르게 발전 중임을 기억하는 것이 중요하지만, 전문 사용자가 알아야 할 중요한 제한 사항을 조사하는 것이 유용합니다:\n\n- 저장소 병목 현상: registry.ollama.ai에 갇히면 혁신과 실험을 억눌게 됩니다. 도커가 Quay.io를 넘어서지 않았다면 어땠을까 상상해보세요! 해결책이 있을 수 있지만, 다양한 모델 소스에 대한 네이티브 솔루션이 큰 발전을 이룰 것이며, 커뮤니티에서 이미 제안을 했습니다.\n- 병렬성을 통한 기회의 놓침: 올라마의 순차적 요청 처리가 현실 성능을 제한합니다. 사용자가 괴롭힘을 겪는 높은 트래픽 시나리오를 상상해보세요. 좋은 소식은 병렬 디코딩이 llama.cpp에 합병되었으며 v0.1.30 주기 중에 풀 리퀘스트된 것입니다. 이 문제 #358을 주시해야 합니다.\n- AVX512의 실망과 새로운 선택지: AVX512 최적화가 기대한 성능 향상을 제공하지 못하는 것은 실망스럽습니다. 실제로 더 좋게 만들려는 시도를 해봤지만 현실을 직면하게 되었습니다: AVX512는 별로다, AVX2보다 느리다 😭 (물론 코어 클록은 절반 이상), 그리고 \"AVX512가 고통스러운 죽음을 맞기를 희망합니다\". Intel AMX는 더 밝은 전망을 제시합니다. 경쟁력 있는 가격, 초기 벤치마크 결과, 특정 작업 부하에서 GPU를 앞서갈 수 있는 잠재력 등이 흥미로운 대안으로 나타납니다. 이 주제에 대해, The Next Platform의 AI 추론이 주로 CPU에서 이루어질 이유에 대한 깊은 의견을 강력히 권장합니다.\n\n## 주요 결론\n\n<div class=\"content-ad\"></div>\n\nGKE에서 Ollama와 함께 LLM을 배포하는 것은 맞춤화, 유연성, 잠재적인 비용 절감 및 LLM 솔루션 내에서의 개인 정보 보호를 우선시하는 사용자들에게 매력적인 옵션을 제공합니다. 이 접근 방식은 상업 플랫폼에서 사용할 수 없는 모델을 활용할 수 있게 해주며 배포 환경에 대한 완전한 제어권을 제공합니다. 중요한 점은 GPU 가속이 최적의 LLM 성능을 위해 필수적이며, 강력한 CPU 기반 인스턴스조차 크게 앞서갑니다. 그러나 Ollama의 현재 제약 사항, 예를 들어 레지스트리 의존성 및 순차적 요청 처리와 같은 것에 주의를 기울이는 것이 중요합니다. 이러한 제약 사항은 Ollama가 계속 성장함에 따라 해결될 가능성이 크며, 그 가능성을 더욱 높일 것입니다.\n\n즐거운 시간 보내셨길 바라며, 저에게 궁금한 점이 있으시면 언제든지 댓글을 남겨주세요.","ogImage":{"url":"/assets/img/2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment_0.png"},"coverImage":"/assets/img/2024-05-18-GKEGemmaOllamaThePowerTrioforFlexibleLLMDeployment_0.png","tag":["Tech"],"readingTime":8},{"title":"롱혼 - K8s를 위한 분산형 블록 스토리지","description":"","date":"2024-05-18 16:50","slug":"2024-05-18-LonghornDistributedBlockStorageforK8s","content":"\n\n\n![Longhorn](/assets/img/2024-05-18-LonghornDistributedBlockStorageforK8s_0.png)\n\n# Longhorn이란 무엇인가요?\n\n## K8s 클러스터에서 영구 저장소가 필요한 이유는 무엇인가요?\n\n컨테이너화된 응용 프로그램에서 데이터는 일반적으로 컨테이너 자체와는 별도로 저장되는 저장 시스템에 저장됩니다. 이는 컨테이너를 쉽게 파기하거나 대체할 수 있게 하면서도 중요한 데이터를 손실하지 않도록 합니다. 영구 데이터를 저장한다는 것은 데이터가 컨테이너가 중지, 다시 시작되거나 삭제되어도 손실되지 않도록 데이터를 저장하는 방식을 의미합니다. 이를 위해 클라우드 네이티브 분산 블록 저장 시스템과 같은 영구 저장소 솔루션을 사용하여 데이터를 저장함으로써 컨테이너가 필요할 때 데이터에 액세스할 수 있도록 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n데이터는 애플리케이션 데이터, 구성 파일, 사용자 데이터, 데이터베이스 및 시간이 지날수록 유지하고 액세스해야하는 다른 유형의 데이터일 수 있습니다.\n\n# Longhorn 작동 방식은?\n\nLonghorn에는 세 가지 주요 구성 요소가 있습니다:\n\n- Longhorn Manager\n- Longhorn Engine\n- Longhorn UI\n\n<div class=\"content-ad\"></div>\n\n## 롱혼 매니저\n\n롱혼 매니저 Pod는 롱혼 클러스터의 각 노드에서 Kubernetes 데몬세트로 실행됩니다. Kubernetes 클러스터에서 볼륨을 생성하고 관리하며 UI 또는 Kubernetes를 위한 볼륨 플러그인의 API 호출을 처리합니다.\n\n롱혼 매니저는 Kubernetes API 서버와 통신하여 새로운 롱혼 볼륨 CRD를 생성합니다. 그런 다음 롱혼 매니저는 API 서버 응답을 모니터링하고 Kubernetes API 서버가 새 롱혼 볼륨 CRD를 생성한 것을 감지하면 새 볼륨을 생성합니다.\n\n## 롱혼 엔진\n\n<div class=\"content-ad\"></div>\n\n롱혼 매니저가 볼륨을 생성하도록 요청받으면, 그 볼륨이 연결된 노드에 롱혼 엔진 인스턴스를 생성하고, 각 복제가 배치될 노드에 각각 복제를 생성합니다. 복제는 최대 가용성을 보장하기 위해 별도의 호스트에 배치되어야 합니다.\n\n롱혼은 롱혼 클러스터의 각 노드에 볼륨마다 별도의 엔진을 생성합니다.\n\n엔진은 롱혼 볼륨의 I/O 작업을 처리하는 구성 요소입니다. 볼륨이 생성되면 롱혼은 해당 볼륨이 실행되는 각 노드에 별도의 엔진을 생성합니다. 이렇게 하면 볼륨 데이터가 해당 볼륨이 실행 중인 노드에 로컬로 저장되도록 보장합니다.\n\n만약 볼륨을 위한 롱혼 엔진을 호스팅하는 노드가 충돌하거나 다른 이유로 이용 불가능 상태가 되면, 롱혼은 해당 장애를 감지하고 자동으로 다른 이용 가능한 노드에 엔진을 재배치합니다. 이 프로세스를 자동 노드 복구라고 합니다.\n\n<div class=\"content-ad\"></div>\n\n엔진은 Pod나 배포(Deployment)와 같은 Kubernetes 객체가 아닙니다. 대신에, 그것은 호스트 운영 체제에서 이진 실행 파일로 실행되는 Longhorn에 특화된 프로세스입니다.\n\n아래 그림은 Longhorn이 작동하는 방식을 보여줍니다:\n\n![Longhorn Architecture](/assets/img/2024-05-18-LonghornDistributedBlockStorageforK8s_1.png)\n\n- Longhorn 볼륨은 세 개의 인스턴스가 있습니다.\n- 각 볼륨에는 Longhorn 엔진이라 불리는 전용 컨트롤러가 있으며 Linux 프로세스로 실행됩니다.\n- 각 Longhorn 볼륨에는 두 개의 복제본이 있고, 각각의 복제본은 Linux 프로세스입니다.\n- 그림에서 화살표는 볼륨, 컨트롤러 인스턴스, 복제본 인스턴스 및 디스크 간의 읽기/쓰기 데이터 흐름을 나타냅니다.\n- 볼륨마다 별도의 Longhorn 엔진을 생성하여, 하나의 컨트롤러가 실패하더라도 다른 볼륨의 기능에는 영향을 미치지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n## 롱혼 UI\n\n롱혼 UI는 롱혼 매니저에 의해 쿠버네티스 배포로 배포되며, 기본적으로 하나의 레플리카로 실행되므로 단일 노드에서 팟으로 실행됩니다.\n\n롱혼 UI는 롱혼 API를 통해 롱혼 매니저와 상호 작용합니다. 롱혼 UI를 통해 스냅샷, 백업, 노드 및 디스크를 관리할 수 있습니다.\n\n또한, 롱혼 UI에 의해 클러스터 워커 노드의 공간 사용량이 수집되고 시각화됩니다.\n\n<div class=\"content-ad\"></div>\n\n다음 그림은 Longhorn UI에서 보는 그림을 보여줍니다:\n\n![Longhorn UI view](/assets/img/2024-05-18-LonghornDistributedBlockStorageforK8s_2.png)\n\n이제 이 그림에서 각 용어의 의미를 자세히 알아보겠습니다:\n\n- Schedulable: Longhorn 볼륨 스케줄링에 사용할 수 있는 실제 공간입니다.\n- Reserved: 다른 애플리케이션과 시스템을 위해 예약된 공간입니다.\n- Used: Longhorn, 시스템 및 다른 애플리케이션에서 사용된 실제 공간입니다.\n- Disabled: Longhorn 볼륨을 스케줄링할 수 없는 디스크/노드의 총 공간입니다.\n\n<div class=\"content-ad\"></div>\n\n# Longhorn 설치 (전제 조건 및 요구 사항)\n\n## 노드 수\n\nLonghorn은 Kubernetes 클러스터에 최소한 하나의 노드로 설치할 수 있지만, 데이터 중복 및 고가용성을 위해 적어도 세 개의 노드가 권장됩니다.\n\n## K8s 클러스터의 각 노드는 다음 요구 사항을 충족해야 합니다:\n\n<div class=\"content-ad\"></div>\n\n- Kubernetes와 호환되는 컨테이너 런타임(Docker v1.13+, containerd v1.3.7)이 필요합니다.\n- Kubernetes 버전은 `v1.21` 이상이어야 합니다.\n- open-iscsi가 설치되어 있어야 하며, 모든 노드에서 iscsid 데몬이 실행 중이어야 합니다.\n\n- RWX 지원을 위해 각 노드에 NFSv4 클라이언트가 설치되어 있어야 합니다.\n\n- 호스트 파일 시스템이 데이터를 저장하기 위한 파일 extents 기능을 지원해야 합니다. 현재 지원하는 파일 시스템은 ext4 및 XFS입니다.\n\n- 다음 Linux 명령 줄 유틸리티가 설치되어 있어야 합니다: bash, curl, findmnt, grep, awk, blkid, lsblk\n\n<div class=\"content-ad\"></div>\n\n- 마운트 전파를 활성화해야 합니다.\n\n- 롱혼 워크로드는 롱혼이 올바르게 배포 및 운영되기 위해 루트 사용자로 실행할 수 있어야 합니다.\n\n# 권장 최소 하드웨어\n\n- 3 노드\n- 노드 당 4 vCPU\n- 노드 당 4 GiB\n- 스토리지를 위한 노드에는 SSD/NVMe 또는 유사한 성능 블록 장치(추천)\n- 스토리지를 위한 노드에는 HDD/스핀 디스크 또는 유사한 성능 블록 장치(확인됨)\n- 볼륨 당 최대 500/250 IOPS(1 MiB I/O)\n- 볼륨 당 최대 500/250 처리량(MiB/s)\n\n<div class=\"content-ad\"></div>\n\n# 운영 체제\n\n- Ubuntu: 버전 (20.04, 22.04)\n- SLES: 버전 (15 SP4)\n- SLE Micro: 버전 (5.3)\n- CentOS: 버전 (8.4)\n- RHEL: 버전 (8.6)\n- Oracle Linux: 버전 (8.6)\n- Rocky Linux: 버전 (8.6)\n\n# Longhorn 기능, 제한 및 단점\n\n## 기능\n\n<div class=\"content-ad\"></div>\n\n- 고가용성: Longhorn은 클러스터 내 여러 노드에 볼륨을 복제하여 데이터의 고가용성을 제공하기 위해 설계되었습니다.\n- 스냅샷 및 백업: Longhorn은 복제본이 다운됐을 때마다 (시스템) 스냅샷을 자동으로 찍고 다른 노드에서 다시 복구 시작합니다.\n- 재해 복구: Longhorn은 재해로부터 회복하기 위해 볼륨을 다시 구축하고 백업 데이터를 복원하는 기능을 제공합니다.\n- 복제: Longhorn은 서로 다른 클러스터 간에 볼륨을 복제하여 여러 위치에 걸쳐 재해 복구 솔루션을 만들 수 있도록합니다.\n- 동적 할당: Longhorn은 Kubernetes에서 볼륨의 동적 할당을 지원하여 Kubernetes API를 사용하여 볼륨을 생성하고 관리할 수 있게 합니다.\n- 볼륨 확장: Longhorn은 볼륨의 온라인 확장을 지원하여 볼륨 크기를 증가시키고 다운타임 없이 볼륨을 확장할 수 있습니다.\n- 모니터링 및 메트릭: Longhorn은 볼륨의 상태 및 성능에 대한 상세한 메트릭 및 모니터링 정보를 제공하여 스토리지 인프라의 문제 해결 및 최적화를 쉽게 할 수 있습니다.\n- 웹 기반 UI: Longhorn은 웹 기반 사용자 인터페이스를 제공하여 볼륨 및 스토리지 인프라를 손쉽게 관리하고 모니터링할 수 있습니다.\n\n## 제한 사항 및 단점\n\n- 버전 간 업그레이드는 복잡한 과정입니다.\n- Longhorn은 높은 버전에서 낮은 버전으로의 다운그레이드를 지원하지 않습니다.\n- Longhorn 블록 스토리지 속도는 로컬 디스크 스토리지 속도보다 약간 느립니다.\n\n# Longhorn 설치\n\n<div class=\"content-ad\"></div>\n\n롱혼은 Helm, kubectl 및 Rancher Apps & Marketplace로 설치할 수 있어요.\n\n롱혼은 100% 오픈 소스 소프트웨어에요. 프로젝트 소스 코드는 여러 저장소에 분산돼 있어요:\n\n- Longhorn Engine — 코어 컨트롤러/레플리카 로직.\n- Longhorn Instance Manager — 컨트롤러/레플리카 인스턴스 라이프사이클 관리.\n- Longhorn Share Manager — NFS 프로비저너로 롱혼 볼륨을 ReadWriteMany 볼륨으로 노출시켜요.\n- Backing Image Manager — 백킹 이미지 파일 라이프사이클 관리.\n- Longhorn Manager — 롱혼 오케스트레이션, Kubernetes용 CSI(Container Storage Interface) 드라이버를 포함하고 있어요.\n- Longhorn UI — 대시보드.\n\n## Helm으로 설치하기\n\n<div class=\"content-ad\"></div>\n\n요구 사항\n\n- 작업 스테이션에는 Helm v2.0+가 설치되어 있어야 합니다.\n\nLonghorn 헬름 차트를 여기에서 확인할 수 있습니다.\n\n헬름 차트의 내용:\n\n<div class=\"content-ad\"></div>\n\n템플릿: 쿠버네티스 매니페스트를 포함하고 있는 폴더로, 차트에 의해 생성되는 리소스를 정의합니다.\n\nvalues.yaml: 차트에서 사용되는 구성 매개변수의 기본 값을 정의하는 파일입니다.\n\nChart.yaml: 차트에 관한 메타데이터를 포함하고 있는 파일로, 이름, 버전, 설명, 유지보수자 등을 포함합니다.\n\n설치\n\n<div class=\"content-ad\"></div>\n\nLonghorn 차트 저장소를 추가해보세요:\n\n```js\n$ helm repo add longhorn https://charts.longhorn.io\n```\n\n차트 저장소에서 로컬 Longhorn 차트 정보를 업데이트해보세요:\n\n```js\n$ helm repo update\n```\n\n<div class=\"content-ad\"></div>\n\nLonghorn 차트 설치:\n\nHelm 2를 이용해 다음 명령어를 실행하면 longhorn-system 네임스페이스를 생성하고 Longhorn 차트를 함께 설치합니다:\n\n```js\n$ helm install longhorn longhorn/longhorn --namespace longhorn-system\n```\n\nHelm 3를 이용해 다음 명령어를 실행하면 먼저 longhorn-system 네임스페이스를 생성하고 Longhorn 차트를 설치합니다:\n\n<div class=\"content-ad\"></div>\n\n```bash\n$ kubectl create namespace longhorn-system\n$ helm install longhorn longhorn/longhorn --namespace longhorn-system\n```\n\n설치가 성공적으로 완료되었는지 확인하려면 설치 후 실행 중인 파드를 확인하여 다음 명령을 실행하면 Longhorn 파드를 볼 수 있어요:\n\n```bash\n$ kubectl get pods --namespace longhorn-system\n```\n\n# Longhorn 볼륨 생성하기\n\n<div class=\"content-ad\"></div>\n\nStep_1: 롱혼 StorageClass 생성하기\n\n다음은 storageclass.yaml이 어떻게 생겼는지입니다:\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\n```\n\n```bash\n$ kubectl apply -f storageclass.yaml\n```\n\n<div class=\"content-ad\"></div>\n\nStep_2: Longhorn StorageClass를 참조하는 PersistentVolumeClaim (PVC)를 만듭니다.\n\n다음은 longhorn-pvc.yaml 파일의 내용입니다.\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: longhorn-vol-pvc\nspec:\n  storageClassName: longhorn\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n```\n\n```bash\n$ kubectl apply -f longhorn-pvc.yaml\n```\n\n<div class=\"content-ad\"></div>\n\nStep 3: Longhorn 볼륨을 사용하는 Pod 생성하기\n\n아래는 pod-with-longhorn.yaml 파일의 내용입니다:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: longhorn-test\nspec:\n  containers:\n  - name: longhorn-test-container\n    image: busybox\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo $(date) >> /mnt/data/date.txt; sleep 1; done\"]\n    volumeMounts:\n    - mountPath: /mnt/data\n      name: longhorn-vol\n  volumes:\n  - name: longhorn-vol\n    persistentVolumeClaim:\n      claimName: longhorn-vol-pvc\n```\n\n이후, longhorn-test 라는 이름의 Pod가 시작되며, Longhorn StorageClass를 참조하는 longhorn-vol-pvc라는 PersistentVolumeClaim도 생성됩니다.\n\n<div class=\"content-ad\"></div>\n\n영구 볼륨 클레임이 Pod에 볼륨으로 마운트됩니다.\n\n# 제거\n\nHelm 2를 사용하여 Longhorn을 제거하려면:\n\n```js\n# 삭제 확인을 위해 Longhorn 설정을 패치합니다\n$ kubectl -n longhorn-system patch -p '{\"value\": \"true\"}' --type=merge lhs deleting-confirmation-flag\n\n# Longhorn을 제거합니다\n$ helm delete longhorn --purge\n```\n\n<div class=\"content-ad\"></div>\n\nHelm 3으로 Longhorn을 제거하는 방법은 다음과 같습니다:\n\n```js\n# 삭제 확인을 위해 Longhorn 설정을 패치\nkubectl -n longhorn-system patch -p '{\"value\": \"true\"}' --type=merge lhs deleting-confirmation-flag\n\n# Longhorn 제거\nhelm uninstall longhorn -n longhorn-system\n\n# Longhorn 네임스페이스 삭제\nkubectl delete namespace longhorn-system\n```\n\n# 결론\n\n요약하자면, Longhorn은 Kubernetes에서 영속적인 저장소를 생성하고 관리하기 위한 솔루션이다. 관리자, 컨트롤러, 레플리카 인스턴스 등의 컴포넌트를 특징으로 하는 아키텍처는 데이터의 내구성과 고가용성을 보장한다. Longhorn의 주요 기능으로는 분산 블록 저장소, 스냅샷, 백업, 볼륨 복제가 있어 상태를 유지해야 하는 응용프로그램을 실행하는 데 이상적이다. Helm 2 또는 3을 사용한 쉬운 설치로, Longhorn은 저장소 프로비저닝과 구성을 단순화하여 Kubernetes 환경에서 개발자와 시스템 관리자에게 가치 있는 도구가 된다.","ogImage":{"url":"/assets/img/2024-05-18-LonghornDistributedBlockStorageforK8s_0.png"},"coverImage":"/assets/img/2024-05-18-LonghornDistributedBlockStorageforK8s_0.png","tag":["Tech"],"readingTime":9},{"title":"ArgoCD ApplicationSet은 v29 버전에서 더 실용적입니다","description":"","date":"2024-05-18 16:49","slug":"2024-05-18-ArgoCdApplicationSetismorepracticalinversionv29","content":"\n\n대부분의 ArgoCD 프로젝트를 따르는 여러분은 이미 이를 알고 있을 것입니다. ArgoCD v2.9에서는 특정 기능이 릴리스의 일부가 되었는데, 이에 대해 이 게시물에서 공유하고 싶습니다.\n\n![ArgoCD v2.9](/assets/img/2024-05-18-ArgoCdApplicationSetismorepracticalinversionv29_0.png)\n\n# 언급한 기능은 무엇이며 왜 이것이 ApplicationSet을 더 실용적으로 만드는가요?\n\nApplicationSet을 사용하여 Application을 생성하는 경우 생성된 Application의 자동 동기화를 비활성화할 수 없다는 것을 이미 알아차리신 분들이 많으실 겁니다.\n\n<div class=\"content-ad\"></div>\n\n이 문제에 대한 자세한 내용은 다음 링크에서 확인할 수 있습니다: https://github.com/argoproj/argo-cd/issues/9101\n\n문제에 대한 요약은 다음과 같습니다.\n\n예를 들어, 제 경우에는 특정한 ApplicationSet이 모든 Kubernetes 클러스터에 대해 특정 Application을 생성하도록 설정되어 있습니다.\n\n클러스터 유지 관리자로서 어느 날, 한 클러스터에서 발생한 문제에 대해 알림을 받았는데, 그 문제가 ApplicationSet을 통해 설치된 응용프로그램과 관련이 있는 것으로 보였습니다.\n\n<div class=\"content-ad\"></div>\n\n특정 클러스터에서 매개변수를 동적으로 변경하여 문제를 완화해야 했습니다. 애플리케이션의 자동 동기화를 비활성화하여 일부 작업을 수행할 수 있도록 시도했지만, ApplicationSet 컨트롤러는 항상 자동 동기화 상태로 되돌리기 때문에 문제가 발생했습니다.\n\n이것은 \"버그\"라기보다는 존경받는 GitOps 원칙을 시행하며 모든 변경 사항은 특정 애플리케이션의 값들을 수동으로 재정의하는 대신 코드를 통해 수행되어야 한다는 동작입니다.\n\n하지만, 제게는 실용적이지 않았습니다. 특히, 새벽 4시에 빠르게 문제를 해결해야 하는 SRE로서 실시간 제품 시스템을 모니터링해야 하는 경우에는 더욱 그렇습니다.\n\n이 제한은 이 특정 사용 사례에만 해당하는 것이 아닙니다. Github의 이슈에서 설명했듯이, 대부분의 개발 작업 흐름도 생성된 애플리케이션에 일부 값을 재정의해야 하는데, 그 또한 불가능합니다.\n\n<div class=\"content-ad\"></div>\n\n# 어떤 기능으로 이 문제를 해결할 수 있을까요?\n\nArgoCD v2.9에서는 애플리케이션세트(ApplicationSet)에서 생성된 애플리케이션의 자동 동기화를 일시적으로 끌 수 있도록 허용합니다.\n\n우리가 해야 할 일은 이 구성 조각을 애플리케이션세트에만 추가하는 것뿐입니다. \n\n![ArgoCD Application Set Configuration](/assets/img/2024-05-18-ArgoCdApplicationSetismorepracticalinversionv29_1.png)\n\n<div class=\"content-ad\"></div>\n\n응용 프로그램 설정 컨트롤러에서 변경 내용이 롤백되지 않고 응용 프로그램의 자동 동기화를 중지할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-ArgoCdApplicationSetismorepracticalinversionv29_2.png)\n\n사실, 이 변경의 일부로 필요에 맞게 syncPolicy에 국한되지 않는 여러 매개변수 간의 차이를 무시할 수 있습니다. 주의할 점은 우리가 재정의하려는 필드가 목록인 경우에는 여전히 일부 제한 사항이 있습니다. 그러나 필요할 때 자동 동기화를 비활성화할 수 있게 되었습니다.","ogImage":{"url":"/assets/img/2024-05-18-ArgoCdApplicationSetismorepracticalinversionv29_0.png"},"coverImage":"/assets/img/2024-05-18-ArgoCdApplicationSetismorepracticalinversionv29_0.png","tag":["Tech"],"readingTime":2},{"title":"Azure IAM을 활용하여 Kubernetes 리소스를 최적화해보세요 관리형 vs 워크로드 ID","description":"","date":"2024-05-18 16:47","slug":"2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity","content":"\n\n## Azure Kubernetes Service (AKS) 배포를 위한 최고의 Identity 솔루션 공개\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*cZynTTP_7qDHrix67vOYyQ.gif)\n\nAzure의 Identity 옵션에 혼란스러워 하고 계신가요? 걱정하지 마세요! 이 블로그는 그 이유들을 명확히 설명하고, 여러분이 완벽한 솔루션을 선택할 수 있도록 도와줍니다!\n\n# 소개\n\n<div class=\"content-ad\"></div>\n\nMicrosoft의 관리 ID 개념을 자세히 살펴보기 전에 이전에 어떻게 작동했는지와 Microsoft가 이 혁신으로 해결하려고 했던 도전 과제를 포함한 다른 측면을 먼저 살펴봅시다. 엔지니어의 관점에서 보면 일반적으로 Azure Kubernetes Services(AKS) 클러스터에 배포된 작업 부하는 Microsoft Entra 애플리케이션 자격 증명이나 관리 ID를 사용하여 Azure Key Vault 및 Microsoft Graph와 같은 Microsoft Entra 보호된 리소스에 액세스해야 합니다. Microsoft Entra Workload ID는 외부 ID 제공자와의 연합을 위해 쿠버네티스의 기본 기능과 통합됩니다.\n\n다음은 다른 개념 간의 간단한 차이점입니다:\n\n- 서비스 주체(Service Principal): 기본적으로 특정 Azure AD 테넌트 또는 다른 Microsoft 서비스 내의 리소스에 액세스하기 위해 응용 프로그램, 서비스 또는 자동화 도구용으로 만든 ID입니다. Azure 리소스에 한정되지 않으며 다양한 Microsoft 서비스에 인증 및 액세스에 사용할 수 있습니다.\n- Pod-Identity (폐기 예정): Microsoft Entra 포드 관리 ID는 Azure 리소스 및 Microsoft Entra ID의 관리 ID를 포드와 연결하기 위해 쿠버네티스 원시 개념을 사용합니다. 관리자는 포드가 Microsoft Entra ID를 ID 제공자로 사용하는 Azure 리소스에 액세스할 수 있도록 쿠버네티스 원시로 ID와 바인딩을 생성합니다.\n- 관리 ID(Managed Identity): Microsoft Entra ID (이전 Azure AD) 테넌트 내의 서비스 또는 리소스에 대한 ID를 제공하는 Azure의 기능입니다. Azure에서 실행되는 애플리케이션 및 서비스에서 사용하는 자격 증명 관리를 간소화하기 위해 설계되었습니다.\n- 연합 ID 자격 증명을 사용한 작업 ID(Workload Identity with Federation Identity Credentials): Microsoft Entra Workload ID는 Service Account Token Volume Projection을 사용하여 포드가 쿠버네티스 ID(즉, 서비스 계정)를 사용할 수 있도록 합니다. 쿠버네티스 토큰이 발급되고 OIDC 연합을 통해 쿠버네티스 애플리케이션은 주석이 달린 서비스 계정을 기반으로 Microsoft Entra ID를 통해 Azure 리소스에 안전하게 액세스할 수 있습니다.\n\n요약하면, 관리 ID는 Azure 리소스를 위해 설계된 특정 유형의 ID로 Azure 서비스에 대한 인증 및 액세스를 보다 안전하게 관리하기 쉽게합니다. 반면에 서비스 주체는 Azure를 포함한 Microsoft 서비스 전반에 사용할 수 있는 더 일반적인 ID로, Azure를 포함한 다양한 목적을 위해 명시적으로 생성하고 구성할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nAzure Kubernetes Service (AKS)에서 어떻게 활용될 수 있는지 살펴봅시다. 간단한 소개부터 시작해서 실제 예제까지 살펴보겠습니다.\n\n# Azure Kubernetes Service 및 관리 ID\n\nAzure Kubernetes Service(AKS)는 응용 프로그램을 보호하기 위한 두 가지 주요 식별 옵션을 제공합니다:\n\n## 관리 ID:\n\n<div class=\"content-ad\"></div>\n\n- AKS 클러스터 또는 더 구체적으로 Virtual Machine Scale Sets (VMSS)에 대한 Azure 리소스용 관리 ID.\n- 자격 증명이 필요하지 않은 다른 Azure 리소스에 대한 Azure RBAC 액세스 제공.\n\n두 가지 유형:\n\n- 시스템 할당: 리소스 수명 주기에 묶임.\n- 사용자 지정: 여러 리소스에 할당할 수 있음.\n\n## 연합 자격 증명을 활용한 워크로드 ID:\n\n<div class=\"content-ad\"></div>\n\n- AKS 팟에서 실행되는 작업 부하를 위한 신원을 제공합니다.\n- 팟 내에 비밀을 저장하지 않고 Azure 리소스에 액세스할 수 있습니다.\n- Microsoft Entra Workload ID는 서비스 어카운트 토큰 볼륨 프로젝션을 사용하여 팟이 쿠버네티스 신원(즉, 서비스 어카운트)을 사용할 수 있게 합니다. 쿠버네티스 토큰이 발급되며 OIDC 연합을 통해 주석이 달린 서비스 어카운트를 기반으로 Azure 리소스에 안전하게 액세스할 수 있습니다.\n\n## 예시:\n\nAzure Key Vault에 액세스해야 하는 애플리케이션이 있는 AKS 클러스터를 고려해보겠습니다.\n\nManaged Identity를 사용하는 경우:\n\n<div class=\"content-ad\"></div>\n\n클러스터를 생성할 때는 --enable-managed-identity 플래그를 설정하여 관리 식별 증명을 생성할 수 있습니다. 이 작업은 Microsoft.ManagedIdentity/userAssignedIdentities의 백그라운드 생성을 트리거합니다.\n\n이 작업의 명령 구문은 다음과 같습니다:\n\n```js\naz aks create -g myResourceGroup -n myManagedCluster --enable-managed-identity\n```\n\n생성된 식별 증명은 그 후 Azure Key Vault 또는 DNS Zones와 같은 리소스에 액세스할 수 있도록 권한을 부여받을 수 있습니다. 이 식별 증명은 VMSS(Virtual Machine Scale Sets)에 할당됩니다. 각 노드 또는 VM은 kubelet을 실행하며, 이를 통해 External-DNS와 같은 팟이 관리 식별 증명을 통해 리소스에 액세스합니다. 아래 다이어그램은 두 개의 관리 식별 증명이 있는 두 노드 풀을 더 명확하고 안전한 시스템 및 응용 프로그램 팟 간에 시스템적으로 접근을 세밀하게 제어할 수 있는 구조를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*9VftZEvhy_k_eskwir2eKw.gif\" />\n\n다음은 프로세스의 간결한 산문 요약입니다:\n\n1. AKS 클러스터에 사용자 지정 관리 ID 할당: 이 단계는 Azure Kubernetes Service (AKS) 클러스터에 사용자 지정 관리 ID를 구성하는 것을 포함합니다. 이렇게 함으로써 AKS 클러스터 내에서 실행되는 각 응용 프로그램이나 서비스가 각각의 자격 증명을 요구하지 않고 Azure 서비스 인증을 위해이 ID를 활용할 수 있습니다.\n\n2. 관리 ID에 Azure Key Vault 액세스 권한 부여: 관리 ID가 AKS 클러스터에 할당된 후 다음 단계는 이 ID가 Azure Key Vault에 액세스 할 필요 권한을 부여하는 것입니다.이 권한 설정은 Key Vault에 저장된 비밀, 키 및 인증서와 같은 민감한 정보에 안전하게 액세스하기 위해 중요합니다.\n\n<div class=\"content-ad\"></div>\n\n3. 관리 식별자를 할당한 AKS 클러스터가 Azure Key Vault에 액세스하는 애플리케이션: AKS 클러스터에 할당된 관리 식별자가 Azure Key Vault에 액세스 권한을 부여받으면 AKS 클러스터 내에서 실행되는 애플리케이션은 이제 Azure Key Vault에 안전하게 액세스할 수 있습니다. 이는 명시적 자격 증명이 필요하지 않기 때문에 최소 권한 원칙을 준수하고 응용 프로그램의 보안 기본을 강화합니다.\n\n이 원활한 통합을 통해 AKS에 배포된 애플리케이션의 비밀 관리 및 액세스 제어 프로세스가 간소화되며, Azure의 관리 식별자 및 Key Vault 서비스를 활용하여 견고하고 안전한 클라우드 네이티브 환경을 제공합니다.\n\n하지만 이제 더 잘 작동하는 방법을 살펴보겠습니다.\n\n페더레이티드 식별자 자격 증명과 함께 Workload Identity 사용하기\n\n<div class=\"content-ad\"></div>\n\nAzure Kubernetes Service (AKS) 클러스터에 워크로드 ID를 설정할 때는 --enable-managed-identity 및 --enable-oidc-issuer 플래그를 모두 지정해야 합니다. 예를 들어 다음과 같이 실행합니다:\n\n```js\naz aks create -g myResourceGroup -n myManagedCluster --enable-oidc-issuer --enable-workload-identity\n```\n\n- --enable-oidc-issuer: 이 옵션은 AKS 클러스터에 대한 OpenID Connect (OIDC) 기능을 활성화합니다. OIDC를 사용하면 애플리케이션이 Azure Active Directory (Azure AD)를 사용하여 인증할 수 있습니다.\n- --enable-workload-identity: 이 옵션은 AKS 클러스터에 대한 Azure Workload Identity를 활성화합니다. Workload Identity를 사용하면 클러스터의 팟이 명시적인 자격 증명을 저장하지 않고도 Azure 리소스에 액세스할 수 있습니다.\n\n다음 figure 3은 과정을 추상적으로 보여줍니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*cZynTTP_7qDHrix67vOYyQ.gif)\n\nFigure 2와는 반대로, 여기서는 가상 머신 규모 집합(VMSS)의 kubelet ID 대신 AKS 클러스터가 발행자 역할 (--enable-oidc-issuer)로 작동하여 해당 워크로드에 대한 토큰을 생성합니다. 이는 관리 신원이 생성되고, 그런 다음 하나 이상의 Azure 리소스에 액세스 할 수 있도록 권한이 부여됩니다. 이 권한은 자격 증명의 연맹을 통해 ServiceAccount에 매핑되며, 이 예에서 우리는 AKS 클러스터를 발행자로 사용하여 액세스 토큰을 발급합니다.\n\n하지만 내부 작동 원리는 무엇인가요?\n\n기본 보안 모델에서 AKS 클러스터는 토큰의 발급자 역할로 작동합니다. OpenID Connect를 사용하여, OAuth 2.0 위에 구축된 프로토콜을 통해 Microsoft Entra ID는 서비스 계정 토큰의 진위를 확인하는 데 필수적인 공개 서명 키를 발견합니다. 이 확인 프로세스는 Microsoft Entra 토큰으로 교환되기 전에 토큰의 정당성을 확인하는 데 중요합니다. 서비스 계정 토큰을 Microsoft Entra 토큰으로 교환하는 과정은 Azure Identity 클라이언트 라이브러리나 Microsoft Authentication Library를 통해 가능하며, 이를 통해 워크로드는 권한을 담언하는 토큰을 활용하여 Azure 리소스에 안전하게 액세스할 수 있습니다. 이 시스템은 Microsoft Entra ID에 의해 확인된 토큰만이 리소스에 액세스하는 데 사용될 수 있도록 하여 최소 권한의 원칙을 유지함으로써 보안성을 강화합니다.\n\n\n<div class=\"content-ad\"></div>\n\n표 4는 작업 흐름을 보여줍니다:\n![workflow](/assets/img/2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity_0.png)\n\n알겠어요! Azure는 어떻게 특정 파드만 Workload Identity를 받아야 한다는 것을 알 수 있을까요?\n\nAzure는 레이블 및 주석을 사용하여 어떤 파드가 Workload Identity를 받아야 하는지 제어하여 올바른 레이블이 있는 경우에만 해당 Azure 리소스에 ServiceAccount를 통해 액세스할 수 있도록 합니다. 이 설정은 Kubernetes 배포 내에서 구성됩니다.\n\n<div class=\"content-ad\"></div>\n\n배포 후에는 다음과 같이 나타납니다:\n\n![이미지 1](/assets/img/2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity_1.png)\n\nAzure에서는 다음과 같이 나타납니다:\n\n![이미지 2](/assets/img/2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity_2.png)\n\n<div class=\"content-ad\"></div>\n\n이 경우에는 애플리케이션이 Azure 키 저장소에 액세스하여 비밀을 가져오고 데이터베이스에 액세스할 수 있으며 역할 할당을 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity_3.png)\n\n다중 매핑은 가능한가요?\n\n네, 마이크로 소프트 엔트라 워크로드 ID에서는 서비스 계정과 관련된 여러 매핑이 가능합니다. 서비스 계정을 마이크로 소프트 엔트라 객체와 연결하는 다양한 구성을 지원합니다:\n\n<div class=\"content-ad\"></div>\n\n- 일대일 매핑은 단일 서비스 계정이 하나의 Microsoft Entra 개체를 참조할 수 있도록 합니다.\n- 다대일 매핑은 여러 서비스 계정이 동일한 Microsoft Entra 개체를 참조할 수 있도록 허용합니다.\n- 일대다 매핑은 단일 서비스 계정이 클라이언트 ID 주석을 변경하여 여러 Microsoft Entra 개체를 참조할 수 있도록 합니다.\n\n어떤 제한 사항이 있을까요?\n\n- 관리되는 ID 당 페더레이티드 ID 자격 증명을 최대 20개까지만 보유할 수 있습니다.\n- 페더레이티드 ID 자격 증명을 처음 추가한 후에도 전파되기까지 몇 초가 소요됩니다.\n- 오픈 소스 프로젝트인 Virtual Kubelet을 기반으로 하는 가상 노드 추가 기능은 지원되지 않습니다.\n- 특정 지역에서는 사용자 지정 관리 ID에서 페더레이티드 ID 자격 증명을 생성하는 것이 지원되지 않습니다.\n\n# 정리\n\n<div class=\"content-ad\"></div>\n\n마무리하며, Azure와 Kubernetes 환경에서 적절한 신원 모델을 선택하는 미묘한 차이를 이해하는 것이 중요합니다.\n\n관리 신원은 Azure 리소스가 다른 Azure 서비스와 상호 작용해야 하는 시나리오에서 빛을 발합니다. Azure 내에서의 간편함과 통합성으로 인해 액세스 관리에 대한 복잡성을 줄이며, 수동으로 자격 증명을 처리하는 것보다 이상적인 선택지가 됩니다.\n\n반면, 피더레이션 신원 자격 증명을 사용하는 워크로드 신원은 동적이고 단기적인 신원이 필요한 환경에서 빛을 발합니다. 이 모델은 특히 Kubernetes 애플리케이션에 적합하며, Azure 리소스로의 액세스 관리를 안전하고 간소화된 방식으로 제공합니다. 액세스 권한을 세밀하게 제어하여 더 맞춤화된 보안 포지션을 제공합니다.\n\nManaged Identities를 사용하는 주요 이점 중 하나는 Managed Identity(MI)의 kubelet 신원에 의존하는 대신, 제공하는 액세스 제어의 세분성입니다. MI를 사용한 kubelet 신원은 VMSS에서 실행되는 모든 Pod에 대해 Azure 리소스에 대한 넓은 액세스 수준을 제공하는 반면, 워크로드 신원은 애플리케이션 또는 서비스 당 액세스 권한을 정의하여 더 정확한 제어가 가능합니다.\n\n<div class=\"content-ad\"></div>\n\n또한, Workload Identity는 ServiceAccounts에 대한 매핑을 용이하게 해주어 하나 이상의 애플리케이션이 필요한 Azure 리소스에 대한 접근을 얻을 수 있도록 도와줍니다. 이 유연성은 더 구체적인 액세스 권한을 제공하여 애플리케이션의 요구에 따라 더욱 보안 프레임워크를 강화합니다.\n\nOtterize와 같은 도구를 사용하면 토큰이 워크로드가 필요한 만큼만 살아 있도록 보장하여 잠재적인 공격 표면을 최소화함으로써 보안을 강화할 수 있습니다.\n\n마지막으로, Managed Identity는 리소스 그룹 내에 존재하는 Azure 리소스임을 명심해야 합니다. 이는 미국 Entra ID 테넌트와 구별되며 전역적인 존재를 갖습니다. 이 차이점은 애플리케이션에서 사용하는 ID의 범위와 수명주기를 이해하는 데 중요합니다.\n\nManaged Identity와 Workload Identity, 그리고 연합 ID 자격 증명을 사용하여 신중한 선택을 통해 보안과 기능성에 균형을 맞출 수 있습니다. 애플리케이션이 필요로 하는 액세스를 제공하되 보안 원칙을 저해하지 않도록 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n# 연락처 정보\n\n질문이 있거나 친근하게 이야기를 나누고 싶거나 놓치지 말아야 할 주제가 있다면, 중간의 코멘트 기능을 사용하지 말고 자유롭게 나의 LinkedIn 네트워크에 추가해 주세요!\n\n# 참고 자료\n\n- https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens\n- https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview?\n- Pod Identity (Deprecated) — https://learn.microsoft.com/en-us/azure/aks/use-azure-ad-pod-identity\n- Microsoft 엔트라 Workload ID\n- OIDC 연합\n- https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview?tabs=dotnet\n- 꼭 놓치지 마세요 — ` https://medium.com/itnext/kubernetes-automate-workload-iam-on-azure-with-otterize-860faa221eac","ogImage":{"url":"/assets/img/2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity_0.png"},"coverImage":"/assets/img/2024-05-18-OptimizeYourKubernetesResourceswithAzureIAMManagedvsWorkloadIdentity_0.png","tag":["Tech"],"readingTime":10},{"title":"K9s를 사용하여 스타일리시하게 Kubernetes 클러스터를 간편하게 관리하기","description":"","date":"2024-05-18 16:46","slug":"2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s","content":"\n\n일상생활에서 마이크로서비스 및 컨테이너 오케스트레이션을 다루는 개발자라면, 이전에 제가 자주 만났던 상황에 있었을지도 모릅니다. 쿠버네티스 명령어를 입력하는 것은 재미있을 수 있지만, 모든 작업과 명령어에 익숙해지면 그저 응소가 된다는 느낌을 받을 수 있습니다.\n\n월요일 아침에 일어나서 월요일 블루가 시작되고 있는데, 한 가지 팟의 로그를 보기 위해 여러 명령어를 입력할 기분이 전혀 아니라고 가정해 봅시다.\n\n어느 순간에는 삶이 매우 단조로워지고, 이것은 철학적인 이야기가 아닙니다! 동일한 오래된 명령 프롬프트나 터미널이 어느 순간 짜증을 유발하기 시작할 수 있습니다. 이해합니다. 당신의 삶에 가질 수 있는 유일한 두 가지 색상은 아니라는 것을 알고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 만약 이렇게 해볼 수 있다고 말한다면:\n\n![이미지1](/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_1.png)\n\n# 이렇게 바뀌길 원하나요:\n\n![이미지2](/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_2.png)\n\n<div class=\"content-ad\"></div>\n\n멋지게 보이죠, 맞지 않나요?! 이렇게 하는 방법을 배워봅시다!\n\n## 단계:\n\n이 인터페이스를 제공하고 우리가 더 자세히 논의할 많은 바로 가기를 제공하는 K9s를 설치하는 과정을 진행해 보겠습니다.\n\nK9s는 Linux, macOS 및 Windows 플랫폼에서 사용할 수 있습니다. 여기에서 자신의 OS에 알맞게 K9s를 설치하세요. 또는 저와 같이 게으르다면, 아래를 따라가 보세요.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 홈브류를 통해 설치하기\n brew install derailed/k9s/k9s\n # 맥포트를 통해 설치하기\n sudo port install k9s\n```\n\n```js\n# 리눅스브류를 통해 설치하기\n brew install derailed/k9s/k9s\n # 팩맨을 통해 설치하기\n pacman -S k9s\n```\n\n```js\n# scoop을 통해 설치하기\nscoop install k9s\n# 초콜렛티를 통해 설치하기\nchoco install k9s\n```\n\n여기까지가 가장 어려운 부분이라고 믿어줘요. 이제부터는 모든 게 매우 쉽고 직관적입니다!\n\n\n<div class=\"content-ad\"></div>\n\n테스트를 위해 Azure에서 클러스터를 만들었어요.\n\n![클러스터 이미지](/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_3.png)\n\n이제 Azure 포털에서 제공하는 명령어를 사용하여 클러스터에 연결해 보겠어요:\n\n![연결 명령어 이미지](/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_4.png)\n\n<div class=\"content-ad\"></div>\n\n지금 명령 프롬프트 또는 터미널에서 k9s -A를 입력하면 멋진 UI가 팝업됩니다! UI의 좌측 상단에 언급된 일부 세부 정보를 확인하여 올바른 클러스터에 연결되어 있는지, 올바른 컨텍스트를 사용하고 있는지, 그리고 한눈에 CPU 및 메모리 활용을 확인할 수 있습니다.\n\n<img src=\"/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_5.png\" />\n\n당신의 주의를 끈 사이, K9s를 사용하여 이동하는 데 사용할 수 있는 몇 가지 기본 명령어를 배워보겠습니다. 기본 네임스페이스의 리소스를 보려면 단순히 1을 누르고 모든 네임스페이스의 기본 뷰로 전환하려면 0을 누릅니다. 화살표 키를 사용하여 탐색하고 리소스 내부로 이동하려면 Enter 키를 누릅니다.\n\n이제 pod, 네임스페이스, 인그레스 등과 같은 리소스에 빠르게 액세스하려면 여기에 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## 참고 사항:\n\n- :ns: 네임스페이스 전환.\n- :pod: 모든 파드 나열.\n- :svc: 모든 서비스 나열.\n- :deploy: 모든 배포 나열.\n- :ing: 모든 인그레스 나열.\n- :jobs: 모든 작업 나열.\n- :cronjob: 모든 크론잡 나열.\n- :hpa: 모든 수평 파드 자동 확장기 나열.\n\n명령 모드에 진입하려면 단순히 ‘:’를 누르고 액세스하려는 리소스 이름을 입력하면 됩니다. 여기에 간단한 예제가 있습니다:\n\n<img src=\"/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 Markdown 형식으로 변경하세요.\n\n<div class=\"content-ad\"></div>\n\n테이블 태그를 마크다운 형식으로 변경하세요.\n\n비슷하게 kubectl logs my-app-pod-849f8d7d6d-x9s7z를 입력하는 대신에, 'l'을 눌러주세요.\n\n![image](/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_9.png)\n\n## 여기에 리소스를 관리하는 또 다른 치트 시트가 있습니다:\n\n- Enter: 선택한 리소스의 세부 정보 보기\n- d: 선택한 리소스 설명\n- l: 선택한 파드 또는 컨테이너의 로그 보기\n- s: 선택한 파드로 셸 접속\n- e: 선택한 리소스 편집\n- ctrl-c: 현재 작업 중지 (로그 또는 셸보기에서 유용함)\n- ctrl-d: 선택한 리소스 삭제 (확인을 위한 프롬프트 표시)\n- /: 리소스 검색 또는 필터링\n- Esc: 필터 지우기 또는 검색/필터 모드 나가기\n- Space: 여러 리소스 선택 (대량 작업에 유용함)\n- v: 리소스 매니페스트 보기\n- b: Kubernetes 벤치마크 표시\n- t: 테이블 및 YAML 모드 간 보기 전환\n\n<div class=\"content-ad\"></div>\n\n참고 자료:\n\n- [K9s 설치](https://k9scli.io/topics/install/)\n- [K9s 명령어](https://k9scli.io/topics/commands/)\n- [K9s GitHub 저장소](https://github.com/derailed/k9s)","ogImage":{"url":"/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_0.png"},"coverImage":"/assets/img/2024-05-18-Hassle-freemanagementofyourKubernetesclusterinstyleusingK9s_0.png","tag":["Tech"],"readingTime":4},{"title":"데브옵스 제로 투 히어로 3  도커에 대해 알아야 할 모든 것","description":"","date":"2024-05-18 16:42","slug":"2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers","content":"\n\n## 데브옵스 업무 흐름에서 도커 사용을 시작하는 완벽한 가이드\n\n### 이 블로그 포스트에서 무엇을 기대해야 하는가?\n\n### 왜 Docker를 사용해야 하는가?\n\n20~30년 전으로 돌아가면, 하드웨어와 설치된 운영 체제(커널 및 UI)가 있었습니다. 어플리케이션을 실행하기 위해서 코드를 컴파일하고 모든 앱 의존성을 정렬해야 했습니다. 다른 어플리케이션이나 어플리케이션 작업량의 증가를 수용하기 위해 새 하드웨어를 구매하고 설치 및 구성해야 했습니다.\n\n<div class=\"content-ad\"></div>\n\n가상화는 하드웨어와 운영 체제 사이에 추가 계층인 하이퍼바이저라는 것을 추가했습니다. 이를 통해 사용자들은 독립된 여러 애플리케이션을 실행하여 가상 머신을 자체 운영 체제와 함께 실행할 수 있었습니다.\n\n그럼에도 불구하고, 우리는 모든 가상 머신에 소프트웨어를 설치하고 종속성을 설정해주어야 했습니다. 애플리케이션이 휴대 가능하지 않았습니다. 일부 기기에서는 작동하지만 다른 기기에서는 작동하지 않았습니다.\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_0.png)\n\n# 도커란 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n도커는 마이크로서비스 기반 응용 프로그램 개발이 가능하도록 함으로써 소프트웨어 빌드 방법을 혁신했습니다.\n\n# 도커는 어떻게 작동하나요?\n\n도커 엔진은 호스트 운영 체제 위에서 실행됩니다. 도커 엔진에는 호스트 시스템에서 도커 컨테이너를 관리하는 서버 프로세스(dockerd)가 포함되어 있습니다. 도커 컨테이너는 응용 프로그램과 그 의존성을 격리시켜 다른 환경에서도 일관되게 실행할 수 있도록 설계되었습니다.\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_1.png)\n\n<div class=\"content-ad\"></div>\n\n도커를 사용하려면 Dockerfile, Docker 이미지, Docker 컨테이너라는 세 가지 개념을 이해해야 합니다.\n\n## Docker 파일(Dockerfile)이란?\n\n## Docker 이미지란 무엇인가요?\n\nDocker 이미지에는 컨테이너 내에서 코드를 실행하는 데 필요한 모든 종속성이 포함되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## Docker 컨테이너란 무엇인가요?\n\n# Docker로 시작하기\n\n## 사전 요구 사항\n\nDocker는 로컬 머신 또는 클라우드 VM에 설치되어 있어야 합니다.\n\n<div class=\"content-ad\"></div>\n\nLinux:\n\n로컬 노트북, 가상 머신 또는 클라우드 VM에서 Linux를 실행 중이라면 패키지 관리자를 사용하여 Docker를 설치할 수 있습니다. 이 블로그 포스트의 지시사항을 따라주세요.\n\nMac 및 Windows:\n\n로컬 머신에서 Docker를 실행할 수 있게 해주는 Docker Desktop을 설치할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n도커를 시작하는 것은 쉬운 일이에요. 아래 명령을 실행하기만 하면 돼요\n\n```js\n도커 런 -d -t --name Thor alpine\n도커 런 -d -t busybox\n```\n\n이 명령은 도커 이미지 alpine과 busybox로부터 2개의 컨테이너를 생성할 거에요. 이 둘은 도커 허브에 저장된 미니멀리스트이면서 퍼블릭인 리눅스 도커 이미지에요.\n\n- -d 옵션은 컨테이너를 백그라운드에서 실행(detach mode)\n- -t 옵션은 tty 터미널을 연결해 줘요.\n- --name 옵션은 컨테이너에 이름을 부여할 수 있어요. 제공하지 않으면 컨테이너는 랜덤 이름을 받게 돼요\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_2.png\" />\n\n참고: 위에 언급된 이미지로 처음 docker run을 실행하면 도커가 로컬 머신에서 이미지를 다운로드(풀)해야 합니다.\n\n## 로컬 머신에서 실행 중인/중지된 컨테이너 목록\n\n```js\ndocker ps # 실행 중인 컨테이너 확인\ndocker ps -a # 모든 컨테이너 목록\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_3.png\" />\n\n## 로컬 머신에 있는 도커 이미지 목록\n\n```js\ndocker image ls\n```\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_4.png\" />\n\n<div class=\"content-ad\"></div>\n\nLinux 기반 도커 컨테이너에 사용된 이미지 크기를 확인할 수 있어요. 우분투, 아마존 리눅스, CentOS 등과 같은 일반적인 Linux 기반 머신과 비교하면 작아요.\n\n실행 중인 컨테이너와 상호 작용하는 방법은 명령을 전달하거나 대화형 세션을 여는 두 가지 방법이 있어요.\n\n```js\n# docker exec -it <컨테이너 ID> <셸>\n```\n\n- docker exec를 사용하면 실행 중인 도커 컨테이너 내부로 진입할 수 있어요.\n- --it는 도커와 대화형 세션을 열어줘요.\n- 셸은 sh, bash, zsh 등을 사용할 수 있어요.\n\n<div class=\"content-ad\"></div>\n\n컨테이너 정보를 얻기 위해 몇 가지 명령을 실행해 봅시다.\n\n```js\ndocker exec -t Thor ls ; ps\n# -t는 tty 세션을 열기 위한 옵션입니다.\n# Thor는 컨테이너 이름입니다.\n# ls ; ps는 ls와 ps 두 개의 명령을 실행합니다.\n\ndocker exec -t 8ad10d1d0660 free -m \n# 8ad10d1d0660은 컨테이너 ID이며, free -m은 메모리 사용량을 확인하는 명령입니다.\n```\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_5.png)\n\n이제 -it 옵션을 사용하여 컨테이너와 대화형 쉘 세션을 열어보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n도커 exec -it 16fb1c59fbea sh \n# 세션을 종료하려면 \"exit\"을 입력하세요\n```\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_6.png)\n\n# 컨테이너 시작, 중지, 삭제\n\n```js\n# 실행 중인 컨테이너를 중지하는 방법\ndocker stop <컨테이너 이름 또는 ID>\n\n# 중지된 컨테이너를 시작하는 방법\ndocker start <컨테이너 이름 또는 ID>\n```\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_7.png)\n\n```js\n# 실행 중인 컨테이너를 종료한 후에 삭제하려면 다음과 같이 명령을 사용합니다.\n# docker stop <컨테이너 이름 또는 ID>\n# docker rm <컨테이너 이름 또는 ID>\ndocker stop 16fb1c59fbea\ndocker rm 16fb1c59fbea\n\n# 아니면 -f 플래그를 사용하여 강제로 컨테이너를 삭제합니다.\ndocker rm -f Thor\n```\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_8.png)\n\n# 도커 이미지 빌드하기\n\n\n<div class=\"content-ad\"></div>\n\n# Docker 네트워킹\n\n도커는 다양한 종류의 네트워크를 제공합니다.\n\n## 1. 기본 브릿지\n\n만약 도커허브에서 nginx 이미지를 사용하여 nginx 컨테이너를 실행 중이고 웹 서버에 액세스하려고 한다면요.\n\n<div class=\"content-ad\"></div>\n\n![screenshot](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_9.png)\n\n스크린샷을 통해 Nginx 웹 서버가 80번 포트에서 실행 중임을 확인할 수 있습니다. NGINX 컨테이너에 로그인하여 curl 127.0.0.1:80을 실행하면 웹 서버에서 HTML 응답을 반환합니다.\n\n- 127.0.0.1은 루프백 주소로 현재 장치(로컬호스트)를 항상 가리킵니다.\n\n![screenshot](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_10.png)\n\n<div class=\"content-ad\"></div>\n\n위에서 본 것처럼, 컨테이너 내부의 웹 서버가 우리가 기대한 대로 작동했습니다.\n\n이제 호스트 머신 (랩톱/가상 머신)에서 웹 서버에 연결해 보겠습니다.\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_11.png)\n\n컨테이너에서 실행 중인 nginx 웹 서버에 연결할 수 없었습니다.\n\n<div class=\"content-ad\"></div>\n\n네트워크 깊이 파고들기\n\n도커 컨테이너를 검사하려면 docker inspect 명령을 실행하고 맨 아래로 스크롤하면 도커가 브릿지 네트워크를 사용하고 네트워크에 대한 자세한 내용을 볼 수 있습니다.\n\n```js\ndocker inspect nginx-container\n```\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_12.png\" />\n\n<div class=\"content-ad\"></div>\n\n```js\n도커 네트워크 목록\n```\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_13.png\" />\n\n네트워크를 검사하면 기본 브릿지 네트워크를 사용하는 컨테이너를 찾을 수 있습니다.\n\n```js\n도커 네트워크 검사 브릿지\n```\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_14.png\" />\n\n## 포트 전달\n\n```js\n도커 실행 -d -p <호스트 포트>:<컨테이너 포트> --name <컨테이너 이름> 이미지\n\n# 포트 전달 -> 컨테이너의 포트 80을 호스트 머신의 포트 5000으로 전달\n도커 실행 -t -d -p 5000:80 --name nginx-container nginx:latest\n```\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_15.png\" />\n\n<div class=\"content-ad\"></div>\n\n![](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_16.png)\n\n도커는 기본 네트워크를 사용하지 말라고 권장하고 대신에 우리만의 네트워크를 생성하도록 하고 있습니다.\n\nbusybox 이미지를 사용하여 2개의 컨테이너를 생성해 봅시다. 컨테이너를 실행한 후, 컨테이너 내부에서 서로 핑을 시도해 봅시다.\n\n2개의 컨테이너를 생성하고, 각 컨테이너의 IP 주소를 얻기 위해 docker inspect를 사용해 보세요.\n\n<div class=\"content-ad\"></div>\n\n아래는 두 컨테이너의 IP 주소가 동일한 네트워크에 있음을 확인할 수 있습니다(기본 브리지 네트워크).\n\n이름과 IP 주소로 서로 핑을 보내 봅시다.\n\n![image1](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_17.png)\n\n![image2](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_18.png)\n\n<div class=\"content-ad\"></div>\n\n위 스크린샷을 보면 두 컨테이너 모두 서로 통신할 수 있지만 이름으로는 연결할 수 없습니다.\n\n## 2. 사용자 정의 브리지 네트워크\n\n네트워크 생성\n\n```js\ndocker network create blog-network\n```\n\n<div class=\"content-ad\"></div>\n\n새로운 네트워크를 사용하여 이름이 nginx-con인 nginx 컨테이너를 만들어보세요.\n\n```bash\ndocker run -itd --network blog-network --name nginx-con nginx\ndocker ps\n```\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_19.png)\n\n도커 컨테이너와 네트워크를 검사해봅시다.\n\n<div class=\"content-ad\"></div>\n\n```js\n도커 네트워크를 검사하는 방법: \n\n```docker network inspect blog-network```\n\n![사진](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_20.png)\n\nNginx 컨테이너를 검사하는 방법: \n\n```docker inspect nginx-con```\n\n![사진](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_21.png)\n```\n\n<div class=\"content-ad\"></div>\n\n이제 호스트 포트를 통해 컨테이너에서 실행 중인 nginx 웹 서버에 액세스하려고 하면 작동하지 않을 것입니다. 여전히 컨테이너에서 실행 중인 웹 서버에 액세스하려면 호스트 포트로의 포트 포워딩을 수행해야 합니다.\n\n그러나 하나가 정리되었습니다 — 이름 해결입니다. 이제 사용자 정의 브리지 네트워크에서 컨테이너를 핑하면 작동해야 합니다.\n\n한 가지 busybox 컨테이너를 만들고 nginx-con 컨테이너를 핑해 보겠습니다.\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_22.png)\n\n<div class=\"content-ad\"></div>\n\n## 3. 호스트 네트워크\n\n```js\ndocker run -td --network host --name nginx-server nginx:latest\ndocker ps\n```\n\n![image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_23.png)\n\n이렇게 하면 nginx 컨테이너가 호스트 네트워크에서 실행됩니다.\n\n<div class=\"content-ad\"></div>\n\n만약 로컬 호스트에서 nginx 서버에 접근하려고 하면, 작동해야 합니다.\n\n만약 컨테이너의 IP 주소를 확인하려면\n\n```js\ndocker inspect nginx-server | grep IPAddress\n```\n\n아래 그림을 참고하세요:\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_24.png)\n\n<div class=\"content-ad\"></div>\n\n뭐라구요? 해당 IP 주소가 없나봐요. 호스트 머신의 IP 주소를 사용한 모양이에요.\n\n다른 종류의 네트워크에 대해 더 알려드릴게요. 하지만 블로그가 너무 길어지고 있네요.\n\n# 도커 볼륨\n\n도커는 로컬 파일 시스템에서 컨테이너의 모든 콘텐츠, 코드 및 데이터를 분리합니다. 이는 도커 데스크탑에서 컨테이너를 삭제하면 그 안의 모든 콘텐츠가 삭제된다는 뜻이에요.\n\n<div class=\"content-ad\"></div>\n\n가끔은 컨테이너가 생성한 데이터를 유지하기를 원할 수 있습니다. 이때 볼륨을 사용할 수 있습니다.\n\n## 바인드 마운트\n\n바인드 마운트를 사용하면 호스트 머신의 파일 또는 디렉토리가 컨테이너로 마운트됩니다.\n\n## 도커 볼륨\n\n<div class=\"content-ad\"></div>\n\n체적은 도커에서 관리하는 로컬 파일 시스템의 위치입니다.\n\n체적은 사용 중인 컨테이너의 크기를 늘리지 않으며, 체적의 내용은 특정 컨테이너의 생명 주기 외부에 존재합니다.\n\n# 도커에서 체적 사용하는 방법\n\n도커에서 체적을 사용하는 방법에는 --mount 및 -v (또는 --volume ) 두 가지가 있습니다. -v 구문은 모든 옵션을 하나의 필드에 결합하며, --mount 구문은 옵션을 분리합니다.\n\n<div class=\"content-ad\"></div>\n\n- -v 또는 --volume: 콜론 문자(:)로 구분된 세 개의 필드로 구성됩니다.\n\n   - 첫 번째 필드는 바인드 마운트의 경우 호스트 머신의 파일 또는 디렉터리 경로이거나 볼륨의 이름입니다.\n   - 두 번째 필드는 컨테이너 내에서 파일 또는 디렉터리가 마운트된 경로입니다.\n   - 세 번째 필드는 선택 사항입니다. ro, z, Z와 같은 옵션들의 쉼표로 구분된 목록입니다.\n\n--mount: 쉼표로 구분된 여러 개의 키-값 페어로 구성됩니다.\n\n   - 마운트의 유형(bind, volume 또는 tmpfs).\n   - 마운트의 소스.\n   - 목적지는 파일 또는 디렉터리가 컨테이너 내에서 마운트된 경로로 값을 취합니다.\n\n<div class=\"content-ad\"></div>\n\n# 예시: Docker 컨테이너와 바인드 마운트\n\n우리는 호스트 머신의 동일한 디렉토리/폴더를 4개의 컨테이너에 마운트했습니다.\n\n```js\nmkdir docker-bind-mount\ndocker run -t -d  -v docker-bind-mount:/app/log  --name captain-america busybox\ndocker run -t -d  -v docker-bind-mount:/app/log  --name thor busybox\ndocker run -t -d  -v docker-bind-mount:/app/log  --name hulk  busybox\ndocker run -t -d  -v docker-bind-mount:/app/log  --name iron-man  alpine\n\n# --mount 옵션을 사용한 명령어\ndocker run -t -d --mount type=bind,source=docker-bind-mount,target=/app/log \\\n  --name captain-america busybox\n```\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_25.png\" />\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_26.png)\n\n각 컨테이너 안에 저장된 로그를 작성해봅시다. 바인드-마운트가 연결된 경로인 /app/log에\n\n![image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_27.png)\n\n자세히 살펴보면, 1번째와 2번째 컨테이너에서 생성한 파일들이 3번째 컨테이너에서 보이며, 1번째에서 3번째 컨테이너에서 생성한 파일이 4번째(Captain-America) 컨테이너에서도 보인다는 점을 알 수 있습니다.\n\n\n<div class=\"content-ad\"></div>\n\n일반적으로 도커 데이터 파일에는 다른 컨테이너의 내용이 포함되어 있습니다.\n\n이 예시는 우리가 어떻게 간단하게 여러 컨테이너 간에 파일/로그를 공유할 수 있는지 보여줍니다.\n\n하지만, 한 가지 주의할 점이 있습니다. 만약 우리가 로컬 호스트로 이동하여 모든 컨테이너에 바인드 마운트한 디렉터리를 목록으로 보면 아무것도 찾을 수 없을 것입니다.\n\n이것은 즉, 도커가 바인드 마운트한 데이터를 다른 컨테이너와 공유할 수 있었지만 데이터가 로컬 호스트에서는 보이지 않는다는 것을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n이 파일들은 바인드 마운트 시 도커 컨테이너 간에 저장되고 공유됩니다.\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_28.png)\n\n도커 컨테이너 중 하나를 검사하고 마운트 섹션으로 이동하면 마운트에 관련된 세부 정보를 볼 수 있습니다.\n\n참고: 도커 inspect 명령은 도커 컨테이너에 대해 더 많은 정보를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# 도커 컨테이너 조회 명령어: docker inspect <컨테이너 이름 또는 컨테이너 ID>\ndocker inspect hulk\n```\n\n![도커 컨테이너 및 볼륨에 대한 자세한 정보](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_29.png)\n\n# 예제: 볼륨을 사용하는 Docker 컨테이너\n\n볼륨 생성하기\n\n\n<div class=\"content-ad\"></div>\n\n```md\n# 도커 볼륨 생성하기\n도커 볼륨 생성 thor-vol\n도커 볼륨 생성 hulk-vol\n\n# 도커 볼륨 목록 확인\n도커 볼륨 목록 보기\n\n# 도커 볼륨   \n# 명령어:\n#  create      볼륨 생성\n#  inspect     한 개 이상의 볼륨에 대한 자세한 정보 표시\n#  ls          볼륨 목록 보기\n#  prune       사용하지 않는 로컬 볼륨 제거\n#  rm          한 개 이상의 볼륨 제거\n```\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_30.png\" />\n\n볼륨 검사하기\n\n```md\n도커 볼륨 inspect thor-vol\n```\n\n<div class=\"content-ad\"></div>\n\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_31.png)\n\n새 컨테이너를 생성하는 동안 이 볼륨을 마운트해 봅시다. 이번에는 — — mount 옵션을 사용할 거에요.\n\n```js\ndocker run -d \\\n  --name thor-container \\\n  --mount type=volume,source=thor-vol,target=/app \\\n  nginx:latest\n\n# 볼륨을 source로 명시할 때 type의 기본 값은 volume입니다.\ndocker run -d \\ \n  --name hulk-container \\\n  --mount source=thor-vol,target=/app \\\n  nginx:latest\n```\n\n```js\ndocker inspect hulk-container\n```   \n  \n\n<div class=\"content-ad\"></div>\n\n\n![Image 1](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_32.png)\n\nLet’s create some logs in both containers and see if the files/logs are visible to both containers.\n\n![Image 2](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_33.png)\n\nAs you can see from the above screenshot, we can share data/logs across containers.\n\n\n<div class=\"content-ad\"></div>\n\n# 도커 볼륨 삭제\n\n```js\n# 도커 볼륨을 제거하기\n# 한 번에 하나 이상의 볼륨을 삭제할 수 있습니다\ndocker volume rm <volume-names> \n```\n\n![이미지](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_34.png)\n\n# 도커 이미지 빌드\n\n<div class=\"content-ad\"></div>\n\n이제 기본적인 도커 개념을 살펴 보았으니, 실습 예제를 해 봅시다.\n\n## 여기서 무엇을 할 건가요?\n\n도커 이미지를 만들어 기본적인 Flask 애플리케이션을 내장시킬 거에요. 그리고 만든 도커 이미지를 도커 허브에 푸시할 거에요.\n\n한 폴더를 만들고 app.py, requirements.txt, 그리고 Dockerfile 이라는 3개의 파일을 생성해 볼까요?\n\n<div class=\"content-ad\"></div>\n\n```js\nDockerfile, app.py, requirements.txt 파일들을 만들었습니다.\n\n<img src=\"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_35.png\" />\n\n각 파일에 아래 코드를 붙여 넣어주세요.\n\napp.py\n```\n\n<div class=\"content-ad\"></div>\n\n```python\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_docker():\n    return 'Hello, Docker!'\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0')\n```\n\nrequirements.txt\n\n```python\nFlask\n```\n\n이제 Docker 이미지를 빌드하기 위한 Dockerfile을 생성할 것입니다.```\n\n<div class=\"content-ad\"></div>\n\nDockerfile\n\n```js\n# 부모 이미지로 공식 Python 실행 환경 사용\nFROM python:3.11\n\n# 작업 디렉토리를 /app으로 설정\nWORKDIR /app\n\n# Python 종속성 파일을 컨테이너의 /app 디렉토리로 복사\nCOPY requirements.txt /app\n\n# requirements.txt에 명시된 필요한 패키지 설치\nRUN pip install --no-cache-dir -r requirements.txt\n\n# flask 앱 파일을 컨테이너의 /app 디렉토리로 복사\nCOPY app.py /app\n\n# 포트 5000을 이 컨테이너 외부에 노출\nEXPOSE 5000\n\n# 컨테이너가 시작될 때 app.py 실행\nCMD [\"python\", \"app.py\"]\n```\n\n도커 이미지 생성\n\n```js\n# docker build -t <이미지-이름> <Dockerfile 경로>\ndocker build -t flask-image .\n```\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 변경되었습니다:\n\n\n![Docker Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_36.png)\n\nDocker Hub repository를 만들어보겠습니다. 아직 계정이 없다면 만드세요 - livingdevopswithakhilesh라는 이름으로 만들었어요.\n\n![Image 태깅](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_37.png)\n\n\n<div class=\"content-ad\"></div>\n\n```md\n# docker tag <local image> <docker hub username>/<repository name>:<tag>\ndocker tag flask-image livingdevopswithakhilesh/docker-demo-docker:1.0\n```\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_38.png)\n\n도커 허브에 이미지를 푸시하세요. 이미지를 푸시하기 전에 docker login을 실행해주세요.\n\n```sh\ndocker login\n# docker push <tagged-image>\ndocker push livingdevopswithakhilesh/docker-demo-docker:1.0\n```\n\n<div class=\"content-ad\"></div>\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_39.png)\n\n로컬 이미지를 삭제하고 Docker Hub에서 이미지를 가져와서 해당 이미지를 사용하여 컨테이너를 실행합니다.\n\n```js\ndocker pull livingdevopswithakhilesh/docker-demo-docker:1.0\n```\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_40.png)\n\n<div class=\"content-ad\"></div>\n\n```js\ndocker run -td -p 8080:5000 --name flask livingdevopswithakhilesh/docker-demo-docker:1.0\n```\n\n![Image](/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_41.png)\n\n이 블로그는 여기까지입니다. 다음 블로그에서 뵙겠습니다. 저와 함께 더 유용한 콘텐츠를 확인하려면 팔로우해주세요.\n\nLinkedin에서 연락하기: [https://www.linkedin.com/in/akhilesh-mishra-0ab886124/](https://www.linkedin.com/in/akhilesh-mishra-0ab886124/)\n\n<div class=\"content-ad\"></div>\n\n시리즈 Devops Zero to Hero의 세 번째 블로그였습니다. 첫 두 개 블로그를 확인해보세요 —","ogImage":{"url":"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_0.png"},"coverImage":"/assets/img/2024-05-18-Devopszerotohero3EverythingyouneedtoknowaboutDockers_0.png","tag":["Tech"],"readingTime":16},{"title":"DevOps 인터뷰 프로세스 개요 지원서부터 선발까지  제4부  Docker","description":"","date":"2024-05-18 16:39","slug":"2024-05-18-OverviewoftheDevOpsInterviewProcessFromApplicationtoSelectionPart4Docker","content":"\n\n도커(Docker)란 무엇인가요?\n\n![도커 이미지](/assets/img/2024-05-18-OverviewoftheDevOpsInterviewProcessFromApplicationtoSelectionPart4Docker_0.png)\n\n도커는 애플리케이션 배포, 확장 및 관리를 컨테이너 내에서 자동화하는 오픈소스 플랫폼입니다. 컨테이너를 사용하면 개발자가 라이브러리 및 기타 의존성과 함께 애플리케이션을 패키징하고 하나의 패키지로 배포할 수 있습니다.\n\n이를 통해 애플리케이션이 작성 및 테스트에 사용된 머신과 다른 사용자 정의 설정이 있을 수 있는 다른 리눅스 머신에서 실행될 것임을 보장합니다.\n\n<div class=\"content-ad\"></div>\n\n도커는 클라이언트-서버 아키텍처를 사용하며 호스트 머신에서 도커 데몬이 실행되고 클라이언트가 데몬과 통신하여 컨테이너를 관리합니다. 도커 이미지는 경량이고 독립적이며 실행 가능한 소프트웨어 패키지로, 응용 프로그램을 실행하는 데 필요한 모든 것(코드, 런타임, 라이브러리, 환경 변수 및 구성 파일)이 포함되어 있습니다. 이러한 이미지를 사용하여 도커 컨테이너를 생성합니다. 이미지는 도커 허브 등의 도커 레지스트리에 저장되어 사용자가 컨테이너화된 응용 프로그램을 효율적으로 공유하고 배포할 수 있도록 합니다.\n\n전반적으로 도커는 응용 프로그램이 실행되는 환경의 복잡성을 관리함으로써 개발 라이프사이클을 간소화하며 개발, 테스트 및 배포 프로세스를 예측 가능하고 효율적으로 만듭니다.\n\n도커 면접에서 무엇을 기대해야 할까요?\n\n고급 도커 면접 준비를 위해서는 컨테이너화 원리, 아키텍처 및 실제 도커 환경에 대한 깊은 이해가 필요합니다. 효율적이고 안전하며 확장 가능한 도커 솔루션을 만드는 데 능숙함을 증명해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n1: Docker Fundamentals\n\n- 컨테이너화 vs 가상화: 전통적 가상 머신과 비교하여 컨테이너의 차이점, 이점 및 제한 사항을 이해해보세요.\n\n- Docker 아키텍처: Docker 데몬, Docker 클라이언트, 이미지, 컨테이너, Docker Hub 및 Dockerfile에 익숙해지세요. 이러한 구성 요소가 Docker 생태계 내에서 상호 작용하는 방식을 이해하세요.\n\n2: Docker 이미지와 컨테이너\n\n<div class=\"content-ad\"></div>\n\n**Dockerfile Best Practices:** 효율적인 Dockerfile 작성 방법을 알고 계세요. 레이어 크기를 최소화하고 캐시 최적화를 위해 명령 순서를 조정하며, 최종 이미지 크기를 줄이기 위해 다단계 빌드를 사용하는 방법을 알고 계세요.\n\n**이미지 관리:** Docker 이미지를 관리하는 방법을 이해하세요. 태깅하기, 레지스트리에 푸시하고 풀하는 방법을 이해하며, 이미지 저장 및 레이어 최적화를 할 수 있습니다.\n\n**컨테이너 라이프사이클 관리:** 컨테이너를 시작, 정지, 일시 정지 및 제거하는 데 능숙하며, 컨테이너 상태와 로그를 관리할 수 있습니다.\n\n**Docker 네트워킹**\n\n<div class=\"content-ad\"></div>\n\n● 네트워킹 모델: Docker의 네트워킹 기능을 이해하세요. 그 중 브리지, 호스트, 오버레이, 맥빌란 네트워크를 포함합니다. 컨테이너 네트워킹을 구성하고 문제 해결할 수 있어야 합니다.\n\n● 포트 매핑과 통신: 컨테이너 포트를 호스트에 노출시키고 다른 네트워크 간에 컨테이너 간 통신을 활성화하는 방법을 알고 계세요.\n\n4: Docker 저장소와 볼륨\n\n● 지속적인 저장소: 상태를 유지해야 하는 애플리케이션의 중요성을 이해하고 Docker 볼륨 및 바인드 마운트를 사용하여 이를 구현하는 방법을 숙지하세요.\n\n<div class=\"content-ad\"></div>\n\n**저장 드라이버**: Docker가 지원하는 다양한 저장 드라이버를 알고, 성능 및 호환성 요구 사항에 따라 적절한 드라이버를 선택하는 방법을 알아보세요.\n\n**5**: Docker Compose\n\n**Docker Compose를 사용한 Orchestration**: 다중 컨테이너 Docker 애플리케이션을 정의하고 실행하는 방법을 알아봅니다. docker-compose.yml 파일에 포함된 구조와 옵션을 이해합니다.\n\n**Best Practices**: Docker Compose의 최상의 방법에 대해 논의해보세요. 환경별 구성 및 비밀 정보 관리와 같은 주제를 다룹니다.\n\n<div class=\"content-ad\"></div>\n\n6: 도커 스웜\n\n- 클러스터 관리: 도커 스웜 클러스터를 설정하고 관리하는 방법을 이해합니다.\n- 서비스 및 스택: 스웜에서 서비스를 배포하고 관리할 수 있으며, 스케일 업/다운을 수행하고 스택을 사용하여 복수 서비스 애플리케이션을 관리할 수 있습니다.\n\n6: 도커 보안\n\n<div class=\"content-ad\"></div>\n\n### 내용\n\n- **컨테이너 보안:** 컨테이너화 및 Docker의 보안 영향을 알아보세요. Docker 컨테이너 및 이미지를 안전하게 유지하는 방법을 이해하세요. 루트 사용자가 아닌 사용자, 취약점을 검색하는 이미지 스캔, 보안을 위한 Docker Bench 사용 등을 포함합니다.\n\n- **네트워크 보안:** 컨테이너 네트워크를 안전하게 유지하고 네트워크 정책을 구현하는 전략에 대해 논의하세요.\n\n- **성능 최적화**\n\n- **모니터링 및 로깅:** Docker 컨테이너 및 호스트를 모니터링하기 위한 도구 및 전략에 익숙해지세요. 로깅 최적 사례 및 Docker 자체 도구 또는 서드파티 솔루션 사용에 대해서도 알아보세요.\n\n<div class=\"content-ad\"></div>\n\n- 자원 제한: CPU 및 메모리 제약을 사용하여 컨테이너 자원을 제한하는 방법을 이해하여 최적의 컨테이너 성능과 자원 공유를 보장하세요.\n\n8: CI/CD 통합\n\n- CI/CD 파이프라인에서의 Docker 활용: Docker를 CI/CD 파이프라인에 통합하여 애플리케이션을 빌드, 테스트 및 배포하는 방법에 대해 논의하세요. Docker 지원을 제공하는 도구 및 플랫폼에 대해 알고 계세요.\n\n9: 고급 주제 및 트렌드\n\n<div class=\"content-ad\"></div>\n\n● Kubernetes vs. Docker Swarm: 컨테이너 오케스트레이션을 위한 Kubernetes와 Docker Swarm의 차이 및 사용 사례를 이해해 보세요.\n\n● 신흥 기술: Docker 및 컨테이너 생태계의 신흥 기술 및 트렌드, containerd 및 BuildKit과 같은 내용에 대해 알아두세요.\n\n이러한 주제들을 숙달하는 것 외에도, 복잡한 문제를 해결하거나 워크플로를 최적화하거나 시스템 아키텍처를 개선하는 데 Docker를 적용한 실제 시나리오에 대해 준비하시기 바랍니다. 이론적 지식과 실무 경험을 혼합하여 보여주는 것이 고급 Docker 인터뷰에서 두각을 나타내는 데 중요합니다.","ogImage":{"url":"/assets/img/2024-05-18-OverviewoftheDevOpsInterviewProcessFromApplicationtoSelectionPart4Docker_0.png"},"coverImage":"/assets/img/2024-05-18-OverviewoftheDevOpsInterviewProcessFromApplicationtoSelectionPart4Docker_0.png","tag":["Tech"],"readingTime":4},{"title":"쿠버네티스 내에서의 데이터베이스 좋은 아이디어인가요","description":"","date":"2024-05-18 16:37","slug":"2024-05-18-DatabaseinKubernetesIsthatagoodidea","content":"\n\nKubernetes/Docker에 데이터베이스를 저장해야 하는지 여전히 논란이 많습니다. Kubernetes(k8s)는 상태가 없는 응용 프로그램을 관리하는 데 뛰어나지만, 특히 PostgreSQL과 MySQL과 같은 데이터베이스와 같은 상태를 유지하는 서비스에서 기본적인 단점을 가지고 있습니다.\n\n이전 글 \"Docker에서 데이터베이스: 좋은 방법인가, 나쁜 방법인가\"에서는 데이터베이스를 컨테이너화하는 장단점에 대해 논의했습니다. 오늘은 K8S에서 데이터베이스를 조율하는 데 어떤 희생을 해야 하는지 탐구하고, 이것이 현명한 결정이 아닌 이유에 대해 살펴보겠습니다.\n\n# 개요\n\nKubernetes(k8s)는 복잡한 상태가 없는 응용 프로그램의 다양한 집합을 더 잘 관리하기 위해 고안된 우수한 컨테이너 조율 도구입니다. StatefulSet, PV, PVC 및 LocalhostPV와 같은 제공 기능에도 불구하고, 이러한 기능들은 여전히 높은 신뢰성을 요구하는 프로덕션 수준의 데이터베이스를 실행하기에는 충분하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n데이터베이스는 \"소 동물\"보다는 \"가축\"과 같이 주의 깊게 양육이 필요한 존재입니다. K8S에서 데이터베이스를 \"가축\"으로 취급한다면 외부 디스크/파일 시스템/스토리지 서비스를 새로운 \"데이터베이스 애완동물\"로 변신시킬 것입니다. 데이터베이스를 EBS/네트워크 스토리지에서 실행하면 신뢰성과 성능에서 상당한 단점이 발생합니다. 그러나 고성능 지역 NVMe 디스크를 사용하면 데이터베이스가 노드에 바인드되어 스케줄할 수 없어져서, K8S에 넣는 주요 목적을 무효화시킬 것입니다.\n\n데이터베이스를 K8S에 배치하는 것은 \"lose-lose(양쪽이 손해)\" 상황으로 이어집니다 - K8S는 비상태성에서의 간단함을 손실하며, 순수한 비상태적 사용처럼 빠르게 재배치, 스케줄링, 파괴, 재구축을 할 유연성이 부족합니다. 반면, 데이터베이스는 신뢰성, 보안, 성능 및 복잡성 비용에 맞교환으로 제한된 \"탄력성\"과 활용도를 얻을 수 있지만 - 가상 머신들도 이를 달성할 수 있습니다. 공중 클라우드 업체 외의 사용자들에게는 단점이 혜택을 제대로 상쇄합니다.\n\nK8S를 통해 나타나는 \"클라우드 네이티브 열기\"는 단지 K8S를 위해 K8S를 채택하는 왜곡된 현상이 되어버렸습니다. 엔지니어들은 대체할 수 없음을 증가시키기 위해 추가 복잡성을 더하고, 한편으로 경영자들은 산업에서 뒤처지는 것을 두려워하며 배포 경쟁에 휩쓸려갑니다. 자전거로 할 수 있는 작업에 탱크를 사용하여 자신을 증명하거나 경험을 쌓고자 하지만, 문제가 그러한 \"용사 퇴치\" 기술이 필요한지 고려하지 않으면, 이러한 종류의 구조적 호흡법은 결국 부정적인 결과로 이어질 것입니다.\n\n네트워크 스토리지의 신뢰성과 성능이 지역 스토리지를 능가할 때까지, 데이터베이스를 K8S에 배치하는 것은 현명한 선택이 아닙니다. 데이터베이스 관리 복잡성을 해결할 수 있는 다른 방법들이 있습니다, RDS와 Pigsty와 같은 오픈 소스 RDS 솔루션을 통한 방법들이 있으며, 이들은 베어 메탈이나 베어 OS에 기반을 두고 있습니다. 사용자들은 자신의 상황과 요구에 근거하여 현명한 결정을 내려야 하며, 이를 위해 장단점을 신중하게 고려해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n# 현상 유지\n\nK8S는 상태가 없는 애플리케이션 서비스를 관리하는 데 뛰어나지만, 초기에는 상태가 있는 서비스에 제한이 있었습니다. 그러나 K8S와 도커가 의도한 용도가 아니었음에도 불구하고 커뮤니티의 확장 열망은 멈출 줄 모릅니다. 전도자들은 K8S를 차세대 클라우드 운영 체제로 묘사하며, 데이터베이스가 반드시 Kubernetes 내의 일반적인 응용 프로그램이 될 것이라 주장합니다. 상태가 있는 서비스를 지원하기 위해 다양한 추상화가 등장했습니다: StatefulSet, PV, PVC, LocalhostPV 등이 있습니다.\n\n수많은 클라우드 네이티브 열정가들이 기존 데이터베이스를 K8S로 이전하려 시도했으며, 결과적으로 데이터베이스에 대한 CRD와 Operator가 증가하였습니다. PostgreSQL을 예로 들면 이미 PGO, StackGres, CloudNativePG, PostgresOperator, PerconaOperator, CYBERTEC-pg-operator, TemboOperator, Kubegres, KubeDB, KubeBlocks 등 다양한 K8S 배포 솔루션이 있습니다. CNCF 생태계가 빠르게 확장되어 복잡성의 놀이터가 되어가고 있습니다.\n\n그러나 복잡성은 비용을 의미합니다. \"비용 절감\"이 주류가 되면서, 반성의 목소리가 나타나고 있습니다. 공중 클라우드에서 K8S를 깊이 활용한 DHH와 같은 Could-Exit 선구자들은 자체 호스팅 오픈소스 솔루션으로 전환 중에 과도한 복잡성으로 인해 K8S를 버리고 Docker와 Kamal이라는 Ruby 도구만을 대안으로 채택했습니다. 많은 사람들이 데이터베이스와 같은 상태가 있는 서비스가 Kubernetes에 적합한지 의심하기 시작했습니다.\n\n<div class=\"content-ad\"></div>\n\nK8S 자체는 상태 지향 애플리케이션을 지원하기 위해 노력하면서, 원래의 컨테이너 오케스트레이션 플랫폼으로부터 벗어나 zun. 쿠버네티스 공동 창립자 팀 호킨은 올해의 KubeCon에서 드문 우려를 표명했습니다. \"K8s is Cannibalizing Itself!\"라고 말하며 \"쿠버네티스는 너무 복잡해졌으며, 자제력을 배워야 합니다. 그렇지 않으면 혁신을 멈추고 기초를 잃을 것입니다.\"\n\n# 승리의 상실\n\n클라우드 네이티브 분야에서 “애완동물”과 “가축”의 비유는 상태 지향 서비스를 설명하는 데 자주 사용됩니다. \"애완동물\"인 데이터베이스는 주의 깊고 개별적인 관리가 필요하지만, \"가축\"은 일회용이며 상태를 가지지 않는 애플리케이션을 나타냅니다(일회용성).\n\n![이미지](/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_0.png)\n\n<div class=\"content-ad\"></div>\n\nK8S의 주요 아키텍처 목표 중 하나는 가축으로 취급할 수 있는 것들을 가축으로 취급하는 것입니다. 데이터베이스에서 \"저장소와 연산을 분리\"하는 시도는 이 전략을 따릅니다: 상태를 가진 데이터베이스 서비스를 K8S 외부의 상태 저장소로 분리하고 K8S 내부에서 순수한 연산으로 분리하는 것입니다. 상태는 EBS/클라우드 디스크/분산 저장소 서비스에 저장되어 \"무상태\" 데이터베이스 부분이 K8S에서 자유롭게 생성, 삭제 및 예약될 수 있도록 합니다.\n\n그러나 데이터베이스, 특히 OLTP 데이터베이스는 디스크 하드웨어에 심하게 의존하며, 네트워크 저장소의 신뢰성과 성능은 아직도 로컬 디스크의 성능을 크게 뒤쳐지고 있습니다. 따라서 K8S는 LocalhostPV 옵션을 제공하여 컨테이너가 데이터 볼륨을 호스트 운영 체제에 직접 사용할 수 있도록 하여 고성능/고신뢰성 로컬 NVMe 디스크 저장소를 활용합니다.\n\n그러나 이것은 딜레마를 제시합니다: K8S의 스케줄링 및 오케스트레이션 능력을 위해 저품질 클라우드 디스크를 사용하고 데이터베이스의 신뢰성/성능을 용인해야할까요? 아니면 호스트 노드에 연결된 고성능 로컬 디스크를 사용하여 모든 유연한 스케줄링 능력을 사실상 상실해야할까요? 전자는 K8S의 작은 보트에 닻을 박아 전반적인 속도와 민첩성을 늦추는 것처럼 보이며, 후자는 배를 일정한 지점에 고정시키는 것과 같습니다.\n\n상태가 없는 K8S 클러스터를 실행하는 것은 간단하고 신뢰할 수 있지만, 물리적 머신의 순수한 운영 체제에서 상태가 있는 데이터베이스를 실행하는 것도 마찬가지입니다. 그러나 둘을 섞으면 K8S는 상태가 없는 유연성과 비현실적인 스케줄링 능력을 잃게 되고 데이터베이스는 신뢰성, 보안, 효율성 및 단숨함과 같은 핵심 속성을 희생하게 되며 데이터베이스에 근본적으로 중요하지 않은 탄력성, 자원 이용 및 Day1 전달 속도를 얻게 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## Pros and Cons\n\n중대한 기술 결정을 내리기 위해서는 가장 중요한 측면이 장단점을 따져보는 것입니다. 여기서 \"품질, 보안, 성능, 비용\"의 순서로, 데이터베이스를 K8S에 배치하는 기술적 트레이드오프와 클래식한 베어 메탈/가상머신 배포와의 비교에 대해 토의해보겠습니다. 모든 것을 다 다루는 포괄적인 논문을 쓰고 싶지는 않습니다. 대신 특정 질문 몇 가지를 제시하여 고려하고 토론하려고 합니다.\n\n품질\n\n<div class=\"content-ad\"></div>\n\nK8S는 물리적 배포와 비교할 때 추가적인 장애 지점과 아키텍처 복잡성을 도입하여 대폭 증가시키며, 장애의 평균 복구 시간을 상당히 연장시킬 수 있습니다. \"Docker에 데이터베이스를 넣는 것이 좋은 아이디어일까?\"라는 글에서, 신뢰성에 관한 논쟁을 제공했었는데, 이는 Kubernetes에도 적용될 수 있습니다. K8S와 Docker는 데이터베이스에 추가적이고 불필요한 의존성과 장애 지점을 도입하며, 커뮤니티 장애 지식 축적과 신뢰성 추적 기록 (MTTR/MTBF)이 부족합니다.\n\n클라우드 공급업체 분류 시스템에서 K8S는 PaaS에 속하고, RDS는 보다 기본적인 IaaS 계층에 속합니다. 데이터베이스 서비스는 K8S보다 높은 신뢰성 요구를 가지고 있습니다. 예를 들어, 많은 기업의 클라우드 관리 플랫폼은 추가 CMDB 데이터베이스에 의존합니다. 이 데이터베이스는 어디에 위치해야 할까요? K8S가 의존하는 것을 관리하게 두면 안 되며, 불필요한 추가적인 의존성을 추가해서도 안 됩니다. 알리바바 클라우드의 세계적인 대형 장애와 디디의 K8S 아키텍처 조정 재해는 우리에게 이 교훈을 전해주었습니다. 게다가, 이미 외부에 있는 데이터베이스 시스템이 있는데, K8S 내에 별도의 데이터베이스 시스템을 유지하는 것은 더욱 정당화하기 어렵습니다.\n\n보안\n\n멀티 테넌트 환경에서의 데이터베이스는 추가적인 공격 표면을 도입하여 더 높은 위험과 더 복잡한 감사 준수 도전을 가져옵니다. K8S는 데이터베이스를 보다 안전하게 만들까요? K8S 아키텍처 조정의 복잡성이 K8S를 잘 모르는 스크립트 키디들을 막을 수도 있지만, 진짜 공격자들에게는 보다 많은 구성 요소와 의존성이 더 넓은 공격 표면을 의미할 때도 있습니다.\n\n<div class=\"content-ad\"></div>\n\n\"BrokenSesame 알리바바 클라우드 PostgreSQL 취약점 기술 세부 정보”에서 보안 인원이 자체 PostgreSQL 컨테이너를 사용하여 K8S 호스트 노드로 이탈하고 K8S API 및 다른 유저들의 컨테이너와 데이터에 액세스했습니다. 이는 명백히 K8S에만 적용되는 문제입니다 — 리스크는 실재하며 이러한 공격이 발생한 바 있으며 심지어 로컬 클라우드 산업 선두주자인 알리바바 클라우드도 침투당했습니다.\n\n![이미지](/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_1.png)\n\n성능\n\n“도커에 데이터베이스를 넣으면 좋은 아이디어인가?”에서 설명한 대로, 추가적인 네트워크 부하, 인그레스 병목 현상 또는 성능이 저하된 클라우드 디스크는 모두 데이터베이스 성능에 부정적인 영향을 미칩니다. 예를 들어 “PostgreSQL@K8s 성능 최적화”에서 밝혀진 것처럼, K8S에서 데이터베이스 성능을 베어메탈과 거의 맞출 정도로 만들기 위해서는 상당한 기술 능력이 필요합니다.\"\n\n<div class=\"content-ad\"></div>\n\n![Database in Kubernetes](/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_2.png)\n\n효율성에 대한 또 다른 오해는 자원 활용입니다. 오프라인 분석 업무와 달리 중요한 온라인 OLTP 데이터베이스는 자원 활용을 증가시키려는 것이 아니라 시스템의 신뢰성과 사용자 경험을 향상시키기 위해 의도적으로 낮춰야 합니다. 많은 조각화된 비지니스가 있는 경우 PDB/공유 데이터베이스 클러스터를 통해 자원 활용률을 향상시킬 수 있습니다. K8S가 옹호하는 탄력성 효율성은 유일한 것이 아닙니다. — KVM/EC2도 이 문제를 효과적으로 해결할 수 있습니다.\n\n비용 측면에서 K8S와 다양한 오퍼레이터는 데이터베이스 관리의 일부 복잡성을 캡슐화하여 DBA가 없는 팀들에게 매력적입니다. 그러나 데이터베이스 관리를 담당하는 데 사용될 때 줄어드는 복잡성은 K8S 자체를 사용함으로써 도입되는 복잡성과는 비교되지 않을 정도입니다. 예를 들어, 무작위 IP 주소 변화와 Pod 자동 재시작은 상태를 저장하지 않는 응용프로그램에는 큰 문제가 되지 않을 수 있지만 데이터베이스에게는 용납할 수 없습니다. 많은 기업이 이러한 동작을 피하기 위해 kubelet을 수정하려고 시도했으며, 결과적으로 더 많은 복잡성과 유지 보수 비용을 도입하게 됐습니다.\n\n“비용 감소와 효율성 향상으로부터 비용 감소와 복잡성 최소화로”에서 언급한 대로, 지적 능력은 공간적으로 축적하기 어렵습니다. 데이터베이스에 문제가 발생하면 데이터베이스 전문가가 해결해야 하고, Kubernetes에 문제가 발생하면 K8S 전문가가 살펴봐야 합니다. 그러나 데이터베이스를 Kubernetes에 넣으면 복잡성이 결합되어 상태 공간이 급격히 증가하지만 개별 데이터베이스 전문가와 K8S 전문가의 지적 대역폭을 쌓기 어렵습니다. 문제를 해결하기 위해서는 이중 전문가가 필요하며, 이러한 전문가들은 명백히 순수한 데이터베이스 전문가보다 훨씬 더 드물고 비쌉니다. 이러한 건축적인 연출은 대다수의 팀, 상위 퍼블릭 클라우드/대기업을 포함해 대규모 장애가 발생할 경우 주요 진전의 원인이 될 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 클라우드 네이티브 열풍\n\n흥미로운 질문이 제기됩니다: K8S가 상태 유지 데이터베이스에 적합하지 않다면, 왜 많은 기업들, 대기업 포함, 이에 급급한 걸까요? 그 이유는 기술적인 측면이 아닙니다.\n\nGoogle이 내부 Borg 우주선을 본따 만든 K8S 전함을 오픈소스로 공개했고, 뒤처지지 않으려는 매니저들이 Google과 동등한 위치에 있을 것이라 생각하여 K8S를 채택하기에 급급했습니다. 그런데 Google 자체는 K8S를 사용하지 않고 있어, AWS를 방해하려고하거나 산업을 오도하기 위한 것이 더 가능성이 높습니다. 그러나 대부분의 기업은 Google과 같은 인력을 운영하기에 훨씬 단순한 방식이 필요할 겁니다. MySQL + PHP, PostgreSQL + Go/Python을 베어 메탈에서 실행하는 것만으로도 이미 많은 기업들이 IPO로 나아가는 길을 걷게 되었습니다.\n\n현대의 하드웨어 환경에서 대부분의 애플리케이션의 복잡성은 전체 수명 주기에 걸쳐 K8S를 사용할 정당성이 부족합니다. 그럼에도 불구하고 K8S를 상징으로 하는 \"클라우드 네이티브\" 열풍은 왜곡된 현상이 되었습니다: 단지 K8S를 사용하기 위해 K8S를 채택한다는 것. 일부 엔지니어들은 자신의 개인 목표를 달성하기 위해 대기업에서 사용하는 \"첨단\"이고 \"멋진\" 기술을 찾아내며, 직장을 옮길 수 있는 기회 혹은 승진을 위해 직장 안정성을 확보하기 위해 복잡성을 더하는 경향이 있습니다. 이 때 그들의 문제를 해결하기 위해 이러한 \"용감한 용사\" 기술이 꼭 필요한지를 고려하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n클라우드 네이티브 랜드스케이프는 화려한 프로젝트로 가득합니다. 매번 새로운 개발 팀은 무언가를 도입하고 싶어합니다: 오늘은 헬름, 내일은 쿠벨라. 밝은 미래와 최고의 효율에 대해 크게 이야기하지만, 실제로는 건축적 복잡성의 산과 \"YAML 아이들\"의 놀이터를 만들어냅니다. 최신 기술을 만지작거리고, 개념을 발명하며, 사용자들이 복잡성과 유지보수 비용을 감당하는 대가로 경험과 평판을 쌓는 말이지요.\n\n![이미지](/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_3.png)\n\n클라우드 네이티브 운동의 철학은 매력적입니다 — 모든 사용자를 위해 퍼블릭 클라우드의 탄탄한 예약 기능을 민주화하기 위해서. K8S는 실제로 상태가 없는 애플리케이션에서 뛰어납니다. 그러나 지나친 열정이 K8S를 원래의 의도와 방향에서 벗어나 상태가 있는 애플리케이션을 오케스트레이팅에서 잘못된 결정으로 버거운 상태로 만듭니다.\n\n# 현명한 결정을 내리는 것\n\n<div class=\"content-ad\"></div>\n\n여러 해 전에 K8S를 처음 만났을 때 나도 열정적이었어요. 그곳은 TanTan이었죠. 2만 개가 넘는 코어와 수백 개의 데이터베이스 클러스터가 있었고, 데이터베이스를 Kubernetes에 넣어보고 모든 가능한 Operator를 테스트해보려고 했어요. 그러나 2-3년의 광범위한 연구와 설계 노력 끝에 제가 진정되었고 그 광기를 버렸어요. 대신에 저희 데이터베이스 서비스를 베어 메탈/운영 체제를 기반으로 설계했죠. 우리에게 K8S가 데이터베이스에 제공한 혜택은 문제와 귀찮음에 비해하다는 것을 깨달았거든요.\n\n데이터베이스를 K8S에 넣어야 할까요? 이것은 상황에 따라 다릅니다. 과도한 자원 판매를 기반으로 하는 공개 클라우드 업체의 경우, 확장성과 이용률이 중요할 것이며, 이는 수익과 이윤과 직접적으로 연결됩니다. 신뢰성과 성능은 후순위일 수 있습니다. 그래도 99.9% 이하의 가용성은 매달 25%의 신용을 보상해야 합니다. 그러나 대부분의 사용자, 우리를 포함하여, 이러한 절충안은 다릅니다. 일회성 Day1 설정, 확장성, 자원 사용률은 주요 관심사가 아닙니다. 신뢰성, 성능, Day2 운영 비용은 이러한 데이터베이스의 핵심 속성 중 가장 중요합니다.\n\n우리는 데이터베이스 서비스 아키텍처를 오픈 소스로 공개했어요. PostgreSQL 배포 및 로컬 기반 RDS 대안인 Pigsty입니다. 우리는 K8S 및 Docker의 \"한 번 빌드하고 어디서든 실행\" 접근 방식 대신 다른 OS 배포판 및 주요 버전에 적응했고, Ansible을 사용하여 K8S CRD IaC와 유사한 API를 통해 관리 복잡성을 해결했어요. 이것은 수고스러운 작업이었지만 올바른 선택이었어요. PostgreSQL을 K8S에 넣는 서투른 시도는 세상이 필요로 하는 것이 아니에요. 그럼에도, 하드웨어 성능과 신뢰성을 극대화한 프로덕션 데이터베이스 서비스 아키텍처가 필요합니다.\n\n![Database in Kubernetes](/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_4.png)\n\n<div class=\"content-ad\"></div>\n\n어쩌면 언젠가는 분산 네트워크 스토리지의 신뢰성과 성능이 로컬 스토리지를 넘어서며 주류 데이터베이스가 스토리지-연산 분리를 기본적으로 지원할 때, 상황이 다시 바뀔 수도 있습니다 — K8S가 데이터베이스에 적합해질 수도 있겠죠. 하지만 현재로서는, 제 생각으로는 심각한 프로덕션 OLTP 데이터베이스를 K8S에 배치하는 것은 미숙하고 부적절하다고 믿습니다. 독자 여러분이 이 문제에 대해 현명한 선택을 하시기를 바랍니다.\n\n# 참고\n\n도커에서의 데이터베이스: 좋은 생각인가요?\n\n\"Kubernetes Founder Speaks Out! K8s Facing a Backlash!\"\n\n<div class=\"content-ad\"></div>\n\n\"도커의 저주: 최고의 해결책이라 생각했지만 결국 '죄악'인가?\"  \n\n\"디디의 장애로 배우는 것\"  \n\n\"Kubernetes에서 PostgreSQL 성능 최적화 기억\"  \n\n\"Kubernetes에서 데이터베이스 실행하기\"\n\n<div class=\"content-ad\"></div>\n\n컴퓨터 하드웨어의 혜택을 다시 누리세요.\n\n비용을 낮추고 효율을 높이는 방법을 찾아보세요.\n\n컴퓨터 하드웨어의 혜택을 다시 누리세요.\n\n우리는 알리바바 클라우드의 역사적 장애로 무엇을 배울 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n클라우드 컴퓨팅을 포기할 시간인가요?\n\n클라우드 SLA는 위약물인가요?","ogImage":{"url":"/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_0.png"},"coverImage":"/assets/img/2024-05-18-DatabaseinKubernetesIsthatagoodidea_0.png","tag":["Tech"],"readingTime":10},{"title":"컨테이너 안에서 RStudio 실행하기","description":"","date":"2024-05-18 16:35","slug":"2024-05-18-RunningRStudioInsideaContainer","content":"\n\n## 로컬 RStudio 설정을 사용하여 컨테이너 내에 RStudio 서버를 설정하는 단계별 가이드\n\n안녕하세요! 이 문서는 로컬 RStudio 설정을 사용하여 컨테이너 내에 RStudio 서버를 설정하는 단계별 가이드입니다. Rocker RStudio 이미지를 사용하여 도커 실행 명령어와 인수를 사용하여 커스터마이징하는 방법을 안내합니다.\n\n이 튜토리얼을 완료하면 다음을 수행할 수 있을 것입니다:\n\n- 컨테이너 내에 RStudio 서버 시작하기\n- 로컬 폴더 마운트하기\n- 로컬 RStudio 설정 복제하기 (색 테마, 코드 스니펫 등)\n- 로컬 Renviron 설정 불러오기\n\n도움이 되길 바라며, 시작해보세요!\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-RunningRStudioInsideaContainer_0.png\" />\n\n# 소개\n\nRStudio는 R 프로그래밍 언어를 위한 기본 IDE입니다. VScode와 같은 일반적인 목적의 IDE와 달리, RStudio는 R 사용자 및 그들의 요구에 특별히 설계 및 구축되었습니다. 이것이 RStudio가 R 사용자들 사이에서 인기를 얻는 이유 중 하나입니다. 기본적으로 RStudio는 Docker를 지원하지 않습니다. 컨테이너 내에서 RStudio를 설정하고 실행하는 주요 방법은 RStudio 서버 버전을 사용하는 것입니다. 이것은 일부 사용자에게 진입 장벽이 될 수 있는 컨테이너 내에서 서버를 설치하고 설정해야 한다는 것을 의미합니다. 다행히 R 이미지의 주요 소스 인 Rocker 프로젝트는 RStudio 서버가 내장되어 있고 사용 준비가 된 이미지를 제공합니다.\n\n이 튜토리얼에서는 Docker Hub에서 사용 가능한 Rocker RStudio 이미지를 사용할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n# 준비 사항\n\n이 튜토리얼에 참여하고 아래 코드를 실행하려면 다음이 필요합니다:\n\n- Docker Desktop (또는 대체품)\n- Docker Hub 계정\n- docker run 명령어의 기본적인 이해\n\n# Rocker로 시작하기\n\n<div class=\"content-ad\"></div>\n\nThe Rocker Project은 내장된 R 이미지의 주요 허브입니다. base-r, tidyverse, ML-verse, shiny, 지리적 공간 등과 같이 서로 다른 R 환경 설정이 제공됩니다. 물론 RStudio 서버 이미지도 포함되어 있습니다. 사용 가능한 모든 R 이미지 목록은 Rocker의 Docker Hub 페이지에서 확인할 수 있습니다.\n\n![image](/assets/img/2024-05-18-RunningRStudioInsideaContainer_1.png)\n\n이번에는 rocker/rstudio 이미지를 사용할 것인데, 이미지 이름 그대로 RStudio 서버가 설치되어 사용할 준비가 되어 있습니다. docker run 명령을 사용하여 이 컨테이너를 대화형 모드로 실행하고 브라우저를 통해 RStudio 서버에 액세스할 수 있습니다.\n\n이미지를 docker pull 명령으로 가져와 시작해보겠습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n>docker pull rocker/rstudio                                                                                                                                            확인\n기본 태그 사용: latest\nlatest: rocker/rstudio에서 가져오는 중\na4a2c7a57ed8: 가져오기 완료\nd0f9831967fe: 가져오기 완료\ne78811385d51: 가져오기 완료\nc61633a20287: 가져오기 완료\n832cef14f2fb: 가져오기 완료\n8395fbba6231: 가져오기 완료\nfb53abdcfb34: 가져오기 완료\nc942edef0d7f: 가져오기 완료\nDigest: sha256:8e25784e1d29420effefae1f31e543c792d215d89ce717b0cc64fb18a77668f3\n상태: rocker/rstudio:latest에 대한 새로운 이미지 다운로드 완료\ndocker.io/rocker/rstudio:latest\n```\n\n이미지가 성공적으로 다운로드되었는지 확인하려면 docker images 명령어를 사용할 수 있습니다:\n\n```js\n>docker images                                                                                                                                                    확인  36초\n저장소          태그         이미지 ID      작성일         크기\nrocker/rstudio  최신        7039fb162243   2일 전       1.94GB\n```\n\n이제 Rocker Project에서 제안한 명령어를 사용하여 docker run 명령어로 컨테이너 내에서 RStudio를 시작해 봅시다.```\n\n<div class=\"content-ad\"></div>\n\n```js\n도커를 실행하고 RStudio 서버를 브라우저에서 열기 전에 위에서 사용한 실행 인수를 검토해보겠습니다:\n\n- rm — 컨테이너 종료 시 자동으로 삭제합니다 (터미널에서 control + c를 누름)\n- ti — 대화형 모드로 컨테이너를 실행합니다\n- e — 환경 변수를 설정합니다. 이 경우 서버 로그인 암호를 'yourpassword'로 정의합니다\n- p — 포트 매핑을 정의합니다. 이 경우 컨테이너의 8787포트를 로컬 머신의 8787포트와 매핑합니다\n\n위 명령을 실행한 후, 로컬 호스트 8787번에서 RStudio 서버에 액세스할 수 있습니다 (예: http://localhost:8787). 이때 로그인 페이지가 나타나며, 여기서는 다음을 사용해야 합니다:\n```\n\n<div class=\"content-ad\"></div>\n\n- 사용자 이름: rstudio\n- 비밀번호: yourpassword (run 명령에서 설정한 대로)\n\n다음 출력을 기대해야 합니다:\n\n<img src=\"/assets/img/2024-05-18-RunningRStudioInsideaContainer_2.png\" />\n\n# 이런! 일시적이네요!\n\n<div class=\"content-ad\"></div>\n\n기본적으로 Docker 컨테이너는 일회성 모드에서 실행됩니다. 컨테이너에 생성하고 저장한 코드나 입력값은 컨테이너 실행 시간이 종료되면 손실됩니다. Docker를 개발 환경으로 사용하고 싶다면 이는 실용적이거나 유용하지 않습니다. 이 문제를 해결하기 위해 우리는 볼륨(v) 인수를 사용할 것이며, 이를 통해 로컬 폴더를 컨테이너 파일 시스템에 마운트할 수 있습니다.\n\n아래 코드는 볼륨 인수를 사용하여 실행 명령을 실행하는 폴더(e.g., .)를 RStudio 서버 홈 폴더에 마운트하는 방법을 보여줍니다:\n\n```js\ndocker run --rm -ti \\\n-v .:/home/rstudio \\\n-e PASSWORD=yourpassword \\\n-p 8787:8787 rocker/rstudio\n```\n\n이제 브라우저로 돌아가서 지역 호스트 주소인 http://localhost:8787을 사용하여 RStudio 서버를 다시 엽니다. RStudio 파일 섹션에는 마운트된 로컬 폴더에 있는 폴더나 파일을 볼 수 있을 것입니다. 제 경우, 튜토리얼 폴더를 마운트하겠습니다. 이 폴더에는 다음과 같은 폴더가 있습니다:\n\n<div class=\"content-ad\"></div>\n\n```js\n.\n├── Introduction-to-Docker\n├── awesome-ds-setting\n├── forecast-poc\n├── forecasting-at-scale\n├── lang2sql\n├── postgres-docker\n├── python\n├── rstudio-docker\n├── sdsu-docker-workshop\n├── shinylive-r\n├── statistical-rethinking-2024\n├── vscode-python\n├── vscode-python-template\n├── vscode-r\n└── vscode-r-template\n```\n\n아래 스크린샷에서 볼 수 있듯이 RStudio 서버에서 로컬 폴더에 접근 가능합니다 (보라색 사각형으로 표시됨):\n\n<img src=\"/assets/img/2024-05-18-RunningRStudioInsideaContainer_3.png\" />\n\n이를 통해 컨테이너에서 실행 시 로컬 폴더로 읽고 쓸 수 있게 되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# 로컬 RStudio 설정 복제하기\n\n이전 섹션에서는 볼륨 인자를 사용하여 로컬 폴더를 컨테이너에 장착하는 방법을 보았습니다. 이를 통해 컨테이너 안에서 작업하면서 코드를 로컬로 저장할 수 있게 되었습니다. 이번 섹션에서는 우리가 컨테이너 내에 있는 것을 사용한 로컬 RStudio 설정을 적용하는 방법을 살펴보겠습니다. 여기서 아이디어는 컨테이너를 시작하고 로컬 설정으로 RStudio 서버를 실행하여 컨테이너를 다시 시작할 때마다 설정을 업데이트할 필요 없이 로컬 설정을 유지하는 것입니다. 이는 색 테마 설정, 코드 스니펫, 환경 변수 등과 같은 로컬 설정을 로딩하는 것을 포함합니다.\n\n로컬 RStudio 구성 폴더를 가진 도커 실행을 업데이트하기 전에, 로컬과 컨테이너 내의 config 폴더의 경로를 식별해야 합니다. 예를 들어, 내 시스템의 경로는 ~/.config/rstudio이고 아래 폴더와 파일들을 포함합니다:\n\n```js\n.\n├── dictionaries\n│   └── custom\n├── rstudio-prefs.json\n└── snippets\n    └── r.snippets\n```\n\n<div class=\"content-ad\"></div>\n\n컨테이너 내의 .config/rstudio 폴더도 /home/rstudio/ 아래에 있습니다. 따라서 다음 매핑을 사용할 것입니다:\n\n```js\n$HOME/.config/rstudio:/home/rstudio/.config/rstudio\n```\n\n또한, 로컬 환경 변수를 사용하기 위해 .Renviron 파일을 마운트하려고 합니다. .Renviron 파일은 로컬 머신의 루트 폴더에 있으며, 로컬 파일을 컨테이너 내의 파일로 매핑하기 위해 동일한 접근 방식을 따릅니다:\n\n```js\n$HOME/.Renviron:/home/rstudio/.Renviron\n```\n\n<div class=\"content-ad\"></div>\n\n이제 모두 함께 추가하고 컨테이너를 다시 시작해봅시다:\n\n```js\ndocker run --rm -ti \\\n-v .:/home/rstudio \\\n-v $HOME/.config/rstudio:/home/rstudio/.config/rstudio \\\n-v $HOME/.Renviron:/home/rstudio/.Renviron \\\n-e PASSWORD=yourpassword \\\n-p 8787:8787 rocker/rstudio\n```\n\n로컬 RStudio 구성 폴더를 컨테이너의 폴더와 연결한 후에는 서버 설정이 이제 내 컴퓨터의 로컬 RStudio 설정과 일치되었습니다:\n\n![Running RStudio Inside a Container](/assets/img/2024-05-18-RunningRStudioInsideaContainer_4.png)\n\n<div class=\"content-ad\"></div>\n\n# 요약\n\n이 튜토리얼은 Rocker의 RStudio 이미지를 사용자 정의하는 데 docker run 명령어를 사용하는 방법에 초점을 맞추고 있습니다. 우리는 볼륨 인수를 사용하여 로컬 폴더를 컨테이너의 작업 디렉토리에 마운트했습니다. 이를 통해 컨테이너 환경에서 작업하고 작업물을 로컬로 저장할 수 있게 되었습니다. 또한 우리는 볼륨 인수를 사용하여 로컬 RStudio 설정을 컨테이너에 복제했습니다. 이를 통해 로컬 환경에서 컨테이너로의 전환이 더 원활해졌습니다. 매개변수를 계속 추가하고 사용함에 따라 명령이 길고 복잡해질 수 있습니다. 실행 설정을 완료하면 Docker Compose를 사용하여 YAML 파일로 전환하는 것이 다음 단계입니다. 컨테이너의 시작 프로세스를 단순화하는 것을 넘어서, Docker Compose는 여러 컨테이너를 시작하는 등 더 복잡한 시나리오를 관리할 수 있게 해줍니다.\n\n# 자원\n\n- RStudio — https://posit.co/products/open-source/rstudio/\n- The Rocker Project — https://rocker-project.org/\n- Docker Hub — https://hub.docker.com/","ogImage":{"url":"/assets/img/2024-05-18-RunningRStudioInsideaContainer_0.png"},"coverImage":"/assets/img/2024-05-18-RunningRStudioInsideaContainer_0.png","tag":["Tech"],"readingTime":7}],"page":"48","totalPageCount":61,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}