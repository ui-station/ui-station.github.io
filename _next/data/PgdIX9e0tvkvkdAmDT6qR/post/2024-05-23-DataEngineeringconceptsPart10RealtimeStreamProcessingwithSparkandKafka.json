{"pageProps":{"post":{"title":"데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리","description":"","date":"2024-05-23 14:05","slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka","content":"\n이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\n\n목차:\n\n1. 스트림 처리란\n2. 카프카의 특징\n3. 카프카 구성\n4. 카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\n5. 스파크 구조화 스트리밍 API\n6. 데이타브릭스 델타 레이크\n7. 실전 프로젝트\n\n이전 데이터 보안 부분으로 이동하는 링크입니다:\n\n## 스트림 처리란?\n\n<div class=\"content-ad\"></div>\n\n일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\n\n![데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png)\n\n스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\n\n카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 카프카 특징\n\n- 견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\n- 유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\n- 확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\n\n  - 파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\n  - 수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\n\n## 카프카 구성\n\n<div class=\"content-ad\"></div>\n\n카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\n\n- 파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\n- 복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\n- 유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\n- 자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\n- ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\n\n## 다른 카프카 서비스\n\nKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\" />\n\nksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\" />\n\n스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png)\n\n## Spark Structured Streaming API\n\nSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png)\n\n<div class=\"content-ad\"></div>\n\n따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\n\n또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png)\n\n## Databricks Delta Lake\n\n<div class=\"content-ad\"></div>\n\nDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\n\nDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\n\n스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 실시간 스트리밍 아키텍처\n\n카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\n\n문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\n\n- 단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\n- 단계 2: 토픽 생성\n- 단계 3: 클러스터 API 키 쌍 생성\n\n위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\n\n- 단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png)\n\nUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\n\n- Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n- 단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\n\n- 단계 7: Kafka 토픽에서 스트림 읽기\n\n```js\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Environmental Reporting\").getOrCreate()\n\nkafkaDF = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"abcd.us-west4.gcp.confluent.cloud:9092\") \\\n    .option(\"subscribe\", \"illegal_dumping\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\" password=\"\";\"\"\") \\\n    .load()\n\nprocessedDF = kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\ndisplay(processedDF)\n```\n\n- 단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pyspark.sql.functions as F\nfrom  pyspark.sql.functions import col, struct, to_json\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\njson_schema = StructType(\n  [\n    StructField(\"incident_type\", StringType(), nullable = False),\n    StructField(\"location\", StringType(), nullable = False),\n    StructField(\"description\", StringType(), nullable = True),\n    StructField(\"contact\", StringType(), nullable = True)\n  ]\n)\n\n# Using Spark SQL to write queries on the streaming data in processedDF\n\nquery = processedDF.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n      .select(F.col(\"value.incident_type\"),F.col(\"value.location\"))\ndisplay(query)\n```\n\nWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\n\nDefine a UDF(User Defined Function) to find out the region from the location:\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the regions_to_states dictionary\nregions_to_states = {\n    'South': ['West Virginia', 'District of Columbia', 'Maryland', 'Virginia',\n              'Kentucky', 'Tennessee', 'North Carolina', 'Mississippi',\n              'Arkansas', 'Louisiana', 'Alabama', 'Georgia', 'South Carolina',\n              'Florida', 'Delaware'],\n    'Southwest': ['Arizona', 'New Mexico', 'Oklahoma', 'Texas'],\n    'West': ['Washington', 'Oregon', 'California', 'Nevada', 'Idaho', 'Montana',\n             'Wyoming', 'Utah', 'Colorado', 'Alaska', 'Hawaii'],\n    'Midwest': ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota',\n                'Iowa', 'Missouri', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana',\n                'Ohio'],\n    'Northeast': ['Maine', 'Vermont', 'New York', 'New Hampshire', 'Massachusetts',\n                  'Rhode Island', 'Connecticut', 'New Jersey', 'Pennsylvania']\n}\n\n#from geotext import GeoText\nfrom geopy.geocoders import Nominatim\n\n# Define a function to extract state names from location text\ndef extract_state(location_text):\n    geolocator = Nominatim(user_agent=\"my_application\")\n    location = geolocator.geocode(location_text)\n    #print(location)\n    #print(type(location.raw))\n    if location:\n        state = location.raw['display_name'].split(',')[-2]\n        return state\n    else:\n        return \"Unknown\"\n\n# Create a UDF to map states to regions\n@udf(StringType())\ndef map_state_to_region(location):\n    state = extract_state(location).strip()\n    for region, states in regions_to_states.items():\n        if state in states:\n            return region\n    return \"Unknown\"  # Return \"Unknown\" for states not found in the dictionary\n\n# Apply the UDF to map states to regions\ndf_with_region = query.withColumn(\"region\", map_state_to_region(query[\"location\"]))\n\ndisplay(df_with_region)\n```\n\n\n<div class=\"content-ad\"></div>\n\n- 단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# VADER 감정 분석기 초기화\nanalyzer = SentimentIntensityAnalyzer()\n\n# Description 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef analyze_sentiment(description):\n    # VADER에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.polarity_scores(description)['neg']\n\n    # 감정 점수를 기반으로 심각도 분류\n    if sentiment_score >= 0.4:\n        return \"High\"\n    elif sentiment_score >= 0.2 and sentiment_score < 0.4:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# 감정 분석을 위한 UDF 생성\nsentiment_udf = udf(analyze_sentiment, StringType())\n\n# 처리된 DataFrame(processedDF)의 description 열에 UDF 적용\n# 실제 DataFrame 및 열 이름으로 \"processedDF\" 및 \"description_column\"을 대체합니다.\nprocessedDF_with_severity = query.withColumn(\"severity\", sentiment_udf(\"description\"))\n\n# 추가된 심각도 열이 있는 DataFrame 표시\ndisplay(processedDF_with_severity)\n```\n\n환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\n\n데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\n\n<div class=\"content-ad\"></div>\n\n![Real-time Stream Processing](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png)\n\n위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\n\n또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\n\n```js\n# 스트리밍 DataFrame을 임시 뷰로 등록\nprocessedDF_with_severity.createOrReplaceTempView(\"incident_reports\")\n\n# 집계를 위한 SQL 쿼리 정의\ntotal_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\"\"\n\nseverity_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\"\"\n\n# 집계 수행\ntotal_incidents_df = spark.sql(total_incidents_query)\nseverity_incidents_df = spark.sql(severity_incidents_query)\n\n```\n\n<div class=\"content-ad\"></div>\n\n\n\n![Data Engineering Concepts](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png)\n\nSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\n\n1. 비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\n2. 전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\n\n- 단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\n\n```js\n# Delta Lake에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \"`result_delta_table`\"\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \"/FileStore/tables/checkpoints\"\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.writeStream\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_location)\\\n  .trigger(processingTime='10 seconds')\\ # 10초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .format(\"delta\")\\\n  .toTable(delta_table_path)\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\n\n![Delta Table as Dataframe](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png)\n\n또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\n\n![Query Delta Table with SQL](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png)\n\n<div class=\"content-ad\"></div>\n\n읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\n\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png","tag":["Tech"],"readingTime":13},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ol: \"ol\",\n    li: \"li\",\n    h2: \"h2\",\n    img: \"img\",\n    ul: \"ul\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"목차:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"스트림 처리란\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"카프카의 특징\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"카프카 구성\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"스파크 구조화 스트리밍 API\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"데이타브릭스 델타 레이크\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"실전 프로젝트\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이전 데이터 보안 부분으로 이동하는 링크입니다:\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"스트림 처리란?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png\",\n        alt: \"데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"카프카 특징\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"카프카 구성\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"다른 카프카 서비스\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Kafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Spark Structured Streaming API\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Spark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Databricks Delta Lake\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Databricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Delta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"실시간 스트리밍 아키텍처\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"단계 2: 토픽 생성\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"단계 3: 클러스터 API 키 쌍 생성\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png\",\n        alt: \"Image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Use the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png\",\n        alt: \"Image\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"단계 7: Kafka 토픽에서 스트림 읽기\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SparkSession\"\n        }), \"\\n\\nspark = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SparkSession\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"builder\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"appName\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Environmental Reporting\\\"\"\n        }), \").\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"getOrCreate\"\n        }), \"()\\n\\nkafkaDF = spark \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"readStream\"\n        }), \" \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"format\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"kafka\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"kafka.bootstrap.servers\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"abcd.us-west4.gcp.confluent.cloud:9092\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"subscribe\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"illegal_dumping\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"startingOffsets\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"earliest\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"kafka.security.protocol\\\"\"\n        }), \",\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"SASL_SSL\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"kafka.sasl.mechanism\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"PLAIN\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"kafka.sasl.jaas.config\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\" password=\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\";\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \") \\\\\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"load\"\n        }), \"()\\n\\nprocessedDF = kafkaDF.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"selectExpr\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"CAST(key AS STRING)\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"CAST(value AS STRING)\\\"\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"(processedDF)\\n\"]\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"functions\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" F\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \"  pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"functions\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" col, struct, to_json\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"types\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructField\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructType\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"MapType\"\n        }), \"\\n\\njson_schema = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructType\"\n        }), \"(\\n  [\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructField\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"incident_type\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"(), nullable = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"False\"\n        }), \"),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructField\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"location\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"(), nullable = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"False\"\n        }), \"),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructField\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"description\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"(), nullable = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \"),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StructField\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"contact\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"(), nullable = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \")\\n  ]\\n)\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Using\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Spark\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"SQL\"\n        }), \" to write queries on the streaming data \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" processedDF\\n\\nquery = processedDF.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"withColumn\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'value'\"\n        }), \", F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_json\"\n        }), \"(F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"col\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'value'\"\n        }), \").\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"cast\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'string'\"\n        }), \"), json_schema))  \\\\\\n      .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"select\"\n        }), \"(F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"col\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"value.incident_type\\\"\"\n        }), \"),F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"col\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"value.location\\\"\"\n        }), \"))\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"(query)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Define a UDF(User Defined Function) to find out the region from the location:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"functions\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" udf\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"types\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Define\"\n        }), \" the regions_to_states dictionary\\nregions_to_states = {\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'South'\"\n        }), \": [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'West Virginia'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'District of Columbia'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Maryland'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Virginia'\"\n        }), \",\\n              \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Kentucky'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Tennessee'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'North Carolina'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Mississippi'\"\n        }), \",\\n              \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Arkansas'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Louisiana'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Alabama'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Georgia'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'South Carolina'\"\n        }), \",\\n              \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Florida'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Delaware'\"\n        }), \"],\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Southwest'\"\n        }), \": [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Arizona'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'New Mexico'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Oklahoma'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Texas'\"\n        }), \"],\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'West'\"\n        }), \": [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Washington'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Oregon'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'California'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Nevada'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Idaho'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Montana'\"\n        }), \",\\n             \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Wyoming'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Utah'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Colorado'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Alaska'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Hawaii'\"\n        }), \"],\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Midwest'\"\n        }), \": [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'North Dakota'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'South Dakota'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Nebraska'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Kansas'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Minnesota'\"\n        }), \",\\n                \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Iowa'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Missouri'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Wisconsin'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Illinois'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Michigan'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Indiana'\"\n        }), \",\\n                \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Ohio'\"\n        }), \"],\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Northeast'\"\n        }), \": [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Maine'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Vermont'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'New York'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'New Hampshire'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Massachusetts'\"\n        }), \",\\n                  \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Rhode Island'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Connecticut'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'New Jersey'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Pennsylvania'\"\n        }), \"]\\n}\\n\\n#\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" geotext \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"GeoText\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" geopy.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"geocoders\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Nominatim\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Define\"\n        }), \" a \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"function\"\n        }), \" to extract state names \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" location text\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"extract_state\"\n        }), \"(location_text):\\n    geolocator = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Nominatim\"\n        }), \"(user_agent=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"my_application\\\"\"\n        }), \")\\n    location = geolocator.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"geocode\"\n        }), \"(location_text)\\n    #\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(location)\\n    #\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"type\"\n        }), \"(location.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"raw\"\n        }), \"))\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"location\"\n        }), \":\\n        state = location.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"raw\"\n        }), \"[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'display_name'\"\n        }), \"].\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"','\"\n        }), \")[-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"]\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" state\\n    \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"else\"\n        }), \":\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Unknown\\\"\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Create\"\n        }), \" a \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"UDF\"\n        }), \" to map states to regions\\n@\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"udf\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"())\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"map_state_to_region\"\n        }), \"(location):\\n    state = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"extract_state\"\n        }), \"(location).\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"strip\"\n        }), \"()\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" region, states \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" regions_to_states.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"items\"\n        }), \"():\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" state \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"states\"\n        }), \":\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" region\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Unknown\\\"\"\n        }), \"  # \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Unknown\\\"\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" states not found \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" the dictionary\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Apply\"\n        }), \" the \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"UDF\"\n        }), \" to map states to regions\\ndf_with_region = query.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"withColumn\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"region\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"map_state_to_region\"\n        }), \"(query[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"location\\\"\"\n        }), \"]))\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"(df_with_region)\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"functions\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" udf\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"types\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" vaderSentiment.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"vaderSentiment\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentimentIntensityAnalyzer\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"VADER\"\n        }), \" 감정 분석기 초기화\\nanalyzer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentimentIntensityAnalyzer\"\n        }), \"()\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Description\"\n        }), \" 텍스트에 대한 감정 분석을 수행하는 함수 정의\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"analyze_sentiment\"\n        }), \"(description):\\n    # \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"VADER\"\n        }), \"에서 compound 감정 점수 가져오기\\n    sentiment_score = analyzer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"polarity_scores\"\n        }), \"(description)[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'neg'\"\n        }), \"]\\n\\n    # 감정 점수를 기반으로 심각도 분류\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" sentiment_score >= \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.4\"\n        }), \":\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"High\\\"\"\n        }), \"\\n    elif sentiment_score >= \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.2\"\n        }), \" and sentiment_score < \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.4\"\n        }), \":\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Medium\\\"\"\n        }), \"\\n    \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"else\"\n        }), \":\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Low\\\"\"\n        }), \"\\n\\n# 감정 분석을 위한 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"UDF\"\n        }), \" 생성\\nsentiment_udf = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"udf\"\n        }), \"(analyze_sentiment, \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"StringType\"\n        }), \"())\\n\\n# 처리된 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"DataFrame\"\n        }), \"(processedDF)의 description 열에 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"UDF\"\n        }), \" 적용\\n# 실제 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"DataFrame\"\n        }), \" 및 열 이름으로 \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"processedDF\\\"\"\n        }), \" 및 \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"description_column\\\"\"\n        }), \"을 대체합니다.\\nprocessedDF_with_severity = query.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"withColumn\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"severity\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sentiment_udf\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"description\\\"\"\n        }), \"))\\n\\n# 추가된 심각도 열이 있는 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"DataFrame\"\n        }), \" 표시\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"(processedDF_with_severity)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png\",\n        alt: \"Real-time Stream Processing\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# 스트리밍 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"DataFrame\"\n        }), \"을 임시 뷰로 등록\\nprocessedDF_with_severity.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"createOrReplaceTempView\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"incident_reports\\\"\"\n        }), \")\\n\\n# 집계를 위한 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"SQL\"\n        }), \" 쿼리 정의\\ntotal_incidents_query = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n    SELECT\\n        incident_type,\\n        COUNT(*) AS total_incidents\\n    FROM\\n        incident_reports\\n    GROUP BY\\n        incident_type\\n\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"\\n\\nseverity_incidents_query = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n    SELECT\\n        incident_type,\\n        severity,\\n        COUNT(*) AS severity_incidents\\n    FROM\\n        incident_reports\\n    GROUP BY\\n        incident_type, severity\\n\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"\\n\\n# 집계 수행\\ntotal_incidents_df = spark.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sql\"\n        }), \"(total_incidents_query)\\nseverity_incidents_df = spark.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sql\"\n        }), \"(severity_incidents_query)\\n\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png\",\n        alt: \"Data Engineering Concepts\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Spark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Delta\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Lake\"\n        }), \"에 스트리밍 데이터를 저장할 경로 정의\\ndelta_table_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"`result_delta_table`\\\"\"\n        }), \"\\n\\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\\ncheckpoint_location = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/FileStore/tables/checkpoints\\\"\"\n        }), \"\\n\\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\\nstreaming_query = processedDF_with_severity.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"writeStream\"\n        }), \"\\\\\\n  .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"outputMode\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"append\\\"\"\n        }), \")\\\\\\n  .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"option\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"checkpointLocation\\\"\"\n        }), \", checkpoint_location)\\\\\\n  .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"trigger\"\n        }), \"(processingTime=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'10 seconds'\"\n        }), \")\\\\ # \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"초ごとに 데이터의 미크로배치를 처리할 트리거 정의\\n  .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"format\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"delta\\\"\"\n        }), \")\\\\\\n  .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"toTable\"\n        }), \"(delta_table_path)\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png\",\n        alt: \"Delta Table as Dataframe\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png\",\n        alt: \"Query Delta Table with SQL\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}