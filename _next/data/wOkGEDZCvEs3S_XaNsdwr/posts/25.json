{"pageProps":{"posts":[{"title":"카프카 클러스터를 쿠버네티스로 이관하는 방법 매끄러운 전환 가이드","description":"","date":"2024-06-23 01:01","slug":"2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes","content":"\n# 배경\n\n저희는 Zendesk에서 자체 Kafka 인프라를 관리합니다. 처음에는 시장에 세련된 관리형 Kafka 서비스가 없었습니다. 그래서 우리는 Chef를 통해 Kafka 인프라를 구축하고 AWS EC2 인스턴스에 배포했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n카프카 클러스터는 두 부분으로 구성됩니다: 카프카 브로커 집합과 주키퍼 노드입니다. 브로커는 카프카 클러스터의 서버로 데이터를 저장하고 관리하며 메시지의 발행과 구독을 처리합니다. 주키퍼 클러스터는 카프카 브로커를 조정하고 관리하는 쿼럼으로, 리더 선출, 클러스터 구성원 관리 및 메타데이터 유지와 같은 작업을 처리합니다.\n\n저희는 VM 기반 인프라에서 Kubernetes (K8s) 기반으로 이동 중입니다. Chef 스택의 지원이 줄어들어 카프카 인프라를 마이그레이션해야 했습니다.\n\n## 원활한 마이그레이션 정의\n\n카프카 클러스터의 원활한 마이그레이션은 데이터 무결성과 일관성을 유지하면서 투명하고 다운 타임이 없는 프로세스를 포함하며, 클라이언트의 최소한의 변경과 수동 개입이 필요한 최소한의 변화를 필요로 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이전 클러스터에서 구성 및 데이터를 복제하고, 클라이언트를 새 클러스터로 전환하며, 운영 연속성과 성능 동등성을 보장하는 것을 포함합니다.\n\n원활한 마이그레이션의 복잡성은 클러스터 규모와 클라이언트 수와 직접적인 관련이 있습니다. 사용 빈도가 낮거나 주로 오프라인 워크로드를 처리하는 클러스터를 마이그레이션하는 것은 비교적 간단할 수 있습니다. 그러나 대규모 실시간 워크로드를 처리하고 상당한 데이터 양을 다루는 클러스터를 마이그레이션하는 것은 훨씬 더 어려운 프로젝트입니다.\n\nZendesk에서는 우리의 Kafka 클러스터가 두 번째 범주에 속합니다. Kafka 클러스터는 상당한 양의 데이터를 관리하며 100개 이상의 서비스에서 사용됩니다.\n\nKafka 클러스터 상태 자세히:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 12개의 Kafka 클러스터\n- 모든 환경을 통틀어 약 100개의 브로커\n- 300TB의 데이터 저장 용량\n- 하루에 300억 건의 메시지\n- 1000개 이상의 토픽\n- 연결된 서비스가 100개 이상\n\n## 주요 도전 과제\n\n이러한 대규모이자 중요한 클러스터를 이주하는 것은 신중히 설계된 디자인이 필요합니다. 주의를 요하는 다수의 세부 사항 중에서, 주요 대응해야 할 도전 과제는 다음과 같습니다:\n\n- 복제 단계 중 데이터 무결성과 클러스터 가용성 보장,\n  클라이언트들에게 원활한 전환 기회를 제공하기 위한 전환 단계 스핑크 요소입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터가 완전히 복제된 후 새 클러스터로 고객이 대규모로 전환될 수 있는 구체적인 전환 시점이 없다는 것을 이해하는 것이 중요합니다. 원활한 이주 요구 사항으로 인해 전환 단계는 지속적인 데이터 복제와 클라이언트 전환을 동시에 수용해야 합니다.\n\n# 이주 디자인\n\n## 일방향 클러스터 간 이주\n\n첫 번째 버전 디자인은 단방향 클러스터 간 이주입니다. 이 방법은 MirrorMaker2를 사용하여 이전 클러스터의 데이터 및 구성을 새 클러스터로 복제합니다. MirrorMaker가 클러스터 간의 초기 데이터 로드를 전송한 후에도 데이터를 동기화 상태로 유지하며, 각 클라이언트가 이전 클러스터와 연결을 끊고 새 클러스터에 연결하는 전환 프로세스를 실행합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_2.png)\n\n문제: 마이그레이션 복잡성이 클라이언트로 이동됩니다.\n\n이 접근 방식은 처음에는 간단하고 견고해 보이지만, Kafka 클라이언트를 관리하는 각 팀에게 마이그레이션의 복잡성을 이동시킵니다. 이는 모든 클라이언트가 자체 커트오버 전략을 개발하고 실행해야 하며, 구현에 따라 복수의 배포가 필요할 수 있습니다.\n\n예를 들어, 단일팀이 한 주제의 프로듀서와 컨슈머를 모두 제어하는 가장 간단한 시나리오에서, 클라이언트는 코드 변경과 다수의 배포를 통해 다음 단계를 거쳐 마이그레이션될 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 모든 소비자를 새 클러스터로 이동합니다.\n- 이전 클러스터로의 생성을 중지합니다.\n- 데이터가 새 클러스터에 완전히 동기화될 때까지 기다립니다.\n- 새 클러스터를 가리키는 생산자를 시작합니다.\n\n주제의 여러 팀이 서로 다른 소비자 그룹을 제어하는 더 복잡한 시나리오에서는 모든 팀이 참여하는 조정된 마이그레이션이 필요합니다. 게다가 소비자가 또 다른 주제의 생성자 역할도 한다면 복잡성이 증가하여 복잡하고 다단계의 마이그레이션 과정으로 이어질 수 있습니다.\n\n부수적인 오버헤드\n\n특정 마이그레이션 디자인에는 몇 가지 부수적인 오버헤드가 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단일 장애 지점: MirrorMaker2 연결 클러스터가 단일 장애 지점입니다. 모든 주제의 소비자가 새 클러스터로 성공적으로 마이그레이션되어 있지만 MirrorMaker2의 실패로 인해 이전 클러스터에서 남은 실시간 데이터 동기화에 실패하는 시나리오를 상상해보세요.\n- 장애 감지 및 장애 조치 작업의 간접성: MirrorMaker2의 실패는 각 팀의 통제 범위를 벗어나지만 클라이언트 장애 조치 프로세스는 완전히 그들에게 의존적이며 이전 버전의 코드 변경이나 재배포를 필요로 합니다.\n- 긴 꼬리 문제: 마이그레이션의 완전한 완료는 가장 느린 클라이언트의 속도에 따라 달려있습니다. 앞서 언급된 복잡성으로 인해 일부 클라이언트는 마이그레이션을 완료하는 데 상당히 오랜 기간이 걸릴 수 있으며 이는 전체 프로젝트 전달 속도를 늦춥니다.\n- 비용: 이 단계에서 브로커에 대한 배로된 컴퓨팅 및 저장소 비용뿐만 아니라 세 배로 증가한 데이터 전송 비용에 직면합니다. 즉, 이전 브로커에서 연결 클러스터로 데이터를 이동하고 새로운 브로커로 다시 옮기는 비용입니다.\n\n## 클러스터 간 마이그레이션은 가능하지만, 더 나은 결과를 추구합니다\n\n이 접근 방식은 클라이언트 관점에서의 원활한 마이그레이션이 아니며 운영 및 비용 대비로 이 규모에서 실용적인 구현이 아닙니다.\n\n## 통합된 Kafka 클러스터 내 브로커 수준의 마이그레이션\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n클러스터 간 이동의 주요 도전 과제는 어느 시점에서든 클라이언트가 다른 클러스터를 가리켜야 한다는 것입니다. 데이터 무결성을 보장하기 위해 커트오버의 타이밍은 각각의 소비자와 생산자에 따라 다릅니다.\n\n이 문제를 해결하기 위해 더 강렬한 질문을 해보는 것은 유용할 수 있습니다: 클라이언트가 클러스터 전환에 대해 무관심할 수 있을까요?\n\n이 기사에서 언급한 대로, 클러스터 이전은 이전 클러스터에서 설정 및 데이터를 새 클러스터로 복제하고 클라이언트를 전환하는 것을 포함합니다.\n\n하지만, 만약 우리가 방정식에서 클라이언트 전환이 필요 없게 한다면 어떻게 될까요? 새로운 클러스터를 구축하는 대신, K8s에서 새로운 브로커 그룹을 구성하고 기존 클러스터에 등록할 수 있습니다. 이렇게 하면 클라이언트가 클러스터를 변경할 필요가 없어집니다. 유일하게 남은 작업은 구체화된 Kafka 클러스터 내에서 이전 브로커에서 새 브로커로 데이터를 전송하는 것뿐입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그렇습니다! 새로운 중개업체가 기존 클러스터에 참여할 예정이기 때문에 구성 이관이 필요하지 않습니다. 필요한 구성은 이미 클러스터 내에 준비되어 있어 프로세스가 간소화됩니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_3.png)\n\n## 중개업체 간 데이터 전송이 클라이언트 스위치오버를 어떻게 달성하나요?\n\n이 프로세스는 카프카 클라이언트 상호작용 메커니즘에 의해 이뤄집니다. 카프카 주제는 일련의 파티션으로 구성되며, 각 파티션에는 여러 개의 레플리카가 있을 수 있습니다. 대부분의 경우, 클라이언트는 리더 레플리카와 상호작용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 브로커에서 다른 브로커로 토픽 데이터를 이동할 때는 기본적으로 한 브로커에서 다른 브로커로 복제본을 전송하는 과정을 포함합니다. 한번 복제본이 완전히 이전되고 리더 복제본으로 선출되면, 클라이언트는 이 변경 사항을 감지하고 새 리더 복제본과 상호 작용하기 시작합니다. 결과적으로, 새로운 브로커와 상호 작용을 시작합니다.\n\n아래 그래프에 설명된 프로세스를 따라, 모든 복제본이 새 브로커에 저장되면 모든 클라이언트가 새 브로커와 상호 작용하며, 이전 브로커는 더 이상 중요한 작업을 수행하지 않는다는 것을 의미합니다.\n\n이 접근 방식이 어떻게 작동하는지 구체적으로 설명해 봅니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단계 1: 동일한 수의 브로커를 K8s에 배포하고 ZooKeeper 하에 동일한 Kafka 클러스터에 등록합니다. 이 단계에서 EC2와 K8s의 브로커는 동일한 클러스터의 멤버입니다. 그러나 EC2 브로커는 여전히 모든 데이터를 보유하고 모든 클라이언트 트래픽을 처리합니다. K8s 브로커는 ZooKeeper와의 하트비트를 유지하는 것 외에는 비활성화 상태입니다.\n- 단계 2: 토픽 파티션 레플리카가 EC2의 브로커에서 K8s로 마이그레이션을 시작합니다. 한 레플리카가 완전히 이동되면 K8s 브로커는 리더 레플리카를 보유하고 있는지에 따라 실제 클라이언트 토픽을 제공하기 시작할 수 있습니다.\n- 단계 3: 모든 토픽 파티션 레플리카가 EC2의 브로커에서 K8s의 브로커로 이전되었습니다. EC2의 브로커는 더 이상 데이터를 보유하지 않고 클라이언트 요청을 처리하지 않습니다.\n- 단계 4: EC2의 브로커를 축소하여 클러스터를 완전히 K8s 브로커로 구성합니다. 이제 마이그레이션이 완료되었습니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_5.png)\n\n위에 언급된 마이그레이션 단계를 따르면 클라이언트 전환이 필요 없이 성공적으로 진행됩니다. 클라이언트는 마이그레이션 전체 과정 동안 일관되게 한 클러스터와 상호 작용하므로, 클라이언트 관점에서 마이그레이션이 완전히 원활해집니다.\n\n# 심층 분석\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 전반적인 개념을 알게 되었으니 구체적인 설정에 대해 다루어 봅시다:\n\n- 네트워크 설정: 중요한 점은 두 가지 다른 브로커 세트를 그룹화하고, 이 하이브리드 카프카 클러스터에서 브로커 검색을 용이하게 하는 것입니다.\n- 이주 구현 및 조정: 새 브로커로 레플리카를 이동하는 구현이 중요하지만, 조정은 더욱 중요합니다. 효율적인 조정은 롤백과 같은 재해 복구 시나리오를 고려해야 합니다.\n- 메트릭과 모니터링: 하이브리드 클러스터의 모니터링을 효과적으로 수행하는 것은 EC2 및 K8s에 있는 브로커가 트래픽을 처리할 때 문제 감지, 디버깅 및 성능 세밀 조정에 중요합니다.\n- 브로커 중단 관리: 고가용성을 위해 서로 다른 가용 영역(AZs)에 있는 여러 브로커가 중단되지 않도록 롤링 브로커를 설정하거나 이주 중에도 방지해야 합니다.\n\n## 브로커 간 및 클라이언트-브로커 통신을 위한 네트워크 설정\n\n브로커 수준의 이주에 대한 중요한 전제 조건은 두 세트의 브로커와 주키퍼 클러스터가 서로에게 비교적 낮은 지연 시간으로 통신할 수 있어야 한다는 것입니다. 게다가 클라이언트는 두 세트의 브로커를 발견하고 낮은 지연 시간으로 통신할 수 있어야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리의 네트워크 아키텍처로 첫 번째 문제가 해결되었습니다. EC2 상의 브로커, Zookeeper 클러스터 및 K8s 클러스터는 서로 다른 서브넷에 배치되었지만, 동일한 AWS 계정 내에 있으며 서브넷은 서로 연결되어 있습니다.\n\n클라이언트가 EC2 및 K8s에 있는 브로커를 발견할 수 있도록 하려면 약간의 작업이 필요합니다. 무엇을 해야 하는지 이해하려면 먼저 클라이언트가 브로커를 어떻게 발견하는지를 파헤쳐야 합니다.\n\n클라이언트-브로커 발견 메커니즘\n\nKafka 클라이언트는 대상 토픽 아래에 있는 파티션의 수와 해당 파티션 복제본이 어떻게 분산되었는지에 따라 올바르게 작동하기 위해 거의 모든 브로커를 알아야 합니다. 전형적인 접근 방식:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 고객은 부트스트랩 브로커 중 하나를 선택하여 어떤 브로커가 어떤 파티션을 담당하는지의 메타데이터를 가져옵니다.\n- 고객은 이러한 브로커들과 연결을 설정하도록 요청합니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_6.png)\n\n따라서 EC2의 브로커에 대해 정의된 리버스 프록시에 추가로, K8s의 브로커에 대한 리버스 프록시도 클라이언트에게 제공되어야 합니다. 모든 브로커가 동일한 클러스터 내에 있으므로, EC2 또는 K8s에 있더라도 어떤 브로커든 클라이언트에게 전체 메타데이터를 제공할 수 있을 것입니다.\n\n![이미지](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_7.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nHashiCorp의 Consul 서비스는 서비스 검색에 사용됩니다. 새로운 리버스 프록시 서비스의 IP 주소를 기존의 Kafka Consul 서비스에 등록함으로써, 클라이언트는 두 세트의 브로커를 모두 발견할 수 있습니다. K8s에서 각 브로커의 홍보 리스너와 해당 홍보 리스너의 리버스 프록시는 모두 K8s 서비스가 될 수 있습니다.\n\n## 이주 실행 및 오케스트레이션\n\n지금까지 잘 진행되고 있습니다. EC2 또는 K8s에서 상호 연결된 브로커로 Kafka 클러스터를 구성하는 것이 가능합니다. 클라이언트는 모든 브로커에 연결하여 발견할 수 있습니다. 해결해야 할 다음 질문은 복제본이 어떻게 다른 브로커로 옮겨질 수 있는지, 그 방법은 무엇인지 입니다.\n\n복제본 이동의 실행은 Cruise Control에 의해 가능합니다. 이는 대규모로 Kafka 클러스터를 관리하기 위한 풍부한 API를 제공합니다. 복제본을 이동하기 위해 사용할 수 있는 엔드포인트 중 하나는 remove_broker 입니다. 이 엔드포인트는 모든 해당 브로커의 복제본을 대상 브로커로 이동시켜 브로커를 비활성화합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ncurl -X POST \"$CRUISE_CONTROL_SERVICE/remove_broker?brokerid=11&concurrent_partition_movements_per_broker=25&destination_broker_ids=31&replication_throttle=100000000\"\n```\n\n여기에서 우리가 필요한 것이 바로 이거에요! 위 요청은 Cruise Control에게 모든 레플리카를 브로커-11에서 브로커-31로 이동하도록 요청합니다. 동시에 한 번에 이동 가능한 레플리카 수는 최대 25개이며, 총 복제 쓰로틀 속도는 약 100Mb/s로 설정되어 있습니다.\n\n## Orchestration\n\n크루즈 컨트롤 엔드포인트를 수동으로 호출하는 것은 이상적이지 않습니다. 안전하거나 효율적이지 않기 때문이죠. 그래서 우리는 이 상황을 위해 Kafka-Migration이라는 오케스트레이션 시스템을 구축했습니다. 이 시스템은 다음과 같은 역할을 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 마이그레이션 조건 확인: 예를 들어, 복제 이동 호출을 실행하기 전에 클러스터가 건강한 상태인지, 대상 및 목적지 브로커가 동일한 가용 영역(AZ)에 있는지 확인합니다.\n- 다양한 마이그레이션 모드: 시스템에는 다양한 마이그레이션 모드가 있습니다. '특정 브로커' 모드는 특정 브로커 세트에서 데이터를 다른 브로커로 이동할 수 있습니다. 'AZ' 모드는 기존 브로커에서 목적지 브로커로 데이터를 이동하는 것을 용이하게 합니다. '완전한 마이그레이션' 모드는 새로운 브로커 세트로의 완전한 데이터 이동을 관리하며 올바른 복제 할당을 보장하고 AZ별로 이동을 조정합니다. 이러한 모드와 규칙을 설정하여 복제 진행이 안전하고 관리 가능한 범위 내에 있음을 보장합니다. 결과적으로 영향을 최소화하고 이동 중 발생하는 사건에 신속히 대응할 수 있습니다.\n- 세밀한 마이그레이션 제어: 단방향 마이그레이션 뿐만 아니라 중지, 재개 및 되돌리기 작업도 지원합니다. 예를 들어, 트래픽 급증으로 인해 마이그레이션을 일시 중지하여 운영 트래픽에 더 많은 대역폭을 제공한 다음 트래픽이 다시 감소하면 마이그레이션을 재개할 수 있습니다.\n\n![링크 없음](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_8.png)\n\n## 메트릭 및 모니터링\n\n메트릭을 수집하고 상단에 모니터를 구축하는 것은 문제 감지와 클러스터가 마이그레이션 중에 어떻게 수행되는지에 대한 통찰을 얻는 데 중요합니다. Kafka에 대해 수집해야 할 세 가지 다른 메트릭 소스가 있습니다: Kafka 브로커 JMX 메트릭, 브로커 시스템 수준 메트릭 및 AWS EBS 메트릭입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJMX 및 EBS 메트릭은 대체로 EC2 및 K8s 전반에 걸쳐 크게 일관성이 있어서, 마이그레이션 중에 본질적으로 변경되지 않습니다. EC2와 K8s 브로커의 메트릭에서 유일한 차이점은 태그에 있습니다. 이 태그를 기존 모니터 시스템으로 다시 매핑하는 것은 간단합니다.\n\n브로커 시스템 레벨 메트릭은 조금 흥미로운데, EC2는 가상 머신이지만 K8s 파드는 여러 개의 컨테이너 그룹입니다. VM 기반 메트릭은 때로 컨테이너 기반 메트릭에서 찾을 수 없는 경우가 있습니다. 예를 들어 호스트 CPU의 작업 부하를 시간별로 측정하고 시스템이 얼마나 바쁜지를 나타내는 로드 평균 메트릭은 컨테이너 수준에 직접적인 동등물이 없습니다. 아마도 CPU 쓰로틀링 비율 메트릭을 가장 유사한 대리자로 사용할 수 있지만 완전히 같지는 않습니다.\n\n문제 탐지를 위해 고수준 메트릭에 의존하는 것이 시간별로 저수준 메트릭을 살펴보는 것보다 좋습니다. 사실, 이러한 고수준 Kafka 브로커 JMX 메트릭은 리소스 고갈 문제의 좋은 지표를 제공할 수 있습니다: 프로듀서 지연 시간, 네트워크 풀 용량, 요청 처리기 용량 및 복제 지연 시간.\n\n## 브로커 중단 관리\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마이그레이션 중에는 브로커 장애를 주의 깊게 관리해야 합니다. 브로커 장애란 브로커가 정상적으로 작동하지 않거나 사용 불가능해지는 문제나 사건을 의미합니다. 이는 강제적인(예: 하드웨어 고장으로 인한) 또는 자발적인(예: 브로커 롤링 재시작으로 인한) 장애일 수 있습니다.\n\n잘못된 장애 관리는 가용성이 낮아지게 됩니다.\n\n3개의 가용 영역(AZ)에 걸쳐 있는 브로커로 이루어진 Kafka 클러스터의 경우, 대부분의 토픽이 3개의 복제본과 최소 2개의 인-싱크 복제본으로 구성되어 있습니다. 한 가용 영역에서 일부 브로커를 잃는 것은 관리할 수 있는 일입니다. 그러나 서로 다른 두 가용 영역에서 브로커 두 대를 잃는 것은 문제가 발생할 수 있습니다. 왜냐하면 최소 인-싱크 복제본이 2개로 설정되어 있으면 \"오프라인 파티션\" 문제가 발생합니다.\n\n이러한 시나리오가 발생하지 않도록 예방하는 시스템을 만들어야 합니다. 강제적인 장애는 피할 수 없는 것이지만 강제적인 장애 발생 시 자발적인 장애를 최소화할 수 있도록 개선할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n커파 모니터링 서비스를 이용한 Pod 중단 예산\n\nK8s에서 Pod 중단 예산(PDB)은 K8s에 배포된 브로커 Pod를 모니터링하기 위해 설정됩니다. PDB는 오프라인 브로커를 감지하면 해당 중단 예산을 깨어 K8s의 다른 브로커가 내쫓히는 것을 방지합니다.\n\n그러나 PDB로는 EC2 인스턴스의 브로커 상태를 감지할 수 있는 방법이 없습니다. 우리는 Kafka 모니터링 서비스를 추가로 설정하여 EC2의 브로커 상태를 모니터링 범위에 포함시켰습니다. 이 서비스는 Under Replicated Partitions (URP)를 확인하여 카프카 클러스터 전반의 상태를 감시합니다. URP가 감지되면 서비스는 준비 상태로 전환됩니다.\n\nPDB는 Kafka 모니터링 서비스와 K8s에 배포된 브로커 모두를 모니터링하도록 구성되어 있습니다. 이로써 브로커 Pod를 직접 감시하여 K8s의 오프라인 브로커뿐만 아니라 Kafka 모니터링 서비스를 통해 EC2의 오프라인 브로커도 감지할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Image 1](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_9.png)\n\n# Zookeeper Migration\n\n카프카 브로커를 EC2 인스턴스에서 K8s로 성공적으로 이전했습니다. 그러나 Zookeeper 클러스터는 여전히 EC2 인스턴스에서 실행 중입니다.\n\n![Image 2](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_10.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n물론, Zookeeper 클러스터를 Kafka 브로커를 마이그레이션하는 방식과 유사하게 마이그레이션할 수 있습니다. 그러나 더 좋은 아이디어가 있습니다! Kafka 버전 3.6에서 Kraft가 드디어 General Availability에 도달했는데, 이는 Kraft로 구동된 컨트롤러 역할을 하는 브로커를 사용하여 Zookeeper를 대체할 수 있음을 의미합니다. 이미 브로커가 K8s에서 실행 중이기 때문에, 컨트롤러 역할을 하는 새로운 브로커 집합을 쉽게 설정할 수 있습니다.\n\n![Kubernetes를 통한 Kafka 클러스터 마이그레이션](/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_11.png)\n\n# 요약\n\n통합된 Kafka 클러스터 내에서 브로커 수준의 마이그레이션 전략을 적용하여, 우리는 모든 Kafka 클러스터를 K8s로 성공적으로 마이그레이션했습니다. 마이그레이션 프로세스는 클라이언트 관점에서 완전히 원활하며, 마이그레이션으로 인한 사건은 전혀 발생하지 않았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nK8s에서 Kafka를 관리하는 것은 EC2보다 훨씬 쉽습니다. K8s가 제공하는 강력하고 확장 가능하며 유연한 플랫폼 덕분에 배포부터 모니터링, 확장, 그리고 자가 치유까지 많은 운영 작업이 자동화됩니다. 잘 정의된 K8s 객체와 Custom Resource Definition(CRD)을 통해 우리는 Kafka 주변에 오퍼레이터를 구축하여 운영 작업을 더욱 자동화할 수 있습니다.\n\n개선된 자동화와 감소된 운영 부담은 우리에게 Kafka 생태계를 더 빠른 속도로 발전시킬 수 있는 기회를 제공할 뿐만 아니라, 팀의 개발 시간을 새로운 개발 작업에 집중할 수 있도록 해줍니다.\n","ogImage":{"url":"/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_0.png"},"coverImage":"/assets/img/2024-06-23-SeamlessTransitionMigratingKafkaClustertoKubernetes_0.png","tag":["Tech"],"readingTime":18},{"title":"Challenge 14 바운스 방법 이해 및 구현","description":"","date":"2024-06-23 00:59","slug":"2024-06-23-Challenge14bounce","content":"\n쿠버네티스 서비스: 생각했던 대로 항상 작동하지 않을 수 있어요.\n\n쿠버네티스에서는 서비스가 존재하지 않아요: IP 주소나 포트를 수신하는 프로세스가 없어요. 대신, 쿠버네티스는 고급 네트워크 규칙을 사용하여 로드 밸런서를 모방해요.\n\n기본적으로, 이러한 네트워크 규칙은 Kube-proxy가 관리하는 iptables 규칙을 사용해요.\n\n하지만, Cilium으로 작성된 eBPF를 사용하는 대안이 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"왜 \"표준\" 로드 밸런서를 사용하지 않는 거죠?\n\n이것은 주로 단일 고장 지점의 문제를 극복하기 위해 설계되었습니다. 각 노드에 규칙이 기입되어 있기 때문에, 단일 구성 요소의 고장이 전체 클러스터에 영향을 미칠 가능성이 적습니다.\n\n그러나 이것은 흥미로운(그리고 예상치 못한) 결과를 초래할 수 있습니다.\n\n세 개의 노드와 두 개의 팟이 있는 다음 클러스터를 고려해보세요.\"\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Challenge14bounce_0.png\" />\n\n외부 트래픽에 파드를 노출하는 NodePort 서비스가 있습니다.\n\nNodePort로 요청을 보내면, 해당 트래픽이 동일 노드의 파드 2에 도착할 확률은 얼마인가요?\n\n- 트래픽은 항상 파드 2에 도착합니다\n- 트래픽이 파드 2에 도착할 확률은 50% 입니다\n- 트래픽이 파드 2에 도착할 확률은 33% 입니다\n- 위의 어느 것도 아닙니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 해결책\n\n왜 그런지 이해하려면 Kube-proxy가 노드에 전달 규칙을 설정하는 방식을 살펴봐야 합니다 (예: iptables, ipvs, eBPF 등).\n\nNodePort를 생성하면 클러스터의 각 노드가 30000~32767 범위의 포트를 노출합니다. 30000번 포트로 가정해 보겠습니다.\n\n`노드 IP`:30000으로 curl 요청을 발행하면 무슨 일이 일어날까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nIP 헤더의 대상 IP 주소는 Kubernetes에 서비스가 존재하지 않기 때문에 다시 작성됩니다(그리고 그들은 트래픽을 리디렉션하고 전달하는 규칙의 모음일 뿐입니다).\n\n두 개의 파드 중 하나가 대상으로 선택되고, 트래픽이 해당 팟에 도달할 수 있습니다.\n\n그래서 각 팟은 트래픽을 받을 확률이 50%입니다.\n\n이는 클러스터에 있는 어느 노드에서 클러스터를 받았는지에 관계없이 발생합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만, 토론할 가치가 있는 두 가지 흥미로운 경계 사례가 있습니다:\n\n- 팟이 없는 노드를 만나면 여전히 트래픽이 팟 1 또는 팟 2로 리디렉션됩니다.\n- 팟 2를 포함한 두 번째 노드를 만나면 트래픽이 여전히 다른 팟으로 이동할 수 있습니다.\n\n이 두 경우 모두 현재 노드는 트래픽을 처리하지 않으며, 추가적인 홉이 발생합니다.\n\n이것이 낭비라고 생각할 수 있지만, 이는 타협점입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n특정 테이블 태그를 마크다운 형식으로 변경하시면 추가 hop이 발생하여 포드 간에 우수한 인트라 클러스터 로드 밸런싱을 할 수 있거나 항상 서비스를 구성하여 현재 노드의 포드로 트래픽을 전달하도록 설정할 수 있습니다.\n\n대기시간이 향상되지만 요청을 부분적으로 로드 밸런싱할 수 있기 때문에 다른 포드보다 연결 수가 더 많은 포드를 얻을 수도 있습니다.\n\n이 내용이 마음에 드셨다면 아래 내용도 좋아하실 수 있습니다:\n\n- Learnk8s에서 운영하는 Kubernetes 강좌.\n- 매주 발행하는 Learn Kubernetes Weekly 뉴스레터.\n- 20주 동안 게시한 20가지 Kubernetes 개념 시리즈.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 링크\n\n리소스 할당에 대해 더 자세히 알고 싶다면, 다음 자료를 꼭 확인해보세요:\n\n- [kubernetes service에 핑을 보낼 수 없는 이유](https://medium.com/@danielepolencic/learn-why-you-cant-ping-a-kubernetes-service-dec88b55e1a3)\n- [kubernetes 네트워크 패킷](https://learnk8s.io/kubernetes-network-packets)\n","ogImage":{"url":"/assets/img/2024-06-23-Challenge14bounce_0.png"},"coverImage":"/assets/img/2024-06-23-Challenge14bounce_0.png","tag":["Tech"],"readingTime":4},{"title":"프로젝트 8  Kubernetes에서 삼계층 애플리케이션 배포 하는 방법","description":"","date":"2024-06-23 00:57","slug":"2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes","content":"\n우리 시리즈의 8번째 프로젝트입니다. 계속 따라오세요!\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*ZBUzOwaMkbfPD3iZXxVH1g.gif)\n\n## 3Tier란 무엇인가요?\n\n사실은 시스템을 3부분으로 나누는 것을 말해요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 표현 계층 (티어 1):\n\n  - 웹 사이트를 열 때 보는 것이 바로 표현 계층입니다. 기본적으로 이 계층은 사용자가 직접 상호 작용하는 계층입니다.\n\n- 논리 계층 (티어 2):\n\n  - 이 계층을 뒷단의 두뇌로 상상해보세요. 사용자 인터페이스를 통해 제공하는 정보를 받아 시스템의 규칙에 따라 처리합니다. 예를 들어 쇼핑 웹 사이트의 경우, 이 계층은 물품의 총 가격을 계산하거나 할인을 적용하며 모든 제품이 재고에 있는지 확인합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 데이터 레이어 (티어 3):\n\n- 데이터가 저장되고 검색되는 곳입니다. 시스템의 메모리와 같습니다.\n- 데이터는 데이터베이스, 파일 또는 다른 데이터 저장 시스템에 저장될 수 있습니다.\n- 데이터 레이어는 시스템이 필요로 하는 정보를 관리하고 저장하는 역할을 합니다.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_0.png)\n\n# 완료 단계 →\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Phase 1\n\nEC2 인스턴스, IAM 사용자 및 EC2의 기본 도구 설정하기\n\n# Phase 2\n\n프론트엔드 및 백엔드 이미지 빌드하기\n\n# Phase 3\n\n쿠버네티스\n\n# Phase 4\n\n애플리케이션 로드 밸런서 및 인그레스 설정하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPhase 5 → 모든 것 파괴하기\n\n# Phase 1 → 기본 EC2, IAM 사용자 및 EC2에 기본 도구 설정\n\n## 단계 1. IAM 사용자 생성\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 'iam'을 클릭하세요.\n\n2. 사용자를 클릭하고 사용자 생성을 클릭하세요\n\n   ![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_2.png)\n\n3. 사용자에게 이름을 지정하고 관리 콘솔에 사용자 액세스 제공에 체크하고 IAM 사용자 옵션을 선택하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_3.png)\n\n5. 사용자의 암호를 선택하십시오 → 다음을 클릭하세요.\n\n6. IAM 사용자에 정책을 직접 첨부 → 다음을 클릭하세요.\n\n참고 → 현재는 관리자 액세스를 제공하겠지만 작업 공간에 정책을 첨부할 때 신중하시기 바랍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_4.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_5.png)\n\n사용자를 검토하고 만들기\n\n7. '사용자 만들기'를 클릭하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Project Screenshot 6](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_6.png)\n\n8. If the password file is autogenerated, download it. Otherwise, you can choose whether to download it or not.\n\n![Project Screenshot 7](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_7.png)\n\n9. Click on your IAM user → Security Credentials.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Project Screenshot 8](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_8.png)\n\n10. 스크롤하여 키에 액세스하고 액세스 키를 생성합니다.\n\n![Project Screenshot 9](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_9.png)\n\n11. 옵션 목록에서 aws cli를 선택하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_10.png\" />\n\n12. 다음을 클릭하고 사용자 이름과 암호의 CSV 파일을 다운로드하세요\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_11.png\" />\n\n## 단계 2. 모든 작업을 수행하는 기본 EC2를 시작합니다 →\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- AWS 콘솔을 열고 EC2로 이동한 다음 EC2 시작을 클릭하세요\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_12.png)\n\n2. 연결을 클릭하고 다음 명령을 실행하세요\n\n```bash\nsudo su\napt update\nmkdir 3-tier\ncd 3-tier\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 깃허브에서 코드를 가져오려면 git clone을 사용하세요\n\n```js\ngit clone https://github.com/Aakibgithuber/Three-tier-Application-Deployment-.git\n```\n\n4. ls를 실행하여 레포지토리 안에 무엇이 있는지 확인하세요\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_13.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 3 → aws cli, 도커, kubectl 및 eksctl 설정\n\n## 1. aws cli 구성\n\nAWS CLI (Amazon Web Services Command Line Interface)는 명령어를 사용하여 AWS 서비스와 상호 작용할 수 있는 도구입니다.\n\n- 아래 명령어를 실행하여 aws cli를 설치합니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 먼저 다음 명령어로 AWS CLI를 설치해 주세요.\n\nsnap install aws-cli --classic\n\n2. 이제 AWS를 구성해야 합니다. 아래 명령어를 사용해 구성해 주세요.\n\naws configure\n\n3. 접근 키와 비밀 키를 요청할 것입니다. 이제 앞서 다운로드한 CSV 파일을 열어서 액세스 키와 비밀 키를 복사해 주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_14.png\" />\n\n4. 모든 것을 그대로 유지하고 Enter 키를 누르세요.\n\n지금까지 AWS CLI를 설정하셨습니다. 이제 Docker를 설정하세요.\n\n## 2. Docker 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다음 명령어를 실행해주세요\n\n```js\napt install docker.io\nusermod -aG docker $USER # 사용자 이름으로 변경해주세요. 예: 'ubuntu'\nnewgrp docker\nsudo chmod 777 /var/run/docker.sock\nwhich docker\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_15.png)\n\n## 3. kubectl 설정하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 Kubernetes 클러스터를 관리하고 상호 작용하는 데 사용되는 명령 줄 도구입니다.\n\n- kubectl을 설치하려면 다음 명령을 실행하십시오.\n\n```js\nsnap install kubectl --classic\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_16.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4. eksctl 설정\n\n이것은 Amazon EKS (Elastic Kubernetes Service) 클러스터를 관리하는 데 사용되는 명령줄 도구입니다.\n\n- eksctl 도구를 설치하려면 다음 명령을 실행하십시오\n\n```js\ncurl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\neksctl version\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_17.png\" />\n\n# Phase 2 → 프론트엔드 및 백엔드 이미지 구축\n\n## 단계 1 → Elastic Container Registry (ECR) 설정\n\n도커 이미지를 저장하는 도커허브와 유사합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- AWS 콘솔로 이동해서 ECR을 검색해주세요.\n- 프론트엔드를 위한 저장소를 만들고 가시성 설정을 \"퍼블릭\"으로 설정해주세요.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_18.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_19.png)\n\n3. 백엔드 저장소 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_20.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_21.png)\n\n## 단계 2 → 프론트엔드 설정\n\n- 터미널에서 프론트엔드 디렉토리로 이동하고 ls 명령어를 실행하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_22.png)\n\n2. Amazon ECR 저장소로 이동하고 푸시 명령 보기를 클릭합니다\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_23.png)\n\n3. 위 명령을 하나씩 실행하여 프론트엔드 이미지를 빌드하고 ECR 저장소에 푸시합니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/l0l7e4u1\ndocker build -t 3-tier-frontend .\ndocker tag 3-tier-frontend:latest public.ecr.aws/l0l7e4u1/3-tier-frontend:latest\ndocker push public.ecr.aws/l0l7e4u1/3-tier-frontend:latest\n```\n\n![Project Image 24](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_24.png)\n\n![Project Image 25](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_25.png)\n\n4. 이미지에서 컨테이너를 실행해 보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 이미지 목록에서 이미지 이름을 복사하세요.\n도커 실행 -d -p 3000:3000 3-tier-frontend:latest\n```\n\n![이미지1](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_26.png)\n\n프론트엔드가 설정되었고 애플리케이션이 실행 중입니다. 애플리케이션을 확인하려면 브라우저에서 해당 주소로 이동하세요 →public-ip:3000\n\n![이미지2](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_27.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 3 → 백엔드 설정\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_28.png)\n\n- 이제 백엔드 디렉토리로 이동하여 백엔드를 설정하세요.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_29.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. ECR 리포지토리로 이동하여 백엔드 리포지토리의 푸시 명령을 보기 위해 클릭하세요\n\n![백엔드 리포지토리 푸시 명령](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_30.png)\n\n3. 위 명령을 단계별로 터미널에서 실행해주세요\n\n```js\naws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/l0l7e4u1\ndocker build -t 3-tier-backend .\ndocker tag 3-tier-backend:latest public.ecr.aws/l0l7e4u1/3-tier-backend:latest\ndocker push public.ecr.aws/l0l7e4u1/3-tier-backend:latest\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_31.png)\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_32.png)\n\n이제 백엔드 이미지가 성공적으로 빌드되었고, 우리가 탄력적인 쿠버네티스 서비스를 생성할 때 사용한 Elastic Container Registry에 푸시되었습니다.\n\n# Phase 3 Kubernetes\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 배포란 무엇인가요?\n\n- 공장을 상상해보세요: 배포는 여러분의 소프트웨어 응용 프로그램의 사본을 생성하고 관리하는 공장으로 생각해보세요.\n- 여러 복제본: 공장이 여러 개의 동일한 항목을 생성할 수 있는 것처럼, Kubernetes의 배포는 응용 프로그램의 여러 사본(복제본)을 생성하고 다룰 수 있습니다.\n- 간편한 업데이트: 응용 프로그램을 변경하거나 업데이트하려면, 배포 시스템이 그것을 원활하게 처리할 수 있습니다. 마치 공장에서 부품을 교체하면서 생산을 멈추지 않는 것처럼요.\n\n2. 서비스란 무엇인가요?\n\n- 수신 데스크를 상상해보세요: Kubernetes의 서비스를 건물의 수신 데스크처럼 생각해보세요.\n- 중앙 연락처: 서비스는 응용 프로그램을 위한 중앙 연락처를 제공합니다. 직접 각 응용 프로그램을 찾는 대신 시스템의 다른 부분들이 서비스와 대화하고, 서비스가 올바른 응용 프로그램을 찾는 방법을 알고 있습니다.\n- 안정적인 주소: 수신 데스크에 일정한 주소가 있는 것처럼, 서비스에도 시스템의 다른 부분들이 응용 프로그램과 통신하기 위해 사용할 수 있는 안정적인 주소가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. Namespace란 무엇인가요?\n\n- 이것은 쿠버네티스 내에서 당신이 어플리케이션을 조직하고 실행할 수 있는 레이블이 붙은 섹션과 같습니다. 각 네임스페이스는 당신의 어플리케이션이 서로 방해하지 않고 자신의 일을 할 수 있는 울타리 치여진 영역과 같습니다.\n- 좀 더 간단하게 말하면, 쿠버네티스에서의 네임스페이스는 다른 프로젝트나 어플리케이션을 분리하고 조직화하며, 쿠버네티스 클러스터의 분주한 환경에서 쉽게 관리할 수 있도록 하는 방법입니다.\n\n## Step 1 → EKS 클러스터 설정하고 네임스페이스 생성하기\n\n- 다음 명령어를 실행하여 EKS 클러스터를 설정하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\neksctl create cluster --name three-tier-cluster --region us-east-1 --node-type t2.medium --nodes-min 2 --nodes-max 2\naws eks update-kubeconfig --region us-east-1 --name three-tier-cluster\nkubectl get nodes\n```\n\n2. 클러스터를 생성하는 데 15~20분이 소요됩니다.\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_33.png)\n\n3. AWS 콘솔에서 AWS CloudFormation을 검색하여 EKS 클러스터 생성 중 발생하는 이벤트를 확인하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 명령어를 사용하여 Namesapce를 생성하세요.\n\n```shell\nkubectl create namespace workshop\nkubectl config set-context --current --namespace workshop\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_36.png\" />\n\n## 단계 2→ 프론트엔드를 위한 배포 및 서비스 생성하기\n\n- k8s_manifests 디렉토리로 이동하면 프론트엔드를 위한 배포 및 서비스 파일을 찾을 수 있습니다.\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_37.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. frontend-deployment.yaml 파일을 편집해야 합니다.\n\n3. 바꿔야 할 것은 이미지 이름입니다.\n\n![image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_38.png)\n\n4. 따라서 ECR 레포지토리로 이동하여 프론트엔드 레포지토리를 선택한 후 \"View public listing\"을 클릭하여 이미지 이름을 복사하고 frontend-deployment.yaml 파일에 붙여넣으세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 다음 명령어를 실행하여 프론트엔드용 배포 및 서비스를 생성하세요.\n\n```js\nkubectl apply -f frontend-deployment.yaml\nkubectl apply -f frontend-service.yaml\n```\n\n## 단계 3→ 백엔드용 배포 및 서비스 생성\n\n- 동일한 폴더에 백엔드 배포 및 서비스인 backend-deployment.yaml 및 backend-service.yaml 파일이 있습니다.\n- backend-deployment.yaml 파일을 편집해야 합니다.\n- 변경해야 할 것은 이미지 이름뿐입니다.\n- 따라서 ECR 저장소로 이동하여 백엔드 저장소를 선택한 다음, \"View Public Listing\"을 클릭하여 이미지 이름을 복사한 후 backend-deployment.yaml 파일 안에 붙여넣으세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 다음 명령을 실행하여 배포 및 백엔드 서비스를 생성하십시오.\n\n```js\nkubectl apply -f backend-deployment.yaml\nkubectl apply -f backend-service.yaml\nkubectl get pods -n workshop\n```\n\n이제 우리의 2계층이 준비되었습니다. 즉, 프론트엔드와 백엔드가 설정되었습니다. 이제 세 번째 계층을 설정해 봅시다.\n\n## 단계 4 → 데이터베이스 계층 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 배포, 서비스 및 시크릿 매니페스트를 저장하는 몽고 폴더를 찾아주세요.\n\n2. 아래 명령어를 실행하여 데이터베이스 티어를 설정하세요.\n\n```js\nkubectl apply -f .\nkubectl get all\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_39.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 모든 세 티어가 준비되었습니다. 하지만 이들을 어떻게 액세스할까요? 클러스터로 외부 트래픽을 전송하기 위해 애플리케이션 로드 밸런서를 생성해야 하고, 내부의 3개 티어 사이를 연결하기 위해 인그레스를 만들어야 합니다.\n\n# 단계 4 → 애플리케이션 로드 밸런서와 인그레스 설정\n\n외부 트래픽을 클러스터로 전송하기 위해 애플리케이션 로드 밸런서를 생성하고, 내부의 3개 티어 사이를 연결할 인그레스를 만들어야 합니다.\n\n## 단계 1 → AWS 로드 밸런서 설정; 설치 및 EKS 클러스터에 연결\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 아래 명령은 ALB의 IAM 정책을 가져옵니다.\n\n```js\ncurl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json\n```\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_40.png)\n\n2. 이 명령어는 첫 번째 명령어에서 설정한 `iam_policy.json` 파일을 사용하여 AWS 계정에서 IAM 정책을 생성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\naws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json\n```\n\n![Image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_41.png)\n\n3. 이 명령은 로드 밸런서 정책을 EKS 클러스터에 적용하여 정책에 따라 EKS 클러스터가 로드 밸런서와 함께 작동하도록 합니다.\n\n```js\neksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=three-tier-cluster --approve\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 명령어는 클러스터에 서비스 계정을 생성하고 추가하여 클러스터가 로드 밸런서 서비스와 작업할 수 있도록합니다.\n\n아래 명령어에서 AWS 계정 번호를 꼭 변경해야 합니다. 그렇지 않으면 작동하지 않습니다.\n\n```js\neksctl create iamserviceaccount --cluster=three-tier-cluster --namespace=kube-system --name=aws-load-balancer-controller --role-name AmazonEKSLoadBalancerControllerRole --attach-policy-arn=arn:aws:iam::767397866747:policy/AWSLoadBalancerControllerIAMPolicy --approve --region=us-east-1\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_43.png\" />\n\n## 모든 정책이 첨부되었습니다. 로드 밸런서를 배포해 봅시다.\n\n5. 이를 위해 helm을 설치해야 합니다. Helm은 쿠버네티스를 사용할 때 소프트웨어를 쉽게 운반하고 관리하는 데 도움이 되는 특별한 도구입니다. 쿠버네티스는 애플리케이션을 실행하기 위한 큰 놀이터와 같습니다.\n\n```js\nsudo snap install helm --classic\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n6. 이제 로드 밸런서를 위해 eks 리포지토리에서 미리 작성된 특정 manifest 파일을 추가해야 해요. helm을 사용하여 다음 명령을 실행해주세요.\n\n```js\nhelm repo add eks https://aws.github.io/eks-charts\n```\n\n7. helm을 사용하여 eks 리포지토리를 업데이트해주세요.\n\n```js\nhelm repo update eks\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n8. EKS 클러스터에 로드 밸런서 컨트롤러를 설치해주세요\n\n```js\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=my-cluster --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\nkubectl get deployment -n kube-system aws-load-balancer-controller\n```\n\n![Image](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_44.png)\n\n이제 로드 밸런서가 작동되니 내부 라우팅을 위해 인그레스를 설정합시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Step 2 → 내부 라우팅을 위한 Ingress 설정\n\n![이미지](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_45.png)\n\n- full_stack_lb.yaml 파일을 찾습니다\n\n```js\nkubectl apply -f full_stack_lb.yaml\nkubectl get ing -n workshop\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_46.png\" />\n\n2. 웹 브라우저로 이동하여 위의 DNS 주소를 붙여넣으세요\n\n<img src=\"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_47.png\" />\n\n축하합니다!! 여러분의 애플리케이션이 로드 밸런서 인그레스를 통해 접근 가능합니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 5 단계 → 모두 삭제하기\n\n- 현재 폴더에서 다음을 실행하세요\n\n```js\nkubectl delete -f .\n```\n\n2. 데이터베이스 계층을 삭제하려면 몽고 폴더로 이동하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\nkubectl delete -f .\n```\n\n3. 클러스터 및 클라우드 형성 스택을 삭제하세요.\n\n```yaml\neksctl delete cluster --name three-tier-cluster --region us-east-1\naws cloudformation delete-stack --stack-name eksctl-three-tier-cluster-cluster\n```\n\n4. AWS의 클라우드 형성 콘솔에서 모든 변경 사항을 확인할 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Project Screenshot](/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_48.png)\n\n## 지금 모든 것이 삭제되었으니 AWS 요금을 줄여줘서 고마워\n\n## 오늘은 여기까지 프로젝트가 완료되었어요. 만약 여기까지 오셨다면 박수를 치세요. 그리고 LinkedIn에서 저를 팔로우해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제 깃허브 계정을 팔로우해주세요!\n","ogImage":{"url":"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_0.png"},"coverImage":"/assets/img/2024-06-23-Project8ThreetierapplicationdeploymentonKubernetes_0.png","tag":["Tech"],"readingTime":28},{"title":"한정된 AI 자원의 최대 활용 Kubernetes 접근법","description":"","date":"2024-06-23 00:52","slug":"2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach","content":"\n## 한정된 AI 트레이닝 가속기의 최적 활용\n\n![image](/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_0.png)\n\nAI 개발의 끊임없는 변화 속에서 헤라클리투스에게 소속된 것으로 알려진 구단속, \"삶에서 유일한 상수는 변화다\"라는 말보다 더 진실한 것은 없는 것 같습니다. AI 분야에서는 변화가 단연 지속적이지만, 그 변화의 속도는 영원히 증가하고 있습니다. 이 독특하고 흥미진진한 시기에서 주목할 점은 AI 팀이 지속적으로 적응하고 개발 프로세스를 조정하는 능력을 비롯한 과제입니다. 적응하지 못하는 AI 개발팀이나 적응이 더딘 팀은 빠르게 구식이 될 수 있습니다.\n\n최근 몇 년간 AI 개발에서 가장 어려운 발전 중 하나는 AI 모델을 훈련하는 데 필요한 하드웨어를 확보하기가 점점 어려워지고 있다는 것입니다. 세계적인 공급망 위기나 AI 칩 수요의 상당한 증가 등으로 인해 AI 개발을 위해 필요한 GPU(또는 대체 트레이닝 가속기)를 구하는 것이 더 어려워졌습니다. 신규 GPU 주문에 대한 기다림 시간이 길어진 점이나 한 때 GPU 기계의 거의 무한한 용량을 제공했던 클라우드 서비스 제공업체(CSP)조차 수요를 따라가기 어려워진 점으로 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n변화하는 시대에 따라 AI 개발 팀들은 한때 AI 액셀러레이터의 무한한 용량에 의존했을 수도 있는데, 접근성이 감소하고 경우에 따라 더 높은 비용이 요구되는 세계에 적응해야 합니다. 새 GPU 머신을 언제든 생성할 수 있다는 능력을 당연하게 여겼던 개발 프로세스는 종종 여러 프로젝트 및/또는 팀에 의해 공유되는 희귀한 AI 리소스 세계의 요구를 충족시키기 위해 수정되어야 합니다. 적응하지 못하는 경우 소멸의 위험이 있습니다.\n\n이 게시물에서는 희귀한 AI 리소스 세계에서 AI 모델 훈련 워크로드를 조율하기 위해 Kubernetes의 사용 방법을 보여드릴 것입니다. 먼저 달성하고자 하는 목표를 명시할 것이며, 그 후에 왜 Kubernetes가 이 도전에 대응하는 적절한 도구인지 설명할 것입니다. 마지막으로 Kubernetes를 사용하여 희귀한 AI 계산 리소스의 사용을 극대화하는 방법을 간단히 보여줄 것입니다. 이어지는 게시물에서는 Kubernetes 기반 솔루션을 향상시키고 클라우드 기반 훈련 환경에 적용하는 방법을 보여줄 계획입니다.\n\n## 면책 조항\n\n본 게시물은 Kubernetes에 대한 사전 지식을 요구하지는 않지만, 기본적인 이해가 도움이 됩니다. 이 게시물은 어떠한 방식으로도 Kubernetes 튜토리얼로 간주되어서는 안됩니다. Kubernetes에 대해 배우고 싶다면 관련 온라인 자료를 참조하시기 바랍니다. 여기서는 자원 활용의 극대화와 우선 순위 지정과 관련된 몇 가지 Kubernetes 속성만 논의할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기서 제시한 방법에 대체 도구 및 기술이 많이 있습니다. 각각 장단점이 있습니다. 저희가 이 게시물에서 하고 있는 것은 순전히 교육적인 목적입니다. 우리가 하는 선택 중 어떤 것도 지지한다고 보지 마십시오.\n\n마지막으로, Kubernetes 플랫폼은 지속적인 개발 중에 있으며, AI 개발 분야의 많은 프레임워크 및 도구도 그렇습니다. 이 게시물에서의 몇 가지 설명, 예제 또는 외부 링크가 읽을 때 더 이상 유효하지 않을 수도 있음을 고려하고, 자신의 설계 결정 전에 가장 최신 솔루션을 고려해주시기 바랍니다.\n\n# 희소한 AI 연산 자원에 적응하기\n\n우리의 토론을 간단히 하기 위해, 우리는 모델을 훈련하기 위해 한 대의 워커 노드를 사용할 수 있다고 가정해 봅시다. 이것은 GPU가 장착된 로컬 머신이거나 AWS의 p5.48xlarge 인스턴스나 GCP의 TPU 노드와 같은 클라우드에서 예약된 계산가속 인스턴스일 수 있습니다. 아래의 예시에서는 이 노드를 \"내 소중한 자원\"이라고 지칭할 것입니다. 일반적으로 우리는 이 기계에 많은 돈을 투자해 왔을 것입니다. 또한 여러 훈련 작업이 단일 연산 자원을 경쟁하며 각 작업이 몇 분에서 며칠까지 소요될 수 있다고 가정할 것입니다. 당연히, 연산 자원의 활용을 극대화하고 중요한 작업을 우선순위로 지정하려고 할 것입니다. 필요한 것은 어떤 형태의 우선순위 큐와 이에 대한 기반 우선순위 기반 스케줄링 알고리즘입니다. 원하는 동작에 대해 조금 더 구체적으로 설명해 보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 일정 요구 사항\n\n- 활용 최대화: 리소스가 지속적으로 사용되도록 하고 싶습니다. 구체적으로 말하자면, 작업을 완료하는 즉시 새로운 작업에 자동으로 시작하도록 합니다.\n- 대기 중인 작업 큐: 고유한 리소스에서 처리 대기 중인 훈련 작업 큐의 존재가 필요합니다. 또한 큐에 새 작업을 생성하고 제출하며, 큐의 상태를 모니터링하고 관리하기 위한 관련 API도 필요합니다.\n- 우선순위 지원: 각 훈련 작업이 연관된 우선순위를 가지도록 하여 더 높은 우선순위를 갖는 작업이 낮은 우선순위의 작업보다 먼저 실행되도록 합니다.\n- 선점: 더 높은 우선순위의 긴급 작업이 제출되었을 때 우리의 리소스가 낮은 우선순위 작업을 처리 중이라면, 실행 중인 작업이 중단되고 긴급 작업으로 교체되기를 원합니다. 중단된 작업은 큐로 반환되어야 합니다.\n\n이러한 요구 사항을 충족하는 솔루션을 개발하기 위한 한 가지 접근 방식은 이미 존재하는 API를 훈련 리소스에 작업을 제출하는 데 사용하고, 원하는 속성이 있는 우선순위 큐의 사용자 정의 구현으로 래핑하는 것입니다. 최소한이 접근 방식에는 대기 중인 작업 목록을 저장하기 위한 데이터 구조, 큐에서 훈련 리소스로 작업을 선택하고 제출하는 전용 프로세스, 작업이 완료되고 리소스가 사용 가능해졌을 때 이를 식별하는 메커니즘이 필요합니다.\n\n대체 접근 방식 및 이 게시물에서 채택하는 방식은 요구 사항을 충족하는 우선순위 기반 일정 관리의 기존 솔루션을 활용하고 교육 개발 워크플로우를 해당 사용에 맞추는 것입니다. Kubernetes와 함께 제공되는 기본 스케줄러가 이러한 솔루션 중 하나의 예입니다. 다음 섹션에서는 희소한 AI 훈련 리소스의 활용을 최적화하는 문제를 해결하는 데 사용할 수 있는 방법을 보여드리겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 쿠버네티스를 활용한 ML 오케스트레이션\n\n이 섹션에서는 ML 훈련 워크로드의 오케스트레이션에 쿠버네티스를 적용하는 철학적인 면을 다루겠습니다. 만약 이런 논의에 참여할 여유가 없고 실제 예시로 바로 넘어가고 싶다면 다음 섹션으로 건너뛰어도 괜찮아요.\n\n쿠버네티스는 많은 개발자들 사이에서 강한 반응을 불러일으키는 소프트웨어/기술적인 솔루션 중 하나입니다. 일부는 쿠버네티스를 강력히 지지하고 적극 활용하는 반면, 다른 일부는 무거우며 서투르고 불필요하다고 생각하기도 합니다. 많은 논의와 마찬가지로 저자의 의견은 중간 어딘가에 진실이 있다고 생각합니다. 쿠버네티스는 생산성을 크게 높여줄 수 있는 이상적인 프레임워크를 제공하는 상황과 개발 프로페셔널에게 모욕이 되는 수준까지 갈 수 있는 상황이 모두 존재합니다. 핵심 질문은, ML 개발이 어느 위치에 속하는지인데요. 쿠버네티스가 ML 모델 훈련에 적합한 프레임워크일까요? 온라인 검색을 통해 간단히 얻을 수 있는 정보에서는 거의 대다수가 강력한 \"예\"라고 결론 내리는 것처럼 보일 수 있지만, 우리는 그것이 항상 옳은 선택인지에 대한 일부 주장을 제시할 것입니다. 그런데 먼저, 쿠버네티스를 사용한 \"쿠버네티스를 활용한 ML 훈련 오케스트레이션\"이라는 용어가 무엇을 의미하는지 명확히 해야 합니다.\n\n쿠버네티스를 사용한 ML에 대한 다양한 온라인 자료가 있지만, 사용 방식이 항상 일치하는 것은 아니라는 점을 명심해야 합니다. 일부 자료는 오로지 클러스터를 배포하기 위해 쿠버네티스를 사용합니다. 클러스터가 가동되면 쿠버네티스 외부에서 훈련 작업을 시작합니다. 다른 자료는 전용 모듈을 사용해 훈련 작업(및 관련 리소스)을 완전히 다른 시스템에서 시작하는 파이프라인을 정의하기 위해 쿠버네티스를 사용합니다. 위 두 예시와는 대조적으로, 다른 많은 자료들은 훈련 워크로드를 쿠버네티스 노드에서 실행되는 쿠버네티스 Job 아티팩트로 정의합니다. 그러나 각각이 주로 초점을 맞추는 특성들이 상당히 다릅니다. 일부는 오토 스케일링 속성을 강조하고 다른 일부는 다중 인스턴스 GPU(MIG) 지원을 강조합니다. 또한 ElasticJob, TrainingWorkload, JobSet, VolcanoJob 등의 훈련 작업을 나타내는 정확한 아티팩트(Job 확장)에 대한 구현 세부사항도 크게 달라집니다. 이 게시물의 맥락에서도 우리는 훈련 워크로드를 쿠버네티스 Job으로 정의한다고 가정할 것입니다. 그러나 토론을 간단히 하기 위해 쿠버네티스의 핵심 객체만 다루고 ML을 위한 쿠버네티스 확장에 대한 토론은 추후 게시물에서 다루도록 하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 케이버네티스를 머신러닝에 사용하는 것에 반대하는 주장들\n\n여기 케이버네티스를 통해 머신러닝 모델을 학습하는 데에 반대할 수 있는 몇 가지 주장들이 있습니다.\n\n- 복잡성: 케이버네티스는 어렵다는 것은 그 사용자들조차 인정해야 합니다. 케이버네티스를 효과적으로 사용하려면 높은 전문 지식이 요구되며, 가파른 학습 곡선이 존재하고, 현실적으로 말하자면 전문 개발팀이 필요합니다. 케이버네티스를 기반으로 한 학습 솔루션을 설계하는 것은 전문가에 대한 의존성을 증가시키고, 결과적으로 문제가 발생할 수 있는 위험을 증가시키며, 개발이 지연될 수 있다는 위험을 증가시킵니다. 많은 대체 머신러닝 학습 솔루션은 개발자 독립성과 자유를 보장하며 개발 과정에서 버그의 위험을 줄일 수 있습니다.\n- 고정된 자원 요구 사항: 케이버네티스의 가장 큰 장점 중 하나인 확장성 — 작업의 수, 클라이언트 수(서비스 응용 프로그램의 경우), 자원 용량 등에 따라 자동으로 컴퓨팅 자원 풀을 확장하고 축소할 수 있는 능력에 대해 가장 주목받는 특성 중 하나입니다. 그러나 머신러닝 학습 작업의 경우 필요한 자원의 수가 일반적으로 학습 중에 고정된다는 점을 고려할 수 있습니다.\n- 고정된 인스턴스 유형: 케이버네티스는 컨테이너화된 응용 프로그램을 오케스트레이션하기 때문에 노드 풀 내의 머신 유형에 대해 매우 유연성을 제공합니다. 그러나 머신러닝의 경우 주로 GPU와 같은 전용 가속기를 갖춘 매우 구체적인 기계가 필요합니다. 게다가, 우리의 작업은 종종 매우 특정 인스턴스 유형에서 최적으로 실행될 수 있습니다.\n- 단일 애플리케이션 아키텍처: 현대 애플리케이션 개발에서 애플리케이션을 미니 서비스라고 불리는 작은 구성 요소로 분해하는 것이 일반적입니다. 케이버네티스는 이 설계의 핵심 구성 요소로 자주 볼 수 있습니다. 머신러닝 학습 응용 프로그램은 설계상 매우 단일식이며, 이는 마이크로 서비스 아키텍처에 자연스럽게 합치기 어려울 수 있다는 주장이 있습니다.\n- 자원 오버헤드: 케이버네티스를 실행하는 데 필요한 전용 프로세스는 각 노드에 일부 시스템 자원을 필요로 합니다. 따라서 이는 학습 작업에 일정한 성능 저하를 초래할 수 있습니다. 학습에 필요한 자원의 비용을 감안할 때, 이를 피하길 선호할 수 있습니다.\n\n물론, 우리는 케이버네티스를 머신러닝을 위한 프레임워크로 선택하는 데 필요한 아주 좋은 이유가 있어야 한다는 한면적인 견해를 가졌습니다. 위에서 제시된 주장에만 근거하여 보면, 케이버네티스를 선택하기 위한 뚜렷한 이유가 필요할 것으로 보입니다. 본 포스트에서 제시된 도전에는 희귀한 AI 컴퓨팅 자원의 유틸리티를 극대화하려는 열망이 바로 위의 주장에도 불구하고 케이버네티스를 사용하게끔 타당하게 만드는 유형의 정당화라고 생각합니다. 저희는 내장된 기본 스케줄러와 우선 순위와 선점을 지원하는 케이버네티스가 이러한 요구 사항을 충족시키는 데 선두주자임을 입증하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 장난감 예시\n\n이 섹션에서는 Kubernetes에 내장된 우선 순위 스케줄링 지원을 보여주는 간단한 예시를 공유하겠습니다. 저희의 데모를 위해 Minikube (버전 v1.32.0)를 사용할 것입니다. Minikube는 로컬 환경에서 Kubernetes 클러스터를 실행할 수 있는 도구로, Kubernetes를 실험하기에 이상적인 플레이그라운드입니다. Minikube의 설치 및 시작하는 공식 문서를 참조해주세요.\n\n## 클러스터 생성\n\nMinikube start 명령을 사용하여 두 노드 클러스터를 생성해보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nminikube start --nodes 2\n\n로컬 Kubernetes 클러스터를 시작하면 마스터(“control-plane”) 노드인 minikube와 하나의 워커 노드인 minikube-m02가 만들어집니다. 이 노드는 단일 AI 리소스를 시뮬레이트합니다. 이를 고유한 리소스 유형으로 식별하기 위해 my-precious 라벨을 적용해보세요:\n\nkubectl label nodes minikube-m02 node-type=my-precious\n\nMinikube 대시보드를 사용하여 결과를 시각화할 수 있습니다. 다음 명령을 실행하여 새로운 셸에서 생성된 브라우저 링크를 열어보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nminikube 대시보드\n```\n\n왼쪽 창에서 노드 탭을 누르면 클러스터 노드의 요약을 볼 수 있습니다:\n\n<img src=\"/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_1.png\" />\n\n## PriorityClass 정의\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 아래에 표시된 priorities.yaml 파일에 low-priority와 high-priority 두 가지 PriorityClasses를 정의합니다. 새 작업은 기본적으로 low-priority 할당을 받게 됩니다.\n\n```yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: low-priority\nvalue: 0\nglobalDefault: true\n\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000\nglobalDefault: false\n```\n\n새 클래스를 클러스터에 적용하기 위해 다음 명령을 실행합니다:\n\n```bash\nkubectl apply -f priorities.yaml\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 작업 생성\n\n아래 코드 블록에 표시된 job.yaml 파일을 사용하여 간단한 작업을 정의합니다. 우리의 데모를 위해, 100초 동안 아무것도 하지 않는 Kubernetes Job을 정의합니다. Docker 이미지로는 busybox를 사용합니다. 실제로는 이를 학습 스크립트와 적절한 ML Docker 이미지로 대체해야 합니다. 이 작업을 특수 인스턴스인 my-precious에서 실행하도록 nodeSelector 필드를 사용하여 리소스 요구 사항을 지정하여 해당 인스턴스에서 한 번에 하나의 작업 인스턴스만 실행되도록 합니다. 작업의 우선순위는 위에서 정의한 대로 저우선순위로 설정됩니다.\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test\nspec:\n  template:\n    spec:\n      containers:\n        - name: test\n          image: busybox\n          command: # 간단한 sleep 명령\n            - sleep\n            - \"100\"\n          resources: # 모든 가능한 리소스 요구\n            limits:\n              cpu: \"2\"\n            requests:\n              cpu: \"2\"\n      nodeSelector: # 우리의 고유한 리소스 지정\n        node-type: my-precious\n      restartPolicy: Never\n```\n\n다음 명령어로 작업을 제출합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl apply -f job.yaml\n```\n\n## 작업 대기열 만들기\n\nKubernetes가 작업을 처리하기 위해 대기열에 넣는 방식을 보여주기 위해 위에서 정의한 작업의 세 개의 동일한 복사본인 test1, test2 및 test3을 만듭니다. 이 세 작업을 jobs.yaml 파일에 그룹화하여 제출합니다:\n\n```js\nkubectl apply -f jobs.yaml\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 이미지는 제출 직후 Minikube 대시보드에서 클러스터의 Workload 상태를 캡처한 것입니다. my-precious가 test1을 처리하기 시작했음을 확인할 수 있습니다. 다른 작업들은 차례를 기다리며 대기 중입니다.\n\n![Workload Status](/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_2.png)\n\ntest1 처리가 완료되면 test2 처리가 시작됩니다:\n\n![Workload Status](/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 우선순위가 높은 다른 작업이 제출되지 않는 한, 우리의 작업은 모두 완료될 때까지 한 번에 하나씩 처리될 것입니다.\n\n## 작업 선점\n\n이제 쿠버네티스가 제공하는 작업 선점 기능을 소개하기 위해 우선순위 설정이 높은 네 번째 작업을 제출할 때 어떻게 작동하는지 보여드리겠습니다:\n\n```js\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-p1\nspec:\n  template:\n    spec:\n      containers:\n        - name: test-p1\n          image: busybox\n          command:\n            - sleep\n            - '100'\n          resources:\n            limits:\n              cpu: \"2\"\n            requests:\n              cpu: \"2\"\n      restartPolicy: Never\n      priorityClassName: high-priority # 높은 우선순위 작업\n      nodeSelector:\n          node-type: my-precious\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 이미지에서 작업 부하 상태의 영향이 표시됩니다:\n\n![Workload Status](/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_4.png)\n\ntest2 작업이 중단되었습니다. 더 높은 우선순위의 test-p1 작업을 처리하기 위해 중단되었고, 대신에 my-precious가 처리를 시작했습니다. test-p1이 완료되면 낮은 우선순위 작업의 처리가 재개될 것입니다. (중단된 작업이 ML 훈련 작업인 경우, 최근 저장된 모델 체크포인트로부터 재개되도록 프로그램할 것입니다).\n\n아래 이미지는 모든 작업이 완료된 후의 작업 부하 상태를 보여줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_5.png)\n\n# 쿠버네티스 확장\n\n우리가 우선순위 기반 스케줄링 및 선점을 위해 시연한 솔루션은 쿠버네티스의 핵심 구성 요소에만 의존했습니다. 실제로는, Kueue와 같은 확장 프로그램이 소개한 기본 기능을 향상시키거나, Run:AI나 Volcano와 같은 쿠버네티스 기반 플랫폼에서 제공되는 전용 ML 특징을 활용할 수 있습니다. 하지만, AI 컴퓨팅 자원의 활용도를 극대화하기 위한 기본 요구사항을 충족하기 위해서는 핵심 쿠버네티스만 있으면 됨을 기억해 주세요.\n\n# 요약\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI 실리콘의 가용성이 감소함에 따라 ML 팀은 개발 프로세스를 조정해야 했습니다. 이전과 달리 개발자들이 필요에 따라 AI 리소스를 자유롭게 사용할 수 있던 과거와는 달리, 이제는 AI 컴퓨트 용량에 제약이 생겼습니다. 이는 전용 장치를 구매하거나 클라우드 인스턴스를 예약하는 등의 방법으로 AI 인스턴스를 확보해야 한다는 필요성을 의미합니다. 게다가, 개발자들은 이러한 자원을 다른 사용자 및 프로젝트와 공유해야 할 가능성을 인지해야 합니다. 희소한 AI 컴퓨트 파워가 최대한 활용되도록하기 위해, 유휴 시간을 최소화하고 핵심 워크로드에 우선 순위를 둔 일정 알고리즘을 정의해야 합니다. 본 기사에서는 쿠버네티스 스케줄러를 활용하여 이러한 목표를 달성하는 방법을 보여주었습니다. 앞서 강조했듯이, 이는 희소한 AI 자원의 유틸리티를 극대화하는 도전에 대처하기 위한 다양한 접근 방식 중 하나에 불과합니다. 당연히 선택하는 방식 및 구현 세부사항은 여러분의 AI 개발의 특정 요구에 따라 다를 것입니다.\n","ogImage":{"url":"/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_0.png"},"coverImage":"/assets/img/2024-06-23-MaximizingtheUtilityofScarceAIResourcesAKubernetesApproach_0.png","tag":["Tech"],"readingTime":15},{"title":"MLOps 작업을 위한 GPU와 함께 Kubernetes 사용 방법","description":"","date":"2024-06-23 00:51","slug":"2024-06-23-KuberneteswithGPUforMLOpsWorkloads","content":"\n이 기사에서는 GPU가 쿠버네티스와 통합되어 기계 학습 워크로드를 실행하는 방법에 대해 논의하고자 합니다.\n\n저는 주변에 있는 NVIDIA GeForce RTX 3050을 가지고 아이디어를 얻었어요 💡... ` Kubernetes\n\n빠르게 쿠버네티스와 GPU를 통합하여 딥 러닝 훈련 배포를 실행해봅시다!\n\n0. Docker가 설치되어 있는지 확인하세요. 아직 설치되지 않았다면 여기에서 Docker를 다운로드하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# NVIDIA Container Toolkit for Docker\n\n- NVIDIA 드라이버를 업데이트해주세요\n\n```js\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda-toolkit-12-5\n```\n\n- 공식 웹사이트에서 해당 OS에 맞는 NVIDIA 컨테이너 툴킷을 설치해주세요.\n- 제가 WSL을 사용하고 있기 때문에 apt 명령어를 통해 설치하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n```\n\n- 저장소에서 패키지 목록을 업데이트합니다:\n\n```js\nsudo apt-get update\n```\n\n- NVIDIA Container Toolkit 패키지를 설치합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nsudo apt-get install -y nvidia-container-toolkit\n```\n\n![KuberneteswithGPUforMLOpsWorkloads](/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_0.png)\n\n- 도커를 실행할 수 있도록 런타임 구성하기\n\n```js\nsudo nvidia-ctk runtime configure --runtime=docker\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 테이블 태그를 Markdown 형식으로 변경한 코드입니다.\n\n<img src=\"/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_1.png\" />\n\n- restart docker\n\n```js\nsudo systemctl restart docker\n```\n\n- Docker Desktop\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만일 Docker Desktop을 사용 중이라면, 설정을 다르게 구성하고 daemon.json을 수정해야 합니다.\n\n- 왼쪽 상단의 설정 ⚙️ 로 이동하여 Docker Engine으로 이동한 다음 다음 구성을 추가하십시오. (,를 잊지 말고 json 구문을 확인해주세요 😅)\n\n```js\n\"runtimes\": {\n  \"nvidia\": {\n    \"args\": [],\n    \"path\": \"nvidia-container-runtime\"\n  }\n}\n```\n\n![이미지](/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. Apply 및 다시 시작을 클릭하고 GPU 확인\n\n다음 명령어로 도커가 런타임으로 GPU를 사용하는지 확인하세요.\n\n```shell\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n![이미지](/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Minikube\n\n로컬에서 Kubernetes 클러스터를 프로비저닝하는 데 Minikube를 사용할 것입니다.\n\n- [여기](링크)에서 운영 체제에 맞게 Minikube를 다운로드하세요.\n- Minikube 이진 파일을 다운로드한 후 다음 명령어로 Minikube를 시작하세요. Minikube를 시작하기 전에 Docker가 실행 중인지 확인하세요.\n\n```js\nminikube start --gpus all --driver=docker --addons=ingress\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMinikube의 nvidia-gpu-device-plugin 애드온은 Kubernetes 클러스터에서 GPU 지원을 활성화하는 데 설계되었습니다. 이 애드온을 사용하면 Kubernetes가 GPU가 필요한 작업로드를 인식하고 예약하여 클러스터에서 GPU 리소스를 사용할 수 있게 됩니다.\n\nNVIDIA GPU 장치 플러그인이란 무엇인가요?\n\nNVIDIA GPU 장치 플러그인은 Kubernetes 클러스터에서 NVIDIA GPU를 사용할 수 있게 하는 Kubernetes 장치 플러그인입니다. 이 플러그인을 사용하면 Kubernetes가 GPU 리소스를 컨테이너에 할당하고 예약하여 응용 프로그램이 GPU 가속을 사용할 수 있도록 합니다.\n\n주요 기능\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- GPU 발견:\n\n  - 노드에서 NVIDIA GPU를 자동으로 발견하여 Kubernetes에서 스케줄 가능한 리소스로 이용할 수 있습니다.\n\n- GPU 리소스 관리:\n\n  - GPU 리소스를 컨테이너에 할당하는 작업을 관리합니다. Pod에 요청된 GPU 수를 확인하고 GPU 리소스를 스케줄링하는 복잡성을 처리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 격리:\n\n- GPU 리소스의 격리를 제공하여 GPU 워크로드가 서로 간섭하지 않도록 보장합니다.\n\n4. 메트릭 및 모니터링:\n\n- GPU 활용에 관한 메트릭을 노출하여 GPU 워크로드의 모니터링과 스케일링에 활용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 쿠버네티스에서 GPU 가용성 확인하기\n\nGPU 노드가 사용 가능한지 노드 세부정보를 확인하여 확인하세요\n\n```js\nkubectl get nodes -o \"custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,CAPACITY:.status.capacity\"\n```\n\n- 작업으로 확인하고 매니페스트 파일을 적용하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: gpu-job\nspec:\n  template:\n    spec:\n      containers:\n        - name: gpu-container\n          image: nvidia/cuda:12.5.0-base-ubuntu22.04\n          resources:\n            limits:\n              nvidia.com/gpu: 1 # Request 1 GPU\n          command: [\"nvidia-smi\"]\n      restartPolicy: Never\n```\n\n```bash\nkubectl apply -f gpu-verify.yaml\n```\n\n![Kubernetes with GPU for MLOps Workloads](/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_4.png)\n\n- 파드 가져오기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl get po\n```\n\n![이미지](/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_5.png)\n\n- GPU 로그 확인\n\n```js\nkubectl logs <pod>\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_6.png)\n\n그리고 시작합니다..!\n\nGPU를 보실 수 있습니다. 업데이트를 기다려주세요. 몇 가지 워크로드를 실행하고 여기에 공유할 예정입니다.\n\n이제 쿠버네티스에서 머신러닝 모델을 훈련할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n읽어 주셔서 감사합니다!\n\n[LinkedIn 프로필](https://www.linkedin.com/in/sivanaik/)\n\n[x.com에서의 프로필](https://x.com/sivanaikk)\n","ogImage":{"url":"/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_0.png"},"coverImage":"/assets/img/2024-06-23-KuberneteswithGPUforMLOpsWorkloads_0.png","tag":["Tech"],"readingTime":9},{"title":"Snowpark Container Services에서 Pytorch 실행하는 방법","description":"","date":"2024-06-23 00:49","slug":"2024-06-23-SoYouWantToJustGetPytorchRunninginSnowparkContainerServices","content":"\n![그림](/assets/img/2024-06-23-SoYouWantToJustGetPytorchRunninginSnowparkContainerServices_0.png)\n\n간략하게 말하자면: hello_world와 세계를 무대낼 수 있는 (Snowpark) 컨테이너 기반 제품 사이의 간극을 메우는 약간 더 고급급 튜토리얼\n\n컨테이너는 복잡할 수 있지만, 꼭 그렇지 않아요. 이 복잡성의 한 이유는 \"다중 노드 컨테이너 컴퓨팅 풀을 활용하여 무제한 규모와 탄성을 쉽게 구축하기 위해 컨테이너 사용\"과 같은 제목의 고급 주제에 바로 뛰어든 가이드가 너무 많기 때문입니다. 이러한 가이드는 종종 컨테이너 전문 지식을 전제로 한, yaml 파일, 도커, 명령줄 등에 대한 가정 지식으로 가득 차 있습니다. 한 마디로, 압도적입니다. 문제가 발생할 때 사용자 에러와 구현 특이점을 구별하기 어려울 수 있습니다. 저와 같은 경우에는 마치 사기 치는 것처럼 느껴질 수 있어서, 더 어려울 수 있습니다.\n\n다행히도 Snowflake는 사용하기 쉬운 Snowpark Container Services로 컨테이너의 세계를 간단하게 만들어 냈습니다. 이 안내서는 기본부터 시작하여 여러분의 실제 목표와 튜토리얼 사이의 간격을 줄이기 위해 설계되었습니다. 컨테이너에 대해 거의 전혀 알지 못하고 튜토리얼과 실제 목표 간의 간극을 메우기 어렵다고 느끼는 것을 가정합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 0. 도커 설치\n\n가장 쉬운 방법은 https://docs.docker.com/engine/install/ 로 이동하여 관련 사본을 다운로드하는 것입니다. 이를 백그라운드에서 실행해야 하므로 다운로드하고 설치한 후에는 앱을 더블 클릭해서 실행하는 것이 좋습니다.\n\n# 1. 가장 간단한 튜토리얼 따르기\n\n간단하다고 했지만, 정말로 간단하고 싶다면 저희 문서의 스노우파크 컨테이너 서비스에 있는 헬로 월드 내용을 따르는 게 어느 것도 이길 수 없습니다. 컨테이너에 대해 전혀 들어보지 못했다면 눈을 감고 따라하고 무언가를 실행한 후에 다음 단계로 넘어가서 무슨 일이 벌어지고 있는지 더 배워보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 2. 로컬 파일 설정하기 (컨테이너화하기 위해)\n\n이 작업을 모두 수행하는 데는 세 가지가 필요합니다\n\n- Python 스크립트 (main.py는 좋은 시작 이름입니다): 컨테이너에서 실행되는 스크립트로, Dockerfile에서 참조됩니다. 이것은 단일 파일이 아니어도 됩니다. 큰 패키지일 수도 있지만, 지금은 간단하게 유지합시다.\n- Dockerfile: Docker 이미지를 빌드하는 방법을 설명하는 파일로, 기본 환경, 패키지 임포트 및 내부 코드 실행 방법과 같은 내용이 포함됩니다. 가상 환경을 구축하는 conda create 과정과 유사한 개념으로 생각해보세요.\n- spec.yaml: Snowflake가 컨테이너를 실행하는 방법을 Snowflake에 알려주는 파일로, 주로 하드웨어 구성 요소를 지정합니다. 이 파일은 snowflake에 특화되어 있지만, docker-compose.yaml에 상당히 유사합니다.\n\nPython 스크립트\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기 쉬운 것부터 시작해봅시다; 파이썬에서 실행할 스크립트를 만들어 보세요. 이는 다른 것을 실행하는 것과 다르지 않을 것입니다; 그저 몇 가지 파이썬의 특이점을 준수해야 합니다, 즉 if **name** 비트). 아마 이런 식일 것입니다:\n\n```python\n# all my fancy code...\n\ndef run_job():\n    #do something like run a pytorch neural network that references my fancy code\n\nif __name__ == \"__main__\":\n    run_job()\n```\n\n너무 길어지지 않게 하려고 전체를 제거했으나 실제 스크립트를 찾을 수 있습니다.\n\n로컬에서는 그냥 main.py를 포함하는 폴더로 이동하여 명령줄에서 \"python3 main.py\"를 입력하면 실행됩니다 (아래 Dockerfile 설명 참조). 이 중요한 것을 기억하는 것이 중요한데, 이것은 임의로 복잡할 수 있기 때문에 print(\"hello world\")나 오픈AI를 꺾을 새로운 LLM과 같은 것일 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Dockerfile\n\n요렇게, Dockerfile이에요; 여기서 파일 유형은 보이지 않아요. 이렇게 보일 거에요:\n\n```js\nARG BASE_IMAGE=continuumio/miniconda3:4.12.0\nFROM $BASE_IMAGE\nRUN conda install python=3.8 && \\\n    pip install --upgrade pip && \\\n    pip install torch==2.0.0 && \\\n    pip install torchdata==0.6.0 && \\\n    pip install snowflake-ml-python==1.0.12 && \\\n    pip install snowflake-snowpark-python==1.9.0 \\\n    pip install accelerate==0.29.3\n\nCOPY main.py ./\nENTRYPOINT [\"python\", \"main.py\"]\n```\n\nARG BASE_IMAGE은 이게 기반이 될 환경인데요, 우리 경우에는 \"continuumio/miniconda3:4.12.0\" 이에요. 더 자세한 내용은 여기에서 찾아볼 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그 이후에는 pip/conda 설치를 실행합니다. 로컬 환경에서와 같이 실행될 것이므로 main.py 파일에서 사용할 필요가 있는 모든 패키지를 나열해주세요.\n\n그런 다음 COPY main.py를 사용하여 컨테이너가 수행할 실제 작업을 정의하는 로컬 파일을 가져옵니다.\n\n마지막으로, ENTRYPOINT가 Python을 실행하는 데 사용됩니다. 이는 스크립트를 실행하기 위해 명령줄에서 실행하는 것과 다른 점이 없습니다.\n\n## spec.yaml\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n좋아요, 마지막으로는 spec.yaml 파일입니다. 여기에서 Snowflake에 이 파일이 작동하도록 실제로 해야 하는 작업을 알려줍니다.\n\n```js\nspec:\n  container:\n  - env:\n      MOUNT_PATH: /dev/shm\n    image: /tutorial_db/data_schema/tutorial_repository/my_2job_image:latest\n    name: main\n    resources:\n      limits:\n        memory: 192G\n        nvidia.com/gpu: 4\n      requests:\n        memory: 188G\n        nvidia.com/gpu: 4\n    volumeMounts:\n    - mountPath: /opt/training-output\n      name: training-output\n    - mountPath: /dev/shm\n      name: dshm\n  volumes:\n  - name: training-output\n    source: '@tutorial_db.data_schema.tutorial_stage'\n  - name: dshm\n    size: 10Gi\n    source: memory\n```\n\n첫 번째 눈에는 복잡해 보일 수 있지만, 실제로 들어가보면 꽤 간단합니다. Snowpark Container Services 문서는 좋은 자료입니다. 여기에서 찾을 수 있습니다.\n\n그럼에도 불구하고, 몇 가지 주요 포인트를 소개해 드리겠습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 이미지: 이는 우리가 4단계와 5단계에서 빌드하고 푸시하는 동일한 이미지입니다\n- 리소스: 우리는 4대의 Nvidia GPU(4개의 A10G)를 지정한 것에 유의합니다. 이는 컴퓨트 풀을 만들 때 GPU_NV_M을 사용할 것이기 때문입니다. 해당 풀에는 Nvidia A10G가 4대 들어 있습니다. 자세한 내용은 여기를 참조하세요.\n- 볼륨: 이는 Snowflake 내에서 작업 실행간에 무언가를 지속시킬 수 있는 능력을 제공합니다.\n\n# 3. Snowflake 준비하기\n\nSnowflake 문서의 자습서에서 잘 다루고 있지만, 완벽성을 위해 SQL도 여기에 포함했습니다:\n\n```js\nUSE ROLE ACCOUNTADMIN;\n\nCREATE ROLE test_role;\n\nCREATE DATABASE IF NOT EXISTS tutorial_db;\nGRANT OWNERSHIP ON DATABASE tutorial_db TO ROLE test_role COPY CURRENT GRANTS;\n\nCREATE OR REPLACE WAREHOUSE tutorial_warehouse WITH\n  WAREHOUSE_SIZE='X-SMALL';\nGRANT USAGE ON WAREHOUSE tutorial_warehouse TO ROLE test_role;\n\nCREATE SECURITY INTEGRATION IF NOT EXISTS snowservices_ingress_oauth\n  TYPE=oauth\n  OAUTH_CLIENT=snowservices_ingress\n  ENABLED=true;\n\nGRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE test_role;\n\nCREATE COMPUTE POOL tutorial_gpu_pool\n  MIN_NODES = 1\n  MAX_NODES = 1\n  INSTANCE_FAMILY = GPU_NV_M;\nGRANT USAGE, MONITOR ON COMPUTE POOL tutorial_gpu_pool TO ROLE test_role;\n\nGRANT ROLE test_role TO USER admin --로그인 사용자명\n\nUSE ROLE test_role;\nUSE DATABASE tutorial_db;\nUSE WAREHOUSE tutorial_warehouse;\n\nCREATE SCHEMA IF NOT EXISTS data_schema;\nCREATE IMAGE REPOSITORY IF NOT EXISTS tutorial_repository;\nCREATE OR REPLACE STAGE tutorial_stage;\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일부 고려할 사항은 다음과 같습니다:\n\n- ACCOUNTADMIN으로 모든 작업을 실행할 수 있다고 생각하지 마세요. 그럴 수 없어요. 이것은 의도된 것이며, 해당 역할이 제공하는 신의 모드 권한을 즐기던 우리에게는 새로운 변화입니다. test_role이 꼭 필요합니다 (또는 다른 동등한 권한).\n- Compute Pools는 이 목록에서 아무 것이나 될 수 있습니다. 저희의 경우 GPU를 사용하고 있습니다 (GPU_NV_M, 이는 4개의 Nvidia A10G를 포함하고 있음) 이는 신경망이 빠르게 실행됩니다.\n\n# 4. 로컬 환경에서 컨테이너 생성하기\n\n먼저, Snowflake에서 다음을 실행하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이미지 저장소 표시\n\n이렇게 보일 것입니다:\n\n![image](/assets/img/2024-06-23-SoYouWantToJustGetPytorchRunninginSnowparkContainerServices_1.png)\n\n저장소 URL이 있습니다 (추론할 수 있지만, 확실한 것이 무릎에 허물지 않는다는 것을 기억하세요)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 명령 프롬포트 또는 터미널에서 로컬 머신의 DockerFile 위치로 이동한 다음 다음을 실행하세요:\n\n```js\ndocker build --rm --platform linux/amd64 -t repository_url/my_2job_image:latest .\n```\n\n여기서 repository_url은 위의 URL로 대체됩니다. 즉, 제 경우에는\n\nsfseeurope-eu-demo211.registry.snowflakecomputing.com/tutorial_db/data_schema/tutorial_repository 입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 도커를 호출하는 것은 도커라고 생각하면 됩니다.\n- 빌드는 도커에 이미지를 빌드하라는 것입니다.\n- --rm은 빌드로 생성된 중간 컨테이너를 제거합니다.\n- --platform은 이를 위해 의도된 플랫폼을 지정합니다.\n- repository_url/my_2job_image:latest .은 만들고 있는 REPOSITORY입니다.\n\n참고로, 마침표(“.”)를 끝에 포함했는지 확인하세요. Docker에 익숙하지 않다면 쉽게 놓칠 수 있습니다.\n\n여기서 중요한 점 몇 가지 주의할 점이 있습니다. 여기서 저는 낯익은 \"latest\" 태그를 사용하고 있지만, 실질적으로 명시적 버전 태그를 사용하는 것이 훨씬 좋습니다. 자세한 설명은 여기를 참고하세요.\n\n- 이제 컨테이너가 생성되었으니, 진짜 생성되었는지 두 번 확인해보겠습니다. 명령줄에서 다음을 실행해주세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 이미지 ls\n```\n\n하시면 도커가 로컬로 보유한 이미지 목록을 확인할 수 있어요.\n\n# 5. 명령줄/터미널을 통한 도커 로그인\n\nSnowflake로 컨테이너를 푸시하려면 도커에 로그인해야 해요. 터미널/명령줄에서 다음 명령을 실행해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```bash\ndocker login <registry_hostname> -u <username>\n```\n\n- registry_hostname은 `orgname`-`acctname`.registry.snowflakecomputing.com 형식을 취하며, sfseeurope-eu_demo1234.registry.snowflakecomputing.com와 유사한 형태를 가질 것입니다.\n- username은 Snowflake GUI에 로그인하는 데 사용하는 사용자 이름입니다.\n- 암호는 docker가 암호를 요청할 것이며, GUI에 로그인하는 데 사용하는 동일한 암호입니다.\n\n# 6. Snowflake로 컨테이너 푸시하기\n\n로컬 이미지도 좋지만, 우리는 그것들을 Snowflake로 옮기고 싶어요! 여기도 간단한 단계입니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 푸시 저장소_주소/my_2job_image:latest\n```\n\n# 7. Snowflake에 YAML 추가하기\n\n거의 다 왔어요. 이제 YAML을 Snowflake로 가져와야 합니다. 이 작업을 하는 가장 좋은 방법은 SNOWSQL을 통해 하는 것이에요. 다운로드 페이지로 이동해주세요. 다운로드를 완료한 후에는 다음과 같이 계정 세부 정보로 로그인해야 합니다.\n\n```js\nsnowsql -a 계정_이름\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n계정 이름은 \"sfseeurope-eu_demo1234\"와 같은 형식일 것입니다 (적어도 제 경우에는요).\n\n사용자 이름과 암호를 입력한 후 CLI에서 SQL을 입력할 수 있습니다. 다음을 수행하려면 아래와 같이 하세요:\n\n```js\nUSE ROLE test_role;\nUSE DATABASE tutorial_db;\nUSE SCHEMA data_schema;\nPUT file:///the_file_location @tutorial_stage\n  AUTO_COMPRESS=FALSE\n  OVERWRITE=TRUE;\n```\n\n또한 Snowflake GUI에서 결과를 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 변환한 내용입니다. 😊\n\n![이미지](/assets/img/2024-06-23-SoYouWantToJustGetPytorchRunninginSnowparkContainerServices_2.png)\n\n# 8. 컨테이너 실행 (서비스 작업으로)\n\n그런 다음 제공된 SQL 명령을 실행하면 됩니다. 데이터베이스, 롤, 스키마 등에 대해 명시적으로 언급했습니다. 이는 올바른 것들을 사용하고 있음을 확실히 하기 위한 것입니다. Snowpark Container Services는 롤 관리에 특히 민감하며 Snowflake의 다른 부분보다 더 그렇습니다. 잠재적인 문제를 피하고 머리아픈 일을 피하려면 다음과 같이 명시적으로 하세요:\n\n```js\nUSE ROLE test_role;\nUSE DATABASE tutorial_db;\nUSE SCHEMA data_schema;\nUSE WAREHOUSE tutorial_warehouse;\nEXECUTE JOB SERVICE\nIN COMPUTE POOL tutorial_gpu_pool\nNAME=tutorial_MT_job_service\nFROM @tutorial_stage\nSPEC='my_job_spec.yaml';\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 9. 잘못된 부분을 해결해보세요\n\n컨테이너는 뭔가 잘못될 때 매우 봉쇄적으로 느껴질 수 있어요. 특히 클라우드 플랫폼에서 실행되고 있는 상황에서 말이죠. 하지만 사실은, 몇 가지 중요한 부분의 코드만 추가했다면 간단합니다. 중요도 순으로 살펴보겠습니다:\n\n- 로컬에서 작동하는지 확인하세요: 당연한 얘기이지만, 가끔은 처음으로 컨테이너에서 무언가를 실행할 때, 컨테이너에 문제가 있거나 코드에 문제가 있을 수 있기 때문에 확인이 필요합니다.\n- 빠르고 더러운 방법: 코드에 몇 가지 프린트 문을 추가하여 진행 상황을 체크하고 코드를 디버깅할 수 있습니다.\n- 로깅: \"탄탄한\" 방법은 로깅 함수를 설정하는 것이지만, 실제로는 print() 문을 사용하는 것과 유사합니다.\n\n2번과 3번은 결과물을 검토할 곳이 있다는 가정하에 이루어집니다. 다행히도, 확인할 공간이 있습니다! 필요한 것은 다음을 실행하는 것뿐입니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```sql\nSELECT SYSTEM$GET_SERVICE_LOGS('TUTORIAL_DB.data_schema.tutorial_MT_job_service', 0, 'main');\n```\n\n이 명령을 실행하면 여러분의 시도에서 발생한 로그 파일이 반환되며, 무슨 문제가 발생했는지 디버깅할 수 있습니다.\n\n# 마무리\n\n이 안내서가 여러분이 컨테이너 사용을 시작하고 복잡한 애플리케이션을 배포할 때 자신감을 갖게 해주어서, 이미 \"장난감\" 이상의 것을 만들었다는 것을 알 수 있기를 바랍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n최종적으로, 컨테이너를 모델 훈련의 수단으로만 보는 분들에게 한 마디 전하고 싶어요. 최근 Snowflake에서는 컨테이너 실행을 더 쉽게 만들어주는 프라이빗 프리뷰를 시작했어요. 노트북에서 일반적인 파이썬 실행과 2번의 클릭만으로 컨테이너 내에서 코드 실행이 가능해졌죠. 앞으로 더 쉬워질 것이니 이 공간을 주의깊게 지켜보세요.\n\n# 추가 자료\n\n전체 문서는 여기에서 확인할 수 있고, 제 동료들은 Snowpark 컨테이너 서비스 활용 방법에 관한 멋진 블로그 포스트를 작성했어요. 위 작업을 완료했다면 다음 단계로 가는 것이 좋겠죠:\n\n- [Snowflake에서 책임 있는 AI: Snowflake Cortex, LLMS, Snowpark 컨테이너 서비스 활용](https://medium.com/snowflake/responsible-ai-on-snowflake-snowflake-cortex-llms-snowpark-container-services-snowflake-a0acc3e93909)\n- [Snowpark 컨테이너 서비스를 활용한 OCR 포함 문서 추출 및 GPU 가속화](https://medium.com/@michaelgorkow/document-extraction-incl-ocr-with-gpu-acceleration-in-snowpark-container-services-d3d9f4d4764a)\n- [Snowflake Cortex 기능 및 Snowpark 컨테이너 서비스를 활용한 콜센터 분석](https://medium.com/snowflake/call-centre-analytics-with-snowflake-cortex-function-and-snowpark-container-services-5e06b4baef46)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n또한 — 리뷰 코멘트를 제공해준 Caleb Baechtold에게 감사드립니다.\n","ogImage":{"url":"/assets/img/2024-06-23-SoYouWantToJustGetPytorchRunninginSnowparkContainerServices_0.png"},"coverImage":"/assets/img/2024-06-23-SoYouWantToJustGetPytorchRunninginSnowparkContainerServices_0.png","tag":["Tech"],"readingTime":16},{"title":"Nextjs 앱을 Dockerize하는 방법","description":"","date":"2024-06-23 00:47","slug":"2024-06-23-DockerizeaNextjsApp","content":"\n만약 Vercel에 Next.js 앱을 배포하고 싶다면, 이 경우에는 컨테이너가 필요하지 않습니다. Next.js는 Vercel에서 만들어지고 유지보수되기 때문에 배포가 쉽게 가능합니다. 그러나 AWS, Google Cloud Run 또는 다른 클라우드 공급업체를 통해 앱을 실행하려는 경우에는 컨테이너가 필요합니다.\n\n온라인에서 찾은 대부분의 기사들은 Node.js 앱을 도커라이즈하는 방법을 설명하지만, Next.js 앱에 초점을 많이 두지 않았습니다. 해결책들은 있지만, 그 해결책들은 몇 가지 오류를 발생시켜 몇 시간이 걸렸습니다. 그래서 제가 그것을 어떻게 했는지, 부딪힌 문제점들, 그리고 그에 대한 제 해결책을 공유하려고 합니다.\n\n# 시작하기!\n\n가지고 계신 Next.js 프로젝트 중 하나를 사용하거나, 공식 Vercel에서 제공하는 예제 중 하나를 클론해 시작하세요. 온라인에서 찾은 다른 해결책들은 애플리케이션을 독립 실행형 애플리케이션으로 빌드하기 위해 next.config.js에 다음 라인들을 추가하는 것이 필수적이라고 제안했지만, 그것을 추가하지 않고도 모든 것이 잘 작동하는 것을 발견했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nconst nextConfig = {\n  output: \"standalone\", // 이 줄이 없어도 잘 작동했다.\n  // ... 다른 설정\n};\n```\n\n## DockerFile\n\n루트 저장소에 동일한 이름을 가진 Dockerfile을 추가하세요. 이 파일에는 빌드할 도커 이미지에 대한 명령을 추가할 것입니다.\n\n제가 만든 두 가지 버전의 Dockerfile을 공유하겠습니다. 하나는 개발 단계/서버 테스트용으로 확실하고 기본적인 단일 단계 버전이며, 다른 하나는 개발 및 프로덕션 모두에 사용할 수 있는 docker-compose.yml과 함께 다중 단계 버전입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n단계별 DockerFile\n\n아래는 파일이 어떻게 생겼는지를 보여주고, 한 줄씩 내가 한 작업을 설명하겠어.\n\n```js\nFROM node:18\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nEXPOSE 3000\nCMD npm run dev\n```\n\n먼저, Docker에 공식 Docker Node 이미지 버전 18을 사용하도록 From node:18을 통해 설정한다. FROM 뒤에 오는 내용을 변경하여 사용하고 싶은 이미지로 변경할 수 있다. 지원되는 목록은 여기에서 확인할 수 있다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nWORKDIR은 이후 명령의 컨텍스트를 설정합니다. 이름은 원하는 대로 지정할 수 있어요. 저는 제 앱을 app이라고 부르겠습니다. 그리고 package.json과 package-lock.json 파일을 컨테이너로 복사한 다음 npm install을 실행하여 모든 종속성을 설치합니다.\n\n그 다음, 프로젝트의 모든 코드(현재 루트 디렉토리)를 WORKDIR로 복사합니다. 제 경우에는 /app이에요.\n\nExpose 3000은 컨테이너에게 앱이 3000 포트에서 실행된다는 것을 알려줍니다.\n\n모든 설정을 끝낸 후에는 CMD npm run dev로 컨테이너에게 개발 서버를 시작하도록 요청합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 Dockerfile로 작업을 마쳤습니다!!\n\n우리가 방금 만든 Dockerfile로부터 Docker 이미지를 빌드하려면, 다음 명령어를 실행하세요. 저는 이를 my-app이라고 이름 지었지만, 원하는 대로 변경해도 상관없습니다. 마지막에 .을 꼭 입력하지 않도록 주의해주세요.\n\n```js\ndocker build -t my-app .\n```\n\n이미지가 생성되면, 다음과 같이 실행할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 런 -p 3000:3000 my-app\n```\n\n3000:3000은 앱을 실행할 포트를 지정하는 것이에요. 저는 3000번 포트에서 실행할 거예요. 그리고 http://localhost:3000 에 접속하면 작동 중인 앱을 볼 수 있어요!\n\n다중 단계 DockerFile\n\n이제 다중 단계 Dockerfile을 만들어서 더 빠르고 효율적인 빌드를 할 수 있게 하고, 운영 환경과 개발 환경을 쉽게 전환할 수 있게 될 거예요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 파일에 있는 내용을 모두 삭제하고 다음 내용으로 대체해주세요.\n\n```js\nFROM node:18-alpine as base\nRUN apk add --no-cache g++ make py3-pip libc6-compat\nWORKDIR /app\nCOPY package*.json ./\nEXPOSE 3000\n\nFROM base as builder\nWORKDIR /app\nCOPY . .\nRUN npm run build\n\n\nFROM base as production\nWORKDIR /app\n\nENV NODE_ENV=production\nRUN npm ci\n\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nextjs -u 1001\nUSER nextjs\n\n\nCOPY --from=builder --chown=nextjs:nodejs /app/.next ./.next\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/package.json ./package.json\nCOPY --from=builder /app/public ./public\n\nCMD npm start\n\nFROM base as dev\nENV NODE_ENV=development\nRUN npm install\nCOPY . .\nCMD npm run dev\n```\n\n이 경우 node:18-alpine를 사용하겠습니다. 이것은 기본 이미지보다 훨씬 작습니다. -alpine를 사용하려면 모든 다른 것보다 먼저 파이썬을 설치해야 합니다. 그래서 그 추가 명령(RUN apk add — no-cache g++ make py3-pip libc6-compat)이 있는 것입니다. 공통 설정을 base 단계에 넣어서 나중에 다른 단계에서 재사용할 수 있도록 했습니다. 원하는 경우 단계 이름을 변경하여 as 다음에 오는 내용을 변경할 수 있습니다.\n\n빌더 단계는 사실상 npm run build를 담당합니다. 이 단계는 production 단계에서 이미 존재하지 않은 경우 COPY — from=builder를 시도할 때 호출됩니다. 보세요? 다중 단계 Dockerfile이 유용한 점이 여기에 있습니다. 빌더 단계는 필요할 때만 호출됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희는 프로덕션 단계에서 NODE_ENV를 production으로 설정하고, 이렇게 하면 성능이 세 배 향상된다고 합니다. 그 다음으로, npm ci를 실행합니다. 이것은 npm install 대신에 지속적인 통합을 위해 사용됩니다.\n\n그 후, 보안상의 이유로 앱을 실행할 비루트 사용자를 추가합니다. 제가 게으른 터라 사용자를 nextjs로 그룹을 nodejs로 설정했습니다.\n\n그 후에, 빌더 단계에서 필요한 에셋을 COPY — from=builder를 통해 복사합니다.\n\n마지막으로, npm start를 호출하여 어플리케이션을 시작합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n개발 스테이지에서는 싱글 스테이지 Dockerfile에서 한 것과 기본적으로 똑같은 작업을 하고 있기 때문에 넘어가도록 할게요.\n\ndocker-compose.yml을 생성하기 전에 Dockerfile이 실제로 빌드되는지 확인하고 싶다면 docker build -t my-app . 및 docker run -p 3000:3000 my-app를 실행할 수 있어요. 테스트하고 싶은 스테이지를 주석 처리하는 것을 잊지 마세요. 예를 들어, 프로덕션 스테이지가 성공적으로 빌드되고 실행되는지 확인하려면 FROM base로 시작하는 부분 이후에 오는 모든 것을 주석 처리해주세요.\n\n## Docker Compose\n\nDocker Compose를 사용하면 긴 명령어를 기억할 필요가 없어요. docker-compose build 및 docker-compose up 명령어를 간편하게 사용할 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n루트 디렉토리에 다음 내용을 가진 docker-compose.yml 파일을 추가해주세요.\n\n```js\nversion: '3.8'\nservices:\n  app:\n    image: openai-demo-app\n    build:\n      context: ./\n      target: dev\n      dockerfile: Dockerfile\n    volumes:\n        - .:/app\n        - /app/node_modules\n        - /app/.next\n    ports:\n      - \"3000:3000\"\n```\n\n여기서 `3.8` 버전은 사용할 Docker Compose 버전을 지정합니다. 이 경우에는 한 개의 서비스인 app을 가지고 있지만, 필요에 따라 더 추가할 수 있습니다.\n\nBuild context는 현재 디렉토리를 지정하며, target은 Docker 이미지를 빌드할 단계를 지정합니다. 프로덕션에서 실행하려면 단순히 target:production으로 설정하시면 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nVolume은 호스트의 ./ 로컬 디렉토리의 내용을 Docker 컨테이너의 /app 디렉토리로 복사하도록 지시합니다.\n\n마지막으로, 호스트 머신의 포트 3000을 컨테이너의 포트 3000으로 매핑합니다. 우리는 컨테이너를 빌드할 때 포트 3000을 노출했으며, 우리의 앱도 3000 포트에서 실행될 것입니다.\n\n## Docker Compose로 테스트하기\n\n마침내 Docker 이미지를 빌드하는 과정에 도달했습니다. 더 빠른 빌드를 위해 Docker의 BuildKit 기능을 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nCOMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker-compose build\n```\n\n빌드가 완료되면 이미지를 실행하여 앱을 시작할 수 있어요.\n\n```js\ndocker-compose up\n```\n\n그 다음에 브라우저에서 http://localhost:3000 로 이동하면 앱이 실행 중인 것을 볼 수 있어요!!!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그게 다야!\n\n아래에는 몇 가지 문제가 있었어요. 내 Docker 컨테이너가 작동하지 않거나 문제의 원인을 찾고 싶다면 고생 중일 수도 있으니 빠르게 공유할게요.\n\n# 내가 겪은 문제들\n\n- Python이 명령줄이나 npm 구성에서 설정되지 않았습니다: (맥 사용자로서) python을 경로에 추가하거나 python을 python3로 별칭 지정, python 삭제 및 다시 설치 등 많은 시도를 해보았지만 어떤 것도 제겐 도움이 되지 않았어요. 그래서 제가 찾아낸 두 가지 해결책을 여기에 소개할게요. (1) node:18(또는 기호하는 다른 버전) 대신 node:18-alpine을 사용하세요. (2) 패키지 설치 전에 RUN apk add --no-cache g++ make py3-pip libc6-compat 를 추가하세요.\n- Docker-compose up을 사용할 때 '/app/.next' 디렉토리에서 프로덕션 빌드를 찾을 수 없음: docker run을 통해 이미지를 실행했을 때는 모든 것이 예상대로 작동했습니다. 해결 방법은 Dockerfile에서 CMD npm start 앞에 CMD [\"npm\",\"run\",\"build\"]를 추가하는 것을 제안하는 솔루션이 있지만, 이렇게 하면 두 개의 CMD를 허용하지 않는 오류가 발생했어요. 제 해결책은 Docker-compose.yml의 volumes에 — /app/.next를 추가하는 것이었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n오늘은 여기까지입니다!\n\n읽어 주셔서 감사합니다! 이 팁들이 도움이 되길 바랍니다!\n","ogImage":{"url":"/assets/img/2024-06-23-DockerizeaNextjsApp_0.png"},"coverImage":"/assets/img/2024-06-23-DockerizeaNextjsApp_0.png","tag":["Tech"],"readingTime":10},{"title":"Docker, Uptime Kuma, 그리고 Traefik을 사용하여 웹사이트 모니터링 하는 방법","description":"","date":"2024-06-23 00:46","slug":"2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite","content":"\n<img src=\"/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_0.png\" />\n\n# 소개\n\n이 문서에서는 Docker/Docker Swarm을 사용하여 로컬 PC 또는 서버에서 웹사이트 모니터링을 설정하는 방법을 보여드리려고 합니다. prometheus, node-exporter, 또는 graphana와 같이 복잡한 모니터링 스택 대신에 NodeJs와 Vue로 작성된 가벼운 대안인 Uptime Kuma를 소개할 예정입니다.\n\n이 프로젝트는 오픈 소스로 GitHub에서 찾을 수 있습니다: https://github.com/louislam/uptime-kuma\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 대안을 사용하게 된 중요한 속성들은 다음과 같아요:\n\n- UI가 아주 멋져요!\n- Docker/Docker Swarm을 사용한 아주 쉬운 설정\n- 매우 간편한 구성\n- Discord, Slack, 이메일 (SMTP) 등을 통한 알림. 전체 목록을 보려면 여기를 클릭하세요.\n\n# 준비 사항\n\nUptime Kuma를 서버 또는 로컬 머신에서 실행하려면 환경을 준비해야 해요. 저는 Traefik을 Docker Swarm에서 실행되는 리버스 프록시로 실행하여 Docker Compose로 배포하는 것을 좋아해요. Traefik을 사용하면 단일 Compose 파일을 만들어서 서비스를 배포하고 도메인에 대한 Let's Encrypt 인증서를 자동으로 발급할 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 두 가지 전제 조건을 간단히 설명한 후, Uptime Kuma를 배포하는 방법을 보여드릴 거에요.\n\n## Docker\n\nDocker는 다양한 종류의 애플리케이션을 개발하고 배포하며 실행하기 위해 널리 사용되는 플랫폼이에요. 이를 통해 인프라를 애플리케이션에서 분리하여 한 기계에서 다른 기계로 소프트웨어를 빠르게 전달할 수 있게 해줘요.\n\nDocker를 사용하여 소프트웨어를 구현하면 흔한 \"내 컴퓨터에서는 작동하는데\" 같은 인프라 문제를 효과적으로 해결할 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n'img' 태그를 Markdown 형식으로 변경해주세요.\n\n![](/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_1.png)\n\n시스템이나 서버에 Docker를 설치하려면 docker.com의 공식 튜토리얼을 따라 진행하세요.\n\n로컬 머신에 Uptime Kuma만 호스팅하고 싶고 Windows를 사용하며 Docker Desktop을 설치할 수 없다면 이 가이드를 따라 진행할 수 있습니다:\n\n## Traefik\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nTraefik이란 무엇인가요?\n\nTraefik은 도커 환경에서 배포된 서비스로 들어오는 요청을 전달하는 데 사용됩니다. 또한 Traefik이 관리하는 모든 도메인에 대해 자동으로 Let’s Encrypt SSL 인증서를 생성할 수 있는 기능이 있습니다.\n\n도커 환경에서 로컬 Traefik 서비스를 설정하려면 이 도커 Compose 파일을 다운로드하고 다음 명령어를 실행하면 됩니다:\n\n1. Traefik에서 사용되는 외부 네트워크를 생성하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. 필요한 변수 내보내기\n\n3. 컨테이너 시작하기\n\n트라픽 대시보드에 접속하려면 https://dashboard.yourdomain.de 로 이동하고 다음으로 로그인하세요:\n\n```js\nusername: devadmin;\npassword: devto;\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 스웜 모드 환경에서 Traefik을 배포하는 방법은 다음과 같습니다. 이 도커 Compose 파일을 다운로드하고 1단계와 2단계를 수행한 후 다음과 같이 배포하면 됩니다:\n\nTraefik에 대한 깊은 이해를 얻으려면 다음 기사에서 도커 내에서 배포하는 방법과 도커 스웜 모드 내에서 배포하는 방법을 읽어보실 수 있습니다.\n\n# Docker를 사용하여 Uptime Kuma 배포하기\n\nUptime Kuma를 배포하는 방법은 세 가지 환경에서 가능합니다: 로컬에서 도커로, 도커와 Traefik을 사용하여 서버에서, 그리고 도커를 실행하는 서버 클러스터에서 Traefik과 함께.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 로컬 Docker 환경에서 배포하기\n\nUptime Kuma를 테스트하는 가장 쉬운 방법은 새로운 Docker Compose 파일을 만들어 다음 내용을 붙여넣어 자신의 장치에서 로컬로 실행하는 것입니다.\n\n이 Docker Compose 파일은 공식 Uptime Kuma 이미지를 사용하여 포트를 1337로 매핑하고 Uptime Kuma의 데이터 폴더를 저장하기 위해 공유 폴더를 사용합니다.\n\n폴더로 이동하여 Docker Compose 파일을 실행하려면 다음과 같이 실행하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 명령이 완료되면 https://localhost:1337/을 열어 Uptime Kuma 인스턴스에 액세스할 수 있습니다.\n\n## Traefik을 사용하여 Docker/Docker Swarm으로 원격으로 배포\n\n고유한 서버를 실행하고 Docker를 사용하여 단일 서버를 실행하거나 Docker의 스웜 모드로 서버 클러스터를 실행하고 있는 경우 Docker Compose 파일을 업데이트해야 합니다.\n\n당신이 나의 튜토리얼에 설명된대로 Traefik 인스턴스를 설정했다고 가정하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n새로운 Docker Compose 파일을 만들고 다음 내용을 붙여넣으세요:\n\n이 Docker Compose 파일을 배포하기 전에 Traefik을 위한 것처럼 도메인을 내보내야 합니다:\n\n다른 설정을 필요에 맞게 조정하세요(Domain, container_name, network 및 volumes) 그리고 Uptime Kuma를 배포하세요:\n\nTraefik을 사용하는 Docker in Docker Swarm 모드의 경우,이 Compose 파일을 사용하여 데이터를 저장하고 싶은 노드에 monitor이름의 새로운 노드 레이블을 작성하고 PRIMARY_DOMAIN을 내보낸 후 아래 명령어로 배포하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n몇 초/분 후에 Uptime Kuma가 성공적으로 배포되어서 https://monitor.PRIMARY_DOMAIN/에서 인스턴스에 액세스할 수 있게 됩니다.\n\n# Uptime Kuma 구성\n\nUptime Kuma가 성공적으로 배포된 후 처음으로 액세스하면 안전한 관리자 비밀번호를 구성해야 합니다.\n\n작업을 완료하신 후에는 이 버튼을 눌러 첫 번째 모니터를 추가해보세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_2.png](/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_2.png)\n\n새로운 UI가 나타나며 특정 동작을 정의할 수 있습니다:\n\n![2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_3.png](/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_3.png)\n\n이제 첫 번째 모니터를 만들었으므로, Discord, 이메일 또는 기타 어떤 유형의 알림도 구성해야 합니다. 모니터가 예상대로 작동하지 않을 때마다 알림을 받게 됩니다. 이 과정은 보통 매우 쉽고 다음으로 설명될 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 디스코드 알림 설정하기 (그리고 슬랙, 팀즈)\n\n디스코드 알림 (또는 팀즈, 슬랙)을 활성화하려면 Uptime Kuma가 경고를 보낼 서버와 채널에 대한 디스코드 (또는 팀즈, 슬랙) 웹훅이 필요합니다. 디스코드에서는 이를 '서버 설정 - 통합 - 웹훅 생성'을 통해 얻을 수 있습니다. 웹훅을 만든 후에 해당 웹훅 URL을 복사합니다. 팀즈나 슬랙에서도 거의 비슷하게 작동해야 합니다.\n\n그런 다음, 모니터를 편집하고 \"알림 설정\"을 누릅니다. 나타나는 대화 상자에서 디스코드 (또는 팀즈, 슬랙)을 선택하고 친숙한 이름을 지어주고 웹훅 URL을 붙여넣습니다. 디스코드의 경우 UI는 다음과 같이 보이어야 합니다:\n\n![디스코드 설정 화면](/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n**테이블 태그를 Markdown 형식으로 변경하면 됩니다.**\n\n기존 모니터에도 적용하거나 새로운 모니터의 기본 설정으로 지정할 수 있습니다.\n\n## 이메일 알림 설정하기\n\n만약 디스코드, 팀즈, 또는 슬랙 알림을 원치 않는다면, 이메일 알림을 생성할 수도 있습니다. 설정하기 위해 이메일 계정의 호스트명, 포트, 사용자명, 비밀번호, 보내는/받는 이메일, 그리고 인증 방법 (StartTLS 또는 SSL)이 필요합니다. 또한 새로운 모니터의 기본 설정으로 지정하거나 기존 모니터에 추가할 수도 있습니다.\n\n# 마지막으로\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 Uptime Kuma를 로컬 또는 Docker 또는 Docker Swarm 환경에 Traefik 역방향 프록시와 함께 설치하는 방법을 보여드렸어요. 이 가벼운 도구는 설정이 쉽고 다양한 구성 옵션을 제공하기 때문에 어떤 웹사이트든 감시하는 데 아주 좋아요. 소프트웨어 개발자나 블로그 호스터라면 Uptime Kuma를 추천드릴게요. 이를 통해 서비스나 블로그가 제대로 작동하는지 확인할 수 있어요.\n\n이 튜토리얼은 여기까지에요. 이제 설정할 수 있을 거에요. 여전히 궁금한 점이 있다면 댓글 섹션에 질문해 주세요.\n\n이 기사를 즐겁게 읽었다면 소중한 생각을 댓글로 남겨주십시오! 피드백을 듣고 싶어요.\n\n누구에게든 이 기사를 공유하고 제 개인 블로그, LinkedIn, Twitter 및 GitHub에서 연락해 주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 글은 제 블로그에도 게재되었습니다: [https://www.paulsblog.dev/use-docker-uptime-kuma-and-traefik-to-monitor-your-website/](https://www.paulsblog.dev/use-docker-uptime-kuma-and-traefik-to-monitor-your-website/)\n","ogImage":{"url":"/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_0.png"},"coverImage":"/assets/img/2024-06-23-UseDockerUptimeKumaandTraefikToMonitorYourWebsite_0.png","tag":["Tech"],"readingTime":10},{"title":"스프링 부트 마이크로서비스 도커라이징 방법 단계별 가이드","description":"","date":"2024-06-23 00:44","slug":"2024-06-23-HowtoDockerizeSpringBootMicroservicesAStep-by-StepGuide","content":"\n![이미지](/assets/img/2024-06-23-HowtoDockerizeSpringBootMicroservicesAStep-by-StepGuide_0.png)\n\n현대 소프트웨어 개발 환경에서, 마이크로서비스 아키텍처는 확장성, 유연성 및 유지 보수성 때문에 상당한 인기를 얻었습니다. 선도적인 컨테이너화 플랫폼인 Docker는 각 서비스를 위한 격리된 환경을 제공하여 지속적인 통합 및 배포 (CI/CD)를 용이하게 하고, 서로 다른 환경에서 일관된 동작을 보장함으로써 이 아키텍처를 보완합니다. 이 글에서는 마이크로서비스의 도커화 과정을 안내하며, 주요 개념, 모범 사례 및 실용적인 단계를 강조할 것입니다.\n\n# 마이크로서비스란?\n\n마이크로서비스 아키텍처는 거대한 단일 응용 프로그램을 작은, 독립적인 서비스로 분해하여 개발, 배포 및 확장이 독립적으로 가능한 아키텍처입니다. 각 마이크로서비스는 특정 기능을 담당하고 다른 서비스와 API를 통해 통신합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Microservices에 Docker를 사용하는 이유?\n\nDocker 컨테이너는 Microservices에 다음과 같은 많은 이점을 제공합니다:\n\n- 격리성: 각 Microservice는 자체 컨테이너에서 실행되어 프로세스와 리소스를 격리합니다.\n- 일관성: Docker 컨테이너는 개발, 테스트 및 프로덕션 환경에서 응용 프로그램이 동일하게 작동함을 보장합니다.\n- 이식성: Docker 이미지는 Docker를 지원하는 모든 플랫폼에서 실행할 수 있습니다.\n- 확장성: 컨테이너는 수요에 따라 쉽게 확장하거나 축소할 수 있습니다.\n- 간편한 CI/CD: Docker는 CI/CD 파이프라인과 잘 통합되어 자동 빌드, 테스트 및 배포를 가능하게 합니다.\n\n# 준비 사항\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Docker 및 Docker Compose에 대한 기본적인 이해도\n- 로컬에서 설정된 Spring Boot 마이크로서비스 프로젝트\n- 공식 Docker 웹사이트에서 다운로드할 수 있는 본인의 컴퓨터에 Docker가 설치되어 있어야 합니다.\n\n# Spring Boot 마이크로서비스를 Docker화하는 단계\n\n- 각 마이크로서비스에 대해 Dockerfile 작성\n\n각 Spring Boot 마이크로서비스에 대해 프로젝트의 루트 디렉토리에 Dockerfile을 생성해야 합니다. 다음은 Spring Boot 애플리케이션을 위한 Dockerfile 예시입니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n쿠폰 서비스용:\n\n```js\n# 공식 OpenJDK 런타임을 부모 이미지로 사용\nFROM openjdk:17-alpine\n\n# JAR 파일을 컨테이너로 복사\nADD target/couponservice-0.0.1-SNAPSHOT.jar couponservice-0.0.1-SNAPSHOT.jar\n\n# 애플리케이션 실행\nENTRYPOINT [ \"java\",\"-jar\",\"couponservice-0.0.1-SNAPSHOT.jar\" ]\n```\n\n상품 서비스용:\n\n```js\n# 공식 OpenJDK 런타임을 부모 이미지로 사용\nFROM openjdk:17-alpine\n\n# JAR 파일을 컨테이너로 복사\nADD target/productservice-0.0.1-SNAPSHOT.jar productservice-0.0.1-SNAPSHOT.jar\n\n# 애플리케이션 실행\nENTRYPOINT [ \"java\",\"-jar\",\"productservice-0.0.1-SNAPSHOT.jar\" ]\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. 다음 명령을 사용하여 메이븐을 통해 두 개의 마이크로서비스를 빌드하세요:\n\n```js\nmvn clean package -DskipTests\n```\n\n3. 도커 이미지 빌드하기\n\n도커 파일을 포함하는 디렉토리로 이동한 후 도커 이미지를 빌드하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 build -f Dockerfile -t coupon_app .\n```\n\n```js\n도커 build -f Dockerfile -t product_app .\n```\n\n4. 데이터베이스 연결을 위한 MySQL 컨테이너 설정합니다.\n\n```js\n도커 run -d -p 6666:3306 --name=docker-mysql --env=\"MYSQL_ROOT_PASSWORD=test1234\" --env=\"MYSQL_DATABASE=mydb\" mysql\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n5. 다음 명령어를 사용하여 Docker 이미지를 실행하십시오:\n\n- MySQL 이미지를 실행하고 단일 명령어로 쿼리를 실행하십시오.\n\n```js\ndocker exec -i docker-mysql mysql -uroot -ptest1234 mydb <tables.sql\n```\n\n2. 다음 명령어를 사용하여 두 마이크로서비스를 실행하십시오.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n도커 실행 -t --name=coupon-app --link docker-mysql:mysql -p 10555:9091 coupon_app\n```\n\n```js\n도커 실행 -t --link docker-mysql:mysql --link coupon-app:coupon_app -p 10666:9090 product_app\n```\n\n6. Postman을 사용하여 애플리케이션 테스트하기.\n\n엔드포인트를 호출하여 Postman을 사용하여 애플리케이션을 테스트할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-HowtoDockerizeSpringBootMicroservicesAStep-by-StepGuide_1.png\" />\n\n<img src=\"/assets/img/2024-06-23-HowtoDockerizeSpringBootMicroservicesAStep-by-StepGuide_2.png\" />\n\n데모에 사용된 마이크로서비스의 소스 코드는 제 GitHub 저장소에서 확인하실 수 있습니다.\n\n# 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSpring Boot 마이크로서비스를 도커화하면 개발, 배포 및 확장 프로세스를 간편하게 만들 수 있어요. 각 마이크로서비스를 위한 도커 파일을 만들고 도커의 환경 일관성 및 격리 기능을 활용함으로써 견고하고 확장 가능한 마이크로서비스 아키텍처를 구축할 수 있어요. 이 안내서에 나온 단계에 따라 Spring Boot 마이크로서비스를 도커화하고 컨테이너화의 모든 잠재력을 활용해 보세요.\n","ogImage":{"url":"/assets/img/2024-06-23-HowtoDockerizeSpringBootMicroservicesAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-06-23-HowtoDockerizeSpringBootMicroservicesAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":6},{"title":"Poetry로 초고속 Python Docker 빌드 하는 방법 ","description":"","date":"2024-06-23 00:42","slug":"2024-06-23-BlazingfastPythonDockerbuildswithPoetry","content":"\n## 느린 번거로운 도커 빌드를 원활한 작업으로 바꿀 수 있는 방법\n\n![image](/assets/img/2024-06-23-BlazingfastPythonDockerbuildswithPoetry_0.png)\n\n프로젝트의 도커 이미지를 빌드하는 것은 일반적으로 재현 가능하고 결정론적인 방식으로 종속성을 설치하는 작업을 포함합니다. 파이썬 커뮤니티에서 Poetry는 이를 달성하기 위한 가장 확실한 도구 중 하나입니다. 그러나 도커 빌드에서 Poetry를 최적으로 활용하지 않으면 성능이 저하되고 긴 빌드 시간으로 개발자 생산성이 저하될 수 있습니다.\n\n본 문서는 이미 Poetry와 도커, 특히 도커 레이어 캐싱 작동 방식에 대해 익숙한 독자들을 가정하고 있으며 빌드를 최적화하기 위한 방법을 찾고 있는 독자를 위해 작성되었습니다. 이 글은 각 최적화의 영향을 이해할 수 있도록 순진한 해결책부터 최적화된 해결책까지 구조화되어 있습니다. 소개는 여기까지, 이제 몇 개의 Dockerfile을 살펴보겠습니다! 💪\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 0. 프로젝트 구조\n\n우리는 이야기할 작은 프로젝트를 사용해 볼까요? 그 이름은 제가 본 산 중에서 최고인 알나푸르나 산으로 임의로 지었습니다 ⛰ 극소 프로젝트 구성은 pyproject.toml, 관련된 poetry.lock, 코드 및 Dockerfile을 포함할 것입니다.\n\n```js\n.\n├── Dockerfile\n├── README.md\n├── annapurna\n│   ├── __init__.py\n│   └── main.py\n├── poetry.lock\n└── pyproject.toml\n```\n\n단순함을 위해, 유명한 fastapi 웹 서버를 poetry add fastapi를 통해 설치하고, 제 프로젝트에서 일반적으로 사용하는 몇 가지 린터들을 설치해보았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n[tool.poetry]\nname = \"annapurna\"\nversion = \"1.0.0\"\ndescription = \"\"\nauthors = [\"Riccardo Albertazzi <my@email.com>\"]\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.11\"\n\nfastapi = \"^0.95.1\"\n\n[tool.poetry.group.dev.dependencies]\nblack = \"^23.3.0\"\nmypy = \"^1.2.0\"\nruff = \"^0.0.263\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n## 1. The naive approach 😐\n\nWhat our Docker build needs to do is having Python and Poetry installed, getting our code, installing the dependencies and setting the project’s entrypoint. This is exactly what we are doing in here:\n\n```js\nFROM python:3.11-buster\n\nRUN pip install poetry\n\nCOPY . .\n\nRUN poetry install\n\nENTRYPOINT [\"poetry\", \"run\", \"python\", \"-m\", \"annapurna.main\"]\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 간단한 Dockerfile은 일을 해내고, 단순한 `docker build .` 명령으로 이미 동작하는 이미지를 얻을 수 있습니다. 실제로 이것은 자습서와 오픈 소스 프로젝트에서 보게 되는 전형적인 Dockerfile로, 이해하기 쉽기 때문입니다. 그러나 프로젝트가 성장하면 번거로운 빌드와 거대한 Docker 이미지로 이끌게 될 것입니다. 저의 결과 Docker 이미지는 실제로 1.1GB입니다! 우리가 볼 최적화는 캐싱을 활용하고 최종 이미지 크기를 줄이는 방향으로 이루어집니다.\n\n## 2. 워밍업 🚶\n\n워밍업을 위한 몇 가지 개선 사항부터 시작해봅시다:\n\n- Poetry 버전을 고정하세요. Poetry는 마이너 버전 간에 중요한 변경 사항이 포함될 수 있으므로 새 버전이 출시될 때 빌드가 갑자기 실패하지 않도록 하려면 로컬에서 사용 중인 버전과 동일한 버전으로 명확히 고정해야 합니다.\n- 필요한 데이터만 COPY하세요. 이렇게 하면 예를 들어 로컬 가상 환경(.venv에 위치)의 불필요한 복사를 피할 수 있습니다. README.md 파일이 없으면 Poetry가 경고를 표시할 것이지만(저는 이 선택을 공유하지 않습니다), 따라서 비어있는 파일을 생성합니다. 로컬 파일을 복사할 수도 있지만 수정할 때마다 Docker 레이어 캐싱을 효과적으로 방지하게 됩니다.\n- 개발 의존성을 설치하지 마세요. 실제로 프로덕션 환경에서는 린터(linters)와 테스트 스위트(test suites)가 필요하지 않으므로 `poetry install --without dev`로 개발 의존성을 설치하지 마세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nFROM python:3.11-buster\n\nRUN pip install poetry==1.4.2\n\nWORKDIR /app\n\nCOPY pyproject.toml poetry.lock ./\nCOPY annapurna ./annapurna\nRUN touch README.md\n\nRUN poetry install --without dev\n\nENTRYPOINT [\"poetry\", \"run\", \"python\", \"-m\", \"annapurna.main\"]\n```\n\n이로써 이미 1.1GB에서 959MB로 줄었습니다. 그리 많지는 않지만, 성실하게 작업한 결과네요.\n\n# 3. Poetry 캐시 정리 🧹\n\n기본적으로 Poetry는 다운로드한 패키지를 캐시하여 이후의 설치 명령에 재사용할 수 있게 합니다. 하지만 우리는 Docker 빌드에서 이를 신경쓰지 않아도 됩니다 (그렇죠?) 그래서 중복 저장소를 제거할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 시는 --no-cache 옵션도 지원하므로 왜 사용하지 않는 걸까요? 나중에 이에 대해 알아보겠죠 ;)\n- 캐시 폴더를 제거할 때는 동일한 RUN 명령어에서 수행해야 합니다. 별도의 RUN 명령어에서 수행하면 캐시는 이전 Docker 레이어 (poetry install을 포함하는 레이어)의 일부로 남아 있어서 최적화가 무용지물이 될 수 있습니다.\n\n이 작업을 수행하는 동안 빌드의 결정적 특성을 더 강화하기 위해 몇 가지 Poetry 환경 변수를 설정합니다. 그 중에서도 가장 논란이 되는 것은 POETRY_VIRTUALENVS_CREATE=1입니다. 도커 컨테이너 내에서 가상 환경을 만들고 싶은 이유가 뭘까요? 솔직히 이 플래그를 비활성화하는 것보다 이 솔루션을 선호합니다. 이 플래그를 비활성화하는 것보다 환경이 가능한 한 격리되고, 무엇보다도 설치가 시스템 Python이나 심지어 Poetry 자체와 충돌하지 않도록 해줍니다.\n\n```js\nFROM python:3.11-buster\n\nRUN pip install poetry==1.4.2\n\nENV POETRY_NO_INTERACTION=1 \\\n    POETRY_VIRTUALENVS_IN_PROJECT=1 \\\n    POETRY_VIRTUALENVS_CREATE=1 \\\n    POETRY_CACHE_DIR=/tmp/poetry_cache\n\nWORKDIR /app\n\nCOPY pyproject.toml poetry.lock ./\nCOPY annapurna ./annapurna\nRUN touch README.md\n\nRUN poetry install --without dev && rm -rf $POETRY_CACHE_DIR\n\nENTRYPOINT [\"poetry\", \"run\", \"python\", \"-m\", \"annapurna.main\"]\n```\n\n# 4. Installing dependencies before copying code 👏\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금까지 잘 진행되고 있지만,\nDocker 빌드가 여전히 매우 고통스러운 문제를 겪고 있어요: 코드를 수정할 때마다 의존성을 다시 설치해야 합니다! 이것은 우리가 코드를 COPY(이것이 Poetry가 프로젝트를 설치하는 데 필요한 것)하기 전에 RUN poetry install 명령을 실행하기 때문입니다. Docker 레이어 캐싱이 작동하는 방식 때문에 COPY 레이어가 무효화될 때마다 다음 레이어도 다시 빌드해야 합니다. 프로젝트가 점점 커질수록 코드 한 줄을 변경하더라도 매우 지루하고 빌드 시간이 길어질 수 있습니다.\n\n해답은 Poetry에 가상 환경을 빌드하는 데 필요한 최소한의 정보를 제공하고 나중에 코드베이스를 COPY 하는 것입니다. 이를 위해 --no-root 옵션을 사용하여 현재 프로젝트를 가상 환경에 설치하지 않도록 지시할 수 있어요.\n\nFROM python:3.11-buster\n\nRUN pip install poetry==1.4.2\n\nENV POETRY_NO_INTERACTION=1 \\\n POETRY_VIRTUALENVS_IN_PROJECT=1 \\\n POETRY_VIRTUALENVS_CREATE=1 \\\n POETRY_CACHE_DIR=/tmp/poetry_cache\n\nWORKDIR /app\n\nCOPY pyproject.toml poetry.lock ./\nRUN touch README.md\n\nRUN poetry install --without dev --no-root && rm -rf $POETRY_CACHE_DIR\n\nCOPY annapurna ./annapurna\n\nRUN poetry install --without dev\n\nENTRYPOINT [\"poetry\", \"run\", \"python\", \"-m\", \"annapurna.main\"]\n\n이제 애플리케이션 코드를 수정해보세요. 마지막 3개의 레이어만 다시 계산되는 것을 볼 수 있어요. 빌드가 엄청나게 빨라졌죠! 🚀\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 가상 환경에서 프로젝트를 설치하려면 추가적인 실행할 `poetry install --without dev` 명령이 필요합니다. 이는 사용자 정의 스크립트를 설치하는 데 유용할 수 있습니다. 프로젝트에 따라 이 단계가 필요하지 않을 수도 있습니다. 어쨌든, 이미 프로젝트 종속성이 설치되어 있기 때문에 이 계층 실행은 매우 빠를 것입니다.\n\n# 5. 다중 단계 Docker 빌드 사용 🏃‍♀\n\n지금까지 빌드는 빠르지만 여전히 큰 Docker 이미지가 생성됩니다. 이 문제를 해결하기 위해 다중 단계 빌드를 사용할 수 있습니다. 최적화는 올바른 작업에 대해 올바른 베이스 이미지를 사용함으로써 달성됩니다:\n\n- Python buster는 개발 의존성이 포함된 큰 이미지이며, 이를 사용하여 가상 환경을 설치할 것입니다.\n- Python slim-buster는 Python을 실행하는 데 필요한 최소한의 종속성만 포함된 작은 이미지이며, 우리 애플리케이션을 실행하는 데 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다단계 빌드를 통해 한 단계에서 다른 단계로 정보를 전달할 수 있습니다. 특히 구축 중인 가상 환경입니다. 주목해 주세요:\n\n- 실행 단계에는 심지어 Poetry가 설치되어 있지 않습니다. 사실 Python 응용 프로그램이 가상 환경이 구축된 후에 실행하는 데 불필요한 종속성이에요. 환경 변수 (예: VIRTUAL_ENV 변수)를 조정하여 Python이 올바른 가상 환경을 인식하도록 만들어주기만 하면 돼요.\n- 간단하게 말씀드리면, 장난감 프로젝트에는 필요 없으므로 두 번째 설치 단계 (RUN poetry install --without dev)를 제거했어요. 하지만 실행 이미지에 여전히 한 번의 명령어로 추가할 수 있어요: RUN pip install poetry && poetry install --without dev && pip uninstall poetry.\n\nDockerfile이 복잡해지면 새로운 빌드 백엔드인 Buildkit을 사용하는 것도 제안드려요. 빠르고 안전한 빌드를 원한다면 그것을 사용하는 게 좋아요.\n\n```js\nDOCKER_BUILDKIT=1 docker build --target=runtime .\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 빌더 이미지, 가상 환경을 구축하는 데 사용됨\n\nFROM python:3.11-buster as builder\n\nRUN pip install poetry==1.4.2\n\nENV POETRY_NO_INTERACTION=1 \\\n POETRY_VIRTUALENVS_IN_PROJECT=1 \\\n POETRY_VIRTUALENVS_CREATE=1 \\\n POETRY_CACHE_DIR=/tmp/poetry_cache\n\nWORKDIR /app\n\nCOPY pyproject.toml poetry.lock ./\nRUN touch README.md\n\nRUN poetry install --without dev --no-root && rm -rf $POETRY_CACHE_DIR\n\n# 실행 이미지, 가상 환경만 실행하는 용도로 사용\n\nFROM python:3.11-slim-buster as runtime\n\nENV VIRTUAL_ENV=/app/.venv \\\n PATH=\"/app/.venv/bin:$PATH\"\n\nCOPY --from=builder ${VIRTUAL_ENV} ${VIRTUAL_ENV}\n\nCOPY annapurna ./annapurna\n\nENTRYPOINT [\"python\", \"-m\", \"annapurna.main\"]\n\n결과는? 런타임 이미지가 6배나 작아졌어요! 6배나! ` 1.1 GB에서 170 MB로 줄었어요.\n\n# 6. Buildkit 캐시 마운트 ⛰\n\n이미 작은 Docker 이미지와 코드 변경 시 빠른 빌드를 얻었는데, 더 얻을 수 있는 게 있을까요? 음… 의존성이 변경되었을 때도 빠른 빌드를 얻을 수 있어요 😎\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 최종 팁은 다른 기능들과 비교했을 때 상대적으로 최근에 도입된 것이기 때문에 많은 사람들이 알지 못합니다. 이 기능은 Buildkit 캐시 마운트를 활용하며, 기본적으로 Buildkit에게 캐싱 목적으로 폴더를 마운트하고 관리하도록 지시합니다. 흥미로운 점은 이러한 캐시가 빌드 간에 지속될 것이라는 것입니다!\n\n이 기능을 Poetry 캐시와 연결하면 (이젠 왜 캐싱을 유지하길 원했는지 알겠죠?) 사실상 빌드할 때마다 의존성 캐시를 얻게 됩니다. 동일한 환경에서 여러 번 이미지를 빌드할 때 의존성 빌드 단계가 빠르게 완료될 것입니다.\n\nPoetry 캐시가 설치 후에 지워지지 않는 점에 유의해야 합니다. 이렇게 함으로써 캐시를 저장하고 빌드 간에 다시 사용할 수 있게 됩니다. 빌드된 이미지에 관리된 캐시를 영속화하지 않을 것이므로 (게다가 런타임 이미지도 아닙니다.) 이것은 괜찮은 접근입니다.\n\n```js\nFROM python:3.11-buster as builder\n\nRUN pip install poetry==1.4.2\n\nENV POETRY_NO_INTERACTION=1 \\\n    POETRY_VIRTUALENVS_IN_PROJECT=1 \\\n    POETRY_VIRTUALENVS_CREATE=1 \\\n    POETRY_CACHE_DIR=/tmp/poetry_cache\n\nWORKDIR /app\n\nCOPY pyproject.toml poetry.lock ./\nRUN touch README.md\n\nRUN --mount=type=cache,target=$POETRY_CACHE_DIR poetry install --without dev --no-root\n\nFROM python:3.11-slim-buster as runtime\n\nENV VIRTUAL_ENV=/app/.venv \\\n    PATH=\"/app/.venv/bin:$PATH\"\n\nCOPY --from=builder ${VIRTUAL_ENV} ${VIRTUAL_ENV}\n\nCOPY annapurna ./annapurna\n\nENTRYPOINT [\"python\", \"-m\", \"annapurna.main\"]\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 최적화의 단점은 무엇일까요? 현재 Buildkit은 캐시 마운트가 매우 CI 친화적이지 않습니다. 왜냐하면 Buildkit은 캐시의 저장 위치를 제어할 수 없기 때문입니다. Buildkit 저장소에서 가장 투표를 많이 받은 오픈 GitHub 이슈라니 놀라운 일이네요 😄\n\n# 요약\n\n이미지를 몇 분 만에 1GB로 만드는 간단하지만 형편없는 Dockerfile을 최적화된 버전으로 변경하여 몇 초 만에 몇 백 MB의 이미지를 생성하는 방법을 살펴보았습니다. 모든 최적화는 주로 몇 가지 Docker 빌드 매니피스를 활용합니다:\n\n- 계층을 작게 유지하여 복사하고 설치하는 항목의 양을 최소화합니다.\n- Docker 레이어 캐싱을 활용하고 캐시 미스를 최대한 줄입니다.\n- 느리게 변경되는 것(프로젝트 의존성)은 빠르게 변경되는 것(애플리케이션 코드)보다 먼저 빌드해야 합니다.\n- Docker 다단계 빌드를 사용하여 런타임 이미지를 최대한 가볍게 만듭니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬 프로젝트를 Poetry로 관리할 때 이러한 원칙을 적용하는 방법을 안내드렸는데, 이와 유사한 원칙은 다른 종속성 관리자(PDM 등) 및 다른 언어에도 적용할 수 있습니다.\n\n빌드가 빨라지고 용량이 작아지는 것을 보고 기쁨의 눈물을 흘리시기를 바랍니다. 추가로 알고 계신 Docker 관련 팁이 있으면 댓글에 남겨주세요! 👋\n","ogImage":{"url":"/assets/img/2024-06-23-BlazingfastPythonDockerbuildswithPoetry_0.png"},"coverImage":"/assets/img/2024-06-23-BlazingfastPythonDockerbuildswithPoetry_0.png","tag":["Tech"],"readingTime":12}],"page":"25","totalPageCount":119,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}