{"pageProps":{"posts":[{"title":"프로메테우스 부하와 카디널리티 대폭 줄이는 방법 필요한 이스티오 레이블만 사용하기","description":"","date":"2024-05-20 17:24","slug":"2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed","content":"\n![image](/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_0.png)\n\n만약 프로메테우스 작업에 노출되어 있다면, 카디널리티 관리가 중요하다는 것을 들어봤을 것입니다. 이것은 관측 구성의 가장 중요한 측면 중 하나이며 시스템 부하를 크게 증가시킬 수 있습니다. Robust Perception에는 왜 이것이 중요한지에 대해 좋은 설명이 있으므로, 왜 이것이 중요한지에 대해 자세히 다루지는 않겠지만, 가장 비용이 많이 드는 메트릭 중 일부를 다룰 수 있도록 메트릭을 조정하는 한 가지 방법을 설명하겠습니다.\n\n구체적으로 이스티오에 대해 이야기할 것입니다. 현재 우리는 v1.18을 사용 중이므로, 참고하셔야 할 필드 중 일부는 이전 버전에서 (또는 신규한 버전에서) 다르게 보일 수 있음을 주의해 주세요. 이 버전에서 예제를 정확히 찾는 데 어려움을 겪어 예제가 유용하게 될 것을 희망합니다.\n\n# 재미있는 부분으로 넘어가보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우선, 우리가 이야기하는 것은 Istio (그리고 간접적으로 Kubernetes)입니다. 우리가 조정하고 있는 구체적인 구성 요소는 Istio Operator입니다.\n익숙하지 않다면, Istio Operator는 istioOperator 사용자 지정 리소스를 관리하는 Kubernetes 컨트롤러입니다.\n그 결과로 Istio Operator는 클러스터에서 Istio 리소스를 만들고 지속적으로 조정합니다.\n이에는 Gateway 리소스, Envoy Filters, Pilot (istiod) 등이 포함됩니다.\n\nIstio 문서에는 우리가 작업하는 기능을 참조하는데, 사용 방법에 대한 예시를 충분히 제공하지 않는 단 한 줄이 있습니다.\n\n그래서 우리는 설정해야 할 istioOperator 매니페스트가 있습니다. 그리고 metric 라벨을 덮어쓰기 위해 tags_to_remove 옵션을 사용한다는 것을 알고 있습니다.\n\n## 참고: Istio 구성을 업데이트할 때는 항상 실제 운영 환경이 아닌 환경에서 테스트해야 합니다.\n\n메트릭 라벨에 대해 이야기하고 있더라도 의도치 않은 영향이 발생할 수 있습니다.\n예를 들어, 사용자 지정 메트릭을 사용하여 스케일링 동작을 정의하는 경우 전체 클러스터의 스케일링 동작을 망가뜨릴 수 있습니다.\n이것은 한 가지 예시일 뿐이지만, 이것을 염두에 두세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_1.png\" />\n\n이제, 실제로 이 구성 변경이 어떻게 보이는지는 명확하지 않습니다. 그래서 여기에 있습니다. 원하는 구성은 inboundSidecar 및 outboundSidecar 속성 모두에 대해 설정됩니다. istiooperator 매니페스트에서 전체 경로는 다음과 같습니다:\n\nspec.telemetry.v2.prometheus.configOverride.inboundSidecar.metrics\n\nspec.telemetry.v2.prometheus.configOverride.outboundSidecar.metrics\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래와 같이 더 완전한 예시를 살펴보세요 (이 이야기에서 필요한 구성만을 간략화하여 포함하였음을 유의해주세요):\n\n```js\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  namespace: ${istio_namespace}\n  name: ${istio_name}\nspec:\n  values:\n    telemetry:\n      enabled: true\n      v2:\n        prometheus:\n          wasmEnabled: true\n          configOverride:\n            inboundSidecar:\n              metrics:\n              - name: requests_total\n                tags_to_remove:\n                - connection_security_policy\n                - destination_cluster\n                - destination_canonical_revision\n                - destination_canonical_service\n                - destination_principal\n                - destination_version\n                - destination_service_name\n                - destination_service_namespace\n                - destination_workload_namespace\n                - source_canonical_service\n                - source_canonical_revision\n                - source_workload_namespace\n                - source_version\n                - source_cluster\n                - source_principal\n              - name: request_bytes\n                tags_to_remove:\n                - destination_cluster\n                - destination_canonical_revision\n                - destination_canonical_service\n                - destination_principal\n                - destination_version\n                - destination_service_name\n                - destination_service_namespace\n                - destination_workload_namespace\n                - source_canonical_service\n                - source_canonical_revision\n                - source_workload_namespace\n                - source_version\n                - source_cluster\n                - source_principal\n                - service\n                - pod\n```\n\n# 이제 무엇을 해야 할까요?\n\nIstio Operator 구성이 업데이트되었습니다. 멋져요! 하지만… 이제 무엇을 해야 할까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아마도 별도의 운영 환경에 액세스할 수 있고 이 업데이트된 매니페스트를 해당 Kubernetes 클러스터에 적용할 수 있을 것으로 예상됩니다. Istio 오퍼레이터 파드의 로그를 추적하여 클러스터 리소스를 동기화하는 것을 확인할 수 있습니다. 구체적으로, Envoy 필터를 업데이트해야 합니다.\n\n만약 구문 오류가 없다면 로그에 다음과 같은 내용이 표시될 것입니다:\n\n```js\n2024-03-18T17:54:21.303088Z info installer 생성된 매니페스트와 캐시 사이에 다음 오브젝트가 다릅니다:\n - ConfigMap:istio-system:istio\n - EnvoyFilter:istio-system:stats-filter-1.16\n - EnvoyFilter:istio-system:tcp-stats-filter-1.16\n - EnvoyFilter:istio-system:stats-filter-1.17\n - EnvoyFilter:istio-system:tcp-stats-filter-1.17\n - EnvoyFilter:istio-system:stats-filter-1.18\n - EnvoyFilter:istio-system:tcp-stats-filter-1.18\n2024-03-18T17:54:21.303181Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/stats-filter-1.16\n- 제거된 리소스를 가지치기 중\n2024-03-18T17:54:21.333325Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/stats-filter-1.17\n2024-03-18T17:54:21.352880Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/stats-filter-1.18\n2024-03-18T17:54:21.372182Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/tcp-stats-filter-1.16\n2024-03-18T17:54:21.392004Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/tcp-stats-filter-1.17\n2024-03-18T17:54:21.409610Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: EnvoyFilter/istio-system/tcp-stats-filter-1.18\n2024-03-18T17:54:21.433381Z info installer 서버 측 적용을 사용하여 오브젝트를 업데이트 중: ConfigMap/istio-system/istio\n```\n\n전체적으로 너무 조심스러운 면이 있다고 생각되어 istio-system 네임스페이스에 있는 모든 것을 재시작하는 기회를 가졌습니다. 이는 모든 게이트웨이 및 istiod를 포함합니다. 이것이 완전히 필요한지는 모르지만, 제가 한 작업입니다. 변경 사항을 적용한 모든 클러스터에서 트래픽을 제거한 다음 변경을 적용하고 다시 시작했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n변경 사항을 적용하는 데 한 가지 더: 제 네임 스페이스 중 일부에서 메트릭 레이블 업데이트가 즉시 적용되지 않는 것을 발견했습니다. 이스티오 프록시 컨테이너가 실제로 Envoy 필터에 변경 사항을 적용하기 전에 다시 시작해야 한다는 이유로 이해됩니다. 100% 사실인지는 모르지만, 다시 말하지만 — 제 네임 스페이스 중 일부에서 제 관측 결과였습니다. 이것이 이례적으로 느껴지지는 않지만, 변경 사항을 할 때 주의하고 확인하는 것이 좋습니다.\n\n# 작별 인사와 예시\n\nKubernetes 클러스터가 서비스로의 매우 많은 요청을 허용한다면, 카디널리티를 줄이는 데 얻게 될 이득은 상당할 것입니다. 레이블을 제대로 검토하는 데 시간이 걸리지만(prometheusRules, 대시보드 등을 확인), 이 작업은 귀중합니다.\n\n고 카디널리티 레이블을 다음과 같이 줄일 때:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n{\n    \"container\":\"istio-proxy\",\n    \"destination_canonical_revision\":\"<dstUID>\",\n    \"destination_canonical_service\":\"<dstApp>\",\n    \"destination_cluster\":\"Kubernetes\",\n    \"destination_service\":\"<dstApp>.<dstApp>-production.svc.cluster.local\",\n    \"destination_service_name\":\"PassthroughCluster\",\n    \"destination_service_namespace\":\"<dstApp>-production\",\n    \"destination_version\":\"<dstUID>\",\n    \"destination_workload\":\"<dstUID>\",\n    \"destination_workload_namespace\":\"<dstApp>-production\",\n    \"instance\":\"<someIP>\",\n    \"job\":\"<appName>\",\n    \"le\":\"0.5\",\n    \"namespace\":\"<appName>-production\",\n    \"pod\":\"<srcUID>-kkd77\",\n    \"reporter\":\"source\",\n    \"request_protocol\":\"http\",\n    \"response_code\":\"200\",\n    \"response_flags\":\"-\",\n    \"service\":\"<appName>\",\n    \"source_app\":\"<appName>\",\n    \"source_canonical_revision\":\"<srcUID>\",\n    \"source_canonical_service\":\"<appName}\",\n    \"source_cluster\":\"Kubernetes\",\n    \"tag\":\"<srcUID>\"\n}\n```\n\n위를 아래의 형식으로 변경해주세요.\n\n```js\n{\n    \"reporter\": \"destination\",\n    \"source_workload\": \"<srcUID>\",\n    \"source_app\": \"<srcApp>\",\n    \"destination_workload\": \"<dstUID>\",\n    \"destination_app\": \"<dstApp>\",\n    \"destination_service\": \"<dstApp>.<dstApp>-production.svc.cluster.local\",\n    \"request_protocol\": \"http\",\n    \"job\":\"<appName>\",\n    \"response_code\": \"200\",\n    \"grpc_response_status\": \"\",\n    \"response_flags\": \"-\",\n    \"le\": \"1\"\n}\n```\n\nS3 저장 비용이 크게 절감되었고, 단 몇 일 만에 압도적인 성능 향상을 볼 수 있었습니다. 또한 Prometheus 인스턴스에서 리소스 사용량이 줄어들었습니다. 다른 말로 — 요청 당 메트릭에서 불필요한 레이블을 식별하고 제거하는 것은 절대적으로 가치 있는 작업입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 작성한 워크스루가 몇몇 프로메테우스 운영자분들께서 관측성 스택을 최적화하는데 도움이 되길 바라요!\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoMassivelyReducePrometheusLoadandCardinalitybyOnlyUsingIstioLabelsYouNeed_0.png","tag":["Tech"],"readingTime":10},{"title":"Kubernetes에서 Jenkins, Docker, Harbor Private Repository 및 ArgoCD를 사용하여 CICD 파이프라인으로 애플리케이션을 빌드하고 배포하는 방법","description":"","date":"2024-05-20 17:22","slug":"2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD","content":"\n오늘날의 소프트웨어 개발 환경에서 고품질 소프트웨어를 효과적으로 생산하기 위해서는 CI/CD 파이프라인이 필수적입니다. 이 글에서는 Jenkins, Docker 이미지, Harbour 프라이빗 저장소, Git 및 ArgoCD를 사용하여 지속적인 통합/배포 파이프라인(CI/CD)을 어떻게 설정하는지 설명합니다. 이를 통해 이미지를 원활하게 생성하고 harbor에 푸시한 다음 쿠버네티스 파드에 자동으로 배포할 수 있습니다.\n\n# 시작하기\n\n일반적인 작업 흐름은 다음과 같습니다.\n\n1. Docker 파일 생성: 먼저 Docker 파일을 만들어야 합니다. 이 파일에서는 DevOps 엔지니어가 애플리케이션에 필요한 환경 및 종속성을 지정합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2. 리포지토리에 코드를 커밋합니다: Git과 같은 리포지토리는 소스 코드와 Docker 파일을 저장합니다. 저는 GitHub에 이 리포지토리를 저장할 것입니다.\n\n3. 젠킨스가 코드를 가져옵니다: 우리는 젠킨스 작업을 시작하여 소스 코드 리포지토리에서 가장 최근의 코드를 가져와 빌드 프로세스를 시작할 수 있습니다.\n\n4. 도커 이미지 빌드 및 푸시: 젠킨스는 Docker 파일을 사용하여 Docker 이미지를 빌드합니다. 젠킨스는 완료된 이미지를 Harbour 개인 리포지토리에 푸시합니다. Harbour 개인 리포지토리를 빌드하는 방법은 이 곳에서 확인할 수 있습니다.\n\n5. 배포 리포지토리 업데이트: 젠킨스는 애플리케이션에 필요한 쿠버네티스 배포 구성이 포함된 배포 파일 리포지토리에 변경 사항을 가합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n6. ArgoCD와 동기화: 모든 것이 최신 상태인지 확인하기 위해 ArgoCD라는 Kubernetes 연속 전달 도구는 배포 소스에서 가장 최근 업데이트를 검색합니다.\n\n7. Kubernetes로 배포: 마지막으로, ArgoCD는 모든 기능이 구성된 것을 보장하고, 이러한 수정 사항을 Kubernetes 클러스터와 동기화하여 응용 프로그램을 지정된 pod에 배포합니다.\n\n## 작업 흐름\n\n# 구현\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 필요한 도구\n\n- Kubernetes 클러스터\n- Docker\n- Jenkins\n- Harbor\n- ArgoCD\n- GitHub\n\n## 1. Docker 파일 생성하기\n\n아래에 언급된 Docker 파일을 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 index.html 파일을 사용하고 있습니다.\n\nGitHub 저장소에서 모든 파일을 찾을 수 있습니다: https://github.com/tanmaybhandge/Harbor_CICD_Pipeline.git\n\n## 2. Jenkins 작업 트리거\n\n저희는 Jenkins에서 두 개의 작업을 구성했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n첫 번째 작업(상위 작업)은 최신 소스 코드를 얻기 위해 GitHub 저장소를 복제하는 작업을 포함합니다. 이 코드에서 Docker 이미지가 생성됩니다. 그런 다음 이미지를 빌드하고 Harbor 프라이빗 레지스트리에 로그인한 후 이미지를 푸시하고 상위 작업의 빌드 번호와 태깅합니다.\n\n이미지가 성공적으로 푸시된 후 상위 작업에 의해 하위 작업이 시작됩니다. 하위 작업은 빌드 번호를 매개변수로 받고 해당 빌드 번호가 될 것입니다. 그런 다음 하위 작업은 이미지 버전을 빌드 번호와 동일하게 수정한 다음 쿠버네티스 배포 파일에 변경 사항을 푸시할 것입니다.\n\n상위 작업과 하위 작업이란 무엇을 의미합니까?\n\n한 Jenkins 작업이 다른 작업을 시작할 때, 프로세스를 시작하는 작업을 상위 작업이라고하고, 결과로 시작되는 작업을 하위 작업이라고합니다. 아래 예시에서 Build_Docker_Image_Push_Harbor은 상위 작업이고 push_image_tag_git은 하위 작업입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 작업: 도커 이미지 빌드 및 Harbor로 푸시\n\n파이프라인 작업 \"Build_Docker_Image_Push_Harbor\"을 생성하고 아래 스크립트를 붙여넣으십시오.\n\n이 Jenkins 파이프라인 스크립트는 GitHub 저장소에서 Docker 이미지를 빌드하고, 해당 이미지를 Harbor 저장소로 푸시하며, 다른 Jenkins 작업 \"push_image_tag_git\"을 트리거하여 GitHub에서 배포 파일을 수정하고 변경 사항을 커밋하는 프로세스를 자동화합니다.\n\n```js\npipeline {\n    agent any\n\n    stages {\n        stage('Build') {\n            steps {\n                // GitHub 저장소에서 코드 가져오기\n                git branch: 'main', url: 'https://github.com/tanmaybhandge/Harbor_CICD_Pipeline.git'\n                sh 'docker build -t library/harbor_cicd_v2 .'\n            }\n        }\n        stage('Harbor로 푸시') {\n            environment {\n                DOCKER_CREDENTIALS = credentials('Harbor')\n            }\n            steps {\n                script {\n                    // 인증 정보를 사용하여 Harbor에 로그인\n                    sh \"docker login -u ${DOCKER_CREDENTIALS_USR} -p ${DOCKER_CREDENTIALS_PSW} 10.1.1.1\"\n\n                    // 이미지에 태그 추가\n                    sh 'docker tag library/harbor_cicd_v2 10.1.1.1/library/harbor_cicd:v${BUILD_NUMBER}'\n\n                    // Harbor로 이미지 푸시\n                    sh 'docker push 10.1.1.1/library/harbor_cicd:v${BUILD_NUMBER}'\n                }\n            }\n        }\n        stage('GitHub 푸시 트리거') {\n            steps {\n                build job: 'push_image_tag_git', wait: true, parameters: [string(name: 'Build_Number_Image', value: \"${BUILD_NUMBER}\")]\n            }\n        }\n    }\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n작업 \"Build_Docker_Image_Push_Harbor\"에서는 선택된 체크박스가 없습니다. 위 스크립트를 그대로 Pipeline 스크립트에 붙여넣기만 하면 됩니다.\n\n![이미지](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_0.png)\n\n## 스테이지\n\n각 스테이지에 대한 설명입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nBuild\n\n- Git Checkout: 지정된 URL에서 저장소를 복제하고 'main' 브랜치를 체크아웃합니다.\n- Docker Build: 저장소의 Dockerfile을 사용하여 Docker 이미지를 빌드하고 library/harbor_cicd_v2로 태깅합니다.\n\nHarbor로 푸시\n\n- 환경 변수: Jenkins에 저장된 'Harbor' ID를 사용하여 Docker 자격 증명을 가져옵니다. 이미 Jenkins에서 Harbor 자격 증명을 만들었습니다 (Harbor의 기본 자격 증명: 사용자 admin, 비밀번호 Harbor12345).\n- Docker Login: 검색한 자격 증명을 사용하여 Harbor 레지스트리에 로그인합니다.\n- Docker Tag: 빌드된 Docker 이미지에 Jenkins 빌드 번호를 포함한 새 태그를 추가합니다. 현재 파이프라인의 태그로 빌드 번호를 사용할 것입니다.\n- Docker Push: 태그가 붙은 Docker 이미지를 지정된 Harbor 저장소로 푸시합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGitHub Push를 트리거합니다.\n\n- 또 다른 작업 트리거: push_image_tag_git 작업을 시작합니다.\n- 매개변수 전달: 현재 빌드 번호를 트리거된 작업에 전송합니다.\n\n## 작업: Push_Image_Tag_Git\n\n이 작업은 GitHub에 호스팅된 Kubernetes 배포 YAML 파일을 수정하여 상위 작업에서 푸시된 새 이미지 태그를 포함시킵니다. 이를 위해 다음이 필요합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Parameterized Trigger Plugin\n- GitHub Plugin\n- push access 권한이 있는 GitHub 자격 증명\n\n작업의 구성은 다음과 같습니다.\n\npush_image_tag_git이라는 이름의 Freestyle 프로젝트를 생성합니다.\n\nA. This project is parameterised 옵션이 선택되었는지 확인한 후, 문자열 매개변수를 적절하게 구성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지1](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_1.png)\n\nB. 소스 코드 관리 섹션에서 Git을 선택하고 저장소 URL을 붙여넣은 다음 GitHub에 권한 및 액세스 권한이 있는 적절한 자격 증명을 선택합니다. 빌드할 브랜치 섹션에 브랜치를 지정하세요. 저는 기본 메인 브랜치를 사용합니다.\n\n![이미지2](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_2.png)\n\nC. 빌드 단계에 실행 셸을 추가하고 다음을 붙여넣으세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 스크립트는 Deployment.yaml 파일에서 이미지 태그를 찾아 해당 버전을 Jenkins 빌드의 ($ 'Build_Number_Image')로 바꿉니다. 또한 Git 커밋을 위해 이메일과 이름을 구성하고, 수정된 모든 파일을 Git 스테이징 영역에 추가하고, 설명적인 메시지로 변경 사항을 커밋합니다.\n\nD. 'Post-Build Actions' 섹션에서 '빌드 성공 시에만 푸시'를 선택한 다음 'Branches' 필드 아래에서 브랜치 이름과 대상 원격 이름을 지정합니다. 저는 브랜치 이름을 'main', 대상 원격 이름을 'origin'으로 지정했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_4.png\" />\n\n## 3. 도커 빌드 작업을 트리거합니다.\n\n수동으로 Build_Docker_Image_Push_Harbor를 트리거하면 아래 작업을 수행하고 Push_image_Tag_Git 작업을 트리거합니다. 두 작업은 다음을 수행합니다.\n\nBuild_Docker_Image_Push_Harbor\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 빌드 단계: GitHub 저장소를 복제하고 Docker 이미지를 빌드합니다.\n- Harbor에 푸시하는 단계: Harbor 레지스트리에 로그인하여 이미지에 빌드 번호를 태깅하고 푸시합니다.\n- GitHub 푸시 트리거 단계: 하향 작업인 Push_image_Tag_Git을 트리거하고 빌드 번호를 매개변수로 전달합니다.\n\nPush_image_Tag_Git\n\n- 쉘 실행: Deployment.yaml 파일에서 이미지 태그를 찾아 이전 젠킨스 빌드에서 지정된 버전으로 대체하고, Git 커밋을 위해 이메일 및 이름을 구성하고, 변경된 모든 파일을 Git 스테이징 영역에 추가하여 설명적인 메시지로 변경 사항을 커밋합니다.\n\n여기 Status 출력 결과입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_5.png\" />\n\n도커 이미지 빌드 및 쿠버네티스 배포 간의 동기화를 보장하기 위해 Deployment.yaml 파일에 지정된 버전이 Build_Docker_Image_Push_Harbor 중에 생성된 빌드 번호를 반영하도록 업데이트됩니다.\n\n<img src=\"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_6.png\" />\n\n## 4. 더 새로운 이미지 버전으로 Pods 다시 생성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nArgoCD 애플리케이션을 설정하여 deployment.yaml 파일을 주시하고 Kubernetes 클러스터에서 모든 것이 동기화되고 최신 상태를 유지하도록 하였습니다. 따라서 우리의 deployment.yaml에서 조정이 발생할 때마다 ArgoCD가 바로 개입하여 손을 거치지 않고도 Kubernetes 배포 환경이 동기화되도록 합니다.\n\n다음은 ArgoCD 애플리케이션의 구성 세부 정보입니다:\n\n- 클러스터: https://kubernetes.default.svc\n- 네임스페이스: webapp\n- 저장소 URL: https://github.com/tanmaybhandge/Harbor_CICD_Pipeline.git\n- 경로: Deployment\n\n쿠버네티스 클러스터에 webapp 네임스페이스를 생성해야 할 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_7.png)\n\n![image](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_8.png)\n\n## 5. 포드에 액세스하기\n\nArgoCD 배포가 모두 정상적으로 보인다면, 배포된 애플리케이션을 테스트할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n노드 포트 서비스를 통해 배포를 노출했습니다. 테스트 목적으로 사용할 수 있습니다. 어플리케이션에 액세스하려면 워커 노드의 IP 주소와 서비스 포트를 사용하십시오. 이 설정을 통해 웹페이지에 쉽게 액세스하여 앱의 기능을 테스트하고 유효성을 검사할 수 있습니다.\n\nMarkdown 형식의 테이블입니다.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: webapp-service\nspec:\n  selector:\n    app: webapp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n```\n\n![웹페이지](/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_9.png)\n\n여기까지 오신 것을 환영합니다! 즐거운 독해가 되었으면 좋겠습니다. 궁금한 점이 있거나 연락하고 싶다면 언제든지 LinkedIn에서 연락해 주세요. 즐거운 하루 보내세요!\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoBuildandDeployApplicationonKuberneteswithCICDPipelineUsingJenkinsDockerHarborPrivateRepositoryandArgoCD_0.png","tag":["Tech"],"readingTime":13},{"title":"Kubernetes에서 JVM 애플리케이션 실행하기 java -jar을 넘어서","description":"","date":"2024-05-20 17:17","slug":"2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar","content":"\n## 쿠버네티스로 오케스트레이션된 컨테이너 환경에서 JVM 애플리케이션을 실행하는 중요한 팁 몇 가지를 발견하세요\n\n[이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_0.png)을 확인해보세요.\n\n## 대상 독자\n\n쿠버네티스 상의 컨테이너 환경에서 JVM 애플리케이션을 실행하고 있고 성능 및 비용을 더 잘 최적화할 수 있을 것 같다고 느끼시나요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여러분은 JVM 애플리케이션을 이러한 환경으로 이전하는 것을 고려 중이지만 효율적으로 어떻게 할지 고민하고 계신가요?\n\n쿠버네티스에서 실행되는 JVM 애플리케이션의 지연시간 및 처리량 문제에 고민하고 계신가요?\n\n또는 간단히 쿠버네티스 환경에서 JVM 애플리케이션을 최적화하는 방법에 대해 더 알고 싶으신가요?\n\n그럼, 이 게시물이 여러분을 위한 것입니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 시작하기 전에, 몇 가지 매우 구체적인 시나리오에서는 아래 제공된 모든 팁이 합리적이지 않을 수 있다는 점을 명확히 해야 합니다. 그러나 대부분의 경우에는 쿠버네티스로 조정된 컨테이너화된 환경 내에서 JVM 내에서 실행 중인 서비스가 있는 사용 사례에서 다음 정보가 매우 유용할 것으로 믿습니다.\n\n다음에 제시할 팁을 더 잘 이해하기 위해 몇 가지 기본 개념을 맞추는 것이 중요합니다:\n\n- Java (프로그래밍 언어) 대 Java 플랫폼/JVM: 자바 프로그래밍 언어를 자바 플랫폼과 혼동해서는 안됩니다. 요즘에는 JVM이 자바 이외의 Kotlin, Scala, Groovy, Clojure 등과 같은 여러 프로그래밍 언어를 실행하고 지원할 수 있습니다. 이 글에서는 프로그래밍 언어 자체보다는 실행 환경(런타임)과 관련하여 주로 JVM에 대해 논의할 것입니다.\n\n![image](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그런 면에서 팁으로 넘어가 보겠습니다.\n\n## 1) 에르고노믹스: 영웅과 악당\n\n오늘날, 단순히 java -jar 명령을 실행하여 JVM 애플리케이션을 실행할 때, JVM에는 실행 환경을 기반으로 적절한 구성을 찾으려는 에르고노믹스라는 기능이 있습니다.\n\n처음에는 아주 좋은 것 아닌가? 라고 생각할 수 있습니다. 그러나 이에 대한 답은: 상황에 따라 다릅니다. 이 기능은 세밀한 튜닝에 대해 걱정하지 않고 애플리케이션을 빌드하고 실행하는 데 도움이 될 수 있지만, 대규모 환경에서는 결과가 최적이 아닐 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJVM에서 Ergonomics가 수행하는 일부 자동 구성 조정 사항 중에는 성능 및 자원 소비에 직접 영향을 미칠 수 있는 Garbage Collector 선택 및 Heap 크기와 관련된 것이 있습니다. 함께 조금 더 자세히 살펴보겠습니다.\n\nGarbage Collector 선택\n\nGC의 선택은 두 가지 조건에 기반합니다: JVM에서 사용 가능한 메모리 양과 CPU의 개수입니다.\n\n규칙은 다음과 같이 작동합니다:\nJava 8 이전: CPU 수가 2 이상이고 메모리 양이 1792MB보다 크면 선택된 GC는 ParallelGC가 됩니다. 이 두 조건 중 하나라도 위의 값보다 낮은 경우에는 선택된 GC가 SerialGC가 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJava 9부터: 조건은 사실상 동일하지만 CPU 수가 2 이상이고 메모리 양이 1792MB보다 큰 경우에는 ParallelGC 대신 G1GC가 선택된 GC로 지정됩니다.\n\n![이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_2.png)\n\n힙 크기\n\n일부 예외를 제외하고 대부분의 경우, -Xmx 매개변수나 -XX:MaxRAMPercentage 플래그를 사용하여 원하는 힙 크기를 지정하지 않을 때, Ergonomics는 사용 가능한 메모리의 ¼를 최대 힙 값으로 구성합니다. 예를 들어, 컨테이너의 메모리 제한이 2GB로 설정된 경우, Ergonomics는 최대 힙 크기를 512MB로 구성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n언급된 예외 사항은 다음과 같습니다:\n\n- 컨테이너에 최대 256MB의 사용 가능한 메모리가 있는 경우, 최대 힙 값은 50%가 됩니다.\n- 컨테이너에 256MB에서 512MB 사이의 사용 가능한 메모리가 있는 경우, 최대 힙 값은 약 127MB가 됩니다.\n\n하지만 이 모든 것이 미치는 영향은 무엇일까요?\n\nGC의 자동 선택에 관해, SerialGC는 장기간 일시 중지 시간을 발생시켜 고출력 서버 환경에서 성능이 좋지 않을 수 있다는 점을 염두에 두어야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만, 여기서 궁금증이 생길 수도 있어요: 내 컨테이너가 1000m의 제한을 가지고 있고 결과적으로 JVM은 하나의 CPU만 사용 가능한 상황에서, 멀티 스레딩 리소스를 활용하는 GC를 사용하는 장점은 무엇일까요?\n\n이에 대한 답변은 구체적인 세부 사항에 대해서는 들어가지 않겠지만, 요약하자면, 컨테이너 관점에서 혼란이 있습니다; 많은 사람들이 1000m(1000 밀리코어)을 1 CPU로 오해합니다. 그러나 1000m은 계산 용량 시간을 나타내며, 이는 노드의 모든 사용 가능한 CPU들 사이에서 분산될 수 있는 것입니다.\n\n이러한 혼란이 생기는 이유는 JVM이 1000m을 1개의 사용 가능한 CPU로 해석하기 때문입니다. 1001m는 2개의 CPU로, 2001은 3개로 해석됩니다.\n\n이를 알고 있으면, JVM이 CPU가 1000m으로 제한된 컨테이너에서도 멀티 스레딩 GC를 활용할 수 있습니다. 이를 달성하기 위해, -XX:ActiveProcessorCount 플래그를 사용하여 JVM의 사용 가능한 CPU 수를 강제로 설정할 수 있으며, 이때 값은 1보다 큰 값을 전달합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSerialGC 외에도 1000m 이하로 다른 GC를 사용할 수 있지만, 사용 사례에 따라 CPU 사용 가능성이 낮아 성능이 저하될 수 있음을 염두에 두는 것이 중요합니다. 사용 가능 할당량(CFS 할당량)을 짧게 초과하여 응용 프로그램 쓰로틀링이 발생할 수 있습니다.\n\nJVM이 사용 중인 GC 구현을 확인하려면 kubectl exec를 사용하여 컨테이너에 액세스하고 다음 명령을 실행하십시오:\n\n```js\njava -XX:+PrintFlagsFinal -version | grep “Use*.*GC “\n```\n\n아래 예시와 유사한 출력을 받게 되며, 사용되고 있는 구현이 부울 값으로 표시됩니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_3.png)\n\n출력물에서 볼 수 있는 예제를 통해 사용 중인 구현은 SerialGC이며, Ergonomics를 통해 구성되었습니다.\n\n힙 크기에 대해, 사용 가능한 메모리의 ¼만 사용한다면, 우리는 자원을 낭비할 수 있습니다. 컨테이너는 애플리케이션을 실행하기 위해 구축된 것이며, 나머지 메모리를 소비할 병렬 프로세스는 가지고 있으면 안 됩니다. 그러나 항상 기억해야 할 중요한 것은 JVM이 힙만으로 구성되지 않으며, 힙 외의 다른 구성 요소도 있어서 우리는 이를 비힙으로 부르고 있습니다. 힙과 비힙 뿐만 아니라 '운영 체제'와 관련된 일부 프로세스도 메모리를 소비하나 작은 양이지만 있습니다. 다음 팁에서 살펴보겠습니다.\n\n![이미지](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJVM의 최대 힙 크기를 확인하려면 kubectl exec를 사용하여 컨테이너에 액세스하고 다음 명령을 실행하세요:\n\n```js\njava -XX:+PrintFlagsFinal -version | grep “ MaxHeapSize”\n```\n\n다음과 같이 예제와 비슷한 출력을 받게 될 것입니다:\n\n<img src=\"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_5.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 예시에서 보는 것처럼, MaxHeapSize가 바이트로 표시되었고 Ergonomics를 통해 구성되었습니다.\n\n팁 요약:\n\n- 고유련한 서버 환경에서 SerialGC를 사용하지 않도록 주의하세요. JVM이 1개의 CPU에만 제한되지 않도록합니다. 이를 달성하는 여러 방법이 있습니다. 예를 들어 컨테이너의 CPU 제한을 1001m 이상으로 조정하거나 -XX:ActiveProcessorCount 플래그를 1보다 큰 값으로 사용하는 등입니다.\n- 컨테이너의 사용 가능한 메모리가 1792MB 미만이고 특정 GC 버전을 강제하지 않으면, Ergonomics가 SerialGC를 선택할 수도 있음을 인식하세요.\n- JVM 인수를 통해 원하는 GC 구현을 지정할 수도 있습니다. 권장 사항으로는 4GB 이하의 힙에는 ParallelGC를 사용하고 4GB 이상의 힙에는 G1을 사용합니다. 더 많은 GC 구현이 있지만, 대부분의 사용 사례를 커버해야 합니다. 특정 GC 사용을 강제하는 예시 몇 가지는 다음과 같습니다:\n\n```js\n# Serial GC\njava -XX:+UseSerialGC -jar meuapp.jar\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 병렬 GC\njava -XX:+UseParallelGC -jar meuapp.jar\n# 병렬 G1 GC\njava -XX:+UseG1GC -jar meuapp.jar\n# 병렬 UseShenandoahGC GC\njava -XX:+UseShenandoahGC -jar meuapp.jar\n# 병렬 Z GC\njava -XX:+UseZGC -jar meuapp.jar\n```\n\n- 성능 향상과 애플리케이션 스로틀링을 피하기 위해 CPU 제한이 2000m 미만인 컨테이너를 피하세요. 많은 상황에서, 1000m CPU 제한이 있는 두 개의 컨테이너보다 2000m CPU 제한이 있는 컨테이너에 단일 JVM이 있는 것이 더 유익할 수 있습니다. 때로는 적은 게 더 나은 선택일 수도 있습니다 — 한 번 생각해보세요.\n- 자원 낭비를 피하려면, -Xmx 매개변수나 -XX:MaxRAMPercentage 플래그를 사용하여 JVM 힙 크기를 적절하게 구성하세요. 다음 팁에서 적절한 힙 크기에 대해 더 이야기하겠습니다.\n\n```js\n# 예시\n```\n\n```js\n# MaxRAMPercentage 사용\njava -XX:MaxRAMPercentage=50.0 -jar meuapp.jar\n# xmx 사용\njava -Xmx1g -jar meuapp.jar\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2) 적절한 메모리 크기 설정: 힙 이상의 JVM의 수명\n\n첫 번째 팁을 읽으면 아마도 궁금해 할 것입니다: 리소스 최적화를 위해 컨테이너의 사용 가능한 메모리의 100%에 대한 힙 크기를 구성하는 것은 왜 아닌가요?\n\n그 질문에 대한 답변은 이 팁의 끝에서 알려드리겠습니다. 그 전에 JVM을 구성하는 메모리 영역을 간단히 설명하고 싶습니다.\n\n1번 팁에 언급된 것처럼 힙 이외에 JVM의 메모리 영역에는 힙 이외라고도 불리는 네이티브 메모리 또는 오프-힙 메모리가 포함됩니다. 비-힙 내에서는 메타스페이스, 코드 캐시, 스택 메모리, GC 데이터 등 여러 가지 중요한 구성 요소가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이러한 구성 요소 각각에 대해 자세히 다루지는 않겠습니다. 이 글의 목적은 단순히 힙 이외에도 JVM의 다른 영역이 있고, 이 경우 애플리케이션이 실행되는 컨테이너에서 호스트로부터 메모리를 소비한다는 점을 명확히하는 것입니다.\n\n이제 당신은 생각할 수 있습니다: \"컨테이너의 사용 가능한 메모리를 힙과 비힙으로 나눌 수 없나요?\"\n\n답은 \"아니요\"입니다. 이 컨테이너의 JVM이 실행되는 기본 이미지는 활성 상태를 유지하기 위해 일정량의 메모리를 소비하는 에뮬레이션 운영 체제도 고려해야 합니다.\n\n그래서 이제 초반 질문에 대한 대답은: 컨테이너의 사용 가능한 메모리의 100%에 힙 크기를 구성하는 것이 왜 좋지 않을까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n힙 크기를 사용 가능한 메모리의 100%로 설정하면, 비-힙 및 운영 체제도 컨테이너 메모리를 사용한다는 것을 감안해야 하므로 OOM(메모리 부족)에 의해 컨테이너가 종료될 수 있습니다.\n\n![image](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_6.png)\n\n그러나 새로운 질문이 발생할 수 있습니다: 이 메모리를 힙, 비-힙 및 운영 체제 사이에 어떻게 분배해야 할까요?\n\n문헌에서는 사용 가능한 메모리의 75%를 힙에 구성하는 것이 안전한 값이라고 언급되는 경우를 본 적이 있습니다. 그러나 실무에서는 60% 이상의 값 사용 시 문제가 발생했습니다. 저는 개인적으로 전문 경험을 통해 힙에 50%를 할당하는 것이 안전한 값이라고 생각합니다. 초기에는 매우 보수적으로 보일 수 있지만, 이보다 높은 값 사용 가능성을 배제하지는 않습니다. 그러나 이를 확인하는 것이 중요합니다. 확인을 위해 부하 테스트를 활용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_7.png\" />\n\n힙 크기에 대해 또 하나 중요한 사항은 매우 작은 힙이 가비지 수집기에 과도한 작업을 유발할 수 있어 CPU 사용량이 늘어나고 애플리케이션 성능이 저하될 수 있다는 것을 이해하는 것입니다. 반면에 매우 큰 힙은 애플리케이션 시작 시간에 상당한 영향을 미칠 수 있고 긴 가비지 수집 시간을 유발할 수 있습니다.\n\n팁 요약:\n\n- 컨테이너의 메모리 사용량을 최적화하려면 JVM의 힙 크기를 50%에서 75% 사이로 구성하고 나머지 값은 비힙 및 운영 체제를 위해 예약하세요.\n- 구성된 힙 크기가 애플리케이션에 적합한지 검증하기 위해 부하 테스트를 수행하고 테스트 실행 중 OOM (메모리 부족 오류)이 발생하는지 확인하세요. JVM 메모리 소비를 추적하기 위해 모니터링 도구를 사용하고 Kubernetes API를 통해 Kubernetes Metrics Server를 사용하여 파드 메트릭을 모니터링할 수 있습니다.\n- 애플리케이션 성능이 저하되지 않도록 매우 작은 힙을 피하세요.\n- 애플리케이션 시작 시간에 영향을 미치지 않고 오랜 가비지 수집 시간을 유발하지 않도록 매우 큰 힙을 피하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3) Xms와 Xmx가 동일하다면: 필요한 양만 말해주세요\n\n이 팁을 살펴보기 전에, Xms와 Xmx에 대한 기본 내용을 알아보겠습니다.\n이 두 매개변수는 JVM에게 최소 (Xms) 및 최대 (Xmx) 힙 크기에 대한 정보를 제공하는 데 사용됩니다.\n\n이렇게 작동합니다. Xms는 JVM에 의해 힙에 할당된 초기 메모리 양을 나타내고, Xmx는 할당할 수 있는 최대 양을 나타냅니다. JVM은 Xms에 정의된 값으로 초기 메모리를 할당하고, 프로그램 실행 중에 필요에 따라 이 값은 Xmx에 정의된 값까지 증가할 수 있습니다. JVM이 Xmx를 초과하는 값으로 할당하려고 할 때 OutOfMemoryError가 발생할 수 있습니다.\n\n컨테이너 사용이 오늘날과 같이 널리 사용되기 전에는 JVM에서 실행되는 애플리케이션이 종종 동일한 서버를 공유했습니다. 이러한 시나리오에서는 Xms 매개변수에는 작은 값을 설정하고 Xmx에는 큰 값을 설정하여 리소스 활용 및 동시에 실행 중인 프로세스 간의 공유를 더 잘 목표로했습니다. 따라서 JVM은 필요할 때만 메모리를 할당하고 사용이 끝나면 호스트로 메모리를 반환했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너를 다룰 때 상황이 달라집니다. 대부분의 경우에는 동일한 컨테이너 내에서 실행 중인 다른 관련 병렬 프로세스가 없습니다. 따라서 프로그램 실행 중에 호스트에 동적으로 메모리를 할당하고 반환할 필요가 없습니다. JVM이 메모리 할당 작업을 처리할 필요가 없도록 하려면 Xms의 값이 Xmx의 값과 같도록 구성할 수 있습니다.\n\n팁 요약:\n\n- JVM이 메모리 할당 및 해제 작업을 처리하는 것을 피하려면 Xms의 값이 Xmx의 값과 같도록 설정하세요. 이를 위해 JVM 구성에서 매개변수 -Xms 및 -Xmx를 사용하십시오.\n\n```bash\n# 예시\njava -Xms2g -Xmx2g -jar meuapp.jar\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4) JVM 컨테이너에서 CPU 과부하: 위험한 실천 방법이지만 일부 경우에는 허용됨\n\n이 팁을 진행하기 전에 Kubernetes의 두 가지 개념인 \"Pod 및 컨테이너 자원 관리\"와 \"서비스 품질(QoS)\"를 설명하는 것이 중요합니다. 이러한 개념은 다음에 다루게 될 주제를 더 잘 이해하는 데 도움이 될 것입니다.\n\nPod 및 컨테이너 자원 관리\n\n이 시점에서 CPU 및 메모리에 대한 요청과 제한 개념을 설명하고 싶습니다. 기본적으로 요청은 컨테이너가 실행되는 데 필요한 최소 자원량을 나타내며, 제한은 클러스터 내에서 컨테이너가 소비할 수 있는 최대 자원량을 나타냅니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n메모리 요청 및 제한은 쿠버네티스에서 Pod를 만들 때 사용하는 YAML 파일의 \"resources\" 섹션에서 구성할 수 있습니다.\n\n간단한 예시는 다음과 같습니다:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\nspec:\n  containers:\n    - name: container-name\n      image: image-name:latest\n      resources:\n        requests:\n          memory: \"1Gi\"\n          cpu: \"1\"\n        limits:\n          memory: \"2Gi\"\n          cpu: \"2\"\n```\n\n이 시점에서, 저는 팁 1에서 다룬 주제와 관련된 질문을 하고 싶습니다. 거기서는 JVM에 대한 인체공학과 사용 가능한 호스트 리소스에 따라 설정된 기본 구성에 대해 이야기했습니다. 질문은 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너 내에서 사용자 정의 없이 JVM을 실행하고, Ergonomics가 설정을 정의하도록 하는 경우, 컨테이너의 요청 및 제한 값이 YAML 예제에 명시된 값과 같다고 가정하면, JVM이 채택하는 GC 설정 및 최대 힙 크기는 무엇일까요? 컨테이너에 정의된 요청 또는 제한 값이 고려될까요?\n\n이 질문에 대한 답변을 얻기 위해 JVM 버전 17을 사용하고 YAML에서의 구성으로 컨테이너를 실행해보겠습니다. 그런 다음, kubectl exec를 통해 컨테이너에 액세스하여 선택된 GC와 최대 힙 크기를 확인할 것입니다.\n\nJVM이 1GB 메모리와 1 CPU에 대한 요청을 존중한다면, 선택된 GC는 SerialGC이 되고 최대 힙 크기는 256MB (1GB의 ¼)가 될 것입니다.\n\n그러나 JVM이 2GB 메모리와 2 CPU의 제한을 존중한다면, 선택된 GC는 G1GC가 되고 최대 힙 크기는 512MB (2GB의 ¼)가 될 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 실행 후의 파드 및 컨테이너 구성입니다:\n\n![Pod Configuration](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_8.png)\n\nErgonomics에 의해 실행된 JVM 구성 결과는 다음과 같습니다:\n\n![JVM Configuration](/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_9.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJVM은 컨테이너의 한계 값을 고려했다는 것이 눈에 띄었습니다. 따라서 선택된 GC는 G1GC이었고, 최대 힙 크기는 512MB로 설정되었는데, 이는 2GB의 ¼에 해당합니다.\n\n이제 리소스 관리 주제에서 잠시 쉬고 두 번째 개념에 대해 이야기해 봅시다.\n\nQoS(Quality of Service)\n\nKubernetes에서의 QoS(Quality of Service)는 요청하고 사용하는 리소스를 기반으로 Pod를 세 가지 범주로 분류하는 방법으로, 이 범주는 각 Pod의 우선 순위를 결정하는 데 사용됩니다. 이러한 분류로는 Guaranteed, Burstable 및 BestEffort가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n각 QoS 범주로 Pod를 분류하는 규칙은 다음과 같은 조건에 의해 정의됩니다:\n\n- 보장된(Guaranteed): 이 수준에서 CPU와 메모리 모두에 대해 요청(request) 및 제한(limit) 값이 지정되어야 합니다. 예시:\n\n```js\n리소스: 요청: 메모리: \"3Gi\";\nCPU: \"2\";\n제한: 메모리: \"3Gi\";\nCPU: \"2\";\n```\n\n- 터프너블(Burstable): 이 수준은 보장된으로 분류되기 위한 규칙을 충족하지 못하는 Pod를 위한 것입니다. Pod의 적어도 한 컨테이너는 메모리 또는 CPU에 대한 요청(request)나 제한(limit)이 있어야 합니다. 예시:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nresources:\nrequests:\nmemory: \"3Gi\"\n\n# Burstable의 또 다른 예:\n\nresources:\nrequests:\nmemory: \"3Gi\"\ncpu: \"1\"\nlimits:\nmemory: \"3Gi\"\ncpu: \"2\"\n\n# 이 예제에서 메모리 요청과 제한은 같지만\n\n# CPU 요청이 제한보다 작으므로,\n\n# 팟은 Burstable로 분류됩니다.\n\n- BestEffort: 컨테이너 내에 CPU 또는 메모리에 대한 요청 또는 제한 구성이 없는 경우 팟은 BestEffort로 분류됩니다. 적어도 하나의 컨테이너에 요청이나 제한 구성이 있는 경우 분류가 Burstable로 변경됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 결국 QoS의 중요성과 수준 범주는 무엇인가요?\n\n클러스터의 노드 중 하나가 과부하 상태거나 자원 부족 상태인 경우 Kubernetes 스케줄러는 QoS 우선 순위를 기반으로 제거할 팟을 선택할 수 있습니다. 이는 다음과 같이 정의됩니다:\n\n- Best-Effort: 이러한 팟은 가장 낮은 우선 순위를 가지며 가장 먼저 제거됩니다. 자원 요청이나 제한이 정의되지 않았기 때문에 클러스터에 영향을 미치지 않고 쉽게 제거할 수 있습니다.\n\n- Burstable: 이러한 팟은 중간 우선 순위를 가지며 최소한의 자원 요청 구성을 가지고 있기 때문에 Best-Effort 팟 이후에 제거됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n보장된: 이러한 팟은 CPU 및 메모리에 대한 요청 및 제한 값이 동일하여 실행에 필요한 충분한 자원이 확보되어 있기 때문에 가장 높은 우선 순위를 갖고 마지막으로 제거됩니다.\n\n좋아요, 이제 위의 두 개념인 \"팟 및 컨테이너 자원 관리\"와 \"QoS(서비스 품질)\"에 대해 논의했으니, 모든 팟을 보장된 상태로 구성하는 것이 가장 좋은 옵션이 될 수 있을지 궁금할 것입니다. 이 수준은 팟에 대한 더 큰 보장과 안정성을 제공하므로, 옳습니까?\n\n그래서 CPU와 메모리에 대한 요청 및 제한을 항상 동일하게 유지하여 모든 팟을 보장된 상태로 구성하는 것은 클러스터 내의 노드가 더 적은 팟을 수용하게 하고, 클러스터 내에서 더 많은 노드가 필요하게 함으로써 환경을 매우 비싸게 만들 수 있습니다. 추가적으로, 이 구성은 자원의 효율적인 활용을 방해할 수 있는 미활용 자원을 남겨 다른 팟과 공유할 수 있는 효과적이지 않은 자원 활용을 초래할 수 있습니다.\n\n반대로, 모든 팟을 보장된 상태로 구성하는 것은 실패와 지연 없이 항상 실행에 필요한 자원을 보장하므로 애플리케이션 안정성을 보장합니다. 게다가 이 구성은 높은 가용성과 성능이 필요한 중요한 애플리케이션에 적합한 옵션이 될 수 있으며, 팟에 최대 우선 순위를 보장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자 그럼 팁으로 넘어가기 전에, 제가 제안하는 것을 정말로 이해하실 수 있도록 하나의 추가적인 개념에 대해 더 이야기를 나누고 싶습니다.\n\n쿠버네티스 클러스터에서 오버부킹(Ovebooking) 개념에 대해 이야기해보겠습니다.\n\n일반적으로 오버부킹은 사용 가능한 것보다 더 많은 리소스를 할당하는 기술로, 할당된 모든 리소스가 동시에 사용되지는 않을 것으로 예상합니다. 예를 들어 항공사에서는 비행기의 총 좌석 수보다 더 많은 좌석을 판매하는 오버부킹을 사용하여 모든 승객이 비행기에 탑승하지는 않을 것을 가정합니다.\n\n쿠버네티스의 맥락에서, 오버부킹은 파드 내의 컨테이너에 할당된 CPU 및 메모리 리소스에 적용될 수 있습니다. 이는 컨테이너에 대한 요청 및 제한에 기반하여 클러스터에 사용 가능한 총 리소스보다 더 많은 리소스를 할당할 수 있는 것을 의미합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예를 들어, 4GB의 메모리와 4개의 CPU가 있는 노드가 있다고 상상해보세요. 이 노드에서 하나의 컨테이너를 가진 두 개의 포드가 각각 다음과 같은 리소스 구성을 갖고 있습니다:\n\n```js\n리소스: 요청: 메모리: \"1Gi\";\nCPU: \"1\";\n제한: 메모리: \"1Gi\";\nCPU: \"3\";\n```\n\nCPU 제한이 3으로 설정되어 있습니다. 두 개의 포드가 실행 중이며 각각 위의 구성을 갖는 한 개의 컨테이너를 고려할 때, 포드 수를 CPU 제한으로 곱하면 결과적으로 6개의 CPU가 되는데, 이는 노드에서 사용 가능한 CPU의 최대 개수를 초과합니다.\n\nKubernetes에서 Overbooking을 다룰 때 중요한 점은 CPU가 \"압축 가능\" 리소스로 간주되지만 메모리는 그렇지 않다는 것입니다. 즉, Kubernetes는 컨테이너가 요청한 양의 CPU를 받도록 보장하고 나머지를 제한할 것입니다. 그러나 컨테이너가 CPU 제한을 초과하기 시작하면 Kubernetes는 해당 컨테이너의 사용을 제한하기 시작할 것이며, 이는 응용 프로그램의 성능 하락으로 이어질 수 있습니다. 단, 해당 컨테이너는 종료되거나 제거되지 않는다는 점을 강조해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nCPU와 달리 메모리는 압축할 수 없는 자원입니다. 이는 노드가 사용 가능한 메모리를 모두 소진하면 Kubernetes가 메모리 공간을 확보하기 위해 어떤 컨테이너를 종료할지 결정해야 한다는 것을 의미합니다.\n\n대부분의 경우 JVM 애플리케이션은 데이터 처리가 많거나 복잡한 계산이 필요한 경우를 제외하고 메모리를 더 많이 요구합니다. 그래서 개발 환경을 효율적으로 관리하려면 다음 팁을 활용해보세요.\n\n클러스터를 효율적으로 활용하기 위해, 메모리에 대해서는 요청 값(request)을 제한 값(limits)과 동일하게 설정하고, CPU에 대해서는 오버북(overbook)해서 요청 값을 제한 값보다 낮게 설정하여 Pod를 Burstable 수준으로 유지하세요. 이렇게 함으로써 CPU 자원을 Pod 간에 공유할 수 있어 클러스터 효율성이 향상될 것입니다.\n\n만약 모든 Pod가 동시에 CPU 제한값을 사용해야 하는 경우(이는 드문 경우입니다), 과도한 CPU 소비를 노드 자동 확장의 트리거로 사용하여 환경의 부하를 균형잡을 수 있습니다. 이러한 시나리오에서 유일한 부작용은 노드를 확장할 때까지 일시적으로 컨테이너가 제한될 수 있다는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 전략은 처음에는 논란이 될 수 있지만, CPU의 가격이 메모리에 비해 비례적으로 높기 때문에 환경 비용을 극적으로 최적화할 수 있습니다.\n\n팁 요약\n\n- 특히 테스트 환경에서 비용을 줄이려면 쿠버네티스 환경에서 JVM 애플리케이션을 실행하는 파드를 Burstable 수준에서 구성하는 것을 고려하십시오. 메모리는 동등한 요청 및 한도를 포함하며 CPU의 경우 요청을 한계보다 낮게 설정하여 오버부킹하세요.\n\n## 5) JVM 및 HPA: 표준 사용해야 할 경우, 메모리보다 CPU를 우선시하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 팁을 진행하기 전에 Horizontal Pod Autoscaler(HPA)에 대해 간단히 이야기하려고 합니다.\n\nHPA는 쿠버네티스의 기능 중 하나로, 팟의 CPU 또는 메모리 사용량에 기반하여 애플리케이션의 복제본 수를 자동으로 조정하는 데 도움을 줍니다. 특정 수요를 충족시키기 위해 복제본 수를 증가하거나 감소시킵니다. 처리 능력이나 메모리에 대한 더 높은 수요가 있을 때 HPA는 복제본 수를 늘립니다. 수요가 줄어들면 HPA는 복제본 수를 감소시킵니다. 이렇게 함으로써 HPA는 피크 수요 기간에도 서비스 가용성을 유지합니다.\n\nJVM 애플리케이션을 다룰 때는 보통 CPU보다 더 많은 메모리를 소비하는 경우가 많습니다. 따라서 HPA를 메모리에 기반으로 스케일링하도록 구성하는 것이 유혹적일 수 있습니다. 그러나, JVM은 쓰레기 수집 및 메모리 할당 프로세스로 인해 메모리 소비에 변동이 발생할 수 있습니다. 이는 메모리를 HPA의 트리거로 사용하여 HPA를 불안정하게 만들어 애플리케이션의 수평 스케일링 프로세스를 방해할 수 있습니다.\n\n이를 알아두면, 기본적으로 쿠버네티스에서는 메모리와 CPU 메트릭을 고려하여 스케일을 조정할 수 있기 때문에, 우리의 유일한 선택은 CPU를 HPA 트리거 구성의 대안으로 고려하는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 CPU를 HPA 트리거로 사용하도록 권장하는 것은 유일한 옵션인 것으로 생각하지 마세요. 실제로 쿠버네티스에서 JVM 애플리케이션을 수평 확장하는 흥미로운 전략이 있습니다. CPU를 메트릭으로 사용하는 것이 관련됩니다. 이에 대해 자세히 알아보겠습니다.\n\n이 기본 아이디어는 다음과 같습니다. 고수요 시나리오에서 JVM은 메모리를 강하게 할당하기 시작하면서 응용 프로그램의 가비지 컬렉터(GC)에 대한 작업이 증가하고 풀 가비지 컬렉션 주기(Full GC)를 포함한 CPU 집약적인 프로세스가 발생합니다. 여기서 HPA 역할이 의미를 갖기 시작합니다. 이 문맥에서 HPA는 응용 프로그램의 수요를 나타내는 지표로 CPU 메트릭을 기반으로 응용 프로그램을 수평 확장하는 데 사용할 수 있습니다.\n\n다음은 이 HPA 구성의 예시입니다:\n\n```js\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nome-do-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nome-do-deployment\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 예제에서 HPA는 배치의 CPU 사용률을 모니터링하고 자동으로 팟 레플리카 수를 조정하여 CPU 사용률을 50%로 유지합니다. 최소 레플리카 수는 1이며, 최대는 5로 설정되어 있어 요구에 따라 응용 프로그램이 수평 확장될 수 있습니다.\n\nHPA는 주기적으로 배치의 CPU 사용률을 모니터링하고, 일정 기간 동안 사용률이 임계값 아래로 유지되면 HPA는 천천히 레플리카 수를 줄이기 시작하여 정의된 최소값에 도달할 때까지 줄입니다. Kubernetes는 기본적으로 스케일 다운 프로세스를 시작하기 전에 5분간 대기합니다.\n\n팁 요약\n\n- JVM(Java Virtual Machine) 애플리케이션의 성능을 향상시키기 위해 Kubernetes의 Horizontal Pod Autoscaler(HPA)에 CPU 메트릭을 기본 구성으로 사용하여 고수요 시나리오에서 애플리케이션 성능을 향상시킬 수 있습니다. 이는 트래픽 급증 시 JVM에 의한 강력한 메모리 할당이 쓰레기 수집기의 작업 부하를 크게 증가시킬 수 있어 전체 가비지 컬렉션 주기(Full GC)와 과도한 CPU 사용량이 발생할 수 있기 때문입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 결론\n\n이 포스트가 너무 길어서 죄송합니다. 제 목적은 포괄적이고 자세한 내용을 제공하는 것이었습니다. 이러한 지식은 오랜 연구와 경력을 통해 얻었으며, 제가 공유한 팁들이 도움이 되었기를 바랍니다.\n\n이것은 단순히 시작에 불과하며, 가능한 제2편에서 이와 같은 팁을 더 공유할 계획입니다.\n\n비판, 제안이 있거나 단순히 내용을 좋아했다면 의견을 남겨 주시기 바랍니다. 함께 공유하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 참고 자료\n\n- Microsoft Learn의 Microsoft for Java Developers\n- Kubernetes의 Kubernetes Documentation\n- Bruno Borges의 YouTube 동영상인 (178) Secrets of Performance Tuning Java on Kubernetes (해당 주제에 대해 본 적이 가장 좋았던 콘텐츠)\n- Google Cloud Blog의 Kubernetes requests vs limits: Why adding them to your Pods and Namespaces matters\n- Java inside docker: What you must know to not FAIL\n- DigitalOcean Kubernetes에서 워크로드를 자동으로 확장하는 방법\n","ogImage":{"url":"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_0.png"},"coverImage":"/assets/img/2024-05-20-RunningJVMApplicationsonKubernetesBeyondjava-jar_0.png","tag":["Tech"],"readingTime":26},{"title":"Kubernetes 시작하기","description":"","date":"2024-05-20 17:13","slug":"2024-05-20-StartingyourjourneyonKubernetes","content":"\n## 최근에 저는 Kubernetes와 관련된 다양한 기술의 사용에 대한 여러 기사를 쓰고 있습니다. Kubernetes 클러스터를 자동으로 구축하는 공개 프로젝트에 대한 기사를 소개하기 전에 Kubernetes 클러스터가 무엇이고 왜 필요한지 설명해야겠다고 생각했습니다.\n\n![StartingyourjourneyonKubernetes_0.png](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_0.png)\n\n# Microservices\n\nKubernetes에 대해 알아보기 전에 이가 해결하는 문제를 이해하는 것이 중요합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n현대 소프트웨어 개발은 마이크로서비스로 이동했습니다. 이 아키텍처는 솔루션을 작은 독립적인 애플리케이션으로 분해하여 API를 통해 연결합니다. 이러한 각 애플리케이션을 마이크로서비스라고합니다.\n\n![Starting Your Journey on Kubernetes](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_1.png)\n\n이 아키텍처는 매우 성공적이며, 솔루션의 개발과 진화를 훨씬 빠르게 할 수 있습니다.\n\n그러나 이 아키텍처의 단점은 이제 여러 마이크로서비스를 모든 서버에 배포하고 그들 사이의 네트워크 연결을 설정해야 한다는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 이를 10, 20, 50, 100개의 마이크로 서비스로 확장해 보세요.\n\n머리가 아프지 않나요? 아직 아프지 않다면, 곧 아플 겁니다.\n\n이러한 모든 마이크로 서비스를 관리하는 것은 어렵습니다. 아키텍처가 인기를 얻고 나면 이를 관리하는 해결책이 등장했습니다.\n\n## 컨테이너화\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희가 해결한 첫 번째 문제는 각각의 마이크로서비스가 서로 다른 종속성 집합을 가질 수 있다는 문제였습니다. 이 종속성에는 패키지, 도구 및 구성 요소가 포함될 수 있는데, 이들이 서로 호환되지 않을 수 있습니다.\n\n컨테이너화를 통해 이 문제를 해결했습니다. 컨테이너화는 각 마이크로서비스가 자체적이고 가벼운 가상 환경인 컨테이너에서 실행될 수 있게 했습니다. 이 컨테이너들은 독립적으로 마이크로서비스를 운영할 수 있었으며 기본 호스트 환경에서 제공되는 네트워크를 통해 서로 연결할 수 있었습니다. 이러한 방식으로 어떠한 호환성 문제도 제거되었습니다.\n\n이로써 부분적인 어려움은 해결되었지만, 이제 이러한 컨테이너를 배포하고 연결해야 합니다. 이때 필요한 것이 쿠버네티스입니다.\n\n# 그렇다면 쿠버네티스란 무엇인가요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n클라우드에 마이크로서비스 애플리케이션을 배포하기로 결정했다고 가정해봅시다. 빠른 검색 결과로 구글에서 개발한 플랫폼인 Kubernetes가 필요하다는 것을 알게 됐습니다. 이 플랫폼은 구글의 비즈니스를 지원하기 위해 개발되었으며 이후에는 클라우드 네이티브 컴퓨팅 재단（CNCF）에 오픈 소스 솔루션으로 기부되었습니다.\n\n귀하의 검색 결과에 따르면, 위키피디아와 많은 다른 사이트들이 Kubernetes를 다음과 같이 설명합니다:\n\n당신은 무슨 생각인지 모르겠지만, 제게는 실제로 무엇인지 이해하는 데 도움이 되지 않았어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 클라우드용 운영 체제\n\n나는 쿠버네티스를 '클라우드'를 위한 운영 체제로 생각합니다.\n\n저는 클라우드를 실제 또는 가상 서버 집합이 실제로 무엇인가의 추상화라고 생각합니다. 이 서버들은 AWS 또는 Azure와 같은 공급 업체에서 제공될 수도 있고 귀하의 데이터 센터에서 제공될 수도 있습니다.\n\n그러나 이 클러스터의 운영 체제로 쿠버네티스를 생각해 봅시다. 먼저 컴퓨터의 운영 체제에 대해 생각해보십시오. 이 운영 체제는 컴퓨터의 리소스를 관리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 메모리 및 디스크 공간\n- 응용 프로그램 및 패키지 설치\n- 사용자\n- 보안\n- 네트워킹 및 연결성\n\nKubernetes는 클러스터에서 비슷한 작업을 수행하며 클라우드에서 마이크로서비스를 관리할 수 있게 합니다.\n\n이를 통해 다음을 수행할 수 있습니다:\n\n- 마이크로서비스에 클라우드 리소스(서버 및 네트워크 구성 요소)를 동적으로 할당하여 필요에 따라 확장하고 축소하며 높은 부분의 신뢰성과 가용성을 달성할 수 있습니다.\n- 마이크로서비스 및 도구를 쉽게 배포할 수 있습니다.\n- 네트워킹 연결성 및 보안을 구성할 수 있습니다.\n- 사용자 및 액세스 보안을 관리할 수 있습니다(예: Role Based Access Control — RBAC).\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n간략히 말해, 클라우드나 실제 또는 가상 클러스터에 마이크로서비스를 배포하고 관리할 수 있는 도구를 제공합니다.\n\n## 다른 옵션이 있나요?\n\n다양한 다른 옵션이 있지만 Kubernetes가 지금까지 가장 인기 있는 옵션입니다.\n\n대안으로는:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Amazon Elastic Compute Cloud\n- OpenShift\n- Google Kubernetes Engine\n- Azure Kubernetes Service\n- Cloud Foundry\n\n이 모든 솔루션은 여러 서버의 클러스터에서 마이크로서비스를 구동하는 컨테이너를 관리합니다.\n\n## Kubernetes를 사용해야 할까요?\n\nKubernetes를 사용해야 할 필요성에 대해 스스로 질문해볼 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n쿠버네티스 클러스터가 제공하는 가장 중요한 기회 중 하나는 휴대 가능한 환경을 만들어준다는 것입니다. 이는 쿠버네티스를 지원하는 어디에서나 배포 아키텍처를 거의 어디론가 가져갈 수 있게 해줍니다. 또한 다른 공급업체에 있는 클러스터를 활용할 수 있어서 더 높은 신뢰성과 가용성을 제공합니다.\n\n또 다른 기능은 일련의 운영 작업을 자동화할 수 있는 능력입니다. 이를 통해 여러 가지 작업을 자동으로 처리해주어 마음의 안정을 줍니다. 이러한 작업에는 다음이 포함될 수 있습니다:\n\n- 자동 업데이트\n- 지속적인 통합/지속적인 배포 (CI/CD)\n- 마이크로서비스 및 관리 도구의 자동 확장\n- 고장난 마이크로서비스의 자동 재시작\n- 리소스 비용 최적화\n- 중앙 집중식 모니터링\n\n따라서 만약 고가용성, 높은 신뢰성, 확장성, CI/CD 및/또는 클라우드 서비스 제공업체의 이동성이 필요하다면 쿠버네티스 클러스터를 고려해보시는 것이 좋습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 한 대의 서버에 단일 대형 애플리케이션을 배포하고 있고 (네, 이 경우가 존재합니다) 확장, 복원력, 신뢰성 또는 이식성에 대한 중요한 요구사항이 없다면, 마이크로서비스, 쿠버네티스와 같은 컨테이너 기반의 관리 시스템을 유지하는 비용을 피하고 애플리케이션을 해당 서버에 배포하는 것이 훨씬 나을 수 있습니다.\n\n# 시작점\n\n좋아요, 그래서 쿠버네티스를 사용하기로 결정했습니다. 이제 더 많은 것을 알고 싶어합니다.\n\n쿠버네티스 학습 곡선을 시작할 때, 리소스를 이해해야 합니다. 간단히 말해서, 쿠버네티스는 리소스를 관리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n리소스는 서버(Node), 애플리케이션(Pod), 서비스(Service) 또는 아래 다이어그램에 표시된 것(그리고 그 외에 더 많은 것들)일 수 있습니다.\n\n![다이어그램](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_2.png)\n\n이 표준 리소스 외에도 Kubernetes는 사용자 정의 리소스를 관리하기 위한 플러그인을 허용합니다.\n\n본질적으로 Kubernetes는 리소스 목록(매니페스트라고 함)을 가져와 이를 클러스터 내 서버(또는 노드)에 구축하고 유지하는 명령어 세트로 변환합니다. 목록을 가져와 Kubernetes가 실제 리소스를 설정하도록 하는 이 개념을 선언적 구성이라고 합니다. 목록을 변경하면 Kubernetes가 클러스터 내에서 필요한 변경 사항을 결정합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-20-StartingyourjourneyonKubernetes_3.png\" />\n\nKubernetes는 이 개념을 더 발전시킵니다. 이는 클러스터 내의 리소스를 모니터링하고, 만약 리소스가 실패하거나 더 많은(또는 적은) 리소스가 필요한 경우에는 자동으로 실행 중인 리소스를 필요에 맞게 조정할 수 있습니다.\n\n부하가 증가하면 더 많은 리소스를 할당할 수 있습니다. 리소스가 실패하면 오작동한 리소스를 삭제하고 새 리소스를 생성할 수 있습니다. 리소스를 업데이트하려면 서비스 중단이 없는 방식으로 업그레이드를 배포할 수 있습니다.\n\n# 리소스의 종류\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n알겠어요, 자원에 대한 토론은 조금 추상적이죠. 자원의 몇 가지 구체적인 예시를 살펴볼게요.\n\n## 노드\n\nKubernetes 클러스터를 생성할 때, 먼저 생성하는 자원은 클러스터를 형성할 서버(실제 또는 가상)입니다. 이러한 서버는 Kubernetes에서 노드라고 합니다.\n\n마스터 노드와 워커 노드가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Starting Your Journey on Kubernetes](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_4.png)\n\n마스터 노드(1개 이상)는 제어 평면을 제공합니다. 제어 평면은 자원 목록(하나 이상의 YAML 파일로 정의된 매니페스트로 알려진)을 가져와서 각 워커 노드 내에서 생성된 자원을 필요한 매니페스트와 일치하도록 관리합니다.\n\n동적으로 워커 노드가 실패하면 해당 자원이 손실되고 제어 평면이 자동으로 해당 자원을 워킹 노드에 다시 생성합니다. 제어 평면은 항상 배포된 자원이 매니페스트 파일의 요청된 목록과 일치하도록 합니다.\n\n일반적으로 클러스터에는 마스터 노드의 홀수 개수와 워커 노드의 홀수 개수가 있습니다. 이것은 일부 응용 프로그램 및 도구에서 지도자를 선정하는 데 도움이 됩니다. 대부분의 예는 1개의 마스터와 3개의 워커를 보여줍니다만 클러스터에는 여러 마스터와 수백 개의 노드가 있을 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 파드\n\n이해해야 하는 다음 기본 리소스는 파드입니다.\n\n파드는 하나 이상의 컨테이너 이미지(예: Docker 이미지)의 그룹으로, 애플리케이션 기능을 제공하는 역할을 합니다.\n\n일반적으로 파드에는 하나의 컨테이너 이미지(애플리케이션 또는 마이크로서비스)만 포함됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPods는 일시적입니다. 이는 Kubernetes가 언제든지 pod의 인스턴스를 삭제하고 옵션으로 클러스터의 다른 곳에 다시 생성할 수 있다는 것을 의미합니다. 특정 인스턴스가 항상 존재한다고 확신할 수 없지만, 원하는 경우 Kubernetes가 사용할 수 있는 인스턴스가 있도록 보장할 것입니다.\n\n일부 경우에는 마이크로서비스를 외부에서 적용할 추가 로직이 필요할 수 있습니다. 이 로직을 동일한 pod 내의 다른 컨테이너 이미지로 추가할 수 있습니다. 주 응용 프로그램을 보조하는 경우 이를 sidecar로 알려져 있습니다.\n\n한 pod 내에는 여러 개의 sidecar가 존재할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 연결성\n\n쿠버네티스는 네트워크의 모든 파드가 다른 파드에 연결할 수 있어야 하며(다른 노드에 있더라도), 네트워크 주소 전환(NAT)이 필요하지 않아야 합니다. 이를 위해 각 노드가 클러스터 파드가 사용할 새로운 서브넷에 가입해야 합니다. 나중에 어떻게 이 작업을 수행하는지 살펴보겠습니다.\n\n마이크로서비스 애플리케이션과 사이드카는 각각의 파드 내에서 로컬호스트 네트워크를 통해 연결되며, 파드 자체는 쿠버네티스 클러스터 서브넷을 통해 통신합니다.\n\n![이미지](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 자원 관리\n\n우리는 두 가지 유형의 리소스만을 살펴보았고, 이에 대한 표면적인 내용만 다뤘습니다.\n\n이제 당신은 마이크로서비스를 배포하기 위해 먼저 하나 이상의 매니페스트 파일을 생성해야 함을 이해했을 것입니다. 이 매니페스트 파일에는 마이크로서비스를 포드로 나열한 후 이를 쿠버네티스에 제공하여 실제로 필요에 따라 클러스터 노드 상의 포드로 마이크로서비스를 배포하도록 남기게 됩니다. 쿠버네티스는 포드 간의 네트워크 연결성을 관리할 것입니다.\n\n매니페스트에는 배포가 가져야 하는 특성들을 제공할 수 있습니다. 이러한 특성들은 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리 인간들이 사용할 이름을 정하고\n다른 리소스가 식별하고 선택할 수 있게 해주는 라벨 세트\n필요한 인스턴스의 숫자\n그리고 훨씬 더...\n\n쿠버네티스는 이러한 모든 특성을 고려하여 요구 사항을 충족시키기 위해 필요한 모든 것을 자동으로 배포합니다.\n\n정말 멋지죠.\n\n쿠버네티스 이야기에는 더 많은 것이 있지만, 다른 기사에서 다룰 예정입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 네임스페이스\n\n계속하기 전에, 쿠버네티스의 다른 멋진 기능 중 하나인 네임스페이스에 대해 설명해야 합니다.\n\n파드와 같은 리소스는 다른 네임스페이스에 있으면 격리되어 처리됩니다. 네임스페이스는 실제 클러스터 내에서 가상 하위 클러스터로 생각할 수 있습니다. 다른 네임스페이스의 리소스도 여전히 다른 네임스페이스의 리소스에 연결할 수 있습니다.\n\n네임스페이스는 이름 충돌을 방지할 뿐만 아니라 접근 제어를 통해 보안을 제공할 수도 있습니다. 이는 하나의 개발자가 한 네임스페이스에 액세스하고 다른 개발자가 다른 네임스페이스에 액세스할 수 있지만 둘 다 서로의 네임스페이스에 액세스할 수 없다는 것을 의미합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n네임스페이스는 Kubernetes에서 생성 및 관리할 때 리소스로 취급됩니다.\n\n클러스터 내의 리소스 사이에 격리를 제공하는 동시에 네임스페이스는 리소스 관리에도 도움이 됩니다. 네임스페이스를 삭제하면 해당하는 모든 리소스도 함께 삭제됩니다.\n\n```js\nkubectl config set-context --current --namespace=new-default\n```\n\n일부 유형의 리소스는 클러스터 수준에서 생성되며 네임스페이스에서 생성할 수 없습니다. 예를 들어 Persistent Volume (PV)는 클러스터 수준에서 생성됩니다. PV를 생성하거나 관리할 때는 네임스페이스를 지정하지 않으며 (기본값 사용되지 않음)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n실제로 네임스페이스는 클러스터 수준의 리소스입니다. 즉, 네임스페이스는 네임스페이스를 가질 수 없기 때문에 서로 중첩시킬 수 없습니다.\n\n## 내부 동작\n\n새로운 것에 대해 배울 때, 먼저 원리를 이해하고 나서 어떻게 작동하는지 빨리 알고 싶어해요.\n\n관심이 있다면, 한번 속을 esditeyus.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 쿠버네티스 구성 요소\n\n각 노드 유형 내에는 쿠버네티스의 다양한 구성 요소가 있습니다.\n\n![이미지](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_7.png)\n\n## 워커 노드\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우선 워커 노드를 살펴보면, 각 워커 노드에는 세 가지 구성 요소가 있습니다:\n\n- Kubelet…제어 플레인에 대한 인터페이스를 제공하고 노드의 상태를 업데이트하며, 제어 플레인이 노드에서 실행 중인 컨테이너를 관리하기 위해 제공하는 명령을 수행합니다.\n- Kube Proxy…컨테이너 내에서 실행 중인 모든 응용 프로그램이 서로 직접 통신할 수 있도록 노드에서 네트워킹 관리를 제공합니다(아래에서 더 자세히 설명합니다).\n- 컨테이너 관리…Kubernetes의 중심에는 애플리케이션이 컨테이너 내에서 이미지로 실행되는 것이 있으므로, 노드는 이러한 컨테이너를 실행할 수 있어야 합니다.\n\n이 세 가지 구성 요소를 통해 클러스터가 응용 프로그램을 실행하고 이를 연결할 수 있습니다.\n\n## 구성 요소 교체\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n궁금하시다면, 이러한 구성 요소는 Kubernetes를 설치할 때 기본적으로 제공됩니다. 그러나 귀하의 특정 요구 사항을 충족하는 구성 요소로 교체할 수도 있습니다.\n\n예를 들어, Service라는 다른 리소스 유형이 있습니다. 이전에 일회성으로 이야기했던 팟은 언제든지 생성 및 소멸할 수 있다는 것을 의미합니다. 따라서 만약 이 팟이 다른 팟에 서비스를 제공한다면, 다른 팟이 그것을 어떻게 찾을까요?\n\n서비스가 등장합니다. 서비스는 해당 서비스로의 요청을 작동 중인 마이크로서비스의 인스턴스로 경로 설정하는 방법을 알고 있는 고정된 지속적인 IP 주소를 제공합니다. 팟이 계속 생성 및 소멸해도 필요할 때 목적지 목록을 유지하고 조정합니다.\n\n이를 위해 Kube Proxy는 노드의 운영 체제에 있는 기본 IP Tables을 구성하여 서비스로의 경로 지정 및 배경 마이크로서비스로의 경로 지정이 항상 작동하도록 보장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n대부분의 사용 사례에는 잘 작동하지만, 서비스 수가 수백 개로 증가함에 따라 IP Tables의 규칙 길이가 너무 길어져 효율성이 떨어지게 됩니다.\n\n이런 상황에서는 표준 Kube Proxy를 대체하여 IP Tables보다 더 효율적인 라우팅 방법을 사용하는 대안으로 교체할 수 있습니다. 관심이 있다면 'Kube Proxy 대안'을 검색해보세요.\n\n이것은 일반적인 쿠버네티스 구성 요소를 특정 사용 사례에 맞게 대체하는 예시입니다.\n\n## 마스터 노드\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마스터 노드에서는 Kublet, Kube Proxy 및 컨테이너 관리 기능도 확인할 수 있습니다. 이를 통해 마스터 노드가 워커 노드로도 작동할 수 있습니다 (나중에 이에 대한 장단점에 대해 이야기하겠습니다).\n\n워커 노드 구성 요소 외에도 마스터 노드에는 다음이 있습니다:\n\n- API 서버… 이는 클러스터를 내부적으로나 kubectl과 같은 명령행 도구를 통해 외부적으로 관리하고 모니터링할 수 있는 API입니다.\n- etcd… Kubernetes가 구성과 상태를 저장하는 데 사용하는 분산 키-값 저장소\n- 스케줄러… 클러스터 전체의 pod를 추적하고 필요에 따라 생성 또는 제거하는 구성 요소입니다 (pod 스케줄링이라고도 함)\n- Kube 컨트롤러 매니저… 필요한 리소스를 변경할 때 현재 상태를 필요한 리소스에 맞추기 위해 무엇을 수행해야 하는지 작업하는 구성 요소로, 스케줄러에게 pod 및 다른 리소스의 요구 사항을 제공합니다.\n- 클라우드 컨트롤러 매니저… 경우에 따라 클러스터는 클라우드 제공업체가 자체를 구성해야 할 필요가 있는데 (예: IP 주소 제공 및 로드 밸런서 제공), 이 작업은 이 구성 요소를 통해 수행됩니다.\n\n이들 구성 요소가 함께 워커 노드를 관리하는 제어 플레인을 형성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 마스터 노드를 워커 노드로 사용\n\n마스터 노드를 사용하여 마이크로서비스를 실행해야 하는지 여부에 대해 궁금해 할 수 있습니다.\n\n한편으로는 마이크로서비스 작업을 마스터 노드에서 실행하지 말아야 하는 이유로 클러스터의 제어에 영향을 미치지 않기를 원하지 않으므로 그렇습니다.\n\n이는 매우 타당한 주장이지만, 오퍼레이터라고 불리는 응용 프로그램이 있습니다. 오퍼레이터는 자동 보조 도구 역할을 합니다. 그들은 마이크로서비스를 설치, 관리, 장애 조치, 백업하고 일반적으로 마이크로서비스를 관리합니다. 이러한 오퍼레이터들은 마이크로서비스보다는 제어 평면 구성 요소와 비슷하게 작용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마스터 노드에서 연산자가 실행되는 것이 이해됩니다.\n\n아마도 궁금해 할 수도 있지만, 두 가지를 모두 가질 수 있을 것입니다. 매니페스트 파일에서 리소스를 정의할 때, 파드가 어느 노드에 배치될 수 있는지에 대한 규칙을 정의할 수 있습니다. 이는 마그넷처럼 작용하여 파드를 끌어당기거나 밀어내는 것과 비슷합니다:\n\n- Affinity(우호성) — 파드를 노드로 끌어당김\n- Taints(오점) — 파드를 노드로 밀어냄\n- Tolerations(용인) — 오점을 상쇄함\n\n이들 간의 전반적인 상호작용은 복잡합니다. 이러한 규칙을 사용하면 워커 노드나 마스터 노드에 파드를 예약할 수 있습니다. 또한 한 노드에 두 개의 파드 인스턴스가 예약되지 않거나, 한 마이크로서비스가 다른 노드에 예약되지 않도록 할 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 컨테이너 네트워크 인터페이스\n\n아직 언급하지 않은 두 가지 구성 요소가 있습니다. 둘 모두 네트워크 구성과 관련이 있습니다.\n\nKubernetes는 모든 팟이 Network Address Translation (NAT)이 필요 없이 서로 통신할 수 있어야 한다고 요구합니다.\n\n이는 Container Network Interface (CNI) 플러그인을 사용하여 달성됩니다. CNI는 Flannel, Calico, Weave, Cilium 등 여러 플러그인이 구현하는 표준이며 실제로 CNI 표준은 기타 컨테이너 관리 시스템에서도 사용됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Starting your journey on Kubernetes](/assets/img/2024-05-20-StartingyourjourneyonKubernetes_8.png)\n\nKublet 컴포넌트가 포드와 그 컨테이너를 시작할 때, CNI를 호출하고 해당 포드에 IP 주소를 할당해 달라고 요청합니다. CNI는 또한 컨테이너 내의 네트워크 요소를 프로비저닝하여 클러스터 서브넷에서 포드 간의 원활한 통신을 가능케 합니다. 이러한 요소들은 포드가 예정된대로 스케줄되면 필요에 따라 삭제 및 업데이트됩니다.\n\nCNI 플러그인은 다양한 솔루션을 사용하여 보편적 네트워크를 구현하는 복잡한 네트워크 구성 요소입니다. 이러한 것들을 설명하는 것은 이 글의 범위를 벗어납니다만, Kubernetes CNI를 검색하면 더 많은 정보를 얻을 수 있습니다.\n\n# DNS\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n네트워크 솔루션의 두 번째 부분은 클러스터용 DNS 서비스입니다. Pod가 서비스에 액세스해야 할 때, 해당 서비스를 도메인 이름을 통해 찾습니다.\n\n이전에 말했듯이, 서비스는 Pod가 일회성 Pod 중 하나 이상에 구현될 수 있음에도 불구하고 해당 서비스를 찾을 수 있는 자원입니다. 이러한 서비스는 클러스터 서브넷 내에서 고정된 영속적인 IP 주소를 부여받습니다.\n\n```js\nkubectl cluster-info dump | grep -m 1 service-cluster-ip-range\nkubectl cluster-info dump | grep -m 1 cluster-cidr\n```\n\nIP 주소를 할당받을 뿐만 아니라, 서비스의 이름이 DNS 항목으로 사용됩니다. 예를 들어, hello라는 이름의 서비스를 world 네임스페이스에 생성한다고 가정해봅시다. 서비스가 생성되고 IP 주소를 부여받는데, 예를 들어 10.96.0.5라고 하겠습니다. 그럼 클러스터 DNS는 엔트리를 업데이트하면서 다음과 같은 항목을 추가합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n10.96.0.5 hello\n```\n\n동시에 파드는 /etc/resolv.conf 파일에 검색 기준을 제공받습니다. 예를 들면:\n\n```js\nsearch world.svc.cluster.local svc.cluster.local cluster.local\nnameserver 10.96.0.10\noptions ndots:5\n```\n\n이는 서비스에 다음을 사용하여 액세스할 수 있음을 의미합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nhello;\nhello.world;\nhello.world.svc;\nhello.world.svc.cluster.local;\n```\n\n필요한 경우 다른 네임스페이스에 있는 서비스를 참조할 수 있음을 알 수 있습니다.\n\nDNS는 클러스터의 팟으로 설치된 coreDNS라는 애플리케이션에서 제공됩니다. 다른 솔루션도 사용할 수 있습니다. 다른 리소스와 마찬가지로 coreDNS는 추가적인 사용자 지정 매핑 규칙을 제공하기 위해 매니페스트 파일을 통해 구성할 수 있습니다.\n\n# 실제 리소스 제한하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이전에 언급했던 대로 Kubernetes는 클라우드용 운영 체제와 같습니다. 이런 표현은 너무 넓은 범위일 수도 있어요. 사실은 클러스터용 운영 체제와 비슷합니다.\n\n또한 운영 체제가 리소스를 관리한다고 언급했습니다. 이 경우 리소스는 CPU, 메모리 및 저장 공간과 같은 계산 리소스를 말합니다.\n\n이 비유는 상당히 일치합니다. Kubernetes에게 파드 내 컨테이너가 사용할 계산 리소스의 양을 할당하고 제한하도록 요청할 수 있어요. 이러한 계산 리소스는 노드가 제공하는 리소스에서 할당됩니다.\n\nKubernetes는 이러한 제한을 Unix 개념인 cgroups를 사용하여 적용합니다. 이 기사에서 cgroups에 대해 다루는 것은 범위를 벗어나지만, 프로세스가 사용하는 리소스를 제한할 수 있다는 것만 언급해도 충분해요. 제한은 다양한 방식으로 적용될 수 있어요. 예를 들어, CPU 제한에 도달하면 응용 프로그램이 간단히 느려지지만, 메모리 제한에 도달하면 오류가 발생하여 파드가 종료되고 재스케줄링될 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n리소스를 생성할 때 Kubernetes에게 지정된 계산 리소스 양이 있는 노드에 리소스를 예약하도록 요청할 수도 있습니다. 이것은 요청입니다. 요청은 제한이 설정되어 있는 것을 의미하지 않으며 요청 및 제한 구성은 독립적인 설정입니다.\n\n# 클러스터에 리소스 정의 및 배포\n\n여기까지 오셨다면 이제 Kubernetes가 하나 이상의 매니페스트 파일에 정의된 리소스 목록을 가져다가 클러스터에 배포한다는 것을 이해하게 될 것입니다. 그런 다음 클러스터를 모니터링하고 대상 상태를 유지하므로 리소스가 실패하면 대체할 수 있습니다.\n\nKubernetes는 또한 매니페스트 파일을 감시하며 필요한 리소스에 변경이 있으면 클러스터를 수정하여 요구 사항과 다시 일치하도록 유지합니다. 이는 리소스를 삭제해야 하는 경우에도 해당됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n알겠어요, 지금 어떻게 매니페스트 파일을 구성하는지 궁금할 수도 있겠죠. 매니페스트 파일은 YAML 파일입니다. 각 리소스는 YAML 파일 내의 문서로 정의됩니다.\n\n아마도 YAML 파일이 이렇게 구분되어있는 여러 문서를 포함할 수 있다는 것을 아실 겁니다:\n\n```js\n---\n# 문서 1\n---\n# 문서 2\n---\n# 문서 3\n---\n```\n\n이것은 여러 리소스를 (각 문서당 하나씩) 단일 매니페스트 파일에 정의할 수 있다는 것을 의미합니다. 여러 파일로 배포를 분리할지 또는 모두 단일 파일에 포함시킬지는 여러분의 선택입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 YAML 파일당 하나의 리소스를 가정합니다.\n\n매니페스트 파일의 기본 구조는 아래와 같습니다:\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-1\n  namespace: test-env\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        testService: service-1\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - name: nginx-index-config\n              mountPath: /usr/share/nginx/html\n      volumes:\n        - name: nginx-index-config\n          configMap:\n            name: nginx-config-1\n```\n\n이 경우 NGINX 프록시를 배포하여 테스트 서비스를 정의하고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 조각 씩 분석해 봅시다.\n\n```js\napiVersion: apps/v1\n...\n```\n\n이 부분은 Kubernetes에 어떤 API를 사용할 것인지 알려줍니다. 이 경우 API는 매니페스트 문서의 구조를 의미하며 (Kubernetes에 액세스하는 데 사용되는 API와는 구분됩니다), 모든 리소스 유형에는 연관된 구조와 정의된 API 버전이 있습니다.\n\n사용자 정의 리소스 정의(CRD)를 포함하는 Kubernetes 확장을 로드하여 해당 사용자 정의 API 정의를 가져올 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 경우 apps/v1은 표준 Kubernetes API입니다.\n\n```js\n...\nkind: Deployment\n...\n```\n\n이것은 Kubernetes에게 우리가 생성하려는 리소스 유형을 알려줍니다. 이 경우 Deployment는 하나 이상의 팟에 응용 프로그램을 생성합니다.\n\n```js\n...\nmetadata:\n  name: nginx-deployment-1\n  namespace: test-env\n...\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n메타데이터 섹션 또는 스탄자는 Kubernetes에 리소스에 대한 정보를 제공합니다. 이 경우에는 Deployment 리소스의 이름을 알려주어 나중에 참조할 수 있도록 합니다. 또한 해당 리소스가 주어진 test-env 네임스페이스에 생성되도록 지정합니다.\n\n다음 섹션에는 더 많은 옵션이 있습니다. 여기서 사용한 옵션만 강조하겠습니다.\n\n```js\n...\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n...\n```\n\n이제 우리는 Kubernetes에 spec 스탄자에서 생성해야 할 리소스의 사양 또는 구성에 대해 알려줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 경우에는 명세서가 다음과 같이 말합니다:\n\n- replicas ... 내 애플리케이션의 인스턴스를 하나 생성합니다.\n- selector ... 이 리소스는 이 규칙과 일치하는 레이블을 갖는 모든 리소스를 참조합니다 (app: ngix).\n\nSelector가 항상 약간 혼란스럽습니다. 다음과 같이 생각해보세요. Kubernetes가 우리의 1개 복제본이 존재하는지 확인하러 오면, 복제본을 생성하거나 제거해야 하는지 확인하기 위해 파드 목록을 쿼리해야 합니다. 이를 찾기 위해 이 선택기 규칙을 사용합니다. 즉, app: nginx와 일치하는 레이블을 갖는 모든 파드입니다.\n\n```js\n...\nspec\n  ...\n  template:\n    metadata:\n      labels:\n        app: nginx\n        testService: service-1\n...\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n배포를 위해 Kubernetes에게 무엇을 배포해야 하는지 알려주어야 합니다. 이 스펙(spec) 안에는 어플리케이션을 배포하는 방법과 내용을 정의하는 템플릿을 지정합니다.\n\n템플릿에는 메타데이터 섹션도 포함되어 있습니다. 이 섹션에는 이 템플릿을 사용하여 생성된 모든 파드에 할당될 레이블이 포함되어 있습니다. 이제 Kubernetes가 클러스터를 확인할 때, app: nginx 레이블이 할당된 파드의 인스턴스를 찾을 수 있습니다. 이는 셀렉터와 일치하며 Kubernetes는 이 어플리케이션이 포함된 파드를 생성하거나(또는 제거)해야 하는지 결정할 수 있습니다.\n\n```js\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: nginx-index-config\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: nginx-index-config\n        configMap:\n          name: nginx-config-1\n```\n\n이제 템플릿 내 spec 섹션은 Kubernetes에게 어플리케이션을 생성하고 설정하는 방법을 알려줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너 ... 이는 파드 배포에 포함해야 할 컨테이너를 정의합니다. 이름은 nginx로 하고 이미지는 nginx를 사용해야 합니다. 이 이미지는 여러분이 만든 이미지일 수도 있고 버전을 포함할 수도 있습니다. 노드에 처음으로 파드를 배포할 때 이미지를 다운로드하지만, 이후에 동일한 파드의 새 인스턴스를 만들어야 할 경우 이미지를 다시 다운로드할 필요가 없도록 캐시합니다.\n\nports ... 이는 애플리케이션이 노출하는 포트를 정의하고 클러스터에서 사용할 수 있어야 합니다.\n\nvolumeMounts ... 이는 외부 볼륨을 컨테이너 내 파일 시스템에 매핑하는 것을 정의합니다.\n\n컨테이너를 정의할 뿐만 아니라, 마운트해야 하는 볼륨도 정의해야 할 수 있습니다. 이는 아무 종류의 스토리지일 수 있지만, 이 경우 ConfigMap이라는 다른 리소스 유형을 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      ...\n      volumes:\n      - name: nginx-index-config\n        configMap:\n          name: nginx-config-1\n```\n\n볼륨의 이름을 nginx-index-config로 지정하였고, 이는 volumeMount stanza에 매핑하는 데 사용됩니다. 그런 다음 볼륨의 유형과 구성을 정의합니다. 이 경우에는 nginx-config-1 configMap입니다.\n\n이것으로 거의 다 끝났습니다. 이제 다음 명령으로 배포할 수 있습니다:\n\n```yaml\nkubectl apply -f <filename> -n <namespace>\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n네임스페이스(-n 또는 --namespace)는 선택 사항이며, 이 경우에는 manifest에서 네임스페이스를 정의했기 때문에 필요하지 않습니다. 사용자에게 선택권을 주려면(또는 기본값을 사용하려면), 메타데이터에서 네임스페이스를 생략하면 됩니다.\n\n물론, 이를 배포하려면 configMap 리소스가 필요합니다. 예시를 보겠습니다:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-config-1\n  namespace: test-env\ndata:\n  index.html: |\n    <html>\n    <h2>Hello world 1!!</h2>\n    </html>\n```\n\nconfigMap 리소스를 사용하면 어플리케이션이나 리소스에 자유로운 형태의 구성을 제공할 수 있습니다. 여전히 API 버전, 리소스 유형 및 메타데이터(이 경우에는 네임스페이스 포함)를 제공하는 것을 볼 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 데이터 스탠자를 정의합니다. 여기서는 표시된 HTML 콘텐츠를 포함하는 index.html 파일이라는 데이터가 정의됩니다.\n\n이를 다른 리소스와 마찬가지로 다음과 같이 배포할 수 있습니다:\n\n```js\nkubectl -f <파일이름>\n```\n\n매니페스트 파일에서 이름 공간 옵션을 정의했기 때문에 이름 공간 옵션은 필요하지 않습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKubernetes는 선언적이기 때문에 파일을 적용하는 순서가 중요하지 않아야 합니다. 그러나 NGINX pod가 configMap을 찾는 등의 지속적인 다시 시작을 피하기 위해 의존성을 먼저 배포하는 것이 좋습니다. 이것은 두 매니페스트를 단일 파일에 포함할 수 있는 이유 중 하나입니다.\n\n# 요약\n\n이 글을 통해 Kubernetes에 대한 좋은 소개를 받았기를 바라며, Kubernetes의 기능, 작동 방식 및 사용 방법에 대해 알아보았습니다.\n\n이 글에서는 많은 내용을 다루었습니다. 여기에는 다음이 포함되어 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Microservices\n- 쿠버네티스란 무엇인가요?\n- 왜 쿠버네티스와 같은 것을 사용해야 하는지\n- 리소스를 기반으로 하는 방법에 대한 설명\n- 네임스페이스란 무엇인가요?\n- 내부 작동 방식\n- CNI가 무엇이며 어떻게 사용되는가요?\n- 클러스터에서 DNS가 구성되고 사용되는 방법\n- 컴퓨팅 리소스에 대한 제한\n- 리소스 정의 및 배포\n\n이것은 쿠버네티스에 대한 간략한 소개일 뿐이며, 더 많은 내용이 있습니다.\n\n본문이 유익하다고 생각되면 박수를 눌러주세요. 그렇게 하시면 사람들이 유용하게 여기는 내용과 앞으로 쓸 기사에 대한 정보를 파악하는 데 도움이 됩니다. 의견이나 제안이 있으시면 댓글에 추가해 주세요.\n","ogImage":{"url":"/assets/img/2024-05-20-StartingyourjourneyonKubernetes_0.png"},"coverImage":"/assets/img/2024-05-20-StartingyourjourneyonKubernetes_0.png","tag":["Tech"],"readingTime":31},{"title":"파이썬 FastAPI를 사용한 Kubernetes로의 Microservices 배포 방법","description":"","date":"2024-05-20 17:11","slug":"2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes","content":"\n![FastAPI 및 Kubernetes를 사용한 미니큐브에 두 마이크로서비스 배포하기](/assets/img/2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes_0.png)\n\n이 기사에서는 FastAPI로 2개의 마이크로서비스를 만들고 이를 미니큐브에 배포할 것입니다. 우리는 쿠버네티스 인그레스를 사용하여 요청을 각각의 마이크로서비스로 라우팅할 것입니다.\n\n먼저, 이 기사에서 사용할 쿠버네티스 기능을 살펴보겠습니다.\n\n- **Kubernetes Deployment**: 쿠버네티스 배포는 애플리케이션 업데이트와 스케일링의 자동화를 담당합니다. 이는 애플리케이션의 원하는 상태를 정의하며, 레플리카 수, 사용할 컨테이너 이미지 및 업데이트 전략을 포함합니다. 배포 컨트롤러는 필요에 따라 파드를 작성하고 업데이트하여 애플리케이션의 실제 상태가 원하는 상태와 일치하도록 합니다. 배포는 롤링 업데이트, 롤백 기능 및 셀프 힐링을 지원하여 변경 사항 중에 애플리케이션 가용성과 안정성을 유지하기 쉽게 합니다. 이 추상화는 쿠버네티스 클러스터에서 스테이트리스 애플리케이션을 배포, 확장 및 관리를 간소화합니다. 자세한 정보는 [링크](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)를 참조하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n**Kubernetes Service:** 쿠버네티스 서비스는 논리적인 포드 집합과 이에 접근하기 위한 정책을 정의하는 추상화입니다. 일반적으로 안정적인 IP 주소와 DNS 이름을 통해 접근합니다. 서비스를 통해 응용 프로그램의 다른 부분 간에 통신할 수 있으며 포드의 IP 주소를 알 필요가 없어집니다. 왜냐하면 그 주소는 변경될 수 있기 때문입니다.\n\n[자세히 알아보기](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)\n\n**Kubernetes Ingress:** 쿠버네티스 인그레스는 쿠버네티스 클러스터 내의 서비스에 대한 외부 액세스를 관리합니다. 저희 어플리케이션에서는 해당 서비스(예: 마이크로서비스 1 또는 마이크로서비스 2)로 트래픽을 라우팅하는 데 사용할 것입니다. 호스트 이름 및 경로에 따라 트래픽을 특정 서비스로 라우팅하는 규칙을 정의합니다. NGINX 또는 Traefik과 같은 인그레스 컨트롤러는 이러한 규칙을 실행하여 중앙 집중형 관리, 로드 밸런싱, SSL 종료 및 경로 기반 라우팅을 제공합니다. 인그레스 리소스를 효과적으로 사용하려면 인그레스 컨트롤러를 배포해야 합니다.\n\n[자세히 알아보기](https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 마이크로서비스 1\n\n마이크로서비스 1은 매우 간단하게 될 것입니다. \"You requested microservice 1\"을 반환하는 하나의 엔드포인트만을 가질 것입니다.\n\nmicroservice_1이라는 폴더를 만들고 main.py라는 파일을 추가하세요.\n\n```python\n# main.py (마이크로서비스 1)\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"You requested microservice 1\"}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n요구사항.txt 파일을 추가하세요\n\n```js\nfastapi==0.111.0\n```\n\n```js\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 이미지를 빌드하고 도커허브에 푸시하세요.\n\n이미지를 username/image_name:version 형식으로 태그해야 합니다. 그렇지 않을 경우 쿠버네티스가 이미지를 인식하지 못할 수 있습니다. 이 명령어는 마이크로서비스 1의 Dockerfile이 있는 디렉토리에서 실행되어야 합니다.\n\n```js\n$ docker build . -t sumangaire96/microservice1:v1\n$ docker push sumangaire96/microservice1:v1\n```\n\n이와 같은 메시지가 나타날 것입니다. 만약 나타나지 않는다면, 도커에 로그인되어 있는지 확인하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes_1.png)\n\n# 마이크로서비스 2\n\n마이크로서비스 2도 엔드포인트가 하나만 있어서 \"You requested microservice 2\"를 반환할 것입니다.\n\nmicroservice_1이라는 폴더를 만들고 main.py라는 파일을 추가하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# main.py (microservice 2)\n\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"You requested microservice 2\"}\n\n\nAdd requirements.txt\n\n\nfastapi==0.111.0\n\n\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```\n\n도커 이미지를 빌드하고 도커허브에 푸시하세요.\n\n```bash\n$ docker build . -t sumangaire96/microservice2:v1\n$ docker push sumangaire96/microservice2:v1\n```\n\n# Minikube\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n미니큐브가 설치되어 있는지 확인하세요. 설치되어 있지 않다면 다음 링크를 따르세요: https://minikube.sigs.k8s.io/docs/start/. 터미널에서 미니큐브 클러스터와 상호 작용하기 위해 kubectl이 설치되어 있는지 확인하세요. 설치되어 있지 않다면 다음 링크를 따르세요: https://kubernetes.io/docs/tasks/tools/.\n\n쿠버네티스 클러스터를 생성하세요.\n\n```js\n$ minikube start\n```\n\n미니큐브에서 인그레스를 활성화하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```bash\n$ minikube addons enable ingress\n```\n\n**1. Create kubernetes manifest file for microservice 1.**\n\n```yaml\n# kubernetes/microservice_1.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: microservice1-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: microservice1\n  template:\n    metadata:\n      labels:\n        app: microservice1\n    spec:\n      containers:\n        - name: microservice1\n          image: sumangaire96/microservice1:v1\n          ports:\n            - containerPort: 80\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: microservice1-service\nspec:\n  selector:\n    app: microservice1\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\n```\n\n**2. Create kubernetes manifest file for microservice 2.**\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\n# kubernetes/microservice_2.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: microservice2-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: microservice2\n  template:\n    metadata:\n      labels:\n        app: microservice2\n    spec:\n      containers:\n        - name: microservice2\n          image: sumangaire96/microservice2:v1\n          ports:\n            - containerPort: 80\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: microservice2-service\nspec:\n  selector:\n    app: microservice2\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\n```\n\n쿠버네티스 인그레스 매니페스트 파일을 생성합니다.\n\n```yaml\n# kubernetes/ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: app-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: microservice1.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: microservice1-service\n                port:\n                  number: 80\n    - host: microservice2.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: microservice2-service\n                port:\n                  number: 80\n```\n\n매니페스트를 적용하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n$ kubectl apply -f microservice_1.yaml\n$ kubectl apply -f microservice_2.yaml\n$ kubectl apply -f ingress.yaml\n\n호스트 이름을 Minikube IP에 매핑하기 위해 호스트 파일을 업데이트하세요.\n\n$ echo \"$(minikube ip) microservice1.local\" | sudo tee -a /etc/hosts\n$ echo \"$(minikube ip) microservice2.local\" | sudo tee -a /etc/hosts\n\n이제 microservice1.local 및 microservice2.local로 이동하면 다음과 같은 출력이 나타납니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 표로 변환한 내용입니다.\n\n| 이미지                                                                                                                                                            |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ![FastAPI와 Kubernetes를 사용한 2024-05-20 날짜의 마이크로서비스 배포 이미지](/assets/img/2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes_2.png)         |\n| ![FastAPI와 Kubernetes를 사용한 2024-05-20 날짜의 또 다른 마이크로서비스 배포 이미지](/assets/img/2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes_3.png) |\n\n축하합니다! FastAPI로 2개의 마이크로서비스를 만들고 Kubernetes를 사용하여 성공적으로 배포했습니다. 앞으로 나올 문서를 기대해 주세요.\n","ogImage":{"url":"/assets/img/2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes_0.png"},"coverImage":"/assets/img/2024-05-20-FastAPIMicroservicesDeploymentUsingKubernetes_0.png","tag":["Tech"],"readingTime":10},{"title":"K8s - kube-proxy 소개","description":"","date":"2024-05-20 17:09","slug":"2024-05-20-K8skube-proxyIntroduction","content":"\n![kube-proxy](/assets/img/2024-05-20-K8skube-proxyIntroduction_0.png)\n\nkube-proxy는 kubelet과 마찬가지로 Kubernetes 시스템 내 각 노드에서 실행되는 데몬입니다. 이것은 클러스터 내에서 기본로드 밸런싱을 담당합니다. kube-proxy의 작동은 서비스 및 엔드포인트/엔드포인트 슬라이스에 기반합니다:\n\n- 서비스: 이들은 팟 그룹을위한 로드 밸런서로 작동합니다.\n- 엔드포인트(및 엔드포인트 슬라이스): 이것들은 서비스에서 동일한 팟 선택기를 사용하여 자동으로 생성된 준비된 팟 IP 시리즈를 열거합니다.\n\nKubernetes의 대부분의 서비스 유형은 클러스터 내부 IP 주소 인 클러스터 IP 주소를 보유하며, 클러스터 외부에서 액세스할 수 없습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nkube-proxy는 여러분이 클러스터 IP 주소로 요청을 안내하고 그 요청이 건강한 파드에 도달하도록 하는 역할을 합니다. kube-proxy에는 네 가지 모드가 있으며, 이 모드에 따라 실행 모드와 구체적인 기능 세트가 변경됩니다:\n\n- Userspace 모드 (폐기 예정): 이 모드에서 kube-proxy는 각 서비스에 대해 포트에서 수신 대기합니다. 트래픽을 수신하면 해당 트래픽을 백엔드 파드 중 하나로 프록시합니다. 이 방법은 성능에 영향을 미치므로 일반적으로 사용되지 않습니다.\n- iptables 모드: 이 모드에서 kube-proxy는 네트워크 규칙을 구성하여 서비스의 트래픽을 올바른 백엔드 파드로 안내합니다. 이 모드는 userspace 모드보다 빠르고 믿을 만합니다. kube-proxy의 운영에 대한 기본 모드입니다.\n- IPVS 모드: IPVS (IP Virtual Server) 모드는 iptables와 유사하지만 해시 테이블을 사용하여 백엔드를 만들어 네트워크 트래픽 측면에서 훨씬 확장 가능하고 효율적입니다.\n- Kernelspace 프록시 모드: 이 프록시 모드는 Windows 노드에서만 사용할 수 있습니다. kube-proxy는 Windows Virtual Filtering Platform (VFP)에 있는 패킷 필터링 규칙을 구성합니다. 이것은 Windows vSwitch의 확장입니다.\n\n# Userspace 모드 (폐기 예정)\n\n최초이자 가장 오래된 운영 모드는 userspace 모드입니다. 이 모드에서 kube-proxy는 웹 서버를 운영하고 모든 서비스 IP 주소를 이 서버로 안내하여 iptables를 활용합니다. 이 웹 서버는 연결을 완료하고 프록시 역할을 수행하여 서비스의 엔드포인트에 나열된 파드로 요청을 전달합니다. 그러나 userspace 모드는 지금은 거의 사용되지 않으며, 이용할만한 강력한 이유가 없다면 피하는 것이 좋습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예를 들어, Cluster IP가 10.0.0.1인 Service S가 있고 이 서비스에는 3개의 백엔드 Pod(A, B, C)가 있습니다. 이제 클라이언트가 Service S에 연결하려면 10.0.0.1로 연결될 것입니다.\n\n- 각 노드에서 실행되는 유저스페이스 모드의 kube-proxy는 이 연결을 가로챕니다. 이는 10.0.0.1로 오는 트래픽을 해당 노드에서 실행 중인 자체 프록시 서버로 전달하는 iptables 규칙을 유지합니다.\n- 프록시 서버는 건강한 백엔드 Pod 목록(A, B, C)을 유지하며, 서비스에 구성된 세션 어피니티 및 로드 밸런싱 알고리즘에 따라 요청을 전달할 Pod를 선택합니다.\n- 프록시 서버는 선택된 Pod에 대한 새로운 연결을 설정하고, 클라이언트의 요청을 보내고, Pod로부터 응답을 받은 후 응답을 클라이언트에게 전달합니다.\n\n이 프로세스는 기능적이지만 네트워크 경로에 추가적인 홉(유저스페이스 프록시 서버)을 도입하여 성능 부담을 유발하며, 다른 모드만큼 효율적이지 않을 수 있습니다. 이러한 이유로 유저스페이스 모드는 요즘 거의 사용되지 않습니다. kube-proxy의 기본 모드는 iptables이며, 고성능의 커널 수준 로드 밸런싱이 필요한 경우 ipvs 모드가 사용됩니다.\n\n# iptables 모드\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`iptables` 모드는 작동에 iptables에만 의존합니다. 이게 기본 모드이자 가장 많이 사용되는 모드입니다. IPVS 모드는 최근에 General Availability (GA) 안정성을 달성했지만, iptables는 이미 잘 알려진 Linux 기술이기 때문에 일부로 인해 이 모드가 더 많이 사용될 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-K8skube-proxyIntroduction_1.png)\n\n`iptables` 모드는 실제 로드 밸런싱이 아니라 연결 분배를 용이하게 합니다. 다시 말해, `iptables` 모드가 연결을 백엔드 팟으로 라우팅하면 해당 연결을 통한 후속 요청은 해당 연결이 종료될 때까지 계속해서 동일한 팟으로 전달됩니다. 최적의 시나리오에서는, 이러한 행동이 간단하고 예측 가능하여 같은 연결 내 연속적인 요청은 백엔드 팟의 로컬 캐싱을 이용할 수 있습니다.\n\n하지만 이 방식은 HTTP/2 연결과 같이 오랜 기간 지속되는 연결 같은 예측할 수 없는 행동을 유발할 수 있습니다. 특히 HTTP/2가 gRPC의 전송 프로토콜로 사용되는 점이 주목할 만합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 예로, 서비스가 X와 Y 두 개의 팟으로 제공되는 상황을 고려해 봅시다. 전형적인 롤링 업데이트 중에 X가 Z로 교체됩니다. 이전 팟인 Y는 모든 기존 연결을 유지하면서도 X 팟이 종료되었을 때 다시 설정해야 했던 연결의 절반을 담당합니다. 이로 인해 Y 팟이 제공하는 트래픽이 상당히 증가할 수 있습니다. 이와 같이 트래픽의 불균형을 야기할 수 있는 여러 상황이 있습니다.\n\n예를 들어, 클러스터 IP가 10.0.0.1이고 백엔드 팟 A, B, C를 갖는 서비스 S가 있다고 가정해 봅시다.\n\niptables 모드에서 클라이언트가 10.0.0.1의 서비스 S에 연결하려고 하면, 연결 요청은 kube-proxy에 의해 가로채집니다.\n\n모든 노드에서 실행되는 kube-proxy는 10.0.0.1로 오는 트래픽을 백엔드 팟(A, B, 또는 C 중 하나)으로 투명하게 전달하는 iptables 규칙을 유지합니다. 이를 위해 패킷의 목적지 IP 주소를 하나의 팟의 IP 주소로 변경하는 NAT 규칙을 생성합니다. kube-proxy는 서비스가 구성한 세션 어피니티와 로드 밸런싱 알고리즘에 기반하여 백엔드 팟을 선택합니다. 이 결정은 연결이 설정될 때 만들어집니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 번 iptables 규칙이 적용되어 패킷의 대상이 변경되면, Linux 커널 자체가 해당 패킷을 선택된 Pod로 전달합니다. 여기서 주요한 점은 한 번 연결이 설정되면, 해당 연결에 대한 모든 패킷이 iptables 규칙에 따라 Linux 커널에 의해 자동으로 선택된 Pod로 전달된다는 것입니다. 각 패킷을 처리하기 위해 kube-proxy가 필요하지 않으므로 iptables 모드가 사용자 공간 모드보다 더 효율적입니다.\n\n위 시나리오를 위한 샘플 iptables 규칙:\n\n```js\n# 서비스 S로 전송되는 트래픽을 일치시키고 대상을 변경하는 규칙\niptables -t nat -A PREROUTING -p tcp -d 10.0.0.1 --dport 80 -j DNAT --to-destination 172.17.0.2:80\niptables -t nat -A PREROUTING -p tcp -d 10.0.0.1 --dport 80 -j DNAT --to-destination 172.17.0.3:80\niptables -t nat -A PREROUTING -p tcp -d 10.0.0.1 --dport 80 -j DNAT --to-destination 172.17.0.4:80\n\n# Pod에서 나가는 트래픽을 노드 자체에서 생성된 것처럼 변형하는 규칙\niptables -t nat -A POSTROUTING -s 172.17.0.0/16 -j MASQUERADE\n```\n\n- 첫 번째 규칙 세트는 nat 테이블의 PREROUTING 체인에 있습니다. 이러한 규칙은 서비스 IP 10.0.0.1로 오는 트래픽과 포트 80으로 가는 트래픽과를 일치시키고 패킷의 대상을 백엔드 Pod 중 하나로 변경합니다.\n- POSTROUTING 체인의 MASQUERADE 규칙은 Pod에서 나오는 패킷의 소스 IP를 노드의 IP로 변경하여 응답이 올바르게 노드로 라우팅되고 클라이언트로 다시 라우팅될 수 있도록 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# IPVS 모드\n\nKubernetes의 kube-proxy에서 IPVS 모드는 IP Virtual Server를 나타냅니다. 이 모드는 iptables나 사용자 공간 모드보다 고성능의 커널 수준 로드 밸런싱 및 더 정교한 로드 밸런싱 알고리즘을 제공합니다.\n\nIPVS 모드에서 kube-proxy는 netfilter 후크를 사용하여 패킷을 캡처하고, 그런 다음 해당 패킷을 리눅스 커널의 IPVS 모듈에 전달합니다. IPVS 모듈은 IP 기반 로드 밸런싱을 수행하고 선택된 로드 밸런싱 알고리즘에 따라 패킷을 백엔드 Pod로 전달합니다. IPVS 로드 밸런싱 알고리즘에는 라운드로빈, 최소 연결, 가장 짧은 예상 지연 등이 포함될 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-K8skube-proxyIntroduction_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nIPVS 모드는 대규모 서비스 규모에 적합하며 iptables보다 일관된 해싱을 제공합니다. 또한 더 나은 네트워크 처리량, 더 나은 프로그래밀 수 및 IP 주소 및 포트 이외의 요소에 기반한 부하 분산 기능을 제공합니다.\n\n예를 들어 클러스터 IP가 10.0.0.1이고 백엔드 Pod A, B 및 C가 있는 S 서비스가 있다고 가정해 봅시다.\n\nIPVS 모드에서:\n\n- 클라이언트가 10.0.0.1에 연결하려고 할 때 연결 요청이 kube-proxy에 의해 잡힙니다.\n- 각 노드에서 실행되는 kube-proxy는 Linux 커널의 IPVS 모듈을 사용하여 10.0.0.1로 오는 트래픽을 백엔드 Pod (A, B 또는 C) 중 하나로 보냅니다.\n- IPVS 모듈은 서비스의 구성된 세션 어 피니티 및 부하 분산 알고리즘을 기반으로 백엔드 Pod를 선택합니다.\n- IPVS 모듈이 패킷의 대상을 업데이트하면 Linux 커널 자체가 선택된 Pod로 패킷을 전달합니다.\n  IPVS 모드는 고급 로드 밸런싱 기능을 제공하며 iptables 또는 사용자 공간 모드보다 더 많은 서비스 및 백엔드를 처리할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기는 kube-proxy가 로드 밸런싱을 설정하는 데 사용할 수 있는 IPVS 명령어를 시뮬레이션한 것입니다:\n\n```js\n# IPVS 서비스 추가\nipvsadm -A -t 10.0.0.1:80 -s rr\n\n# 백엔드 서버(팟) 추가\nipvsadm -a -t 10.0.0.1:80 -r 172.17.0.2:80 -m\nipvsadm -a -t 10.0.0.1:80 -r 172.17.0.3:80 -m\nipvsadm -a -t 10.0.0.1:80 -r 172.17.0.4:80 -m\n```\n\n-s rr 옵션은 스케줄링 방법을 라운드 로빈으로 설정합니다. -m 옵션은 포워딩 방법을 마스커레이팅으로 설정하며, 이는 SNAT과 유사합니다. 이러한 명령어로 규칙을 설정한 후, 다음과 같은 내용을 표시할 때 룰을 확인할 수 있습니다:\n\n```js\n# IPVS 규칙 표시\nipvsadm -L -n\n\nIP Virtual Server 버전 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.0.0.1:80 rr\n  -> 172.17.0.2:80                Masq    1      0          0\n  -> 172.17.0.3:80                Masq    1      0          0\n  -> 172.17.0.4:80                Masq    1      0          0\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# KernelSpace Mode\n\n**KernelSpace** 모드는 가장 최근에 추가된 기능이며 Windows 시스템에서만 사용할 수 있습니다. Kubernetes를 Windows에서 사용할 때 유저스페이스 모드에 대한 대체 방법을 제공합니다. 왜냐하면 iptables과 ipvs는 리눅스에 특화된 기능들이기 때문입니다.\n","ogImage":{"url":"/assets/img/2024-05-20-K8skube-proxyIntroduction_0.png"},"coverImage":"/assets/img/2024-05-20-K8skube-proxyIntroduction_0.png","tag":["Tech"],"readingTime":9},{"title":"디브옵스를 배우고 싶지만 어떻게 시작해야 할지 모르겠다구요 여기 가이드가 도와줄게요","description":"","date":"2024-05-20 17:07","slug":"2024-05-20-WanttoLearnDevOpsbutDontKnowWheretoStartHeresYourGuide","content":"\n안녕하세요 여러분, 이 블로그에서는 데브옵스 학습 과정에서 배운 몇 가지 팁을 공유하려고 합니다. 데브옵스 여정을 시작한 방법, 따라 온 경로 및 더 나은 미래를 위해 따를 수 있는 방법에 대해 설명할 것입니다. 또한 제 LinkedIn 프로필을 확인하여 현재까지 데브옵스에서 한 일들과 제 GitHub에 있는 모든 데브옵스 프로젝트를 확인할 수 있습니다.\n\n도구 경로에 대한 설명도 할 예정이에요. 공유하는 링크들에서 제 모든 노트를 찾을 수 있으며 사용한 강의도 함께 공유할 예정입니다.\n\n원하신다면 제 노트에 기여할 수도 있습니다 (GitHub 저장소에 있습니다). Pull 요청을 만들면 리뷰하고 병합할 것입니다.\n\n이제 시작해봅시다. 학습 경로에 들어가기 전에 먼저 데브옵스를 이해하려고 노력해 볼 것입니다. 이것은 책의 정의가 아니라 현재까지 모아 온 데브옵스에 대한 몇 마디일 뿐입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 데브옵스란 무엇인가요?\n\n간단히 말해서, 데브옵스는 조직이 개발 및 운영 팀을 하나로 통합하는 방법론입니다. 이를 통해 두 팀이 협력하여 효율적으로 애플리케이션을 개발하고 배포할 수 있습니다.\n\n데브옵스는 애플리케이션을 개발하고 배포하는 프로세스를 자동화하는 다양한 도구를 포함한 종합적인 용어입니다.\n\n데브옵스의 주요 이점 중 하나는 이러한 프로세스를 자동화함으로써 수작업 방법에 비해 시간 소요가 적고 오류 가능성이 낮아진다는 점입니다. 이 자동화는 시장 진입 시간(TTM)을 단축시킴으로써 빠르게 성장하는 기업에 이상적입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 어떻게 나는 데브옵스 학습 여정을 시작했을까? 🤔\n\n솔직히 처음에는 데브옵스를 배우기로 계획한 적이 없었습니다. 제 여정은 클라우드 컴퓨팅에 대한 흥미로 시작되었습니다. 클라우드 기술에 더 심층적으로 파고들수록, 데브옵스와 클라우드 컴퓨팅이 밀접하게 관련되어 있고 매우 잘 보완되는 것을 깨달았습니다.\n\n클라우드와 데브옵스에 대해 학습을 시작할 때 저는 파이썬에 대한 기본적인 이해만 있었습니다. 올바른 학습 자료와 다양한 기술을 배울 최적의 순서를 찾는 데 어려움을 겪었습니다. 길을 찾기 위해 블로그를 많이 읽고, 수많은 YouTube 비디오를 시청했습니다. 이러한 자료들은 데브옵스와 클라우드 컴퓨팅 분야에서 경력을 시작하는 방법을 이해하는 데 도움이 되었습니다.\n\n# 마주한 어려움들은 무엇인가요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 직면한 주요 어려움 중 하나는 올바른 학습 자원을 찾는 것이었습니다. 무수히 많은 자습서, 블로그, 강좌 및 비디오가 있지만, 모든 학습자에게 적합한 것은 아닙니다. 이 어려움을 극복한 방법은 다음과 같습니다:\n\n- 자원 과부하: 많은 자원이 제공되어 어떤 것을 따라가야할지 결정하기가 압도적이었습니다. 제 학습 스타일과 일치하는 자료를 찾기 위해 다양한 자료를 살펴본 시간이 많았습니다.\n- 진도와 이해: 모든 사람은 서로 다른 속도와 방식으로 학습합니다. 제 진도와 이해 수준에 맞는 자원을 찾아야 했습니다. 어떤 자료는 너무 고급스러웠고, 다른 것들은 너무 기초적이었습니다.\n\n# 학습 경로\n\n다음은 저의 데브옵스 여정 중 따라 온 학습 경로입니다. 이미 알고 있는 기술은 무시하셔도 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-20-WanttoLearnDevOpsbutDontKnowWheretoStartHeresYourGuide_0.png)\n\n1. 프로그래밍 기초부터 시작하세요 (Python 또는 Golang)\n   가장 먼저 해야 할 일은 프로그래밍 언어를 배우는 것입니다. Python과 Golang은 좋은 선택지입니다. 변수, 반복문, 함수와 같은 기초부터 시작해보세요.\n\n2. 컴퓨터 네트워킹 이해하기\n   다음으로 컴퓨터 네트워킹에 대해 잘 파악해보세요. 이것은 컴퓨터가 서로 연결하고 통신하는 방법에 관한 것입니다. IP 주소, DNS 및 기본 네트워킹 개념을 배워보세요.\n\n3. Linux와 Bash에 익숙해지기\n   대부분의 DevOps 도구는 Linux에서 실행되므로 이 운영 체제에 대해 잘 알고 있어야 합니다. Bash는 리눅스의 명령줄 도구로, 스크립트를 작성하고 작업을 자동화하는 데 사용할 것입니다. 걱정하지 마세요, Linux 전문가가 되어야 할 필요는 없습니다. 기본적인 것에 익숙해지기만 하면 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n4. Git을 이용하여 버전 관리 마스터하기\n   Git은 코드의 변경사항을 추적하는 데 도움이 되는 도구입니다. 여러분의 프로젝트를 위한 타임머신 같은 역할을 합니다. 여러분의 작업을 저장하고 변경사항을 추적하며 다른 사람들과 협업하는 방법을 배워보세요.\n\n5. 클라우드 제공업체(AWS, Azure, Google Cloud) 배우기\n   클라우드 제공업체는 여러분이 인터넷을 통해 컴퓨터와 저장소를 빌릴 수 있게 해줍니다. AWS(Amazon Web Services)는 인기 있는 선택지이지만 Azure와 Google Cloud도 훌륭합니다. 이 플랫폼을 활용하여 여러분의 애플리케이션을 배포하고 관리하는 방법을 배워보세요.\n\n6. Docker로 컨테이너화에 몰두하기\n   Docker는 여러분의 애플리케이션을 패키징하여 여러분의 노트북부터 거대한 클라우드 서버까지 어디에서든 실행할 수 있게 해줍니다. 이는 애플리케이션을 위한 휴대용 컨테이너라고 생각하시면 됩니다. 이러한 컨테이너를 만들고 관리하는 방법을 배워보세요.\n\n7. CI/CD 도구 선택하기\n   CI/CD는 지속적 통합/지속적 배포를 의미합니다. Jenkins, CircleCI, GitLab CI와 같은 도구들은 여러분의 코드를 테스트하고 배포하는 프로세스를 자동화해줍니다. Jenkins는 흔히 선택되는 도구입니다. 자동화된 파이프라인을 설정하여 개발 프로세스를 효율적으로 운영하는 방법을 배워보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n8. Terraform을 사용해 인프라 관리하기\n   Terraform은 서버 및 데이터베이스와 같은 인프라를 코드로 정의할 수 있는 도구입니다. 이는 환경을 자동으로 설정하고 청소할 수 있음을 의미합니다. 클라우드 리소스를 관리하기 위해 Terraform 스크립트를 작성하는 방법을 배우세요.\n\n9. Ansible을 사용해 구성 자동화하기\n   Ansible은 서버 구성을 자동화하는 데 도움이 됩니다. 각 서버를 수동으로 설정하는 대신 스크립트인 Playbook을 작성하여 작업을 수행할 수 있습니다. 이렇게 함으로써 시간을 절약하고 오류를 줄일 수 있습니다.\n\n10. Kubernetes 또는 Docker Swarm을 사용해 컨테이너 조정하기\n    많은 컨테이너를 가지고 있을 때는 이를 관리해야 합니다. Kubernetes (일반적으로 K8S로 불림)와 Docker Swarm은 컨테이너화된 애플리케이션을 배포, 관리 및 확장하는 데 도움이 되는 도구입니다. 이 도구들을 사용하여 모든 것을 원활하게 유지하는 방법을 배우세요.\n\n11. Prometheus와 Grafana로 모니터링하기\n    모니터링은 애플리케이션의 건강을 유지하는 데 중요합니다. Prometheus는 애플리케이션 성능에 대한 데이터를 수집하며, Grafana는 이 데이터를 아름다운 대시보드로 시각화할 수 있게 해줍니다. 문제가 커지기 전에 문제를 파악할 수 있도록 모니터링을 설정하는 방법을 배우세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이러한 단계를 따라가면 DevOps의 견고한 기초를 구축할 수 있습니다. 기초부터 시작하여 각 단계를 당신의 속도로 진행해보세요. 즐겁게 배우세요!\n\n# 위 기술에 대한 자료:\n\n위에서 언급한 기술에 대한 자료를 찾고 싶으시다면, 제가 학습한 대부분의 내용을 담은 GitHub 저장소가 있습니다. 이 저장소를 팔로우하고 자신의 노트를 병합하기 위한 풀 리퀘스트도 생성할 수 있습니다. 각 저장소마다 제가 작업한 도구 목록을 찾을 수 있습니다.\n\nAnsible 학습 저장소: 여기를 클릭하세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMarkdown 형식:\n\n- [도커 학습 레포지토리: 클릭하세요](https://github.com/docker-learning)\n- [쿠버네티스 학습 레포지토리: 클릭하세요](https://github.com/kubernetes-learning)\n- [Argo CD 학습 레포지토리: 클릭하세요](https://github.com/argo-cd-learning)\n- [젠킨스 학습 레포지토리: 클릭하세요](https://github.com/jenkins-learning)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGit, Networking, Linux Repository: [여기를 클릭해주세요](https://www.example.com)\n\n이제 사용한 리소스를 공유하겠습니다. 걱정 없이 사용하실 수 있을 거에요. 도움이 될 거라 믿어요.\n\n- 클라우드 컴퓨팅을 배우기 위해서는 해당 공식 학습 플랫폼을 사용하시면 됩니다. 예를 들어, AWS에는 AWS Skill Builder, Azure에는 Microsoft Learn 등이 있습니다.\n- DevOps 도구를 배우기 위해서는 해당 공식 문서를 효과적으로 활용하는 것을 추천해요. 만약 튜토리얼을 원하신다면, YouTube의 Abhishek Veeramala를 추천해요. 그의 YouTube 플레이리스트를 통해 다양한 도구에 대한 내용을 학습하실 수 있습니다. 그는 여러 도구에 대한 많은 Zero to Hero 시리즈도 만들었어요. Git, Linux, 그리고 Networking 관련된 비디오도 Abhishek Veeramala의 YouTube 채널에서 찾아보실 수 있습니다.\n\n이러한 기술들을 가장 잘 이해하고 실시간 경험을 하기 위해, 라이브 부트캠프에 참여하는 것을 권장해요. 클라우드와 DevOps를 위한 많은 무료 부트캠프들이 인터넷 상에서 제공되고 있어요. 만약 부트캠프에 참가할 기회가 있다면, 이 기회를 최대한 활용해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLinkedIn에서 연결합시다: LinkedIn 프로필\n\n더 많은 실전 프로젝트를 살펴보세요 (제 저장소가 도움이 되셨다면, GitHub에서 저를 팔로우하는 것을 잊지 마세요): 내 GitHub 계정\n","ogImage":{"url":"/assets/img/2024-05-20-WanttoLearnDevOpsbutDontKnowWheretoStartHeresYourGuide_0.png"},"coverImage":"/assets/img/2024-05-20-WanttoLearnDevOpsbutDontKnowWheretoStartHeresYourGuide_0.png","tag":["Tech"],"readingTime":8},{"title":"도커 도컴포즈를 사용하여 PostgreSQL을 설치하는 방법","description":"","date":"2024-05-20 17:06","slug":"2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose","content":"\nPostgreSQL은 개발 커뮤니티에서 널리 사용되는 인기 있는 관계형 데이터베이스 관리 시스템입니다.\n\n![이미지](/assets/img/2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose_0.png)\n\nDocker Compose는 여러 컨테이너로 구성된 Docker 응용 프로그램을 정의하고 실행할 수 있는 강력한 도구입니다. 여러 컨테이너를 쉽게 관리하기 위해 구성을 단일 YAML 파일에 정의할 수 있어요.\n\nPostgreSQL은 개발 커뮤니티에서 널리 사용되는 인기 있는 관계형 데이터베이스 관리 시스템입니다. Docker Compose를 사용하면 PostgreSQL 인스턴스를 컨테이너에서 쉽게 설정하고 실행할 수 있어요. 이는 개발, 테스트 및 배포 목적으로 훌륭한 솔루션일 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 블로그 포스트에서는 Docker Compose를 사용하여 PostgreSQL을 설치하는 단계를 안내해 드리겠습니다.\n\n단계 1: Docker 설치\n\nDocker Compose를 설치하고 사용하기 전에 시스템에 Docker가 설치되어 있는지 확인해야 합니다. Docker는 Windows, macOS 및 Linux 운영 체제용으로 제공됩니다. 공식 Docker 웹사이트에서 적절한 버전의 Docker를 다운로드할 수 있습니다.\n\n단계 2: Docker Compose 설치\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커를 시스템에 설치한 후에는 공식 도커 컴포즈 설치 안내에 따라 도커 컴포즈를 설치할 수 있어요. 운영 체제에 따라 설치 단계가 조금씩 다를 수 있지만, 일반적으로 직관적입니다.\n\nStep 3: 도커 컴포즈 파일 생성하기\n\n도커 컴포즈를 사용하여 PostgreSQL 컨테이너를 생성하기 위해 구성을 도커 컴포즈 파일에 정의해야 해요. 원하는 디렉토리에 docker-compose.yml이라는 새 파일을 만들고 다음 코드를 붙여넣어 주세요:\n\n```js\nversion: '3.9'\n\nservices:\n  postgres:\n    image: postgres:14-alpine\n    ports:\n      - 5432:5432\n    volumes:\n      - ~/apps/postgres:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=S3cret\n      - POSTGRES_USER=citizix_user\n      - POSTGRES_DB=citizix_db\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위와 같은 명령어입니다:\n\n- up은 컨테이너를 구동합니다.\n- -d는 백그라운드 모드로 실행합니다.\n\n이 파일에서는 official PostgreSQL 이미지를 사용하는 db라는 단일 서비스를 정의합니다. 또한 default postgres 사용자의 암호를 지정하기 위해 POSTGRES_PASSWORD 환경 변수를 설정합니다. 마지막으로 컨테이너 내의 /var/lib/postgresql/data 디렉토리에 로컬 ./data 디렉토리를 장착하여 PostgreSQL 데이터를 저장하는 위치를 지정합니다.\n\n단계 4: PostgreSQL 컨테이너 시작\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPostgreSQL 컨테이너를 시작하려면 터미널에서 docker-compose.yml 파일이 있는 디렉토리로 이동하고 다음 명령을 실행하세요:\n\n```js\ndocker-compose up -d\n```\n\n```js\n➜ docker-compose up -d\nCreating network \"pg_default\" with the default driver\nCreating pg_postgres_1 ... done\n```\n\n위 명령을 사용하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 'up' 명령어를 사용하면 컨테이너를 올릴 수 있습니다.\n- '-d' 옵션은 detached 모드로 실행합니다.\n\n이 명령어는 컨테이너를 detached 모드로 시작합니다. 이는 백그라운드에서 실행됨을 의미합니다. 컨테이너가 실행 중인지 확인하려면 다음 명령어를 실행하세요:\n\n```js\ndocker ps\n```\n\n```js\n➜ docker-compose ps\n    Name                   Command              State                    Ports\n------------------------------------------------------------------------------------------------\npg_postgres_1   docker-entrypoint.sh postgres   Up      0.0.0.0:5432-\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n스텝 5: PostgreSQL 컨테이너 중지 및 제거\n\nPostgreSQL 컨테이너를 중지하고 제거하려면 터미널에서 docker-compose.yml 파일이 있는 디렉토리로 이동한 후 다음 명령을 실행하세요:\n\n```js\ndocker-compose down\n```\n\n이렇게 하면 컨테이너가 중지되고 제거됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 컴포즈를 사용하여 PostgreSQL을 설치하는 것은 컨테이너 내에서 빠르게 PostgreSQL 인스턴스를 설정할 수 있는 간단한 과정입니다. 도커 컴포즈 파일에서 컨테이너의 구성을 정의하여 여러 컨테이너를 쉽게 관리하고 확장할 수 있습니다.\n\n도커 컴포즈를 사용하여 PostgreSQL을 설치하려면 먼저 시스템에 도커와 도커 컴포즈를 설치해야 합니다. 그 후 도커 컴폏 파일을 만들고 PostgreSQL 컨테이너의 구성을 정의할 수 있습니다. 그런 다음 컨테이너를 시작하고 psql과 같은 PostgreSQL 클라이언트를 사용하여 연결할 수 있습니다.\n\n도커 컴포즈를 사용하면 PostgreSQL 컨테이너를 쉽게 관리하고 확장할 수 있으며 개발 및 테스트 환경을 설정하는 간단하고 반복 가능한 방법을 제공합니다. 전반적으로 도커 컴포즈는 복잡한 애플리케이션을 관리하고 배포하는 프로세스를 간소화할 수 있는 강력한 도구입니다.\n","ogImage":{"url":"/assets/img/2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose_0.png"},"coverImage":"/assets/img/2024-05-20-DockerHowtoInstallPostgreSQLusingDockerCompose_0.png","tag":["Tech"],"readingTime":5},{"title":"도커 컨테이너에서 상호 TLS를 구현하는 방법","description":"","date":"2024-05-20 17:05","slug":"2024-05-20-HowtoImplementMutualTLSwithDockerContainers","content":"\n컨테이너와 마이크로서비스의 등장으로 인해 서비스 간의 통신이 HTTP와 같은 프로토콜을 통해 더 자주 발생할 가능성이 높아졌습니다. 그러나 서비스가 신뢰할 수 없는 네트워크를 통과하는 경우(예: 클라우드 내), 그들의 통신이 안전한지 어떻게 확인할 수 있을까요? 한 가지 방법은 상호 전송 계층 보안(mTLS)을 통해 가능합니다. 이를 통해 서비스들은 상호 인증(서비스가 자신이라고 주장하는 대로인지 어떻게 알 수 있을까요?) 및 통신 암호화를 할 수 있습니다. 그렇다면 mTLS는 어떻게 작동할까요? 한 번 자세히 알아봅시다.\n\n두 당사자 간의 상호 인증 및 암호화는 분명히 새로운 것이 아닙니다. SSH 및 IPSec(대부분의 VPN 기술을 구동하는 프로토콜의 기반이 되는)과 같은 프로토콜의 기초가 되었으며 최근에는 Istio와 Linkerd와 같은 서비스 메시 프로젝트에서도 채택되었습니다.\n\n운영용 사례에서 서비스 메시는 mTLS를 쉽게 활용할 수 있는 좋은 방법이지만, 서비스 메시를 채택하기 전에 두 개의 도커 컨테이너 간의 단순한 mTLS 구현이 어떻게 작동하는지 궁금할 수 있습니다.\n\n자세한 내용은 다음 GitHub 저장소를 참조해주세요: https://github.com/blhagadorn/mutual-tls-docker\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 기본 클라이언트 및 서버 설정\n\nGo를 사용하여 기본 클라이언트 및 서버를 설정해 봅시다 - GitHub 저장소의 01-client-server-basic 디렉토리로 이동하여 따라해 보세요. 기본 클라이언트 및 서버를 설정한 후에는 mTLS를 추가할 것입니다.\n\n기본 서버의 요약은 다음과 같습니다 (여기서 찾을 수 있습니다)\n\n```js\nfunc helloHandler(w http.ResponseWriter, r *http.Request) {\n  io.WriteString(w, \"Hello, world without mutual TLS!\\n\")\n}\nfunc main() {\n  http.HandleFunc(\"/hello\", helloHandler)\n  log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기본적으로 우리는 8080포트에서 /hello 경로를 청취하고 호출되면 문자열을 반환합니다.\n\n여기 기본 클라이언트의 주요 내용입니다 (여기에서 찾을 수 있습니다):\n\n```js\n r, err := http.Get(\"http://localhost:8080/hello\")\n if err != nil {\n   log.Fatal(err)\n }\n defer r.Body.Close()\n body, err := ioutil.ReadAll(r.Body)\n```\n\n기본적으로, http://localhost:8080/hello로 HTTP GET 요청을 만들고 응답을 쓰기로 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 지금까지 한 것을 빌드하고 실행해 보세요. 01-client-server-basic/ 디렉토리 안에서 모두 수행합니다.\n\n```js\n$ docker build -t basic-server -f Dockerfile.server . && docker run -it --rm --network host basic-server\n```\n\n서버를 계속 실행한 채로 유지하고, 동일한 디렉토리에서 새 창을 열어서 클라이언트를 실행하세요:\n\n```js\n$ docker build -t basic-client -f Dockerfile.client . && docker run -it --rm --network host basic-client\n> Hello, world without mutual TLS!\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n성공했습니다! 이제 각각의 도커 컨테이너에서 클라이언트와 서버가 대화하고 있습니다. 도커 컨테이너 간에 공유 네트워크를 만드는 --network host 옵션의 사용에 주목해 주세요. 따라서 두 컨테이너에서 localhost가 동일합니다.\n\n선택적으로 클라이언트를 실행하는 동안 tcpdump를 사용하여 평문 전송이 확인됩니다:\n\n```js\n$ docker run -it --network host --rm dockersec/tcpdump tcpdump -i any port 8080 -c 100 -A\n> Date: Sat, 03 Feb 2024 15:05:20 GMT\n> Content-Length: 33\n> Content-Type: text/plain; charset=utf-8\n> Hello, world without mutual TLS!\n```\n\n우리는 TLS가 사용되지 않았음을 알 수 있습니다. 간단히 말해서, 텍스트를 읽을 수 있기 때문에 (암호화되었다면 가독성이 떨어졌을 것입니다).\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 상호 TLS 추가하기\n\n상호 TLS를 추가하려면 먼저 연결에 사용할 개인 키와 해당 인증서를 생성해야 합니다. 만약 GitHub 저장소를 따라가고 있다면 예제의 나머지 부분을 확인하기 위해 02-client-server-mtls 디렉토리로 이동해주세요.\n\n```js\nopenssl req -newkey rsa:2048 \\\n-nodes -x509 \\\n-days 3650 \\\n-keyout key.pem \\\n-out cert.pem \\\n-subj \"/C=US/ST=Montana/L=Bozeman/O=Organization/OU=Unit/CN=localhost\" \\\n-addext \"subjectAltName = DNS:localhost\"\n```\n\n여기서는 로컬호스트의 CN(공통 이름)과 SAN(대체 주체 이름)을 포함하는 대응하는 공개 키가 포함된 개인 키(key.pem)와 인증서(cert.pem)를 생성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n참고: CN은 폐기되었으며 대부분의 최신 TLS 라이브러리에서 SAN을 포함하여 설정해야 합니다. 예를 들어 Golang의 net/http도 그렇습니다. 이 예제에서는 일부 라이브러리가 아직 CN을 설정하거나 이를 예비 수단으로 사용하는 경우를 고려하여 둘 다 설정했습니다.\n\n인증서(공개 키)와 개인 키는 여기서 몇 가지 역할을 합니다. 첫째, 개인/공개 키 조합은 세션 설정을 위해 통신을 암호화하는 데 사용됩니다. 둘째, 인증을 위해 인증서 정보가 사용되며, 해당 인증서로 보호하려는 도메인 이름은 localhost(즉, SAN)입니다.\n\n이제 클라이언트 코드(client-mtls.go 파일)를 살펴봅시다. 다음 함수는 주어진 인증서와 키로 HTTPS 클라이언트를 반환합니다:\n\n```go\nfunc getHTTPSClientFromFile() *http.Client {\n  caCert, err := ioutil.ReadFile(\"cert.pem\")\n  if err != nil {\n    log.Fatal(err)\n  }\n  caCertPool := x509.NewCertPool()\n  caCertPool.AppendCertsFromPEM(caCert)\n  cert, err := tls.LoadX509KeyPair(\"cert.pem\", \"key.pem\")\n  if err != nil {\n    log.Fatal(err)\n  }\n  client := &http.Client{\n    Transport: &http.Transport{\n      TLSClientConfig: &tls.Config{\n        RootCAs:      caCertPool,\n        Certificates: []tls.Certificate{cert},\n      },\n    },\n  }\n  return client\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기에서 몇 가지가 발생하고 있습니다. 먼저, RootCAs가 생성한 인증서 풀(하나의 인증서만 포함)로 설정되고 있습니다. 이는 클라이언트가 다른 인증 기관을 확인하는 데 사용할 루트 인증서 집합입니다. 예제에서 중간 인증서를 생성하지 않았기 때문에 이것은 별 의미가 없지만, 여러 거래에서 이것은 신뢰의 루트를 정의합니다(루트가 서명한 모든 인증서가 유효할 것입니다). 둘째로, 우리 클라이언트가 안전한 연결을 설정할 때 서버에 전달할 인증서인 cert.pem을 전달하고 있습니다. 또한, 인증서 키 쌍은 통신을 암호화하는 데 사용할 비공개 키인 key.pem을 포함하고 있습니다.\n\n이제 관련 서버 코드를 살펴보겠습니다:\n\n```js\nfunc main() {\n  http.HandleFunc(\"/hello\", helloHandler)\n  caCert, err := ioutil.ReadFile(\"cert.pem\")\n  if err != nil {\n    log.Fatal(err)\n  }\n\n  caCertPool := x509.NewCertPool()\n  caCertPool.AppendCertsFromPEM(caCert)\n  tlsConfig := &tls.Config{\n    ClientCAs:  caCertPool,\n    ClientAuth: tls.RequireAndVerifyClientCert,\n  }\n  tlsConfig.BuildNameToCertificate()\n  server := &http.Server{\n    Addr:      \":8443\",\n    TLSConfig: tlsConfig,\n  }\n  log.Fatal(server.ListenAndServeTLS(\"cert.pem\", \"key.pem\"))\n}\n```\n\n서버 구성은 클라이언트 구성과 매우 유사합니다(이는 상호 인증이기 때문에 이해되는 바입니다). 루트 인증기관이 비슷하게 정의되어 있고, TLS 구성이 설정되며, 최종적으로 서버가 인증서와 인증서 키 쌍을 사용하여 수신 대기를 시작합니다. 클라이언트와 마찬가지로 서버도 연결하려는 모든 이해 관계자에게 자신의 인증서를 전달합니다(클라이언트가 인증서의 공개 키에 따라 통신을 암호화할 수 있도록 하는 TLS 핸드셰이크의 일부로), 그리고 또한 이 키는 메시지를 암호화하고 인증서의 공개 키의 소유권을 확인하는 데 사용됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 02-client-server-mtls 디렉토리 안에서 예제를 실행해 보겠습니다.\n\n먼저 서버를 실행하겠습니다:\n\n```js\n$ docker build -t mtls-server -f Dockerfile.server . && docker run -it --rm --network host mtls-server\n```\n\n이후 클라이언트를 실행해주세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n$ docker build -t mtls-client -f Dockerfile.client . && docker run -it --rm --network host mtls-client\n> 안녕하세요, 상호 TLS로 세계여!\n```\n\n다시 성공했어요!\n\n우리는 다시 tcpdump로 확인할 수 있습니다. 평문이 없고 컨테이너 간 통신이 암호화되어 있는지 확인할 수 있어요.\n\n```js\n$ docker run -it --network host --rm dockersec/tcpdump tcpdump -i any port 8443 -c 100 -A\n>..V.(.@.................................. .&............0.........\nO.f........\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n결과가 완전히 알아볼 수 없고 암호화를 사용하고 있어서 읽을 수 없는 것을 주목해주세요.\n\n# 빠른 다이어그램\n\n아래 다이어그램을 참조하시면 방금 수행한 mTLS 상호 작용에 대해 시각적으로 설명한 것을 보실 수 있습니다.\n\n![mTLS 상호 작용 다이어그램](/assets/img/2024-05-20-HowtoImplementMutualTLSwithDockerContainers_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 마무리\n\n지금까지 상호 TLS 없이 한 클라이언트-서버 상호 작용과 상호 TLS가 포함된 다른 클라이언트-서버 상호 작용을 성공적으로 만들었습니다. 우리는 TLS를 추가하기 위해 로컬호스트를 SAN으로 사용한 키와 인증서를 생성했습니다. 이후에 우리는 클라이언트 코드를 편집하여 루트 CA에 대한 TLS 구성을 포함시키고 통신을 암호화할 인증서와 개인 키를 지정했습니다. 서버 코드도 마찬가지로 루트 CA를 지정하고, 서버가 청취해야 할 인증서와 키를 지정했습니다.\n\n이제 mTLS를 마이크로서비스 간에 서비스 메시를 통해 고려할 수 있는 기초가 생겼기를 바랍니다. 저희의 예에서는 한 번 인증서와 키를 생성하고 수동으로 구성에 입력했지만, 서비스 메시는 종종 짧은 갱신 주기로 자동으로 해당 인증서를 회전하고, 일반 통신을 측면 프록시를 통해 라우팅하며 — 그런 다음 일반 통신을 mTLS로 업그레이드하고 목적지에 도달하면 복호화합니다. 본질적으로 mTLS는 보이지 않고, 이것이 강력한 프록시 구성과 제어 계층의 마법입니다.\n\n이 글을 통해 컨테이너화된 작업을 보호하는 방법에 대해 조금이나마 배우셨기를 바랍니다. 그리고 언제나 — 더 많은 글을 보기 위해 저를 Medium에서 팔로우하거나 Twitter에서 확인해 보세요.\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoImplementMutualTLSwithDockerContainers_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoImplementMutualTLSwithDockerContainers_0.png","tag":["Tech"],"readingTime":10},{"title":"도커 다이어트 보안 및 속도를 위한 데비안 이미지","description":"","date":"2024-05-20 17:03","slug":"2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed","content":"\n<img src=\"/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_0.png\" />\n\n컨테이너화된 세상에서 이미지 크기가 중요합니다. 작은 이미지는 더 빠른 배포, 줄어든 저장 요구 사항 및 향상된 보안으로 이어집니다. Google의 Distroless Docker 이미지가 등장하는 곳입니다.\n\nDistroless 이미지는 극단적으로 최소한한 이미지입니다. 패키지 관리자, 셸 및 불필요한 유틸리티를 포함한 무겁고 복잡한 운영 체제 계층을 제거합니다. 대신, 이들은 당신의 애플리케이션이 실행하기 위해 필요한 것에만 집중합니다: 애플리케이션 자체와 필수 런타임 종속성입니다.\n\n# 왜 Distroless를 사용해야 할까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDistroless 이미지를 사용하는 이점은 매우 많습니다:\n\n- 크기가 작음: Distroless 이미지는 극히 작아서 종종 몇 메가바이트에 불과합니다. 이는 배포가 빨라지고 컨테이너 레지스트리 및 클러스터의 저장 공간을 줄이는 것을 의미합니다.\n- 향상된 보안: 불필요한 구성 요소를 제거함으로써 Distroless 이미지는 잠재적인 취약점을 위한 공격 표면을 줄입니다. 관리해야 할 패키지가 적기 때문에 보안 패치 작업도 매우 간단해집니다.\n- 성능 향상: 작은 이미지는 컨테이너의 시작 시간이 빨라지도록 돕습니다.\n\n# Distroless로 시작하기\n\nDistroless는 다양한 언어 및 런타임을 지원하는 기본 이미지를 제공합니다. 다음은 이를 사용하는 빠른 가이드입니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 이미지 선택: 사용 가능한 이미지를 탐색하려면 GitHub의 Distroless 프로젝트(https://github.com/GoogleContainerTools/distroless)로 이동하세요. 인기 있는 옵션으로는 일반 목적의 애플리케이션용 gcr.io/distroless/base 및 언어별 이미지인 gcr.io/distroless/java17이 있습니다.\n- Dockerfile 빌드: Dockerfile에서 선택한 Distroless 이미지를 기본 레이어로 사용하세요.\n- 애플리케이션 복사: COPY 명령을 사용하여 애플리케이션 코드와 필요한 종속성을 이미지로 복사하세요.\n- 엔트리포인트 정의: Distroless 이미지에는 셸이 없으므로 ENTRYPOINT 명령을 사용하여 애플리케이션의 엔트리포인트를 정의해야 합니다. 이는 벡터 형태로 지정해야 합니다. (예: [\"/app/my_app\"])\n\n# JAVA를 위한 Distroless 이미지 사용\n\nDistroless 이미지 사용의 장점을 설명하기 위해 Java 애플리케이션 개발 프로세스에 이를 원활하게 통합하는 방법을 탐색해볼까요?\n\n애플리케이션 이미지를 빌드하는 데 사용된 샘플 Dockerfile입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nFROM gcr.io/distroless/java17:nonroot\nEXPOSE 8080\nCOPY target/*.jar app.jar\nCMD [\"app.jar\"]\n```\n\n취약점 수와 이미지 크기를 비교한 분석 결과, 상당한 개선이 있었습니다. 이러한 향상을 보여주는 자세한 통계는 다음과 같습니다.\n\n이전에 사용한 openjdk 베이스 이미지\n\n이전 취약점 통계: 총 96개 | 중요: 0 | 높음: 23 | 중간: 73\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_1.png)\n\nJava distroless 이미지 사용 중\n\n현재 취약점 통계: 총 1 | 심각: 0 | 높음: 0 | 중간: 1\n\n![이미지](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n베이스 Java 이미지 크기: 226MB [이미지 크기의 50% 감소]\n\n# NGINX에 Distroless 이미지 사용하기\n\n미리 빌드된 Nginx Distroless 이미지가 즉시 사용 가능하지 않기 때문에, 베이스 Distroless 이미지를 사용하여 커스텀 이미지를 만드는 방법을 살펴보겠습니다.\n\n애플리케이션 이미지를 빌드하는 데 사용된 샘플 Dockerfile입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# Nginx를 빌드 이미지로 사용\nFROM nginx:1.25 as build\n\n# 메인 이미지로 distroless nonroot 사용\nFROM gcr.io/distroless/base-debian11:nonroot\nCOPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt\nCOPY --from=build /etc/passwd /etc/passwd\nCOPY --from=build /etc/group /etc/group\nUSER nginx\n\n# 빌드 이미지로부터 필요한 Nginx 라이브러리 복사\nCOPY --from=build /lib/x86_64-linux-gnu/libdl.so.2 /lib/x86_64-linux-gnu/libdl.so.2\nCOPY --from=build /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libc.so.6\nCOPY --from=build /lib/x86_64-linux-gnu/libz.so.1 /lib/x86_64-linux-gnu/libz.so.1\nCOPY --from=build /lib/x86_64-linux-gnu/libcrypt.so.1 /lib/x86_64-linux-gnu/libcrypt.so.1\nCOPY --from=build /lib/x86_64-linux-gnu/libpthread.so.0 /lib/x86_64-linux-gnu/libpthread.so.0\nCOPY --from=build /lib64/ld-linux-x86-64.so.2 /lib64/ld-linux-x86-64.so.2\nCOPY --from=build /usr/lib/x86_64-linux-gnu/libssl.so.3 /usr/lib/x86_64-linux-gnu/libssl.so.3\nCOPY --from=build /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0\nCOPY --from=build /usr/lib/x86_64-linux-gnu/libcrypto.so.3 /usr/lib/x86_64-linux-gnu/libcrypto.so.3\n\n# 빌드 이미지로부터 필요한 이진 파일 및 디렉토리 복사\nCOPY --from=build /usr/sbin/nginx /usr/sbin/nginx\nCOPY --from=build /etc/nginx /etc/nginx\nCOPY --from=build --chown=nginx:nginx /var/cache/nginx /var/cache/nginx\nCOPY --from=build --chown=nginx:nginx /var/log/nginx /var/log/nginx\nCOPY --from=build --chown=nginx:nginx /usr/share/nginx/html /usr/share/nginx/html\n\nCOPY nginx.conf /etc/nginx/nginx.conf\nCOPY default.conf /etc/nginx/conf.d/\n\nENTRYPOINT [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"]\n```\n\nNginx 베이스 이미지의 취약점 카운트와 이미지 크기를 비교 분석하였습니다.\n\n이전에 사용한 nginx 알파인 베이스 이미지\n\n이전 취약성 통계 \"Total: 32 | Critical: 1 | High: 13 | Medium: 18\"\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Dockerona Diet Distroless Images for Security Speed 3](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_3.png)\n\nUsing base distroless image crafted with nginx\n\nCurrent Vulnerability Stats: Total: 0 | Critical: 0 | High: 0 | Medium: 0\n\n![Dockerona Diet Distroless Images for Security Speed 4](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기본 Nginx 이미지의 크기: 28MB [이미지 크기 30% 감소]\n\n# 주의 사항\n\nDistroless는 큰 장점을 제공하지만 몇 가지 주의할 점이 있습니다:\n\n- 쉘 액세스 없음: Distroless 이미지에는 디자인상 셸이 없습니다. 컨테이너 내에서 디버깅을 하려면 추가 도구나 멀티 스테이지 빌드가 필요할 수 있습니다.\n- 기능 제한: 패키지 관리자가 없기 때문에 애플리케이션이 필요로 하는 추가 라이브러리, 유틸리티 또는 인증서를 명시적으로 포함해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Distroless 또는 Alpine...어느 쪽을 선택해야 할까요?\n\nDistroless와 Alpine 이미지의 장단점을 이해하여, 귀하의 애플리케이션 요구사항과 개발 흐름에 가장 적합한 결정을 내릴 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_5.png)\n\n# 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDistroless 이미지는 안전하고 효율적인 도커 컨테이너를 만드는 매력적인 옵션입니다. 미니멀리즘을 포용함으로써 더 작은 공격 표면, 빠른 배포 및 향상된 성능을 얻을 수 있습니다. 다음 번에 도커 이미지를 만들 때는 여분의 부담을 줄이고 Distroless 기본 이미지를 선택하는 것을 고려해보세요.\n","ogImage":{"url":"/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_0.png"},"coverImage":"/assets/img/2024-05-20-DockeronaDietDistrolessImagesforSecuritySpeed_0.png","tag":["Tech"],"readingTime":8}],"page":"89","totalPageCount":119,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}