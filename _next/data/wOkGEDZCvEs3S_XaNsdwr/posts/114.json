{"pageProps":{"posts":[{"title":"ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요","description":"","date":"2024-05-17 19:45","slug":"2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons","content":"\n![ChatGPT-4o](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png)\n\nAI 세계가 다시 소란스럽습니다.\n\n새로운 OpenAI 업데이트 덕분에요.\n\n새로운 모델 GPT-4o는 이미 영화 \"Her\"의 AI와 비교되고 있습니다.\n\nChatGPT-4o는 놀랄만한 능력과 전반적인 이해력에서 큰 발전이 있어 보입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSo friendly!\n\n---\n\n가장 놀라운 것은 무료로 사용할 수 있다는 점입니다!\n\n![이미지](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_1.png)\n\n유료 버전은 여전히 더 큰 컨텍스트 창을 가지고 있을 수 있지만, OpenAI가 가장 첨단 AI를 무료로 제공한 사실은 정말 놀랍습니다.\n\n아직 데모를 보지 않았다면, 지금바로 확인하는 것을 추천합니다. ChatGPT-4o가 다음을 수행하는 것을 확인할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1 — 과외\n\n이 데모에서는 ChatGPT-4o가 학생을 가르치며 수학 문제를 해결하고 해결책을 안내하는 것을 보여줍니다.\n\nAI가 문제를 자연스러운 방식으로 설명하는 것은 훈련과 온라인 학습의 미래가 어떻게 될지 보여줍니다!\n\n내 아내는 과외교사이며 세션 전체가 사람처럼 들리는 것에 놀랐다고 할 수 없을 정도입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2 — 실시간 번역\n\n또한 놀랄 만한 실시간 번역 기능을 제공합니다. 두 명의 사람이 영어와 이탈리아어로 이야기할 때 따라 말하는 것을 볼 수 있습니다.\n\n저도 직접 사용해 보았는데, 그 성능은 정말 인상적입니다.\n\n번역기 및 그들의 앱들은 어떻게 영향을 받을지 이미 걱정되고 있을 것 같네요!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3 - 시각 능력\n\n데모에서 가장 좋았던 점은 팀이 시각 장애인을 도와 실시간으로 주변 환경을 설명하는 ChatGPT4-o를 보여준 것이었습니다.\n\n이것은 전 세계 사람들을 돕는 데 큰 도약이 될 것입니다.\n\n그가 주변 환경을 자연스럽게 설명하고 상호 작용을 얼마나 자연스럽게 했는지에 굉장히 감명받았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 AI가 인류가 다음 단계로 발전하는 데 도움을 줄 것이라는 완벽한 예시입니다.\n\n# 이제, 나쁜 소식 ..\n\nAI가 한 걸음씩 나아갈수록.. 새로운 위험이 소개됩니다.\n\n새로운 비전 및 음성 기능은 사이버 범죄자에겐 꿈의 소재가 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위는 데모 시청과 (100% 주관적인) 나의 경험을 통해 생각해 본 위험 중 일부입니다.\n\n## 1 — 사회 공학 2.0\n\n인간과 같은 대화를 나눌 수 있는 자연스러운 AI는 사회 공학을 미친 정도로 증가시킬 수 있습니다.\n\n우리는 이미 GenAI에 의한 딥페이크와 음성 사기를 보았지만, GPT-4o의 향상된 멀티모달 능력은 사이버 범죄자들이 무시할 수 없는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n잠재적인 잘못된 정보 캠페인과 ID 도용 가능성이 엄청나요.\n\nGPT-4o의 실시간 대화 능력은 사이버 범죄자들이 AI-기반 vishing 공격을 만들어낼 수 있도록 할 것입니다.\n\n희생자들은 합법적인 사람과 소통하고 있다고 믿으며 민감한 정보를 폭로하거나 거래를 승인하게 되는 속임수를 당할 수 있습니다.\n\n이러한 공격은 새로운 것은 아니지만 GPT-4o와 같은 AI와 함께 그 규모와 정교함이 대대적으로 증가할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2 — AI-Powered Malware \\* 2\n\nAI-powered malware is already here, so there is nothing new about that\n\nWhat stood out to me was the video in which two GPT-4os were interacting with each other.\n\nImagine AI training another AI to evade controls and become better at compromising environments.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 모델 간의 상호작용은 인공지능이 서로 발전함에 따라 사이버 범죄의 미래가 어떻게 보일지도 모릅니다.\n\n## 3 — 다국어 공격\n\n실시간 번역이 가능하다는 것은 놀라우면서도 무섭습니다.\n\n사이버 범죄자들은 이제 다양한 언어로 전환하며 공격의 피해 범위를 확대할 수 있을 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다국어 기업에서 일하는 직원들 중 사회 공학에 취약하지 않은 사람들도 언어로 대화하는 AI에 노출되면 자신을 열어보게 될 수 있습니다!\n\n# 미래의 한 눈독\n\nAI가 발전함에 따라 우리는 몇 달마다 미지의 영역으로 나아가는 것 같아요.\n\n기업들은 보안 시스템을 조정하고 AI와의 전투에 AI를 활용해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n흥미로운 데모를 보고 든 생각 중 일부를 소개해 드릴게요.\n\n- AI 기반 보안 도구를 도입하여 AI 기반 사회 공학 및 악성 소프트웨어의 이상한 패턴이나 행동을 감지합니다.\n- 직원 및 사용자가 AI 기반 사회 공학을 인식하는 방법에 대해 교육하십시오. 여전히 이메일 피싱 공격에 대해 이야기 중이라면 PowerPoint 프레젠테이션을 업데이트하세요!\n- 언어별 보안 프로토콜을 고려해 보세요. 언어 인식 기능을 갖춘 보안 프로토콜을 개발하여 다국어로 의심스러운 통신을 식별하고 표시합니다.\n\n다음 해에는 이것이 낡은 정보처럼 보일 수도 있느니라!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다중 모달, 다중 언어 인공 지능이 이제 여기 있으며, GPT-4o는 통제와 위험 관점에서 큰 발전이 이루어졌습니다.\n\n몇 달동안의 위협 전망이 사용자들(그리고 사이버 범죄자들)이 새로운 모델을 이해하고 대척을 하는 모습을 살펴봅시다.\n\n![ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_2.png](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_2.png)\n\nTaimur Ijlal은 핀테크 업종에서 20년 이상의 국제 경험을 갖춘 다중 수상 경력을 지닌 정보 보안 리더입니다. Taimur는 링크드인이나 유튜브 채널 \"클라우드 보안 가이\"에서 연결할 수 있습니다. 클라우드 보안, 인공 지능, 일반적인 사이버 보안 직업 조언에 대해 꾸준히 게시하고 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png"},"coverImage":"/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png","tag":["Tech"],"readingTime":7},{"title":"반복 신경망 시퀀스 모델링 소개","description":"","date":"2024-05-17 19:43","slug":"2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling","content":"\n![img](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png)\n\n많은 문제와 현상은 순차적으로 발생합니다. 대표적인 예로는 음성, 날씨 패턴, 시계열 등이 있습니다. 이러한 시스템들의 다음 위치는 이전 상태에 따라 달라집니다.\n\n안타깝게도, 전통적인 신경망은 이러한 유형의 데이터를 처리하거나 예측할 수 없습니다. 왜냐하면 입력값을 독립적으로 분석하기 때문에 데이터가 실제로 순차적이라는 개념을 이해하지 못하기 때문입니다.\n\n그렇다면, 이러한 유형의 데이터를 어떻게 예측할 수 있을까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"우리는 순환 신경망이라고 불리는 것으로 넘어갑니다!\n\n표준 신경망에 익숙하지 않다면, 확인할 블로그 시리즈가 있어요! RNN으로 계속 진행하기 전에 이 일반적인 신경망이 어떻게 작동하는지 알아보는 것을 권장합니다.\n\n# 순환 신경망이란 무엇인가요?\n\n다음은 순환 신경망(RNNs)을 설명하는 다이어그램입니다:\"\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![RecurrentNeuralNetworksAnIntroductiontoSequenceModelling](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_1.png)\n\n왼쪽에는 순환 뉴런이 있고, 오른쪽에는 시간에 따라 펼쳐진 순환 뉴런이 있습니다. RNN은 바닐라 피드포워드 신경망과 비슷해 보이지만, 이전 반복 실행에서 입력을 받는 중요한 차이점이 있습니다.\n\n그래서 그들을 \"순환\"이라고 부르는 것입니다. 각 단계의 출력이 시간 안에 전파되어 다음 단계의 값을 계산하는 데 도움이 됩니다. 시스템에는 어떤 내재적 \"기억\"이 있어서 모델이 과거의 패턴을 추적할 수 있습니다.\n\n예를 들어, Y_1을 예측할 때, X_1의 입력 및 이전 시간 단계 Y_0에서의 출력을 사용할 것입니다. Y_0가 Y_1에 영향을 미치기 때문에 Y_0가 Y_2에도간접적으로 영향을 줄 수 있다는 것을 알 수 있습니다. 이 알고리즘의 순환성을 명확하게 보여주는 사례입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 숨겨진 상태\n\n문학 작품에서는 일반적으로 숨겨진 상태라는 개념을 볼 수 있습니다. 주로 순환 뉴런을 통해 전달되는 h로 표시됩니다.\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_2.png)\n\n간단한 경우에는 숨겨진 상태가 셀의 출력인 경우도 있습니다. 즉, h=Y입니다. 그러나 우리가 나중에 살펴볼 것처럼, 장기 단기 메모리(LSTM) 및 게이트 순환 유닛(GRU)과 같은 보다 복잡한 셀의 경우에는 이것이 항상 참일 수 있는 것은 아닙니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n따라서 각 뉴런으로 전달하는 것과 각 뉴런으로부터의 전달을 명시적으로 하는 것이 가장 좋습니다. 이것이 대부분의 문헌에서 위와 같이 표시되는 이유입니다.\n\n# 이론\n\n순환 뉴런의 각 숨겨진 상태는 다음과 같이 계산할 수 있습니다:\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기서:\n\n- h_t는 시간 t에서의 은닉 상태입니다.\n- h\\_'t−1'는 이전 시간 단계의 은닉 상태입니다.\n- x_t는 시간 t에서의 입력 데이터입니다.\n- W_h는 은닉 상태에 대한 가중치 행렬입니다.\n- W_x는 입력 데이터에 대한 가중치 행렬입니다.\n- b_h는 은닉 상태에 대한 편향 벡터입니다.\n- σ는 활성화 함수로, 일반적으로 tanh 또는 sigmoid 함수를 사용합니다.\n\n그리고 각 순환 뉴런의 출력을 예측하는 방법은 다음과 같습니다:\n\n![Recurrent Neurons](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위와 같이:\n\n- y_t는 시간 t에서의 출력입니다.\n- W_y는 출력과 관련된 가중치 행렬입니다.\n- b_y는 출력 편향 벡터입니다.\n\n보시다시피 표기법과 변수 대부분은 일반 피드포워드 신경망과 유사합니다. 유일한 차이점은 숨겨진 상태의 전달로, 그것은 모델이 출력을 예측하는 데 사용할 다른 입력이나 특성으로 볼 수 있습니다.\n\n각 숨겨진 층은 여러 반복 뉴런을 포함할 수 있으므로 각 후속 입력 뉴런에 숨겨진 상태의 벡터를 전달하게 됩니다. 이를 통해 네트워크는 데이터에서 더 복잡한 패턴을 포착하고 표현할 수 있습니다. 각 시간 단계에서 미니 신경망으로 생각할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 작업 예시\n\n우리는 RNN 내부에서 실제로 무슨 일이 일어나고 있는지 설명하기 위해 간단한 예제를 살펴볼 수 있습니다. 이 예제는 매우 단순한 시나리오일 것이지만, 알아야 할 주요 직관력을 설명해줄 것입니다. 실제로 현실에서는 어떤 문제도 이렇게 간단하지 않을 겁니다!\n\n## 설정\n\n1, 2, 3의 숫자 시퀀스가 있다고 가정해 보겠습니다. 이 시퀀스에서 다음 숫자인 4를 예측하기 위해 RNN을 훈련하려고 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n당신의 RNN은 다음과 같은 구조를 가지고 있을 것입니다:\n\n- 하나의 입력 뉴런\n- 하나의 은닉 뉴런\n- 하나의 출력 뉴런\n\n가중치와 바이어스를 랜덤하게 초기화할 수 있습니다:\n\n- W_x (입력에서 은닉으로의 가중치): 0.5\n- W_h (은닉에서 은닉으로의 가중치): 1.0\n- b_h (은닉 바이어스): 0\n- 𝑏_𝑦 (출력 바이어스): 0\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 텍스트를 친근한 톤으로 한국어로 번역해 드리겠습니다.\n\n다음 활성화 함수를 사용해주세요:\n\n- 은닉층: tanh\n- 출력층: 없음 (identity/linear)\n\n초기 은닉 상태 값:\n\n- h_0 = 0\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Time Step 1 (Input: 1)\n\n다음은 첫 번째 숨겨진 상태입니다:\n\n![첫 번째 숨겨진 상태](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_5.png)\n\n그리고 출력은 다음과 같이 계산됩니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_6.png)\n\n이 예시에서 출력 활성화 함수는 identity이므로 출력 값은 hidden state 값과 동일합니다. 그러나 많은 문제에서 항상 그런 것은 아니라는 것을 기억하세요.\n\n## 시간 단계 2 (입력: 2)\n\n이제 최근에 계산된 h_1 값 사용하여 시간 단계 2에서 다음 입력 값을 위한 위 과정을 반복할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 테이블 태그를 바꿔본 것입니다.\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_7.png)\n\n한번 더, 우리는 시간 단계 2에서 출력 값을 계산합니다:\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_8.png)\n\n## 시간 단계 3 (입력: 3)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막 입력 값과 세 번째 타임 스텝에서는 다음 이미지가 예측 모델을 보여줍니다:\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_9.png)\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_10.png)\n\n현재 모델은 다음 숫자를 0.984로 예측하고 있습니다. 실제 값인 4와는 분명히 멀리 떨어져 있습니다. 실제로는 더 많은 훈련 세트를 사용하여 시간을 거슬러 거슬러 역전파를 수행하여 매개 변수를 최적화할 것입니다. 이 내용은 다음 글에서 다룰 예정입니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다행히도 이 모든 계산과 최적화는 PyTorch와 TensorFlow와 같은 패키지를 통해 Python에서 수행됩니다. 제가 이 기사에서 이를 하는 방법의 예시를 나중에 보여 드리겠습니다!\n\n# RNN의 종류\n\n위의 예는 많은 입력으로부터 하나의 RNN 프로세스의 논리적인 과정을 설명하고 있습니다. 우리는 여러 입력(1,2,3)으로 시작하여 시퀀스에서 다음 숫자를 예측하기 위해 노력하고 있는데, 이는 단일 값입니다.\n\n그러나 다른 작업을 위한 여러 종류의 RNN이 있으며, 우리는 지금 그것들을 살펴볼 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 일대일\n\n이것은 단일 예측을 내놓는 입력 세트가 하나인 전통적인 신경망입니다. 이것은 일반적인 지도 학습 문제를 해결하는 데 도움이 됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_11.png)\n\n## 일대다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하나의 입력이 여러 출력으로 이어집니다. 이미지 캡션을 만들거나 음악을 생성하는 데 사용할 수 있습니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_12.png)\n\n## Many-To-One\n\n여러 입력이 하나의 최종 출력을 생성합니다; 이 아키텍처는 감성 분석에 사용됩니다. 영화 리뷰를 제공하면 영화가 좋은지 나쁜지에 따라 +1 또는 -1을 할당합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_13.png)\n\nThis one gets an input at every step and produces an output at each step. This architecture is used for machine translation and also for problems like speech tagging.\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_14.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 인코더-디코더\n\n마지막으로, 인코더-디코더 네트워크를 사용할 수 있습니다. 이는 많은 개별 데이터를 입력으로 받아 하나의 데이터를 출력하는 네트워크와, 그로부터 다시 많은 개별 데이터를 출력으로 하는 네트워크로 구성됩니다. 이는 주로 한 언어로 된 문장을 다른 언어로 번역하는 데 사용됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_15.png)\n\n# PyTorch 예시\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위는 PyTorch에서 간단한 RNN을 구현하는 간단한 예제입니다. 위에서 해결한 문제를 시연합니다. 입력이 1,2,3이고 순서에 따라 다음 숫자를 예측하려고 합니다.\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# RNN 모델 정의\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(-1)\n        h_0 = torch.zeros(1, x.size(0), self.hidden_size)\n        rnn_out, _ = self.rnn(x, h_0)\n        out = self.fc(rnn_out[:, -1, :])\n        return out\n\n# 데이터셋\ntrain = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\ntarget = torch.tensor([5], dtype=torch.float32)\n\n# 모델 설정\ninput_size = 1\nhidden_size = 1\noutput_size = 1\nmodel = SimpleRNN(input_size, hidden_size, output_size)\n\n# 손실 및 옵티마이저\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    output = model(train.unsqueeze(0)).squeeze()  # 배치 차원 추가 및 목표 형태와 일치하도록 압축\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n# 다음 숫자 예측하는 함수\ndef predict(model, input_seq):\n    with torch.no_grad():\n        input_seq = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n        output = model(input_seq).squeeze().item()\n    return output\n\n# 예제 테스트 세트\ntest = [2, 3, 4]\npredicted = predict(model, test)\nprint(f'Input: {test}, Predicted Next Number: {predicted:.2f}')\n```\n\n1000번의 에폭 후 출력 결과는 5입니다! 이 경우에는 모델이 실제로 1000번의 역전파로 훈련되었기 때문에 성능이 우리가 위에서 손으로 계산한 예제보다 훨씬 좋습니다.\n\n소스 코드는 저의 GitHub에서 확인하실 수 있습니다: (GitHub URL)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 장점 대비 단점\n\n이 모든 새롭게 습득한 정보를 바탕으로 RNN의 주요 장단점을 살펴보겠습니다:\n\n## 장점\n\n- 이전 입력값의 형태를 기억할 수 있어서 순차적 데이터를 다루는 데 도움이 됩니다.\n- 정확한 가중치와 편향이 모든 시간 단계에서 공유되어, 더 적은 매개변수와 더 나은 일반화를 이끌어냅니다.\n- 재귀적 성격으로 인해 RNN은 가변 길이의 순차 데이터를 처리할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단점\n\n- RNN(순환 신경망)은 장기 기억 문제로 이어지는 사라지는 그래디언트 문제에서 상당히 고통받습니다.\n- 각 시간 단계는 이전 단계의 출력에 의존하기 때문에 RNN은 병렬 처리할 수 없어 계산 효율이 떨어집니다.\n\n# 요약\n\nRNN은 시퀀스 모델링에 매우 유용하며, 이전 실행의 정보와 메모리를 유지한 채 다음 예측으로 전파됩니다. 그들의 장점은 임의 길이의 입력을 처리할 수 있으며, 모델 크기가 이 입력 크기로 증가하지 않는다는 것입니다. 그러나 재귀적인 성격을 가지고 있기 때문에 병렬화할 수 없어 계산 효율이 낮으며, 사라지는 그래디언트 문제로 심각하게 고통받을 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 또 다른 것!\n\n무료 뉴스레터 'Dishing the Data'를 갖고 있어요! 매주 더 나은 데이터 과학자가 되기 위한 조언과 분야에서의 경험을 나누고 있어요.\n\n# 저와 연결해보세요!\n\n- LinkedIn, X (트위터), 또는 인스타그램\n- 기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 제 유튜브 채널!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 참고 자료 및 더 읽을거리\n\n- Stanford RNN Cheatsheet\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. September 2019. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649.\n","ogImage":{"url":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png"},"coverImage":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png","tag":["Tech"],"readingTime":15},{"title":"ICD 코딩을 위한 LLM 탐험 - 파트 1","description":"","date":"2024-05-17 19:38","slug":"2024-05-17-ExploringLLMsforICDCodingPart1","content":"\n## LLM(Large Language Model)을 활용한 자동 진단 코딩 시스템 구축\n\n임상 코딩은 흔히 쓰이는 용어는 아니지만, 대부분의 국가에서 건강 관리 체계와 상호작용하는 모든 사람에게 중대한 영향을 미칩니다. 임상 코딩은 환자 건강 기록에서 의학 정보(진단 및 수술 등)를 표준화된 숫자 또는 알파벳 코드로 번역하고 매핑하는 것을 포함합니다. 이러한 코드는 청구, 건강 관리 분석 및 환자가 적절한 치료를 받을 수 있도록 하는 데 중요합니다.\n\n![image](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png)\n\n임상 코딩은 일반적으로 의료 전문가가 수행합니다. 이러한 코더들은 다양한 진단과 수술을 위한 특정 코드가 포함된 복잡하고 종종 계층적인 코딩 용어를 탐색합니다. 따라서 코더들은 사용된 코딩 용어에 대한 깊은 이해와 경험을 가져야 합니다. 그러나 문서를 수동으로 코딩하는 것은 느릴 수 있고, 오류가 발생할 수 있으며, 상당한 인적 전문 지식이 필요하여 병목 현상이 발생할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n심층 학습은 임상 코딩 자동화에 중요한 역할을 할 수 있습니다. 복잡한 의료 정보를 코드로 추출하고 번역함으로써, 심층 학습 시스템은 인간 중심 시스템 내에서 가치 있는 도구로 작용할 수 있습니다. 이러한 시스템은 코더들을 지원하여 대량의 데이터를 신속하게 처리하고 정확성을 향상시킬 수 있습니다. 이는 행정 업무를 간소화하고 청구 오류를 줄이며 환자 치료 결과를 향상시킬 수 있습니다.\n\n이 첫 번째 부분에서는 ICD 코딩이 무엇인지, 자동 코딩 시스템이 효과적으로 극복해야 하는 다양한 도전에 대해 설명합니다. 또한 대용량 언어 모델(LLM)이 이러한 문제를 극복하는 데 효과적으로 활용할 수 있는 방법을 분석하고, 최근 논문에서 LLM을 효과적으로 활용한 알고리즘을 적용하여 ICD 코딩에 성공적으로 적용하는 방법을 설명합니다.\n\n## 목차:\n\n- ICD 코딩이란 무엇인가?\n- 자동 ICD 코딩의 도전 요소는 무엇인가?\n- LLM이 자동 ICD 코딩에 어떻게 도움이 될까?\n- \"Off-the-shelf 대용량 언어 모델을 이용한 자동 임상 코딩\" 논문 탐색\n- 논문에 설명된 기법 구현\n- 결론\n- 참고문헌\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# ICD 코딩이란 무엇인가요?\n\n국제질병분류(ICD) 코딩은 세계보건기구에서 개발 및 유지보수하는 임상 용어 시스템입니다 [1]. 대부분의 국가에서 환자의 모든 진단, 증상 및 절차를 범주화하고 코딩하는 데 사용됩니다.\n\n환자의 진단과 의료 절차를 기록하는 의료 기록은 ICD 코딩에 매우 중요합니다. ICD 용어는 대략 75,000가지 다른 코드로 구성된 트리 구조를 특징으로 하여 방대한 정보를 효율적으로 정리합니다. 이러한 문서를 정확하게 코딩하는 것이 중요합니다. 정확한 코딩은 적절한 청구를 보장하며 의료 분석 품질에 영향을 미치며 환자 치료 결과, 보상 및 의료 효율성에 직접적으로 영향을 줍니다.\n\n# 자동 ICD 코딩에서 어떤 도전들이 있을까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nICD 코딩은 효과적으로 작동하기 위해 자동화된 시스템이 극복해야 할 여러 가지 도전이 있습니다.\n\n## ICD 코딩의 레이블 다양성:\n\n중요한 도전 중 하나는 레이블의 광범위한 출력 공간입니다. ICD 코드는 많고 각 코드는 미세한 세부 사항에서 차이가 있을 수 있습니다. 예를 들어, 오른손에 영향을 주는 상태와 왼손에 영향을 주는 상태는 서로 다른 코드를 갖게 됩니다. 또한 의료 기록에서 드물게 나타나는 희귀 코드의 긴 꼬리가 존재하여, 이러한 코드를 학습하고 정확하게 예측하기 어렵게 만들 수 있습니다.\n\n## 새로운 ICD 코드에 대한 적응:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n번거로우시겠지만, 테이블 태그를 마크다운 형식으로 변경해드릴게요!\n\n전통적인 데이터셋인 MIMIC-III [2] 같은 경우는 종합적이지만, 종종 ICD 코드의 범위를 훈련 말뭉치에 포함된 코드로 제한합니다. 이 제한은 의료 기록에서 ICD 코드로의 딥러닝 모델을 다중 레이블 분류 문제로 처리하는 데 새로운 코드가 도입된 경우 모형 훈련 이후에 어려움을 겪을 수 있음을 의미합니다. 이는 재훈련이 필요하고 잠재적으로 어려울 수 있게 만듭니다.\n\n## 정보 추출 및 문맥 활용:\n\n또 다른 주요 과제는 의료 기록에서 정보를 정확하게 추출하고 문맥에 맞게 처리하는 것입니다. ICD 코딩은 근본적으로 정보 검색 문제로, 의료 기록에서 진단을 식별하는 것 뿐만 아니라 이러한 진단을 해당 ICD 코드로 올바르게 매핑하는 데 필요한 모든 보완 정보를 포착해야 합니다. 따라서 자동화된 시스템이 의료 기록에서 여러 진단을 추출하고 적절히 문맥화하여 ICD 코드로 정확하게 매핑되도록 하는 것이 중요합니다.\n\n![이미지](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"Contextualization\"란 여기서 무엇을 의미할까요? 의학 노트를 다룰 때 진단을 맥락에 맞게 처리하는 것은 관련 세부사항과 관련된 정보 — 예를 들어 영향을 받는 신체 부위 및 질환의 증상 — 를 연결하여 진단을 완전히 특성화하는 것을 의미합니다. 일반적으로 이 작업은 관계 추출로 참조됩니다.\n\n# 대규모 언어 모델(LLMs)이 자동 ICD 코딩에 어떻게 도움이 되나요?\n\n자동 ICD 코딩의 과제를 다룰 때, 대규모 언어 모델 (LLMs)은 이러한 문제에 대처하는 데 적합하며, 특히 새로운 레이블에 대한 적응성과 복잡한 정보 추출 작업을 관리하는 능력으로 인해 잘 역할을 합니다. 그러나 여기서의 포인트는 LLMs가 자동 ICD 코딩에 대한 최상의 해결책이거나 이러한 문제를 해결할 수 있는 유일한 해결책인 것을 주장하는 것이 아니라, 자동 ICD 코딩 시스템이 극복해야 하는 주요 과제들을 설정함으로써 LLMs의 능력을 최대한 활용하여 이를 해결할 수 있는지를 분석하는 것입니다.\n\n## 새로운 및 드문 ICD 코드에 대한 적응:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLLMs는 견고한 제로샷 및 퓨샷 학습 능력을 보여주며, 적은 예시와 프롬프트에서 제공된 지침으로 새로운 작업에 적응할 수 있습니다. 검색 증강 생성 (RAG)은 미세 조정 없이도 LLM이 새로운 작업에 적응하기 위해 더 많은 맥락 정보에 접근할 수 있는 패러다임입니다. 이는 특히 기존의 훈련 데이터셋에서 자주 나타나지 않을 수 있는 새로운 및/또는 희귀한 ICD 코드에 LLM을 조정하는 데 유용합니다. 이를 단지 몇 가지 설명 또는 사용 사례로부터 합니다.\n\n## 맥락 정보:\n\nLLMs는 임상 분야에서의 제로샷 관계 추출에서 효과적으로 확인되었습니다. 제로샷 관계 추출은 LLM이 해당 관계에 대해 이전에 구체적인 훈련을 받지 않고 텍스트에서 관계를 식별하고 분류할 수 있도록 합니다. 이를 통해 의료 코딩에서의 진단을 더 잘 맥락화하여 더 정확한 ICD 코드를 가져올 수 있습니다.\n\n# \"Automated clinical coding using off-the-shelf large language models\" 논문 탐색하기:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLLM을 ICD 코딩에 적용한 최근 연구를 탐색하다가, 특정한 세부 조정 없이 LLM을 활용한 ICD 코딩에 관한 매우 흥미로운 논문을 발견했습니다. 저자들은 LLM을 활용한 ICD 코딩을 위해 LLM-지도된 트리 탐색이라는 방법을 개발했습니다 [5].\n\n## 이 방법은 어떻게 작동하나요?\n\nICD 용어는 계층적인 트리 구조입니다. 각 ICD 코드는 이 계층적 구조 내에 존재하며, 부모 코드는 더 일반적인 상태를 다루고, 자식 코드는 특정 질병을 상세히 설명합니다. ICD 트리를 탐색하면 더 구체적이고 세분화된 진단 코드로 이어집니다.\n\nLLM-지도된 트리 탐색에서는 탐색이 루트에서 시작되고 LLM을 사용하여 탐색할 가지를 선택하며, 모든 경로가 고갈될 때까지 반복적으로 계속합니다. 실제로 이 과정은 트리의 임의의 수준에서 모든 코드의 설명과 의료 노트를 LLM에 프롬프트로 제공하고, 해당 의료 노트에 대한 관련 코드를 식별하도록 요청하는 것으로 구현됩니다. 각 인스턴스에서 LLM에 의해 선택된 코드는 더 구체적으로 탐색되고 조사됩니다. 이 방법을 사용하면 가장 관련성이 높은 ICD 코드가 식별되며, 이후 임상 노트에 대한 예측 레이블로 할당됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예시를 통해 이를 명확히 해보겠습니다. ICD 코드 1과 ICD 코드 2라는 두 개의 루트 노드를 가진 트리를 상상해보세요. 각 노드는 코드를 특성화하는 평문 설명을 가지고 있습니다. 초기 단계에서 LLM에게 의학 노트와 코드 설명이 제공되고 의학 노트와 관련된 코드를 식별하도록 요청됩니다.\n\n이 시나리오에서 LLM은 의학 노트와 관련이 있는 것으로 판단된 ICD 코드 1과 ICD 코드 2를 식별합니다. 알고리즘은 각 코드의 자식 노드를 조사합니다. 각 부모 코드는 더 구체적인 ICD 코드를 나타내는 두 개의 자식 노드를 가지고 있습니다. ICD 코드 1부터 시작하여, LLM은 ICD 코드 1.1과 ICD 코드 1.2의 설명을 사용하여 의학 노트를 기반으로 관련 코드를 결정합니다. LLM은 ICD 코드 1.1이 관련이 있다고 결론 내리고, ICD 코드 1.2는 관련이 없다고 판단합니다. ICD 코드 1.1에는 더 이상의 자식 노드가 없으므로, 알고리즘은 할당 가능한 코드인지 확인하고 문서에 할당합니다. 그 다음 알고리즘은 ICD 코드 2의 자식 노드를 평가합니다. LLM을 호출하여, ICD 코드 2.1이 관련이 있는 것으로 판단합니다. 이것은 간단화된 예시이며, 실제로는 ICD 트리는 광범위하고 깊기 때문에 알고리즘은 각 관련된 노드의 자식을 탐색하거나 트리의 끝에 도달하거나 유효한 탐색을 소진할 때까지 계속됩니다.\n\n## 핵심\n\n- 이 방법은 LLM의 세밀한 조정이 필요하지 않습니다. 대신, 제공된 설명을 기반으로 LLM의 의료 노트를 상황에 맞게 이해하고 관련 ICD 코드를 동적으로 식별할 수 있는 능력을 활용합니다.\n- 더 나아가, 본 논문은 LLM이 프롬프트에 관련 정보가 주어질 때 대규모 출력 공간에 효과적으로 적응할 수 있으며, macro-average 지표 측면에서 드문 코드에서 PLM-ICD [6]를 앞지를 수 있다는 것을 보여줍니다.\n- 이 기술은 또한 파라메트릭 지식에 기초하여 의학 노트의 ICD 코드를 예측하도록 LLM에 직접 요청하는 기준선을 능가합니다. 이는 LLM을 임상 코딩 작업을 해결하기 위한 도구나 외부 지식과 통합하는 잠재력을 강조합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단점\n\n- 알고리즘은 트리의 각 수준에서 LLM을 호출합니다. 이로 인해 트리를 탐색하는 동안 LLM 호출 횟수가 많아지며, ICD 트리의 광범위함이 이에 더해집니다. 이는 단일 문서를 처리하는 데 높은 대기 시간과 비용으로 이어집니다.\n- 저자들이 논문에서 언급한 바와 같이, 관련 있는 코드를 정확하게 예측하려면 LLM이 모든 수준에서 부모 노드를 올바르게 식별해야 합니다. 한 수준에서 실수가 발생하더라도, LLM은 최종 관련 코드에 도달할 수 없게 됩니다.\n- 저자들은 MIMIC-III와 같은 데이터셋을 사용하여 메소드를 평가할 수 없었습니다. 외부 서비스로의 데이터 전송을 금지하는 제한 사항으로 인하여 OpenAI의 GPT 엔드포인트와 같은 외부 서비스로의 데이터 전송이 불가능했습니다. 대신, 저자들은 CodiEsp 데이터셋 [7,8]의 테스트 세트를 사용하여 해당 방법을 평가했습니다. 해당 데이터셋은 250개의 의학 노트를 포함하고 있습니다. 이 데이터셋의 크기가 작은 것은 해당 방법이 대규모 임상 데이터셋에서의 성능을 아직 입증하지 못했음을 시사합니다.\n\n# 논문에서 설명한 기술 구현하기\n\n이 기술을 구현하여 그 작동 방식을 더 잘 이해해 봅시다. 논문에서 언급했듯이, 해당 논문은 평가를 위해 CodiEsp 테스트 세트를 사용합니다. 이 데이터셋은 스페인어 의학 노트와 이에 대응하는 ICD 코드로 구성되어 있습니다. 데이터셋에는 영어로 번역된 버전도 포함되어 있지만, 저자들은 스페인어 의학 노트를 GPT-3.5를 사용하여 영어로 번역하였으며, 이를 통해 사전 번역된 버전을 사용하는 것보다 성능이 약간 향상되었다고 주장했습니다. 이 기능을 복제하고 노트를 영어로 번역해 봅시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndef construct_translation_prompt(medical_note):\n    \"\"\"\n    Construct a prompt template for translating Spanish medical notes to English.\n\n    Args:\n        medical_note (str): The medical case note.\n\n    Returns:\n        str: A structured template ready to be used as input for a language model.\n    \"\"\"\n    translation_prompt = \"\"\"You are an expert Spanish-to-English translator. You are provided with a clinical note written in Spanish.\nYou must translate the note into English. You must ensure that you properly translate the medical and technical terms from Spanish to English without any mistakes.\nSpanish Medical Note:\n{medical_note}\"\"\"\n\n    return translation_prompt.format(medical_note = medical_note)\n```\n\nNow that we have the evaluation corpus ready, let’s implement the core logic for the tree-search algorithm. We define the functionality in get_icd_codes, which accepts the medical note to process, the model name, and the temperature setting. The model name must be either “gpt-3.5-turbo-0613” for GPT-3.5 or “meta-llama/Llama-2–70b-chat-hf” for Llama-2 70B Chat. This specification determines the LLM that the tree-search algorithm will invoke during its processing.\n\nEvaluating GPT-4 is possible using the same code-base by providing the appropriate model name, but we choose to skip it as it is quite time-consuming.\n\n```js\ndef get_icd_codes(medical_note, model_name=\"gpt-3.5-turbo-0613\", temperature=0.0):\n    \"\"\"\n    Identifies relevant ICD-10 codes for a given medical note by querying a language model.\n\n    This function implements the tree-search algorithm for ICD coding described in https://openreview.net/forum?id=mqnR8rGWkn.\n\n    Args:\n        medical_note (str): The medical note for which ICD-10 codes are to be identified.\n        model_name (str): The identifier for the language model used in the API (default is 'gpt-3.5-turbo-0613').\n\n    Returns:\n        list of str: A list of confirmed ICD-10 codes that are relevant to the medical note.\n    \"\"\"\n    assigned_codes = []\n    candidate_codes = [x.name for x in CHAPTER_LIST]\n    parent_codes = []\n    prompt_count = 0\n\n    while prompt_count < 50:\n        code_descriptions = {}\n        for x in candidate_codes:\n            description, code = get_name_and_description(x, model_name)\n            code_descriptions[description] = code\n\n        prompt = build_zero_shot_prompt(medical_note, list(code_descriptions.keys()), model_name=model_name)\n        lm_response = get_response(prompt, model_name, temperature=temperature, max_tokens=500)\n        predicted_codes = parse_outputs(lm_response, code_descriptions, model_name=model_name)\n\n        for code in predicted_codes:\n            if cm.is_leaf(code[\"code\"]):\n                assigned_codes.append(code[\"code\"])\n            else:\n                parent_codes.append(code)\n\n        if len(parent_codes) > 0:\n            parent_code = parent_codes.pop(0)\n            candidate_codes = cm.get_children(parent_code[\"code\"])\n        else:\n            break\n\n        prompt_count += 1\n\n    return assigned_codes\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 논문과 비슷하게, ICD-10 트리에 액세스하는 simple_icd_10_cm 라이브러리를 사용합니다. 이를 통해 트리를 탐색하고, 각 코드에 대한 설명에 액세스하며 유효한 코드를 식별할 수 있습니다. 먼저, 트리의 첫 번째 수준에서 노드를 가져옵니다.\n\n```js\nimport simple_icd_10_cm as cm\n\ndef get_name_and_description(code, model_name):\n    \"\"\"\n    ICD-10 코드의 이름과 설명을 검색합니다.\n\n    Args:\n        code (str): ICD-10 코드.\n\n    Returns:\n        tuple: 형식화된 설명과 코드의 이름이 포함된 튜플을 반환합니다.\n    \"\"\"\n    full_data = cm.get_full_data(code).split(\"\\n\")\n    return format_code_descriptions(full_data[3], model_name), full_data[1]\n```\n\n루프 내부에서 각 노드에 해당하는 설명을 얻습니다. 이제 의료 노트와 코드 설명을 기반으로 LLM을 위한 프롬프트를 작성해야 합니다. 우리는 논문에서 제공된 세부 정보를 기반으로 GPT-3.5와 Llama-2용 프롬프트를 작성합니다.\n\n```js\nprompt_template_dict = {\"gpt-3.5-turbo-0613\" : \"\"\"[사례 노트]:\n{note}\n[예시]:\n<예시 프롬프트>\n위식도 역류병\n장전위\n\n<응답>\n위식도 역류병: 예, 환자에게 오메프라졸 처방함.\n장전위: 아니오.\n\n[작업]:\n다음 ICD-10 코드 설명 각각을 고려하고 사례 노트에 관련 언급이 있는지 평가하십시오.\n예시의 형식을 정확히 따르십시오.\n\n{code_descriptions}\"\"\",\n\n\"meta-llama/Llama-2-70b-chat-hf\": \"\"\"[사례 노트]:\n{note}\n\n[예시]:\n<코드 설명>\n* 위식도 역류병\n* 장전위\n* 급성비인두염 [감기]\n</코드 설명>\n\n<응답>\n* 위식도 역류병: 예, 환자에게 오메프라졸 처방함.\n* 장전위: 아니오.\n* 급성비인두염 [감기]: 아니오.\n</응답>\n\n[작업]:\n예시 응답 형식을 정확히 따르십시오. (예) 판단하기 전에 전체 설명과 (예|아니오) 판단을 입력한 후에 새 줄을 추가하십시오.\n다음 ICD-10 코드 설명을 고려하고 사례 노트에서 관련 언급이 있는지 확인하십시오.\n\n{code_descriptions}\"\"\"\n}\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n의료 기록과 코드 설명에 기반한 프롬프트를 지금 만들어 보겠습니다. 프롬프트 및 코딩에서 우리에게 이점은 GPT-3.5 및 Llama 2 모두와 상호 작용하기 위해 동일한 openai 라이브러리를 사용할 수 있다는 것입니다. 단, Llama-2가 deepinfra를 통해 배포되어야 합니다. deepinfra는 LLM에 요청을 보내기 위한 openai 형식도 지원합니다.\n\n```js\ndef construct_prompt_template(case_note, code_descriptions, model_name):\n    \"\"\"\n    주어진 케이스 노트와 ICD-10 코드 설명을 평가하는 프롬프트 템플릿 구성\n\n    Args:\n        case_note (str): 의료 케이스 노트\n        code_descriptions (str): ICD-10 코드 설명을 단일 문자열로 포맷팅\n\n    Returns:\n        str: 언어 모델에 입력으로 사용할 준비된 구조화된 템플릿\n    \"\"\"\n    template = prompt_template_dict[model_name]\n\n    return template.format(note=case_note, code_descriptions=code_descriptions)\n\ndef build_zero_shot_prompt(input_note, descriptions, model_name, system_prompt=\"\"):\n    \"\"\"\n    시스템 및 사용자 역할에 대한 제로샷 분류용 프롬프트 빌드\n\n    Args:\n        input_note (str): 입력 노트 또는 질의\n        descriptions (list of str): ICD-10 코드 설명 리스트\n        system_prompt (str): 선택적 초기 시스템 프롬프트 또는 지시\n\n    Returns:\n        list of dict: 각 메시지의 역할 및 내용을 정의하는 구조화된 사전 목록\n    \"\"\"\n    if model_name == \"meta-llama/Llama-2-70b-chat-hf\":\n        code_descriptions = \"\\n\".join([\"* \" + x for x in descriptions])\n    else:\n        code_descriptions = \"\\n\".join(descriptions)\n\n    input_prompt = construct_prompt_template(input_note, code_descriptions, model_name)\n    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": input_prompt}]\n```\n\n프롬프트를 구성한 후, 이제 LLM을 호출하여 응답을 받겠습니다:\n\n```js\ndef get_response(messages, model_name, temperature=0.0, max_tokens=500):\n    \"\"\"\n    채팅-완성 API를 통해 지정된 모델로부터 응답을 획득\n\n    Args:\n        messages (list of dict): API 입력용 구조화된 메시지 목록\n        model_name (str): 쿼리할 모델의 식별자\n        temperature (float): 응답의 무작위성을 제어하는 값, 0이면 결정론적\n        max_tokens (int): 응답의 토큰 수 제한\n\n    Returns:\n        str: 모델에서의 응답 메시지 내용\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens\n    )\n    return response.choices[0].message.content\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n좋아요, 우리가 출력물을 얻었어요! 이 응답으로부터, 이제 LLM이 추가적인 탐색을 위해 관련있는 노드들과 거부한 노드들을 식별하기 위해 각 코드 설명을 구문 분석합니다. 우리는 출력 응답을 새 줄로 나누고 각 응답을 분할하여 LLM의 각 코드 설명에 대한 예측을 식별합니다.\n\n```js\ndef remove_noisy_prefix(text):\n    # 문자 또는 숫자가 뒤따르고 점과 선택적 공백으로 시작하는 문자열의 제일 앞에 있는 숫자나 문자를 제거합니다.\n    cleaned_text = text.replace(\"* \", \"\").strip()\n    cleaned_text = re.sub(r\"^\\s*\\w+\\.\\s*\", \"\", cleaned_text)\n    return cleaned_text.strip()\n\ndef parse_outputs(output, code_description_map, model_name):\n    \"\"\"\n    모델 출력을 구문 분석하여 주어진 설명 매핑에 따른 ICD-10 코드를 확인합니다.\n\n    Args:\n        output (str): 확인을 포함하는 모델 출력입니다.\n        code_description_map (dict): 설명과 ICD-10 코드의 매핑입니다.\n\n    Returns:\n        list of dict: 확인된 코드 및 해당 설명의 목록입니다.\n    \"\"\"\n    confirmed_codes = []\n    split_outputs = [x for x in output.split(\"\\n\") if x]\n    for item in split_outputs:\n        try:\n            code_description, confirmation = item.split(\":\", 1)\n            if model_name == \"meta-llama/Llama-2-70b-chat-hf\":\n                code_description = remove_noisy_prefix(code_description)\n\n            if confirmation.lower().strip().startswith(\"yes\"):\n                try:\n                    code = code_description_map[code_description]\n                    confirmed_codes.append({\"code\": code, \"description\": code_description})\n                except Exception as e:\n                    print(str(e) + \" Here\")\n                    continue\n        except:\n            continue\n    return confirmed_codes\n```\n\n이제 루프의 나머지를 살펴봅시다. 지금까지 우리는 프롬프트를 구성했고, LLM으로부터 응답을 받았으며, 출력을 구문 분석하여 LLM에 의해 관련이 있다고 판단된 코드를 식별했습니다.\n\n```js\nwhile prompt_count < 50:\n    code_descriptions = {}\n    for x in candidate_codes:\n        description, code = get_name_and_description(x, model_name)\n        code_descriptions[description] = code\n\n    prompt = build_zero_shot_prompt(medical_note, list(code_descriptions.keys()), model_name=model_name)\n    lm_response = get_response(prompt, model_name, temperature=temperature, max_tokens=500)\n    predicted_codes = parse_outputs(lm_response, code_descriptions, model_name=model_name)\n\n    for code in predicted_codes:\n        if cm.is_leaf(code[\"code\"]):\n            assigned_codes.append(code[\"code\"])\n        else:\n            parent_codes.append(code)\n\n    if len(parent_codes) > 0:\n        parent_code = parent_codes.pop(0)\n        candidate_codes = cm.get_children(parent_code[\"code\"])\n    else:\n        break\n\n    prompt_count += 1\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 예측된 코드를 반복하며 각 코드가 \"leaf\" 코드인지 확인합니다. 이는 코드가 유효하고 할당 가능한 ICD 코드임을 보증하는 것입니다. 예측된 코드가 유효하면 LLM이 그 의료 노트에 대한 예측으로 간주합니다. 유효하지 않으면 상위 코드에 추가하여 ICD 트리를 더 탐색하기 위해 자식 노드를 얻습니다. 더 이상 탐색할 상위 코드가 없을 경우 루프를 탈출합니다.\n\n이론적으로 의료 노트 당 LLM 호출 수는 임의로 높을 수 있으며, 알고리즘이 많은 노드를 탐색하는 경우 지연 시간이 증가할 수 있습니다. 저자는 의료 노트 당 최대 50회 프롬프트/LLM 호출로 처리를 종료하는 최대 수를 시행했습니다. 이 한계는 우리가 구현에서도 채택합니다.\n\n## 결과\n\n이제 GPT-3.5와 Llama-2를 LLM으로 사용하여 트리 탐색 알고리즘의 결과를 평가할 수 있습니다. 우리는 알고리즘의 성능을 마이크로-평균 및 매크로-평균 정밀도, 재현율 및 F1 점수를 통해 평가합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![ExploringLLMsforICDCodingPart1_2](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_2.png)\n\n구현 결과는 논문에 보고된 점수와 대략적으로 일치하지만 주목할 만한 차이점이 있습니다.\n\n- 이 구현에서 GPT-3.5의 마이크로 평균 측정 지표는 보고된 값보다 약간 뛰어나지만, 매크로 평균 측정 지표는 보고된 값보다 조금 부족합니다.\n- 마찬가지로 Llama-70B의 마이크로 평균 측정 지표는 보고된 값과 일치하거나 조금 뛰어나지만, 매크로 평균 측정 지표는 보고된 값보다 낮습니다.\n\n앞서 언급했듯이, 이 구현은 몇 가지 미세한 차이점을 가지고 있어 최종 성능에 영향을 미칩니다. 이 구현이 원본 논문과 어떻게 다른지에 대한 보다 자세한 내용은 링크된 저장소를 참조해 주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n이 방법을 이해하고 구현하는 것은 여러 측면에서 나에게 매우 유익했습니다. 이를 통해 대규모 언어 모델(LLMs)의 강점과 약점에 대해 보다 세밀하게 이해할 수 있었고 임상 코딩 사례에서 그것을 구현할 수 있었습니다. 구체적으로, 코드에 관련된 중요한 정보에 동적으로 접근할 수 있는 경우 LLMs는 임상 문맥을 효과적으로 이해하고 관련 코드를 정확하게 식별할 수 있다는 것이 분명해졌습니다.\n\nLLMs를 임상 코딩을 위한 대리자로 활용하는 것이 성능을 더욱 향상시킬 수 있는지 탐구하는 것이 흥미로울 것입니다. 생명공학 및 임상 텍스트에 대한 외부 지식 소스가 논문이나 지식 그래프 형태로 풍부하게 제공되는 상황에서 LLM 대리자는 의료 문서를 보다 세밀한 단위로 분석하는 워크플로에 활용될 수 있습니다. 또한 필요한 경우 외부 지식을 참고하여 최종 코드에 도달할 수 있도록 동적으로 도구를 활용할 수도 있습니다.\n\n## 감사의 글\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 방법을 평가하는 데 도움을 준 이 논문의 주 저자 Joseph에게 큰 감사를 표합니다!\n\n- 참고 자료:\n\n[1] https://www.who.int/standards/classifications/classification-of-diseases\n\n[2] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. W. H., Feng, M., Ghassemi, M., … & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database Sci. Data, 3(1), 1.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n[3] Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., & Sontag, D. (2022). 대형 언어 모델은 소수의 적은 데이터로도 임상 정보를 추출합니다. arXiv 사전 인쇄 arXiv:2205.12689.\n\n[4] Zhou, H., Li, M., Xiao, Y., Yang, H., & Zhang, R. (2023). 임상 관계 추출을 위한 LLM Instruction-Example Adaptive Prompting (LEAP) 프레임워크. medRxiv : 의학과학 사전 인쇄 서버, 2023.12.15.23300059. https://doi.org/10.1101/2023.12.15.23300059\n\n[5] Boyle, J. S., Kascenas, A., Lok, P., Liakata, M., & O’Neil, A. Q. (2023, 10월). 상업용 대형 언어 모델을 사용한 자동 임상 코딩. NeurIPS 2023에서 Deep Generative Models for Health Workshop 발표.\n\n[6] Huang, C. W., Tsai, S. C., & Chen, Y. N. (2022). 사전 훈련된 언어 모델로 자동 ICD 코딩하기: PLM-ICD. arXiv 사전 인쇄 arXiv:2207.05289.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMiranda-Escalada, A., Gonzalez-Agirre, A., Armengol-Estapé, J., & Krallinger, M. (2020). CLEF (Working Notes), 2020에서 CodiEsp Track의 비영어 임상 사례에 대한 주석, 가이드라인 및 솔루션에 대한 개요.\n\nMiranda-Escalada, A., Gonzalez-Agirre, A., & Krallinger, M. (2020). CodiEsp corpus: ICD10 (CIE10)로 코드화된 골드 표준 스페인어 임상 사례 - eHealth CLEF2020 (1.4) [데이터 세트]. Zenodo. https://doi.org/10.5281/zenodo.3837305 (CC BY 4.0)\n","ogImage":{"url":"/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png"},"coverImage":"/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png","tag":["Tech"],"readingTime":23},{"title":"살아있는 인공지능의 첫 걸음, 바디 인텔리전스","description":"","date":"2024-05-17 19:35","slug":"2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence","content":"\n두 개의 세계가 충돌하고 있어요.\n\n우리는 방금 두 가지 분야에서 로봇 공학의 상당한 도약을 목격했어요. Figure AI의 말하는 로봇과 Google의 일반적인 에이전트 SIMA가 그겁니다.\n\n하지만, 아니요, 이것은 일반적인 인공 지능(AGI)은 아니에요. 일부 사치스럽지만 입증되지 않은 주장이 돌아다니고 로봇들이 우리를 다 죽일 거라고 말하진 않을 거예요.\n\n그러나 두 소식이 이미 자체로 흥미롭지만, 그들 간의 시너지는 내 관점에서 AI 구현 지능의 첫 번째 발걸음이라고 생각해요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# GPT-4의 구현\n\n그러면 Figure AI는 뭔가요?\n\nFigure AI는 로봇 회사로, CEO의 말에 따르면 \"위험하고 원하지 않는 직업이 필요하지 않도록 하여 미래 세대가 더 행복하고 의미 있는 삶을 살도록 허용하는 로봇을 구축하고 있습니다.\"\n\n이미 유사한 웅장한 사명을 가진 회사들을 들어보셨을 것입니다. 그러나 OpenAI, NVIDIA, Jeff Bezos, 그리고 Intel이 시장에 제품이 없는 회사에 대해 26억 달러 평가액에서 6억 7500만 달러 시리즈 투자를 한다면, 그들이 무엇인가를 찾고 있다는 것을 알 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 왜 모두가 이 회사에 대해 이야기하는 걸까요? 이 비디오 때문입니다.\n\n그 링크를 클릭하는 것을 강력히 권장합니다. 간단히 말하자면, 이 비디오를 통해 사람과 상호작용하는 로봇이 여러 작업을 민첩하게 수행한다는 것을 보여줍니다.\n\n흥미로운 점은 Figure AI의 로봇의 핵심에는 GPT-4V, OpenAI의 주요 다중모달 대형 언어 모델인 MLLM이 있다는 것입니다.\n\n다시 말해, Figure AI의 로봇은 '화신 ChatGPT'의 처음으로, 즉 LLMs가 실체화된 작업도 할 수 있는 능력이 생긴 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 대부분의 사람들이 놓칠 수도 있는데, 로봇이 이전 동작에 대한 이유를 설명하면서 사람에게 작업을 수행하도록 요청하는 순간에 주목하여 주길 바랍니다.\n\n이 질문이나 요청이 사소한 것은 아니라는 점을 분명히 이해해야 합니다. 모델이 동시에 여러 작업을 수행할 수 있는 능력을 보여주기 위해 일부러 그랬던 것입니다.\n\n구체적으로 말하자면, 그들은 GPT-4를 세밀하게 조정하여 텍스트와 작업 표현을 출력하도록 만들었는데, 전자는 보코더를 통해 음성으로 디코딩되고, 후자는 액추에이터 동작으로 디코딩되어 몸을 움직이도록 합니다.\n\nFigure AI의 로봇의 기본 메커니즘에 대해서는 많이 알지 못하지만, Deepmind의 RT-2 모델과 같은 예시를 통해, 우리는 이미 LLM을 로봇 동작이나 텍스트를 출력하도록 훈련하는 방법을 증명한 연구자들이 있기에 꽤 좋은 아이디어를 얻을 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png)\n\n그러나 RT-2는 GPT-4의 능력에 가까운 두 LLM인 PALM-E와 PALI-X를 사용하여 훈련된 것을 고려할 때, Figure AI의 로봇 뒤에 있는 LLM이 이 모델이 지금까지 존재한 가장 고급 비전-언어-행동 모델일 수 있다는 주장이 가능합니다.\n\n하지만 현실적으로 생각해 봅시다.\n\n그것은 여러 차례 연습된 것일 수 있는 매우 제한된 데모였습니다. 사실, 로봇이 일반적인 용도의 능력 측면에서 아직 매우 초기 단계에 있다는 것은 거의 확실합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n따라서, 이 로봇들의 여정에서 다음 단계를 상상하기 위해 구글이 방금 공개한 것에서 영감을 받을 수 있습니다.\n\nSIMA, 3D 총론적 에이전트.\n\n하지만 이번에는 Figure AI의 경우와 달리, 에이전트에 대해 훨씬 더 많은 정보를 갖고 있습니다.\n\n# SIMA, 최초의 진정한 총론적 에이전트?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGoogle의 확장 가능한 인공 지능 멀티월드 에이전트(SIMA) 프로젝트는 어떤 3D 환경에서 임의의 언어 명령을 이해하고 실행할 수 있는 인공 지능(AI) 시스템을 만들고자 합니다.\n\n이 계획은 언어를 지각 및 다양한 가상 세계에서 실제 행동과 결합하여 처리함으로써 일반적인 AI 개발에서 중요한 과제에 대처합니다. 이는 연구 환경 및 상업 비디오 게임을 포함한 다양한 가상 세계를 대상으로 합니다.\n\n간단히 말하면, SIMA는 언어 요청을 받아들이고 이를 3D 환경에서 키보드 및 마우스 조작으로 변환합니다.\n\n그러나 고려해야 할 중요한 요소는 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 에이전트들의 목표는 \"자원을 수집하고 집을 짓는\" 등 사용자가 제시하는 자연어 지침대로 작업을 수행하는 것입니다.\n\n간단히 말해, 모델은 해당 환경에서 상호작용할 때 사람들과 똑같은 입력을 갖고 있으므로 사람들보다 약점이 없다는 것을 의미하며, 이는 기접근 API나 이와 유사한 것에 대한 접근 권한이 없다는 것을 의미합니다.\n\n결과적으로, 요구된 작업을 수행하는 유일한 방법은 해당 작업을 수행하는 데 사람이 수행할 키보드 및 마우스 작업을 예측하는 것입니다.\n\nSIMA 에이전트는 다음 구성 요소로 이루어져 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 마크다운 형식으로 변경한 텍스트입니다.\n\n![이미지](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_1.png)\n\n- 인코더:\n\n  - 텍스트 인코더: 언어 명령을 모델이 해석할 수 있는 임베딩으로 번역합니다.\n  - SPARC 개발을 기반으로 한 이미지 인코더.\n  - 비디오 인코더.\n\n2. Multi-modal Transformer + Transformer XL: 두 개의 트랜스포머 아키텍처로, 전자는 모달 간 교차 어텐션을 수행하고, 후자는 이전 상태를 취해 새로운 상태를 식별합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 정책: 선택된 행동이 600가지 가능한 기술 중 하나로 결정되는 분류 헤드\n\n여기에는 해석할 내용이 많기 때문에 단계별로 진행해 봅시다.\n\n## 세계 처리\n\n대부분의 최신 모델들에서와 마찬가지로, 첫 번째 단계는 입력을 \"인코딩\"하는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적인 용어로 설명하자면, 입력 데이터(텍스트와 비디오)를 가져와 해당 인코더를 사용하여 이러한 데이터 포인트를 벡터 임베딩으로 변환하는 것이 아이디어입니다.\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_2.png)\n\n하지만 왜 이 작업을 하는 것일까요?\n\n이 변환을 수행함으로써 각 요소가 해당 개념의 의미론을 캡처하는 밀집 벡터로 표현됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다시 말해, 의미론적으로 유사한 개념은 유사한 벡터를 갖게 되어 벡터 공간에서 표현되었을 때 더 가까이 위치하게 됩니다:\n\n![Concept Vectors](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_3.png)\n\n또 다른 중요한 이점은 '개념을 벡터로 변환함'으로, 인간이 아주 무의식적으로 수행하는 '우리 세계를 이해하는' 행위를 수학적인 연습(컴퓨터에 이상적)으로 변환하여, 개념이 이제 수치로 표현된다는 것입니다.\n\n특히, 이 모델은 이러한 벡터들 사이의 유사성(그들 사이의 거리)을 계산하여, 어떤 개념이 다른 것들과 유사한지를 알아냅니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예를 들어, 모델은 '개'와 '고양이'가 유사한 속성 (포유류, 길들여진 동물, 네 다리 등)을 공유하는 유사한 개념을 나타낸다는 것을 알 수 있습니다.\n\n분명히 말씀드리자면, AI가 각 숫자를 다루는 방식은 아니며, AI가 중요시하는 것은 벡터 간의 근접성입니다. 만약에 도움이 될 경우, 벡터의 의미를 단순히 벡터 자체로만 생각하지 마십시오 (우리는 AI가 정말 '개'가 무엇인지 아는지 확신할 수 없습니다) 대신, 가장 가까운 이웃들의 합으로 생각하고, 모델에게 다른 것들과 유사하다는 신호를 보내는 것으로 여기면 됩니다 (강아지는 고양이와 유사하며 문과는 다릅니다).\n\n비슷한 이유로 유사성은 데이터 유형이 다른 상황에 집중하는 데 중요한 역할을 합니다.\n\n다양한 데이터 유형에서 나온 벡터로 오는 여러 모달 상황에 집중하는 데 유용합니다.\n\n기본 아이디어는 '개'라는 단어와 '개의 이미지'가 유사한 벡터를 공유하도록 원한다는 것으로, 이것은 모델에게 두 가지가 동일한 개념을 나타낸다는 것을 알려줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 텍스트와 이미지는 구조적으로 매우 다른 데이터 유형이기 때문에 서로 다른 인코더가 필요합니다.\n\n이것은 문제입니다. 비슷한 개념에 대해 유사한 임베딩을 생성하도록 보장하며 별도의 인코더를 훈련해야 합니다.\n\n이 문제를 해결하기 위해 SIMA는 SPARC 이미지 인코더를 사용합니다.\n\n매우 최근에 개발된 SPARC 인코더는 대부분의 다른 이미지 인코더와 매우 유사한 방식으로 훈련됩니다(대비 학습을 사용), 그러나 미세 구체적인 세부 사항을 더 잘 캡처할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가장 흔한 이미지 인코더들은 이미지의 지역 세부사항을 제대로 포착하지 못하는 문제가 있습니다.\n\n네, 그들은 이미지가 무엇에 대해인지를 알려줄 수 있지만, 이미지의 전역 의미를 설명하는 데 도움이 되지 않는 중요한 작은 세부사항을 놓치기도 합니다. 그럼에도 불구하고 많은 경우에 중요합니다.\n\nSPARC는 유사한 방법을 제안하지만 매우 흥미로운 요소를 추가합니다.\n\n예를 들어, \"바구니 속 고양이와 개\"를 나타내는 이미지-텍스트 쌍이 있다고 가정해 봅시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_4.png\" />\n\n- 먼저, SPARC는 이미지를 패치로 나눕니다.\n- 그런 다음 각 패치에 대해 텍스트 설명 중 하나의 토큰을 할당합니다. 예를 들어, 패치가 개의 일부를 나타내면 \"개\"라는 단어가 할당됩니다.\n- 이 작업은 모든 패치(왼쪽 색상 그리드의 수직 열)에 대해 수행되며, 텍스트 설명의 여러 측면을 다루는 패치의 경우 각각에 가중치가 할당됩니다.\n\n- 특정 개념에 할당된 모든 패치를 가지고 있으면 각 패치가 '개'와 같은 특정 단어에 부여하는 가중치를 그룹화하여 이 그룹화된 임베딩을 실제 단어와 비교합니다. 그러나 여기에는 중요한 점이 있습니다. 이미지 전역에 대해 적용하는 대신, 모든 개념에 대해 이를 다섯 번 적용합니다.\n\n하지만 모든 이 작업을 하는 이유는 무엇일까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지역성. 이미지의 특정 부분이 이제 특정 개념에 할당되었으므로 모델은 이제 두 가지를 알게 됩니다:\n\n- 이미지에서 어떤 개념이 더 많이 표현되는지\n- 이러한 개념이 이미지에서 어디에 위치하는지.\n\n일반인이 이해하기 쉽게 설명하면 SPARC와 다른 이미지 인코더 간의 주요 차이점은 텍스트 설명의 개별 단어를 이미지의 특정 부분에 할당한다는 사실에 있습니다.\n\n이러한 방식으로 이미지의 특정 부분의 그룹화된 임베딩이 '개' 단어 토큰에 크게 편향되어 있다면, 이미지의 해당 영역에는 아마도 개가 포함되어 있을 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSIMA의 중요한 요소입니다. 3D 에이전트는 요청된 작업의 일환으로 상호 작용할 특정 객체를 식별할 수 있어야합니다.\n\n마지막으로, 비디오 인코더에 대해 이야기하면, 모델이 과거 상태를 고려할 수 있도록 포함되어 있습니다. 중요한 것은 비디오 인코더가 시간적 인식을 포함한다는 것인데, 이는 텍스트나 이미지 인코더가 제공할 수 없는 기능입니다.\n\n환경의 현재 상태뿐만 아니라 과거의 환경 상태와 취해진 조치에 따라 다음 조치를 취할지 결정하기 때문에 이는 중요합니다.\n\n## 최적의 정책 선택\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제공된 정보를 바탕으로 SIMA는 다른 인코더들에 의해 생성된 표현을 사용하여 변환 모델 세트를 활용합니다. 이 모델은 LLM이 단어를 예측하는 대신 작업을 출력하여 에이전트가 실행할 키보드 및 마우스 조작을 지시합니다.\n\n한편, 왜 Gemini, 구글의 MLLM을 사용하는 대신 모델의 주요 '두뇌'로 이상한 Transformers 세트를 사용했는지 궁금할 수도 있습니다.\n\n이유는 아마도 예산 때문인데, 연구자들 자신들이 기술 보고서에서 SIMA의 분명한 다음 단계는 Gemini를 사용하는 것이라고 인정했기 때문입니다.\n\n이것은 매우 흥미로운데, 최고의 '두뇌'를 사용하지 않았음에도 놀라운 결과를 얻었다고 하니, 이제 우리가 곧 살펴볼 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 진정한 일반화자\n\n지금쯤이면 알 수 있겠지만, 목표는 여태까지 한 게임에서도 능숙한 에이전트를 훈련시키는 것이었습니다. 게임을 해본 적이 없는 게임조차도요.\n\n훈련 후 SIMA 에이전트는 다양한 범주로 구분된 600가지 다양한 기본 작업을 수행할 수 있었습니다. 이러한 범주에는 네비게이션, 동물 상호작용, 음식 등이 포함되어 있었습니다:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSIMA가이러한작업을수행하고있는모습을여기에서확인할수있습니다. 게다가, SIMA는매우유망한결과를얻어 언급할만한가치가있습니다.\n\n우선, 다양한게임에서훈련을받았음에도불구하고, 평균적으로SIMA는 단일게임에서전문화된 에이전트들보다 더우수한성능을보였습니다... 그특정게임안에서:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_6.png)\n\n더욱인상적인점은, 여러가지다른게임에서, 에이전트가영이의업무(non-trivial tasks)를달성했으며, 이중대부분은얼마들이나예시없이(zero-shot tasks)성과를거두었으며, 다시한번전문화된 에이전트들을이겼습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_7.png\" />\n\n이는 과거에 본 적 없는 환경에 놓여도 모델이 일반적으로 잘 수행되었으며, 때로는 Goat Simulator 3와 같은 경우에는 전문화된 에이전트(해당 게임에서만 훈련된 에이전트)보다 뛰어난 성과를 보였습니다.\n\n그렇다면 이 모든 것이 의미하는 바는 무엇인가요?\n\n간단히 말하자면, 우리는 게임 간의 지식 전이의 구체적인 증거를 관찰하고 있다는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다시 말해, 모델은 일부 게임에서 유용하게 적용할 수 있는 의미 있는 기술을 학습합니다 (예: 키보드로 이동하는 방법을 배우는 것과 같이).\n\n더욱이, 이러한 기술들은 상당히 높은 품질을 갖고 있어서 이 일반화된 에이전트가 많은 게임에서 특화된 에이전트들을 이기는 것을 의미하며, 이는 일반화된 접근 방식이 그들이 다양한 환경에 적용할 우수한 기술을 학습하는 데 도움이 된다는 것을 시사합니다.\n\nFigure AI의 발전과 함께 이러한 결과들이 어떠한 맥락에서 중요성을 얻을 수 있는 경우, 전체적으로 매우 인상적인 결과입니다.\n\n# 로보틱스에게 훌륭한 주였습니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI 로봇 기술은 요즘 정말 빠르게 발전하고 있어요.\n\n- 한 쪽에서, Figure AI는 우리가 점점 더 많은 수동 작업 범위를 알리는 인간형 로봇을 만들어내는 데 탁월한 실력을 보여줍니다.\n- 반면에, SIMA는 3D 환경에서 최초의 일반 적 에이전트를 보여주고 있음을 암시합니다.\n\n하지만 우리가 깨닫는 것은 시너지의 잠재력입니다.\n\n이러한 에이전트들을 실생활 상황으로 가져오는 데는 아직 이르지만, 이 두 분야 사이의 융합은 다음 단계로 자연스러운 발전을 이루고 있습니다; SIMA가 훈련의 장소일 뿐만 아니라, Figure AI 로봇들이 일반적인 에이전트의 구현체 역할을 하기 때문이죠.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그리고 다른 기업들도 구체적 지능에 대한 자신들의 시사를 제시하며 경쟁이 치열해지고 있습니다. 많은 기존 기업들이 인류의 기술이 충분히 준비되어 있다고 느끼고 있어서 다음 큰 도전, 인공지능을 실생활에 적용하는 것에 착수할 준비가 되어있다고 느끼는 것이 분명합니다.\n","ogImage":{"url":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png"},"coverImage":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png","tag":["Tech"],"readingTime":15},{"title":"내일의 기계들","description":"","date":"2024-05-17 19:34","slug":"2024-05-17-MachinesofTomorrow","content":"\n# 내일의 기계: AI 원래부터 초지능 및 넘인간성까지, 어떻게 AI가 우리의 세계를 형성할 것인가\n\n아마존에서 \"내일의 기계\" 구매\n\n아마존이나 굿리드에서 리뷰 작성\n\n우리의 뉴스레터 구독하여 모든 챕터를 받아보세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Machines of Tomorrow](/assets/img/2024-05-17-MachinesofTomorrow_0.png)\n\n우리의 세상을 형성할 AI.\n\n모두가 쉽게 접근할 수 있는 쉬운 읽기 스타일로 작성된 \"Machines of Tomorrow\"은 인공 지능의 역사와 설명을 철저하게 소개함으로써 독자들이 실제 전망을 파악하고 최종적으로 이 책의 핵심 개념과 더 깊은 참여 수준을 얻게 합니다. 다가오는 수십 년 동안 사회들이 AI로 무엇을 기대해야 하는지에 대한.\n\nAI가 가까운 미래와 중기에 고용과 교육에 어떤 영향을 미칠까요? AI가 우리 사회, 경제, 정부, 문화 및 지정학에 어떤 영향을 미칠까요? AI 구현의 유토피아적 및 디스토피아적 결과는 무엇이며 누가 영향을 받을 것인가요? 중국과 새로운 AI 무기 경쟁에 대해 실제로 무슨 일이 벌어지고 있을까요? 합성생물학이 정말 인간-사이보그 공존으로 이끌 것이며 결국 인간 진화에 영향을 미칠까요? 이 기술의 불가피한 진전에 대비하려면 리더들에게 지금 어떤 것을 요구해야 할까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPedro Uria-Recio와 Randy McGraw가 제기하고 대답한 몇 가지 신선하고 논쟁적인 질문들 중 일부입니다. Pedro Uria-Recio는 McKinsey 컨설턴트 출신, Randy McGraw는 AI 부문 최고 책임자 및 Amazon 전 임직원으로 이들은 수년간 이 기술을 전 세계적으로 활용해 왔습니다.\n\n기술자일지라도 주부일지라도 사업가일지라도 학생일지라도 유권자일지라도 정책 입안자일지라도 혹은 단순히 인류의 미래에 대해 궁금해하는 분이라도, \"Machines of Tomorrow\"은 인공지능의 복잡성을 탐험하는 데 필수적인 안내를 제공합니다.\n\n인류와 AI가 얽히는 지점.\n\nAI는 우리의 새로운 마음입니다. 로봇공학은 새로운 신체입니다. 우리는 탄소와 실리콘의 교차로에서 새로운 종이 어떻게 될까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI가 기하급수적으로 발전하고 있습니다.\n\n일반적인 AI. 안드로이드와 사이보그. 합성 생물학. 양자 컴퓨팅. 마음의 모방. 이 모든 것들이 어떻게 펼쳐질까요?\n\nAI 독재가 위협적입니다.\n\nAI는 진실을 무효화하고, 자유를 재정의하며, 일자리 부족을 불러올 것입니다. 우리는 아직도 AI를 모두의 이익을 위해 형성할 수 있을까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지구정치의 역학화.\n\n초인지능이 숭배받을 것입니다. 중국과 미국은 인공지능에 대한 견해를 놓고 충돌할 것입니다. 정치는 종족 정체성을 중심으로 펼쳐질 것입니다.\n\n인류의 위대한 서사시.\n\n신화에서 큐브릭까지. 아리스토텔레스에서 샘 알트만까지. 레오나르도 다 빈치에서 보스턴 다이내믹스까지. 오늘부터 초인지능까지.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음 장 `` 차례\n\n참고 자료, 용어 해설\n\n\"내일의 기계\"를 아마존에서 구입하세요\n\n아마존이나 구글 리드에서 리뷰 작성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리의 뉴스레터 구독을 신청하시면 모든 챕터를 받아보실 수 있어요\n\n작성자: Pedro Uria-Recio, Randy McGraw와 함께\n\nmachinesoftomorrow.ai에서 확인해보세요\n\n![이미지](/assets/img/2024-05-17-MachinesofTomorrow_1.png)\n","ogImage":{"url":"/assets/img/2024-05-17-MachinesofTomorrow_0.png"},"coverImage":"/assets/img/2024-05-17-MachinesofTomorrow_0.png","tag":["Tech"],"readingTime":4},{"title":"라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기","description":"","date":"2024-05-17 19:32","slug":"2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi","content":"\n## Phi-2, Mistral, 그리고 LLaVA와 같은 모델을 Raspberry Pi에서 Ollama를 사용하여 로컬에서 실행하기\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png)\n\n자신의 기기에서 큰 언어 모델(LLMs)이나 시각 언어 모델(VLMs)을 실행해 보고 싶은 적이 있었나요? 아마 그렇겠죠. 그러나 모든 것을 처음부터 설정하고 환경을 관리하고 적절한 모델 가중치를 다운로드해야 한다는 생각, 그리고 기기가 해당 모델을 처리할 수 있는지에 대한 미심쩍음 때문에 조금 망설이셨을 겁니다.\n\n그보다 한 발 더 나아가보죠. 크기가 신용카드보다 작은 기기인 Raspberry Pi에서 자체 LLM 또는 VLM을 운영하고 있다고 상상해보세요. 불가능한 것일까요? 전혀 그렇지 않아요. 내가 이 게시물을 작성하고 있으니까 가능한 일이죠.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 가능하다면, 그렇게 해도 괜찮아요. 하지만 왜 그걸 하고 싶으세요?\n\n현재에는 가장자리의 LLM이 매우 비현실적으로 보입니다. 하지만 특정한 이러한 예외적인 사용 사례는 시간이 지나면 성숙해질 것이며, 기기에서 실행되는 올 로컬 생성형 AI 솔루션을 이용해 멋진 가장자리 솔루션이 구축될 것입니다.\n\n이것은 무엇이 가능한지 알아보고자 하는 것입니다. 만약 이것이 컴퓨팅 규모의 극단단에 가능하다면, 라즈베리 파이와 강력한 서버 GPU 사이의 어느 수준에서든 가능할 것입니다.\n\n전통적으로, 가장자리 AI는 컴퓨터 비전과 밀접하게 관련되어 왔습니다. LLMs와 VLMs의 배포를 탐색함으로써 가장자리에 이러한 새로운 면을 추가하면 이 분야에서 떠오르고 있는 흥미로운 측면을 발견할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가장 중요한 것은, 최근에 얻은 Raspberry Pi 5에서 뭔가 재미있는 것을 하고 싶었습니다.\n\n그래서, Raspberry Pi에서 이 모든 것을 어떻게 구현할 수 있을까요? Ollama를 사용해서!\n\n## Ollama가 무엇인가요?\n\nOllama는 처음부터 설정하기 귀찮은 일 없이 개인 컴퓨터에서 로컬 LLM을 실행하는 데 최적의 솔루션이 되어 나타났습니다. 몇 가지 명령어로 모든 것을 문제없이 설정할 수 있습니다. 모든 것이 독립적으로 구성되어 있으며, 몇 가지 기기 및 모델에서의 제 경험에 따르면 훌륭하게 작동합니다. Raspberry Pi에서 실행시켜 놓고 원한다면 다른 응용프로그램 및 기기에서 호출할 수 있는 모델 추론을 위한 REST API까지 노출합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_1.png\" />\n\n또한, 명령 줄 인터페이스에 대해 걱정하는 사람들을 위해 웅장한 AI UI/UX인 Ollama 웹 UI도 있습니다. 이것은 바로 지금 기본적으로 로컬 ChatGPT 인터페이스입니다, 만약 당신이 원한다면요.\n\n이 두 오픈 소스 소프트웨어는 현재 최상의 로컬 호스팅 LLM 경험을 제공한다고 생각합니다.\n\nOllama 및 Ollama 웹 UI는 LLaVA와 같은 VLM도 지원하며, 이는 엣지 생성적 AI 사용 사례에 대한 더 많은 가능성을 엽니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 기술 요구 사항\n\n다음만 있으면 됩니다:\n\n- Raspberry Pi 5 (속도가 느린 설정을 위해 4 버전 사용 가능) — 7B 모델에 맞게 8GB RAM 모델을 선택하세요.\n- SD 카드 — 최소한 16GB, 크기가 클수록 더 많은 모델을 넣을 수 있습니다. Raspbian Bookworm이나 Ubuntu와 같은 적합한 OS가 이미 로드되어 있어야 합니다.\n- 인터넷 연결\n\n이전에 언급했듯이, Raspberry Pi에서 Ollama를 실행하는 것은 이미 하드웨어 스펙트럼의 극단에 가깝습니다. 기본적으로 Raspberry Pi보다 강력한 장치는 Linux 배포판을 실행하고 유사한 메모리 용량을 가지고 있다면, 이론적으로는 Ollama와 이 포스트에서 논의된 모델을 실행할 수 있어야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. Ollama 설치하기\n\nRaspberry Pi에 Ollama를 설치하기 위해서는 리소스를 절약하기 위해 Docker를 사용하지 않을 것입니다.\n\n터미널에서 다음 명령을 실행하세요.\n\n```js\ncurl https://ollama.ai/install.sh | sh\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 명령을 실행한 후 아래 이미지와 유사한 내용을 보게 될 것입니다.\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_2.png)\n\n출력에 따라서 Ollama가 실행 중인지 확인하려면 0.0.0.0:11434로 이동하십시오. 'WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only mode.'라는 메시지가 표시되는 것은 Raspberry Pi를 사용하고 있기 때문에 정상입니다. 그러나 NVIDIA GPU가 있는 것으로 알려진 장치에서 이 지침을 따르고 있다면, 무언가 잘못되었습니다.\n\n문제 또는 업데이트가 있는 경우 Ollama GitHub 리포지토리를 참조하십시오.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2. 명령 줄을 통한 LLM 실행\n\nOllama 공식 모델 라이브러리에서 Ollama를 사용하여 실행할 수 있는 모델 목록을 살펴보세요. 8GB 라즈베리 파이에서 7B보다 큰 모델은 맞지 않습니다. 마이크로소프트의 2.7B 크기인 Phi-2 LLM을 MIT 라이센스로 사용하겠습니다.\n\n기본 Phi-2 모델을 사용할 것이지만, 여기에서 찾을 수 있는 다른 태그 중에서 필요한 것을 자유롭게 사용해도 됩니다. Phi-2 모델 페이지를 확인하여 상호 작용하는 방법을 살펴보세요.\n\n터미널에서 다음을 실행하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n올라마 편리해요\n```\n\n아래 출력물과 유사한 것을 볼 때, 이미 라즈베리 파이에서 LLM이 작동 중임을 확인했습니다! 정말 간단해요.\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_3.png)\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_4.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기타 모델인 Mistral, Llama-2 등도 시도해 볼 수 있어요. 다만 모델 가중치를 저장할만큼 SD 카드에 충분한 공간이 있어야 해요.\n\n모델이 클수록 출력이 느려질 수 있어요. 예를 들어, Phi-2 2.7B에서는 초당 약 4토큰을 얻을 수 있어요. 하지만 Mistral 7B를 사용하면 세대 속도가 초당 약 2토큰으로 느려집니다. 토큰은 대략 한 단어와 동일한 양이에요.\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_5.png)\n\n이제 Raspberry Pi에서 LLMs가 실행되고 있지만 아직 끝나지 않았어요. 터미널이 모두에게 익숙한 것은 아니에요. Ollama 웹 UI도 실행해 보겠어요!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3. Ollama 웹 UI 설치 및 실행\n\n공식 Ollama 웹 UI GitHub 저장소의 지침을 따라 Docker 없이 설치할 것입니다. 최소한 Node.js 버전이 `= 20.10 이 되어야 한다고 권장하므로, 해당 권장사항을 따를 것입니다. 또한 Python은 적어도 3.11 이어야 한다고 권장하나, Raspbian OS에는 이미 그 버전이 설치되어 있습니다.\n\n먼저 Node.js를 설치해야 합니다. 터미널에서 다음을 실행하세요.\n\n```js\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - &&\\\nsudo apt-get install -y nodejs\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 향후 독자들을 위해 20.x를 더 적합한 버전으로 변경하려면 변경하십시오.\n\n그런 다음 아래 코드 블록을 실행하십시오.\n\n```js\ngit clone https://github.com/ollama-webui/ollama-webui.git\ncd ollama-webui/\n\n# 필요한 .env 파일 복사\ncp -RPp example.env .env\n\n# Node를 사용하여 프론트엔드 빌드\nnpm i\nnpm run build\n\n# 백엔드와 함께 프론트엔드 제공\ncd ./backend\npip install -r requirements.txt -- break-system-packages\nsh start.sh\n```\n\n이는 GitHub에서 제공된 것을 약간 수정한 것입니다. 가상 환경을 사용하거나 --break-system-packages 플래그를 사용하는 등의 최선의 방법을 따르지 않고 간단하게하고자 하였으니 유의하시기 바랍니다. uvicorn을 찾을 수 없다는 오류가 발생하면 터미널 세션을 재시작하십시오.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모든 것이 제대로 진행되면 라즈베리 파이의 8080 포트를 통해 http://0.0.0.0:8080로 Ollama 웹 UI에 액세스할 수 있거나, 동일한 네트워크에서 다른 장치를 통해 접속하고 있다면 http://라즈베리 파이의 로컬 주소:8080/을 통해 접속할 수 있습니다.\n\n![이미지 설명](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_6.png)\n\n계정을 생성하고 로그인한 후, 아래 이미지와 유사한 화면이 보여야 합니다.\n\n![이미지 설명](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_7.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모델 가중치를 이전에 다운로드했다면, 아래와 같이 드롭다운 메뉴에 표시될 것입니다. 그렇지 않은 경우 설정으로 이동하여 모델을 다운로드할 수 있습니다.\n\n![이미지 1](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_8.png)\n\n![이미지 2](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_9.png)\n\n전체적으로 사용자 인터페이스는 매우 깔끔하고 직관적이므로 설명할 것이 별로 없어요. 정말로 아주 훌륭하게 구현된 오픈 소스 프로젝트입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_10.png)\n\n## 4. Ollama 웹 UI를 통해 VLM 실행하기\n\n이 글을 시작할 때 언급했듯이, VLM도 실행할 수 있습니다. LLaVA를 실행해보겠습니다. 이것은 Ollama에서도 지원되는 인기 있는 오픈 소스 VLM입니다. 실행하려면 인터페이스를 통해 'llava'를 가져와서 가중치를 다운로드하면 됩니다.\n\n안타깝게도, LLM과는 달리 Raspberry Pi에서 이미지를 해석하는 설정에 꽤 많은 시간이 걸립니다. 아래 예시는 처리하는 데 약 6분 정도 걸렸습니다. 대부분의 시간은 아마 이미지 측면이 아직 제대로 최적화되지 않았기 때문인데, 이는 앞으로 확실히 변경될 것입니다. 토큰 생성 속도는 초당 약 2토큰입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_11.png)\n\n## 모든 것을 감싸며\n\n지금까지 이 기사의 목표를 거의 달성한 상태입니다. 요약하자면, 우리는 Ollama와 Ollama Web UI를 사용하여 Raspberry Pi에서 Phi-2, Mistral, LLaVA와 같은 LLMs와 VLMs를 실행하는 방법을 성공적으로 이끌어냈습니다.\n\nRaspberry Pi나 다른 작고 가장한 장치에서 실행되는 로컬 호스팅 LLMs에 대한 다양한 사용 사례들을 상상할 수 있습니다. 특히, 4토큰/초가 Phi-2 크기의 모델을 위해 일부 사용 사례에 대해 스트리밍 속도로는 합당해 보입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n'작은' LLM 및 VLM의 분야는 얼마 전에 다소 모순적으로 이름이 지어졌지만 '큰' 지칭을 받는 활발한 연구 분야입니다. 최근에는 꽤 많은 모델이 출시되었습니다. 이 새로운 트렌드가 계속되어 더 효율적이고 간결한 모델들이 계속 출시되기를 바랍니다! 다가오는 몇 달 동안 살펴봐야 할 분야입니다.\n\n면책 성명: 저는 Ollama나 Ollama Web UI와 어떠한 관련도 없습니다. 모든 의견과 견해는 제 개인적인 것이며 어떤 조직도 대표하지 않습니다.\n","ogImage":{"url":"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png"},"coverImage":"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png","tag":["Tech"],"readingTime":11},{"title":"라즈베리 파이를 원격으로 접속하는 방법 Tailscale을 활용한 포괄적인 가이드","description":"","date":"2024-05-17 19:30","slug":"2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale","content":"\n<img src=\"/assets/img/2024-05-17-원격으로RaspberryPi에액세스하는포괄적인가이드Tailscale을사용하여\" />\n\n현재 연결된 세상에서 장치에 원격으로 액세스하는 능력은 놀라운 융통성과 유용성을 제공합니다. 라즈베리 파이 서버를 관리하거나 파일에 액세스하거나 어디에서든 IoT 장치를 제어하려는 경우 안전한 네트워크 액세스는 중요합니다. 이 안내서는 와이어가드(WireGuard) 암호화를 활용하여 안전한 네트워크를 만드는 강력하고 안전한 서비스인 Tailscale을 사용하는 데 초점을 맞춥니다. 우리는 Raspberry Pi에 Tailscale을 설정하여 가정 네트워크 외부에서 액세스하는 단계를 안내할 것입니다.\n\n# 원격 액세스 소개\n\n원격 액세스를 통해 인터넷을 통해 다른 위치에서 Raspberry Pi에 연결할 수 있습니다. 이겢은 서버를 관리하거나 업데이트를 실행하거나 집을 벗어난 상태에서 미디어 파일에 액세스하는 데 특히 유용할 수 있습니다. 그러나 포트 전달과 같은 전통적인 방법은 네트워크를 보안 위험에 노출시킬 수 있습니다. Tailscale은 안전한 VPN을 만들어 더 안전하고 간단한 대안을 제공합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 필요한 준비물\n\n- 인터넷에 연결된 라즈베리 파이 헤드리스 시스템\n- Tailscale 계정: 개인용으로 무료로 제공됩니다.\n- SSH 접근: 라즈베리 파이에 소프트웨어를 설치하고 구성하기 위해 필요합니다.\n\n# 단계 1: 라즈베리 파이에 Tailscale 설정하기\n\n라즈베리 파이에 연결하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 아래 명령어를 사용하여 Raspberry Pi에 SSH로 연결하세요:\n\n```js\nssh pi@<IP-ADDRESS>\n```\n\n`<IP-ADDRESS>` 자리에 Raspberry Pi의 실제 IP 주소를 입력해주세요.\n\nTailscale 설치:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n라즈베리 파이에 Tailscale 패키지 소스를 추가하세요:\n\n```js\ncurl -fsSL https://pkgs.tailscale.com/stable/raspbian/buster.gpg | sudo apt-key add -\ncurl -fsSL https://pkgs.tailscale.com/stable/raspbian/buster.list | sudo tee /etc/apt/sources.list.d/tailscale.list\n```\n\n패키지 목록을 업데이트하고 Tailscale을 설치하세요:\n\n```js\nsudo apt update\nsudo apt install tailscale\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n시작 및 Tailscale 인증:\n\n- Raspberry Pi에서 Tailscale 서비스를 시작합니다:\n\n```js\nsudo tailscale up\n```\n\n터미널에서 제공된 인증 지침을 따르세요. 이는 URL을 방문하고 Tailscale 계정에 로그인하여 장치를 인증하는 과정을 포함합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 2: 안전한 액세스를 위한 Tailscale 설정\n\nRaspberry Pi의 Tailscale IP 확인:\n\n인증이 완료되면 Raspberry Pi에 Tailscale IP 주소가 할당됩니다:\n\n```js\ntailscale ip -4\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n접근을 위한 Raspberry Pi 원격 액세스입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n클라이언트 장치에 Tailscale 설치하기:\n\n- 원격 액세스에 사용할 장치(예: 노트북, 스마트폰)에 Tailscale을 다운로드하고 설치하세요. Tailscale 앱은 Windows, macOS, Linux, iOS 및 Android용으로 제공됩니다.\n\n클라이언트 장치 인증하기:\n\n- 라즈베리 파이 설정과 유사하게 클라이언트 장치에서 Tailscale을 시작하고 Tailscale 계정을 통해 인증하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nRaspberry Pi에 SSH로 연결하기:\n\n- 두 장치가 Tailscale 네트워크에 연결되면, Raspberry Pi로 SSH를 사용할 수 있습니다. Raspberry Pi의 Tailscale IP를 사용하여 다음과 같이 명령을 입력하세요:\n\n```js\nssh pi@<TAILSCALE-IP>\n```\n\n위 명령에서 `TAILSCALE-IP`를 이전에 메모한 Tailscale IP 주소로 대체해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 4: Tailscale를 사용하여 기기 관리하기\n\n- 연결된 기기 모니터링하기:\n\n- Tailscale 관리자 콘솔을 사용하여 모든 연결된 기기를 확인하고 권한을 관리할 수 있습니다.\n\n- 서브넷 설정하기 (옵션):\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 만약 라즈베리 파이의 모든 트래픽을 Tailscale 네트워크를 통해 라우팅하고 싶다면 서브넷 라우트를 설정할 수 있습니다. 이것은 Tailscale 네트워크의 다른 기기들이 액세스해야 하는 Pi 상의 서비스가 있는 경우 유용합니다.\n\n# 단계 5: 보안 강화\n\n- 정기적인 업데이트:\n\n- 라즈베리 파이와 모든 기기를 최신 Tailscale 및 OS 업데이트로 업데이트하여 보안 패치가 적용되도록 유지하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 강력한 인증 사용하기:\n\n- 라즈베리 파이와 Tailscale 계정에 강력하고 고유한 암호를 사용하세요. 추가적인 보안을 위해 이중 인증 (2FA)을 활성화하는 것을 고려해보세요.\n\n# 결론\n\n이제 라즈베리 파이에 Tailscale을 성공적으로 설정하여 전 세계 어디에서나 안전하게 액세스할 수 있습니다. Tailscale의 사용 편의성과 견고한 보안 기능은 가정 기기에 원격 액세스하기 위한 탁월한 선택지로 제공됩니다. 개인 프로젝트 또는 복잡한 IoT 시스템을 관리하든, Tailscale은 연결성과 제어를 유지하기 위한 확장 가능하고 안전한 방법을 제공합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 안내서는 원격 액세스를 위해 Tailscale을 설정하고 사용하는 방법에 대한 종합적인 소개를 제공합니다. 그러나 가능성은 여기서 끝나지 않습니다. Tailscale을 사용하면 네트워크를 확장하고 더 많은 기기를 추가하며 설정을 향상시키기 위해 자동화 스크립트를 통합할 수 있습니다. 원격 기기 관리의 세계가 여러분의 손끝에 있고, Tailscale은 이를 접근 가능하고 안전하게 만들어줍니다.\n\n## 리소스:\n","ogImage":{"url":"/assets/img/2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale_0.png"},"coverImage":"/assets/img/2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale_0.png","tag":["Tech"],"readingTime":7},{"title":"수제 가슴 냉동고 아이스 배스를 만들고 싶으세요","description":"","date":"2024-05-17 19:29","slug":"2024-05-17-SoyouwanttobuildaDIYchestfreezericebath","content":"\n<img src=\"/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png\" />\n\n안녕, 미친놈! 너무너무 미쳤구나! 이런 걸 하고 싶어하는 너, 진짜 미치기 일보 직전이구나. 하지만 사실 나도 꽤 미친 놈이야. 만나서 반가워. 얼음물 가득한 냉동고 안으로 들어갈 얘기 좀 나눠볼까?\n\n단계 1: 몇 가지 꼭 해봐야지…진짜요.\n\n야, 차가운 목욕을 하는 열 두 명한테 물어보면, 다들 자신만의 \"완벽한\" 사이즈의 냉동고를 추천해주겠지. 너도 그것이 400 리터이어야 한다. 너도 14평방 피트라면 안 되잖아... 만약 이걸 하려면, 판매점에 가서 확인해보라고. 들어가 봐. 시체도 들어갈 수 있는지 묻는 농담을 해봐. 체포당하고. 그걸 어떻게 설명할 건지 볼까...\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 진지하게 생각해보세요. 들어오세요. 편안히 앉으세요. 머리를 넣을지 고려해보세요. 움직일지 결정할지 또는 그냥 해버릴지 고려해보세요. 내가 산 식품 냉동고는 너무 작아. 작동은 되나요? 그렇죠... 근데 편안해요? 아니에요.\n\n단계 2: 확보하기\n\n이것은 가장 쉬운 단계에요. 두 가지 방법으로 할 수 있어요. 먼저 가게에 가서 사거나 Facebook Marketplace, Craigslist 또는 어디에서 중고로 구해오세요. 원하는 종류, 원하는 크기를 찾아서 이루어내세요.\n\n집에 가져와서 바닥에 어떤 종류의 패딩을 놓고 (제가 아이들용 폼 패딩을 사용했어요) 바퀴를 제거하세요. 무거운 것을 분산시키고, 물과 몸이 들어가면 더 힘들게 일해야 해요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n단계 3: 갈아내기\n\n이제 힘들어지네요. 자신을 위해 Ryobi 샌더를 구입했어요. 하지만 모든 벽을 갈아내보세요. 프라이머와 폰드 실드가 잘 부착되려면 표면이 거칠어야 해요. 또한 유해 물질을 마시지 않도록 마스크를 착용하세요.\n\n단계 4: JB 워터 웰드로 고쳐 보세요\n\n마린 실리콘이나 방수용 화이트 제품을 사용해도 좋지만, 작동하려면 오래 지속되길 원한다면 JB 워터 웰드를 사용하세요. 마침표. 이게 전체 작업에서 가장 어려운 부분이지만, 실제로 작동하기 때문에 가치가 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파란색 나일론 장갑 몇 개를 구입하세요. 100개가 들어있는 팩팅이 필요할 거에요. 그리고 욕조 가장자리를 포함하여 욕조 모서리 전체에 충분한 양의 JBWW를 밀어 넣으세요. 위의 사진을 참고하여 실제로 채워졌는지 확인해주세요. JBWW가 장갑에 달라 붙고 잘 작동하지 않는 것 같으면 새로운 장갑을 착용해주세요. 제 말 믿어요, 나중에 저에게 감사할 거예요.\n\n그리고 냉동고에 배수구가 있나요? 그것도 채워주세요. 믿어요. 처음부터 누수 방지가 되도록 하고 싶으실 거예요.\n\n모두 채워지면 며칠 동안 그대로 둬주세요.\n\n단계 5: 바탕 도료하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그 다음으로, 그 야수에 맞추어 스프레이 페인트를 뿌려보세요! 러스톨리움 제품을 사용하지 마세요. 아마도 그 제품은 실패할 거에요. 덧붙여 말하면, 다음 단계가 그것에 잘 안붙을 거에요. 두 번 읽어 보세요.\n\n저는 내 것에서 이 다음 단계를 잘못했어요, 그러니 사진이 아니라 글에 주의하세요.\n\n욕조 안쪽 전체 및 욕조 외부 입술에 이르는 길 전체에 프라이머를 바르세요. 저는 내 것에서 내부 가장자리까지만 프라이먹를 바른 거에요.\n\nStep 6: 파운드 실드를 바르세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 연못 실드가 필요합니다. 매일 몸을 담그게 될 이 물건에 아끼지 말고 투자해야 합니다. 이 물건은 유기물에 안전해야 하며, 즉, 물고기와 사람에게 안전해야 합니다. 방수가 가능한 무언가를 찾아서 대충 봉인하려고 하지 마세요. 건강을 위해서 하는 일인 만큼, 싼 길로 가려다 몸을 해치지 마세요.\n\n페인팅을 하는데 제안할 만한 점은, 붓을 사용하고 작은 구역에서 혼합하는 것입니다. 즉, 연못 실드 상자 안의 두 캔을 함께 섞지 말고, 한 번에 모두 칠하는 것을 피하세요. 화학 혼합물을 만들고, 한꺼번에 섞으면 혼합용 통에 큰 덩어리가 생길 수 있어요 (믿어요).\n\n또한 빠른 속도로 진행하세요… 먼저 1/3 정도 혼합하고, 제대로 섞이면 되도록 빨리 칠하세요. 그런 다음 두 번째로 혼합하고, 되도록 빨리 칠하세요. 마지막으로 세 번째를 혼합하고, 되도록 빨리 칠하세요. 이 물건은 빨리 굳어가기 때문에 통 안이 아니라 욕조 위에 잘 덧칠되도록 해야 합니다.\n\n저는 제 것에 알코올을 혼합하지 않았지만, 여러분이 옳다고 생각하는 대로 하세요. 저는 제 것에 한 층만 발랐는데, 정말 좋은 성과를 내고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n7단계: 기다려주세요\n\n48시간 후에 시작해도 괜찮다고 하지만, 더 안전하게 한 주 정도 기다려서 확실히 고정되게 해주세요.\n\n8단계: 채워주세요\n\n물을 반 정도 채워주세요. 그리고 기다리세요. 어디서든 누수가 있나요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아니요?\n\n괜찮아요, 잠시만 기다려주세요.\n\n![Step 9: Inkbird time](/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n물을 부어 둔 채로 하루 정도 안 눌렀나요?\n\n축하해요!\n\n켜고 시작해봐요. 인크버드를 전원에 연결하고, 가슴 냉동고를 인크버드에 연결하고 설정값을 입력해주세요. 제가 설치할 때 사용한 동영상이에요.\n\n저는 추운 것을 원하지만 얼지 않게 하려고 3도와 좌우 2도를 선택했어요. 얼릴까 봐 걱정돼요. 그러니까 냉동고가 부서지거나 파손되거나 접착제가 떨어지지 않도록 하는 게 중요해요. 오랫동안 사용하고 싶어서 말이죠. 하지만 어떻게 할지는 당신맘대로에요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n또한 내가 링크하고 있는 Inkbird 제품은 방수 센서가 장착된 업그레이드된 버전인 것으로 보입니다. 예전 버전을 구입한다면 절대 방수 처리가 되어있지 않기 때문에 꼭 주위에 JBWW를 사용해 주세요.\n\n단계 10: 몇 년 동안 계속해서 하루종일 얼음 목욕을 즐기세요!\n\n당신의 결과물도 보여주세요! 이번이 처음이라 완벽히 한 것은 아니라고 생각합니다. 몇 가지를 다르게 했으면 좋았을 것 같지만, 이 가이드를 따라하면 이루고 싶으시다면 큰 도움이 될 것입니다.\n\nwww.coldfeat.com에서 제 소식을 만나요! 당신과 연결하고 싶어요!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n곧 시작하겠습니다! 혹시 어떤 특정 언어로 프로그래밍을 하시나요? 저에게 필요한 계속되는 안내를 해주세요. 😊✨\n","ogImage":{"url":"/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png"},"coverImage":"/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png","tag":["Tech"],"readingTime":6},{"title":"자초에서 시작하는 자아 찾기","description":"","date":"2024-05-17 19:27","slug":"2024-05-17-KnowYourselfSewYourself","content":"\n## DIY: 니가 스소!\n\n![이미지](/assets/img/2024-05-17-KnowYourselfSewYourself_0.png)\n\n## 얽힌 자아의 깨달음\n\n몇 년 전의 한 업무 회의에서, 운영자들은 두 개의 이분법적 선택을 대표하는 슬라이드를 보여주었습니다. 참가자들에게 두 옵션 중 어떤 것을 선택할지 기반으로 방의 한쪽으로 이동하라고 요청했습니다. 정렬된 후, 우리는 반대편 사람을 찾아가 우리의 입장을 논의해야 합니다. 그들은 젓가락을 사용하는지 여부, 화장지를 위로 넣는지 아래로 넣는지, 제로 인박스를 유지하거나 357개의 읽지 않은 이메일이 있는지와 같은 재미있는 주제들이었습니다. 한 이미지에서는 헤드폰 코드를 꼬아 정리하여 정돈하거나 전선이 얽힌 가방에 넣는 것 중 하나를 선택하라고 했습니다. 나는 후자를 선택했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저는 \"다른 편\"에서 온 친구이자 동료와 대화를 나눌 때, 그녀가 헤드폰 코드를 꼬리 말아 정리한 후에 정리하는 것이 \"미래의 나에게 선물\"이 된다고 언급했습니다. 그 말을 듣고 나도 한번 시도해보기로 결심했지만, 겪은 고충은 결국 저는 엉킨 사람이고 그렇게 되어야 한다는 것을 더 확신시켰습니다. 또한 왜 그런지를 이해하는 데 도움이 되었습니다: 헤드폰을 깔끔하게 정리하는 것이 어렵고, 해제하는 것은 그렇지 않다는 것입니다. 한 전략은 나에게 더 많은 노력을 요구하며, 그것은 제 친구에게는 더 많은 노력을 요하는 전략이 아닙니다.\n\n저는 저번 주말, 아무런 준비 없이 내게 플리스 넥 워머를 직접 만들기로 갑자기 결심한 것을 기억했습니다.\n\n## 혼돈 가운데 창작하기\n\n저는 머리가 짧아 목이 차가워집니다. 겨울 초반에 이스티에서 넥 워머를 둘러보았지만, 스스로를 위해 무언가를 만들고 돈을 덜 쓸 수 있다고 결심했지만, 결심한 대로 행동하지 않았습니다. 당신은 그냥 스카프를 착용하면 되지 않을까 생각할 수 있겠지만(제가 많이 가지고 있습니다), 긴 끝 부분이 늘어져서 짜증이 나고, 저는 더 꼭 맞는 것을 원했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n뉴 잉글랜드에서 토요일에 발생한 얼음 폭풍으로 나는 드디어 목도리 꿈을 이룰 수 있는 완벽한 조건을 얻었어: 목이 춥고 할 일이 없었거든. 지난 해 어머니가 선물해준 스크랩 후리스 가방을 뒤져보니 충분히 크다고 생각되는 조각들을 찾을 수 있었어. 머리와 목 주위에 걸쳐보고 확인했더니 충분한 크기였어.\n\n나의 어머니는 패치워크를 하는데 정말 능숙해. 그녀는 패치워크를 선호하고 사랑하는 일로 꼽는다. 그러나 그녀는 천으로 무엇이든 만들 수 있어. 그녀는 그에 맞는 기술과 인내심을 가지고 있어. 정확하고 사려 깊고 성실히, 약간의 철저함, 그녀의 모서리는 깔끔하고, 솔기는 모조리 균일해. 그녀는 나에게도 이런 방식으로 하라고 가르쳐주었어 (그러려했지만 그렇게 되지 않았어).\n\n난 자르고 재는 것을 싫어해, 그렇게 늘 그렇었어. 어머니는 준비 단계가 마지막 작품에 반드시 영향을 미치게 된다는 이야기를 했었어 - 아이템이 맞지 않을 수도 있고, 천이 뭉치거나 간극이 생길 수도 있고, 재료를 제대로 준비하지 않으면 다른 문제가 발생할 수 있다고. 하지만 나는 뒤엉킨 헤드폰 소유자야. 나에게는 목도리 프로젝트에 두 가지 노력 방식이 있었어: 지루하고 지루하고 짜증나게 하는 일, 측정을 하고 직선으로 잘라내고 깔끔하게 모든 것을 핀으로 고정하는 일; 또는 생동감 있고 놀랍고 순조로움으로 문제를 해결하는 일. 나는 후자를 선택했어.\n\n그리고 나는 근사하게도 자주 몸 어디에 \"혼돈\"이라는 단어의 타투를 하고 싶어했었다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-17-KnowYourselfSewYourself_1.png)\n\n## 내 목워머\n\n난 대략 한 절반 정도의 주아로 불쾌한 작업을 시작했어. 그래서 시작할 때 절단 매트를 꺼내는 노력을 기울였지. 내 어렸을 적에 엄마가 나에게 회전 절단 날을 무서워하게 했어. 그래서 손가락을 봉투에 담아 응급실로 가야 할까 봐 가능한 경우에는 피하려 해. 그러나 이 경우에는 그 위험을 감수해야 했어.\n\n이전에 선택한 피슬로 머리와 목을 감싸 둘 때, 예쁜 보라색 스크랩 피슬의 한 부분이 딱 맞게 긴 것을 깨달았어. 그래서 그 영역에서 길쭉한 직사각형을 잘라내려 노력했어. 길이를 위해 기존 끝점을 사용하고 균일한 모양을 만들기 위해 노력했지. 초기 직사각형의 높이에 대해 추측을 했어. 나중에 보라색 가장자리를 안감 플리스 주위로 감을 것이기 때문에 약간 짧게 나올 거라는 것을 알고 있었기 때문이지 (“약간”에 구체적인 측정치는 없어).\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n작업을 마치고 나면, 안감 플리스에서 같은 길이의 조금 더 짧은 직사각형을 자르았어요. 이 작업 중에는 어떤 것의 폭이나 길이를 측정한 적이 없어서, 아직도 각 부분의 크기가 어떻게 되는지 잘 몰라요. 이 시점에서 나는 보라색 플리스 위에 도트무늬 플리스를 올려 위아래에 약간의 보라색 부분이 드러나도록 하고, 그런 다음 이 보라색 가장자리 폭을 접어서 고정했어요.\n\n이때, 어머니는 봤던 것을 먼저 입어보라고 했겠지만, 나는 그렇게 하지 않았어요. 혼돈스러운 수공예 정신으로, 나는 소잉 머신을 꺼내서 붐비는 식탁 위에 놓았어요. 정리되지 않은 종이들과 천 냅킨들을 한쪽으로 밀어내고 두툼한 피치도를 떼었어요. 제가 만든 직사각형의 길이를 따라 접힌 가장자리를 따라 솔직히 소었습니다. 나는 내 솔링 허용치에 특별히 신경쓰지 않았지만, 가장자리에서 멀리 떨어진 위치에 실을 유지하려고 했어요. 편하게 뽀 그나 목에 닿지 않게 마감부를 둘 수 있도록, 그리고 직선으로 유지하려고 노력했어요.\n\n다음으로, 직사각형의 끝을 겹쳐 튜브 형태로 만들고, 이를 바깥쪽을 안쪽으로 향하도록 함께 핀으로 고정했어요. 그런 다음 다시 솔잎습니다. Voilà! 그게 전부예요.\n\n![이미지](/assets/img/2024-05-17-KnowYourselfSewYourself_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n목워머를 착용해 보았을 때 부드럽고 따뜻하며 딱 맞는 직경이었지만, 원하는 것보다 조금 더 높았습니다. 당겨서 쓰면 코와 볼을 가릴 수 있어서 춥고 날씨가 추운 날에 나가기에 좋을 수도 있지만, 제가 상상했던 것과는 좀 달랐습니다. 위쪽을 몇 인치 자르고 가장자리를 마무리하는 것은 너무 어렵지 않을 것 같았지만, 성급하지 않기로 결정했습니다. 잠시 입고 다녀보고 느낌을 살펴볼 거에요.\n\n## 성찰\n\n많은 패스/불합격 과목에서 합격 성적은 대략 70% 달성하는 것과 동등하다고 합니다. 이 숫자는 내 머리 속에서 자주 나옵니다. 영국 육아 저자 사라 오크웰-스미스는 부모들이 \"시간당 70% 정도 정확하게 맞추려고 애쓰고 다른 30%에 대해 너무 걱정하지 말아야 한다\"고 말합니다. 이 말은 제 아이가 어렸을 때 저를 안심시켰습니다. 제가 평생 완벽주의자였는지 확신하지는 못하지만, 학교에서 완벽한 성적을 받는 것을 좋아했고 딱 적합한 성적으로는 실망스러워했을 것입니다. 요즘에는 모든 일(직업, 부모님 역할 등)에서 일상 목표로 70%의 기준을 갖고 있습니다.\n\n내 목워머는 쉽게 합격 성적을 받을만한 것 같아요. 만약 더 계획적이고 성실하며 집중력이 있다면 A+ 프로젝트가 되었을지도 모르겠어요. 그렇지만 아마 그렇게 마무리하지 못했을 거에요. 막 발로 진행해서 공예하는 과정에서 문제를 마주할 것으로 예상했지만, 그 정도는 아니었습니다. 비교적 쉽고 스트레스가 없었어요. 이 모든 것은 제 기대를 능가했다는 것을 의미합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n나는 어머니에게 지난 해 주머니에 대해 문자를 보냈고 내가 어떻게 만들었는지 얘기했을 때, 다음 번에 조언을 해줄 수도 있거나 적어도 나의 무법함에 잠시 웃음을 참지 않을까 싶었어요. 그러나 대신, 그녀는 내 노력에 감격했고, “바로 뛰어든 게 좋네. 시도해보기 전까지는 알 수 없으니까, 그리고 플리스는 너그러워.” 라고 써 보내주셨어요. 나는 그녀가 다르게 했을 것이라는 것을 알고 있어요 (궁극적으로 더 나은 결과를 얻을 거라는 것을 알면서도), 그리고 그녀도 내가 한 방식대로 해야 했다는 걸 알고 있어요.\n\n유니크한 창작 방법을 나누는 DIY 다이어리를 운영해 주시는\nAmanda Laughtland님에게\n항상 감사드립니다.\n","ogImage":{"url":"/assets/img/2024-05-17-KnowYourselfSewYourself_0.png"},"coverImage":"/assets/img/2024-05-17-KnowYourselfSewYourself_0.png","tag":["Tech"],"readingTime":6},{"title":"로봇에 모터 컨트롤러를 추가하는 방법","description":"","date":"2024-05-17 19:25","slug":"2024-05-17-HowToAddAMotorControllerToYourROSRobot","content":"\n로봇에 직접주행 및 오도메트리 기능을 부여하세요!\n\n![이미지](/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_0.png)\n\nROS/ROS2와 함께 사용할 모바일 로봇을 만들고 있다면, 먼저해야 할 일 중 하나는 모터 컨트롤러를 통합하는 것입니다. 모터 컨트롤러의 목적은 네비게이션 스택과 같은 상위 레벨 소프트웨어로부터 메시지를 수신하고, 이를 모터를 구동하는 신호로 변환하는 것입니다. 또한 모터의 인코더에서 정보를 받아 로봇의 속도와 위치를 계산할 수 있습니다. 추가로 배터리 전압이 변동하거나 지형이 다를 때에도 일관된 바퀴 제어를 얻을 수 있습니다. 흥미가 생겼나요? 전형적인 차이 드라이브 로봇을 위한 ROS 모터 컨트롤러를 만들기 위해 필요한 모든 것과 참고용으로 사용할 수 있는 작동하는 코드가 준비되어 있습니다!\n\n이 프로젝트의 전체 코드는 GitHub 레포지토리에 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n배경\n\n내가 전에 썼던 ROS가 유용한 이유, ROS가 하는 일, 그리고 물리적 로봇과 어떻게 통합되는지에 대한 개요에 대해 읽은 것으로 가정하겠습니다.\n\n나는 아두이노, Pi Pico 또는 Teensy와 같은 일반적인 마이크로컨트롤러에 어느 정도 익숙하다고 가정합니다. 이 경우 Teensy를 사용할 것이지만, 다양한 마이크로컨트롤러 유형에 걸쳐 개념은 거의 동일합니다.\n\n중요한 개념\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모터 컨트롤러의 역할은 전형적으로 /cmd_vel과 같은 주제에 Twist 유형의 메시지를 받는 것입니다. 이 메시지는 두 구성 요소로 이루어져 있습니다. 첫째, 미터/초로 분할된 원하는 선형 속도를 정의합니다. 두 번째로는 원하는 각속도인 라디안/초의 회전 속도를 포함합니다.\n\n예를 들어, 내비게이션 스택은 이 메시지를 보낼 수 있으며, 이 메시지는 로봇을 1m/s로 직진하면서 초당 0.1 라디안으로 회전하도록 모터 컨트롤러에 지시합니다. x 방향은 앞쪽으로, Z 방향은 지면과 직교한 상단 방향이고, y는 좌우입니다. 결과적으로 곡선 경로가 생성됩니다.\n\n```js\nlinear: x: 1.0;\ny: 0.0;\nz: 0.0;\nangular: x: 0.0;\ny: 0.0;\nz: 0.1;\n```\n\n평범한 미끄럼 방지 구동 방식의 로봇은 일반적으로 가로 이동하거나 직접 올라가는 능력이 없으므로 선형 구성 요소는 주로 x 값을 사용합니다. 마찬가지로, 이 유형의 대부분 로봇은 Z 축을 중심으로 명령된 각속도만 사용할 것입니다. 방향과 단위에 대한 ROS 규칙은 REP 103에서 찾을 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것이 지나치게 복잡해 보일 수도 있어요. 메시지 유형이 로봇이 물리적으로 따를 수 없는 구성요소를 가지는 이유가 뭘까요? 메시지 유형은 여러 유형의 로봇에서 사용될 것이기 때문이에요. 홀로노믹 드라이브 로봇은 옆으로 이동해서 미끄러짐 없이 움직일 수 있어요. 쿼드콥터 로봇은 회전하지 않고 동시에 위, 옆, 앞으로 움직일 수 있어요. 이 메시지 유형은 다양한 종류의 로봇에서 사용할 수 있도록 설계되었어요.\n\n각도 구성요소를 신경 써야 하는 이유가 뭘까요? 나중에 이것이 정말 유용해질 거에요. 직선으로 운전 중에 조금씩 벗어나기 시작할 경우에 코스 수정을 쉽게 보낼 수 있어요. 제자리에서 회전 명령을 쉽게 보낼 수 있어요. 우리가 아래에서 볼 때, 수학도 쉬워질 거에요.\n\n그래서 모터 컨트롤러는 cmd_vel에서 특정 선형 및 각속도 혼합을 주문받고, 그것이 모터 속도로 어떤 의미인지 계산해야 해요. 여기가 멋진 곳이에요. 곧 수학으로 돌아올게요 - 무서워하지 마세요. 먼저, 모터 속도를 정확히 제어하는 법에 대해 이야기해야 해요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n간단한 로봇을 만들었다면, 아마도 DC 모터의 속도를 제어할 수 있다는 개념에 익숙할 것입니다. 전압을 펄스로 제어함으로써 DC 모터의 속도를 조절할 수 있다는 아이디어에 편안해지셨을 겁니다. 전압이 켜져 있는 비율이 높을수록 모터는 최고 속도까지 빨리 회전합니다. 이를 펄스 폭 변조(Pulse Width Modulation)이라고 하며, 단순한 로봇에서도 동작하지만 문제가 있습니다. 가장 큰 문제는 모터 속도가 배터리 전압에 따라 변경된다는 것입니다. 모터가 회전하는 속도는 적용된 전압의 함수이므로 배터리가 방전될수록, 주어진 속도를 유지하기 위해 더 높은 켜는 시간 비율이 필요합니다. 또 다른 문제는 주어진 전압에 대해 한 모터가 조금이라도 다른 모터보다 빨리 회전한다면 어떻게 될까요? 양쪽에 동일한 비율을 보낸다면 로봇은 곡선 형태로 이동합니다. 또한 지형에 의해 바퀴 중 하나가 부분적으로 막혀 있다면 특정 바퀴 속도를 달성하는 데 더 많은 전력이 필요합니다. 일정한 속도를 명령하고 실제로 그 속도를 얻고 싶다면 휠로부터 피드백을 받아야 합니다. 우리는 폐쇄 루프 시스템이 필요합니다.\n\n운전 모터의 경우, 통합 자기 엔코더가 있는 모터를 사용하고 싶습니다. 홀 센서는 모터 축에있는 자석이 센서를 지날 때마다 펄스를 생성하는 데 사용됩니다. 이 펄스를 마이크로컨트롤러로 보내어 카운트하고 실제 바퀴 속도를 계산합니다. (이를 위해 기어 비율 및 기타 몇 가지 세부 정보가 필요합니다 — 나중에 자세히 살펴보겠습니다)\n\n아래 이미지에서 각 모터의 뒤쪽에 엔코더가 달려 있는 것을 볼 수 있습니다.\n\n![image](/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래서 이제 마이크로컨트롤러는 PWM 펄스폭을 변화시킴으로써 모터의 속도를 제어하고, 샤프트의 실제 속도를 읽을 수 있게 되었습니다. 이는 배터리 전압, 지형, 또는 로봇의 왼쪽 구동 바퀴에 감겨있는 골든 리트리버 털과는 독립적으로 작동합니다. 우리는 이제 폐쇄 루프를 가지고 있으며 이제 어디론가 가고 있습니다. 그래서.. 우리가 원하는 바퀴 속도를 어떻게 얻을까요?\n\nPID 제어\n\n그것이 PID 제어 루프의 역할입니다. PID 루프는 다른 사람들에 의해 더 잘 이해되고 다뤄진 기사들에서 충분히 다루어졌기 때문에, 여기서는 다시 다루지 않겠습니다. 좋은 소개는 여기에 있고, 모터 제어에 좀 더 특화된 것은 여기에 있습니다. Youtube에도 좋은 비디오들이 있습니다. PID에 대해 배운 내용을 로봇에 매핑하는 방법을 요약하면 다음과 같습니다. 각 모터마다 한 개의 루프가 필요합니다. 인코더는 펄스 형태로 피드백을 제공하며, 이를 사용하여 바퀴 RPM을 계산합니다. 설정점은 선속도와 각속도에서 계산된 바퀴 속도입니다 (곧 설명될 것). 출력은 주로 0에서 255까지 변할 수 있는 펄스폭의 백분율입니다. 몇 초에 한 번씩 PID 루프는 설정점을 실제 바퀴 속도와 비교하고, 모터의 PWM 출력을 조정하여 배터리 전압, 부하 또는 다른 변수와 관계없이 목표치에 유지합니다. 이에는 나중에 다룰 튜닝 프로세스가 필요하며, 기사의 나중 부분에서도 이에 대해 논의할 것입니다. 지금은 바퀴 속도가 어떻게 계산되는지로 넘어갑시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모터 컨트롤러는 로봇이 주어진 선속도와 각속도로 동시에 이동하도록 요청받은 메시지를 받았어요. 바퀴가 해야 할 일을 어떻게 계산하는 걸까요?\n\n여기 수학적 설명이 잘 나와 있어요. 거기서 이를 배웠어요. 이제 Twist 메시지가 선속도와 각속도를 따로 분리하는 이유가 분명해질 거예요.\n\n먼저 선속도부터 시작해봅시다. 그것은 상당히 쉬워요. 우리는 특정 바퀴의 속도를 PID 컨트롤러를 사용하여 정확하게 설정하는 방법을 알아요. 직선 운동에서 두 바퀴는 같은 속도로 회전해야 하므로, 우리는 단순히 우리가 원하는 전진 속도를 만들어내기 위한 그 바퀴 속도가 무엇인지 계산하고 두 바퀴 모두 그 속도로 설정하면 되요. 주요 구동 바퀴의 반경을 알아야 하는데, 그것으로부터 바퀴 둘레를 계산할 수 있어요. 그것이 바퀴 한 바퀴 회전마다 로봇이 이동하는 거리를 의미하죠. 그 후, 구동 바퀴의 기어 비율과 바퀴 당 엔코더 틱 수를 알아야 해요. 그럼 필요한 모터 속도를 계산할 충분한 정보가 생기게 되요.\n\n각속도를 계산하는 수학은 약간 복잡해요. 두 바퀴의 속도 차이를 도입해 로봇이 올바른 속도로 회전하도록 해야 해요. 우리는 바퀴베이스를 알아야 하는데, 이는 드라이브 바퀴 사이의 거리를 말해요. 처음에 내가 처음에 한 것처럼 각 바퀴의 안쪽 가장자리부터 측정하지 마세요. 회전 속도에 상당한 오차가 발생할 거에요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음은 코드가 모이게 됩니다:\n\n변수 data.linear.x는 요청된 속도의 x 구성 요소이고, data.angular.z는 로봇의 요청된 회전입니다. ROS 메시지가 이렇게 속도를 분리하여 제공하면 희망하는 바퀴 속도를 계산하기가 매우 간단해집니다. right_rpm과 left_rpm 변수는 각 모터에 대한 PID 루프의 세트 포인트입니다. 이것은 원하는 동작을 생성하기 위해 필요합니다.\n\n오도메트리 계산\n\n모터 속도를 올바르게 설정하는 것 외에도 모터 컨트롤러는 로봇의 위치와 방향을 데드 레커닝을 통해 추정해야 합니다. 로봇이 어디에 있고 어느 방향을 향해 있는지 추적하기 위해 모든 이동, 각도 및 선형 이동을 합산합니다. 완벽한 것은 아닙니다 — 바퀴 슬립과 누적 오차로 인해 오랜 시간과 거리에 걸쳐 부정확해지지만, 매우 유용한 도구입니다. 짧은 거리에서는 꽤 정확하며, SLAM 또는 GPS와 같은 다른 위치 결정 도구와 결합하면 개선된 전체 추정치를 얻을 수 있습니다. 위치의 다른 측정치를 결합하여 전반적인 위치 추정치를 더 정확하게 얻는 과정은 센서 퓨전의 예시이며, 일반적으로 칼만 필터를 통해 수행됩니다. 그렇다면 위치를 어떻게 계산할까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n수학 부분이 잘 다루어졌네요. 저는 Andrew Kramer의 예제 코드를 수정하여 계산한 후, 결과로부터 odometry 메시지를 채웠어요.\n\n내 로봇에서, 원래의 마이크로프로세서가 자원이 매우 부족했기 때문에, 나는 PID 루프를 마이크로컨트롤러에서만 실행하고 Raspberry Pi에서 Python 모터 컨트롤러 노드에서 odometry 계산을 수행하기로 선택했어요. 아래 코드 스니펫은 그 작동 방식을 보여줍니다.\n\nPython 노드는 원하는 바퀴 회전 속도를 마이크로컨트롤러로 보내요. 마이크로컨트롤러는 각 모터의 실제 인코더 틱 수를 매 초 20회 응답해요. 아래의 l_tick_cb() 함수는 Python 스크립트에서, 좌측 모터 인코더 틱 수가 수신될 때마다 작동하는 함수에요. 우측 모터의 콜백은 이 코드 스니펫에서 제외되었어요.\n\n실제 모터 틱 수가 수신될 때마다, 마지막 업데이트 이후에 주어진 쪽이 얼마나 이동했는지 계산되고, 로봇의 새로운 위치 추정 값이 계산돼요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n변수 theta는 로봇이 시작 방향에 대해 회전한 각도에 대한 현재 추정치입니다. 오도미트리 메시지를 완전히 채우려면 더 많은 정보가 필요합니다. 이 코드는 현재 속도를 계산하고 공분산 및 변환 필드를 채웁니다. 변환에 대해 자세한 내용은 다른 글에서 다루겠지만, 지금 당장 알아두실 점은 이 메시지에 대해 \"odom\" 및 \"base_link\" 자식 프레임으로 설정하는 것이 좋다는 것입니다. 이는 ROS 표준을 준수합니다. 곧 이에 대해 기사를 쓸 예정이지만, REP 105 및 최고 TF 설명은 이해를 시작하기에 좋은 자리입니다.\n\n오도멧리 변환은 쿼터니언으로 브로드캐스트됩니다. 함수를 사용하여 엔코더 값 (theta)으로부터 쿼터니언을 생성하며, 롤/피치는 우리의 미분 구동 로봇에 대해 제로로 가정됩니다. 이 코드는 해당 토론에서 가져온 것입니다.\n\n공분산은 주어진 측정치에 대한 예상 오차를 보고하는 메커니즘이며, 칼만 필터와 같은 센서 융합 중에 사용됩니다. 주어진 설정에 대한 계산 방법을 아직 이해하지 못하므로, 작은 미분 구동 로봇에 대해 찾은 전형적인 값들을 사용했습니다. 여기 개선할 수 있는 부분이 많이 있습니다.\n\n이 코드는 Github 리포지토리에서 마이크로컨트롤러에서 실행됩니다. 이 코드는 왼쪽 및 오른쪽 바퀴 RPM을 설정값으로 받아들이고, 모터를 펄스 폭 변조 (PWM)를 통해 제어하는 PID 루프를 실행합니다. 또한 각 바퀴의 사이클 동안 카운트된 엔코더 틱 수를 보고하여 코드 상에서 실제 바퀴 회전수를 계산합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n어떤 하드웨어가 있는지 궁금하신가요?\n\n여기 메인 구성 요소를 보여주는 블록 다이어그램이 있습니다.\n\n![하드웨어 블록 다이어그램](/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_2.png)\n\n보시다시피 Teensy는 모터 드라이버 모듈로 PWM 듀티 사이클을 출력하여 원하는 속도로 모터를 구동합니다. 모터 엔코더로부터 ticks를 받아 실제 속도를 계산하고 제어 루프를 닫습니다. 이들은 모터 PID 루프의 피드백입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 컨트롤러에서는 rosserial_arduino를 사용하여 원하는 바퀴 RPM 주제에 대해 구독하고 odometry/velocity 계산을 위해 각 바퀴의 실제 틱을 게시합니다.\n\n내 컨트롤러는 전압 분배기를 통해 팩 전압을 측정하고 전류 센서의 출력을 사용합니다.\n\n출력을 선택할 때는 모터 드라이버에 대해 PWM 출력을 사용해야 하며 엔코더 입력에 대해 일반 디지털 핀을 사용해야 합니다. Teensy에 대한 차트를 아래에서 볼 수 있습니다 — 다른 마이크로컨트롤러에 대한 유사한 차트도 있습니다.\n\n![Teensy 차트](/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPWM 출력은 모터 드라이버의 4개 입력으로 보내집니다. 내 로봇은 저전류 모터를 사용하므로 이와 같은 저렴한 LM298 듀얼 H-브리지 드라이버를 선택했습니다. 다양한 유형의 모터를 구동하는 다양한 보드가 많이 있습니다. 브러시리스 모터는 다른 유형의 컨트롤러를 사용하지만 많은 모터가 PWM 입력을 사용합니다. LM298 사용 방법에 대한 좋은 안내서가 여기 있습니다.\n\n![로봇의 전자 부품 덱](/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_4.png)\n\n여기에는 로봇의 전자 부품 덱이 있습니다. 라즈베리 파이 4 위에 앉은 쉴드 PCB에 Teensy가 보입니다.\n\n![로봇의 전자 부품 덱](/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마이크로컨트롤러 선택\n\n처음에는 Arduino Uno용으로 이 코드를 작성했었는데, 주로 RAM이 부족해지는 문제가 발생했습니다. rosserial_arduino에는 보다 강력한 마이크로컨트롤러가 강력히 권장됩니다. 이 컨트롤러의 다음 버전은 ROS2용 Pi Pico를 사용하여 micro-ros로 만들 것입니다. 원칙적으로 이 코드는 rosserial_arduino에서 지원하는 모든 프로세서 상에서 작은 수정만으로도 실행되어야 합니다.\n\nTeensy 버전은 매우 잘 실행되었고, 현재 모든 프로젝트를 ROS2로 마이그레이션 중이며, Pico가 더 저렴합니다. 작동이 준비된 경우 해당에 대해 기사를 게시할 예정입니다.\n\nPID 튜닝\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPID 루프를 조정해야 하는데, 이는 이 글의 범위를 벗어납니다. 웹 검색을 통해 표준 접근 방식을 찾을 수 있습니다. 마이크로컨트롤러 코드에는 ROS 주제 에코 명령을 통해 조정 가능한 P, I 및 D 값을 구독자로 활성화하는 모드가 포함되어 있습니다. 이는 rqt_graph를 사용하여 명령된 rpm 대 실제 rpm을 그래프로 표시하면 튜닝 속도가 크게 향상됩니다.\n\n결론\n\n이 글이 실제 작동 예제를 제시하고 모든 자원을 한 곳에 모아서 작성한 것에 도움이 되었기를 바라며, 여러분의 프로젝트에 유용한 출발점이 되었으면 좋겠습니다. 개선할 점이 있다면 알려주시고, 여러분에게 가치 있었다면 알려주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 프로젝트의 코드는 주로 저의 것이지만, 일부는 다른 소스들을 참고하거나 수정한 것입니다.\n","ogImage":{"url":"/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_0.png"},"coverImage":"/assets/img/2024-05-17-HowToAddAMotorControllerToYourROSRobot_0.png","tag":["Tech"],"readingTime":12}],"page":"114","totalPageCount":119,"totalPageGroupCount":6,"lastPageGroup":19,"currentPageGroup":5},"__N_SSG":true}