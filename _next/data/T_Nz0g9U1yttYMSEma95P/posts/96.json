{"pageProps":{"posts":[{"title":"LLM을 사용하여 데이터베이스를 쿼리하는 동안 RAG를 사용하는 데 마주하는 주요 4가지 문제 및 해결 방법","description":"","date":"2024-05-18 18:18","slug":"2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit","content":"\nThe Advent of LLMs shows the ability of machines to comprehend natural language. These capabilities have helped engineers to do a lot of amazing things, such as writing code documentation and code reviews, and one of the most common use cases is code generation; GitHub copilot has shown the capability of AI to comprehend engineers’ intention for code generation, such as Python, Javascript, and SQL, though LLM’s comprehension AI could understand what we want to do and generate code accordingly.\n\n# Using LLM to solve Text-to-SQL\n\nBased on the code generation capability of LLMs, many people have started considering using LLMs to solve the long-term hurdle of using natural language to retrieve data from databases, sometimes called “Text-to-SQL.” The idea of “Text-to-SQL” is not new; after the presence of “Retrieval Augmented Generation (RAG)” and the latest LLM models breakthrough, Text-to-SQL has a new opportunity to leverage LLM comprehension with RAG techniques to understand internal data and knowledge.\n\n![Top 4 Challenges using RAG with LLMs to Query Database Text-to-SQL and how to solve it](/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# RAG를 사용한 텍스트-SQL의 도전 과제\n\n텍스트-SQL 시나리오에서 사용자는 LLM이 생성한 결과를 신뢰하기 위해 정밀도, 보안 및 안정성을 갖추어야합니다. 그러나 실행 가능하고 정확하며 보안이 제어된 텍스트-SQL 솔루션을 추구하는 것은 간단하지 않습니다. 여기에서는 자연어를 통해 데이터베이스를 쿼리하기 위해 RAG를 사용한 LLM 사용의 네 가지 주요 기술적 도전 과제를 요약해보았습니다: 컨텍스트 수집, 검색, SQL 생성 및 협업.\n\n![이미지](/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_1.png)\n\n## 도전 과제 1: 컨텍스트 수집 도전과제\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다양한 원본 간 상호 운용성: 다양한 소스, 메타데이터 서비스 및 API 간에 원활하게 검색 및 통합된 정보를 일반화하고 표준화하는 것이 중요합니다.\n- 데이터와 메타데이터의 복잡한 링킹: 이는 데이터를 해당 문서 저장소의 메타데이터와 연결하는 것을 포함합니다. 관련성, 계산 및 집계와 같은 메타데이터, 스키마 및 컨텍스트를 저장하는 것이 포함됩니다.\n\n## 도전 과제 2: 검색 도전과제\n\n- 벡터 저장소의 최적화: 인덱싱 및 청킹과 같은 벡터 저장소를 최적화하기 위한 기술을 개발하고 구현하는 것은 검색 효율성과 정확도 향상에 중요합니다.\n- 의미 검색의 정확도: 도전 과제는 질의 이해의 뉘앙스에 있으며 이는 결과의 정확도에 중대한 영향을 미칠 수 있습니다. 이는 일반적으로 쿼리 재작성, 다시 순위 지정 등과 같은 기술을 포함합니다.\n\n## 도전 과제 3: SQL 생성 도전과제\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- SQL 쿼리의 정확성 및 실행 가능성: 정확하고 실행 가능한 SQL 쿼리를 생성하는 것은 상당한 도전입니다. 이를 위해서는 LLM이 SQL 구문, 데이터베이스 스키마, 그리고 다양한 데이터베이스 시스템의 특정 방언에 대한 깊은 이해가 필요합니다.\n- 쿼리 엔진 방언 적응: 데이터베이스는 종종 SQL 구현에서 고유한 방언과 뉘앙스를 가집니다. 이러한 차이에 적응하고 다양한 시스템 간에 호환되는 쿼리를 생성할 수 있는 LLM을 설계하는 것은 도전의 복잡도를 더 높이는 요소입니다.\n\n## 도전 4: 협업 도전\n\n- 집단 지식 축적: 도전은 다양한 사용자 그룹으로부터 수집된 집단적인 통찰과 피드백을 효과적으로 수집, 통합, 그리고 활용하여 LLM이 검색하는 데이터의 정확성과 관련성을 향상하는 메커니즘을 만드는 데에 있습니다.\n- 접근 제어: 데이터를 검색하는 것에 대한 다음으로 중요한 도전은 존재하는 조직 데이터 접근 정책 및 개인정보 보호 규정이 새로운 LLM 및 RAG 아키텍처에도 적용되도록 보장하는 것입니다.\n\n더 많은 정보를 원하시나요? 각 도전에 대해 미래 게시물에서 자세히 공유할 계획입니다. 알림을 받으려면 Medium에서 팔로우해주세요!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 어떻게 문제를 해결할 수 있을까요? LLM을 위한 의미론적 레이어.\n\n위의 과제들을 해결하기 위해서, 우리는 LLM과 데이터 소스 사이에 레이어가 필요합니다. 이 레이어를 통해 LLM이 비즈니스 의미론과 메타데이터를 데이터 소스로부터 학습할 수 있게 되며, 이 레이어는 종종 \"의미론적 레이어\"라고 불리는 것이 필요합니다. 의미론적 레이어는 의미론과 데이터 구조 간의 연결을 해결하고, 액세스 제어와 식별 관리를 조정하여 정확한 사용자만이 정확한 데이터에 액세스하도록 보장해야 합니다.\n\nLLM을 위한 의미론적 레이어에는 무엇이 포함되어야 할까요? 여기서 몇 가지 측면으로 일반화해봅시다.\n\n## 데이터 해석 및 표현\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 비즈니스 용어 및 개념: 시맨틱 레이어는 비즈니스 용어와 개념의 정의를 포함합니다. 예를 들어, \"수익\"과 같은 용어는 시맨틱 레이어에 정의되어 있어서 비즈니스 사용자가 BI 도구에서 \"수익\"을 조회할 때 시스템이 어떤 데이터를 검색하고 어떻게 계산할지 정확히 알고 있습니다.\n\n- 데이터 관계: 이것은 서로 다른 데이터 엔티티 간의 관계를 정의합니다. 예를 들어, 고객 데이터가 판매 데이터와 어떻게 관련되는지 또는 제품 데이터가 재고 데이터와 연결되는 방법 등이 있습니다. 이러한 관계는 복잡한 분석을 수행하고 통찰을 얻는 데 중요합니다.\n\n- 계산 및 집계: 시맨틱 레이어에는 종종 미리 정의된 계산 및 집계 규칙이 포함됩니다. 이는 사용자가 예를 들어 금년 매출을 계산하기 위해 복잡한 수식을 작성하는 방법을 알 필요가 없다는 것을 의미합니다. 시맨틱 레이어는 내부 데이터 원본을 기반으로 이러한 작업을 정의 및 규칙에 따라 처리합니다.\n\n## 데이터 액세스 및 보안\n\n- 보안 및 액세스 제어: 이것은 누가 어떤 데이터에 액세스할 수 있는지를 관리할 수도 있습니다. 사용자가 액세스 권한을 부여받은 데이터만 볼 수 있고 분석할 수 있도록 보장하여 데이터 프라이버시를 유지하고 규정을 준수하는 데 중요합니다.\n\n## 데이터 구조 및 조직\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 소스 매핑: 시맨틱 레이어는 비즈니스 용어와 개념을 실제 데이터 소스에 매핑합니다. 이는 각 비즈니스 용어에 해당하는 데이터베이스 테이블과 열을 지정하고, BI 도구가 올바른 데이터를 검색할 수 있도록 합니다.\n- 다차원 모델: 일부 BI 시스템에서 시맨틱 레이어에는 다차원 모델(예: OLAP 큐브)이 포함되어 복잡한 분석과 데이터 슬라이싱/다이싱이 가능합니다. 이러한 모델은 사용자가 쉽게 탐색하고 분석할 수 있는 차원과 측정 값을 구성합니다.\n\n## 메타데이터\n\n- 메타데이터 관리: 메타데이터를 관리합니다. 이는 데이터에 대한 데이터로서, 데이터 원본, 변환, 데이터 계보 등 데이터를 이해하는 데 도움이 되는 모든 정보가 포함됩니다.\n\n# WrenAI 소개\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_2.png)\n\nWrenAI는 오픈 소스입니다. 데이터, LLM API 및 환경 어디에서든 WrenAI를 배포할 수 있습니다. 직관적인 온보딩 및 사용자 인터페이스가 함께 제공되어 몇 분 안에 데이터소스에서 데이터 모델을 연결하고 구축할 수 있습니다.\n\nWrenAI의 하부에는 이전 섹션에서 언급한 LLM을 위한 \"Wren Engine\"이라는 프레임워크를 개발했습니다. Wren Engine은 GitHub에서도 오픈 소스로 제공됩니다. Wren Engine에 관심이 있다면 댓글을 남겨주시기 바랍니다. 앞으로 나올 글에서 아키텍처와 디자인에 대해 더 자세히 공유할 계획입니다.\n\n## WrenAI에서의 모델링\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터 소스와 연결이 완료되면 자동으로 모든 메타데이터를 수집하며 WrenAI UI를 통해 비즈니스 의미론과 관계를 추가할 수 있습니다. 미래의 의미론적 검색을 위해 자동으로 벡터 저장소를 업데이트할 것입니다.\n\n![이미지](/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_3.png)\n\n## 질문하고 따라가기\n\n모델링을 마치고 나면 비즈니스 질문을 시작할 수 있습니다. WrenAI는 가장 관련성 높은 결과 3개를 찾아 제공할 것입니다. 옵션 중 하나를 선택하면 해당 데이터의 출처 및 요약을 단계별 설명으로 제공해 드립니다. 이를 통해 WrenAI가 제안하는 결과를 더 자신 있게 사용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nWrenAI로부터 결과를 받으면 반환된 결과를 기반으로 깊은 통찰이나 분석을 위한 후속 질문을 할 수 있습니다.\n\n![image](/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_4.png)\n\n## 지금 GitHub에서 WrenAI를 사용해보고 커뮤니티에 참여해보세요!\n\n👉 GitHub: https://github.com/Canner/WrenAI\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n👉 디스코드: https://discord.gg/5DvshJqG8Z\n","ogImage":{"url":"/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_0.png"},"coverImage":"/assets/img/2024-05-18-Top4ChallengesusingRAGwithLLMstoQueryDatabaseText-to-SQLandhowtosolveit_0.png","tag":["Tech"],"readingTime":8},{"title":"장소 LLM 통찰 구조화 및 비구조화 데이터 분석을 위한 BigQuery, Gemini","description":"","date":"2024-05-18 18:15","slug":"2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics","content":"\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nBigQuery 워크로드 내에서 Gemini 1.0 Pro (텍스트 전용) 및 Gemini 1.0 Pro Vision (멀티모달) 두 가지 LLM 모델을 통합하는 흥미로운 기술을 시연하겠습니다. 이를 통해 Low-code 생성적 인사이트 생성 경험을 제공할 수 있습니다. BigQuery에서 원격 모델 엔드포인트로 지원되는 모델인 Gemini 1.0 Pro와 같이, 데이터베이스 쿼리 내에서 모델을 호출하기 위해 ML.GENERATE_TEXT 구조를 직접 사용할 수 있습니다. 기본적으로 원격 모델로 사용할 수 없거나 생성적 AI 호출에 더 많은 사용자 정의가 필요한 경우 (또는 데이터베이스 내에서 원격으로 액세스하려는 API가 있는 경우), REMOTE FUNCTIONS 접근 방식을 사용할 수 있습니다. 두 시나리오를 모두 다루기 위해 블로그 글을 2개의 섹션으로 나눠서 설명하겠습니다:\n\n## #1 원격 모델 호출:\n\n- 이 섹션은 SELECT 쿼리에서 ML.GENERATE_TEXT를 사용하여 BigQuery 내에서 Gemini 1.0 Pro를 호출하는 방법을 안내합니다.\n- 모델이 이미 BigQuery의 원격 모델로 사용 가능하고 기본 제공으로 사용하려는 경우에 이 접근 방법을 사용할 수 있습니다. 사용하려는 모델의 상태를 이 설명서에서 확인할 수 있습니다.\n- 안내 사례:\n\n인터넷 아카이브 책 데이터셋(공개적으로 BigQuery에서 사용 가능)에 대한 위치 요약기를 구축하며, BigQuery에서 Gemini 1.0 Pro의 원격 모델을 ML.GENERATE_TEXT 구조를 통해 호출하는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_1.png\" />\n\n## #2 원격 함수 구현:\n\n- 이 섹션에서는 Gemini 1.0 Pro Vision을 구현한 클라우드 함수를 호출하는 방법에 대해 안내합니다. 이 클라우드 함수는 BigQuery에서 원격 함수로 노출됩니다.\n- 사용하려는 모델이 원격 모델로 제공되지 않거나 사용 사례에서 더 많은 유연성 및 사용자 정의가 필요한 경우 이 접근 방식을 사용하십시오.\n- 안내용 사용 사례:\n\n기준 이미지와 테스트 이미지를 비교하는 이미지 유효성 검사기를 구축합니다. 이를 위해 외부 테이블에 테스트 이미지 스샷을 포함하는 데이터 세트를 만들고 Gemini 1.0 Pro Vision에 대해 확인하도록 요청합니다. 이를 위해 Gemini Pro Vision 호출을 구현한 Java 클라우드 함수를 만들고 이를 BigQuery에서 원격 함수로 호출합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_2.png\" />\n\n# BigQuery\n\nBigQuery은 서버리스, 멀티 클라우드 데이터 웨어하우스로, 바이트부터 페타바이트까지 최소한의 운영 오버헤드로 확장이 가능합니다. 이것은 ML 트레이닝 데이터를 저장하기에 좋은 선택지가 됩니다. 내장된 BigQuery Machine Learning (BQML)과 분석 기능을 통해 SQL 쿼리만 사용하여 노코드 예측을 생성할 수 있습니다. 게다가, 페더레이티드 쿼리로 외부 소스에서 데이터에 접근할 수 있어 복잡한 ETL 파이프라인이 필요하지 않습니다. BigQuery가 제공하는 모든 것에 대해 BigQuery 페이지에서 자세히 읽어볼 수 있습니다. 우리는 텍스트 요약 사례에 사용되는 원격 모델을 호출하기 위해 BigQuery ML의 ML.GENERATE_TEXT 구조를 사용할 것입니다.\n\n우리는 BigQuery를 구조적 및 반구조적 데이터를 분석하는 데 도움이 되는 완전 관리형 클라우드 데이터 웨어하우스로 알고 왔습니다. BigQuery는 비정형 데이터에서 모든 분석 및 ML을 수행할 수 있도록 확장되었습니다. 우리는 이미지 데이터를 저장하기 위해 객체 테이블을 사용할 것이며, Gemini Pro Vision 모델을 사용하여 이미지 유효성을 검증하는 원격 기능 사례에 필요한 데이터를 저장할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 데모\n\n이 블로그의 나머지 부분에서는 위에서 설명한 유즈 케이스 섹션에 자세히 기술된 실제 예제로 두 가지 유즈 케이스를 모두 시연하겠습니다. 유즈 케이스별 구현에 들어가기 전에 두 가지 유즈 케이스에 필요한 사전 설정 및 공통 단계를 완료해 봅시다.\n\n# 설정\n\n- Google Cloud Console에서 프로젝트 선택기 페이지에서 Google Cloud 프로젝트를 선택하거나 만듭니다.\n- 클라우드 프로젝트에 청구가 활성화되어 있는지 확인하십시오. 프로젝트에 청구가 활성화되어 있는지 확인하는 방법을 알아보세요.\n- Google Cloud에서 미리 로드된 bq를 실행하는 명령줄 환경인 Cloud Shell을 사용할 것입니다. Cloud 콘솔에서 오른쪽 상단의 'Cloud Shell 활성화'를 클릭하세요.\n- 애플리케이션 구축 및 제공을 위한 지원을 위해서, Duet AI를 활성화해 봅시다. Duet AI Marketplace로 이동하여 API를 활성화하세요. 또는 Cloud Shell 터미널에서 다음 명령을 실행할 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ngcloud services enable cloudaicompanion.googleapis.com –project PROJECT_ID\n```\n\n5. 이미 하지 않았다면, 이 구현을 위해 필요한 API를 활성화하세요.\n\nBigQuery, BigQuery Connection, Vertex AI, Cloud Storage APIs\n\ngcloud 명령어 대신 이 링크를 사용하여 콘솔을 통해 진행할 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# BigQuery Dataset과 외부 연결 생성하기\n\nBigQuery 데이터셋은 애플리케이션의 모든 테이블과 객체를 포함하는 컨테이너입니다. BigQuery 연결은 Cloud Function과 상호작용하는 데 사용됩니다. 원격 함수를 생성하려면 BigQuery 연결을 만들어야 합니다. 데이터셋과 연결을 생성하는 방법을 알아보겠습니다.\n\n- Google Cloud Console에서 BigQuery 페이지로 이동한 후 프로젝트 ID 옆에 있는 3개 수직 점 아이콘을 클릭하세요. 나타나는 옵션 중에서 “데이터 집합 만들기”를 선택하세요.\n- “데이터 집합 만들기” 팝업에서 아래와 같이 데이터 집합 ID를 “gemini_bq_fn”로 입력하고 지역 값을 기본 값인 “US (다중 지역…)”으로 설정하세요.\n\n![이미지](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. BigLake Connection을 사용하면 외부 데이터 원본에 연결할 수 있으면서 세밀한 BigQuery 액세스 제어와 보안을 유지할 수 있습니다. 우리의 경우에는 Vertex AI Gemini Pro API를 사용합니다. 우리는 이 연결을 사용하여 Cloud Function을 통해 BigQuery의 모델에 액세스할 것입니다. 아래 단계를 따라 BigLake Connection을 만들어보세요:\n\na. BigQuery 페이지의 탐색기 창에서 ADD를 클릭하세요:\n\n![이미지](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_4.png)\n\nb. 소스 페이지에서 외부 데이터 원본에 대한 연결을 클릭하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nc. 팝업창에 아래 외부 데이터 원본 세부정보를 입력하고 CREATE CONNECTION을 클릭하세요:\n\n![이미지](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_5.png)\n\nd. 연결이 생성되면, 연결 구성 페이지로 이동하여 액세스 권한 부여를 위한 서비스 계정 ID를 복사하세요:\n\n![이미지](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\ne. IAM 및 관리 페이지를 열고 액세스 부여를 클릭한 후 새 주체 탭에 서비스 계정 ID를 입력하고 아래에 표시된 역할을 선택한 다음 저장을 클릭하세요.\n\n![그림](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_7.png)\n\n# Use case #1 Remote Model Invocation\n\n여기서는 Vertex AI Gemini Pro foundation 모델을 기반으로 BigQuery에 모델을 만들 것입니다. 이미 데이터 세트와 연결 설정이 완료되었습니다. 이제 3단계만으로 Gemini Pro 모델의 원격 모델 호출을 시연합니다. SQL 쿼리만 사용하여 LLM 애플리케이션이 가동됩니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 테이블 및 모델 생성\n\n인터넷 아카이브 도서 데이터셋을 예시로 들어서 BigQuery에서 공개로 사용할 수 있도록 소스로 가져왔다고 가정해봅시다.\n\n## BigQuery 테이블 생성\n\n위의 예제로부터 공개적으로 이용 가능한 BigQuery 데이터셋에서 약 50개의 레코드를 보유할 수 있는 테이블을 생성해봅시다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nBigQuery SQL 에디터 창에서 다음과 같이 DDL (데이터 정의 언어) 문을 실행해보세요:\n\n```sql\ncreate or replace table gemini_bq_fn.books as (\nselect *\nfrom\nbigquery-public-data.gdelt_internetarchivebooks.1905 limit 50);\n```\n\n이 쿼리는 이전에 생성한 데이터셋에 \"books\" 라는 새로운 테이블을 생성합니다.\n\n## BigQuery 모델 생성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모델을 생성하려면 BigQuery SQL 편집기 창에서 다음 DDL을 실행하세요:\n\n```js\nCREATE MODEL `gemini_bq_fn.gemini_remote_model`\nREMOTE WITH CONNECTION `us.gemini-bq-conn`\nOPTIONS(ENDPOINT = 'gemini-pro');\n```\n\n모델이 생성되었음을 확인하고 방금 생성된 모델을 볼 수 있는 옵션이 제공됩니다.\n\n## 새로운 생성 AI 애플리케이션을 테스트해보세요!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그것이에요! 이제 ML.GENERATE_TEXT 문을 사용하여 새로 생성한 생성 모델을 테스트해 보겠습니다.\n\n```js\nSELECT ml_generate_text_llm_result as Gemini_Response, prompt as Prompt\nFROM ML.GENERATE_TEXT(MODEL `gemini_bq_fn.gemini_remote_model`,\n  (select '텍스트 요약기와 표준화기를 당신은 개발했어요. 주소 정보를 포함한 다음 텍스트에서 표준화하고 하나의 표준화된, 통합된 주소를 출력해야 합니다. 빈 값으로 반환해서는 안 됩니다. 왜냐하면 이 필드의 텍스트에서 합리적인 데이터를 가져오는 방법을 알기 때문이에요: ' ||\nsubstring(locations, 0, 200) as prompt\nfrom `gemini_bq_fn.books`),\nSTRUCT(\n  TRUE AS flatten_json_output));\n```\n\n다음 결과가 표시되어야 합니다:\n\n<img src=\"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_8.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우와! 이렇게 쉽게 BigQuery ML에서 데이터베이스의 원격 모델을 사용할 수 있어요.\n\n이제 다른 Vertex AI 모델을 사용해 빅쿼리 원격 함수를 시도해봅시다. 예를 들어, 빅쿼리에서 원격으로 모델을 사용하는 방법을 더 맞춤화하고 유연하게 사용하고 싶다고 가정해봅시다. 현재 지원되는 모델은 이 문서에서 참조할 수 있어요.\n\n# 사용 사례 #2 원격 함수 구현\n\n여기서는 Gemini 1.0 Pro Vision foundation 모델을 구현하는 Java Cloud Function을 기반으로 빅쿼리에서 함수를 생성할 거에요. 먼저 Gemini 1.0 Pro Vision 모델을 사용해 이미지를 비교하기 위해 Java Cloud Function을 생성하고 배포하고, 그 다음에는 빅쿼리에서 배포된 Cloud Function을 호출하는 원격 함수를 생성할 거에요. 기억해 주세요, 빅쿼리에서의 원격 함수 실행에 대해 동일한 절차를 따를 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Java 클라우드 함수 만들기\n\nGen 2 클라우드 함수를 Java로 생성하여 외부 테이블에 저장된 베이스라인 이미지와 테스트 이미지를 비교하는 기능을 구축할 것입니다. 이 작업은 BigQuery의 테스트 이미지 스크린샷이 포함된 데이터셋을 사용하며 Gemini Pro Vision 모델 (Java SDK)을 이용하여 REST 엔드포인트에 배포됩니다.\n\n# Java 클라우드 함수\n\n- Cloud Shell 터미널을 열고 루트 디렉토리나 기본 작업 공간 경로로 이동합니다.\n- 상태 표시줄의 왼쪽 하단에 있는 Cloud Code 로그인 아이콘을 클릭하고 Cloud Functions을 생성할 Google Cloud 프로젝트를 선택합니다.\n- 다시 아이콘을 클릭하고 이번에는 새 응용 프로그램을 만드는 옵션을 선택합니다.\n- \"새 응용 프로그램 생성\" 팝업에서 Cloud Functions 응용 프로그램을 선택합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image1](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_9.png)\n\n5. Select the \"Java: Hello World\" option from the next pop-up:\n\n![image2](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_10.png)\n\n6. Provide a name for the project in the project path. In this case, it is \"Gemini-BQ-Function\".\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n7. 새로운 Cloud Shell Editor 보기에서 프로젝트 구조가 열린 것을 확인해야합니다:\n\n![이미지](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_11.png)\n\n8. 이제 pom.xml 파일의 `dependencies`...`/dependencies` 태그 안에 필요한 종속성을 추가해주세요.\n\n```xml\n<dependency>\n      <groupId>com.google.cloud</groupId>\n      <artifactId>google-cloud-vertexai</artifactId>\n      <version>0.1.0</version>\n   </dependency>\n\n     <dependency>\n      <groupId>com.google.code.gson</groupId>\n      <artifactId>gson</artifactId>\n      <version>2.10</version>\n     </dependency>\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n9. \"HelloWorld.java\" 클래스의 이름을 더 의미 있는 이름인 \"GeminiBigQueryFunction.java\"로 변경하세요. 클래스 이름을 이에 맞게 변경해야 합니다.\n\n10. 아래 코드를 복사하고 파일 \"GeminiBigQueryFunction.Java\"의 플레이스홀더 코드를 대체하세요. Github 레포지토리에서 소스를 참조해주세요.\n\n```js\npackage cloudcode.helloworld;\nimport java.io.BufferedWriter;\nimport com.google.cloud.functions.HttpFunction;\nimport com.google.cloud.functions.HttpRequest;\nimport com.google.cloud.functions.HttpResponse;\nimport com.google.cloud.vertexai.VertexAI;\nimport com.google.cloud.vertexai.api.Blob;\nimport com.google.cloud.vertexai.api.Content;\nimport com.google.cloud.vertexai.generativeai.preview.ContentMaker;\nimport com.google.cloud.vertexai.api.GenerateContentResponse;\nimport com.google.cloud.vertexai.api.GenerationConfig;\nimport com.google.cloud.vertexai.api.Part;\nimport com.google.cloud.vertexai.generativeai.preview.PartMaker;\nimport com.google.cloud.vertexai.generativeai.preview.GenerativeModel;\nimport com.google.cloud.vertexai.generativeai.preview.ResponseStream;\nimport com.google.cloud.vertexai.generativeai.preview.ResponseHandler;\nimport com.google.protobuf.ByteString;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Base64;\nimport java.util.List;\nimport java.util.Arrays;\nimport java.util.Map;\nimport java.util.LinkedHashMap;\nimport com.google.gson.Gson;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonArray;\nimport java.util.stream.Collectors;\nimport java.lang.reflect.Type;\nimport com.google.gson.reflect.TypeToken;\nimport java.io.ByteArrayOutputStream;\nimport java.io.InputStream;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\n\n\npublic class GeminiBigQueryFunction implements HttpFunction {\n    private static final Gson gson = new Gson();\n\n\n  public void service(final HttpRequest request, final HttpResponse response) throws Exception {\n    final BufferedWriter writer = response.getWriter();\n   // 요청 본문을 JSON 객체로 가져옵니다.\n    JsonObject requestJson = new Gson().fromJson(request.getReader(), JsonObject.class);\n    JsonArray calls_array = requestJson.getAsJsonArray(\"calls\");\n    JsonArray calls = (JsonArray) calls_array.get(0);\n    String baseline_url = calls.get(0).toString().replace(\"\\\"\", \"\");\n    String test_url = calls.get(1).toString().replace(\"\\\"\", \"\");\n    String prompt_string = calls.get(2).toString().replace(\"\\\"\", \"\");\n    String raw_result = validate(baseline_url, test_url, prompt_string);\n    raw_result = raw_result.replace(\"\\n\",\"\");\n    String trimmed = raw_result.trim();\n    List<String> result_list = Arrays.asList(trimmed);\n    Map<String, List<String>> stringMap = new LinkedHashMap<>();\n    stringMap.put(\"replies\", result_list);\n    // 직렬화\n    String return_value = gson.toJson(stringMap);\n    writer.write(return_value);\n  }\n\n\npublic String validate(String baseline_url, String test_url, String prompt_string) throws IOException{\n  String res = \"\";\n    try (VertexAI vertexAi = new VertexAI(\"YOUR_PROJECT\", \"us-central1\"); ) {\n      GenerationConfig generationConfig =\n          GenerationConfig.newBuilder()\n              .setMaxOutputTokens(2048)\n              .setTemperature(0.4F)\n              .setTopK(32)\n              .setTopP(1)\n              .build();\n    GenerativeModel model = new GenerativeModel(\"gemini-pro-vision\", generationConfig, vertexAi);\n    String context = prompt_string;\n    Content content = ContentMaker.fromMultiModalData(\n     context,\n     PartMaker.fromMimeTypeAndData(\"image/png\", readImageFile(baseline_url)),\n     PartMaker.fromMimeTypeAndData(\"image/png\", readImageFile(test_url))\n    );\n    GenerateContentResponse response = model.generateContent(content);\n     res = ResponseHandler.getText(response);\n  }catch(Exception e){\n    System.out.println(e);\n  }\n  return res;\n}\n\n\n  // 지정된 URL의 이미지 데이터를 읽어옵니다.\n  public static byte[] readImageFile(String url) throws IOException {\n    URL urlObj = new URL(url);\n    HttpURLConnection connection = (HttpURLConnection) urlObj.openConnection();\n    connection.setRequestMethod(\"GET\");\n    int responseCode = connection.getResponseCode();\n    if (responseCode == HttpURLConnection.HTTP_OK) {\n      InputStream inputStream = connection.getInputStream();\n      ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n      byte[] buffer = new byte[1024];\n      int bytesRead;\n      while ((bytesRead = inputStream.read(buffer)) != -1) {\n        outputStream.write(buffer, 0, bytesRead);\n      }\n      return outputStream.toByteArray();\n    } else {\n      throw new RuntimeException(\"Error fetching file: \" + responseCode);\n    }\n  }\n}\n```\n\n11. 이제 Cloud Shell 터미널로 이동하여 아래 명령을 실행하여 클라우드 함수를 빌드하고 배포하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ngcloud functions deploy gemini-bq-fn --runtime java17 --trigger-http --entry-point cloudcode.helloworld.GeminiBigQueryFunction --allow-unauthenticated\n```\n\n여기에 결과는 아래와 같은 형식으로 REST URL이 생성됩니다:\n\nhttps://us-central1-YOUR_PROJECT_ID.cloudfunctions.net/gemini-bq-fn\n\n12. 터미널에서 다음 명령을 실행하여 이 클라우드 함수를 테스트해보세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ngcloud functions call gemini-bq-fn --region=us-central1 --gen2 --data '{\"calls\":[[\"https://storage.googleapis.com/img_public_test/image_validator/baseline/1.JPG\", \"https://storage.googleapis.com/img_public_test/image_validator/test/2.JPG\", \"PROMPT_ABOUT_THE_IMAGES_TO_GEMINI\"]]}'\n```\n\n임의의 샘플 프롬프트에 대한 응답:\n\n<img src=\"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_12.png\" />\n\n제네릭 Cloud Function을 사용하여 Gemini Pro Vision 모델 구현이 준비되었습니다. 이제 이 엔드포인트를 직접 BigQuery 원격 함수 내에서 BigQuery 데이터에 사용하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 빅쿼리 오브젝트 테이블 및 원격 함수 만들기\n\n이 데모 애플리케이션에서는 클라우드 스토리지 버킷을 생성해 보겠습니다:\n\n- 클라우드 스토리지 콘솔로 이동하여 생성 버튼을 클릭하여 버킷을 만듭니다.\n- 버킷에 이름을 제공하고 \"demo-bq-gemini-public\"과 같은 이름을 지정한 다음 \"이 버킷에서의 공개 액세스 방지 강화\" 옵션의 선택 해제(공개로 유지)를 기억하세요. 이 데모에서는 이 버킷을 공개 액세스로 설정하고 있지만, 권장하는 방법은 공개 액세스를 방지하고 필요에 따라 특정 서비스 계정에 권한을 부여하는 것입니다.\n- 방금 만든 클라우드 스토리지 버킷의 PERMISSION 탭에서 권한 설정을 보고 변경할 수 있습니다. 원칙을 추가하려면 VIEW BY PRINCIPALS 탭 아래의 GRANT ACCESS를 클릭하고 (특정 계정을 위한) 서비스 계정 ID를 입력하거나 \"allUsers\" (공개 액세스에 대한)를 입력한 후 역할을 \"Storage Object Viewer\"로 설정하고 저장을 클릭합니다.\n- 이제 버킷이 생성되었으므로 OBJECTS 탭으로 이동하여 이미지를 업로드하고 UPLOAD FILES를 클릭하여 업로드하세요.\n- 비교하기 위해 기준 및 테스트 이미지를 업로드하세요.\n\n이 데모를 위해 3개의 객체를 생성하고 기준이고 test1 및 test2를 공개로 사용할 수 있도록 만들었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# BigQuery 객체 테이블 생성\n\nBigQuery에서 외부 객체 테이블을 만들어 생성한 연결 및 데이터셋을 사용하여 버킷의 비구조화된 데이터에 액세스할 수 있습니다. BigQuery 쿼리 에디터 창에서 다음과 같은 DDL(데이터 정의 언어) 문을 실행하세요:\n\n```js\nCREATE OR REPLACE EXTERNAL TABLE `gemini_bq_fn.image_validation`\nWITH CONNECTION `us.gemini-bq-conn`\nOPTIONS(object_metadata=\"SIMPLE\", uris=[\"gs://demo-bq-gemini-public/*.JPG\"]);\n```\n\n이 쿼리는 이전에 만든 데이터셋에 \"image_validation\"이라는 새 객체 테이블을 생성해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# BigQuery 원격 함수 생성\n\nBigQuery에서 Java Cloud Function을 호출하는 원격 함수를 만들어 봅시다. Gemini Pro Vision 모델을 구현한 Java Cloud Function을 호출할 것입니다. 이 함수는 동일한 데이터셋에 만들 것입니다. BigQuery 콘솔의 SQL 편집 창에서 다음 DDL을 실행해 주세요:\n\n```js\nCREATE OR REPLACE FUNCTION `gemini_bq_fn.FN_IMAGE_VALIDATE` (baseline STRING, test STRING, prompt STRING) RETURNS STRING\n  REMOTE WITH CONNECTION `us.gemini-bq-conn`\n  OPTIONS (\n    endpoint = 'https://us-central1-********.cloudfunctions.net/gemini-bq-fn',\n    max_batching_rows = 1\n  );\n```\n\n이렇게 하면 BigQuery에 원격 함수가 생성됩니다. 위의 DDL에는 3개의 매개변수가 있습니다. 처음 두 매개변수는 이전 단계에서 생성된 객체 테이블에 저장된 이미지의 URL입니다. 마지막 매개변수는 모델(Gemini Pro Vision)에 대한 프롬프트입니다. 이 시그니처를 파싱하는 Java Cloud Functions 코드를 참조하시기 바랍니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nGson().fromJson(request.getReader(), JsonObject.class);\nJsonArray calls_array = requestJson.getAsJsonArray(\"calls\");\nJsonArray calls = (JsonArray) calls_array.get(0);\nString baseline_url = calls.get(0).toString().replace(\"\\\"\", \"\");\nString test_url = calls.get(1).toString().replace(\"\\\"\", \"\");\nString prompt_string = calls.get(2).toString();\n```\n\n# BigQuery에서 Gemini 호출하기!\n\n이제 원격 함수가 생성되었으니, 테스트 이미지를 프롬프트와 대조하여 이미지 유효성을 확인하는 원격 함수를 테스트하기 위해 SELECT 쿼리에서 사용해봅시다:\n\n테스트 이미지가 참조와 어떤지 확인하기 위한 쿼리:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nselect gemini_bq_fn.FN_IMAGE_VALIDATE(\n'https://storage.googleapis.com/demo-bq-gemini-public/Baseline.JPG',\nREPLACE(uri, 'gs://', 'https://storage.googleapis.com/') ,\n'전문 이미지 유효성 검사자이며 JSON 결과로 응답할 수 있는 이미지 유효성 검사자입니다. 여기에서 2개의 이미지를 찾을 수 있습니다. 첫 번째 이미지는 기준 이미지이고 두 번째 이미지는 테스트 이미지입니다. 두 번째 이미지가 첫 번째 이미지와 텍스트 측면에서 유사한지 확인하세요. \"YES\" 또는 \"NO\"인 SIMILARITY, 백분율인 SIMILARITY_SCORE, 문자열인 DIFFERENCE_COMMENT 3가지 속성이 포함된 JSON 형식으로만 응답하세요.' ) as IMAGE_VALIDATION_RESULT\nfrom `gemini_bq_fn.image_validation`\nwhere uri like '%TEST1%';\n```\n\n위 쿼리를 TEST1.JPG 및 TEST2.JPG와 함께 시도해보세요. 아래와 유사한 결과를 보게 될 것입니다:\n\n<img src=\"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_13.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기준 이미지:\n\n![이미지1](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_14.png)\n\n테스트 이미지:\n\n![이미지2](/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_15.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위에서 확인할 수 있듯이, 두 이미지 모두 Duet AI 클라우드 콘솔 뷰를 가지고 있지만, 두 이미지의 텍스트는 모델에 의해 생성된 JSON 형식에 따라 다릅니다.\n\n# 혜택 및 사용 사례\n\n- 데이터에 GenAI를 적용하세요: 데이터 이동, 중복 및 추가 복잡성이 더 이상 필요하지 않습니다. 동일한 BigQuery 환경 내에서 데이터를 분석하고 인사이트를 생성할 수 있습니다.\n- 향상된 분석: Gemini의 자연어 설명은 데이터에 새로운 이해의 층을 더해주며, SQL 쿼리만을 사용하여 이를 달성할 수 있습니다.\n- 확장성: 이 솔루션은 대규모 데이터셋과 복잡한 분석을 쉽고 Low-Code 방식으로 처리할 수 있습니다.\n\n실제 사례: 금융(시장 트렌드 분석), 소매(고객 감정), 의료(의료 보고서 요약) 등 분석 및 비즈니스 팀이 비교적 적은 노력, 자원 및 익숙한 언어 및 도구를 선택하여 이를 구현할 수 있는 시나리오를 고려해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n축하합니다. Gemini 모델이 BigQuery에 통합되어 데이터 분석을 넘어 데이터 이야기꾼이 되셨습니다. 데이터셋 안에 숨겨진 이야기를 찾아내고 통찰력을 이해하는 방법을 변화시킬 수 있습니다. 지금 실험을 시작하세요! 이 기술을 여러분의 데이터셋에 적용하여 데이터 안에 깔려있는 이야기들을 발견해보세요. BigQuery가 객체 테이블(External Tables)에서 비구조적인 데이터를 지원하므로, 이미지 데이터에 대한 생성적 인사이트를 만들기 위해 Gemini Pro Vision을 사용해보세요. 더 깊은 안내를 위해서 Vertex AI, BigQuery Remote Functions 및 Cloud Functions 문서를 참고하세요. 이 프로젝트의 Github 저장소는 여기에 있습니다. 이 학습으로 어떤 것을 구축하시는지 저에게 알려주세요!\n","ogImage":{"url":"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_0.png"},"coverImage":"/assets/img/2024-05-18-In-PlaceLLMInsightsBigQueryGeminiforStructuredUnstructuredDataAnalytics_0.png","tag":["Tech"],"readingTime":25},{"title":"2024년 소프트웨어 개발자를 위한 내가 가장 좋아하는 SQL과 데이터베이스 강좌들","description":"","date":"2024-05-18 18:13","slug":"2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024","content":"\n## 소프트웨어 개발자가 SQL 및 데이터베이스 개념을 깊이 학습할 수 있는 최고의 온라인 강좌들입니다.\n\n![이미지](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_0.png)\n\n안녕하세요 여러분, SQL과 데이터베이스를 배우고 최고의 Udemy 강좌를 찾고 있다면, 당신이 올바른 곳에 왔습니다.\n\n이전에는 SQL을 배울 수 있는 최적의 위치와 최고의 무료 SQL 강좌를 공유했었는데요, 그 안에는 Udemy나 Coursera 및 다른 웹사이트의 무료 강좌들이 포함되어 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 초보자와 중급 개발자를 위한 Udemy의 최고의 SQL 강좌를 소개할 것입니다.\n\nSQL은 오랜 시간 동안 중요한 기술 기술이었지만 데이터 과학 및 데이터 분석의 등장으로 인해 데이터의 중요성이 현재 세상에서 더욱 중요해졌습니다.\n\n요즘 회사들 사이에서 데이터 과학 및 분석 직업은 높은 수요가 있으며 사용자들의 대량 데이터 및 기타 정보를 활용하여 이 데이터에 대한 통찰을 얻고 회사의 성장을 위한 더 나은 결정을 내리는 데 중요한 역할을 합니다. 데이터와 관련된 모든 직업이 SQL 언어를 배우는 것을 필요로 한다는 공통점이 있습니다.\n\nSQL은 회사의 데이터를 저장하기 위한 데이터베이스를 구축하고 데이터베이스와 상호 작용하기 위해 SQL 쿼리라고 불리는 명령을 사용하여 정보를 추출하고 데이터 분석 목적을 위해 필요한 정보만 남기기 위해 필터링하는 사람들을 위한 가장 인기 있는 언어입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터베이스는 테이블의 모음이며, 각 테이블에는 데이터를 보유하는 행(row)과 열(column)이 포함되어 있습니다.\n\n이 언어를 배우는 것은 대부분의 다른 언어보다 쉽습니다. 심지어 Python보다도 쉽죠. SQL을 배우는 데 투자한 시간과 비용은 데이터 관련 분야의 취업을 원하는 학생들에게 좋은 투자입니다. 이는 당신을 경쟁자들보다 우위에 서게 할 겁니다.\n\n온라인에서 수천 개의 SQL 코스가 제공되지만, 당신의 시간과 노력을 가치 있게 만들어주는 코스를 찾는 것은 쉽지 않습니다. 이 글에서는 내 검색 결과에 따라 가장 좋은 코스를 제안하겠습니다.\n\n그런데, 만약 급한 대로 배우려 한다면, Udemy의 '15 Days of SQL: The Complete SQL Masterclass 2024' 코스를 참여하는 것을 제안합니다. 이 Udemy의 새 SQL 코스는 실생활 프로젝트에서 SQL을 딱 15일 만에 가르쳐줍니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_1.png\" />\n\n# 2024년 소프트웨어 개발자를 위한 최고의 SQL 및 데이터베이스 강좌 6선 - Udemy 및 Coursera 온라인 학습\n\n2024년에 온라인으로 배울 수 있는 최고의 Udemy 강좌 목록입니다. 이 강좌들은 SQL을 사용해 본 적은 있지만 깊이 있는 지식으로 습득하고 싶은 초보자 및 중급 개발자들을 위한 적합한 강좌입니다.\n\n## 1. The Complete SQL Bootcamp 2024: 처음부터 전문가까지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nJose Portilla가 만든 이 강좌에는 41만 명 이상의 수강생이 있습니다. 이 데이터베이스나 이 언어에 이전 경험이 없는 초보자를 위한 이 가이드로 SQL 언어 학습 여정을 시작하는 것을 적극 추천합니다.\n\n이 강좌를 통해 SELECT 및 COUNT와 같은 간단한 SQL 명령어를 데이터베이스에 적용하는 방법, 그리고 GROUP BY 문을 사용하는 방법을 배울 수 있습니다. 또한 이 강좌는 PostgreSQL을 기반으로 하며 PostgreSQL 데이터베이스를 사용합니다.\n\n그런 다음 JOIN 명령어를 사용하여 여러 테이블에서 데이터를 검색하는 방법을 배우고 특정 데이터를 추출하기 위한 일부 고급 SQL 명령어를 익힐 수 있습니다. 마지막으로 PostgreSQL 데이터베이스에서 데이터베이스 및 테이블을 생성하는 방법도 배울 수 있습니다.\n\n여기 이 강좌에 가입할 수 있는 링크가 있습니다 - The Complete SQL Bootcamp 2024: 제로부터 히어로까지 변화하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_2.png\" />\n\n## 2. The Ultimate MySQL Bootcamp [Udemy Course]\n\n또 한 번 소개할 만한 좋은 강의는 이 최고의 MySQL 부트캠프이다. 이 코스에는 20시간 이상의 비디오 콘텐츠와 26.4만 명의 학생이 참여하고 있다.\n\n먼저 MySQL 데이터베이스의 중요 개념과 해당 환경을 컴퓨터에 설치하는 방법을 이해할 수 있게 될 것이고, 이후 MySQL에서 데이터베이스와 테이블을 생성하는 방법으로 나아갈 것이다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러면 이 데이터베이스에 데이터를 삽입하는 방법과 기타 사항을 배울 것입니다.\n\nSQL 언어에서 CRUD 명령문에 대해 알게 될 것입니다: 생성(Create), 조회(Read), 갱신(Update), 삭제(Delete) 쿼리에 대해 배울 것입니다. 또한 집계 함수에 대해 배우고 논리 연산자의 힘을 탐색할 것입니다.\n\n마지막으로 Node.js와 MySQL 데이터베이스를 사용하여 작은 웹 앱을 만들 것입니다.\n\n이 코스에 참여하기 위한 링크는 여기에 있습니다 — The Ultimate MySQL Bootcamp\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_3.png)\n\n## 3. 데이터 분석 및 비즈니스 인텔리전스를 위한 MySQL\n\n만약 데이터 분석가가 되려고 한다면, 이 강의가 적합할 것입니다. SQL 언어뿐만 아니라 Tableau 소프트웨어와 결합하여 데이터 시각화를 쉽게 할 수 있습니다.\n\n우선 데이터베이스가 어떻게 작동하고 데이터를 저장하는지 이해하고, MySQL을 설치하고 SQL 명령어로 연습을 시작할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSQL의 기본 명령어인 SELECT, INSERT, UPDATE, DELETE 및 집계 함수와 몇 가지 고급 주제를 학습한 후, 마지막으로 Tableau 소프트웨어와 결합하여 데이터 시각화를 수행할 수 있습니다.\n\n이 강좌에 참여하려면 다음 링크를 클릭하세요 — MySQL for Data Analytics and Business Intelligence\n\n[링크](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_4.png)\n\n## 4. SQL 초보자를 위한강좌\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 강좌에는 8시간 이상의 비디오 콘텐츠가 포함되어 있으며 MySQL 데이터베이스를 사용하여 시네마 예매 시스템을 만드는 실제 예제를 제공합니다.\n\nSQL 언어를 사용하기 전에 시스템에 MySQL 데이터베이스를 설치하고 주요 및 외래 키, 테이블과 같은 데이터베이스 개념을 이해할 수 있습니다.\n\n이 언어를 사용하여 테이블과 많은 테이블에서 데이터를 선택하고 간단한 SQL 명령을 사용하여 정보를 추출하는 방법을 배우게 됩니다. 데이터베이스 설계 및 데이터베이스 내에서 다양한 관계를 이해하고 시네마 예매 시스템과 같은 프로젝트를 개발합니다.\n\n이 강좌에 가입하려면 여기를 클릭하세요 - SQL for Beginners\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![마크다운](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_5.png)\n\n## 5. 초보를 위한 Microsoft SQL\n\n우리 목록에서 마지막으로 소개하는 이 코스는 마이크로소프트 SQL 서버에서 SQL 언어를 사용하는 방법을 가르쳐 줍니다. 이는 수백만 명의 사용자가 데이터베이스로 사용하고 있는 서비스를 사용하는 데 도움이 될 것입니다.\n\n먼저 간단한 SQL 명령어를 이해하고 적용한 다음, WHERE 절을 사용하여 데이터를 필터링하고 데이터를 정렬하며 여러 테이블에서 데이터를 추출하고 집계 함수를 사용하는 방법을 배울 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 수업에 참여하려면 다음 링크를 클릭해주세요 — Microsoft SQL for Beginners\n\n![Microsoft SQL for Beginners](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_6.png)\n\n## 6. 데이터 과학을 위한 SQL\n\nUdemy를 좋아하지 않거나 Coursera와 같은 인기 있는 학습 플랫폼에서 최고의 SQL 과정을 찾고 있다면, 이 수업을 확인해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터 과학을 위한 SQL은 Coursera에서 가장 인기 있는 강좌 중 하나입니다.\n\nSQL의 기본을 마스터하여 데이터 과학자처럼 데이터를 분석할 수 있게 될 것입니다.\n\n이 강좌를 마친 후 여러 종류의 데이터, 문자열과 정수를 사용하고, 기본 및 복잡한 데이터 선택 쿼리를 수행할 수 있으며 SQL의 원리를 이해할 수 있게 될 것입니다.\n\nWomen in Data의 창립자/CEO이자 데이터 과학자인 Sadie St. Lawrence가 이 Coursera 강좌를 가르칩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 코스에 가입하려면 링크를 확인해보세요 — SQL For Data Science\n\n![이미지](/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_7.png)\n\n그리고, Coursera 코스가 유용하다고 생각되시나요? 전 세계적으로 유명한 기업과 대학에서 만들어졌기 때문에 그렇습니다. Coursera Plus에 가입하는 것을 추천드립니다. 이 구독 플랜은 Coursera의 가장 인기 있는 강좌, 전문 강의, 프로페셔널 인증, 그리고 가이드 프로젝트에 무제한 액세스를 제공해요. 매년 $399이나 월 단위로 $59이 들지만, 돈을 완전히 가치 있게 쓸 수 있을 거라고 생각해요. 왜냐하면 무제한 인증서를 받을 수 있기 때문이거든.\n\n2024년에 SQL과 데이터베이스를 배우기 위한 최고의 Udemy와 Coursera 온라인 강좌는 여기까지에요. 이 목록에는 SQL 기본 사항과 데이터베이스 기초를 배울 수 있는 수업들, 그리고 MySQL, PostgreSQL, 그리고 Microsoft SQL Server와 같은 인기 데이터베이스를 배울 수 있는 온라인 강좌가 포함되어 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 SQL을 배우는 데 적합한 데이터베이스로 수업을 듣는다는 것을 의미합니다. 이것은 초보자들의 관점에서 매우 중요합니다.\n\nSQL 언어를 배우는 것은 데이터 과학자나 데이터 분석가와 같은 데이터와 관련된 모든 직업의 중요한 부분입니다.\n\n웹 개발자라도 데이터베이스를 사용하여 이 언어를 배우고 프로페셔널하게 사용해야 합니다. 왜냐하면 이것이 당신의 경쟁자들에게 이점을 줄 것이기 때문입니다.\n\n만약 이러한 강좌들을 좋아하지 않고 연습이 가득한 부트캠프 스타일의 강좌를 찾고 있다면 Andrei Negaoie의 Complete SQL and Databases Bootcamp 강좌가 시작하기에 좋은 강좌입니다. 이 강좌는 주요 SQL 개념을 가르치기 위한 연습과 SQL 쿼리가 가득합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그리고, 2024년에 SQL을 배우는 가장 좋은 방법을 보여주는 ByteByteGo의 멋진 비디오가 여기 있어요!\n\n탐색해볼 수 있는 다른 SQL 및 개발 코스\n\n- JavaScript를 배우기 위한 10가지 최고의 Udemy 코스\n- Python을 배우기 위한 10가지 최고의 Udemy 코스\n- 2024년을 위한 10가지 최고의 Udemy 코스\n- 풀 스택 웹 개발자로 성장하기 위한 10가지 코스\n- 2024년에 TypeScript를 무료로 배울 수 있는 10가지 코스\n- 초보자를 위한 Angular를 배우기 위한 나의 좋아하는 코스\n- 무료로 Ruby 및 Rails를 배울 수 있는 5가지 코스\n- 2024년 React JS 개발자 로드맵\n- 웹 개발자를 위한 PHP 및 MySQL 학습을 위한 5가지 수업\n- 무료로 블록체인 기술을 배울 수 있는 5가지 코스\n- Oracle 및 Microsoft SQL Server 데이터베이스를 배울 수 있는 5가지 코스\n- 초보자를 위한 10가지 Python 웹 개발 코스\n- 풀 스택 개발자 로드맵\n- Servlet, JSP 및 JDBC를 배울 수 있는 무료 강좌 5개\n- Java 및 DevOps 엔지니어를 위한 Docker 무료 코스 5가지\n- 2024년에 JavaScript를 배울 수 있는 13가지 무료 코스\n- Java에서 RESTful 웹 서비스를 배우기 위한 3권의 책 및 강좌\n- 2024년에 Angular를 배울 수 있는 5가지 무료 코스\n- 풀스택 개발자가 배워야 할 10가지 프레임워크\n\n지금까지 이 기사를 읽어주셔서 감사합니다. 만약 SQL을 배우기 위한 이 최고의 Udemy 코스들이 마음에 든다면, 친구들과 동료들과 공유해주세요. 이 목록에는 Udemy의 최고의 MySQL, PostgreSQL 및 Microsoft SQL Server 코스가 포함되어 있습니다. 질문이나 피드백이 있으시면, 댓글을 남겨주세요.\n\n참고: 만약 SQL 및 데이터베이스에 새로 입문한 분이라면, 여행을 시작할 무료 SQL 코스를 찾고 계시다면, 초보자를 위한 무료 SQL 및 데이터베이스 코스도 확인해보세요. 이 코스들은 Udemy 및 Coursera에서 법적으로 무료로 제공되며 SQL 개념, 데이터베이스 기본 개념, SQL 쿼리 작성 방법 등을 배울 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_0.png"},"coverImage":"/assets/img/2024-05-18-MyFavoriteSQLandDatabaseCoursesforSoftwareDevelopersin2024_0.png","tag":["Tech"],"readingTime":11},{"title":"ML, 데이터 팀을 위한 Gen AI","description":"","date":"2024-05-18 18:10","slug":"2024-05-18-MLGenAIfordatateams","content":"\n## 고전적인 ML 사용 사례와 Gen AI를 위한 신뢰성 있는 설계 구축\n\nAI와 ML은 대부분의 데이터 팀에게 중요한 주제입니다. 회사들은 AI로 실질적인 영향을 얻고 있으며, 데이터 팀은 이 중심에 있어 자신의 작업을 ROI에 결부시키는 원하는 방법을 얻고 있습니다.\n\n최근 예로, AI가 스웨덴의 '지금 살고 나중에 지불' 핀테크 Klarna를 위해 700명의 정근 연애를 자동화하는 데 도움을 주었습니다. Intercom은 이제 AI 중심의 고객 서비스 플랫폼이 되었으며, 임원들은 Gen AI 사용 사례를 구현하는 데 직접적으로 연관된 OKR을 가지고 있습니다.\n\n이 게시물에서는 데이터 팀에서 일하는 경우 이것이 무슨 의미를 하는지 살펴보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 데이터 팀에서의 AI 현황\n\nAI는 많이 발전했습니다. 실제로 그렇습니다. 스탠포드 대학의 2024 AI 지수 보고서에 따르면 AI는 이미지 분류, 시각적 추론, 그리고 영어 이해와 같은 여러 벤치마크에서 인간의 성능을 넘어섰다고 합니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n요즘에는 ML 및 AI에 대한 수요가 급증하여 많은 데이터 팀이 업무 우선 순위를 재조정하게 되었습니다. 이는 ML 및 AI에서 데이터 팀의 역할에 대한 질문을 답하지 못한 채 남아 있습니다. 저희 경험상 데이터가 소유한 부분과 엔지니어가 소유한 부분 사이의 경계가 여전히 모호한 상황입니다.\n\ndbt가 최근 수천 명의 데이터 실무자를 대상으로 조사한 결과, 데이터 팀이 AI 및 ML에 참여하는 정도에 대한 정보를 얻을 수 있었습니다.\n\nAI 도입의 신호는 있지만, 대부분의 데이터 팀은 아직 일상적인 업무에 AI를 사용하고 있지 않습니다. 현재 응답자 중 1/3만이 오늘날 AI 모델 훈련을 위한 데이터를 관리하고 있습니다.\n\n![그림](/assets/img/2024-05-18-MLGenAIfordatateams_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 곧 변경될 수 있습니다. 55%의 사람들이 곧 AI가 자가 데이터 탐색을 위해 혜택을 누리기를 기대하고 있습니다.\n\n![AI 및 ML use cases](/assets/img/2024-05-18-MLGenAIfordatateams_2.png)\n\n이는 우리가 1,000개 이상의 데이터 팀과 대화한 경험을 반영한 것입니다. 현재의 노력은 주로 데이터 분석을 위한 데이터 준비, 대시보드 유지 및 이해관계자 지원에 집중되어 있지만, AI 및 ML에 투자하고자 하는 욕망이 있습니다.\n\n# AI 및 ML 사용 사례\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nML과 AI가 수십 년 동안 존재해왔다는 것을 알아야 합니다. 최신 AI 모델인 Gen AI 모델은 텍스트에서 SQL 코드를 생성하거나 비즈니스 질문에 자동으로 답변하는 것과 같은 첨단 사용 사례에 가장 적합할 수 있지만, 분류 및 회귀 모델과 같은 더 검증된 방법들도 중요한 목적을 가지고 있습니다.\n\n가장 인기 있는 기술들 중 일부는 다음과 같습니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_3.png)\n\n# 고전적인 머신 러닝 사용 사례\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n대부분의 팀은 아직 전통적인 머신러닝 방법을 사용하지 않고 있습니다. 예를 들어, 분류, 회귀, 이상 감지와 같은 방법들이 있습니다. 이러한 방법들은 특히, 당신이 예측하고자 하는 명확한 결과 (예: 위험한 고객)와 예측 기능 (예: 가입 국가, 나이, 이전 사기)이 명확한 감독 학습에 유용할 수 있습니다.\n\n이러한 시스템들은 종종 설명하기 쉽고, 각 기능의 상대적 중요성을 추출할 수 있어 이를 통해 이유를 설명하기 쉽습니다. 이로써 이해관계자에게 고위험 고객을 거부하는 결정이 내려진 이유를 설명할 수 있게 됩니다.\n\n아래의 머신러닝 시스템은 고객 위험 점수 모델을 강조하며, 새로 가입한 사용자가 고위험 고객인지 거부해야 할 가능성이 얼마나 높은지를 예측합니다.\n\n![image](/assets/img/2024-05-18-MLGenAIfordatateams_4.png)\n\n다양한 소스에서 수집된 원시 데이터를 활용하여 예측 기능을 구축하며, 이는 데이터 과학자의 전문 지식과 모델이 식별한 예상치 못한 패턴을 결합합니다. 핵심 개념은 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Sources 및 데이터 마트: 시스템에서 추출된 원시 및 가공되지 않은 데이터로 데이터 과학자가 관련성이 있는 것으로 판단한 것\n- 특성: ML 모델에 공급되는 전처리된 데이터 (예: 대도시의 거리, 나이, 이전 사기)\n- 레이블: 이전 위험한 고객을 기반으로 한 목표 출력 (예/아니오)\n- 트레이닝: 기계 학습 모델에 내부 매개변수나 가중치를 레이블된 예시에 기반하여 조정하여 정확한 예측을 수행할 수 있도록 가르치는 반복적인 프로세스\n- 추론: 트레이닝 단계 이후 새로운, 보이지 않은 데이터에 대해 예측이나 분류를 수행하기 위해 훈련된 기계 학습 모델을 사용하는 것\n\n데이터 팀과의 협업을 통해, 전통적인 ML 작업 흐름의 많은 부분이 데이터 웨어하우스로 이동되어 데이터 소스 및 피처 저장소의 기반이 되는 것을 볼 수 있습니다. 주요 데이터 웨어하우스는 이를 직접 제공하도록 시작했으며(예: BigQuery ML), 미래에는 전체적인 ML 작업 흐름이 데이터 웨어하우스로 완전히 이동할 것을 시사합니다.\n\n전통적인 ML 모델의 성공을 위한 일반적인 도전 과제는 다음과 같습니다:\n\n- 이용 가능한 데이터를 바탕으로 모델이 원하는 결과를 정확하고 적합한 수준으로 예측할 수 있는가\n- 달성된 정확도와 적합도 수준이 비즈니스에 대한 ROI로 충분한가\n- 이 작업을 수행하기 위해 우리가 해야 하는 트레이드 오프는 무엇인가(예: 위험한 고객을 검토하기 위해 더 많은 운영 직원)\n- 모델 유지 및 모니터링에 대한 유지와 모니터링의 비용은 얼마인가\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 차세대 및 Gen AI 사용 사례\n\n최근 몇 년간 차세대 및 특히 Gen AI 사용 사례에 대한 이야기가 소개되었으며 ChatGPT 3의 효율성으로 유명해졌습니다. 이 분야는 새로운 것이며 비즈니스 ROI가 아직 증명되지 않았지만 잠재력은 매우 큽니다.\n\n아래는 데이터 팀을 위해 본 Gen AI 사용 사례 중에서 가장 인기 있는 몇 가지입니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n오늘의 사용 사례는 크게 두 가지 영역으로 그룹화될 수 있어요.\n\n- 비즈니스 가치 향상 — 고객 지원 챗봇에서 간단한 고객 상호 작용을 자동화하거나 고객 답변을 관련 지식 베이스 기사와 매칭하는 등 비즈니스 프로세스를 자동화하거나 최적화합니다.\n- 데이터 팀 생산성 향상 — 근본적인 데이터 워크플로우를 단순화하여 기술에 능통하지 않은 분석가가 ‘텍스트를 SQL로’ 쓸 수 있도록 하거나 비즈니스 이해자가 제시한 자연어 질문에서 답변을 생성함으로써 비즈니스 이해자의 즉각적인 요청을 줄입니다.\n\n아래는 비즈니스에 관련된 특정 데이터 말뭉치를 기반으로 ChatGPT의 사용자 버전을 설정하는 샘플 아키텍처입니다. 시스템은 두 부분으로 구성됩니다: (1) 도메인 데이터의 데이터 적재 및 (2) 실시간으로 질문에 답변할 수 있도록 데이터를 쿼리합니다.\n\n![image](/assets/img/2024-05-18-MLGenAIfordatateams_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예제 정보 검색 시스템 (출처: Langchain)\n\n첫 번째 단계는 문서를 벡터 저장소에 로드하는 것입니다. 이 과정에는 서로 다른 소스에서 데이터를 결합하거나 엔지니어들과 함께 생 데이터를 다루는 것, 그리고 모델이 교육받지 않아도 되는 데이터를 수동으로 제거하는 것(예: 고객 만족도 낮은 지원 응답)이 포함될 수 있습니다.\n\n- 특정 텍스트 말뭉치에서 텍스트로 데이터 소스 로드\n- 전처리하고 텍스트를 작은 조각으로 나누기\n- 단어들의 유사성에 따라 단어의 벡터 공간을 만들기 위해 임베딩 만들기\n- 임베딩을 벡터 저장소에 로드하기\n\n임베딩에 익숙하지 않다면, 단어나 문서의 숫자적 표현이고 이들 사이에 존재하는 의미와 관계를 포착하는 것이다. 아래 코드 스니펫을 실행하면 실제로 무엇인지 볼 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nfrom gensim.models import Word2Vec\n# 문장 말뭉치 정의\ncorpus = [\n    \"the cat sat on the mat\",\n    \"the dog barked loudly\",\n    \"the sun is shining brightly\"\n]\n# 문장 토큰화\ntokenized_corpus = [sentence.split() for sentence in corpus]\n# Word2Vec 모델 학습\nmodel = Word2Vec(sentences=tokenized_corpus, vector_size=3, window=5, min_count=1, sg=0)\n# 단어 임베딩 획득\nword_embeddings = {word: model.wv[word].tolist() for word in model.wv.index_to_key}\n# 단어 임베딩 출력\nfor word, embedding in word_embeddings.items():\n    print(f\"{word}: {embedding}\")\n```\n\n도메인 데이터를 벡터 저장소에 입력한 후, 사전에 학습된 LLM을 세밀하게 조정하여 도메인과 관련된 질문에 답변하는 시스템을 확장할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_7.png)\n\n예시 정보 검색 시스템 (출처: Langchain)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n사용자는 채팅과 새로운 질문을 결합하여 후속 질문을 할 수 있습니다.\n위의 임베딩 및 벡터 저장소를 사용하여 유사 문서를 찾을 수 있습니다.\n큰 언어 모델(ChatGPT와 같은)을 사용하여 유사 문서를 활용하여 응답을 생성할 수 있습니다.\n\n다행히도 Meta와 Databricks와 같은 기업들이 교육 및 오픈소스 모델을 제공하고 있으므로 (Huggingface는 현재 1000여 개 이상의 Llama 3 오픈소스 모델을 보유하고 있습니다) 자체 모델을 교육시키기 위해 수백만 달러를 소비할 필요가 없습니다. 대신 기존 모델을 데이터로 세밀하게 조정하세요.\n\n위와 같은 LLM(Large Language Model) 기반 시스템의 효과는 그들에게 주어지는 데이터의 품질에 달려 있습니다. 따라서 데이터 전문가들은 여러 소스에서 가져온 가능한 많은 데이터를 피드하는 것이 장려되며, 이들 소스가 어디에서 오는지 추적하고 데이터가 예상대로 흐르는지 확인하는 것이 최우선 과제여야 합니다.\n\nGen AI 모델의 성공을 위한 전형적인 도전 과제는:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 모델을 충분히 훈련할 만한 데이터가 있나요? 개인정보 문제로 사용이 제한되는 데이터가 있나요?\n- 모델이 해석 가능하고 설명 가능해야 하는가요? 예를 들어 고객이나 규제기관을 위해\n- LLM을 훈련하고 세부 조정하는 것에 대한 잠재적 비용은 무엇인가요? 그 혜택이 이 비용을 상회하나요?\n\n# AI와 ML에서 데이터 품질의 중요성\n\n당신의 주요 데이터 전달은 의사 결정에 도움을 주는 BI 대시보드를 위해 무작위 통찰을 제공할 때, 인간이 개입합니다. 인간의 직관과 기대로 인해 데이터 문제나 설명할 수 없는 추세가 종종 발견됩니다 — 그리고 아마도 몇 일 안에 해결됩니다.\n\nML과 AI 시스템은 다릅니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nML 시스템이 수백 개 또는 수천 개의 다양한 소스에서 가져온 기능에 의존하는 것은 흔한 일입니다. 간단한 데이터 문제처럼 보일 수 있는 것들 — 누락된 데이터, 중복, 널 값 또는 빈 값, 이상치 — 이들은 비즈니스에 중대한 문제를 일으킬 수 있습니다. 이를 세 가지 다른 방법으로 생각해 볼 수 있습니다.\n\n- 비즈니스 중단 — 모든 사용자 ID가 비어 있는 중대한 오류는 새 사용자 가입 승인 비율이 90% 감소할 수 있습니다. 이러한 유형의 문제는 비용이 많이 들지만 종종 초기에 발견됩니다.\n- 드리프트 또는 '잠재적' 문제 — 이는 고객 분포의 변경이나 특정 세그먼트에 대한 누락된 값을 포함할 수 있으며, 이로 인해 체계적으로 부정확한 예측이 발생할 수 있습니다. 이러한 문제는 발견하기 어려우며, 몇 달 또는 몇 년 동안 지속될 수 있습니다.\n- 체계적인 편향 — Gen AI와 같은 경우, 데이터 수집에 대한 인간의 판단이나 결정으로 편향이 발생할 수 있습니다. 구글의 Gemini 모델에서 발생한 편견과 같이 최근 예들은 이러한 결과가 가져다 줄 수 있는 결과를 강조했습니다.\n\n회귀 모델을 지원하거나 LLM을 위한 새로운 텍스트 말뭉치를 작성 중이더라도, 새로운 모델을 개발하는 연구자가 아닌 한, 업무의 대부분은 데이터 수집 및 전처리에 관련될 것입니다.\n\n![이미지](/assets/img/2024-05-18-MLGenAIfordatateams_8.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nML 시스템은 모델이 하나의 부분에 불과한 대규모 생태계입니다. — Google on Production ML Systems\n\n일반적으로, 화면 왼쪽에 위치할수록 오류를 모니터링하기 어려울 수 있습니다. 수백 개의 입력 및 원시 소스가 있어서 때로는 데이터 관련 전문가의 통제 영역을 벗어날 수 있으며, 데이터는 수천 가지 방법으로 잘못될 수 있습니다.\n\n![MLGenAIfordatateams_9](/assets/img/2024-05-18-MLGenAIfordatateams_9.png)\n\n모델 성능은 ROC, AUC 및 F1 점수와 같이 잘 알려진 메트릭을 사용하여 간단히 모니터링할 수 있으며, 이러한 메트릭은 모델 성능의 단일 측정 항목을 제공합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n상류 데이터 품질 문제의 예시\n\n- 결측 데이터: 데이터셋의 불완전하거나 없는 값은 모델이 일반화하고 정확한 예측을 하는 능력에 영향을 미칠 수 있습니다.\n- 일관성 없는 데이터: 서로 다른 소스 또는 시간에 따라 다양한 형식, 단위 또는 표현으로 인한 데이터 변이는 모델 학습 및 추론 중 혼동과 오류를 유발할 수 있습니다.\n- 이상치: 대부분의 관측치와 유별난 점이 큰 데이터의 이상치 또는 특이치는 모델 학습에 영향을 주고 편향적이거나 부정확한 예측을 유발할 수 있습니다.\n- 중복 레코드: 데이터셋에 중복된 항목이 들어 있는 경우 모델의 학습 과정을 왜곡시킬 수 있으며, 모델이 훈련 데이터에서 성능이 우수하지만 새로운, 보지 못한 데이터에서는 성능이 저하될 수 있습니다.\n\n데이터 이동의 예시\n\n- 계절별 제품 선호도: 계절에 따른 고객 선호도의 변화가 전자 상거래 추천에 영향을 미칩니다.\n- 금융 시장 변동: 경제적 사건으로 인한 시장의 급격한 변동이 주식 가격 예측 모델에 영향을 미치는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLLM에 대한 텍스트 데이터의 데이터 품질 문제 예시\n\n- 품질이 낮은 입력 데이터: 챗봇은 정확한 과거 사례 해결을 기반으로 작동합니다. 이 데이터의 정확성에 따라 봇의 효과가 결정되며, 잘못된 정보를 배우는 것을 피해야 합니다. 고객 만족도나 해결 점수가 낮은 답변은 모델이 잘못된 정보를 학습했을 수 있다는 신호일 수 있습니다.\n- 오래된 데이터: 의료 상담 봇은 오래된 정보에 의존할 수 있어서 관련성이 적은 권장 사항을 제공할 수 있습니다. 특정 일자 이전에 작성된 연구는 더 이상 목적에 부합하지 않을 수 있음을 나타낼 수 있습니다.\n\n# 신뢰할 수 있는 머신 러닝 및 인공지능 시스템 구축\n\n우리는 데이터 팀이 소프트웨어 엔지니어링과 비교했을 때 신뢰할 수 있는 데이터 시스템을 제공하는 데 신뢰받지 못한다고 믿습니다. 인공지능 파동은 \"쓰레기를 넣으면 쓰레기가 나온다\" 모델과 그 모든 함의를 기하급수적으로 확장하고 있습니다. 모든 기업이 경쟁 우위를 위한 데이터를 활성화하는 새로운 방법을 찾는 압박 속에 있을 때입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n특정 도구와 시스템이 모델 성능을 모니터링하기 위해 사용되지만, 이러한 도구들은 종종 데이터 웨어하우스의 상위 소스와 데이터 변환을 고려하지 않습니다. 데이터 신뢰성 플랫폼은 이를 위해 구축되었습니다.\n\n<img src=\"/assets/img/2024-05-18-MLGenAIfordatateams_10.png\" />\n\n# 안정적인 ML 및 AI 시스템 구축을 위한 다섯 가지 요추\n\n고품질의 제품용 ML 및 AI 시스템을 지원하고 유지하기 위해 데이터 팀은 엔지니어들의 최상의 실천 방법을 채택해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![img](/assets/img/2024-05-18-MLGenAIfordatateams_11.png)\n\n- 성실한 테스트 — ML 및 AI 시스템에 공급되는 상위 소스 및 출력이 의도적으로 테스트되어야 함 (이상값, 널 값, 분포 변화, 품질)\n- 소유자 관리 — ML 및 AI 시스템은 명확한 소유자가 할당되어 문제를 통지받고 조치를 취하기를 기대해야 함\n- 사건 처리 — 심각한 문제는 명확한 SLA 및 에스컬레이션 경로를 가진 사건으로 취급되어야 함\n- 데이터 제품 마인드셋 — ML 및 AI 시스템으로 공급되는 전체 가치 사슬을 하나의 제품으로 고려해야 함\n- 데이터 품질 메트릭스 — 데이터 팀은 ML 및 AI 시스템의 가동 시간, 오류, SLA 등 핵심 메트릭을 보고할 수 있어야 함\n\n한 축에만 집중하는 것은 드물게 충분하지 않습니다. 명확한 소유권이 없는 채로 테스트에 과도하게 투자하면 문제가 슬립할 수 있습니다. 소유에 투자하지만 의도적으로 사건을 관리하지 않으면 심각한 문제가 너무 오랫동안 해결되지 않을 수 있습니다.\n\n![img](/assets/img/2024-05-18-MLGenAIfordatateams_12.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n중요한 점은 다섯 가지 데이터 신뢰성 기둥을 구현하는 데 성공한다고 해도 문제가 발생하지 않는다는 것이 아니라, 단지 미리 발견할 가능성이 더 높아지고 자신감을 키워 고객에게 시간이 지남에 따라 어떻게 개선되고 있는지 전달할 수 있다는 것입니다.\n\n# 요약\n\n현재 데이터 팀 중 33%만이 AI 및 ML 모델을 지원하지만 대부분은 가까운 미래에 지원할 것으로 예상합니다. 이러한 변화는 데이터 팀이 비즈니스 중요 시스템을 지원하고 소프트웨어 엔지니어처럼 더 많이 일해야 한다는 새로운 세계에 적응해야 한다는 것을 의미합니다.\n\n- 데이터 팀에서의 AI 상황 - AI 시스템은 이미지 분류, 시각적 추론 및 영어 이해와 같은 여러 기준에서 성능이 향상되고 있습니다. 현재 데이터 팀 중 33%가 생산 중인 AI 및 ML을 사용하지만 55%의 팀이 예상됩니다.\n- AI 사용 사례 - 분류 및 회귀에서 Gen AI까지 다양한 ML 및 AI 사용 사례가 있습니다. 각 시스템은 도전적인 과제를 제기하지만 \"고전적인 ML\"과 Gen AI 간의 차이는 명백합니다. 우리는 이를 고전적인 고객 위험 예측 모델과 정보 검색 챗봇을 통해 살펴봤습니다.\n- AI 및 ML 시스템의 데이터 품질 - 데이터 품질은 ML 및 AI 프로젝트의 성공에 가장 중요한 위험 중 하나입니다. AI 및 ML 모델이 종종 수백 개의 데이터 소스에 의존하는데, 문제를 수동으로 감지하는 것은 거의 불가능합니다.\n- 믿을 수 있는 데이터를 위한 다섯 가지 단계 - ML 및 AI 시스템을 지원하고 유지하기 위해 데이터 팀은 엔지니어처럼 더 많이 일해야 합니다. 이에는 지속적인 테스트, 명확한 소유권, 사건 관리 프로세스, 데이터 제품 마인드셋 및 가동 시간 및 SLA와 같은 지표에 대한 보고 능력이 포함됩니다.\n","ogImage":{"url":"/assets/img/2024-05-18-MLGenAIfordatateams_0.png"},"coverImage":"/assets/img/2024-05-18-MLGenAIfordatateams_0.png","tag":["Tech"],"readingTime":15},{"title":"데이터 웨어하우징을 위한 5가지 사이버보안 팁","description":"","date":"2024-05-18 18:08","slug":"2024-05-18-5CybersecurityTipsforDataWarehousing","content":"\n<img src=\"/assets/img/2024-05-18-5CybersecurityTipsforDataWarehousing_0.png\" />\n\n데이터 웨어하우징은 대규모 AI 및 기계 학습 애플리케이션을 훨씬 더 관리하기 쉽게 만듭니다. 모든 것을 한 곳에 가지고 있으면 더 빠르고 정확한 분석이 가능해지지만, 동시에 일부 보안 문제를 야기할 수도 있습니다. 이러한 대규모로 통합된 데이터베이스는 사이버 범죄자들에게 유혹이 되는 대상이므로 면밀한 보호가 필요합니다.\n\n조직 간에도 데이터 웨어하우스 자체가 다양하듯이 특정 보안 시스템도 다양합니다. 그럼에도 불구하고 설정과는 상관없이 몇 가지 모범 사례를 도입해야 합니다. 고려해야 할 다섯 가지 주요 사이버 보안 팁을 소개합니다.\n\n# 1. 데이터 익명화 및 암호화\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터 창고에서 모든 데이터를 암호화하는 것이 첫 번째 단계입니다. 데이터에 높은 암호화 표준을 적용하면, 해커들이 액세스하더라도 그것이 쓸모없게 만들어질 것입니다. 홀모모르픽 암호화와 같은 새로운 기술은 당신이 데이터를 복호화하기 전에도 암호화된 데이터를 사용할 수 있도록 한 단계 더 나아간 것입니다.\n\n사용하는 데이터 유형에 따라 데이터를 익명화해야 할 수도 있습니다. 이는 개인 식별자를 제거하여 개인 정보 침해를 방지하는 프로세스입니다. 실제 세계의 수치를 합성 데이터로 교체하는 것이 가장 안전한 방법이지만, 데이터가 실제 세계 사람들을 반영해야 하는 경우 역동적 익명화가 좋은 대안입니다.\n\n## 2. 액세스 권한 제한\n\n데이터 창고 사이버 보안의 다음 단계는 사용자의 액세스 권한을 제한하는 것입니다. 이 작업에 접근하는 가장 좋은 방법은 최소 권한 원칙(Least Privilege Principle, PoLP)을 따르는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPoLP(Principle of Least Privilege)는 작업을 올바르게 수행하기 위해 필요한 것만 액세스할 수 있어야 한다고 주장합니다. 기계 학습 모델과 작업하지 않는 직원은 기계 학습 훈련을 위해 구체적으로 데이터 웨어하우스에 액세스할 수 없어야 합니다. 마찬가지로, 데이터 과학자는 급여 데이터를 볼 수 없어야 합니다.\n\n액세스 권한 제한은 두 가지 주요 이점이 있습니다. 첫째, 주어진 데이터 웨어하우스에 영향을 미칠 수 있는 사람 수를 줄임으로써 발생하는 74%의 데이터 침해와 관련된 인적 오류를 최소화합니다. 둘째, 공격자가 한 계정을 침해하면 측면 이동을 최소화합니다.\n\n# 3. 인증 조치 향상\n\n권한 제한은 신뢰할 수 있는 방법으로 누가 누구인지 판별할 수 있을 때에만 효과가 있음을 기억하세요. 따라서, PoLP를 강력한 인증 조치와 함께 실행해야 합니다. 가장 기본적인 수준에서는 다중 요소 인증(MFA)이 시행되어야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMFA는 여러 방법으로 실행할 수 있지만, 모든 방법이 동일한 수준의 보안을 제공하는 것은 아닙니다. 예를 들어 SMS 기반 인증은 이메일 인증보다 더 안전합니다. 특정 장치에 액세스가 필요하기 때문입니다. 생체 인증은 암호보다 해킹이 더 어려울 수 있지만, 공격자가 생체 데이터에 액세스하면 변경할 수 없으므로, 민감한 창고에는 이상적이지 않을 수 있습니다.\n\n# 4. 데이터 분류 및 조직화\n\n데이터 웨어하우징 보안에서 놓치기 쉬운 하지만 여전히 중요한 단계는 데이터를 분류하는 것입니다. 조직화는 사이버 보안 문제보다는 작업 문제처럼 보일 수 있지만, 중요한 보안적 영향을 많이 미칩니다.\n\n먼저, 볼 수 없는 것은 안전으로 보호할 수 없습니다. 보안 소프트웨어 사용자의 약 60%가 데이터의 40% 미만만 분석한다고 합니다. 이는 중요한 취약점을 놓칠 수 있거나 침해를 인식하지 못할 수 있음을 의미합니다. 조직의 부재는 시각성을 제한하기 때문에 데이터를 분류하여 그룹으로 구성하여 보다 철저한 취약점 분석과 빠른 사고 대응을 가능하게 해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n분류는 엑세스 권한을 정제하는 데도 도움이 됩니다. 데이터를 사용 또는 민감도에 따라 정렬하면 누가 액세스할 수 있는지 결정하고 해당 정책을 시행하는 데 도움이 됩니다. 또한 행동 생체 인식을 구현할 수 있어 이로 인해 평상시에는 액세스할 수 없는 데이터에 접근하는 경우 경고를 받을 수 있습니다.\n\n# 5. 창고를 면밀히 모니터링하십시오\n\n이러한 변경 사항을 시행한 후 데이터 창고를 지속적으로 모니터링해야 합니다. 어떤 방어 기법도 100% 효과적일 수는 없지만, 신속한 대응은 침해 사건 발생 시 피해를 최소화할 것입니다. 새로운 위협에 대응하거나 실시간 보안 사건에 대응할 수 있는 유일한 방법은 지속적인 모니터링을 통해 가능합니다.\n\n인공지능과 자동화는 여기서 꼭 필요합니다. 24시간 수동 모니터링은 많은 보안 인력이 필요합니다. 대부분의 기관에게는 선택사항이 아닙니다. 세계적으로 노동력 수요가 증가하더라도 사이버 보안 직원은 340만 명이 부족합니다. 자동화된 네트워크 모니터링은 실시간 사건 제한 및 부족한 보안 직원을 보완하기 위한 경고를 제공할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 데이터 웨어하우징 보안에 대한 주의가 필요합니다\n\n데이터 웨어하우스는 한 대의 큰 데이터베이스를 보호하는 것이 여러 개의 자원에 분산하는 것보다 쉽기 때문에 보안이 개선됩니다. 그러나 동시에, 그 크기 때문에 눈에 띄는 관심을 끄는 경우가 있습니다, 특히 민감한 정보를 저장하는 경우에는 더욱 그렇습니다.\n\n이러한 위험 요소들을 고려하여 데이터 웨어하우징 사이버 보안은 필수적입니다. 기존의 보안 시스템에 다음 다섯 가지 모범 사례를 통합하여 데이터 웨어하우스를 가능한 한 안전하게 유지하십시오.\n\n최초 게시물: OpenDataScience.com\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nOpenDataScience.com에서 데이터 과학 관련 기사를 더 읽어보세요. 초보자부터 고급 수준까지의 튜토리얼과 안내서를 만나보실 수 있습니다! 매주 목요일마다 최신 소식을 받아보고 싶으시다면 여기를 클릭하여 주간 뉴스레터를 구독해보세요. 또한 Ai+ 트레이닝 플랫폼을 통해 언제 어디서든 데이터 과학을 학습할 수 있습니다. ODSC 이벤트에 참석하고 싶으신가요? 다가오는 이벤트에 대해 더 알아보고 싶으시다면 여기를 클릭해주세요.\n","ogImage":{"url":"/assets/img/2024-05-18-5CybersecurityTipsforDataWarehousing_0.png"},"coverImage":"/assets/img/2024-05-18-5CybersecurityTipsforDataWarehousing_0.png","tag":["Tech"],"readingTime":5},{"title":"Delta 테이블을 REST API를 통해 노출하는 방법","description":"","date":"2024-05-18 18:06","slug":"2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs","content":"\n## 델타 테이블을 제공하기 위해 토론 및 테스트된 세 가지 아키텍처\n\n![이미지](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_0.png)\n\n# 1. 소개\n\n메달리온 아키텍처 내의 델타 테이블은 일반적으로 데이터 제품을 생성하는 데 사용됩니다. 이러한 데이터 제품은 데이터 과학, 데이터 분석 및 보고를 위해 사용됩니다. 그러나 데이터 제품을 REST API를 통해 노출하는 것도 일반적인 문제입니다. 이 아이디어는 이러한 API를 더 엄격한 성능 요구 사항을 갖춘 웹 앱에 내장하는 것입니다. 중요한 질문은 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데팔타 테이블에서 데이터를 읽는 것이 웹 애플리케이션에 빠르게 서비스할 수 있을까요?\n솔루션을 확장할 수 있는 컴퓨팅 레이어가 필요할까요?\n엄격한 성능 요구 사항을 충족시키기 위한 스토리지 레이어가 필요할까요?\n\n이러한 질문에 대해 심층적으로 다루기 위해 세 가지 아키텍처가 다음과 같이 평가됩니다: 아키텍처 A — API의 라이브러리, 아키텍처 B — 컴퓨팅 레이어 및 아키텍처 C — 스토리지 레이어. 아래 이미지 참조하세요.\n\n![image](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_1.png)\n\n블로그 글의 나머지 부분에서 세 가지 아키텍처에 대한 설명을 제공하고, 배포 및 테스트를 수행한 후 결과를 도출합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 2. 아키텍처 설명\n\n## 2.1 아키텍처 A: DuckDB와 PyArrow를 사용한 API 내 라이브러리\n\n이 아키텍처에서는 API가 직접 델타 테이블에 연결되어 있으며 중간에 계산 레이어가 없습니다. 이는 데이터가 API 자체의 메모리와 계산을 사용하여 분석된다는 것을 의미합니다. 성능을 향상시키기 위해 내장 데이터베이스 DuckDB와 PyArrow의 Python 라이브러리를 사용합니다. 이러한 라이브러리는 API에서 필요한 열만로드되도록 보장합니다.\n\n![이미지](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 아키텍처의 장점은 데이터를 중복으로 만들 필요가 없으며 API와 델탔 테이블 사이에 필요한 레이어가 없다는 것입니다. 이는 구성 요소가 적다는 것을 의미합니다.\n\n이 아키텍처의 단점은 확장하기 어렵고 모든 작업을 API의 컴퓨팅 및 메모리에서 처리해야 한다는 것입니다. 특히 많은 양의 데이터를 분석해야 하는 경우에는 특히 도전적입니다. 이는 많은 레코드, 큰 컬럼 또는 많은 동시 요청에서 나올 수 있습니다.\n\n## 2.2 아키텍처 B: Synapse, Databricks 또는 Fabric을 사용하는 컴퓨팅 레이어\n\n이 아키텍처에서 API는 컴퓨팅 레이어에 연결되고 델탔 테이블에 직접 연결되지 않습니다. 이 컴퓨팅 레이어는 델타 테이블에서 데이터를 가져와 데이터를 분석합니다. 컴퓨팅 레이어는 Azure Synapse, Azure Databricks 또는 Microsoft Fabric일 수 있으며 일반적으로 잘 확장됩니다. 데이터는 컴퓨팅 레이어로 중복되지 않지만 컴퓨팅 레이어에서 캐싱을 적용할 수 있습니다. 이 블로그의 남은 부분에서는 Synapse Serverless로 테스트 되었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_3.png)\n\n이 아키텍처의 장점은 데이터를 중복하여 저장할 필요가 없으며 아키텍처가 잘 확장된다는 것입니다. 또한 대규모 데이터 세트를 처리하는 데 사용할 수 있습니다.\n\n이 아키텍처의 단점은 API와 델타 테이블 사이에 추가적인 레이어가 필요하다는 것입니다. 이는 더 많은 이동 부품을 유지 및 보안해야 한다는 의미입니다.\n\n## 2.3 아키텍처 C: Azure SQL이나 Cosmos DB를 사용한 최적화된 저장 레이어\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 아키텍처에서 API는 델타 테이블에 직접 연결되지 않고, 델타 테이블이 복제된 다른 저장 계층에 연결됩니다. 다른 저장 계층은 Azure SQL 또는 Cosmos DB일 수 있습니다. 이 저장 계층은 데이터를 빠르게 검색하기 위해 최적화될 수 있습니다. 이 블로그의 나머지 부분에서는 Azure SQL을 사용하여 테스트를 진행합니다.\n\n![이미지](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_4.png)\n\n이 아키텍처의 장점은 저장 계층이 인덱스, 파티셔닝 및 머티얼라이즈드 뷰를 사용하여 데이터를 빠르게 읽을 수 있도록 최적화될 수 있다는 것입니다. 이는 주로 요청-응답 웹 앱 시나리오에서 요구 사항입니다.\n\n이 아키텍처의 단점은 데이터가 중복되어야 하며 API와 델타 테이블 사이에 추가적인 계층이 필요하다는 것입니다. 이는 더 많은 구성 요소를 유지보수하고 보안해야 한다는 의미입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n블로그의 나머지 부분에서 아키텍처를 배포하고 테스트합니다.\n\n# 3. 아키텍처 배포 및 테스트\n\n## 3.1 아키텍처 배포\n\n아키텍처를 배포하기 위해 이전 장에서 논의한 세 가지 솔루션을 배포하는 GitHub 프로젝트가 생성되었습니다. 해당 프로젝트는 아래 링크에서 확인할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nhttps://github.com/rebremer/expose-deltatable-via-restapi\n```\n\n다음은 GitHub 프로젝트를 실행할 때 배포될 내용입니다:\n\n- 표준 테스트 데이터 세트 WideWorldImporterdDW full에서 시작한 델타 테이블. 테스트 데이터 세트는 50백만 건의 레코드와 22개 열로 구성되어 있으며 1개의 큰 설명 열이 있습니다.\n- 모든 아키텍처: API로 작용하는 Azure Function.\n- 아키텍처 B: 컴퓨팅 계층으로 작용하는 Synapse Serverless.\n- 아키텍처 C: 최적화된 저장 계층으로 작용하는 Azure SQL.\n\n배포된 후 테스트를 실행할 수 있습니다. 다음 단락에서 테스트에 대해 설명하겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3.2 테스트 아키텍처\n\n아키텍처를 테스트하기 위해 다양한 유형의 쿼리 및 다른 스케일링을 적용할 것입니다. 다양한 유형의 쿼리는 다음과 같이 설명할 수 있습니다:\n\n- 11개의 작은 열(char, integer, datetime)을 포함하는 20개 레코드를 조회합니다.\n- 각 필드당 500자 이상을 포함하는 큰 설명 열이 포함된 2개 열을 사용하여 20개 레코드를 조회합니다.\n- 그룹별 데이터 집계, having, max, average를 사용한 데이터 집계.\n\n아래에서 쿼리를 설명합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```sql\n-- 쿼리 1: 대형 텍스트 없이 11개 열의 포인트 조회\nSELECT SaleKey, TaxAmount, CityKey, CustomerKey, BillToCustomerKey, SalespersonKey, DeliveryDateKey, Package\nFROM silver_fact_sale\nWHERE CityKey=41749 and SalespersonKey=40 and CustomerKey=397 and TaxAmount > 20\n-- 쿼리 2: 500자 이상의 Description 열\nSELECT SaleKey, Description\nFROM silver_fact_sale\nWHERE CityKey=41749 and SalespersonKey=40 and CustomerKey=397 and TaxAmount > 20\n-- 쿼리 3: 집계\nSELECT MAX(DeliveryDateKey), CityKey, AVG(TaxAmount)\nFROM silver_fact_sale\nGROUP BY CityKey\nHAVING COUNT(CityKey) > 10\n```\n\n다음과 같이 스케일링이 가능합니다:\n\n- 아키텍처 A의 경우, 데이터 처리는 API 자체에서 수행됩니다. 이는 API의 컴퓨트 및 메모리가 앱 서비스 플랜을 통해 사용된다는 것을 의미합니다. SKU Basic(1코어 및 1.75GB 메모리) 및 SKU P1V3 SKU(2코어, 8GB 메모리)로 테스트될 것입니다. 아키텍처 B 및 C의 경우에는 처리가 다른 곳에서 이루어지기 때문에 이러한 정보는 해당하지 않습니다.\n- 아키텍처 B의 경우, Synapse Serverless가 사용됩니다. 스케일링은 자동으로 이루어집니다.\n- 아키텍처 C의 경우, 표준 티어의 Azure SQL 데이터베이스가 125 DTU로 사용됩니다. CityKey에 인덱스가 없는 상태와 CityKey에 인덱스가 있는 상태에서 테스트될 것입니다.\n\n다음 단락에서 결과가 설명됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3.3 결과\n\n아키텍처를 배포하고 테스트한 후에는 결과를 얻을 수 있습니다. 다음은 결과 요약입니다:\n\n![Results](/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_5.png)\n\n아키텍처 A는 SKU B1로 배포할 수 없습니다. 만약 SKU P1V3가 사용된다면, 컬럼 크기가 크지 않다면 결과는 15초 이내에 계산될 수 있습니다. 모든 데이터를 API 앱 서비스 계획에서 분석한다는 점을 유의하십시오. 너무 많은 데이터가로드되면(많은 행, 큰 컬럼 및/또는 많은 동시 요청으로),이 아키텍처는 확장하기 어려울 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아키텍처 B는 Synapse Serverless를 사용하여 10-15초 내에 작동합니다. 계산은 데이터를 가져와 분석하기 위해 자동으로 조정되는 Synapse Serverless에서 이루어집니다. 성능은 세 가지 유형의 쿼리에 대해 일관되게 유지됩니다.\n\n아키텍처 C는 Azure SQL을 사용할 때 인덱스가 생성되면 가장 잘 작동합니다. 조회 쿼리 1과 2의 경우 API는 대략 1초 내에 응답합니다. 쿼리 3은 전체 테이블 스캔이 필요하며 성능은 다른 솔루션과 거의 동일합니다.\n\n# 3. 결론\n\n중재 아키텍처의 Delta 테이블은 일반적으로 데이터 제품을 생성하는 데 사용됩니다. 이러한 데이터 제품은 데이터 과학, 데이터 분석 및 보고서 작성에 사용됩니다. 그러나 일반적으로 Delta 테이블을 REST API를 통해 노출하는 것도 자주 묻는 질문 중 하나입니다. 이 블로그 포스트에서는 이와 같은 장단점을 갖는 세 가지 아키텍처가 설명되어 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nArchitecture A: DuckDB 및 PyArrow를 사용하여 API 내 라이브러리를 활용하는 아키텍처입니다.\n이 아키텍처에서는 API가 직접 델타 테이블에 연결되어 중간 계층이 없습니다. 이는 모든 데이터가 메모리에서 분석되고 Azure Function의 연산을 함께 함을 의미합니다.\n\n- 이 아키텍처의 장점은 추가 리소스가 필요하지 않다는 것입니다. 이는 유지 및 보안해야 하는 부분이 적기 때문에 이점으로 작용합니다.\n- 이 아키텍처의 단점은 API 자체에서 모든 데이터를 분석해야 하기 때문에 확장성이 떨어진다는 것입니다. 따라서 소량의 데이터에만 사용해야 합니다.\n\nArchitecture B: Synapse, Databricks 또는 Fabric을 사용한 컴퓨팅 레이어.\n이 아키텍처에서는 API가 컴퓨팅 레이어에 연결됩니다. 이 컴퓨팅 레이어는 델타 테이블에서 데이터를 가져와 분석합니다.\n\n- 이 아키텍처의 장점은 확장성이 좋고 데이터가 중복되지 않습니다. 집계를 수행하며 대량의 데이터를 분석하는 쿼리에 적합합니다.\n- 이 아키텍처의 단점은 조회 쿼리에 일관되게 5초 이내의 응답을 받는 것이 불가능하다는 것입니다. 또한 추가 리소스를 보안 및 유지해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아키텍처 C: Azure SQL 또는 Cosmos DB를 사용한 최적화된 저장 계층입니다.\n\n이 아키텍처에서는 API가 최적화된 저장 계층에 연결됩니다. 델타 테이블이 미리 이 저장 계층으로 복제되며 데이터를 검색하고 분석하는 데 사용됩니다.\n\n- 이 아키텍처의 장점은 인덱스, 파티셔닝, 머티얼라이즈드 뷰를 사용하여 룩업의 빠른 쿼리를 위해 최적화될 수 있다는 것입니다. 이것은 종종 요청-응답 웹 앱에 필요한 요구사항입니다.\n- 이 아키텍처의 단점은 데이터가 다른 저장 계층으로 중복되어 동기화가 유지되어야 한다는 것입니다. 또한 추가 자원을 보안하고 유지해야 합니다.\n\n안타깝게도, 완벽한 해결책은 없습니다. 이 글은 REST API를 통해 델타 테이블을 노출하는 데 가장 적합한 아키텍처를 선택하는 데 도움을 주기 위한 가이드를 제시했습니다.\n","ogImage":{"url":"/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_0.png"},"coverImage":"/assets/img/2024-05-18-HowtoExposeDeltaTablesviaRESTAPIs_0.png","tag":["Tech"],"readingTime":10},{"title":"Power BI 최적화 차원 모델링에서 서로간키의 필요성","description":"","date":"2024-05-18 18:05","slug":"2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling","content":"\n![Surrogate Key](/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_0.png)\n\n# 문제 설명\n\n최근에 나는 대체키를 조합키 대신 사용하는 것을 연구하는 업무를 맡았습니다. 지금까지 우리 팀은 레코드를 고유하게 나타내고 차원과 사실을 차원 모델에 연결하는 데 조합 키를 사용했습니다. 조합 키는 작업을 수행했지만 Power BI 측에서 쿼리 실행 시간이 만족스럽지 않았습니다. 조인이 오랜 시간이 걸렸는데 그 이유는 조합된 키의 열 크기가 큰 것입니다. Power BI에서는 대규모 관계(키) 열이있을 때 쿼리가 훨씬 느리게 실행됩니다.\n\n내 목표는 Power BI에서 쿼리 실행 시간을 줄일 방법을 찾는 것이었습니다. 차원과 사실을 결합하기 위해 대체 키를 조합 키로 교체하는 것은 제가 자세히 탐색한 대안 중 하나였습니다. 이 기사에서는 대체 키의 정의, 목적 및 구현하는 대안 방법에 대해 안내하겠습니다. 실용적인 예제는 PySpark에서 보여집니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 대리 키란 무엇인가요?\n\n남들의 말을 바꾸는 대신, 관계형 데이터베이스의 맥락에서 대리 키 및 다른 유형의 키를 다루는 이 글을 참고해보세요.\n\n내가 대리 키를 정의하는 방식은 이러하다. 대리 키는 인공적이며 비즈니스나 현실 세계와 관련이 없으며 어떠한 비즈니스 개념과도 연결되지 않는다. 레코드를 고유하게 식별하는 데 도움이 되는 고유성을 가지고 있으며 대부분 (항상은 아니지만) 정수 형태이다.\n\n아래 직원 테이블에서 직원 ID 열이 대리 키의 예시입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Surrogate Key](/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_1.png)\n\n서로게이트 키는 여러 가지 이유로 존재합니다. 하나의 명확한 답변은 해당 테이블에 자연 키의 명백한 후보가 없고 레코드를 고유하게 식별하기 위해 (서로게이트 키라고도 함) 가짜 키를 생성해야하는 경우입니다.\n\n두 번째 명백한 이유는 시간의 시험을 견디는 능력입니다. 이 이유는 데이터 웨어하우스 디자인의 맥락에서 더 관련이 있습니다. 기본 키로 자연 키에 의존하는 것은 데이터베이스 수준에서 시간이 지남에 따라 변경 사항에 취약해질 수 있습니다.\n\n구글의 주식 심볼을 예로 들 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n원래 구글 주식의 종류는 GOOG 주식 심볼을 가지고 있었습니다. 2014년 4월, 회사는 GOOGLE 주식 심볼 아래 새로운 주식 종류를 만들었습니다. 데이터베이스 디자인에서 구글 주식을 나타내는 주요 키로 원래 GOOG 심볼을 사용하고 있다면, 이 변경 사항을 반영하기 위해 적절한 수정을 해야 할 것입니다. Kimball (1998)은 이를 정확히 이유로 자연 키를 사용하는 것을 지양해야 한다고 주장했습니다. 그는 트랜잭션 데이터베이스의 자연 키는 \"생산의 지시에 따라 생성, 형식화, 업데이트, 삭제, 재활용 및 재사용\"되기 때문에 트랜잭션 데이터베이스 수준에서 발생하는 변경 사항에 지속적으로 의존해야 한다고 설명했습니다.\n\n대리 키의 세 번째 이유는 쿼리 성능이 더 좋다는 것입니다. 이는 특히 데이터 웨어하우스의 맥락에서 맞닿은 사실입니다. 차원 모델은 차원과 사실 사이에서 많은 조인을 수행해야 한다. 수치 (일반적으로 정수 또는 smallint)인 대리 키를 사용하여 조인하는 것은 문자열로 이루어진 자연 키나 복합 키로 조인하는 것보다 빠릅니다. 이 이유에 대해 더 자세히 알아보려면 여기를 읽어보세요.\n\n# 대리 키 생성의 대체 방법\n\n대리 키 구현에 대한 제 연구로 결과적으로 대리 키를 생성하는 주요 방법은 두 가지라고 결론내렸습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 가지 방법은 단조 증가하는 정수를 생성하는 전통적인 함수를 통해 정수를 만드는 것입니다. 이 함수는 테이블의 고유한 값에 대해 정수를 생성합니다.\n\n다른 대안은 암호 해싱 함수를 사용하는 것입니다. 해싱 함수는 데이터 자체에 대해 고유한 값을 생성하며, \"동일한 입력 집합은 항상 동일한 출력 집합을 생성한다\"는 의미입니다(Connors, 2022). 해싱 함수의 몇 가지 예는 crc32, md5, sha1/ sha2 등이 있습니다.\n\n이 두 가지 방법을 PySpark에서 어떻게 구현하는지 살펴봅시다.\n\n## 단조 증가하는 정수\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n쇼핑 목록에 제품 목록과 제품이 목록에 추가된 타임스탬프가 있는 테이블이 있다고 상상해보세요.\n\n| Product   | Timestamp           |\n| --------- | ------------------- |\n| Product A | 2024-09-15 10:30:00 |\n| Product B | 2024-09-15 11:45:00 |\n| Product C | 2024-09-15 13:20:00 |\n\n제품 차원의 서로 다른 키를 생성하려면 monotonically_increasing_id() 함수를 사용해야 합니다. 이 함수를 적용하기 전에 제품 목록이 고유한지 확인해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 제품 테이블은 바나나와 빵이 2일에 나누어서 입력되어서 두 번씩 나타납니다. 따라서 단계는 제품 열만 선택한 다음, 값을 고유하게 만들고 monotonically_increasing_id() 함수를 적용하는 것입니다.\n\n한 가지 더 고려해야 할 사항은 테이블에 NULL 값이 있는지 여부입니다. 이를 함수를 적용하기 전에 필터링해야 합니다. NULL 값에 대한 대체 키 ID를 부여하고 싶지 않다면 필터링을 해야 합니다.\n\n```js\nfrom pyspark.sql.functions import monotonically_increasing_id, col\n\n#clean\ndf_select=df.select(\n  col('product')\n  )\ndf_distinct=df_select.distinct()\n\n#monotonically_increasing_id 함수를 사용하여 대체 키 생성\ndf_sk=df_distinct.withColumn(\n  \"surrogate_key\",\n  monotonically_increasing_id()\n  )\n```\n\n출력 결과는 다음과 같을 것입니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_3.png)\n\n모노토닉하게 증가하는 정수는 결정론적이 아니며, 동일한 값 목록에 대해 이 기능을 실행해도 항상 동일한 대리 키가 할당되지는 않습니다. 예를 들어, 사과는 항상 ID 1이 할당되는 것은 아닙니다. 대리 키의 유지보수는 따라서 몇 가지 추가 작업이 필요할 수 있습니다.\n\n이를 해결하는 한 가지 방법은 사실보다 차원을 먼저 로드하는 것입니다. 저의 팀에서 구현한 방법은 먼저 대리 키를 사용하여 차원을 생성한 다음 사실에서 차원을 사용하여 대리 키를 조회하는 것입니다. 이는 차원과 사실의 참조 무결성을 보장합니다.\n\n## 해싱 함수\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n해싱 함수에 대한 내 연구는 그들이 무엇인지 이해하고 대체 키 생성을 위한 전통적인 함수들이 언제 선호되는지를 이해하는 데 국한되어 있었습니다. 따라서 해싱 함수들 간의 차이점이나 그들을 어떻게 구현하는지에 대해 자세히 다루지는 않겠습니다. PySpark에서 구현 측면을 다루는 이 Medium 기사를 발견했습니다.\n\n나는 해싱 함수를 선택하지 않은 이유는 해싱 함수를 사용하여 생성된 대체 키들이 종종 더 긴 문자열 값을 갖기 때문입니다. 나가 해결하려고 하는 문제는 Power BI에서 조인의 쿼리 성능을 개선하는 것이었습니다. 긴 합성 키를 해싱 함수에 의해 생성된 긴 키로 바꾸는 것은 나의 문제에 적합한 해결책이 아니었습니다.\n\n하지만 해싱 함수를 매력적으로 만드는 것은 해싱 함수를 통해 생성된 대체 키들이 상기에서 논의된 단조증가 정수보다 유지하기가 훨씬 더 쉽고 (그리고 덜 복잡)이라는 점입니다. 해싱 함수는 결정론적이기 때문에 동일한 입력 세트는 항상 동일한 출력을 생성합니다. 이상적으로는 차원 및 사실을 병렬로 대체 키를 생성할 수 있습니다.\n\n# 요약\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 Power BI에서 쿼리 성능을 향상시키는 데 중점을 두며, 차원 모델의 컴포지트 키에 대한 효율적인 대안으로서 대리키의 사용을 탐색했습니다.\n\n비즈니스 의미를 가지지 않는 고유 식별자인 대리키는 데이터 웨어하우스에서 차원과 사실 사이의 관계 관리를 크게 최적화할 수 있습니다.\n\n우리는 이러한 키를 생성하는 다양한 방법을 탐구했습니다. PySpark에서 실용적인 예제를 통해 각 접근 방식의 이점과 고려 사항을 고려하여 데이터 아키텍처 요구에 맞는 올바른 전략을 선택하는 데 도움이 될 수 있도록 안내했습니다.\n\n# 자원\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n| Author          | Title                                                                    | Year  | Source        |\n| --------------- | ------------------------------------------------------------------------ | ----- | ------------- |\n| Ben             | Database Keys: The Complete Guide (Surrogate, Natural, Composite & More) | 2022  | Database Star |\n| Connor, Dave    | Surrogate Keys In dbt: Integers or Hashes?                               | 2022  | dbt           |\n| Kimball, R.     | Surrogate Keys                                                           | 1998  | Kimball Group |\n| Stiglich, Peter | Performance Benefits of Surrogate Keys in Dimensional Models             | ----- | EWS Solutions |\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n| 저자            | 제목                                                  | 출판연도 | 출처                                   |\n| --------------- | ----------------------------------------------------- | -------- | -------------------------------------- |\n| Wikstrom, Max   | Power BI Data Types In Relationships- Does It Matter? | 2022     | Data, Business Intelligence and Beyond |\n| Zaman, Ahmed Uz | PySpark Hash Functions: A Comprehensive Guide         | 2023     | Medium                                 |\n","ogImage":{"url":"/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_0.png"},"coverImage":"/assets/img/2024-05-18-SpeedingUpPowerBITheCaseforSurrogateKeysinDimensionalModeling_0.png","tag":["Tech"],"readingTime":9},{"title":"YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기","description":"","date":"2024-05-18 18:03","slug":"2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer","content":"\n그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.\n\n![YoutubeDataPipeline](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png)\n\n## 사용 사례: —\n\n상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 제안된 해결책: —\n\n위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:\n\n— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.\n\n— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.\n\n이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.\n\n## 구현\n\n구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 데스크톱을 설치해보세요 — https://www.docker.com/get-started/\n\n머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.\n\n```js\ndocker --version\nDocker version 20.10.23, build 7155243\n```\n\n최신 apache/airflow 이미지를 받아보세요\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```sh\n도커 pull apache/airflow\n```\n\n다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.\n\n```sh\n도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone\n```\n\nAWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nconfig\n\n```json\n[default]\nregion = ap-south-1\n```\n\ncredentials\n\n```json\n[default]\naws_access_key_id = AKIB******AXPCMO\naws_secret_access_key = 4D7HkaIBsqu***********+0AT2a8j\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.\n\n도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.\n\n![이미지](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png)\n\n웹 브라우저를 열고 `http://localhost:8080` 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`aws_write_utility.py` 파일을 만들 때 평소처럼 진행하시면 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport boto3\nimport json\nimport uuid\n\ndef write_json_to_s3(json_data, bucket_name, key_name):\n\n    # S3 클라이언트 초기화\n    s3 = boto3.client('s3')\n\n    # JSON 데이터를 바이트로 변환\n    json_bytes = json.dumps(json_data).encode('utf-8')\n\n    # JSON 데이터를 S3에 쓰기\n    s3.put_object(Bucket=bucket_name, Key=key_name, Body=json_bytes)\n\n\ndef generate_uuid():\n    \"\"\"UUID와 유사한 문자열 생성.\"\"\"\n    return str(uuid.uuid4())\n```\n\nyoutube_comments.py\n\n```js\n# -*- coding: utf-8 -*-\n\n# youtube.commentThreads.list를 위한 샘플 Python 코드\n# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:\n# https://developers.google.com/explorer-help/code-samples#python\n\nimport os\n\nimport googleapiclient.discovery\nimport aws_write_utility\nfrom aws_write_utility import write_json_to_s3\n\ndef start_process():\n    # 로컬에서 실행 시 OAuthlib의 HTTPS 확인 비활성화\n    # 제품 환경에서는 이 옵션을 활성화하지 마세요.\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    DEVELOPER_KEY = \"AIzaS*****************PiwBdaP_IE\"\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey=DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=\"snippet,replies\",\n        videoId=\"r_K*****PKU\"\n    )\n    response = request.execute()\n\n    process_comments(response)\n\n\ndef process_comments(response_items):\n\n    # 예시 S3 버킷 및 키 이름\n    bucket_name = 'youtube-comments-analysis'\n    key_name = 'data/{}.json'.format(aws_write_utility.generate_uuid())\n\n    comments = []\n    for comment in response_items['items']:\n        author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n        comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n        publish_time = comment['snippet']['topLevelComment']['snippet']['publishedAt']\n        comment_info = {'author': author, 'comment': comment_text, 'published_at': publish_time}\n        comments.append(comment_info)\n    print(f'총 {len(comments)}개의 댓글 처리 완료.')\n    write_json_to_s3(comments, bucket_name, key_name)\n```\n\nyoutube_dag.py\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom youtube_comments import start_process\n\n# 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 5, 16),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DAG 객체 생성\ndag = DAG(\n    'youtube_python_operator_dag',\n    default_args=default_args,\n    description='Python 함수를 호출하는 간단한 DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n# PythonOperator 작업 생성\npython_task = PythonOperator(\n    task_id='my_python_task',\n    python_callable=start_process,\n    dag=dag,\n)\n\n# 작업 간 의존성 정의\npython_task\n\n# DAG 등록\ndag\n```\n\n위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.\n\n좋아요!!\n\n이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png"},"coverImage":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png","tag":["Tech"],"readingTime":8},{"title":"AI로 기후 변화를 고칠 수 있을까 데이터 전문가의 견해","description":"","date":"2024-05-18 18:01","slug":"2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd","content":"\n![image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_0.png)\n\n기후 변화는 짜증나는 주제입니다. 정치인들은 그에 대해 의미 있는 조치를 취하기에 열의를 보이지 않습니다. 대부분의 사람들은 우리와 같은 힘없는 존재로 느껴지며 어떻게 도울지 모릅니다.\n\n그럼에도 불구하고, 기후 변화는 일어나고 있으며 아마도 가속화되고 있습니다(이 블로그 게시물의 데이터에서 나중에 확인하겠지요). 우리는 매 여름이 지난 여름보다 더 덥다는 세계에 살고 있는 것 같군요.\n\n처음으로 태어난 세대로서 가끔씩 심각하게 생각할 때가 있습니다. 미래 기후 대재앙을 겪게 될 자녀들을 이 세상에 데리고 오는 것이 공정한 일인지에 대해 🤔. 한편, 어떤 사람들은 우리를 (우리로부터) 구해 줄 수 있는 초지능 AI에 기대를 걸어 놓고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 문제에 대한 명확성을 얻기 위해 기후 변화에 관한 가장 중요한 데이터 지점들을 수집했습니다. 그리고 이 블로그 포스트에서는 데이터 관련 열광적인 사람의 시각을 통해 현재 상황을 함께 공유하겠습니다 📈.\n\n시작해봅시다!\n\n👉 참고: 이 블로그 포스트의 비디오 버전을 시청하거나 제 Youtube 채널에서 데이터 보고서를 확인할 수도 있습니다:\n\n# 펼쳐지는 기후 이야기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 온실 효과\n\n2023년은 1850년 이후 기록된 가장 더운 해로, 이는 이례적인 해였습니다. 사실, 일부 데이터 세트가 제안하는 바에 따르면, 연간 평균 기온이 산업화 이전 기준 기간을 1.5°C 이상 초과하는 것은 이번이 처음입니다. 만약 이 용어가 익숙하지 않다면, 산업화 이전 시기는 1880년부터 1900년까지의 기준 기간입니다.\n\n이것이 그냥 이상 현상이거나 엘니뇨가 이 이례적으로 더운 해를 일으켰다고 주장할 수도 있습니다. 하지만 다음 그래프에서 볼 수 있듯이, 지난 4년 동안 평균 기온에서 매우 확고한 상승 추세를 볼 수 있습니다.\n\n![그래프](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파리협정 아래에서 많은 국가들이 장기적인 지구 온난화를 1.5°C(2.7°F) 이상으로 제한하려는 포부적인 목표를 세웠습니다. 이 목표는 많은 연도에 걸친 기후 상태를 기반으로 하기 때문에 1.5°C를 초과하는 단일 연도는 자동으로 이 목표를 위반한 것으로 간주되지 않습니다. 그러나 이것은 파리협정 목표를 초과하고 온클유어(Codesphere)글로벌 sandbox를 얼마나 가까이 왔는지에 대한 뚜렷한 경고 신호입니다.\n\n인간들이 대기 중에 더 많은 이산화탄소를 방출하면, 기후 온난화가 다음 10년 동안 정기적으로 1.5°C를 초과할 가능성이 높습니다.\n\n아래 그래프를 보면 대기 중 CO2 수준이 과거 백만 년 동안 어떤 시점에서도 점선 위로 올라가지 않았음을 알 수 있습니다. 그리고 빨간색으로 튀어나오는 이 스파이크는 지난 70년을 대표합니다. 나로서는 확실히 그 위에 앉고 싶지 않네요!\n\n![그래프 이미지](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지난 백 년 동안의 이산화탄소 배출 증가는 대부분 화석 연료 사용과 산업에 기인한다고 볼 수 있습니다. 다시 말해, 인간의 활동입니다.\n\n![Image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_3.png)\n\n기후 변화에 관한 정부간 기후변화협약 합동 심의 보고서에 따르면, 2030년까지 약속된 국가별 기여(NDCs)는 2030년대 초반에 온도가 1.5°C 상승할 것으로 나타났습니다. 만약 그게 사실이라면, 이세기 말까지 온도 상승을 2.0°C 이하로 유지하려고 노력할지도 모릅니다.\n\n기온 상승으로 인해 더위가 심한 지역은 더 덥게, 비가 많이 오는 지역은 더 많이 비가 오게 되며, 극한 기상 현상의 위험성과 강도는 크게 증가할 것으로 예상됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1.5°C 온난화 시, 지구 인구의 약 14%가 매 5년마다 적어도 한 번은 심한 폭염에 노출될 것으로 예상됩니다. 그러나 2°C 온난화 시, 해당 비율은 37%까지 늘어납니다. 극한 폭염 또한 보편화될 것으로 예상되며, 2°C 온난화 시에는 일부 국가에서 매년 치명적인 폭염이 발생할 수 있습니다.\n\n2°C를 넘는 온난화는 모든 극단적인 상황을 더욱 심각하게 만들 것으로 예상되며, 빈발하는 허리케인, 가뭄 및 산불이 포함됩니다. 더 많은 생태계가 심한 압력에 시달리게 될 것이며, 일부는 간단히 살아남지 못할 것입니다.\n\n인류에게는 먹을 음식을 생산해내기 어려워질 수도 있다는 의미입니다. 많은 사람들이 이주할 수 있고, 이는 국가를 불안정하게 만들 수 있습니다.\n\n## 녹는 얼음과 높아지는 바다수준\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n극단적인 날씨를 한쪽에 두고 또 다른 큰 문제가 있습니다: 극지방의 녹는 얼음입니다. 위 문제는 위성 자료와 지상 관측을 통해 모니터링할 수 있습니다.\n\n21세기 초부터 남극과 그린란드 빙하는 질량이 감소했습니다.\n\n![image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_4.png)\n\n남극과 그린란드 빙하는 지구의 물 중에서 99% 이상을 차지합니다. 이 둘이 완전히 녹는다면, 예상된 바다 수위 상승은 67.4미터 (223피트)에 이를 것으로 추정됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래서, 우리는 멸망했을까요? 기후 종말은 피할 수 없을까요? 인공지능은 어떨까요? 우리를 구할 수 있을까요?\n\n자, 이제 인공지능이 우리가 기후 변화와 싸우는 데 어떻게 도움을 줄 수 있는지에 대해 이야기해 봅시다!\n\n## 인공지능이 기후 변화 대응을 돕는 방법\n\n2023년과 2024년에는 인공지능 개발에서 엄청난 발전이 있었습니다. GPT4, Gemini, 그리고 많은 오픈 소스 언어 모델들이 대중에게 공개되면서 기후 변화 대응을 함께 할 수 있게 되었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI는 아직 새로운 기후 정책을 작성하고 시행할 수는 없습니다만 (아직), 정치인들이 그런 것을 허용하지 않을지도 모르겠군요. 그러면 여러분이 궁금해할 수도 있는데, AI가 실제로 우리가 많은 기후 문제에 대처하는 데 어떻게 도움을 줄 수 있을까요?\n\nAI는 가장 기본적인 수준에서 우리가 무슨 일이 벌어지고 있는지 이해하는 데 도움을 줄 수 있고 문제를 인정하는 데 도움이 될 수 있습니다. 제 연구에서 본 AI의 주요 활용은 다음과 같은 범주에 속합니다:\n\n- 모니터링;\n- 예측;\n- 최적화.\n\n## 온실가스 (GHG)의 실시간 모니터링을 향해\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기후 추적은 인공 지능과 기계 학습을 활용하여 전 세계적으로 온실 가스 배출량을 계산하는 이니셔티브로, 실시간 정확도를 향한 발전을 목표로 합니다. 이는 위치와 원천별 온실 가스 배출량에 대한 강력하고 무료이면서 독립적인 개요를 제공합니다. 저의 고향인 베트남 남부를 살펴보면, 석유 및 가스 분야에서 많은 배출량을 볼 수 있어 놀라지 않습니다. 또한 호치민 시 공항 주변에서 배출량이 집중되어 있는 것을 확인할 수 있습니다. 이 데이터는 Climate Trace 웹사이트에서 다운로드하여 자신의 연구에 활용할 수 있습니다. 정말 놀라운 일이죠!\n\n## 인공 지능이 빙산을 사람보다 10,000배 빨리 맵핑합니다\n\n또한, 연구자들은 위성 이미지에서 남극 빙산의 규모를 빠르고 정확하게 맵핑하고 모니터링하는 데 신경망 모델을 활용해왔습니다. 이는 빙산이 해양으로 녹는 양을 정량화하는 데 중요합니다.\n\n위성 이미지에서는 빙산, 해빙, 구름이 모두 하얗게 보여서 실제 빙산을 식별하기 어려운 경우가 많습니다. 그러나 신경망 모델은 이 작업을 훨씬 더 정확하고 효율적으로 처리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## AI를 활용한 쓰레기 재활용\n\nAI도 쓰레기 관리를 더 효율적으로 만들고 있어요. 쓰레기는 메탄의 큰 배출원이며 상당 수의 이산화탄소 배출을 담당하고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n물체 감지를 위해 머신 러닝 시스템을 사용하여 한 스타트업이 2022년에 67가지 종류의 쓰레기 범주에서 320억 개의 폐기물 항목을 추적했습니다. 회사는 회수할 수 있는 재료의 평균량으로 86톤을 식별했지만, 이 재료는 폐기물 처리장에 보내지고 있습니다. 전 세계의 대형 슈퍼마켓도 수요를 예측하고 이를 통해 폐기물을 줄이기 위해 AI를 사용하고 있습니다.\n\n## AI가 바다를 청소하고 있습니다\n\n네덜란드의 환경 단체인 The Ocean Cleanup은 바다로부터 플라스틱 오염물을 제거하는 데 도움을 주기 위해 AI와 다른 기술을 사용하고 있습니다.\n\n물체를 감지하는 신경망 알고리즘은 해당 조직이 원격 지역에 있는 해양 폐기물의 상세지도를 작성하는 데 도와주고 있습니다. 이 해양 폐기물은 그 후 수거 및 제거될 수 있으며, 이는 이전의 트롤러 및 항공기를 사용한 청소 방법보다 효율적입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 인공지능이 기후 재해를 예측하는 데 도움을 줍니다\n\n인공지능 모델은 온도와 기후 재해를 더 정확하게 예측하는 데 도움이 되었습니다. GraphCast라는 AI 모델은 10일간의 기상 예측을 제공하는 것뿐만 아니라 극단적인 기상 현상에 대한 빠른 경보도 제공합니다. 이 모델은 태풍의 경로를 예측하고 홍수 위험과 관련된 대기 강과 극한 온도의 발생을 예측할 수 있습니다. 이는 생명을 구할 수 있는 잠재력이 있다는 것을 의미합니다.\n\n지금까지 저는 AI가 주로 기계 학습 도구로 사용되어왔다고 느꼈고, AI가 자체적으로 새로운 해결책을 만드는 데 사용되는 것을 보지 못했습니다. 앞으로, 만약 우리가 인공 일반 지능을 갖게 된다면, 이러한 종류의 AI가 더 많은 일을 할 수도 있을 것입니다.\n\n# 어떻게 인공지능이 상황을 악화시키고 있는지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 몇몇 회의론자들(source 1, source 2)은 AI가 \"행성을 구하는 데 도움이 될 것\"에 너무 낭만적으로 생각해서는 안 된다고 생각합니다. 그들은 인공 지능이 기후 위기 해결에 도움이 될 것이라는 주장이 잘못되었다고 믿습니다.\n\nGPT4와 Gemini 같은 모델은 훈련하고 실행하는 데 엄청난 양의 에너지가 필요합니다.\n\n![image](/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_5.png)\n\nAI 모델의 탄소 발자국을 정확하게 계산하는 것도 어렵습니다. 왜냐하면 OpenAI나 Google과 같은 기업들은 보통 자신들의 모델에 대한 상세 명세를 게시하지 않기 때문입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가장 간단한 형태로, AI 모델의 탄소 발자국은 모델을 훈련하는 데 필요한 에너지에 쿼리의 수와 각 쿼리가 필요로 하는 에너지를 더한 것과 같습니다. 이 모든 것은 하드웨어의 에너지 효율성에 의해 곱해질 것입니다.\n\n탄소 발자국 = (전기 에너지 훈련 + 쿼리 수 × 전기 에너지 추론) × CO2e 데이터 센터/KWh,\n\n여기서 CO2e 데이터 센터는 데이터 센터의 CO2 효율성을 의미합니다.\n\n대부분의 기업은 AI 모델을 제공하는 데 (추론 수행) 훈련하는 것보다 훨씬 더 많은 에너지를 사용합니다. 실제로, 에너지의 90%가 제공하는 데 사용된다고 추정됩니다. AI의 전기 수요는 산업과 발걸음을 맞추기 위해 데이터 센터의 2배 증설이 필요하다는 의미입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n미국에서는 AI가 막대한 양의 전력을 필요로 하기 때문에 에너지 수요를 충족하기 위해 여전히 오래된 석탄 발전소가 필요합니다. 정말 무서운 일이죠!\n\nOpenAI와 Google과 같은 기업은 자사 모델의 환경 영향을 일반적으로 공개하지 않습니다. 현재까지는 추측과 예측만 있습니다. 나는 대규모 AI 모델에 대한 환경 영향 보고가 규제되어야 한다고 생각합니다.\n\n# 지난 수십 년간의 추이\n\n최근 몇 년간 화석 연료의 증가 추세가 있는지 확인하기 위해 지난 10년간의 에너지 소비를 더 자세히 살펴보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n풍력 및 태양 에너지와 같은 재생 에너지 생산을 더 싸고 매력적으로 만드는 기술들도 있습니다. 많은 사람들이 친환경적인 솔루션을 선호하면서 전기 자동차가 많은 유럽 국가에서 일상적인 것으로 자리 잡았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모든 것이 잘되었다는 것을 의미하는 것은 아니며, AI 모델의 발전이 환경에 눈에 띄는 피해를 일으킨 적이 없다는 것은 아닙니다. 이것은 앞으로 몇 년 동안 더 명백하게 관찰될 수 있을 것으로 예상됩니다.\n\n# 결론\n\n우리가 본 바와 같이, 우리가 조심하지 않으면 AI는 이중날을 수도 있습니다. 현재는 혜택이 비용을 상쇄하는지에 대해 명확한 증거가 충분하지 않습니다.\n\n한편, 데이터는 우리가 아무것도 하지 않으면 기후 변화가 어떻게 진행될지 명백하게 보여줍니다. 그러나 우리는 해결책과 인간의 창의력에 한이 없음을 보았습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n많은 개인, 스타트업, 그리고 기관들이 기후 위험의 다양한 측면을 해결하기 위해 노력하고 있어요.\n\n이것이 저에게 매우 긍정적인 느낌을 줍니다. 세상을 바꾸고 싶다면, 먼저 변화가 가능하다고 믿어야 하며 어떤 문제도 해결하기에는 너무 크지 않다고 생각해야 합니다. 더 많은 젊은 사람들이 영향력 있는 자리로 진출하면서 기후 변화를 우선순위로 삼고 새로운 해결책에 열을 올리고 있어요. 기대를 해봅니다. 수 년 후에는 인공지능이 우리에게 미래 기후를 위한 중요한 한걸음을 내딛도록 도와줄 것이며, 혜택이 실제로 비용을 상회할 것이라고 믿어요.\n\n더 많은 정보를 찾고 싶다면, 아래의 Datalore 노트북에 있는 데이터 보고서를 확인해보세요!\n\n# 데이터 소스\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. 1850년부터 1900년까지의 전 세계 평균 기온(ºC)\n\n- Met Office: [링크](https://climate.metoffice.cloud/temperature.html#datasets)\n\n2. 지난 800,000년 동안 대기 중 CO2 수준\n\n- 모든 연구: [링크](https://www.ncei.noaa.gov/access/paleo-search/study/17975)\n- 수정된 코어 CO2 데이터 800,000–현재까지(2001): 남극 빙하 코어 수정된 복합 및 개별 코어 CO2 데이터\n- 2002년부터 2023년까지의 기간에는 여기에서 사용 가능한 현대 계기 데이터를 사용해야 합니다: [링크](https://climate.nasa.gov/vital-signs/carbon-dioxide/)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 세계 온실 가스 배출\n\n- [ourworldindata.org/greenhouse-gas-emissions](https://ourworldindata.org/greenhouse-gas-emissions)\n\n4. 국가별 석탄, 석유 및 가스 소비\n\n- [ourworldindata.org/fossil-fuels](https://ourworldindata.org/fossil-fuels)\n- 석탄 소비에 대한 정보: [ourworldindata.org/grapher/coal-consumption-per-capita?tab=chart&country=High-income+countries~Upper-middle-income+countries~Lower-middle-income+countries](https://ourworldindata.org/grapher/coal-consumption-per-capita?tab=chart&country=High-income+countries~Upper-middle-income+countries~Lower-middle-income+countries)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n5. Antarctic 및 Greenland 빙하의 녹는 얼음 시트: 1992-2020 IPCC AR6를 위한 빙하량 균형\n\n- [https://ramadda.data.bas.ac.uk/repository/entry/show?entryid=77b64c55-7166-4a06-9def-2e400398e452](https://ramadda.data.bas.ac.uk/repository/entry/show?entryid=77b64c55-7166-4a06-9def-2e400398e452)\n- [https://www.epa.gov/climate-indicators/climate-change-indicators-ice-sheets](https://www.epa.gov/climate-indicators/climate-change-indicators-ice-sheets)\n\n6. LLMs 훈련 비용\n\n- [https://tinyml.substack.com/p/the-carbon-impact-of-large-language](https://tinyml.substack.com/p/the-carbon-impact-of-large-language)\n- [https://www.statista.com/statistics/1384418/co2-emissions-when-training-llm-models/](https://www.statista.com/statistics/1384418/co2-emissions-when-training-llm-models/)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nhttps://blog.jetbrains.com에 원래 게시되었습니다. 2024년 4월 10일.\n","ogImage":{"url":"/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_0.png"},"coverImage":"/assets/img/2024-05-18-CanAIFixClimateChangePerspectiveofaDataNerd_0.png","tag":["Tech"],"readingTime":15},{"title":"위치 기반의 위성 이미지의 시계열을 표시하는 대화형 지도 만들기","description":"","date":"2024-05-18 17:58","slug":"2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery","content":"\n![image](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_0.png)\n\n# Table of Contents\n\n- 🌟 Introduction\n- 📌 Area of Interest (AOI)\n- 💾 Loading Sentinel-2 Imagery\n- ⏳ Extracting Time Series from Satellite Imagery\n- 🌍 Developing an Interactive Map with Time Series\n- 📄 Conclusion\n- 📚 References\n\n## 🌟 Introduction\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한동안 사용자가 특정 위치를 클릭할 때 상호작용 맵에 시계열 데이터가 표시되는 쉽고 직관적인 방법을 찾고 있었습니다. 저는 folium 라이브러리를 탐색하고 위성 이미지에서 추출한 시계열 데이터베이스를 folium과 연결하는 방법을 알아냈습니다. 오늘은 내 방법을 공유하겠습니다.\n\n이 게시물에서는 두 가지 함수를 작성할 것입니다. 첫 번째 함수는 위성 데이터를 다운로드하지 않고 로드하고, 두 번째 함수는 데이터와 타임스탬프를 추출하여 데이터 형식의 시계열을 생성합니다. 그런 다음 AOIs(관심 지역)를 순환하여 두 함수를 실행하고 AOI에 대한 시계열 데이터를 추출할 것입니다. 마지막으로 생성된 시계열 데이터와 folium 라이브러리를 사용하여 이를 상호작용적인 지도에 지리적으로 표시할 것입니다.\n\n이 게시물을 마치면 어떤 변수나 매개변수에 대해 추출된 시계열 데이터를 상호작용적인 맵과 시각적으로 표시할 수 있게 될 것입니다. 예를 들어, 캘리포니아 호수 면적의 시계열을 추출하고 상호작용적 지도에 표시하겠습니다. 그러나 흥미를 갖거나 이러한 조언과 꿀팁을 찾고 있었다면 계속 읽어보세요!\n\n## 📌 관심 지역 (AOI)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n소개에서 언급한 대로 상호작용적 지도에서 어떤 위치의 변수에 대한 시계열 데이터를 표시하려면 다음 단계를 따를 수 있습니다. 이 예시에서는 캘리포니아의 몇 개 호수의 물 픽셀을 계수하고 2024년에 촬영된 모든 Sentinel-2 이미지를 사용하여 표면적을 계산할 것입니다. QGIS에서 그 호수 주변에 폴리곤을 그리고 그것들을 shapefile로 저장했습니다. 관심 영역 (AOI)에 대한 shapefile을 만드는 방법을 배우고 싶다면, Medium의 첫 번째 스토리에서 \"🛠️ QGIS에서 Shapefile 생성\"이라는 섹션을 참조해주세요.\n\n다음은 QGIS에서 호수 주변에 그린 폴리곤의 스냅샷입니다:\n\n![lakes](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_1.png)\n\n## 💾 Sentinel-2 영상 불러오기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 섹션의 목표는 다운로드 없이 아카이브된 위성 이미지를 메모리에 로드하는 것입니다. 특정 지역에 대한 오랜 기간에 걸친 위성 이미지 다운로드는 시간이 많이 소요되며 계산 비용이 많이 들며 비효율적일 수 있습니다. 특히 전체 장면에서 작은 영역의 변화를 탐색하려면 문제가 될 수 있습니다.\n\n이러한 문제를 극복하기 위해 12줄의 코드만 사용하여 다운로드 없이 위성 이미지를 로드하는 방법을 보여주는 포스트를 작성했습니다. 해당 포스트에서 확인할 수 있습니다.\n\n이 섹션에서는 해당 포스트에서 제시된 템플릿을 사용하여 함수를 작성할 것입니다. 이 함수를 사용하면 특정 기간에 대한 AOI 위성 이미지를 쉽게 로드할 수 있습니다. 해당 기간이 길든 짧든 상관없이:\n\n```python\nfrom pystac_client import Client\nfrom odc.stac import load\n\ndef search_satellite_images(collection=\"sentinel-2-l2a\",\n                            bbox=[-120.15,38.93,-119.88,39.25],\n                            date=\"2023-01-01/2023-03-12\",\n                            cloud_cover=(0, 10)):\n    \"\"\"\n    Collection, 범위, 날짜 범위 및 구름 덮개를 기반으로 위성 이미지를 검색합니다.\n\n    :param collection: Collection 이름 (기본값: \"sentinel-2-l2a\").\n    :param bbox: 경계 상자 [min_lon, min_lat, max_lon, max_lat] (기본값: Lake Tahoe 지역).\n    :param date: 날짜 범위 \"YYYY-MM-DD/YYYY-MM-DD\" (기본값: \"2023-01-01/2023-12-30\").\n    :param cloud_cover: 구름 덮개 범위를 나타내는 Tuple (최소, 최대) (기본값: (0, 10)).\n    :return: 검색 기준에 따라 로드된 데이터.\n    \"\"\"\n    # 검색 클라이언트 정의\n    client=Client.open(\"https://earth-search.aws.element84.com/v1\")\n    search = client.search(collections=[collection],\n                            bbox=bbox,\n                            datetime=date,\n                            query=[f\"eo:cloud_cover<{cloud_cover[1]}\", f\"eo:cloud_cover>{cloud_cover[0]}\"])\n\n    # 일치하는 항목 수 출력\n    print(f\"발견된 이미지 수: {search.matched()}\")\n\n    data = load(search.items(), bbox=bbox, groupby=\"solar_day\", chunks={})\n\n    print(f\"데이터에서의 날짜 수: {len(data.time)}\")\n\n    return data\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 함수를 사용하면 위성 이미지 검색을 위한 매개변수를 지정할 수 있어서 다양한 관심 영역 및 시간대에 유연하고 쉽게 사용할 수 있습니다. 수집, 경계 상자, 날짜 범위 및 구름 양 등과 같은 기준에 따라 위성 이미지를 검색합니다. 이는 이미지를 찾기 위해 Earth Search API를 사용하고 일치하는 수를 출력하며 큐브 형식으로 클립 된 이미지를 반환합니다.\n\n## ⏳ 위성 영상에서 시계열 추출\n\n이제 AOI를 위해 클립 된 이미지를로드하는 함수가 있으므로 찾고있는 정보를 추출하는 두 번째 함수를 정의해야합니다. 이미지와 함께 필요한 정보를 추출하고 맵에 표시하는 다음 단계에서 사용할 수 있도록 DataFrame에 넣어 시계열 데이터베이스로 고려할 수 있습니다. 다시 한 번 필요한 데이터를 추출 할 수 있지만, 전체 캘리포니아 호수의 표면적을 볼 수있는 것이 흥미로울 것으로 생각되어 2024 년에 Sentinel-2로 촬영 된 모든 이미지에서 최근 이미지를 포함하여 주요 캘리포니아 호수의 표면적을 볼 수 있습니다.\n\n이를 위해 Sentinel-2 이미지의 씬 분류 레이어에 있는 물 픽셀로 분류 된 픽셀을 추출 할 수 있습니다. 다시 말해, 각 씬에서 물 픽셀 수를 세어야합니다. 픽셀 해상도가 10m x 10m임을 감안하면 수를 100 제곱 미터 (10m x 10m)로 곱하면 각 호수의 표면적이 나올 것입니다. 그러나 여기서는 위성에 의해 촬영 된 이미지가 각 씬에서 전체 호수를 커버하도록해야합니다. 이를 설명하기 위해 1 월 7 일과 1 월 4 일에 촬영 된 이 두 장의 이미지 중 하나에 캡처된 호수 중 하나를 살펴 보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef count_water_pixels(data,lake_id):\n    \"\"\"\n    각 시점의 Sentinel-2 SCL 데이터에서 물 픽셀 수를 계산합니다.\n\n    :param data: Sentinel-2 SCL 데이터가 포함된 xarray Dataset입니다.\n    :return: 날짜, 물 횟수 및 눈 횟수가 포함된 DataFrame입니다.\n    \"\"\"\n\n    water_counts = []\n    date_labels = []\n    water_area = []\n    coverage_ratio = []\n\n    # 시간 단계 수를 확인합니다.\n    numb_days = len(data.time)\n\n    # 각 시간 단계를 반복합니다.\n    for t in range(numb_days):\n        scl_image = data[[\"scl\"]].isel(time=t).to_array()\n        dt = pd.to_datetime(scl_image.time.values)\n        year = dt.year\n        month = dt.month\n        day = dt.day\n\n        date_string = f\"{year}-{month:02d}-{day:02d}\"\n        print(date_string)\n\n        # 물에 해당하는 픽셀 수를 계산합니다.\n        count_water = np.count_nonzero(scl_image == 6)  # 물\n\n        surface_area = count_water * 10 * 10 / (10 ** 6)\n\n        count_pixels = np.count_nonzero((scl_image == 1) | (scl_image == 2) | (scl_image == 3) | (scl_image == 4) | (\n                    scl_image == 5) | (scl_image == 6) | (scl_image == 7) | (scl_image == 8) | (scl_image == 9) | (\n                                               scl_image == 10) | (scl_image == 11))\n        total_pixels = data.dims['y'] * data.dims['x']\n\n        coverage = count_pixels * 10 * 10 / 1e6\n        lake_area = total_pixels * 10 * 10 / 1e6\n\n        ratio = coverage / lake_area\n\n        print(coverage)\n        print(lake_area)\n        print(ratio)\n\n        if ratio < 0.8:\n            continue\n\n        # 추가\n        water_counts.append(count_water)\n        date_labels.append(date_string)\n        water_area.append(surface_area)\n        coverage_ratio.append(ratio)\n\n    # 날짜 레이블을 pandas datetime 형식으로 변환합니다.\n    datetime_index = pd.to_datetime(date_labels)\n\n    # DataFrame을 구성하기 위한 딕셔너리 생성\n    data_dict = {\n        'Date': datetime_index,\n        'ID': lake_id,\n        'Water Counts': water_counts,\n        'Pixel Counts': count_pixels,\n        'Total Pixels': total_pixels,\n        'Coverage Ratio': coverage_ratio,\n        'Water Surface Area': water_area\n    }\n\n    # DataFrame 생성\n    df = pd.DataFrame(data_dict)\n\n    return df\n```\n\n이 함수는 데이터셋의 각 시간 단계를 반복하여 물 픽셀 수를 계산하고 표면적을 계산하며 커버리지 비율을 계산합니다. 커버리지 비율이 80% 미만이면 시간 단계가 건너뜁니다. 그런 다음 횟수, 날짜, 표면적 및 커버리지 비율을 리스트에 추가하고 해당 값과 물 ID 및 총 픽셀 수가 포함된 DataFrame을 반환합니다.\n\n커버리지 문제와 해결하는 속임수에 대해 자세히 알아보려면 이 포스트의 섹션 (📈 통계 파일에서 대염해 지역의 시계열)을 참조해주세요:\n\n## 🌍 시계열과 함께 상호작용하는 지도 개발하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 섹션에서는 세 개의 스크립트를 작성할 것입니다. 첫 번째 스크립트는 다각형(AOI)의 바운딩 박스와 중심 좌표를 추출하는 함수입니다. 첫 번째 함수(search_satellite_images)를 실행하려면 바운딩 박스가 필요하며, 맵에 호수를 표시하는 데 중심 좌표가 필요합니다. 다음 코드로 이 작업을 수행할 수 있습니다:\n\n```js\nimport geopandas as gpd\nimport pandas as pd\n\ndef get_centroids_and_bboxes(shapefile_path):\n    \"\"\"\n    shapefile을 처리하여 각 다각형의 ID, 중심점, 바운딩 박스(bbox)를 포함하는 DataFrame을 반환합니다.\n    :param shapefile_path: shapefile의 경로.\n    :return: 각 다각형의 ID, 중심점, 및 bbox가 있는 pandas DataFrame.\n    \"\"\"\n\n    # shapefile 불러오기\n    gdf = gpd.read_file(shapefile_path)\n\n    # EPSG:4326으로 재투영\n    gdf_proj = gdf.to_crs(\"EPSG:4326\")\n\n    centroids = []\n    bboxes = []\n\n    # 각 다각형을 처리하여 중심점과 bbox 얻기\n    for index, row in gdf_proj.iterrows():\n        # 중심점\n        centroid_lat = row.geometry.centroid.y\n        centroid_lon = row.geometry.centroid.x\n        centroids.append((centroid_lat, centroid_lon))\n\n        # 바운딩 박스\n        minx, miny, maxx, maxy = row.geometry.bounds\n        bbox = (minx, miny, maxx, maxy)\n        bboxes.append(bbox)\n\n    # DataFrame 생성\n    df = pd.DataFrame({\n        'ID': gdf_proj.index,\n        'Centroid_Lat': [lat for lat, lon in centroids],\n        'Centroid_Lon': [lon for lat, lon in centroids],\n        'BBox_Min_Lon': [bbox[0] for bbox in bboxes],\n        'BBox_Min_Lat': [bbox[1] for bbox in bboxes],\n        'BBox_Max_Lon': [bbox[2] for bbox in bboxes],\n        'BBox_Max_Lat': [bbox[3] for bbox in bboxes]\n    })\n\n    return df\n\nshapefile_path = \"lakes_boundry.shp\"\nlakes_df = get_centroids_and_bboxes(shapefile_path)\nprint(lakes_df)\n```\n\n위 단계를 따르고 코드를 성공적으로 실행하면, 다음 형식의 다각형에 대한 유사한 DataFrame이 표시될 것입니다:\n\n<img src=\"/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_4.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음 스크립트는 2024년 센티넬-2에 의해 촬영된 모든 이미지를 호수 위에서 루핑하고 두 번째 함수를 실행하여 커버리지 비율이 80%보다 높은 경우 각 이미지에서 표면적을 계산하며 각 호수의 표면적을 시계열로 보여주는 DataFrame을 보고하는 것을 포함합니다:\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\n\nall_water_pixels_dfs = []\n\nfor lake_id in lakes_df.ID:\n    print(lake_id)\n    lake_df = lakes_df[lakes_df['ID'] == lake_id]\n\n    if not lake_df.empty:\n        bbox = [lake_df.iloc[0].BBox_Min_Lon, lake_df.iloc[0].BBox_Min_Lat,\n                lake_df.iloc[0].BBox_Max_Lon, lake_df.iloc[0].BBox_Max_Lat]\n\n        data = search_satellite_images(collection=\"sentinel-2-l2a\",\n                                       date=\"2024-01-01/2024-05-14\",\n                                       cloud_cover=(0, 5),\n                                       bbox=bbox)\n        # Pass the lake_id\n        water_pixels_df = count_water_pixels(data, lake_id)\n\n        # Append\n        all_water_pixels_dfs.append(water_pixels_df)\n\n# Concatenate all DataFrames into a single DataFrame\nfinal_df = pd.concat(all_water_pixels_dfs, ignore_index=True)\n```\n\n최종 DataFrame은 이미지 날짜, 물 픽셀 수, 총 픽셀 수, 커버리지 비율 및 표면적을 요약하여 다음과 같이 보여집니다:\n\n![이미지](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n거의 다 왔어요!\n\n지도 상에서 시계열을 보기 위해 마지막 한 단계가 남았습니다. 이제 표면적의 시계열 데이터가 있으므로 Folium 라이브러리를 사용하여 두 가지를 표시할 수 있습니다: (1) 지도상의 호수 중심을 지점으로 표시하고 (2) 각 호수를 클릭하면 팝업으로 표면적의 시계열을 보여주는 그래프를 표시합니다. 다음 코드로 이 작업을 수행할 수 있습니다:\n\n```js\nimport folium\nimport plotly.express as px\nimport os\n\n# 시계열 플롯을 그려 HTML로 저장하는 함수\ndef plot_timeseries_for_spot(spot_id, ts_df):\n    df_spot = ts_df[ts_df['ID'] == spot_id]\n    print(df_spot)\n    fig = px.line(df_spot, x='Date', y='Water Surface Area', title=f'Time Series for Lake {spot_id}')\n\n     # X 및 Y 축 레이블 추가\n    fig.update_layout(\n        xaxis_title=\"Date\",\n        yaxis_title=\"Water Surface Area (sq km)\"\n    )\n\n    filepath = f'tmp_{int(spot_id)}.html'\n    fig.write_html(filepath, include_plotlyjs='cdn')\n    return filepath\n\n# 지도 생성\nm = folium.Map(location=[35.5, -119.5], zoom_start=7)\n\n# Plotly 시계열 팝업이 있는 마커 추가\nfor index, row in lakes_df.iterrows():\n    html_path = plot_timeseries_for_spot(row['ID'], final_df)\n    iframe = folium.IFrame(html=open(html_path).read(), width=500, height=300)\n    popup = folium.Popup(iframe, max_width=2650)\n    folium.Marker([row['Centroid_Lat'], row['Centroid_Lon']], popup=popup).add_to(m)\n\nm.save('map_with_timeseries.html')\n\n# 임시 HTML 파일 정리\nfor spot_id in lakes_df['ID']:\n    os.remove(f'tmp_{spot_id}.html')\n```\n\n이 스크립트에서는 함수가 각 호수의 시계열 데이터를 필터링하고, Plotly를 사용하여 라인 플롯을 생성하고, 플롯을 HTML 파일로 저장합니다. 다음으로 Folium을 사용하여 지도를 초기화합니다. 그런 다음 호수 DataFrame을 반복하면서, 각 호수의 중심 좌표에 마커를 추가하고, 각 마커에 팝업을 연결하여 시계열 플롯을 표시합니다. 최종 지도는 HTML 파일로 저장됩니다. 마지막으로, Plotly 플롯에 생성된 임시 HTML 파일을 삭제하여 정리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n완료되었습니다!\n\n콘텐츠 폴더에 생성된 HTML 파일을 열면 지도에 표시된 각 호수의 중심 좌표를 볼 수 있습니다.\n\n![지도](/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_6.png)\n\n각 호수를 클릭하여 시계열이 표시되는지 확인해 보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 모든 노력 끝에 이렇게 실용적인 지도가 만들어졌네요, 맞나요? :D\n\n## 📄 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n거의 매달 새로운 패키지와 라이브러리들이 나와서 데이터를 추출하고 분석하며 표시하고 시각화하는 법을 실용적으로 제공합니다. 그러나 이 분야에서 아직 남아 있는 두 가지 과제가 있습니다. 첫 번째는 데이터를 정확하게 분석하기 위해서는 테라바이트 또는 페타바이트에서 추출된 데이터가 정확한지 확인하기 위해 충분한 경험이 필요합니다. 두 번째는 이러한 라이브러리들을 연결하여 의미 있는 것을 만들어내는 아키텍처를 만드는 것입니다.\n\n이미지 처리에서는 처리된 데이터에서의 간단한 실수가 중대한 오류로 이어질 수 있는 점을 강조해보았습니다. 시각화 부분에서는 Folium, Plotly, 그리고 새로운 이미지를 추출하기 위한 API를 연결하여, 리모트 센싱 관측을 사용하여 다양한 현상을 모니터링하는 유용한 도구를 만들 수 있음을 보여주었습니다. 이 글을 읽는 데 즐거움을 느끼시기를 바라며, 궁금한 사항이 있으시면 언제든지 연락 주세요.\n\n## 📚 참고 자료\n\nhttps://github.com/stac-utils/pystac-client/blob/main/docs/quickstart.rst\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nhttps://www.element84.com/earth-search/examples/\n\nSentinel 데이터용 Copernicus Sentinel 데이터 [2024]\n\nCopernicus 서비스 정보용 Copernicus 서비스 정보 [2024]\n\n📱 더 많은 흥미로운 콘텐츠를 제공하는 다른 플랫폼에서 저와 소통하세요! LinkedIn, ResearchGate, Github 및 Twitter.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 링크를 통해 확인할 수 있는 관련 게시물이 있습니다:\n","ogImage":{"url":"/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_0.png"},"coverImage":"/assets/img/2024-05-18-CreateanInteractiveMaptoDisplayTimeSeriesofSatelliteImagery_0.png","tag":["Tech"],"readingTime":16}],"page":"96","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true}