{"pageProps":{"posts":[{"title":"도커 기본 요약 시트","description":"","date":"2024-05-27 17:20","slug":"2024-05-27-DockerBasicCheatSheet","content":"\n![Docker Basic Cheat Sheet](/assets/img/2024-05-27-DockerBasicCheatSheet_0.png)\n\n# Basic Commands:\n\n## Container Lifecycle:\n\n- docker run: Create and start a container.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n$ docker run -d --name my_container nginx\n```\n\ndocker start/stop/restart: 컨테이너를 시작, 중지 또는 재시작합니다.\n\n```js\n$ docker stop my_container\n$ docker start my_container\n$ docker restart my_container\n```\n\ndocker ps: 실행 중인 컨테이너 목록을 표시합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n$ docker ps\n```\n\ndocker ps -a 명령어를 사용하면 모든 컨테이너(중지된 것 포함)를 보여줍니다.\n\n```js\n$ docker ps -a\n```\n\n## 이미지 관리:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 풀: 레지스트리에서 이미지를 다운로드합니다.\n\n```js\n$ docker pull ubuntu\n```\n\n도커 빌드: Dockerfile에서 이미지를 빌드합니다.\n\n```js\n$ docker build -t my_image .\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 이미지: 모든 로컬 이미지를 목록으로 확인할 수 있어요.\n\n```js\n$ docker images\n```\n\n도커 rmi: 이미지를 삭제할 수 있어요.\n\n```js\n$ docker rmi my_image\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 컨테이너 작업:\n\n## 컨테이너와 상호 작용하기:\n\n도커 exec: 실행 중인 컨테이너에서 명령을 실행합니다.\n\n```js\n$ docker exec -it my_container bash\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 첨부: 실행 중인 컨테이너에 연결합니다.\n\n```js\n$ docker attach my_container\n```\n\n도커 로그: 컨테이너 로그를 확인합니다.\n\n```js\n$ docker logs my_container\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 컨테이너 자원 관리:\n\n도커 복사: 컨테이너와 호스트 간 파일 복사.\n\n```js\n$ docker cp file.txt my_container:/path/to/destination\n```\n\n도커 일시정지/재개: 실행 중인 컨테이너를 일시정지하거나 다시 시작합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n$ docker pause my_container\n$ docker unpause my_container\n\ndocker inspect: 디테일한 컨테이너 정보 표시\n\n$ docker inspect my_container\n\n# 네트워킹:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 네트워킹:\n\n도커 네트워크 목록: 사용 가능한 네트워크를 나열합니다.\n\n```js\n$ docker network ls\n```\n\n도커 네트워크 생성: 새 네트워크를 생성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n$ docker network create my_network\n```\n\n도커 네트워크 연결/해제: 컨테이너를 네트워크에 연결하거나 연결을 해제합니다.\n\n```js\n$ docker network connect my_network my_container\n$ docker network disconnect my_network my_container\n```\n\n# 볼륨 관리:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 볼륨:\n\n도커 볼륨 목록: 볼륨 목록을 표시합니다.\n\n```js\n$ docker volume ls\n```\n\n도커 볼륨 생성: 볼륨을 생성합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n$ docker volume create my_volume\n```\n\n도커 볼륨 삭제: 볼륨 제거하기.\n\n```js\n$ docker volume rm my_volume\n```\n\n도커 볼륨 조회: 자세한 볼륨 정보 표시하기.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```sh\n$ docker volume inspect my_volume\n```\n\n# 친절한 영어로 🚀\n\nIn Plain English 커뮤니티에 참여해주셔서 감사합니다! 떠나시기 전에:\n\n- 작가를 박수로 응원하고 팔로우 해주세요 ️👏️️\n- 팔로우해주세요: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼도 방문해주세요: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io에서 더 많은 콘텐츠를 만나보세요\n","ogImage":{"url":"/assets/img/2024-05-27-DockerBasicCheatSheet_0.png"},"coverImage":"/assets/img/2024-05-27-DockerBasicCheatSheet_0.png","tag":["Tech"],"readingTime":6},{"title":"도커 포트 정말로 노출되는 것은 무엇인가요","description":"","date":"2024-05-27 17:18","slug":"2024-05-27-DockerPortsWhatAreYouReallyPublishing","content":"\n## 포트나 보안에 대해서만 하는 것이 아니라, 키보드 뒤에 있는 사람들이 중요해요.\n\n하나의 명령어로 전체 애플리케이션, 환경 및 의존성을 모두 구축하는 것은 꿈 같은 일이에요. Docker가 어떻게 작동하는지 안다면, 응용 프로그램을 안전하게 배포하는 놀라운 도구에요.\n\n하지만 처음 써보는 사람들에게는 방화벽 설정을 모두 우회하고 컨테이너를 공개 인터넷에 노출시키는 일이 무서울 수 있어요. 최근 Docker 네트워킹 문서를 읽다가 페이지에 큰 주황색 경고문을 보고, Docker를 처음 사용했을 때 놀랐던 일을 떠올렸어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사는 도커의 이 특정 문제에 초점을 맞추고 있지만, 제 제기 사항은 실제로 도커와는 별개입니다. 도커가 현재의 방식으로 작동하는 이유는 충분히 타당합니다. 문서화된 정보도 상당히 많이 있습니다. 네트워킹 페이지의 선명한 경고가 있고, 조금 더 깊이 읽어보면 도커와 방화벽이 상호작용하는 내용을 명시적으로 설명하는 단락을 찾을 수 있습니다.\n\n이 기사는 이 문제를 중심으로 한 응답과 주변의 개발자 태도에 대한 제 관찰에 관한 이야기입니다. 제 불평은 소프트웨어 개발 산업이 어떻게 도구를 사용하지만 작동 방식을 이해하지 않는 개발자를 만들기 위해 구성되어 있으며, 그렇게 하면 결국 자신의 발밑을 책임질 때 비판한다는 것입니다.\n\n응용 프로그램을 개발하고 배포하는 데 큰 진입 장벽이 없습니다. 건축물을 지으려면 통과해야 하는 문과 제한이 있습니다. 건축이 시작되기 전에 승인을 받아야 하며 안전, 환경 및 용도 관련 법규를 준수하기 위해 계획이 승인되어야 합니다. 전기, 배관 및 가스와 같은 중요한 시스템들은 특히 규정 준수를 위해 건물이 점검되기 전까지 사용될 수 없습니다. 반면에 소프트웨어를 작성하고 배포하는 것을 아무도 막지 않습니다. 이 자유는 우리의 직업의 장점이지만, 동시에 양날의 검이기도 합니다.\n\n전문적인 세계에서 안전한 응용 프로그램을 보장하기 위해 경험 많은 개발자들로부터의 코드 리뷰, 보안 감사, 침투 테스트 및 다른 점검들이 이상적으로 이루어져야 합니다. 그러나 놀이삼아 응용 프로그램을 작성한 개인 개발자들 중 모든 사람이 이러한 점검을 수행할 노하우를 가지고 있거나 수행해야 한다는 것을 알지 못할 수도 있습니다. 바로 이러한 개발자들을 이 기사에서 다루고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 문제\n\n이 문제를 처음 마주했던 상황을 알려드리겠습니다. 사이드 프로젝트를 위해 공개 데이터 세트로 내 데스크톱에서 작은 MySQL 데이터베이스를 실행했습니다. 데이터베이스는 3306 포트에서 작동 중이었지만 방화벽 설정으로 모든 수신 연결을 차단하였습니다. 데이터를 다운로드하고 원본 소스의 데이터를 업데이트하여 필요한 새로운 데이터를 작업할 수 있도록 로컬 데이터베이스를 갱신하는 작은 Python 스크립트를 실행할 수 있었습니다. 시간이 지남에 따라 MySQL의 설치된 버전에서 Docker 컨테이너로 전환하였습니다. Python 스크립트를 변경하지 않고 계속 동작하게 하기 위해 `docker run` 명령에 `-p 3306:3306` 인수를 추가하기만 했습니다.\n\n특히 Docker 네트워킹 문서에서 이전에 언급한 큰 주황색 경고를 읽었을 때 기억합니다:\n\n“와우, 분명히 중요하군요! 사람들이 읽도록 큰 주황색 경고로 표시되어 다행이에요. 방화벽 설정을 다시 확인해야겠네요.”\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n어느 날, 전혀 관련 없는 네트워크 문제를 디버깅하고 있었어요 — 아마도 노트북을 사용해서 Samba 공유를 디버깅하려고 다른 방에 있었던 것 같아요. 데스크톱에서 어떤 포트가 열려 있는지 확인하기 위해 nmap을 실행했는데, 방화벽에 3306번 포트에 대한 연결을 거부하는 명시적인 규칙이 있음에도 불구하고 열려 있다는 것을 발견했을 때 정말 놀랐어요.\n\n다행히 제 경우에는 집 네트워크로만 제한되어 있었어요. 데이터베이스는 비밀번호로 보호되어 있었지만(비록 상대적으로 취약한 비밀번호였지만), 공개 정보만 포함되어 있었어요. 그럼에도 온라인의 전 세계에 데이터를 노출시키는 것에 대해 얼마나 많은 개발자들이 응용 프로그램의 데이터베이스를 포트를 공개적으로 공개하면서 알지 못하고 있는지 생각하니 무서웠어요.\n\n# 아마 저만 그런 것은 아니겠죠?\n\n분명히 나만 문제를 걱정하고 있는 것은 아닌 것 같아요. 이와 비슷한 버그 리포트가 있고, 다양한 플랫폼에 흩어진 토론과 기사들이 있으며, 이 문제를 해결하려는 프로젝트도 있어요. 버그 리포트의 토론들을 읽어보면, 이게 실제로 문제인지에 대한 끝없는 논쟁들을 찾을 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이전에 언급한 대로, 나의 불평은 도커가 이렇게 작동하는 것이 아니라, 오히려 이러한 문제와 토론에 대답할 때 많은 사람들이 보이는 경멸적인 태도입니다. 아래 Reddit 댓글은 9년 전에 작성되었지만, 제 주장을 완벽히 보여 줍니다.\n\n그리고\n\n그러한 태도는 건설적이지 않습니다. 평균적인 스스로 가르친 취미 개발자가 응용 프로그램을 배포할 때 보안 모베스트 프랙티스에 능숙하다면 좋겠지만, 현실은 이를 강요할 장벽이나 관문이 없는 세계에서 살고 있다는 것입니다.\n\n# 학습에 대한 부가적인 노트\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n당신이 알지 못하는 것은 당신이 무엇을 모른다는 것입니다.\n\n우리는 피드백을 통해 가장 잘 배웁니다 — 코드를 작성하다가 뭔가 잘못되었다는 걸 바로 알 수 있기 때문에, 그 때 바로 수정하고 동작하는 걸 보는 것은 긍정적인 피드백을 주며, 그 문제를 해결하는 방법에 대한 접근 방식을 강화합니다. 이 즉각적인 피드백 루프는 프로젝트를 시작하고 발생하는 도전에 대처하며 코딩을 배우기 쉽게 만듭니다. 그러나 작성한 코드가 유지보수가 어렵거나 보안에 취약하면, 몇 달이든 몇 년이든 그런 피드백을 얻지 못할 수 있습니다. 그렇기 때문에 보안과 같은 개념은 시행착오를 통해 쉽게 스스로 학습할 수 없습니다.\n\n# 이 구체적인 도커 문제에 대한 해결책\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기 계신 것이면 컨테이너를 보호하는 방법을 찾고 계실 것 같아요. 그렇다면, 지금까지 함께 가지고 머물러 주셔서 감사드려요. 여기 몇 가지 방안이 있어요:\n\n- 최적의 해결책은 실제로 포트를 전혀 공개하지 않는 것이죠. 공개적으로 해당 포트를 노출하고자 할 때가 아니라면요. 컨테이너 간 통신을 위해, 최상의 방법은 해당 트래픽을 위해 특별히 설정된 Docker 네트워크를 구축하고, 컨테이너가 해당 네트워크에 연결되도록 하는 것입니다. 주요 제한 사항은 컨테이너 내부에서 실행 중인 프로세스에는 작동하지 않는다는 것이에요. 모든 어플리케이션이 컨테이너 친화적이지는 않고 때때로 호스트에서 컨테이너에 액세스해야 하는 프로세스를 실행해야 할 수도 있어요.\n- 흔한 제안일 수 있지만, 공식 문서에서 명확히 권장하지 않는 것 중 하나는 /etc/docker/daemon.json에서 iptables를 false로 설정하는 것이에요. 이렇게 하면 Docker가 네트워크 규칙을 추가하지 않기 때문에 컨테이너 내의 네트워킹이 전혀 작동하지 않를 거에요. 이 경로를 선택하려면 규칙을 수동으로 추가해야 해요. 이 작업은 쉬운 편이 아니며 — 제대로 알지 못한다면 — 보안 설정 오류나 반대로 서버에 액세스 권한을 상실할 수도 있어요.\n- 컨테이너 네트워킹을 망가뜨리지 않는 약간 더 나은 방법은 기본 주소 바인딩을 로컬 전용으로 설정하는 것이죠. 이것은 /etc/docker/daemon.json에서 ip를 127.0.0.1로 설정하여 수행할 수 있어요. 사실, 처음부터 이것이 기본 설정이 되었어야 한다고 주장할 수도 있어요. 컨테이너가 호스트 외부에서만 액세스 가능하도록 명시적으로 구성되어야 하는 경우에만 예외적으로 구성되어야 한다고 생각해요. 하지만 그것을 바꾸는 것은 지금과 같이 늦은 시각에선 너무 늦어버린 일이에요. 저는 사용했던 방법인데요, 그러나 이 방법에는 한 가지 단점이 있어요. 이것은 컨테이너의 이식성에 반하는 것이죠. 다른 호스트에서 컨테이너를 실행하려면 설정을 변경해야 하는 것을 기억해야 해요. 이상적으로, 동일한 방식으로 모든 호스트에서 컨테이너가 실행되기를 원할 거에요. 호스트에서 설정을 변경을 기억하지 않으면 안전하지 않아진 컨테이너가 생각만 하고 있기는 좋지 않아요.\n- 일반적인 문제에 대한 최선의 방법은 Docker 명령어와 docker-compose.yml에서 명시적 IP 주소 바인딩을 사용하는 것에 익숙해지는 것이어야 해요. 절대로 -p 3306:3306을 작성하지 않으시고, 대신 -p 127.0.0.1:3306:3306을 작성하는 데 익숙해져야 하며, 실제로 외부로 포트를 노출해야 하는 경우에 대해서 명확하게 설정해야 해요: -p 0.0.0.0:3306:3306. 저는 Docker 네트워크가 선택사항이 아닐 때 채택한 방법이에요.\n\n이제, 내가 대답을 모르는 질문은 다양한 튜토리얼이 데이터베이스를 전세계에 노출하도록 권장하는 문제를 어떻게 해결할지일 거예요.\n\n# 일반 문제에 대한 해결책\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n몇 년째 쓰고 싶었던 이 글은 조금 화제적인 내용이에요. 개발자들의 태도가 저의 속앓이거든요. \"내가 아는 것을 모르면 하는 건 안 된다\"는 게이트키퍼식 마인드는 너무나 흔합니다.\n\n하지만 이 이상한 직업에서 우리 중 많은 사람들이 돈을 벌기 위해 하지만 다른 많은 사람들은 그냥 즐기기 위해 하는 경우도 있어요. 그래서 취미로 하는 보안 노력만으로 어플리케이션을 VPS나 예비 랩탑에 올리는 것에서 많은 흠들이 생기는 게 좀 불안해지죠.\n\n큰 그림에서 해결책은 이렇습니다:\n\n- 뭔가를 만들고 싶어하는 미숙한 개발자를 위해: 계속하세요! 가능한 한 많이 읽으세요. 사용하는 도구의 공식 문서를 건너뛰지 마세요. 많이 흡수하세요. 아직 모르는 것이 있음을 인식하고 배우는 가장 좋은 방법은 계속 새로운 것을 시도하는 것이라는 것을 기억하세요.\n- 널리 사용되는 오픈 소스 도구에 기여하고 있는 숙련된 개발자들에게, 기본 동작에 대한 중요한 설계 결정을 내리는 경우: 문서를 읽지 않을 것으로 예상되는 사람들을 보호하는 의무가 여전히 있음을 기억하세요. 다른 사람들이 써놓은 빠르게 훑어보는 튜토리얼을 의존하는 사람들에게도 말이죠. 우리는 안전한 기본값을 선택할 의무가 있습니다 — Docker에는 이미 늦었을지 모르지만, 다음 도구에는 이 기회를 잡을 수도 있을 겁니다. 우리의 도구가 의도되지 않은 방식으로 사용될 때면 대뜻하지 않고 무례하지 말고, 가르치고 교육의 순간으로 삼아보세요. 아마는 더욱 배울 수도 있죠.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n비슷한 경험이 있나요? 기본적으로 보안에 취약한 방식으로 사용된 다른 도구를 보신 적이 있나요?\n","ogImage":{"url":"/assets/img/2024-05-27-DockerPortsWhatAreYouReallyPublishing_0.png"},"coverImage":"/assets/img/2024-05-27-DockerPortsWhatAreYouReallyPublishing_0.png","tag":["Tech"],"readingTime":8},{"title":"MERN 스택 애플리케이션 도커화 단계별 가이드","description":"","date":"2024-05-27 17:17","slug":"2024-05-27-DockerizingaMERNStackApplicationAStep-by-StepGuide","content":"\nMERN 스택 애플리케이션을 구축하는 것은 도커화 및 여러 환경 관리와 관련해 도전적일 수 있습니다. 도커를 사용하면 애플리케이션을 컨테이너로 패키징하여 다양한 환경 간에 쉽게 이동할 수 있도록 도와줄 수 있습니다.\n\n이 블로그 포스트에서는 Docker와 Docker Compose를 사용하여 MERN 스택 애플리케이션을 컨테이너화하는 과정을 안내해 드리겠습니다. Docker와 Docker Compose는 함께 작동하여 컨테이너화된 애플리케이션의 개발, 배포 및 관리를 간편화하는 데 도움이 되는 두 가지 강력한 도구입니다.\n\nDocker는 애플리케이션과 그 종속성을 표준화된 단위인 컨테이너로 패키징할 수 있는 플랫폼입니다. 이러한 컨테이너는 가볍고 이식성이 있으며, Docker가 설치된 시스템의 기반이 되는 운영 체제에 관계없이 일관되게 실행될 수 있습니다.\n\nDocker Compose는 쉽게 다중 컨테이너 애플리케이션을 정의하고 실행하기 위한 도구입니다. YAML 파일(일반적으로 docker-compose.yml로 명명됨)을 사용하여 애플리케이션의 서비스(컨테이너)와 그들 간의 관계를 구성할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n시작하기 전에 시스템에 다음 항목이 설치되어 있는지 확인하세요:\n\n- Docker\n- Node.js\n\n그리고 도커와 관련된 기본적인 이해와 명령어가 있는 것으로 가정합니다.\n\n## 단계 1: MERN 애플리케이션 설정하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가정하에 기본적인 MERN 애플리케이션이 다음과 같이 구성되어 있다고 가정하고, 다음과 같은 Dockerfile 및 docker-compose 파일을 만들어야 합니다.\n\nmy-mern-app/\n├── backend/\n│ ├── Dockerfile\n│ ├── package.json\n│ ├── server.js\n├── frontend/\n│ ├── Dockerfile\n│ ├── package.json\n│ ├── public/\n│ ├── src/\n├── docker-compose.yml\n\n## 단계 2: 백엔드와 프론트엔드 도커 파일 설정\n\n백엔드 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 지시사항을 백엔드 도커 파일에 포함해야 합니다.\n\n```js\n# backend/Dockerfile\n\n# 공식 Node.js Alpine 기반 이미지 사용\nFROM node:20.11.1-alpine\n\n# 작업 디렉토리 설정\nWORKDIR /app\n\n# package.json 및 package-lock.json 복사\nCOPY package*.json ./\n\n# 의존성 설치\nRUN npm install\n\n# 나머지 애플리케이션 코드 복사\nCOPY . .\n\n# 실행 중인 앱의 포트 노출\nEXPOSE 5000\n\n# 애플리케이션 실행\nCMD [\"npm\", \"start\"]\n```\n\n컨테이너의 기본 이미지로는 Alpine 리눅스 배포판을 기반으로 한 Node.js 런타임 버전 20.11.1을 사용하고 있습니다. Alpine 이미지는 일반적으로 더 작고 다운로드 속도가 빠릅니다. WORKDIR /app은 컨테이너 내부의 작업 디렉토리를 /app으로 설정합니다. 이후의 모든 명령은 이 디렉토리에서 실행됩니다. COPY package\\*.json ./는 로컬 머신에서 컨테이너로 package.json과 package-lock.json(있는 경우)을 복사합니다. 이 파일들은 종속성을 설치하는 데 사용됩니다.\n\n이는 npm install을 컨테이너 내에서 실행하여 package.json에 지정된 모든 종속성을 설치합니다. COPY . .는 나머지 애플리케이션 코드를 컨테이너의 작업 디렉토리로 복사합니다. EXPOSE 5000은 컨테이너가 실행 중인 포트 5000에서 수신하는 것을 Docker에 알립니다. 이는 내부 포트를 호스트 머신의 외부 포트에 매핑하는 데 유용합니다. CMD [\"npm\", \"start\"]는 컨테이너 시작 시 실행할 명령을 지정합니다. 일반적으로 package.json에 정의된 start 스크립트를 사용하여 서버를 시작하는 npm start를 실행합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n프론트엔드 설정\n\n프론트엔드 도커 파일에 아래 지침을 포함해야 합니다.\n\n```js\n# frontend/Dockerfile\n\n# 공식 Node.js Alpine 기반 이미지 사용\nFROM node:20.11.1-alpine\n\n# 작업 디렉토리 설정\nWORKDIR /app\n\n# package.json 및 package-lock.json 복사\nCOPY package*.json ./\n\n# npm이 더 긴 타임아웃을 가지고 캐시를 사용하도록 설정\nRUN npm config set cache /app/.npm-cache --global\nRUN npm config set fetch-retries 10\nRUN npm config set fetch-retry-mintimeout 40000\nRUN npm config set fetch-retry-maxtimeout 220000\n\n# 종속성 설치\nRUN npm install\n\n# 나머지 애플리케이션 코드 복사\nCOPY . .\n\n# 애플리케이션이 실행되는 포트 노출\nEXPOSE 3000\n\n# 애플리케이션 실행\nCMD [\"npm\",\"start\"]\n```\n\n프론트엔드 및 백엔드 애플리케이션용 도커 파일을 설정할 때, 대부분의 지시사항이 매우 유사하다는 것을 알게 될 것입니다. 주요 차이점은 타임아웃 및 캐시 구성에 있습니다. 특정 요구 사항에 따라 추가 단계가 필요할 수도 있고 아닐 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 컨테이너 내의 캐시 디렉토리를 사용하세요 (/app/.npm-cache).\n- 패키지를 가져올 때 재시도 횟수를 늘리세요 (fetch-retries).\n- 패키지를 가져오는 데 걸리는 최소 및 최대 시간을 늘리세요 (fetch-retry-mintimeout 및 fetch-retry-maxtimeout).\n\n이러한 설정은 불안정한 네트워크 환경에서 종속성을 다운로드할 때 신뢰성을 향상시킬 수 있습니다. 인터넷 연결이 제대로 되지 않을 때 도움이 될 수 있어요 :(.\n\n## 단계 3: 도커 컴포즈 설정\n\n루트 도커 컴포즈 파일에 아래 구성을 포함해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# docker-compose.yml\n\nversion: '3.8'\nservices:\nbackend:\nbuild: ./backend\nports: - '5000:5000'\nfrontend:\nbuild: ./frontend\nports: - '3000:3000'\n\n`backend`: 백엔드 서비스를 정의합니다. 백엔드 디렉토리에서 Docker 이미지를 빌드하고 포트 5000으로 매핑합니다. `frontend`: 프론트엔드 서비스를 정의합니다. 프론트엔드 디렉토리에서 Docker 이미지를 빌드하고 포트 3000으로 매핑합니다.\n\n만약 몽고 DB와 같은 추가 서비스를 추가해야 한다면, 다음과 비슷한 추가 서비스를 backend에 의존하도록 추가하면 됩니다.\n\nversion: '3.8'\nservices:\nbackend:\nbuild: ./backend\nports: - '5000:5000'\ndepends_on: - mongo\nmongo:\nimage: mongo:latest\nports: - '27017:27017'\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 4: 애플리케이션 빌드 및 실행\n\n프로젝트 루트에서 다음 명령을 실행하여 애플리케이션을 빌드하고 시작합니다:\n\n```js\ndocker-compose up — build\n```\n\nDocker Compose가 이미지를 빌드하고 컨테이너를 시작합니다. 프론트엔드는 http://localhost:3000에서, 백엔드는 http://localhost:5000에서 접속할 수 있습니다. 참조 코드는 여기에서 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 Docker Compose를 사용하여 MERN 스택 애플리케이션을 성공적으로 Docker화했습니다. 이 설정은 각 구성 요소에 대해 격리된 환경을 제공하여 애플리케이션을 관리하고 배포하기 쉽게합니다. 설정을 더 맞춤화하여 개발 및 프로덕션 요구 사항에 맞게 사용할 수 있습니다.\n\n환영합니다...!\n","ogImage":{"url":"/assets/img/2024-05-27-DockerizingaMERNStackApplicationAStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-05-27-DockerizingaMERNStackApplicationAStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":7},{"title":"하이브 메타스토어 HMS 스키마를 유니티 카탈로그로 이관하기","description":"","date":"2024-05-27 17:16","slug":"2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog","content":"\n<img src=\"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png\" />\n\nHMS에서 Unity Catalog로의 이주 이야기에 오신 것을 환영합니다.\n\n본 글에서는 HMS에서 Unity Catalog로의 이주 과정을 공유하고자 합니다. HMS를 Unity Catalog로 마이그레이션하기 위한 여러 도구들이 있음을 알고 있습니다. 특히 현재 시장에서 인기를 끌고 있는 UCX가 있습니다. 아직 UCX를 탐험해보지는 않았지만, 앞으로 UCX를 살펴볼 예정입니다.\nUCX를 사용해보고 싶다면, https://github.com/databrickslabs/ucx 에서 확인하고 그 경험을 공유해주세요.\n\n본 글의 범위\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 외부 테이블을 Unity 카탈로그로 이주합니다.\n\n이 글에서 다루겠습니다.\n\n- 관리형 테이블을 Unity 카탈로그로 이주합니다.\n  https://medium.com/@data_engineering_0216/migrate-managed-table-to-unity-catalog-ab4dbba9d6aa\n- 뷰를 Unity 카탈로그로 이주합니다.\n  https://medium.com/@data_engineering_0216/migrate-views-from-hive-metastore-to-unity-catalog-7aac5ec1da50\n- 메타데이터 기반 권한 관리\n  https://medium.com/@data_engineering_0216/unity-catalog-permissions-f1e6221cbc68\n- https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate\n\n외부 테이블을 Unity 카탈로그로 이주합니다.\n\n준비물\n\n- 메타스토어 또는 카탈로그 관리자 권한\n- Unity 카탈로그가 활성화된 Databricks 워크스페이스\n- Unity 카탈로그가 활성화된 클러스터\n- Databricks 접근 커넥터\n- 마운트 지점과 동등한 스토리지 자격 증명 및 외부 위치(읽기 및 쓰기 권한 필요)\n- 워크스페이스 또는 클러스터 수준에서 기본 카탈로그 설정: 선택 사항\n  클러스터 구성: spark.databricks.sql.initial.catalog.name gold_dv\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n솔루션을 깊이 살펴보겠습니다. 사용자 정의 Python 함수인 migrate_tables_to_unity_catalog을 살펴봅시다. 이 코드는 외부 테이블을 하나의 (hive_metastore) 카탈로그에서 다른 카탈로그(Unity Catalog)로 동기화하는 함수를 정의합니다.\n\n이 함수는 다음과 같은 매개변수를 사용합니다:\n\n- src_ct_name: 원본 카탈로그의 이름.\n- src_databases: 원본 카탈로그에서 가져온 데이터베이스 사전의 목록.\n- dst_ct_name: 대상 카탈로그의 이름.\n- exclude_databases: 마이그레이션에서 제외할 데이터베이스 이름의 목록.\n- full_reset: 전체 리셋을 수행해야 하는지 여부를 나타내는 부울 플래그.\n\n이 함수는 먼저 src_databases 매개변수에서 데이터베이스 이름 목록을 작성합니다. 제외할 데이터베이스가 있는 경우 해당 데이터베이스를 목록에서 필터링합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼, 각 데이터베이스를 처리하는 process_database 내부 함수를 정의합니다. full_reset이 True 인 경우, 대상 카탈로그의 데이터베이스를 삭제하고 새 데이터베이스를 생성한 후 소스 카탈로그에서 대상 카탈로그로 스키마를 동기화합니다. full_reset이 False 인 경우, 새 데이터베이스를 생성하고 스키마를 동기화합니다. full_reset이 제공되지 않은 경우, 재설정 모드를 요청하는 메시지를 출력합니다.\n\nconcurrent.futures.ThreadPoolExecutor를 사용하여 함수는 각 데이터베이스를 병렬로 처리하도록 제출합니다. 모든 futures가 완료되기를 기다리고 처리 중 발생한 예외를 처리합니다.\n\n이 함수를 사용하려면 필요한 매개변수를 제공하고 함수를 호출해야 합니다.\n\n```python\nimport concurrent.futures\n\n\ndef migrate_tables_to_unity_catalog(src_ct_name, src_databases, dst_ct_name, exclude_databases, full_reset):\n    \"\"\"\n    한 카탈로그에서 다른 카탈로그로 관리되는 모든 테이블을 복사합니다.\n\n    매개변수:\n        src_ct_name (str): 원본 카탈로그의 이름.\n        src_databases (list): 원본 카탈로그에서의 데이터베이스 딕셔너리 목록.\n        dst_ct_name (str): 대상 카탈로그의 이름.\n        exclude_databases (list): 마이그레이션에서 제외할 데이터베이스 이름 목록.\n\n    반환:\n        None\n\n    예외:\n        None\n\n    예시:\n        src_ct_name       = \"hive_metastore\"\n        src_databases     = spark.sql(f\"SHOW DATABASES IN {src_ct_name}\").collect()\n        dst_ct_name       = \"uc_dv\"\n        exclude_databases = [\"poc\", \"temp_tbd\", \"default\"]\n        migrate_managed_tables_to_unity_catalog(src_ct_name, src_databases, dst_ct_name, exclude_databases)\n\n    \"\"\"\n\n    list_of_db = []\n    for db in src_databases:\n        dbName = db['databaseName']\n        list_of_db.append(dbName)\n    if exclude_databases:\n        databases = [x for x in list_of_db if x not in exclude_databases]\n    else:\n        databases = list_of_db\n\n    print(databases)\n\n    def process_database(db, full_reset):\n\n        if full_reset == True:\n            drop_db = f\"DROP DATABASE IF EXISTS {dst_ct_name}.{db} CASCADE\"\n            display(spark.sql(drop_db))\n            create_db = f\"CREATE DATABASE IF NOT EXISTS {dst_ct_name}.{db}\"\n            display(spark.sql(create_db))\n            query = f\"SYNC SCHEMA {dst_ct_name}.{db} from {src_ct_name}.{db}\" # SYNC SCHEMA uc_dv.gold from hive_metastore.clean\n\n            print(query)\n            display(spark.sql(query))\n        elif full_reset == False:\n            create_db = f\"CREATE DATABASE IF NOT EXISTS {dst_ct_name}.{db}\"\n            display(spark.sql(create_db))\n            query = f\"SYNC SCHEMA {dst_ct_name}.{db} from {src_ct_name}.{db}\" # SYNC SCHEMA uc_dv.gold from hive_metastore.clean\n\n            print(query)\n            display(spark.sql(query))\n        else:\n            print(\"재설정 모드를 제공해주세요\")\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = [executor.submit(process_database, db, full_reset) for db in databases]\n        # 모든 futures가 완료되기를 기다림\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                # 각 future의 결과를 가져옴\n                result = future.result()\n            except Exception as e:\n                # 발생한 예외 처리\n                pass\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래 코드는 다음 작업을 수행합니다:\n\n```js\nsrc_ct_name       = \"hive_metastore\"\nsrc_databases     = spark.sql(f\"SHOW DATABASES IN {src_ct_name}\").collect()\ndst_ct_name       = \"uc_dv\" #spark.sql(\"SELECT current_catalog()\").collect()[0]['current_catalog()']\nexclude_databases = [\"poc\", \"temp_tbd\", \"default\"]\nfull_reset         = False\n\nmigrate_tables_to_unity_catalog(src_ct_name,src_databases,dst_ct_name,exclude_databases,full_reset)\n```\n\n- \"src_ct_name\" 변수를 정의하여 값 \"hive_metastore\"를 할당합니다.\n- Spark를 사용하여 src_ct_name 카탈로그 내의 데이터베이스 목록을 검색하는 SQL 쿼리를 실행하고 결과를 src_databases 변수에 할당합니다.\n- Spark를 사용하여 현재 카탈로그를 검색하는 SQL 쿼리를 실행하고 결과를 dst_ct_name 변수에 할당합니다.\n- 마이그레이션 프로세스에서 제외될 데이터베이스 이름을 포함하는 \"exclude_databases\" 목록을 정의합니다.\n- 값이 False인 부울 변수 \"full_reset\"을 정의합니다.\n- migrate_tables_to_unity_catalog 함수를 src_ct_name, src_databases, dst_ct_name, exclude_databases 및 full_reset 매개변수로 호출합니다. 이 함수는 소스 카탈로그에서 대상 카탈로그로 테이블을 마이그레이션하는 역할을 합니다.\n\n위의 Python 함수는 hive_metastore의 외부 테이블을 unity Catalog로 몇 분 안에 마이그레이션하는 데 유용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 코드를 Databricks 노트북에 복사하여 Adf나 Databricks 일정으로 매일 실행하면 Unity Catalog로 완전히 마이그레이션할 때까지 도움을 줄 수 있습니다. 예를 들어, hive_metastore에 새 테이블을 추가하면 일정이 자동으로 새로운 테이블을 Unity Catalog에 동기화합니다.\n\nUnity Catalog로의 마이그레이션 여정에 도움이 되기를 바라며, 궁금한 점이 있으시면 언제든지 물어보세요.\n","ogImage":{"url":"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png"},"coverImage":"/assets/img/2024-05-27-MigrateHivemetastoreHMSSchematoUnityCatalog_0.png","tag":["Tech"],"readingTime":8},{"title":"데이터브릭스 Q2 로드맵 W2W4","description":"","date":"2024-05-27 17:15","slug":"2024-05-27-DatabricksQ2RoadmapW2W4","content":"\n![이미지](/assets/img/2024-05-27-DatabricksQ2RoadmapW2W4_0.png)\n\n# 소개\n\n저는 어떤 이유 때문인지 원래 초대를 놓쳐서 한 주를 뒤처져 이 글을 작성했습니다만, 최신 Databricks 분기 로드맵 웨비나에서 발표된 주요 소식들을 강조하고 싶었습니다. 고객 아카데미 계정을 가지신 분들은 재생 영상을 거기서도 볼 수 있습니다.\n\n# 유니티, 유니티, 유니티\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-27-DatabricksQ2RoadmapW2W4_1.png\" />\n\n작년 분기 웨비나에서 언급했었지만, Databricks는 플랫폼 내 데이터 거버넌스의 미래로 Unity Catalog에 올인했습니다. 새 작업 영역은 기본적으로 Unity로 활성화되며, 오래된 작업 영역이 더 나은 지원을 위해 이전해야 할 때가 올 것입니다.\n\n몇 달 동안 Unity 이주 작업을 진행해 온 사람으로서 이야기하자면, 그것은 유용하며 상기한 그래픽에 나열된 많은 이점을 제공합니다. 어떤 부분은 때로는 도전적일 수도 있지만 (아마도 과장되었다고 할 수 있을 정도로), 시간이 지남에 따라 그 과정이 더 쉬워질 것이라고 확신합니다.\n\nUnity의 새로운 기능 측면에서 외부 파티션 메타데이터에 대한 더 나은 지원이 곧 추가될 예정이며, 현재보다 Parquet 데이터 액세스 속도를 향상시킬 것입니다. 게다가 기존 계보를 통합하고 싶은 사람들을 위해 BYOL(본인의 계보 가져오기)이라는 개념도 곧 나올 예정입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 대시보드/Databricks SQL\n\n지난 몇 달 동안 Databricks 내의 대시보드는 확실히 새롭게 변화했습니다. 곧 다른 사람들과 대시보드를 공유할 수 있는 기능이 추가될 예정이며, 이를 통해 새로운 사용자를 워크스페이스에 등록하는 데 여러 채널을 통해 지나야 하는 사용자들에게 큰 도움이 될 것입니다. 또한 대시보드는 웹페이지/앱에 플러그인으로 추가될 예정이므로, 더 많은 호환성을 원하는 사용자들을 위한 것입니다.\n\n반면에 Databricks SQL은 몇 가지 좋은 향상이 예정되어 있습니다. SQL 작성의 개념을 도입하여 쿼리와 협업 편집을 위한 Git 통합을 지원할 것입니다. SQL 스크립트는 트랜잭션 수준의 처리 및 루프와 같은 구조 지원을 제공할 것입니다.\n\n저도 이게 아니었군요, 그러나 변형 데이터 유형이 드디어 Databricks에 추가될 예정입니다. JSON 필드 작업을 하는 사람으로써, 이것이 Snowflake에서 큰 도움이 되었고, 여기에도 도입되어 기쁩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 워크플로우/노트북\n\n저는 주로 워크플로우에 Databricks를 사용하기 때문에, 이 섹션이 웨비나에서 나타날 때 놓치지 않아요. Delta Table 업데이트 후 워크플로우를 트리거하고 싶었던 적이 있다면, 테이블 트리거의 개념을 사용하면 이를 쉽게 할 수 있어요. 이제 워크플로우에서 센서의 개념이 더 많이 사용되어 추가 작업 관리 도구에 절대적으로 의존할 필요가 없게 되었다는 것은 좋은 일이에요.\n\n개발자 경험은 DAB를 위한 새로운 VS Code 통합으로 개선될 것이에요. Databricks Connect가 Databricks와 작업을 이끄는 모든 코드 간에 더 원활한 지원을 제공하는 긍정적인 발전이었기 때문에, 자산 번들을 보다 쉽게 개발하고 테스트할 수 있는 능력은 제게 자연스러운 진전 같아요.\n\n# 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이번에 나오는 새로운 릴리스에 대해 기대되는 것이 많네요. 개발 노력을 계획하기 위해 항상 유익한 세션을 마련해 주셔서 Databricks에게 항상 감사드립니다.\n","ogImage":{"url":"/assets/img/2024-05-27-DatabricksQ2RoadmapW2W4_0.png"},"coverImage":"/assets/img/2024-05-27-DatabricksQ2RoadmapW2W4_0.png","tag":["Tech"],"readingTime":3},{"title":"현재 날짜나 시간을 기준으로 데이터브릭에서 여러 파일을 동적으로 로드하는 방법","description":"","date":"2024-05-27 17:13","slug":"2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour","content":"\nBatch 시스템에서는 여러 번 클라이언트가 이미 추출된 파일을 동일한 경로에 유지하고, 도착한 추출 파일이 Bronze 레이어에 로드되기를 원하는 경우가 많습니다.\n\n따라서 이 경우, 개발자는 디렉토리에 있는 파일 목록 중에서 현재 날짜나 현재 시간에 도착한 파일만 고려하여 해당 파일의 내용을 Bronze 레이어에만 로드할 수 있도록 해야 합니다.\n\n이미지를 보면 오늘 날짜는 2024년 5월 25일임을 알 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2024년 5월 22일에 첫 번째 파일이 도착했습니다.\n2024년 5월 23일에 두 번째 파일이 도착했습니다.\n2024년 5월 24일에 세 번째 파일이 도착했습니다.\n나머지 네 개의 파일은 오늘인 2024년 5월 25일에 도착했습니다.\n\n오늘 도착한 첫 번째 파일 내용은 다음과 같습니다 -\n\n![이미지](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_1.png)\n\n오늘 도착한 두 번째 파일 내용은 다음과 같습니다 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Third File](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_3.png)\n\nThe content of the Fourth File arrived today is as follows -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_4.png\" />\n\n# 현재 날짜 기반으로 데이터브릭에서 여러 파일 동적으로 로드하는 방법\n\n그래서, 작업은 최근 네 개의 파일을 처리하고 그 네 개 파일의 내용을 브론즈 레이어의 테이블에 추가 모드로 로드하는 것입니다.\n\n단계 1: 브론즈 테이블 생성하기 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n먼저 다음 \"Spark SQL\" 쿼리를 사용하여 \"practice\"라는 데이터베이스를 생성하십시오 -\n\n```js\n%sql\nUSE hive_metastore;\nCREATE DATABASE IF NOT EXISTS practice\n```\n\n데이터베이스는 Databricks 워크스페이스에 생성됩니다 -\n\n![이미지](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼 다음 \"Spark SQL\" 쿼리를 사용하여 외부 델타 테이블 \"person_bronze\"을 생성하세요 -\n\n```js\n%sql\nCREATE TABLE IF NOT EXISTS hive_metastore.practice.person_bronze\n(\n  FirstName STRING NOT NULL,\n  LastName STRING NOT NULL,\n  City STRING NOT NULL,\n  Company STRING NOT NULL\n)\nLOCATION \"dbfs:/mnt/iobdatabronze/practice-zone/delta-table/person_bronze\"\n```\n\n외부 델타 테이블은 Databricks 워크스페이스의 \"practice\" 데이터베이스 내부에 생성됩니다 -\n\n![image](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n생성된 External Delta Table \"person_bronze\"의 폴더 경로는 제공된 위치에 ADLS에 생성되었습니다-\n\n![image](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_7.png)\n\n단계 2: 현재 날짜인 2024년 5월 25일을 기준으로 Databricks에서 가장 최근 네 개의 파일을 로드하는 Python 코드를 작성해 보겠습니다.\n\n단계 2.1: Python의 \"datetime\" 모듈을 사용하여 \"현재 날짜\"의 값을 가져와, 도착 파일의 이름에 사용된 형식과 일치하도록 \"현재 날짜\"의 값을 포맷팅해 주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport os\nfrom datetime import datetime\nfrom functools import reduce\n\n# Python의 \"datetime\" 모듈을 사용하여 현재 날짜 가져오기\ncurrent_date = datetime.now()\nprint(current_date)\n\n# 현재 날짜의 값을 파일 이름 형식에 맞게 포맷팅하여 출력\nfile_name_date_format = current_date.strftime(\"%Y%m%d\")\nprint(file_name_date_format)\n```\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_8.png\" />\n\n단계 2.2: 파일이 보관된 ADLS 디렉토리의 \"마운트된 경로\"를 지정합니다.\n그런 다음, 해당 지정된 디렉토리에서 모든 파일을 나열합니다.\n마지막으로, 그 지정된 디렉토리의 파일 이름에 \"현재 날짜\"가 포함된 파일만 걸러내어 Python List에 저장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 파일이 위치한 ADLS 디렉토리의 마운트 경로 지정하기\ndirectory_path = \"/mnt/iobdatalanding/practice-zone/input-files/\"\n\n# 지정된 디렉토리에 있는 모든 파일 나열하기\nall_files = os.listdir(\"/dbfs\" + directory_path)\nprint(all_files)\n\n# 현재 날짜가 파일 이름에 포함된 파일만 필터링하여 Python 리스트에 저장하기\nmatching_files = [matching_file for matching_file in all_files if file_name_date_format in matching_file]\nprint(matching_files)\n```\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_9.png\" />\n\n단계 2.3: 먼저 \"빈 Python 리스트\"를 생성하세요.\n그런 다음 각 일치하는 파일의 내용을 각각 별도의 DataFrame에 로드하세요.\n마지막으로 각 DataFrame을 이미 생성된 \"Python 리스트\"에 \"객체\"로 추가하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 빈 리스트 생성\nlist_of_dfs = []\n\n# 각 일치하는 파일을 각각 별도의 데이터프레임으로 로드\nfor file_name in matching_files:\n    # 실제 파일 경로 생성\n    file_path = os.path.join(directory_path, file_name)\n    # 각 파일마다 데이터프레임 생성\n    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n    # 각 데이터프레임을 빈 리스트에 객체로서 각각 저장\n    list_of_dfs.append(df)\n\nprint(list_of_dfs)\n```\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_10.png\" />\n\n2.4단계: 각 데이터프레임의 모든 값들을 단일 데이터프레임으로 연결하고, 각 데이터프레임의 값들이 이제 \"파이썬 리스트\"의 각 객체로 되는 단일 데이터프레임을 생성하기 위해 \"reduce()\" 함수와 \"union()\" 메서드를 사용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 각 데이터프레임의 모든 값을 연결하여 하나의 데이터프레임으로 만들기\n# 이때 각 데이터프레임의 값은 이제 Python List의 각 객체이며 \"reduce()\" 함수를 사용하여 \"union()\" 메서드와 함께 결합합니다.\nfinal_df = reduce(lambda df1, df2: df1.union(df2), list_of_dfs)\ndisplay(final_df)\n```\n\n결과 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_11.png\" />\n\n따라서 위 이미지에서 오늘 도착한 네 개의 파일에서 모든 레코드의 조합이 포함된 \"final_df\" 데이터프레임이 있음을 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n당신이 개발자십에 관심이 있나봐요! 친철한 톤으로 번역해 드리겠습니다.\n\n스텝 2.5: 'final_df' DataFrame의 내용을 Bronze Table \"person_bronze\"에 삽입하세요.\n\n```js\n# 'final_df' DataFrame의 내용을 Bronze Table \"person_bronze\"에 삽입\nfinal_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"hive_metastore.practice.person_bronze\")\n```\n\n다음 \"스파크 SQL\" 쿼리를 사용하여 'person_bronze' Bronze Table에 방금 삽입된 데이터가 있는지 확인하세요 -\n\n```js\n%sql\n-- 'person_bronze' Bronze Table에 데이터가 있는지 확인\nSELECT * FROM hive_metastore.practice.person_bronze;\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_12.png\" />\n\n# 현재 시간을 기반으로 한 Databricks에서 여러 파일 로드하기\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_13.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금은 2024년 5월 25일이고, 현재 시간은 오후 8시입니다. 위 그림에서 볼 수 있듯이, 마지막 두 파일이 \"현재 시간\"인 즉, 8시에 도착했습니다.\n\n그러므로, 마지막 두 파일을 처리하고 그 두 파일의 내용을 브론즈 레이어의 테이블에 추가 모드로 로드하는 작업입니다.\n\n# 현재 날짜 또는 시간에 따라 Databricks에서 여러 파일을 동적으로 로드하는 방법\n\n배치 시스템에서는 고객이 이미 추출된 파일을 도착한 추출된 파일과 동일한 경로에 유지하고 브론즈 레이어에 로드하길 원하는 경우가 많습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n따라서 현재 일 또는 현재 시간에 도착한 파일만 고려하여 디렉터리에있는 파일 목록에서 해당 파일의 내용을 처리하고 브론즈 계층으로 로드하는 개발자의 책임이 있습니다.\n\n![이미지](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_14.png)\n\n오늘은 2024년 5월 25일입니다. 위 이미지에서 볼 수 있듯이 -\n\n첫 번째 파일은 2024년 5월 22일에 도착했습니다.\n두 번째 파일은 2024년 5월 23일에 도착했습니다.\n세 번째 파일은 2024년 5월 24일에 도착했습니다.\n나머지 네 번째 파일은 2024년 5월 25일, 즉 오늘 도착했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n오늘 도착한 첫 번째 파일 내용은 다음과 같습니다 -\n\n![First File](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_15.png)\n\n오늘 도착한 두 번째 파일 내용은 다음과 같습니다 -\n\n![Second File](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_16.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n현재 시간에 도착한 첫 번째 파일의 내용은 다음과 같습니다 -\n\n![](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_17.png)\n\n현재 시간에 도착한 두 번째 파일의 내용은 다음과 같습니다 -\n\n![](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_18.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nStep 1: Create a Bronze Table -\n\n이미 첫 번째 부분에서 생성되었습니다.\n\nStep 2: 오늘 현재 시간 기준으로 Databricks에서 마지막 두 파일을로드하는 Python 코드 작성 시작, 즉, 2024년 5월 25일 오후 8시.\n\nStep 2.1: Python의 \"datetime\" 모듈을 사용하여 \"현재 날짜\"의 \"현재 시간\" 값을 가져와서 \"현재 날짜\"의 \"현재 시간\" 값을 적시되어 파일 도착 이름 및 \"날짜\" 및 \"시간\" 부분과 일치하도록 형식화하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport os\nfrom datetime import datetime\nfrom functools import reduce\n\n# Python의 \"datetime\" 모듈을 사용하여 현재 날짜의 현재 시간을 가져옵니다.\ncurrent_date_and_time = datetime.now()\nprint(current_date_and_time)\n\n# 현재 시간의 값을 파일 이름의 날짜 및 시간 형식과 일치하도록 형식화합니다.\nfile_name_date_and_hour_format = current_date_and_time.strftime(\"%Y%m%d%H\")\nprint(file_name_date_and_hour_format)\n```\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_19.png\" />\n\n단계 2.2: 파일이 보관된 ADLS 디렉토리의 \"Mounted Path\"를 지정합니다.\n그런 다음, 해당 지정된 디렉토리에서 모든 파일을 나열합니다.\n마지막으로 해당 지정된 디렉토리의 파일 이름 중 \"현재 날짜\"의 \"현재 시간\"이 있는 파일만 필터링하여 Python List에 저장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 파일이 위치한 ADLS 디렉토리의 마운트 경로를 지정합니다\ndirectory_path = \"/mnt/iobdatalanding/practice-zone/input-files/\"\n\n# 지정된 디렉토리에 있는 모든 파일을 나열합니다\nall_files = os.listdir(\"/dbfs\" + directory_path)\nprint(all_files)\n\n# 현재 날짜의 현재 시간을 파일 이름에 포함하는 파일만 필터링합니다\nmatching_files = [matching_file for matching_file in all_files if file_name_date_and_hour_format in matching_file]\nprint(matching_files)\n```\n\nOutput -\n\n![다이나믹 파일로드 방법](/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_20.png)\n\n단계 2.3: 먼저 \"빈 Python List\"를 생성합니다.\n그런 다음 각 일치하는 파일의 내용을 각각 별도의 DataFrame으로 로드합니다.\n마지막으로, 각 해당 DataFrame을 이미 생성된 \"Python List\"에 \"객체\"로서 추가합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 빈 목록 생성\nlist_of_dfs = []\n\n# 각 일치하는 파일을 각각 별도의 데이터프레임으로 불러오기\nfor file_name in matching_files:\n    # 실제 파일 경로 생성\n    file_path = os.path.join(directory_path, file_name)\n    # 각 파일에 대한 데이터프레임 생성\n    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n    # 각 데이터프레임을 빈 목록에 객체로 각각 저장\n    list_of_dfs.append(df)\n\nprint(list_of_dfs)\n```\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_21.png\" />\n\n단계 2.4: 각 데이터프레임의 모든 값들을 단일 데이터프레임으로 연결합니다. 각 데이터프레임의 값은 이제 \"Python List\"의 각 객체이며, \"reduce()\" 함수와 \"union()\" 메소드를 사용합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 각 DataFrame의 모든 값들을 Python 리스트의 각 객체로 사용하여 하나의 DataFrame으로 결합하십시오. \"reduce()\" 함수를 사용하고 \"union()\" 메서드를 함께 사용하십시오.\n\nfinal_df = reduce(lambda df1, df2: df1.union(df2), list_of_dfs)\ndisplay(final_df)\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_22.png\" />\n\n따라서 위 이미지에서 현재 시간에 도착한 두 파일의 레코드를 모두 포함하는 \"final_df\" DataFrame을 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n스텝 2.5: \"final_df\" 데이터프레임의 내용을 첫 번째 부분에 표시된 PySpark 코드를 사용하여 Bronze 테이블 \"person_bronze\"에 삽입합니다.\n\n마지막으로, 첫 번째 부분에 표시된 \"Spark SQL\" 쿼리를 사용하여 방금 삽입한 데이터가 Bronze 테이블 \"person_bronze\"에 있는지 확인하세요.\n","ogImage":{"url":"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_0.png"},"coverImage":"/assets/img/2024-05-27-HowToDynamicallyLoadMultipleFilesinDatabricksBasedonCurrentDateorHour_0.png","tag":["Tech"],"readingTime":16},{"title":"델타 레이크에서의 스키마 진화 - 데이터브릭스","description":"","date":"2024-05-27 17:09","slug":"2024-05-27-SchemaEvolutioninDeltaLake-Databricks","content":"\n요즘 현대의 빅 데이터 세계에서는 클라이언트가 소스 빅 데이터 파일을 보내서 처리하는 경우가 많습니다. 이 소스 파일의 \"구조\"는 시간이 지남에 따라 계속 변화합니다.\n\n소스 빅 데이터 파일과 처리할 때 \"스키마 불일치\"를 처리하는 적절한 메커니즘이 사용되지 않으면, 데이터가 최종적으로 저장되는 \"대상 테이블\"과 소스 파일로부터 데이터를 처리하는 전체 \"파이프라인\"이 실패할 수 있습니다.\n\n\"스키마 불일치\" 상황을 처리하기 위해, Databricks는 \"스키마 병합(Merge Schema)\"이라는 기능을 제공합니다.\n\n# \"스키마 진화(Schema Evolution)\" 소개\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가정해보겠습니다. 도착한 소스 대규모 데이터 파일의 데이터 처리를 브론즈 레이어의 델타 테이블로 처리하는 \"파이프라인\"이 있습니다. 이 \"구조\"는 다음과 같습니다 -\n\n![structure](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_0.png)\n\n또한, 이와 같은 \"구조\"를 가진 델타 테이블이 브론즈 레이어에 생성되었습니다.\n\n![schema](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_1.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n특정 시간이 지난 후에, 도착하는 소스 대용량 데이터 파일의 \"구조\"가 다음과 같이 추가 열 \"City\"를 수용할 수 있도록 변경됩니다 -\n\n![image](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_2.png)\n\n하지만, 이 새로운 추가 열 \"City\"는 도착하는 소스 대용량 데이터 파일의 초기 \"구조\"를 기반으로 생성된 Bronze Layer의 Delta Table에 수용할 수 없습니다.\n따라서 도착하는 소스 대용량 데이터 파일의 데이터 처리를 Bronze Layer의 Delta Table로 처리하는 \"파이프라인\"은 \"스키마 불일치\" 상황으로 인해 실패할 것입니다.\n\n더욱이, 나중에 도착하는 소스 대용량 데이터 파일의 \"구조\"가 다시 변경되어 기존 열 \"LastName\"을 제거하고 추가 열 \"Company\"를 수용할 수 있게 될 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![2024-05-27-SchemaEvolutioninDeltaLake-Databricks_3.png](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_3.png)\n\n만약 \"Bronze Layer\"의 델타 테이블의 \"스키마\"가 \"City\" 열을 수용할 수 있도록 수동으로 변경되었다면, 이전에 업데이트된 \"스키마\"에 \"Company\" 열의 정보가 없기 때문에 \"Bronze Layer\"의 델타 테이블은 여전히 새롭게 도착한 \"Company\" 열을 수용할 수 없습니다.\n따라서, 도착하는 소스 대규모 데이터 파일의 데이터 처리를 담당하는 \"파이프라인\"은 여전히 \"스키마 불일치\" 상황으로 인해 실패할 것입니다.\n\n## \"스키마 진화\"란?\n\n도착하는 소스 대규모 데이터 파일의 \"구조\"가 변경되는 경우, 추가 열을 수용하거나 이미 존재하는 열을 제거하기 위해 델타 테이블의 \"스키마\"를 조정하는 것을 \"스키마 진화\"라고 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 예시로 “Schema Evolution” 설명하기\n\n단계 1: 다음의 “Spark SQL” 쿼리를 사용하여 “practice” 데이터베이스를 생성합니다 -\n\n```js\n%sql\nUSE hive_metastore;\nCREATE DATABASE IF NOT EXISTS practice\n```\n\n데이터베이스는 Databricks 워크스페이스에 생성됩니다 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_4.png)\n\n단계 2: 다음 \"Spark SQL\" 쿼리를 사용하여 외부 델타 테이블 \"person_bronze\"을 생성합니다 -\n\n```js\n%sql\n-- 브론즈 테이블 \"person_bronze\" 생성\nCREATE TABLE IF NOT EXISTS hive_metastore.practice.person_bronze\n(\n  Id INT,\n  FirstName STRING,\n  LastName STRING\n)\nLOCATION \"/mnt/iobdatabronze/practice-zone/delta-table/person_bronze\"\n```\n\n외부 델타 테이블은 Databricks 워크스페이스의 \"practice\" 데이터베이스 내에 생성됩니다 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![2024-05-27-SchemaEvolutioninDeltaLake-Databricks_5](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_5.png)\n\n단계 3: 다음의 \"Spark SQL\" 쿼리를 사용하여 Delta Table \"person_bronze\"에 다음 두 레코드를 수동으로 삽입합니다 -\n\n```js\n%sql\nINSERT INTO hive_metastore.practice.person_bronze (Id, FirstName, LastName) VALUES(1, 'Souvik', 'Roy');\nINSERT INTO hive_metastore.practice.person_bronze (Id, FirstName, LastName) VALUES(2, 'Swaralipi', 'Roy');\n```\n\n\"Just inserted data\"가 Bronze Table \"person_bronze\"에 있는지 \"Spark SQL\" 쿼리를 사용하여 확인합니다 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n%sql\nSELECT * FROM hive_metastore.practice.person_bronze;\n```\n\nOutput -\n\n![Schema Evolution in Delta Lake](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_6.png)\n\nStep 4: 고객이 새로운 소스 빅 데이터 파일을 추가하여 새로운 열 \"City\"를 보냅니다. 새로운 소스 빅 데이터 파일의 내용은 다음과 같습니다 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_7.png)\n\n이제, 새로운 소스 빅 데이터 파일을 DataFrame으로 읽어 들입니다.\n\n```python\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# 도착한 새로운 소스 빅 데이터 파일의 구조 정의\nperson_schema = StructType([\n    StructField(\"Id\", IntegerType(), False),\n    StructField(\"FirstName\", StringType(), False),\n    StructField(\"LastName\", StringType(), False),\n    StructField(\"City\", StringType(), False)\n])\n\n# 도착한 새로운 소스 빅 데이터 파일의 내용 읽기\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").schema(person_schema).load(\"/mnt/iobdatalanding/practice-zone/input-files/Person_1.csv\")\ndisplay(df)\n```\n\n출력 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`<img src=\"/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_8.png\" />`\n\nDataFrame의 내용을 Bronze Layer의 Delta Table \"person_bronze\"에 추가하려고 시도해보세요. 다음과 같은 오류 \"Delta 테이블에 쓸 때 스키마 불일치가 감지되었습니다\"가 발생합니다 -\n\n```js\ndf.write\n  .format(\"delta\")\n  .mode(\"append\")\n  .saveAsTable(\"hive_metastore.practice.person_bronze\");\n```\n\nOutput -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 이미지에서 알 수 있듯이, 새로 도착한 \"City\" 열의 정보는 브론즈 레이어의 델타 테이블 \"person_bronze\"의 \"Schema\"에 포함되어 있지 않습니다.\n이것이 바로 브론즈 레이어의 Delta 테이블 \"person_bronze\"에 DataFrame의 내용을 추가하는 \"append\" 작업이 실패한 이유입니다.\n\n단계 5: 시간이 지남에 따라 소스 파일의 \"구조\"가 변경된 시나리오를 처리하기 위해, 변경된 열을 \"대상 Delta 테이블\"에 추가하기 위해 Databricks에서 제공하는 \"mergeSchema\" 기능을 사용하여 DataFrame의 내용을 Delta 테이블에 쓰는 코드에서 \"mergeSchema\" 기능을 \"true\"로 설정합니다. 이를 위해 다음 구문을 사용합니다 -\n\n```js\ndf.write\n  .format(\"delta\")\n  .option(\"mergeSchema\", \"true\")\n  .mode(\"append\")\n  .saveAsTable(\"hive_metastore.practice.person_bronze\");\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 PySpark 코드가 오류를 발생시키지 않고 성공적으로 실행되었습니다.\n\n\"Spark SQL\" 쿼리를 사용하여 브론즈 테이블 \"person_bronze\"에 방금 삽입된 데이터가 있는지 확인해보세요.\n\n```js\n%sql\nSELECT * FROM hive_metastore.practice.person_bronze;\n```\n\n결과 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 이미지에서 볼 수 있듯이 새로운 소스 빅 데이터 파일에서 가져온 새 콘텐츠가 Bronze Layer의 Delta Table에 성공적으로 삽입된 것을 확인할 수 있습니다. 새로운 추가 열 \"City\"가 추가되었습니다.\n새로 추가된 열 \"City\"에 대해 기존의 Bronze Layer의 Delta Table의 기존 행에는 값이 제공되지 않았기 때문에 기존 행의 \"City\" 열 값에 \"NULL\"이 설정되었습니다.\n\n단계 6: 클라이언트가 다시 새로운 소스 빅 데이터 파일을 보내왔는데, 이번에는 새로운 추가 열 \"Company\"와 이전에 있던 열 \"LastName\"이 삭제되었습니다. 새로운 소스 빅 데이터 파일의 내용은 다음과 같습니다 -\n\n![이미지](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_11.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 새로운 소스 빅데이터 파일을 DataFrame으로 읽어봅시다.\n\n```python\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# 도착한 새로운 소스 빅데이터 파일의 구조 정의\nperson_schema = StructType([\n    StructField(\"Id\", IntegerType(), False),\n    StructField(\"FirstName\", StringType(), False),\n    StructField(\"City\", StringType(), False),\n    StructField(\"Company\", StringType(), False)\n])\n# 도착한 새로운 소스 빅데이터 파일의 내용 읽기\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").schema(person_schema).load(\"/mnt/iobdatalanding/practice-zone/input-files/Person_2.csv\")\ndisplay(df)\n```\n\n결과 -\n\n<img src=\"/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_12.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 Databricks에서 제공하는 \"mergeSchema\" 기능을 사용하여 DataFrame의 내용을 Bronze 레이어의 Delta Table \"person_bronze\"에 추가해 보세요.\n\n```js\ndf.write\n  .format(\"delta\")\n  .option(\"mergeSchema\", \"true\")\n  .mode(\"append\")\n  .saveAsTable(\"hive_metastore.practice.person_bronze\");\n```\n\n위의 작업은 오류가 발생하지 않고 성공적으로 실행될 것입니다.\n\n삽입된 데이터가 Bronze Table \"person_bronze\"에 올바르게 입력되었는지 확인하려면 \"Spark SQL\" 쿼리를 사용하세요 -\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n%sql\nSELECT * FROM hive_metastore.practice.person_bronze;\n```\n\nOutput -\n\n![Schema Evolution in Delta Lake](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_13.png)\n\n위 이미지에서 새로운 소스의 빅 데이터 파일에서 가져온 새로운 내용이 브론즈 레이어의 델타 테이블에 성공적으로 삽입되었음을 확인할 수 있습니다. 새로 추가된 열 \"Company\"가 추가되었습니다.\n새로 추가된 열 \"Company\"를 위해 기존 델타 테이블의 이미 존재하는 행들에는 값이 제공되지 않았기 때문에, 기존 행들의 \"City\" 열 값으로 \"NULL\"이 설정되었습니다.\n또한, \"LastName\" 열은 이미 브론즈 레이어의 델타 테이블의 \"스키마\"에 존재하지만, 새로운 소스의 빅 데이터 파일의 내용에는 존재하지 않기 때문에, 새로운 소스에서 오는 행들에 대해 \"LastName\" 열 값으로 \"NULL\"이 설정되었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Databricks의 \"mergeSchema\" 기능의 제한\n\n## 소스 파일의 열 이름과 대상 테이블의 열 이름이 같지만 두 열의 데이터 유형이 다른 경우:\n\n고객이 새로운 소스 빅 데이터 파일을 전송했습니다. 열의 수 및 열의 이름은 Bronze 레이어의 Delta 테이블과 동일하지만 새 소스 빅 데이터 파일의 \"Company\" 열의 데이터 유형은 \"Integer\"입니다.\n새 소스 빅 데이터 파일의 내용은 다음과 같습니다 -\n\n![이미지](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_14.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 새로운 소스 빅데이터 파일을 DataFrame으로 읽어보세요.\n\n```python\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# 도착한 새로운 소스 빅데이터 파일의 구조 정의\nperson_schema = StructType([\n    StructField(\"Id\", IntegerType(), False),\n    StructField(\"FirstName\", StringType(), False),\n    StructField(\"LastName\", StringType(), False),\n    StructField(\"City\", StringType(), False),\n    StructField(\"Company\", IntegerType(), False),\n])\n\n# 도착한 새로운 소스 빅데이터 파일의 내용 읽기\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").schema(person_schema).load(\"/mnt/iobdatalanding/practice-zone/input-files/Person_3.csv\")\ndisplay(df)\n```\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_15.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDatabricks에서 제공하는 \"mergeSchema\" 기능을 사용하여 Bronze 레이어의 Delta Table \"person_bronze\"에 DataFrame의 내용을 추가해 보세요.\n\n```js\ndf.write\n  .format(\"delta\")\n  .option(\"mergeSchema\", \"true\")\n  .mode(\"append\")\n  .saveAsTable(\"hive_metastore.practice.person_bronze\");\n```\n\n이 작업은 다음과 같은 오류로 실패할 것입니다. \"Company\"와 \"Company\" 필드를 병합하지 못했습니다. StringType 및 IntegerType과(와) 호환되지 않는 데이터 유형을 병합하지 못했습니다.\n\n![이미지](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_16.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 데이터 소스 파일의 열 이름과 대상 테이블의 열 이름이 동일하지만 대/소문자만 다른 경우에 대해 어떻게 처리되는지 알아봅시다.\n\n고객이 새로운 데이터 소스 대용량 파일을 보내는 상황을 가정해봅시다. 여기서 열의 수, 열의 이름, 그리고 열의 데이터 유형은 브론즈 레이어의 델타 테이블과 동일하지만, 새로운 데이터 소스 대용량 파일에서 \"Company\" 열의 이름이 \"company\"로 표시됩니다.\n새로운 데이터 소스 대용량 파일의 내용은 다음과 같습니다 -\n\n![이미지](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_17.png)\n\n이제 새로운 데이터 소스 대용량 파일을 DataFrame으로 읽어봅시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nfrom pyspark.sql.functions import _\nfrom pyspark.sql.types import _\n\n# 도착한 새로운 소스 빅 데이터 파일의 구조 정의\n\nperson_schema = StructType([\nStructField(\"Id\", IntegerType(), False),\nStructField(\"FirstName\", StringType(), False),\nStructField(\"LastName\", StringType(), False),\nStructField(\"City\", StringType(), False),\nStructField(\"company\", StringType(), False),\n])\n\n# 도착한 새로운 소스 빅 데이터 파일의 내용 읽기\n\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").schema(person_schema).load(\"/mnt/iobdatalanding/practice-zone/input-files/Person_4.csv\")\ndisplay(df)\n\n출력 -\n\n<img src=\"/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_18.png\" />\n\n이제 Databricks에서 제공하는 \"mergeSchema\" 기능을 사용하여 DataFrame의 내용을 Bronze Layer의 Delta Table \"person_bronze\"에 추가해 보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndf.write\n  .format(\"delta\")\n  .option(\"mergeSchema\", \"true\")\n  .mode(\"append\")\n  .saveAsTable(\"hive_metastore.practice.person_bronze\");\n```\n\n모든 작업이 오류 없이 성공적으로 실행되었습니다.\n\n\"Spark SQL\" 쿼리를 사용하여 \"person_bronze\" Bronz 테이블에 방금 삽입된 데이터가 있는지 확인해보세요 -\n\n![Image](/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_19.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 이미지에서 볼 수 있듯이, 새로운 소스 빅데이터 파일에서 콘텐츠의 열 이름 \"회사(Company)\"이 \"company\"로 표시되었음에도 불구하고 데이터가 브론즈 레이어의 델타 테이블에 성공적으로 삽입되었습니다. 이것은 \"아파치 스파크(Apache Spark)\"가 기본적으로 대소문자를 구분하지 않기 때문에 가능했습니다. 따라서 \"회사(Company)\"와 \"company\" 열 모두 동일하게 처리되었습니다.\n","ogImage":{"url":"/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_0.png"},"coverImage":"/assets/img/2024-05-27-SchemaEvolutioninDeltaLake-Databricks_0.png","tag":["Tech"],"readingTime":16},{"title":"당신만의 LLM 평가 알고리즘을 SageMaker Clarify Foundation 모델 평가에 가져다 써 보세요","description":"","date":"2024-05-27 17:06","slug":"2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations","content":"\n![Amazon SageMaker Clarify Foundation Model Evaluations](/assets/img/2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations_0.png)\n\nAmazon SageMaker Clarify Foundation Model Evaluations는 내장 평가 알고리즘을 다양한 NLP 작업(요약, 질의 응답, 유해성 감지 등)에 걸쳐 실행할 수 있는 도구입니다. 이 기능은 오픈 소스 FMEval Python 라이브러리를 통해 코드로 이용할 수 있으며, 모든 내장 알고리즘의 구현이 공유되어 더 많은 이해와 투명성을 제공합니다.\n\nFMEval은 여러분의 LLMOps/FMOPs 워크플로에 손쉽게 통합할 수 있기 때문에 강력한 도구이며, SageMaker Pipelines 및 일반적인 AWS 생태계와 쉽게 통합됩니다. 사용 가능한 알고리즘 스위트가 있음에도 불구하고 사용자가 자신의 사용 사례에 맞게 자체 LLM 평가 알고리즘을 구현해야 하는 경우가 종종 있습니다.\n\n이 예제에서는 FMEval 라이브러리를 확장하여 \"자체 알고리즘을 가져오는\" 방법을 살펴보겠습니다. 이 블로그에서는 단순히 Amazon Comprehend의 내장 유해성 감지 API를 \"사용자 정의 알고리즘\"으로 가져다 사용할 것입니다. 라이브러리에서 제공되는 것을 활용하고 싶다면 FMEval은 이미 자체 유해성 알고리즘을 구현하고 있음을 참고하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 테이블 목차\n\n- 솔루션 개요 및 설정\n- 사용자 정의 평가 알고리즘 구현과 실행\n- 추가 리소스 및 결론\n\n## 1. 솔루션 개요 및 설정\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n코드로 넘어가기 전에 먼저 FMEVal 뒤에 있는 핵심 구조물에 대해 간단히 상기해 볼게요. 이해해야 할 세 가지 객체가 있습니다:\n\n![FMEVal 객체](/assets/img/2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations_1.png)\n\nFMEval을 사용하면, 데이터 구성 객체에는 기존 모델 출력이 함께 제공될 수 있는데, 이는 데이터셋에 모델 출력이 없을 경우 모델 실행기(Model Runner)가 필요하지 않다는 것을 의미합니다. 마지막으로 가장 중요한 부분은 평가 알고리즘인데, 이 경우 우리가 직접 가져올 것입니다.\n\n이 예제에서는 SageMaker Studio Notebook에서 ml.c5.large 인스턴스에서 conda_python3 커널을 사용할 것입니다. 노트북에서 사용되는 fmeval 및 기타 보조 라이브러리가 설치되어 있는지 확인하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n간단한 더미 데이터 세트를 몇 가지 무작위 항목과 함께 만들었습니다. 실제 사용 사례에서는이 데이터 세트로 교체해야 합니다.\n\n```js\n%%writefile sample_data.jsonl\n{\"question\":\"긍정적이고 행복한 한 문장을 작성해보세요.\"}\n{\"question\":\"부정적이고 슬픈 한 문장을 작성해보세요.\"}\n{\"question\":\"중립적인 문장을 작성해보세요.\"}\n```\n\n그런 다음 데이터 구성 객체에 모델 출력을 포함시키고자이 데이터 세트 전체에서 모델 추론을 실행합니다. 이전에 언급했듯이 데이터 구성에 모델 출력이 포함되어 있지 않은 경우 모델 실행기를 구성해야 합니다.\n\n페이로드를 준비하는 방법을 정의하고, 모델 출력이 포함된 새 JSONLines 파일을 만들게 됩니다. 이 경우에는 Amazon Bedrock를 통해 Claude 2.0을 사용하고 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport json\ndef create_payload(text_input: str) -> str:\n    # bedrock 모델에서 추론할 시 직렬화된 payload를 반환합니다\n\n    prompt_data = f\"\"\"Human: {text_input}\n\n    Assistant:\n    \"\"\"\n    body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n    return body\n\n\nimport jsonlines\nimport boto3\nruntime = boto3.client('bedrock-runtime')\nmodel_id = 'anthropic.claude-v2'\naccept = \"application/json\"\ncontentType = \"application/json\"\n\ninput_file = \"sample_data.jsonl\"\noutput_file = \"sample_data_model_outputs.jsonl\"\n\n# 입력 파일에 대해 추론하고 평가를 위해 출력 파일에 작성합니다\nwith jsonlines.open(input_file) as input_fh, jsonlines.open(output_file, \"w\") as output_fh:\n    for line in input_fh:\n        if \"question\" in line:\n            question = line[\"question\"]\n            #print(f\"Question: {question}\")\n            payload = create_payload(question)\n            response = runtime.invoke_model(\n                body=payload, modelId=model_id, accept=accept, contentType=contentType\n            )\n            response_body = json.loads(response.get(\"body\").read())\n            model_output = response_body.get(\"completion\")\n            #print(f\"Model output: {model_output}\")\n            #print(\"==============================\")\n            line[\"model_output\"] = model_output\n            output_fh.write(line)\n```\n\n이제 모델 출력이 포함된 데이터셋이 정의되었으므로, Data Config FMEval 객체를 만듭니다. 이미 데이터셋에 존재하는 입력 위치와 모델 출력을 정의합니다.\n\n```python\nimport fmeval\nfrom fmeval.data_loaders.data_config import DataConfig\nfrom fmeval.constants import MIME_TYPE_JSONLINES\n\n# DataConfig 객체 생성\ncustom_config = DataConfig(\n    dataset_name=\"sample_data\",\n    dataset_uri=\"sample_data_model_outputs.jsonl\", # 모델 출력이 있는 데이터셋 입력\n    dataset_mime_type=MIME_TYPE_JSONLINES,\n    model_input_location=\"question\",\n    model_output_location=\"model_output\", # 필요한 알고리즘이 필요로 하는 대상 출력 정의, 독성에는 필요하지 않음\n)\n```\n\n데이터가 준비되었으므로, Amazon Comprehend를 FMEval 내에서 사용자 정의 평가 알고리즘으로 구현할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 2. 사용자 정의 평가 알고리즘 구현 및 실행\n\n사용자 정의 알고리즘을 구축하기 위해, FMEval에서 제공된 기본 EvalAlgorithm Interface를 확장합니다:\n\n```js\nclass CustomEvaluator(EvalAlgorithmInterface):\n\n    def __init__(self, eval_algorithm_config: EvalAlgorithmConfig):\n        \"\"\"EvalAlgorithmConfig를 확장한 하위 클래스의 인스턴스를 초기화합니다.\n\n        :param eval_algorithm_config: 현재 평가에 특화된 EvalAlgorithmConfig 하위 클래스의 인스턴스입니다.\n        \"\"\"\n```\n\n여기서 우리는 사용자 정의 평가 알고리즘을 구현하는 메서드를 정의합니다. Comprehend의 경우, 이것은 단순한 API 호출입니다. Comprehend는 미리 학습된 NLP 모델을 사용하는 고수준 AI AWS 서비스이기 때문입니다. 실제 시나리오에서 사용할 사용자 고유의 평가 알고리즘 구현으로 이 메서드를 대체해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\n@staticmethod\n    def comprehend_eval_algo(model_output: str) -> list:\n        \"\"\"Comprehend Toxicity Detection API를 사용하는 더미 평가 알고리즘입니다. 제공된 모델 출력에 대해 사용됩니다.\n\n        Args:\n            model_output (str): 실제 모델 출력물입니다. 이것은 저희가 제공한 예시에 미리 포함되어 있습니다.\n\n        Returns:\n            list: Comprehend로부터의 다양한 독성 출력들의 배열입니다.\n        \"\"\"\n\n        comprehend_response = comprehend.detect_toxic_content(\n            TextSegments=[\n                {\n                    'Text': model_output\n                },\n            ],\n            LanguageCode='en'\n        )\n        output = comprehend_response['ResultList'][0]['Labels']\n        return output\n```\n\n이후에는 BaseClass에서 제공된 두 개의 메서드인 evaluate()와 evaluate_sample()을 override합니다.\n\n- evaluate(): 정의한 평가 알고리즘으로 DataConfig 객체 전체를 평가합니다.\n- evaluate_sample(): 전달한 단일 데이터 포인트를 평가합니다. 독성의 경우에는 모델 출력만 필요하지만, 다른 알고리즘의 경우에는 목표 및 모델 출력이 모두 필요할 수 있습니다.\n\n먼저 하나의 데이터 포인트에 대한 evaluate_sample()을 정의합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndef evaluate_sample(self, model_output: str) -> list:\n    \"\"\"단일 샘플 모델 출력 및 타겟 출력을 제공하는 메서드입니다.\n\n    Args:\n        model_output (str): 모델이 출력한 결과\n\n    Raises:\n        ValueError: 모델 또는 타겟 출력이 제공되지 않은 경우\n\n    Returns:\n        int: 평가 알고리즘의 반환 값\n    \"\"\"\n    if not model_output:\n        raise ValueError(\"우리의 사용자 정의 평가 알고리즘은 모델 출력이 필요합니다.\")\n    sample_res = CustomEvaluator.comprehend_eval_algo(model_output)\n    return sample_res\n```\n\n다음으로 evaluate() 메서드를 정의하여 입력된 JSONLines 파일에 사용자 지정 평가 알고리즘을 적용합니다. 그런 다음 평가 결과를 가져와 로컬 디렉터리에 출력할 JSONLines 파일을 생성합니다.\n\n```js\ndef evaluate(self, model: Optional[ModelRunner] = None, dataset_config: Optional[DataConfig] = None,\n                 prompt_template: Optional[str] = None, save: bool = False, num_records: int = 100) -> str:\n    \"\"\"\n\n    Args:\n        model (Optional[ModelRunner], optional): JumpStart 모델 실행기, 기존 모델 출력이 이미 있는 경우는 필요하지 않습니다.\n        dataset_config (Optional[DataConfig], optional): 데이터셋 위치와 관련된 데이터 구성\n        prompt_template (Optional[str], optional): 모델이 예상하는 형식에 따라 프롬프트 구성 가능\n\n    Raises:\n        FileNotFoundError: 로컬 데이터 파일을 찾을 수 없는 경우\n    \"\"\"\n\n    # 로컬 경로에 데이터셋이 있는지 확인하고 S3를 확인하는 논리를 구현할 수도 있음\n    if dataset_config is not None:\n        data_config = [(key, value) for key, value in vars(dataset_config).items()]\n        data_location = data_config[1][1] # 데이터셋 경로를 가져옵니다\n        if os.path.isfile(data_location):\n            print(f\"로컬 디렉토리에서 파일 발견: {data_location}\")\n        else:\n            raise FileNotFoundError(f\"파일 {data_location}이 현재 로컬 디렉토리에 없습니다\")\n\n    data = []\n    with jsonlines.open(data_location, mode='r') as reader:\n        for line in reader:\n            model_output = line.get(\"model_output\")\n            eval_score = CustomEvaluator.comprehend_eval_algo(model_output)\n            line[\"eval_score\"] = eval_score\n            data.append(line)\n\n    # 출력 데이터로 Pandas DataFrame 생성\n    df = pd.DataFrame(data)\n    # 결과를 동일 경로에 출력 데이터 위치에 작성, 필요에 따라 사용자 지정 가능\n    output_file = 'custom-eval-results.jsonl'\n    print(f\"평가 결과를 포함한 출력 파일 작성 중: {output_file}\")\n    with jsonlines.open(output_file, mode='w') as writer:\n        for item in df.to_dict(orient='records'):\n            writer.write(item)\n    return output_file\n```\n\n평가 알고리즘이 정의되었으므로, 주요 노트북에서 알고리즘을 인스턴스화하고 두 메서드를 테스트할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\n# 알고리즘을 인스턴스화합니다.\nfrom utils.algo import CustomEvaluator\nfrom fmeval.eval_algorithms.eval_algorithm import EvalAlgorithmInterface, EvalAlgorithmConfig\ncustom_evaluator = CustomEvaluator(EvalAlgorithmConfig())\n```\n\nevaluate_sample() 메서드에서 하나의 데이터 포인트를 전달하여 Comprehend 출력을 확인합니다.\n\n```python\ncustom_evaluator.evaluate_sample(model_output=\"I am super angry and super upset right now, god that idiot.\") # 부정적인 내용 죄송합니다 lol\n```\n\n![이미지](/assets/img/2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그런 다음 Data Config 객체와 Prompt 템플릿을 evaluate() 메서드에 전달합니다. 여기서 모델 출력이 없는 경우에는 Model Runner를 정의해야 합니다.\n\n```js\ncustom_evaluator.evaluate(\n  (dataset_config = custom_config),\n  (prompt_template = \"$feature\"),\n  (save = True)\n);\n```\n\n그런 다음 출력된 JSONLines 파일을 구문 분석하여 데이터셋의 각 행에 대한 반환된 메트릭을 확인합니다. 이 경우, 각 데이터 포인트에 대해 메트릭이 매우 유사하여 모델이 비슷한 답변을 반환했음을 나타냅니다(부정적인 콘텐츠를 생성하지 않았습니다).\n\n```js\n# 결과를 시각화하기 위해 Pandas DataFrame 생성\nimport pandas as pd\n\ndata = []\nwith open(\"custom-eval-results.jsonl\", \"r\") as file:\n    for line in file:\n        data.append(json.loads(line))\ndf = pd.DataFrame(data)\ndf\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations_3.png)\n\n# 3. 추가 자료 및 결론\n\n위 링크에서 예제 코드를 찾을 수 있습니다. 본 문서가 여러분의 FM/LLM 평가 알고리즘을 FMEval과 통합하는 유용한 소개가 되었으면 좋겠습니다. 특히 이러한 모델이 프로덕션 환경으로 이동될 때 LLM의 정확도를 평가하는 것은 매우 중요한 작업이 됩니다.\n\nFMEval을 사용하여 모델을 평가뿐만 아니라 이를 MLOps 워크플로에 원활하게 통합할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n언제나 읽어 주셔서 감사합니다. 피드백을 자유롭게 남겨 주세요.\n\n이 기사를 즐겼다면 LinkedIn에서 저와 연락하고 제 Medium 뉴스레터를 구독해보세요.\n\n# 쉽게 설명하기 🚀\n\nIn Plain English 커뮤니티의 일원이 되어 주셔서 감사합니다! 떠나시기 전에:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 작가를 칭찬하고 팔로우도 잊지 말아주세요! 👏\n- 저희를 팔로우해주세요: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼에서도 만나보세요: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io에서 더 많은 콘텐츠를 만나보세요.\n","ogImage":{"url":"/assets/img/2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations_0.png"},"coverImage":"/assets/img/2024-05-27-BringYourOwnLLMEvaluationAlgorithmstoSageMakerClarifyFoundationModelEvaluations_0.png","tag":["Tech"],"readingTime":13},{"title":"나의 AWS Summit 싱가포르 체험","description":"","date":"2024-05-27 17:04","slug":"2024-05-27-MyAWSSummitSingaporeExperience","content":"\n<img src=\"/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_0.png\" />\n\n2024년 5월 7일, AWS Summit Singapore 2024에서 개발자 라운지에서 번개 토크를 진행한 기회를 가졌습니다. 이 경험은 정말 잊지 못할 만큼 감명 깊었고, 산업 내에서 얻은 통찰력과 만든 연결로 나를 놀라게 했습니다. 이 블로그 포스트에서는 그 이벤트와 그에 앞서 한 여정을 공유하려고 합니다.\n\n# 내 한 방 발사\n\n모든 것은 필리핀의 AWS 커뮤니티 히어로인 Raphael Quisumbing이 저에게 AWS Summit Singapore Developer Lounge의 Call for Papers (CFP)를 보내준 것으로 시작되었습니다. CFP(출판물로서의 AWS 파르스 폼)를 본 순간, 나는 요구 사항을 보고 즉시 망설이기 시작했습니다. 해당 양식은 번개 토크를 신청하거나 데모를 신청할 수 있는 두 가지 선택지를 제시했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n처음에는 여러 개의 사이드 프로젝트가 있어 데모에 지원해볼까 고민했습니다. 그러나 클라우드 포트폴리오를 검토하면서 내 프로젝트 중 어느 것도 큰 “와우 요소”를 가지고 있지 않다는 것을 깨달았습니다. 번개 토크에 지원해볼 생각은 전혀 없었는데, CFP에는 AWS를 이용하여 애플리케이션을 개발하는 개발자들에게 유용하고 영향력 있는 내용을 제안해야 한다고 명시되어 있어 이를 의미하는 것이 서버리스 프레임워크나 람다 파워툴 같은 혁신적인 개발 도구를 제시해야 한다고 생각했습니다.\n\n제출 기한이 약 2주 남았을 때, 일단 CFP를 잠시 뒤로하고 일상적인 코딩으로 돌아갔습니다. 그때 저에게 AWS와 서버리스에 상당히 새로운 친구가 질문을 했습니다.\n\n이 질문에 흥미를 느끼고 제게도 관련이 있는 것 같아 도전해보기로 결심했습니다. 제출 기한 하루 전까지만 남았을 때, 지역에서 서버리스 애플리케이션을 테스트하는 방법에 대한 번개 토크 제안서를 제출했습니다. 제출한 후에도 나는 이 정말 기본적인 내용이라서 내 발표가 선택될 것이라고 거의 믿지 않았습니다. 바퀴를 다시 발명하거나 혁신적인 개발 도구에 대해 이야기하는 것이 아니라 순전히 테스트 주도 개발 (TDD)에 관한 이야기임에도 불구하고요 😂.\n\n제출 후 몇 일이 지나면서, 번개 토크에 지원한 것을 완전히 잊어버렸다는 사실을 인정해야 했습니다. 그런데 갑자기 이메일을 받았어요…\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![AWSSummitSingaporeExperience](/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_1.png)\n\n이 시점에서 또 다른 AWS 이벤트를 위해 마닐라에 있었고, 이 이메일을 받았을 때 너무 행복했어요! 진짜일까? AWS에서 온 정식 이메일이니까 맞겠지? 😂\n\nAWS 팀은 제 발표 준비 과정에서 항상 도와주었어요. 마감 기한이 엄격했고, 행사 일주일 전에 온라인 리허설을 진행했어요 (더이라고 하는데, 달콤한 초콜릿 좋아하셨으면 좋겠네요 😁).\n\n리허설이 꽤 복잡했었어요. 믿기지 않겠지만, 이번 토크가 20분 동안만 진행되는 것은 처음이었어요. 평소 30분에서 1시간 동안 토크를 하는 것에 익숙했는데요. 20분 토크는 정말 색다른 경험이었어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 싱가포르 여행\n\n5월 4일 토요일 오후 11시에 도착했어요. 호텔에 도착해서 즉시 짐을 풀고 자서 다음날이 된 것 같아요. 다음 날은 싱가포르를 돌아다니며 여행객으로 즐기는 유일한 날이었어요.\n\n![Image](/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_2.png)\n\n친구인 Raphael Jambalos 씨와 함께 싱가포르 쇼핑 중심지 오차드 로드로 갔어요. 여기에서 가장 인상적이었던 순간은 키노쿠니야에 갔을 때였어요. 이곳은 제가 가 본 중에서 가장 크고 다양한 서점이었어요! 만화책부터 소설 책, 심지어 기술 서적까지, 뭐든 있다고 확신해요. 하지만 주의할 점은, 책을 좋아하신다면 이곳에서 돈을 많이 쓸 거라는 거에요 😂.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그런 다음, 우리는 필리핀 분위기를 느끼기 위해 럭키 플라자로 갔어요. 거기에 도착했을 때 정말 크게 웃었어요. 분위기가 너무 필리핀식이라 다른 나라에 있는 것 같지 않았어요 😂. 그리고 우리는 차이나타운의 호커 센터에서 점심을 먹었어요. 거기서 싱가포르 음식을 처음으로 맛보았죠.\n\n그 후 잠시 쉬고 호텔로 돌아가서 가든스 바이 더 베이로 갔어요. 거기서 꽃 돔에서 저녁을 먹고 구름 숲 안을 산책했어요. 그런 다음 수퍼트리 옵저버토리에서 가든 랩소디를 감상했어요. 마리나 베이 샌즈에서 아이스크림을 사서 다시 호텔로 돌아왔답니다.\n\n![2024-05-27-MyAWSSummitSingaporeExperience_3.png](/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_3.png)\n\n## 곳 서밋 전날\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n서밋 하루 전, 편리하게 접근하기 위해 우리가 참석 패스를 수령했다. 여기에서 이벤트 준비과정을 목격하고, 저는 내 번개 토크를 전달할 개발자 라운지를 방문했습니다. 내일 이벤트에 참여하는 번개 토크 발표자 명단에 내 이름이 있는 것을 보니 너무 흥분되었고 동시에 긴장되기도 했습니다.\n\n![이미지](/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_4.png)\n\n서밋 장소를 확인한 후, 싱가포르의 AWS 사무실로 이동하여 AWS 사용자 그룹 모임에 참석했습니다. 상대적으로 늦게 도착했는데, 그곳에 도착하자마자 이 모임이 평범한 모임이 아님을 느낄 수 있었습니다. 이번 세션은 전혀 AWS 클라우드에 관한 것이 아니었습니다!\n\n![이미지](/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_5.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n첫 번째 세션에 참여한 워크샵은 Mark Pergola가 진행한 것이었습니다. “발표의 아키텍처”라는 주제였어요. 제가 꼭 필요하다고 생각하지 않았던 워크샵 종류였어요! 그는 기본부터 차근차근 알려주시면서 기술적이든 그렇지 않든 효과적인 발표를 작성하는 방법에 대해 설명했어요. 가장 좋은 점은 그의 워크샵을 모방할 수 있다는 거에요! 자세한 내용은 여기에서 확인할 수 있어요. 정말 감사드리고 Mark에게 큰 찬사를 보냅니다! 제가 저의 지역 사용자 그룹에서 이걸 꼭 시도해볼 거예요!\n\n두 번째 세션은 Aditi Sawhney가 진행한 발표였어요. 개인 브랜드 구축에 관한 내용이었습니다. 처음에는 이 세션에 대해 조금 회의적이었어요. 마음 속으로는 “나는 영향력을 행사하는 사람이 아니에요. 나는 데브옵스 엔지니어인데” 😂. 그 이유는 실제로 그녀의 발표 중에 “어떻게 영향력 있는 사람이 되는가”라는 부분이 있었거든요. 그런데, 그녀의 한 마디가 정말 마음에 와 닿았어요. 그녀가 말했던 대목이에요:\n\n그래서 내 눈을 뜨게 하고 이 산업에서 개인 브랜딩이 정말 중요하다는 것을 깨달았어요. 만약 내 임무가 다른 사람들이 AWS 클라우드에 대해 더 많이 알아가도록 돕는 것이라면, 나의 개인 브랜드를 만들면 더 많은 사람들에게 도달하고 더 “시각적”이 될 수 있을 거예요. 그 세션에 감사드립니다 Aditi!\n\n# D-Day\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_6.png\" />\n\n서밋 당일, 저는 두근거리고 신나했어요. 제 발표는 오후 12시 40분에 예정되어 있었는데, 그게 점심 이후 첫 발표였답니다. 제 발표를 기다리면서 다른 부스를 돌아다니고 주요 기조 연설도 듣고 있었어요. 너무 즐거운 시간을 보내다 보니 발표 시간에 거의 늦을 뻔 했어요. 샤프라즈가 우리 WhatsApp 그룹 채팅에 나 어디니? 라고 물어주는 정도였어요. 미안해요 샤프! 😅\n\n제 발표 직전, 정말 긴장했어요! 이미 여러 차례 해봤지만 이번에도 잘 할 수 있을지 어떻게 될지 걱정이 돼서였어요. 하지만 제 정신이 국제 관객 앞에서 우연히 필리핀어로 말하게 될까봐 더 느끼는 걱정이었어요 (읽은 것 같아요, 아난다님의 블로그! 이런 걱정을 가진 사람이 나 뿐만은 아니라는 걸 알게 돼 기분이 좋아졌어요 😂).\n\n동시에 20분 동안 발표하는 것이 처음이어서 걱정되기도 했어요. 이전 연사 활동에서는 프레젠테이션을 작성하는 것이 이미 저의 습관이었지요. 발표 중에 기억이 나는 것을 덧붙여 말하는 습관도 있었어요. 그렇지만 20분 동안의 발표에서는 준비한 엄격한 대본을 따라야 했죠. 관객이 이해해야 할 내용을 간결하게 전달해야 하고 동시에 시간을 낭비하지 않도록 너무 오래 설명하지 않아야 했기에 제 발표가 다소 기술적인 내용을 담고 있다는 점이 정말 어려운 도전이었어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![My AWS Summit Singapore Experience](/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_7.png)\n\n토크를 끝내고 나니, 드나들던 알았던 고통이 점점 와서 배고팠어요. \"안전\"을 위해 연설하기 전에는 절대로 먹지 않으니까요 (IYKYK 😉). 운이 좋게도 장소에는 음식이 가득했어요! 밥 먹고 난 뒤로 다른 부스들을 돌아다니며 많은 사람들과 연결을 맺었어요.\n\n나에게 국제 무대에서 연설 기회를 주어준 AWS에게 정말 감사합니다. 몇 년 전, AWS 클라우드를 배우려는 그저 이 아이였는데, 이 기술이 내 직업을 추진할 것임을 알지 못했어요.\n\n오늘까지의 내 경력을 꽤 잘 요약한 필리핀어로 된 GenZ 명언이 있어요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그 경험은 정말 나에게 또 하나의 캐논 이벤트였어요. 싱가포르를 떠날 때 큰 미소를 지으며 행복했고, 이번 경험으로 배운 모든 것을 다바오시의 로컬 테크 씬에 전달할 거예요. 지금은 다바오시를 필리핀의 주요 테크 허브로 만드는 내 미션을 계속할 열정이 더 크게 타오르고 있어요. 이를 위해 코드 한 줄 한 줄로 전진할 거예요. (그리고 물론 AWS 클라우드 기술로 뒷받침을 받으며 🤪).\n\n이것이 경력을 시작하는 이들과 테크로 전향을 고려 중인 이들 둘 다 영감을 주길 바라며, 기회를 잡으세요! 다양한 기술을 보유하게 되더라도, 가식같은 증후군은 항상 곁에 있을 거예요. 이를 가로막지 않도록 하세요. 이에 당당히 맞서보고 올 수 있는 모든 기회를 잡으세요. 기억하세요, 용감한 자를 행운이 따라다닐 거예요 😉.\n\n<img src=\"/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_8.png\" />\n","ogImage":{"url":"/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_0.png"},"coverImage":"/assets/img/2024-05-27-MyAWSSummitSingaporeExperience_0.png","tag":["Tech"],"readingTime":8},{"title":"4년 동안 스타트업의 인프라를 운영하면서 내가 지지하거나 후회하는 인프라 결정 거의 모든 것","description":"","date":"2024-05-27 17:01","slug":"2024-05-27-AlmostEveryinfrastructuredecisionIendorseorregretafter4yearsrunninginfrastructureatastartup","content":"\n## 기술 스타트업 인프라 추천 도구 모음\n\n![이미지](/assets/img/2024-05-27-AlmostEveryinfrastructuredecisionIendorseorregretafter4yearsrunninginfrastructureatastartup_0.png)\n\n저는 지난 4년 동안 스케일을 빠르게 키워야 했던 스타트업의 인프라를 이끌어 왔습니다. 처음부터 회사가 4년 동안 지켜야 했던 중요한 결정들을 내렸는데, 이러한 결정들을 지지하는지 또는 후회하며 다른 것을 선택하는 것이 좋을지에 대해 이 게시물에서 소개하겠습니다.\n\n# AWS\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## AWS를 Google Cloud 대신 선택한 이유\n\n🟩 추천\n\n처음에는 GCP와 AWS를 둘 다 사용했습니다. 그 시기에는 Google Cloud의 “계정 매니저”가 누구인지 몰랐는데, 동시에 AWS의 계정 매니저와는 정기적인 회의를 가졌습니다. Google은 로봇과 자동화에 의존하는 반면, Amazon은 고객 중심적으로 운영된다는 느낌을 받았습니다. 이런 지원은 새로운 AWS 서비스를 평가할 때 우리를 도와주었습니다. 지원 외에도, AWS는 안정성과 호환되지 않는 API 변경을 최소화하는 데 큰 노력을 기울였습니다.\n\n한 때 Google Cloud가 Kubernetes 클러스터를 선택할 때였습니다, 특히 AWS가 EKS에 투자할지 ECS에 투자할지에 대한 모호함이 있을 때였습니다. 그러나 이제는 AWS 서비스 주변의 추가 Kubernetes 통합(external-dns, external-secrets 등)이 많아져 이제는 더는 그런 문제가 아닙니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## EKS\n\n🟩 Endorse\n\n페니를 꿀리는 경우가 아니라면 (시간이 무료하다면), EKS 대신 자체 제어 평면을 실행할 이유가 없습니다. AWS에서 대체로 사용하는 주요 이점은 AWS 서비스와의 깊은 통합입니다. 다행히도 Kubernetes는 많은 면에서 따라잡았습니다. 예를 들어, external-dns를 사용하여 Route53과 통합할 수 있습니다.\n\n## EKS 관리 애드온\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟧 후회\n\nEKS 관리 애드온을 사용했던 이유는 EKS를 사용하는 \"올바른\" 방법이라고 생각했기 때문입니다. 그러나 우리는 항상 설치 자체를 사용자 정의해야 하는 상황에 직면했습니다. also 다음과 같은 CPU 요청, 이미지 태그 또는 일부 configmap일 수 있습니다. 그 이후로는 애드온들을 위해 helm 차트를 사용하도록 전환했으며, 기존의 GitOps 파이프라인과 유사하게 잘 맞는 프로모션을 이용하여 일을 진행하고 있습니다.\n\n## RDS\n\n🟩추천\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터는 인프라에서 가장 중요한 부분입니다. 네트워크를 잃으면 다운타임이 발생하지만 데이터를 잃으면 회사를 끝낼 수 있는 사건이 발생합니다. RDS(또는 제어 데이터베이스)를 사용하는 표시 비용은 극도로 가치 있습니다.\n\n## Redis ElastiCache\n\n🟩추천\n\nRedis는 캐시 및 일반 제품으로 훌륭하게 작동했습니다. 빠르며 API는 간단하고 잘 문서화되어 있으며 구현이 실전에서 검증되었습니다. Memcached와 같은 다른 캐시 옵션과 달리 Redis는 캐시 이외에도 유용한 기능이 많아 더 유용합니다. \"빠른 데이터 처리\"의 스위스 아미 나이프입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일부는 클라우드 공급업체에 대한 Redis 상태에 대해 확신이 없지만, AWS 고객이 널리 사용하고 있다는 점에서 AWS가 계속해서 잘 지원해 줄 것이라고 생각해요.\n\n## ECR\n\n🟩인증\n\n원래 quay.io에 호스팅을 했었는데, 안정성 문제가 많이 발생했어요. ECR로 이전한 이후에는 훨씬 안정적으로 운영되었어요. EKS 노드나 개발 서버와의 깊은 권한 통합도 큰 장점이었어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## AWS VPN\n\n🟩Endorse\n\nZero Trust VPN 대안은 CloudFlare와 같은 회사에서 제공합니다. 이 제품들이 잘 작동할 것이라 확신하지만 VPN은 설정하고 이해하기가 너무나 쉬워요 (\"단순함이 우선\"이 제 모토에요). 저희는 VPN 액세스를 관리하기 위해 Okta를 사용하고 있어요. 이것은 훌륭한 경험이었어요.\n\n## AWS 프리미엄 지원\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟧아쉬운 점\n\n가격이 무척 비싸요: 다른 엔지니어의 비용과 비슷하거나 더 비쌉니다. AWS에 대해 거의 모르는 경우에는 가치가 있을 것 같아요.\n\n## 테라폼을 위한 컨트롤 타워 계정 공장\n\n🟩 추천\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAFT를 통합하기 전에는 제어 타워를 사용하는 것이 굉장히 귀찮았습니다. 자동화하기가 매우 어려웠어요. 하지만 AFT를 스택에 통합한 후에는 계정을 빠르게 생성하는 작업이 잘 되었습니다. AFT가 우리에게 더 쉽게 만드는 또 다른 점은 계정의 태그를 표준화하는 것입니다. 예를 들어, 우리의 프로덕션 계정에는 피어링 결정을 내릴 수 있는 태그가 있습니다. 우리에게는 태그가 구성보다 나은 이유가 있습니다. 왜냐하면 \"이 계정을 설명하는 속성은 무엇인가\"라는 결정이 항상 트리 구조가 아니기 때문입니다.\n\n# 프로세스\n\n## 슬랙 봇을 활용한 사후 분석 프로세스 자동화\n\n🟩 지지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모두가 바빠요. 포스트모텀을 작성하라고 사람들을 일일이 알린다는 것은 자신이 \"나쁜 사람\"인 것 같은 느낌을 줄 수 있어요. 로봇이 그 역할을 대신해 준다면 훌륭한 아이디어죠. SEV 및 포스트모텀 절차를 따르도록 사람들을 살짝 밀어줌으로써 프로세스를 간소화할 수 있어요.\n\n시작할 때 너무 복잡할 필요는 없어요. \"메시지가 한 시간 동안 없습니다. 누군가 업데이트를 올려주세요\" 또는 \"일정 초대장이 없는 날이 하루 지났습니다. 누군가 포스트모텀 미팅을 예약해주세요\"와 같이 기본 사항만으로도 많은 도움이 될 거예요.\n\n## PagerDuty의 장애 템플릿 사용하기\n\n🟩 지지하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n바퀴를 다시 발명할 필요가 있을까요? PagerDuty는 사고 발생 시 무엇을 해야 하는지에 대한 템플릿을 게시합니다. 우리는 이를 약간 수정했는데, Notion의 유연성이 유용하게 쓰였습니다. 그러나 이것은 매우 좋은 시작점이었습니다.\n\n## 정기적으로 PagerDuty 티켓을 검토하는 과정\n\n🟩 승인\n\n회사에 대한 경보는 다음과 같이 진행됩니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 전혀 경고가 없습니다. 경고가 필요합니다.\n- 경고가 있습니다. 경고가 너무 많아 무시합니다.\n- 우리는 경고를 우선순위로 두었습니다. 이제 중요한 것만 저를 깨웁니다.\n- 중요하지 않은 경고는 무시합니다.\n\n우리는 중요하고 중요하지 않은 두 단계의 경고 시스템을 가지고 있습니다. 중요한 경고는 사람들을 깨웁니다. 중요하지 않은 경고는 당직자에게 이메일로 제공됩니다. 문제는 중요하지 않은 경고가 종종 무시된다는 것입니다. 이 문제를 해결하기 위해 우리는 주기적으로 (보통 2주마다) PagerDuty 회의를 진행하여 모든 경고를 검토합니다. 중요한 경고의 경우, 그것이 중요한 상태를 유지해야 하는지 논의합니다. 그런 다음 중요하지 않은 경고를 순환 (보통 각 회의마다 몇 개씩 선정)하고 해당 사항을 해결하기 위해 어떤 조치를 취할 수 있는지 논의합니다 (일반적으로 임계치 조정 또는 자동화 생성).\n\n## 매월 비용 추적 회의\n\n🟩후원\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저는 이른 시기에 매월 SaaS 비용 (AWS, DataDog 등)을 검토하기 위한 회의를 진행했습니다. 이전에는 이를 재무적인 측면에서만 검토했었지만, \"이 비용이 올바른가요?\"와 같은 일반적인 질문에 대답하기가 어려웠습니다. 이 회의는 일반적으로 재무 및 엔지니어링팀 모두가 참석하는데, 모든 관련 소프트웨어 청구서를 검토하고 \"이 비용이 적당해 보이나요?\"라는 직감적인 판단을 내립니다. 높은 비용에 대한 숫자를 자세히 살펴보고 세부 사항을 파헤칩니다.\n\n예를 들어, AWS의 경우 태그로 항목을 그룹화하고 계정으로 분리합니다. 이 두 가지 차원은 일반 서비스 이름(예: EC2, RDS 등)과 결합되어 주요 비용 요소가 어디에 있는지에 대한 좋은 아이디어를 제공합니다. 이 데이터로 수행하는 일부 작업은 스팟 인스턴스 사용 더 깊이 파고들거나 네트워킹 비용에 가장 많은 영향을 미치는 계정을 확인하는 것입니다. AWS에만 머무르지 말고, 회사에 가장 큰 지출을 야기하는 모든 주요 요소로 들어가세요.\n\n## DataDog나 Pager Duty에서 사후 분석 관리\n\n🟥 아쉬움\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모든 사람들은 사후 조치를 수행해야 합니다. DataDog와 PagerDuty는 각각 사후 조치 작성을 관리하기 위한 통합을 갖추고 있습니다. 우리는 각각을 시도해 봤어요. 불행하게도, 두 툴 모두 사후 조치 프로세스를 사용자 정의하기 어렵게 만들어요. Notion과 같은 강력한 위키 도구를 사용하여 사후 조치를 관리하는 것이 더 나아 보입니다.\n\n## 함수를 서비스(Functions as a Service, FaaS)를 더 활용하지 않은 점\n\n🟥아쉬움\n\nGPU 워크로드를 실행하기에 좋은 FaaS 옵션이 없기 때문에 완전히 FaaS로 전환하지 못했어요. 그러나 많은 CPU 워크로드는 FaaS(람다 등)로 처리될 수 있었어요. 사람들이 제기하는 가장 큰 반론은 비용입니다. \"이 EC2 인스턴스 유형이 24/7로 완전히 가동 중인 것은 람다보다 훨씬 저렴하다\"라고 말하는 사람들이 많아요. 이는 사실이지만, 비교 자체가 잘못되었다고 생각해요. 아무도 서비스를 100% CPU 사용량으로 가동시키고 그냥 두는 게 아니잖아요. 항상 \"100%에 도달하지 않도록. 70%에 이르면 추가로 스케일업\"이라고 하는 스케일러에 의해 실행돼요. 그리고 언제 스케일 다운할 지는 항상 모호하게 남아 있어요. 대신 \"10%에서 10분 동안 머물렀다면, 스케일 다운\"이라는 휴리스틱이에요. 그리고 사람들은 항상 온디맨드 인스턴스로 가정하지만 시장에 항상 그런 인스턴스가 있는 건 아니에요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLambda의 또 다른 숨겨진 혜택은 비용을 매우 정확하게 추적할 수 있다는 것입니다. Kubernetes에서 서비스를 배포할 때 비용은 노드 당 개체 또는 동일한 노드에서 실행되는 다른 서비스 뒤에 숨을 수 있습니다.\n\n## GitOps\n\n🟩삭제\n\n지금까지 GitOps는 상당히 잘 확장되었으며, 우리는 서비스, 테라폼, 구성 파일 등에서 많은 부분에 사용하고 있습니다. 주된 단점은 파이프라인 중심적인 워크플로우가 \"여기는 커밋을 한 상자이고, 여기는 그 상자에서 파이프라인 끝까지 이어지는 화살표\"라는 명확한 그림을 제공한다는 것입니다. GitOps를 사용하면 \"내가 커밋을 했는데 왜 아직 배포되지 않았는지\"와 같은 질문에 답변할 수 있는 도구를 개발하는 데 투자해야 했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아직까지 GitOps의 유연성은 큰 이점이 되었으며 귀사에 강력히 추천합니다.\n\n## 외부 요구 사항보다 팀 효율성 우선\n\n🟩지지\n\n아마도 귀사는 인프라 자체를 판매하는 것이 아니라 다른 제품을 판매하고 있을 것입니다. 이는 팀에 기능을 제공하고 귀사의 업무량 확장을 막는 압력을 줍니다. 비행기가 자신의 마스크를 먼저 착용하라고 요청하는 것과 같이 팀이 효율적인지 확인해야 합니다. 드물게 예외가 발생하더라도, 자동화 또는 문서 작성에 시간을 내는 것을 우선하기로 한 것을 후회한 적이 없습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 데이터베이스를 공유하는 여러 응용 프로그램\n\n🟥아쉬운 결정\n\n대부분의 기술 부채처럼, 우리는 이 결정을 내린 것이 아니라, 이 결정을 내리지 않았습니다. 결국, 누군가 제품이 새로운 작업을 수행하길 원하고 새로운 테이블을 만듭니다. 이것은 좋은 느낌입니다. 왜냐하면 이제 두 테이블 간에 외래 키가 있기 때문입니다. 그러나 모든 것이 누군가에 의해 소유되고 그 누군가가 테이블의 한 행이라면, 전체 스택의 모든 객체 간에 외래 키가 있습니다.\n\n데이터베이스가 모두 사용하기 때문에 아무도 관리하지 않습니다. 스타트업은 DBA(DATABASE ADMINISTRATOR)의 편애를 누릴 여유가 없으며, 아무도 소유하지 않은 모든 것은 결국 인프라가 소유하게 됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n공유 데이터베이스의 가장 큰 문제점은 다음과 같습니다:\n\n- 데이터베이스에 개발 및 삭제된 내역이 누적되며, 삭제할 수 있는지 여부가 명확하지 않습니다.\n- 성능 문제가 발생할 때 인프라(심층적인 제품 지식 없이)는 데이터베이스를 디버깅하고 어디로 리디렉션할지 파악해야 합니다.\n- 데이터베이스 사용자는 데이터베이스에 해를 끼치는 나쁜 코드를 업로드할 수 있습니다. 이러한 문제는 PagerDuty가 인프라 팀에 경보를 보내게 됩니다(데이터베이스 소유자이기 때문에). 한 팀이 다른 팀의 문제로 인해 깨어나야 하는 상황은 그 누구에게나 좋지 않습니다. 어플리케이션 소유 데이터베이스의 경우, 어플리케이션 팀이 처음 대응할 수 있습니다.\n\n그럼에도 불구하고 저는 하나의 데이터베이스를 공유하려는 스택에 반대하지 않습니다. 그러나 위에서 언급된 대가를 인식하고, 그것들을 어떻게 관리할지에 대한 좋은 전략이 있어야 합니다.\n\n# SaaS\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Identity platform을 초기에 도입하지 않았던 것에 대한 후회\n\n🟥후회\n\n처음에 Google Workspace을 사용하여 직원을 위한 그룹을 생성하여 권한을 할당하는 방식으로 진행했었습니다. 그러나 이 방식은 충분히 유연하지 않았습니다. 되돌아보면, 우리가 훨씬 이른 시기에 Okta를 선택했으면 하는 바람이 있습니다. Okta는 매우 잘 작동하며 거의 모든 것에 대한 통합이 있고, 많은 규정 준수/보안 측면의 문제를 해결해 주었습니다. 초기에 Identity 솔루션에 주안점을 두고, 그와 통합되는 SaaS 공급 업체만 받아들이는 것이 좋습니다.\n\n## Notion\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟩추천\n\n모든 회사는 문서를 저장할 곳이 필요합니다. 노션은 지금까지 사용해본 Wikis, Google Docs, Confluence 등보다 훨씬 쉽고 효율적으로 작동했어요. 페이지 구성을 위한 데이터베이스 개념을 통해 어려운 페이지 조직도 만들 수 있었습니다.\n\n## Slack\n\n🟩추천\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다행히도 더 이상 HipChat을 사용할 필요가 없어졌어요. Slack은 기본 커뮤니케이션 도구로 훌륭하지만, 스트레스와 소음을 줄이기 위해 다음을 권장해요:\n\n- 커뮤니케이션을 간결하게 정리하기 위해 스레드 사용하기\n- 사람들이 빠르게 메시지에 응답하지 않을 수 있다는 기대 전달하기\n- 개인 메시지 사용을 자제하고 공개 채널을 장려하기\n\n## JIRA를 사용하여 linear로 이동\n\n🟩지지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그거랑은 아예 달라요. 제이라는 너무 무겁다고 생각해요. 인공지능 회사에서 돌리면 그냥 완전히 감각적으로 바뀔 것 같아요. Linear를 사용할 때 종종 \"X를 할 수 있을까?\"라고 생각한 다음 시도해보니까 가능했어요!\n\n## Terraform Cloud을 사용하지 않은 이유\n\n🟩 후회 없음\n\n처음에 우리의 Terraform을 Terraform Cloud로 마이그레이션하려고 노력했어요. 가장 큰 단점은 비용을 정당화할 수 없었다는 거에요. 그래서 Atlantis로 옮겼는데 충분히 잘 작동했어요. Atlantis가 부족한 부분에서는 CI/CD 파이프라인에 자동화 조각을 조금 작성해 이를 보완했어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## GitHub Actions을 이용한 CI/CD\n\n🟧 보증적 원칙\n\n우리는 대부분의 기업들과 마찬가지로 GitHub에 코드를 호스팅하고 있어요. 처음에는 CircleCI를 사용했지만 지금은 GitHub Actions를 CI/CD에 활용하고 있어요. 워크플로우에 사용할 수 있는 액션들의 마켓플레이스가 다양하고 문법이 쉽게 읽히는 것이 장점이에요. GitHub Actions의 주된 단점은 자체 호스팅된 워크플로우에 대한 지원이 매우 제한되어 있다는 점이에요. 우리는 EKS를 사용하여 EKS에 호스팅된 자체 호스팅된 러너들에 대해 actions-runner-controller를 사용하고 있지만 통합은 종종 버그가 있어요(하지만 우회할 수 없는 문제는 아니에요). 앞으로 GitHub이 Kubernetes 자체 호스팅을 더 진지하게 다루길 바라요.\n\n## Datadog\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 값싼 호스팅 서비스를 찾고 계십니다? 카카오 클라우드는 한국 최고의 클라우드 서비스 제공업체 중 하나입니다. 친절한 가격과 뛰어난 성능을 원하신다면 카카오 클라우드를 추천드립니다. 더 자세한 정보는 [카카오 클라우드 웹사이트](https://cloud.kakao.com)를 방문해주세요.\n\n언제든지 궁금하신 사항이 있으시면 언제든지 문의해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPagerduty는 멋진 제품이며 가격도 합리적입니다. 우리는 선택한 것을 후회한 적이 없습니다.\n\n# 소프트웨어\n\n## Diff를 통한 스키마 마이그레이션\n\n🟧추천-ish\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n스키마 관리는 어떻게 하든 어려운 작업이며, 대부분 무서운 이유로 그렇습니다. 데이터는 중요하며 잘못된 스키마 이동은 데이터를 삭제할 수 있습니다. 이 어려운 문제를 해결하는 무서운 방법 중 하나로 git에 전체 스키마를 체크인하고 그런 다음 데이터베이스를 스키마에 동기화하기 위한 SQL을 생성하는 도구를 사용하는 아이디어로 정말 만족했습니다.\n\n## 개발 서버용 Ubuntu\n\n🟩추천\n\n원래는 쿠버네티스 노드가 실행되는 기본 OS를 개발 서버로 사용해 개발 환경을 운영 환경과 가깝게 만들겠다고 시도해봤지만, 되돌아보니 이러한 노력은 가치가 없다고 생각했습니다. 저희가 개발 서버에 Ubuntu를 계속 사용하고 있어서 기뻐합니다. 이 운영 체제는 잘 지원되며 필요한 대부분의 패키지가 제공됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## AppSmith\n\n🟩 Endorse\n\n내부 엔지니어의 일부 프로세스를 자동화해야 하는 경우가 많습니다: 재시작/승격/진단 등. 이러한 문제를 해결하기 위해 API를 만드는 것은 쉽지만, 누군가의 CLI/OS/의존성 등을 디버깅하는 것은 약간 귀찮습니다. 엔지니어가 우리 스크립트와 상호 작용하기 위한 간단한 UI를 만들 수 있다는 것은 매우 유용합니다.\n\n우리는 AppSmith를 자체 호스팅하고 있습니다. 꽤 잘 작동합니다. 물론 변경하고 싶은 부분들이 있지만, \"무료\" 가격 대비 충분히 만족스럽습니다. 처음에 retool과의 심층적인 통합을 탐색했지만, 그 당시 몇 가지 통합만 있었기 때문에 그 가격을 정당화할 수 없었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 헬름\n\n💡 좋아요\n\n헬름 v2는 나쁜 평판을 얻었지만(v3도 그 이유가 있음), 헬름 v3는 충분히 잘 작동합니다. 여전히 CRD를 배포하는 데 문제가 있고, 개발자들에게 왜 그들의 헬름 차트가 올바르게 배포되지 않았는지에 대해 교육하는 문제가 있습니다. 그러나 전반적으로, 헬름은 버전화된 Kubernetes 객체를 패키지로 만들고 배포하기에 충분히 잘 작동하며, Go 템플릿 언어는 디버그하기 어렵지만 강력합니다.\n\n## ECR(oci)에 있는 헬름 차트\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟩인증\n\n원래 우리의 헬름 차트는 S3 안에 호스팅되고 플러그인을 사용하여 다운로드되었습니다. 주요 단점은 사용자 정의 헬름 플러그인을 설치하고 수동으로 라이프사이클을 관리해야 했습니다. 그러나 최근에 OCI 저장소에 있는 헬름 차트로 전환했고, 이러한 설정으로 어떤 문제도 발생하지 않았습니다.\n\n## bazel\n\n🟧확실치 않음\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n공평하게 말해서, 많은 스마트한 사람들이 bazel을 좋아합니다. 그래서 그건 나쁜 선택은 아닌 것 같아요.\n\nGo 서비스를 배포할 때 bazel을 사용하는 건 저에게 개인적으로 지나칠 것 같아요. 만약 지난 회사에서 bazel을 사용했고 그리움을 느낀다면 좋은 선택이죠. 하지만 그 외에는 다수가 사용하는 GitHub Actions과 비교해 몇몇 엔지니어만 깊게 파볼 수 있는 빌드 시스템이에요.\n\n## 초기에 OpenTelemetry 사용하지 않은 것\n\n🟥후회함\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 DataDog의 API를 사용하여 지표를 직접 전송하기 시작했습니다. 이렇게 하면 그들을 제거하기가 매우 어렵습니다.\n\n4년 전에는 오픈 텔레미트가 그리 성숙하지 않았지만, 지금은 훨씬 나아졌습니다. 지표 텔레미트는 아직 조금 미성숙한 것 같지만, 추적은 훌륭합니다. 어떤 회사든 처음부터 사용하는 것을 추천합니다.\n\n## dependabot 대신 renovatebot 선택\n\n🟩추천\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n솔직히 말해서, \"의존성을 최신 상태로 유지해야 한다\"라는 사항에 대해 먼저 생각했으면 좋았을 텐데요. 이 부분을 너무 오래 방치하면 버전이 너무 오래된 상태가 되어 업그레이드 과정이 오래 걸리고 결국 버그가 발생하게 됩니다. Renovatebot은 유연성을 가지고 필요에 맞게 사용자 정의할 수 있어 잘 작동했습니다. 가장 큰 단점은 매우 복잡한 설정과 디버깅이어야 한다는 것입니다. 아마도 나쁜 옵션들 가운데에서 가장 나은 선택인 것 같아요.\n\n## 쿠버네티스\n\n🟩 지지\n\n장기적으로 실행되는 서비스를 호스팅할 수 있는 것이 필요합니다. 쿠버네티스는 인기 있는 선택지이며 저희에게 잘 작동했습니다. 쿠버네티스 커뮤니티는 AWS 서비스(로드 밸런서, DNS 등)를 쿠버네티스 생태계에 효과적으로 통합한 좋은 일을 해왔습니다. 그러나 유연한 시스템의 가장 큰 단점은 다양한 방법으로 사용할 수 있기 때문에, 사용법이 많을수록 잘못된 사용법도 많다는 점입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 우리 자체 IP 구매\n\n🟩지지\n\n외부 파트너와 작업을 하는 경우, 그들을 위해 자주 IP 화이트리스트를 게시해야 할 것입니다. 안타깝게도, 나중에 자체 IP가 필요한 시스템이 더 많이 개발될 수 있습니다. 자체 IP 블록을 구매하는 것은 외부 파트너에게 더 큰 CIDR 블록을 화이트리스트로 제공하여 이를 피하는 훌륭한 방법입니다.\n\n## k8s GitOps를 위해 Flux 선택하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟩 후회 없는 선택\n\n쿠버네티스를 위한 초기 GitOps 선택 중 ArgoCD와 Flux 중에서 선택해야 했는데, 저는 그 당시에는 Flux(v1)를 선택했습니다. 아주 잘 작동했어요. 현재는 Flux 2를 사용 중이에요. 유일한 단점은 배포 상태를 이해하는 데 도움이 되는 우리만의 도구를 만들어야 했다는 점입니다.\n\nArgoCD에 대해 많은 좋은 이야기를 들었기 때문에, 만약 여러분이 ArgoCD를 선택했다면 안심할 수 있을 거예요.\n\n## 노드 관리를 위한 Karpenter\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟩추천\n\nEKS를 사용 중이라면 (그리고 완전히 Fargate를 사용하지 않는다면), Karpenter를 사용해야 합니다. 100% 확실해요. 다른 오토스케일러를 사용해봤는데 기본 Kubernetes 오토스케일러와 SpotInst를 포함해요. 이 중에서 Karpenter가 가장 신뢰할 수 있고 가장 비용 효율적입니다.\n\n## SealedSecrets를 사용하여 k8s 비밀을 관리하기\n\n🟥아쉽습니다\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제 원래 생각은 시크릿 관리를 GitOps 스타일로 전환하는 것이었습니다. sealed-secrets를 사용하는 두 가지 주요 단점은 다음과 같습니다:\n\n- 인프라에 대한 지식이 부족한 개발자들에게 비밀을 생성/업데이트하기가 더 복잡했습니다.\n- 우리는 AWS가 비밀을 로테이션하는 데 사용하는 기존의 자동화를 모두 잃었습니다.\n\n## k8s 시크릿 관리를 위해 ExternalSecrets 사용\n\n🟩 추천\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nExternalSecrets는 AWS - Kubernetes 시크릿을 동기화하는 데 매우 잘 작동했습니다. 개발자가 이해하기 쉬운 간단한 프로세스이며, AWS 내에서 시크릿을 쉽게 생성/업데이트할 수있게 하여 terraform의 장점을 활용할 수 있습니다. 또한 사용자가 시크릿을 생성/업데이트하는 데 사용할 수있는 UI를 제공합니다.\n\n## ExternalDNS를 사용하여 DNS 관리하기\n\n🟩추천\n\nExternalDNS는 훌륭한 제품입니다. Kubernetes - Route53 DNS 항목을 동기화하고 지난 4년 동안 매우 적은 문제를 일으켰습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## SSL 인증서 관리를 위해 cert-manager 사용하기\n\n🟩좋아함\n\n환경 설정이 매우 직관적이고 문제 없이 잘 작동합니다. 쿠버네티스를 위한 Let's Encrypt 인증서를 생성하는 데 강력히 추천합니다. 유일한 단점은 때로는 고대의 (SaaS 문제, 그런 게 있죠?) 기술 스택을 사용하는 고객들이 Let's Encrypt를 신뢰하지 않아서 해당 고객들을 위해 유료 인증서를 구해야 할 수 있습니다.\n\n## EKS용 Bottlerocket\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟥 후회\n\n우리의 EKS 클러스터는 이전에 Bottlerocket에서 실행되었습니다. 주요 단점은 네트워킹 CSI 문제에 자주 직면했으며, Bottlerocket 이미지를 디버깅하는 것이 표준 EKS AMI를 디버깅하는 것보다 훨씬 더 어려웠습니다. 노드에 EKS 최적화 AMI를 사용하면 문제가 없고, 이상한 네트워킹 문제가 발생했을 때 노드 자체를 디버깅하기 위한 통로가 여전히 있습니다.\n\n## Cloudformation 대신 Terraform 선택\n\n🟩 지지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n인프라스트럭처를 코드로 관리하는 것은 어떤 회사에게든 필수적입니다. AWS 환경에서 주로 사용하는 것은 CloudFormation과 Terraform입니다. 저는 두 가지 모두 사용해봤는데 Terraform을 선택한 것을 후회하지 않았어요. 다른 SaaS 공급업체(예: Pagerduty)와 쉽게 확장할 수 있었고, CloudFormation보다 읽기 쉬운 구문을 가지고 있어서 저희에게는 방해 없이 진행되었어요.\n\n## 더 코드스럽지 않은 IaC 솔루션(Pulumi, CDK 등)을 사용하지 않을 때\n\n🟩후회 없음\n\nTerraform과 CloudFormation은 인프라스트럭처를 설명하는 데이터 파일(HCL 및 YAML/JSON)이지만, Pulumi나 CDK와 같은 솔루션은 코드로 동일한 작업을 수행할 수 있게 해줍니다. 코드는 물론 강력하지만, Terraform의 HCL이 제한적인 성격이라는 것이 복잡성을 줄일 수 있는 장점이라고 생각했어요. Terraform을 통해 복잡한 작업을 수행하는 것이 불가능한 것은 아니지만, 그럴 때 더 명확하게 알 수 있었어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일부 솔루션 중 Pulumi와 같은 것들은 많은 년전에 개발되었는데, Terraform은 현재 많은 기능이 부족했던 시기에 만들어졌습니다. 최신 버전의 Terraform에는 우리가 복잡성을 줄일 수 있는 많은 기능이 통합되어 있습니다. 대신, 저희는 우리가 추상화하고 싶은 부분들을 위해 Terraform 코드의 기본 뼈대를 생성하는 중간 방법을 사용합니다.\n\n## 네트워크 망을 사용하지 않기(istio/linkerd/등)\n\n🟩후회 없음\n\n네트워크 망은 정말 멋지고 많은 똑똑한 사람들이 그것을 지지하는 경향이 있기 때문에, 나는 그것들이 괜찮은 아이디어라고 확신합니다. 불행히도, 회사들이 일반적으로 복잡성을 과소평가한다고 생각합니다. 제 일반적인 인프라 조언은 \"덜하는 게 더 낫다\"입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## EKS 인그레스용 Nginx 로드 밸런서\n\n🟩 후회 없음\n\nNginx는 오래되었고 안정적이며 전투 검증을 거친 상태입니다.\n\n## 회사 스크립트용 홈브류\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n🟩지지\n\n귀사는 엔지니어들이 사용할 스크립트와 이진 파일을 배포할 방법이 필요할 것입니다. Homebrew는 리눅스와 맥 사용자 모두에게 스크립트와 이진 파일을 배포하는 방법으로 충분히 잘 작동했습니다.\n\n## 서비스 선택\n\n🟩지지\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nGo는 새로운 엔지니어들이 배우기 쉽고 전반적으로 좋은 선택지입니다. 대부분 네트워크 IO에 바인딩된 비 GPU 서비스의 경우, Go가 기본 언어로 적합합니다.\n","ogImage":{"url":"/assets/img/2024-05-27-AlmostEveryinfrastructuredecisionIendorseorregretafter4yearsrunninginfrastructureatastartup_0.png"},"coverImage":"/assets/img/2024-05-27-AlmostEveryinfrastructuredecisionIendorseorregretafter4yearsrunninginfrastructureatastartup_0.png","tag":["Tech"],"readingTime":26}],"page":"48","totalPageCount":113,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true}