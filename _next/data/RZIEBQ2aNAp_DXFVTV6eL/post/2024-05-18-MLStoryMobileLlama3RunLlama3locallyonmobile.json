{"pageProps":{"post":{"title":"ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기","description":"","date":"2024-05-18 20:07","slug":"2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile","content":"\n\n# 소개\n\n2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.\n\n언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.\n\n하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png\" />\n\n시작하기 전에, 먼저 파이프라인을 이해해 봅시다.\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png\" />\n\n자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기\n\n단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.\n\n```js\n# 저장소를 복제합니다.\ngit clone https://github.com/NSTiwari/Llama3-on-Mobile\n```\n\n단계 1: MLC-LLM 설치하기\n\n<div class=\"content-ad\"></div>\n\n모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.\n\n```js\n!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f https://mlc.ai/wheels\n!pip install numpy==1.23.5\n```\n\n단계 2: 라이브러리 가져오기\n\n```js\nimport mlc_llm\nimport torch\nfrom huggingface_hub import snapshot_download\n```\n\n<div class=\"content-ad\"></div>\n\nStep 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요\n\n```js\n# HF 계정에 로그인합니다.\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n# Llama-3-8B-Instruct 모델을 다운로드합니다.\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"/content/Llama-3-8B-Instruct/\")\n```\n\nStep 4: GPU가 활성화되었는지 확인하세요\n\n```js\n!nvidia-smi\n\n# CUDA가 사용 가능한지 확인합니다.\ntorch.cuda.is_available()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n```\n\n<div class=\"content-ad\"></div>\n\nStep 5: 모델 이름과 양자화 유형 구성\n\n```js\nMODEL_NAME = \"Llama-3-8B-Instruct\"\nQUANTIZATION= \"q4f16_1\"\n```\n\nStep 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환\n\n다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/\n```\n\n7단계: 토큰 파일 생성\n\n이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.\n\n```js\n!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \\\n    --conv-template llama-3 --context-window-size 8192 --prefill-chunk-size 1024  \\\n    -o /content/$MODEL_NAME-$QUANTIZATION-android/\n```\n\n<div class=\"content-ad\"></div>\n\n8단계: Android 형식으로 모델 컴파일하기\n\n여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.\n\n```js\n!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.json \\\n    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.tar\n```\n\n9단계: 모델을 Hugging Face에 올리기 🤗\n\n<div class=\"content-ad\"></div>\n\n마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.\n\n```js\nfrom huggingface_hub import whoami\nfrom pathlib import Path\n\n# 출력 디렉토리.\noutput_dir = \"/content/\" + MODEL_NAME + \"-\" + QUANTIZATION + \"-android/\"\nrepo_name = \"Llama-3-8B-q4f16_1-android\"\nusername = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\nrepo_id = f\"{username}/{repo_name}\"\n```\n  \n```js\nfrom huggingface_hub import upload_folder, create_repo\n\nrepo_id = create_repo(repo_id, exist_ok=True).repo_id\nprint(output_dir)\n\nupload_folder(\n    repo_id=repo_id,\n    folder_path=output_dir,\n    commit_message=\"Quantized Llama-3-8B-Instruct model for Android.\",\n    ignore_patterns=[\"step_*\", \"epoch_*\"],\n)\n```\n\n다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:\nhttps://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\n\n<div class=\"content-ad\"></div>\n\n여기 완전한 Colab 노트북을 찾을 수 있습니다.\n\n좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.\n\n## Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)\n\n이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.\n\n<div class=\"content-ad\"></div>\n\n윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.\n\n저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.\n\n비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.\n\n## 섹션 III: 빌드 종속성 설치\n\n<div class=\"content-ad\"></div>\n\n### 단계 1: Rust 설치하기\n\n안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png)\n\n표준 설치를 계속하려면 옵션 1을 선택하세요.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png)\n\n```js\nsudo curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nStep 2: Install NDK and CMake in Android Studio\n\nOpen Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.\n\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png\" />\n\n제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png\" />\n\n```js\n# MLC-LLM Python 패키지와 TVM Unity 컴파일러 설치.\npython3 -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly\n\n# 아래 명령어를 사용하여 설치 확인:\npython3 -c \"import mlc_llm; print(mlc_llm)\"\n```\n\n<div class=\"content-ad\"></div>\n\n**단계 4: CMake 설치하기**\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png)\n\n```js\n# CMake 설치하기.\nsudo apt-get install cmake\n```\n\n**단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기**  \n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png\" />\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png\" />\n\n```js\ncd /home/tiwarinitin1999/  \n\n# MLC-LLM 저장소를 복제합니다.\ngit clone https://github.com/mlc-ai/mlc-llm.git\ncd mlc-llm\n\n# 저장소의 서브모듈을 업데이트합니다.\ngit submodule update --init --recursive\n\n# Llama3-on-Mobile 저장소를 복제합니다.\ncd /home/tiwarinitin1999/\ngit clone https://github.com/NSTiwari/Llama3-on-Mobile.git\n```\n\n단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.\n\n<div class=\"content-ad\"></div>\n\nMLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png)\n\n```js\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat\nmkdir dist\ncd dist\nmkdir prebuilt\ncd prebuilt\n```\n\n그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png\" />\n\n```js\n# 퀀터이즈된 Llama3-8B-Instruct 가중치의 HF 리포지토리를 복제합니다.\ngit clone https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git\n```\n\n7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.\n\ndist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.\n\n<div class=\"content-ad\"></div>\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png)\n\n```bash\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist\nmkdir lib\ncd lib/\n```\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png)\n\nStep 8: mlc-package-config.json 파일 구성하기\n\n<div class=\"content-ad\"></div>\n\nMLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n    \"device\": \"android\",\n    \"model_list\": [\n        {\n            \"model\": \"Llama-3-8B-q4f16_1-android\",\n            \"bundle_weight\": true,\n            \"model_id\": \"llama-3-8b-q4f16_1\",\n            \"model_lib\": \"llama-q4f16_1\",\n            \"estimated_vram_bytes\": 4348727787,\n            \"overrides\": {\n                \"context_window_size\":768,\n                \"prefill_chunk_size\":256\n            }         \n        }\n    ],\n    \"model_lib_path_for_prepare_libs\": {\n        \"llama-q4f16_1\": \"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar\"\n    }\n}\n```\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png)\n\n9단계: 경로에 환경 변수 설정하기\n\n<div class=\"content-ad\"></div>\n\n<html>\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png\" />\n</html>\n\n```js\nexport ANDROID_NDK=/home/tiwarinitin1999/Android/Sdk/ndk/27.0.11718014\nexport TVM_NDK_CC=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang\nexport TVM_HOME=/home/tiwarinitin1999/mlc-llm/3rdparty/tvm\nexport JAVA_HOME=/home/tiwarinitin1999/Downloads/android-studio/jbr\nexport MLC_LLM_HOME=/home/tiwarinitin1999/mlc-llm\n```\n\nStep 10: 안드로이드 빌드 파일 생성\n\n마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.\n\n<div class=\"content-ad\"></div>\n\n```sh\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat\npython3 -m mlc_llm package\n```\n\n![Screenshot](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png)\n\n명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.\n\n![Screenshot](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png)\n```\n\n<div class=\"content-ad\"></div>\n\n위 명령은 다음 파일을 생성합니다:\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png)\n\n소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:\n\n소스 디렉토리:\n/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output\n\n<div class=\"content-ad\"></div>\n\n목적지 디렉토리:\n/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output\n\n이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n  \"model_list\": [\n    {\n      \"model_id\": \"llama-3-8b-q4f16_1\",\n      \"model_lib\": \"llama-q4f16_1\",\n      \"model_url\": \"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\",\n      \"estimated_vram_bytes\": 4348727787\n    }\n  ]\n}\n```\n\n설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png\" />\n\n모든 구성이 설정되었습니다.\n\n## 섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기\n\n안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.\n\n<div class=\"content-ad\"></div>\n\n\n[![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png)]\n(https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif)\n\n모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.\n\n오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.\n\n\n<div class=\"content-ad\"></div>\n\n위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.\nhttps://github.com/NSTiwari/Llama3-on-Mobile\n\n작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.\n\n## 감사의 글\n\nMobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.\n\n<div class=\"content-ad\"></div>\n\n## 참고 자료 및 자원\n\n- Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북\n- MobileLlama3 GitHub 저장소\n- 변환된 가중치를 위한 HuggingFace 저장소\n- Meta사의 Llama3 모델들\n- MLC-LLM\n- MLC-LLM용 Android SDK","ogImage":{"url":"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png"},"coverImage":"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png","tag":["Tech"],"readingTime":12},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h1>소개</h1>\n<p>2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.</p>\n<p>언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.</p>\n<p>하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.</p>\n<p>시작하기 전에, 먼저 파이프라인을 이해해 봅시다.</p>\n<p>자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.</p>\n<h2>섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기</h2>\n<p>단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.</p>\n<pre><code class=\"hljs language-js\"># 저장소를 복제합니다.\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/NSTiwari/Llama3-on-Mobile</span>\n</code></pre>\n<p>단계 1: MLC-LLM 설치하기</p>\n<p>모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.</p>\n<pre><code class=\"hljs language-js\">!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//mlc.ai/wheels</span>\n!pip install numpy==<span class=\"hljs-number\">1.23</span><span class=\"hljs-number\">.5</span>\n</code></pre>\n<p>단계 2: 라이브러리 가져오기</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> mlc_llm\n<span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"hljs-keyword\">import</span> snapshot_download\n</code></pre>\n<p>Step 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">HF</span> 계정에 로그인합니다.\n<span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"hljs-keyword\">import</span> notebook_login\n<span class=\"hljs-title function_\">notebook_login</span>()\n\n# <span class=\"hljs-title class_\">Llama</span>-<span class=\"hljs-number\">3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> 모델을 다운로드합니다.\n<span class=\"hljs-title function_\">snapshot_download</span>(repo_id=<span class=\"hljs-string\">\"meta-llama/Meta-Llama-3-8B-Instruct\"</span>, local_dir=<span class=\"hljs-string\">\"/content/Llama-3-8B-Instruct/\"</span>)\n</code></pre>\n<p>Step 4: GPU가 활성화되었는지 확인하세요</p>\n<pre><code class=\"hljs language-js\">!nvidia-smi\n\n# <span class=\"hljs-variable constant_\">CUDA</span>가 사용 가능한지 확인합니다.\ntorch.<span class=\"hljs-property\">cuda</span>.<span class=\"hljs-title function_\">is_available</span>()\n\ndevice = torch.<span class=\"hljs-title function_\">device</span>(<span class=\"hljs-string\">\"cuda:0\"</span> <span class=\"hljs-keyword\">if</span> torch.<span class=\"hljs-property\">cuda</span>.<span class=\"hljs-title function_\">is_available</span>() <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"cpu\"</span>)\ndevice\n</code></pre>\n<p>Step 5: 모델 이름과 양자화 유형 구성</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-variable constant_\">MODEL_NAME</span> = <span class=\"hljs-string\">\"Llama-3-8B-Instruct\"</span>\n<span class=\"hljs-variable constant_\">QUANTIZATION</span>= <span class=\"hljs-string\">\"q4f16_1\"</span>\n</code></pre>\n<p>Step 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환</p>\n<p>다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.</p>\n<pre><code class=\"hljs language-js\">!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/\n</code></pre>\n<p>7단계: 토큰 파일 생성</p>\n<p>이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.</p>\n<pre><code class=\"hljs language-js\">!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \\\n    --conv-template llama-<span class=\"hljs-number\">3</span> --context-<span class=\"hljs-variable language_\">window</span>-size <span class=\"hljs-number\">8192</span> --prefill-chunk-size <span class=\"hljs-number\">1024</span>  \\\n    -o /content/$MODEL_NAME-$QUANTIZATION-android/\n</code></pre>\n<p>8단계: Android 형식으로 모델 컴파일하기</p>\n<p>여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.</p>\n<pre><code class=\"hljs language-js\">!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.<span class=\"hljs-property\">json</span> \\\n    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.<span class=\"hljs-property\">tar</span>\n</code></pre>\n<p>9단계: 모델을 Hugging Face에 올리기 🤗</p>\n<p>마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"hljs-keyword\">import</span> whoami\n<span class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Path</span>\n\n# 출력 디렉토리.\noutput_dir = <span class=\"hljs-string\">\"/content/\"</span> + <span class=\"hljs-variable constant_\">MODEL_NAME</span> + <span class=\"hljs-string\">\"-\"</span> + <span class=\"hljs-variable constant_\">QUANTIZATION</span> + <span class=\"hljs-string\">\"-android/\"</span>\nrepo_name = <span class=\"hljs-string\">\"Llama-3-8B-q4f16_1-android\"</span>\nusername = <span class=\"hljs-title function_\">whoami</span>(token=<span class=\"hljs-title class_\">Path</span>(<span class=\"hljs-string\">\"/root/.cache/huggingface/\"</span>))[<span class=\"hljs-string\">\"name\"</span>]\nrepo_id = f<span class=\"hljs-string\">\"{username}/{repo_name}\"</span>\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"hljs-keyword\">import</span> upload_folder, create_repo\n\nrepo_id = <span class=\"hljs-title function_\">create_repo</span>(repo_id, exist_ok=<span class=\"hljs-title class_\">True</span>).<span class=\"hljs-property\">repo_id</span>\n<span class=\"hljs-title function_\">print</span>(output_dir)\n\n<span class=\"hljs-title function_\">upload_folder</span>(\n    repo_id=repo_id,\n    folder_path=output_dir,\n    commit_message=<span class=\"hljs-string\">\"Quantized Llama-3-8B-Instruct model for Android.\"</span>,\n    ignore_patterns=[<span class=\"hljs-string\">\"step_*\"</span>, <span class=\"hljs-string\">\"epoch_*\"</span>],\n)\n</code></pre>\n<p>다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:\n<a href=\"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\" rel=\"nofollow\" target=\"_blank\">https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android</a></p>\n<p>여기 완전한 Colab 노트북을 찾을 수 있습니다.</p>\n<p>좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.</p>\n<h2>Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)</h2>\n<p>이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.</p>\n<p>윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.</p>\n<p>저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.</p>\n<p>비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.</p>\n<h2>섹션 III: 빌드 종속성 설치</h2>\n<h3>단계 1: Rust 설치하기</h3>\n<p>안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.</p>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png\" alt=\"이미지\"></p>\n<p>표준 설치를 계속하려면 옵션 1을 선택하세요.</p>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png\" alt=\"image\"></p>\n<pre><code class=\"hljs language-js\">sudo curl --proto <span class=\"hljs-string\">'=https'</span> --tlsv1<span class=\"hljs-number\">.2</span> -sSf <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//sh.rustup.rs | sh</span>\n</code></pre>\n<p>Step 2: Install NDK and CMake in Android Studio</p>\n<p>Open Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.</p>\n<p>제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-variable constant_\">MLC</span>-<span class=\"hljs-variable constant_\">LLM</span> <span class=\"hljs-title class_\">Python</span> 패키지와 <span class=\"hljs-variable constant_\">TVM</span> <span class=\"hljs-title class_\">Unity</span> 컴파일러 설치.\npython3 -m pip install --pre -U -f <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly</span>\n\n# 아래 명령어를 사용하여 설치 확인:\npython3 -c <span class=\"hljs-string\">\"import mlc_llm; print(mlc_llm)\"</span>\n</code></pre>\n<p><strong>단계 4: CMake 설치하기</strong></p>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png\" alt=\"이미지\"></p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">CMake</span> 설치하기.\nsudo apt-get install cmake\n</code></pre>\n<p><strong>단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기</strong></p>\n<pre><code class=\"hljs language-js\">cd /home/tiwarinitin1999/  \n\n# <span class=\"hljs-variable constant_\">MLC</span>-<span class=\"hljs-variable constant_\">LLM</span> 저장소를 복제합니다.\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/mlc-ai/mlc-llm.git</span>\ncd mlc-llm\n\n# 저장소의 서브모듈을 업데이트합니다.\ngit submodule update --init --recursive\n\n# <span class=\"hljs-title class_\">Llama3</span>-on-<span class=\"hljs-title class_\">Mobile</span> 저장소를 복제합니다.\ncd /home/tiwarinitin1999/\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//github.com/NSTiwari/Llama3-on-Mobile.git</span>\n</code></pre>\n<p>단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.</p>\n<p>MLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.</p>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png\" alt=\"이미지\"></p>\n<pre><code class=\"hljs language-js\">cd /home/tiwarinitin1999/mlc-llm/android/<span class=\"hljs-title class_\">MLCChat</span>\nmkdir dist\ncd dist\nmkdir prebuilt\ncd prebuilt\n</code></pre>\n<p>그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.</p>\n<pre><code class=\"hljs language-js\"># 퀀터이즈된 <span class=\"hljs-title class_\">Llama3</span>-8B-<span class=\"hljs-title class_\">Instruct</span> 가중치의 <span class=\"hljs-variable constant_\">HF</span> 리포지토리를 복제합니다.\ngit clone <span class=\"hljs-attr\">https</span>:<span class=\"hljs-comment\">//huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git</span>\n</code></pre>\n<p>7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.</p>\n<p>dist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.</p>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png\" alt=\"image\"></p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">cd</span> /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist\n<span class=\"hljs-built_in\">mkdir</span> lib\n<span class=\"hljs-built_in\">cd</span> lib/\n</code></pre>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png\" alt=\"image\"></p>\n<p>Step 8: mlc-package-config.json 파일 구성하기</p>\n<p>MLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:</p>\n<pre><code class=\"hljs language-js\">{\n    <span class=\"hljs-string\">\"device\"</span>: <span class=\"hljs-string\">\"android\"</span>,\n    <span class=\"hljs-string\">\"model_list\"</span>: [\n        {\n            <span class=\"hljs-string\">\"model\"</span>: <span class=\"hljs-string\">\"Llama-3-8B-q4f16_1-android\"</span>,\n            <span class=\"hljs-string\">\"bundle_weight\"</span>: <span class=\"hljs-literal\">true</span>,\n            <span class=\"hljs-string\">\"model_id\"</span>: <span class=\"hljs-string\">\"llama-3-8b-q4f16_1\"</span>,\n            <span class=\"hljs-string\">\"model_lib\"</span>: <span class=\"hljs-string\">\"llama-q4f16_1\"</span>,\n            <span class=\"hljs-string\">\"estimated_vram_bytes\"</span>: <span class=\"hljs-number\">4348727787</span>,\n            <span class=\"hljs-string\">\"overrides\"</span>: {\n                <span class=\"hljs-string\">\"context_window_size\"</span>:<span class=\"hljs-number\">768</span>,\n                <span class=\"hljs-string\">\"prefill_chunk_size\"</span>:<span class=\"hljs-number\">256</span>\n            }         \n        }\n    ],\n    <span class=\"hljs-string\">\"model_lib_path_for_prepare_libs\"</span>: {\n        <span class=\"hljs-string\">\"llama-q4f16_1\"</span>: <span class=\"hljs-string\">\"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar\"</span>\n    }\n}\n</code></pre>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png\" alt=\"이미지\"></p>\n<p>9단계: 경로에 환경 변수 설정하기</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-variable constant_\">ANDROID_NDK</span>=<span class=\"hljs-regexp\">/home/</span>tiwarinitin1999/<span class=\"hljs-title class_\">Android</span>/<span class=\"hljs-title class_\">Sdk</span>/ndk/<span class=\"hljs-number\">27.0</span><span class=\"hljs-number\">.11718014</span>\n<span class=\"hljs-keyword\">export</span> <span class=\"hljs-variable constant_\">TVM_NDK_CC</span>=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang\n<span class=\"hljs-keyword\">export</span> <span class=\"hljs-variable constant_\">TVM_HOME</span>=<span class=\"hljs-regexp\">/home/</span>tiwarinitin1999/mlc-llm/3rdparty/tvm\n<span class=\"hljs-keyword\">export</span> <span class=\"hljs-variable constant_\">JAVA_HOME</span>=<span class=\"hljs-regexp\">/home/</span>tiwarinitin1999/<span class=\"hljs-title class_\">Downloads</span>/android-studio/jbr\n<span class=\"hljs-keyword\">export</span> <span class=\"hljs-variable constant_\">MLC_LLM_HOME</span>=<span class=\"hljs-regexp\">/home/</span>tiwarinitin1999/mlc-llm\n</code></pre>\n<p>Step 10: 안드로이드 빌드 파일 생성</p>\n<p>마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.</p>\n<pre><code class=\"hljs language-sh\"><span class=\"hljs-built_in\">cd</span> /home/tiwarinitin1999/mlc-llm/android/MLCChat\npython3 -m mlc_llm package\n</code></pre>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png\" alt=\"Screenshot\"></p>\n<p>명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.</p>\n<p><img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png\" alt=\"Screenshot\"></p>\n<pre><code>\n&#x3C;div class=\"content-ad\">&#x3C;/div>\n\n위 명령은 다음 파일을 생성합니다:\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png)\n\n소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:\n\n소스 디렉토리:\n/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output\n\n&#x3C;div class=\"content-ad\">&#x3C;/div>\n\n목적지 디렉토리:\n/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output\n\n이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n  \"model_list\": [\n    {\n      \"model_id\": \"llama-3-8b-q4f16_1\",\n      \"model_lib\": \"llama-q4f16_1\",\n      \"model_url\": \"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\",\n      \"estimated_vram_bytes\": 4348727787\n    }\n  ]\n}\n</code></pre>\n<p>설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.</p>\n<p>모든 구성이 설정되었습니다.</p>\n<h2>섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기</h2>\n<p>안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.</p>\n<p>[<img src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png\" alt=\"image\">]\n(<a href=\"https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif\" rel=\"nofollow\" target=\"_blank\">https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif</a>)</p>\n<p>모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.</p>\n<p>오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.</p>\n<p>위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.\n<a href=\"https://github.com/NSTiwari/Llama3-on-Mobile\" rel=\"nofollow\" target=\"_blank\">https://github.com/NSTiwari/Llama3-on-Mobile</a></p>\n<p>작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.</p>\n<h2>감사의 글</h2>\n<p>MobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.</p>\n<h2>참고 자료 및 자원</h2>\n<ul>\n<li>Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북</li>\n<li>MobileLlama3 GitHub 저장소</li>\n<li>변환된 가중치를 위한 HuggingFace 저장소</li>\n<li>Meta사의 Llama3 모델들</li>\n<li>MLC-LLM</li>\n<li>MLC-LLM용 Android SDK</li>\n</ul>\n</body>\n</html>\n"},"__N_SSG":true}