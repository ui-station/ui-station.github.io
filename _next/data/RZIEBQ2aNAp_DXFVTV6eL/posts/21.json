{"pageProps":{"posts":[{"title":"꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake","description":"","date":"2024-05-23 14:09","slug":"2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines","content":"\n\n<img src=\"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png\" />\n\n빅 데이터 시대에는 정보 관리가 지속적인 싸움입니다. 데이터 양이 급증함에 따라 저장뿐만 아니라 가치 있는 통찰을 추출하기 위한 효율적인 처리와 분석도 필요합니다. 이것이 꿈의 팀이 나타나는 곳입니다: Snowflake, Databricks 및 Delta Lake. 각각이 강력한 솔루션으로 클라우드 기반의 솔루션을 결합하여 효율적인 데이터 파이프라인을 만들어내어 당신을 챔피언처럼 데이터를 관리할 수 있게 합니다.\n\n플레이어와 그들의 역할:\n\n- Snowflake: 주역 쿼터백. 구조화된 데이터를 저장하고 분석하는 데 뛰어난 클라우드 기반 데이터 웨어하우스인 Snowflake는 데이터 분석가에게 친숙한 사용자 친화적 SQL 인터페이스를 통해 있는대로 데이터 탐색 및 효율적인 쿼리를 가능하게 합니다. 또한, Snowflake는 확장성과 보안을 자랑하며 데이터가 항상 접근 가능하고 보호되도록 보장합니다.\n- Databricks: 만회하는 와이드 리시버. Apache Spark를 기반으로 한 Databricks는 대규모 데이터 처리와 고급 분석의 달인입니다. 다양한 소스에서 데이터 수집, 데이터 변환 (정제 및 가공) 및 심지어 머신러닝 모델을 구축하고 배포하는 것과 같은 작업에서 빛을 발합니다. Databricks는 분석을 위해 데이터를 준비하는 일꾼 역할을 합니다.\n- Delta Lake: 신뢰할 수 있는 러닝 백. 데이터 호수 위에 구축된 오픈 소스 저장 레이어인 Delta Lake는 데이터 안정성의 챔피언입니다. 기존 또는 반구조적 데이터에 구조와 관리성을 추가하여 데이터 호수 안에 위치한 Delta Lake는 ACID 트랜잭션 (데이터 일관성 보장), 스키마 강제 사항 (데이터 구조 정의), 데이터 버전 관리 (데이터 변경 추적)와 같은 기능을 제공합니다. 데이터의 정확성과 신뢰성을 보장하는 데이터의 기초로 생각할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n승리의 플레이북: 간소화된 데이터 파이프라인 구축\n\n성능과 효율성을 최적화하여 각 단계를 효율적으로 흐르는 데이터 파이프라인을 상상해보세요. Snowflake, Databricks 및 Delta Lake가 이를 달성하는 방법은 다음과 같습니다:\n\n- Touchdown — 데이터 수집: Databricks는 다양한 소스(데이터베이스, API 또는 데이터 레이크에 있는 raw 파일 등)에서 데이터를 가져오는데 사용되는 다양한 커넥터를 통해 데이터를 추출합니다.\n- The Handoff — 데이터 변환: Databricks가 중심에 위치합니다. Spark의 처리 능력이 빛나며 주입된 데이터를 노트북을 사용하여 정제, 변환 및 준비합니다. 이 단계에서는 누락된 값을 처리하고 데이터 유형을 변환하거나 새로운 기능을 도출하는 등 분석 전에 모두 중요한 과정입니다.\n- The Choice of Plays — ETL 대 ELT: 여기서 데이터 저장을 위해 두 가지 옵션이 있습니다:\n\n  - ETL (추출, 변환, 로드): 이 플레이에서 Databricks는 변환된 데이터를 데이터 웨어하우스인 Snowflake로 이관하는 중심 역할을 합니다. Snowflake는 데이터를 안전하게 저장하여 쿼리 및 분석에 쉽게 사용할 수 있게 합니다.\n  - ELT (추출, 로드, 변환): 이 플레이는 Delta Lake의 강점을 활용합니다. 원본 데이터는 데이터 레이크에 직접 들어가며, Databricks는 클라우드 환경 내에서 데이터를 직접 Delta 테이블에 변환합니다. 이 접근 방식은 데이터 이동을 최소화하지만 분석을 위해 Delta 테이블에 의존하기 전에 데이터 품질을 보장해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n4. 승리의 춤 — 데이터 분석 및 소비: Snowflake가 슬기롭게 떠나다. 데이터 분석가 및 데이터 과학자들은 Snowflake의 SQL 인터페이스를 활용하여 Snowflake에 저장된 데이터에 대한 철저한 탐색, 시각화 및 고급 분석을 할 수 있습니다. 또한, 구체적인 사용 사례를 위해 Delta 테이블을 직접 쿼리하거나 실시간 스코어링과 분석을 위해 훈련된 머신러닝 모델을 Snowflake에 배포할 수 있습니다. 이를 통해 데이터로부터 가치 있는 통찰력을 얻을 수 있습니다.\n\n샘플 코드 실행 (Delta Lake를 사용한 ETL):\n\n```js\n# Databricks 노트북 - Python\n\n# 1. Snowflake에 연결\nsnowflake_conn = {\n    \"host\": \"your_snowflake_account.snowflakecomputing.com\",\n    \"user\": \"your_username\",\n    \"password\": \"your_password\",\n    \"database\": \"your_database\",\n    \"schema\": \"your_schema\"\n}\n\n# 2. CSV 파일에서 데이터 로드 (데이터 레이크에 저장된 것으로 가정)\ndf = spark.read.csv(\"path/to/your/data.csv\", header=True)  # 헤더가 있는 CSV를 가정\n\n# 3. 데이터 변환 (예시: 결측값 처리)\ndf = df.fillna(0, subset=[\"column_with_missing_values\"])  # 결측값을 0으로 대체\n\n# 4. 변환된 데이터를 Snowflake 테이블에 작성\ndf.write.format(\"snowflake\").options(**snowflake_conn).mode(\"overwrite\").saveAsTable(\"your_snowflake_table_name\")\n```\n\n게임 넘어서: 파이프라인 최적화\n\n<div class=\"content-ad\"></div>\n\n팀의 승리를 위해서는 지속적인 최적화가 필요해요. 여기 몇 가지 추가 팁이 있어요:\n\n- 올바른 전략 선택하기: ETL 및 ELT 방법 중 선택할 때 데이터 양, 처리 복잡성 및 원하는 대기 시간과 같은 요소를 신중하게 고려해주세요.\n- 데이터 품질이 중요해요: 통찰력의 무결성을 보장하기 위해 데이터 품질 점검을 파이프라인 전체에 구현해주세요.\n- 전략 실행하기: Airflow나 Databricks Workflows와 같은 도구를 활용하여 데이터 파이프라인의 실행을 일정화하고 조정하여 정보가 원활하게 흐를 수 있도록 해주세요.\n- 비용 관리: Snowflake와 Databricks는 사용량 기반의 가격 모델을 갖고 있어요. 자원 사용량을 모니터링하고 Databricks의 유휴 클러스터를 중지하는 등 비용 절감 방안을 도입해주세요.\n\n최종 엔딩: 데이터의 힘을 발휘해보세요\n\nSnowflake, Databricks, 그리고 Delta Lake를 결합하여 견고하고 확장 가능한 데이터 관리 시스템을 만들 수 있어요. 이 꿈의 팀은 대용량 데이터의 과제에 대처하고, 원시 정보를 실행 가능한 통찰력으로 변화시키는 능력을 부여해줘요. 상상해보세요:\n\n<div class=\"content-ad\"></div>\n\n- 정보에 대한 빠른 통찰력: 간소화된 데이터 파이프라인은 유용한 데이터에보다 빠르게 액세스할 수 있도록 도와주어 데이터 기반의 결정을 이전보다 빨리 내릴 수 있습니다.\n- 개선된 데이터 거버넌스: Delta Lake 및 Snowflake의 보안 기능은 데이터의 보호와 신뢰성을 보장하여 데이터 분석에 대한 신뢰를 증진시킵니다.\n- 미래를 위한 유연성: 이 아키텍처는 확장 가능하게 설계되었습니다. 데이터 요구 사항이 발전함에 따라, 새로운 데이터 소스, 처리 요구 사항 및 분석 요구 사항을 수용하기 위해 파이프라인을 조정하고 확장할 수 있습니다.\n\n그러니 자신감 있게 필드에 발을 들이세요. Snowflake, Databricks 및 Delta Lake가 곁에 있는 당신은 데이터를 관리하고 그 참된 잠재력을 개방할 수 있는 승리팀입니다.","ogImage":{"url":"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png"},"coverImage":"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터베이스DBT 간단 정리","description":"","date":"2024-05-23 14:08","slug":"2024-05-23-DBTinaNutshell","content":"\n\n이 게시물은 일반적으로 DBT(Data Build Tool)에 대한 내 이해와 누가 사용해야 하고 사용하지 말아야 하는지에 중점을 둘 것입니다. 앞으로의 게시물에서 더 자세히 다룰 예정입니다.\n\nDBT(Data Build Tool)와 Snowflake❄️를 통해 비즈니스 문제 해결하기👨‍💻\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_0.png)\n\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 1 : [링크](https://lnkd.in/gASCckRR)\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 2 : [링크](https://lnkd.in/gMqfKZRW)\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 3 : [링크](https://lnkd.in/g8BuWy66)\n\n<div class=\"content-ad\"></div>\n\n◉ 참여하기:\n👉 GitHub: [링크](https://lnkd.in/ggt3ZzUx)\n🚀 기여하고, 복제하고, 공유하세요!\n\n◉ YouTube 채널 찾기🎥: [링크](https://lnkd.in/esW5M3vb)\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_1.png)\n\n- DBT란 무엇인가요?\n- DBT는 무료인가요?\n- DBT가 해결하고 있는 문제는 무엇인가요?\n- ETL의 어느 부분에서 DBT를 사용하나요?\n- DBT가 지원하는 데이터 어댑터(데이터 플랫폼)는 무엇인가요?\n- DBT가 Databricks와 같은 기존 제품과 다른 점은 무엇인가요?\n- DBT를 배우고 사용하는 방법은 무엇인가요?\n\n<div class=\"content-ad\"></div>\n\n## DBT이란 무엇인가요?\n\nDBT(data build tool)는 SQL 스크립트(.sql)와 YAML 스크립트(.yml)를 사용하여 워크플로우를 변환하는 데 사용되는 오픈 소스 도구(Core 버전)입니다.\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_2.png)\n\n- SQL 스크립트는 CTE를 사용하여 데이터를 모듈화된 방식으로 변환하는 데 도움이 됩니다.\n- YAML 스크립트는 스키마, 설명 및 열에 대한 테스트 규칙(Null이 아닌 값, 고유 값 등)을 정의하는 데 도움이 됩니다.\n\n<div class=\"content-ad\"></div>\n\n## DBT은 무료인가요?\n\nDBT에는 DBT 코어와 DBT 클라우드 두 가지 버전이 있습니다.\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_3.png)\n\n- DBT 코어: CLI(Command Line Interface) 버전으로, 간단한 pip 명령어 `pip install dbt-core`를 사용하여 설치할 수 있습니다. 연결을 위해 어댑터(snowflakes, SQL Server)를 설치하려면 `pip install dbt-snowflake`를 사용하세요.\n- DBT 클라우드: GIT 및 어댑터(데이터 소스)를 통합하고 드래그 앤 드롭 기능을 사용하여 워크스페이스를 구성할 수 있는 GUI(Graphical User Interface)를 제공하며, 모델(당신의 .sql 파일)을 예약할 수 있는 기능이 추가되어 있습니다.\n\n<div class=\"content-ad\"></div>\n\n## DBT가 해결하려는 문제는 무엇인가요?\n\n![DBTinaNutshell_4](/assets/img/2024-05-23-DBTinaNutshell_4.png)\n\n원활한 협업:\n\n- 협업을 위한 통합 플랫폼을 즐기세요.\n- 효율적인 팀워크를 위한 CI/CD-Git 통합을 활용하세요.\n\n<div class=\"content-ad\"></div>\n\n편리한 데이터 변환:\n\n- 번거로움 없이 간단한 SQL 선택 문을 사용하여 데이터를 변환하세요.\n\n테스트로 신뢰성 확보:\n\n- 사용자 정의 테스트 케이스를 포함하여 데이터 변환을 테스트하세요.\n\n<div class=\"content-ad\"></div>\n\n편리하게 배포하고 일정을 계획하세요:\n\n- 코딩을 개발 및 프로덕션과 같은 다양한 환경에 배포하고 일정을 조정하세요.\n\n간편하게 작업 문서화하세요:\n\n- 간단한 .yml 파일로 전체 프로세스를 문서화하세요.\n- 데이터 변환 여정에 대한 포괄적인 문서를 작성하세요.\n\n<div class=\"content-ad\"></div>\n\n## ETL에서 DBT는 어느 부분에 사용되나요?\n\nDBT는 다른 ELT✅ 방법을 사용합니다❌ ETL이 아닌데요, 여기서는 데이터 플랫폼으로 모든 데이터를 추출 및 로드하고 DBT를 사용하여 변형한 후 다양한 사용 사례를 위해 다시 데이터 플랫폼에 로드합니다.\n\n<img src=\"/assets/img/2024-05-23-DBTinaNutshell_5.png\" />\n\n## DBT는 어떤 데이터 어댑터(데이터 플랫폼)를 지원하나요?\n\n<div class=\"content-ad\"></div>\n\n지금은 다음 데이터 플랫폼을 직접 지원합니다:\n\n- Snowflakes\n- Google Big Query\n- Data Bricks\n- AWS Redshift\n- Trino\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_6.png)\n\nTrino는 직접적으로가 아닌 Trino를 통해 여러 데이터 소스에 연결하는 데 사용됩니다. 자세한 내용은 이미지를 참조하세요.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-DBTinaNutshell_7.png\" />\n\n## DBT와 Databricks와 같은 다른 기존 제품과의 차이점은 무엇인가요?\n\n- DBT: DBT는 주로 데이터웨어하우스 내에서 데이터 변환 및 문서화 문제를 .sql 및 .yml 파일을 사용하여 CICD-git 환경에서 간단하고 효과적으로 처리하는 것에 중점을 둡니다.\n- Databricks (유명한 예시): Databricks는 대용량 데이터 분석을 위한 협업 환경을 제공하는 통합 분석 플랫폼입니다. 데이터 엔지니어링, 머신 러닝, 협업 데이터 과학 기능이 포함되어 있으며 주로 분산 데이터 처리를 위해 Spark와 함께 사용됩니다.\n\n## 어떻게 DBT를 학습하고 사용할 수 있을까요?\n\n<div class=\"content-ad\"></div>\n\n- DBT를 배우기 시작하려면 해당 웹사이트에서 시작할 수 있어요. 총 16개의 다양한 코스가 초급, 중급 및 고급 수준으로 분류되어 있어요.\n\n![DBTinaNutshell_8](/assets/img/2024-05-23-DBTinaNutshell_8.png)\n\n- DBT를 더 잘 배울 수 있도록 많은 매체 기사와 유튜브 비디오들이 있어요 (DBT 콘텐츠에 대한 업데이트 받으시려면 지켜봐주세요! 다가오는 게시물 및 DBT 관련 비디오에 대한 업데이트를 받으시려면 저를 팔로우해주세요).\n\nLeo Godin의 DBT 시리즈 게시물: 링크\n\n<div class=\"content-ad\"></div>\n\n나에 대해 더 알아보기:\n\n저는 데이터 과학 애호가🌺이며, 수학, 비즈니스 및 기술이 데이터 과학 분야에서 더 나은 결정을 내리는 데 어떻게 도움이 될 수 있는지에 대해 배우고 탐구하고 있습니다.\n\n더 많은 내용 보기: https://medium.com/@ravikumar10593/\n\n내 모든 핸들 찾기: https://linktr.ee/ravikumar10593\n\n<div class=\"content-ad\"></div>\n\n나의 뉴스레터를 찾아보세요: [https://substack.com/@ravikumar10593](https://substack.com/@ravikumar10593)","ogImage":{"url":"/assets/img/2024-05-23-DBTinaNutshell_0.png"},"coverImage":"/assets/img/2024-05-23-DBTinaNutshell_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리","description":"","date":"2024-05-23 14:05","slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka","content":"\n이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\n\n목차:\n\n1. 스트림 처리란\n2. 카프카의 특징\n3. 카프카 구성\n4. 카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\n5. 스파크 구조화 스트리밍 API\n6. 데이타브릭스 델타 레이크\n7. 실전 프로젝트\n\n이전 데이터 보안 부분으로 이동하는 링크입니다:\n\n## 스트림 처리란?\n\n<div class=\"content-ad\"></div>\n\n일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\n\n![데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png)\n\n스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\n\n카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n## 카프카 특징\n\n- 견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\n- 유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\n- 확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\n\n  - 파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\n  - 수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\n\n## 카프카 구성\n\n<div class=\"content-ad\"></div>\n\n카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\n\n- 파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\n- 복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\n- 유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\n- 자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\n- ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\n\n## 다른 카프카 서비스\n\nKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\" />\n\nksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\" />\n\n스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png)\n\n## Spark Structured Streaming API\n\nSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png)\n\n<div class=\"content-ad\"></div>\n\n따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\n\n또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png)\n\n## Databricks Delta Lake\n\n<div class=\"content-ad\"></div>\n\nDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\n\nDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\n\n스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\n\n<img src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n## 실시간 스트리밍 아키텍처\n\n카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\n\n문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\n\n- 단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\n- 단계 2: 토픽 생성\n- 단계 3: 클러스터 API 키 쌍 생성\n\n위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\n\n- 단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png)\n\nUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\n\n- Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png)\n\n\n<div class=\"content-ad\"></div>\n\n- 단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\n\n- 단계 7: Kafka 토픽에서 스트림 읽기\n\n```js\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Environmental Reporting\").getOrCreate()\n\nkafkaDF = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"abcd.us-west4.gcp.confluent.cloud:9092\") \\\n    .option(\"subscribe\", \"illegal_dumping\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\" password=\"\";\"\"\") \\\n    .load()\n\nprocessedDF = kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\ndisplay(processedDF)\n```\n\n- 단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pyspark.sql.functions as F\nfrom  pyspark.sql.functions import col, struct, to_json\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\njson_schema = StructType(\n  [\n    StructField(\"incident_type\", StringType(), nullable = False),\n    StructField(\"location\", StringType(), nullable = False),\n    StructField(\"description\", StringType(), nullable = True),\n    StructField(\"contact\", StringType(), nullable = True)\n  ]\n)\n\n# Using Spark SQL to write queries on the streaming data in processedDF\n\nquery = processedDF.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n      .select(F.col(\"value.incident_type\"),F.col(\"value.location\"))\ndisplay(query)\n```\n\nWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\n\nDefine a UDF(User Defined Function) to find out the region from the location:\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the regions_to_states dictionary\nregions_to_states = {\n    'South': ['West Virginia', 'District of Columbia', 'Maryland', 'Virginia',\n              'Kentucky', 'Tennessee', 'North Carolina', 'Mississippi',\n              'Arkansas', 'Louisiana', 'Alabama', 'Georgia', 'South Carolina',\n              'Florida', 'Delaware'],\n    'Southwest': ['Arizona', 'New Mexico', 'Oklahoma', 'Texas'],\n    'West': ['Washington', 'Oregon', 'California', 'Nevada', 'Idaho', 'Montana',\n             'Wyoming', 'Utah', 'Colorado', 'Alaska', 'Hawaii'],\n    'Midwest': ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota',\n                'Iowa', 'Missouri', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana',\n                'Ohio'],\n    'Northeast': ['Maine', 'Vermont', 'New York', 'New Hampshire', 'Massachusetts',\n                  'Rhode Island', 'Connecticut', 'New Jersey', 'Pennsylvania']\n}\n\n#from geotext import GeoText\nfrom geopy.geocoders import Nominatim\n\n# Define a function to extract state names from location text\ndef extract_state(location_text):\n    geolocator = Nominatim(user_agent=\"my_application\")\n    location = geolocator.geocode(location_text)\n    #print(location)\n    #print(type(location.raw))\n    if location:\n        state = location.raw['display_name'].split(',')[-2]\n        return state\n    else:\n        return \"Unknown\"\n\n# Create a UDF to map states to regions\n@udf(StringType())\ndef map_state_to_region(location):\n    state = extract_state(location).strip()\n    for region, states in regions_to_states.items():\n        if state in states:\n            return region\n    return \"Unknown\"  # Return \"Unknown\" for states not found in the dictionary\n\n# Apply the UDF to map states to regions\ndf_with_region = query.withColumn(\"region\", map_state_to_region(query[\"location\"]))\n\ndisplay(df_with_region)\n```\n\n\n<div class=\"content-ad\"></div>\n\n- 단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# VADER 감정 분석기 초기화\nanalyzer = SentimentIntensityAnalyzer()\n\n# Description 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef analyze_sentiment(description):\n    # VADER에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.polarity_scores(description)['neg']\n\n    # 감정 점수를 기반으로 심각도 분류\n    if sentiment_score >= 0.4:\n        return \"High\"\n    elif sentiment_score >= 0.2 and sentiment_score < 0.4:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# 감정 분석을 위한 UDF 생성\nsentiment_udf = udf(analyze_sentiment, StringType())\n\n# 처리된 DataFrame(processedDF)의 description 열에 UDF 적용\n# 실제 DataFrame 및 열 이름으로 \"processedDF\" 및 \"description_column\"을 대체합니다.\nprocessedDF_with_severity = query.withColumn(\"severity\", sentiment_udf(\"description\"))\n\n# 추가된 심각도 열이 있는 DataFrame 표시\ndisplay(processedDF_with_severity)\n```\n\n환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\n\n데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\n\n<div class=\"content-ad\"></div>\n\n![Real-time Stream Processing](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png)\n\n위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\n\n또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\n\n```js\n# 스트리밍 DataFrame을 임시 뷰로 등록\nprocessedDF_with_severity.createOrReplaceTempView(\"incident_reports\")\n\n# 집계를 위한 SQL 쿼리 정의\ntotal_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\"\"\n\nseverity_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\"\"\n\n# 집계 수행\ntotal_incidents_df = spark.sql(total_incidents_query)\nseverity_incidents_df = spark.sql(severity_incidents_query)\n\n```\n\n<div class=\"content-ad\"></div>\n\n\n\n![Data Engineering Concepts](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png)\n\nSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\n\n1. 비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\n2. 전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\n\n- 단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\n\n```js\n# Delta Lake에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \"`result_delta_table`\"\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \"/FileStore/tables/checkpoints\"\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.writeStream\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_location)\\\n  .trigger(processingTime='10 seconds')\\ # 10초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .format(\"delta\")\\\n  .toTable(delta_table_path)\n```\n\n\n\n<div class=\"content-ad\"></div>\n\n그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\n\n![Delta Table as Dataframe](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png)\n\n또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\n\n![Query Delta Table with SQL](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png)\n\n<div class=\"content-ad\"></div>\n\n읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\n\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png","tag":["Tech"],"readingTime":13},{"title":"데이터 엔지니어링 디자인 패턴","description":"","date":"2024-05-23 14:04","slug":"2024-05-23-DataEngineeringDesignPatterns","content":"\n\n디자인 패턴은 소프트웨어 엔지니어들만을 위한 것은 아닙니다. 최신 데이터 솔루션을 구축하는 데 도움이 되는 인기있는 데이터 엔지니어링 디자인 패턴을 알아봅시다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png)\n\nELT 패턴: 추출, 로드, 변환\n\n이는 RDBMS 세계에서 인기있던 ETL 패턴의 후속입니다. 데이터 엔지니어들이 다양한 소스(RDBMS, API 또는 스크래핑)에서 데이터를 추출하고, S3, ADLS Gen 2, 또는 GCS와 같은 객체 스토어에 로드한 후 Databricks와 같은 현대적인 도구를 사용하여 효율적으로 변환하는 인기있는 공통 패턴입니다.\n\n<div class=\"content-ad\"></div>\n\n![Lakehouse Pattern](/assets/img/2024-05-23-DataEngineeringDesignPatterns_1.png)\n\nDatabricks은 Lakehouse 아키텍처의 선두주자입니다. 이는 데이터 레이크(Data Lake)와 데이터 웨어하우스(Datawarehouse)를 통합합니다. 원시 또는 가공된, 구조화된 또는 반구조화된 데이터가 모두 하나의 환경 안에 모두 사용 가능하며 하나의 플랫폼(Databricks)에서 액세스할 수 있습니다.\n\n![Lakehouse Pattern](/assets/img/2024-05-23-DataEngineeringDesignPatterns_2.png)\n\n<div class=\"content-ad\"></div>\n\n중개 아키텍처 패턴:\n\n메달리온 아키텍처 패턴은 주로 Databricks와 함께 사용되며 이제는 사실상의 표준이 되었습니다. 데이터 처리를 위해 원본 데이터인 청동, 정리 및 풍부화된 데이터인 은, 그리고 비즈니스 수준의 집계한 데이터인 금 레이어로 이루어져 있습니다.\n\nDeltaLake 아키텍처 패턴:\n\nDelta Lake은 초기에 Databricks에서 개발된 오픈 소스 프로젝트입니다. 이 프로젝트는 데이터 호수에 안정성을 제공합니다. Deltalake는 ACID 트랜잭션, 타임 트래벌, z-order, CDC, 스키마 진화 및 기타 최적화로 데이터 호수를 개선합니다.\n\n<div class=\"content-ad\"></div>\n\nKappa Architecture Pattern:\n\nKappa Architecture에서는 모든 데이터가 스트림으로 처리됩니다. 이는 기존에 배치 처리로 다뤄졌을 것들을 연속적인 데이터 스트림으로 처리하는 것을 의미합니다. 시스템은 데이터가 도착하는 즉시 실시간으로 처리하며 별도의 배치로 처리하지 않습니다. Kappa Architecture는 실시간 처리에 최적화되어 있지만, \"배치\" 데이터로 간주될 수 있는 대규모의 과거 데이터를 스트림으로 재생하여 처리할 수 있습니다. Databricks Autoloader는 Kappa Architecture를 구현하는 데 가장 적합합니다.\n\nServerless Architecture Pattern:\n\n데이터 엔지니어링에서 서버리스는 클라우드 지역/IP 제약 사항이나 기타 기본 인프라 제약 조건에 대해 걱정하지 않고 데이터 파이프라인을 구축할 수 있습니다. 특히 변수 사용 패턴을 갖는 워크로드에 대한 즉시 클러스터 가용성과 비용 효율적인 스케일링을 위해 유용합니다. 많은 클라우드 제공업체가 이를 제공하고 있습니다. Databricks SQL은 Serverless를 제공하며, SQL 웨어하우스를 3초 미만의 시간 안에 론칭할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\nMLFlow 아키텍처 패턴:\n\nMLflow는 Databricks가 개발한 오픈 소스 프로젝트로, 실험, 재현성, 모델 배포 및 모델 제공을 포함한 기계 학습 라이프사이클을 관리하는 데 사용됩니다.\n\n이러한 패턴은 견고하고 확장 가능한 데이터 시스템을 설계하는 데 중요합니다. Databricks를 이용하면 이러한 패턴을 더욱 간편하게 구현할 수 있으며 효율적인 데이터 처리 및 분석을 위한 통합 도구를 제공합니다.\n\n좋아하는 패턴이 빠졌나요? 댓글 섹션에서 알려주세요. 흥미로운 내용이라고 생각되면 반가워 하지 마세요.","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png","tag":["Tech"],"readingTime":3},{"title":"데이터브릭의 DBRX를 사용하면 실시간으로 학습할 수 있어요, 미세 조정 필요 없어요","description":"","date":"2024-05-23 14:02","slug":"2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning","content":"\n![DBRX](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_0.png)\n\nDBRX이란 무엇인가요?\n\nDBRX는 Databricks의 최신 언어 모델 중 하나입니다. 언어 모델이란 충분한 예제를 학습하여 인간의 언어나 다른 유형의 복잡한 데이터를 인식하고 해석할 수 있는 컴퓨터 프로그램입니다. 많은 언어 모델들은 수십억 또는 수백만 기가바이트에 달하는 인터넷에서 수집한 데이터를 기반으로 학습됩니다.\n\nDBRX는 Databricks에서 개발한 오픈, 일반 목적의 언어 모델입니다. 다양한 표준 벤치마크를 통해 DBRX는 이미 알려진 오픈 언어 모델들에 대한 최신 결과를 보여주고 있습니다.\n\n<div class=\"content-ad\"></div>\n\nDBRX는 고밀도 전문가 혼합(MoE) 아키텍처 덕분에 오픈 모델 중 효율성 면에서 최첨단 기술을 선도합니다. LLaMA2-70B보다 추론이 최대 2배 빠르고, 전체 및 활성 매개변수 개수 측면에서 Grok-1의 약 40% 크기입니다.\n\nDBRX는 총 1320억 개의 매개변수 중 360억 개가 어떤 입력에 대해서도 활성인 미세 구조 MoE 아키텍처를 사용하는 대형 언어 모델(LLM)입니다. 디코더 전용이며, 12조 개의 텍스트 및 코드 데이터 토큰을 포함하는 대규모 데이터 세트에서 다음 토큰 예측을 사용하여 교육되었습니다. Mixtral 및 Grok-1과 같은 유사한 모델과 달리, DBRX는 세부 접근 방식을 채용하며, 16개 전문가를 활용하고 그 중 4개를 선택합니다. 반면 다른 모델들은 8개 전문가를 가지고 2개를 선택합니다.\n\n전통적인 언어 모델은 최근 이벤트나 트레이닝 데이터 외의 정보를 예측하는 능력에 제한이 있어서 현재 주제에 대한 쿼리에 대해 효과적이지 않을 수 있습니다. 이 제한으로 인해 언어 모델의 생성 능력을 보완하기 위해 검색 증강 생성(RAG)이 필요해졌습니다. 외부 소스를 통합함으로써 RAG는 특히 모델 훈련 데이터를 넘어서는 최근 이벤트나 주제에 대한 질의에 대한 정확도와 시기적절성을 향상시킵니다.\n\nDBRX를 선택하는 이유는 무엇일까요?\n\n<div class=\"content-ad\"></div>\n\n- 오픈 소스 모델 우위: 데이터브릭스의 DBRX 모델은 LLaMA2-70B, Mixtral 및 Grok-1과 같은 주요 오픈 소스 모델과 비교하여 우수한 성능을 보여줍니다. 이는 데이터브릭스가 언어 이해, 프로그래밍, 수학 및 논리와 같은 여러 영역에서 오픈 소스 모델 품질 향상에 기여하겠다는 의지를 나타냅니다. 이는 데이터브릭스가 지원하기를 자랑스럽게 생각하는 트렌드입니다.\n\n![이미지 1](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_1.png)\n\n- 전용 모델보다 우위: DBRX는 다양한 벤치마크에서 GPT-3.5를 능가하여, 데이터브릭스의 다양한 고객 기반 중에서 전용 모델 대신 오픈 소스 모델을 선호함에 대한 주목할만한 변화와 조화를 이룹니다. 데이터브릭스는 고객이 오픈 소스 모델을 사용자 지정하여 특정 요구 사항에 맞게 맞춤화하여 더 나은 품질과 속도를 달성할 수 있다는 능력을 강조합니다. 이는 기업과 조직에서 오픈 소스 모델의 도입을 가속화시킬 수 있는 가능성을 제시합니다.\n\n![이미지 2](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_2.png)\n\n<div class=\"content-ad\"></div>\n\n- DBRX의 MoE 아키텍처로 향상된 효율성과 확장성: MegaBlocks 연구 및 오픈소스 프로젝트에서 개발된 DBRX의 Mixture-of-Experts(MoE) 모델 아키텍처는 초당 처리된 토큰 수에 대해 높은 속도를 제공합니다. Databricks는 이 혁신이 미래 오픈소스 모델이 MoE 구조를 채택하도록 이끌어내며, 대규모 모델의 훈련을 유지하면서 빠른 처리량을 유지할 수 있도록 할 것으로 기대합니다. DBRX는 총 1320억 개의 매개변수 중 언제든지 360억 개의 매개변수만을 활용하며, 속도와 성능 사이의 균형을 제공하여 사용자에게 효율적인 솔루션을 제공합니다.\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_3.png)\n\n언어 모델을 위한 지식 창고!!!\n\nLLM을 위한 지식 창고는 외부 소스의 실시간 정보를 추가함으로써 언어 모델을 더 스마트하게 만듭니다. 이는 다양한 주제에 걸쳐보다 정확하고 의미 있는 응답을 제공하는 데 도움을 줍니다. RAG, 또는 검색 증강 생성,은 이의 주요 구성 요소로서 정확한 정보를 찾아 활용하여 응답을 개선하는 데 도움을 줍니다.\n\n<div class=\"content-ad\"></div>\n\n![그림](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_4.png)\n\nRAG(검색 보강 생성)이 무엇인가요?\n\n전통적인 언어 모델은 최근 사건이나 학습 데이터 외의 정보를 예측하는 능력이 제한되어 있어서 현재 주제에 대한 질의에 덜 효과적입니다. 이 한계로 인해 검색 보강 생성이 필요해졌습니다.\n\nRAG 또는 검색 보강 생성은 언어를 이해하고 생성하는 새로운 방법입니다. 이 방법은 두 가지 종류의 모델을 결합합니다. 먼저, 관련 정보를 검색하고, 그 정보를 기반으로 텍스트를 생성합니다. 이 두 가지를 함께 사용함으로써 RAG는 놀라운 성과를 거두었습니다. 각 모델의 강점이 서로의 약점을 보완하기 때문에, RAG는 자연어 처리의 혁신적인 방법으로 각광을 받고 있습니다.\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식으로 표를 변환한 내용입니다.\n\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_5.png)\n\n벡터 저장소(Vector Store)는 무엇인가요?\n\n벡터 저장소와 벡터 검색은 현대 정보 검색 시스템의 필수 구성 요소입니다.\n\n- **벡터 저장소(Vector Store)**: 각 정보를 벡터로 나타내어 데이터를 저장하는 데이터베이스와 같은 역할입니다. 이러한 방식을 사용하면 각 정보를 다차원 공간에서 수학적으로 표현한 벡터로 저장할 수 있습니다. 이는 기존 인덱싱 방법이 아닌 유사성 측정 기준에 따라 데이터를 효율적으로 저장하고 검색할 수 있도록 합니다.\n\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_6.png)\n\n벡터 검색: 벡터 간 유사성을 비교하여 관련 정보를 찾는 과정입니다. 키워드 매칭이나 기타 전통적인 검색 기술 대신 벡터 검색은 벡터 저장소에서 유사한 벡터를 식별하여 정확한 쿼리 용어를 포함하지 않더라도 의미론적으로 관련된 결과를 반환합니다.\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_7.png)\n\nHands-on RAG 데모:\n\n<div class=\"content-ad\"></div>\n\n제가 Databricks를 플랫폼으로 선택한 이유는 DBRX라는 기본 모델과 엔드 투 엔드 기계 학습 및 딥 러닝 워크플로에 맞춘 도구 및 라이브러리들을 제공하기 때문입니다.\n\n코드 개요:\n\n- Databricks Foundational Models에서 Chat 모델(DBRX)과 임베딩 모델을 가져오기\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_8.png)\n\n<div class=\"content-ad\"></div>\n\n2. Hugging Face에서 GPT Tokenizer를 가져오기\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_9.png)\n\n3. 챗 어시스턴트를 위한 클래스 생성: 이 클래스는 Tokenizer, Embedding, Docs 및 LLM을 입력으로 사용하여 클래스 객체를 인스턴스화합니다.\n\n- get_pdf_text(): 제공된 문서에서 텍스트를 추출합니다.\n- Chunk_return(): 텍스트를 입력으로 받아 토큰화하고 거대한 텍스트를 청크로 분할합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_10.png)\n\n- get_vector_text(): 청크를 삽입하고 FAISS 인덱스를 사용하여 내장된 콘텐츠를 색인화하여 Vector Library를 반환합니다.\n\n![image](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_11.png)\n\n- get_conve_chain(): 대화형 Q&A 체인을 가져오는 메서드이며, DBRX의 프롬프트 템플릿 및 언어 모델과 함께 반환합니다.\n- query(): 이 메서드는 LLM 체인과 Vector Library를 호출하여 언어 모델 (DBRX)에 공급하는 데 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n예시:\n\n- RAG에 대해 DBRX에 쿼리하는 방법\n\n![이미지](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_12.png)\n\n- 지식 창과 함께 DBRX에 쿼리하기\n\n<div class=\"content-ad\"></div>\n\n\n![DBRX](/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_13.png)\n\n결론:\n\nDatabricks의 최고 수준 언어 모델인 DBRX는 언어 및 코드를 이해하는 데 뛰어나지만 최근 업데이트에 대한 처리가 약간 어려울 수 있습니다. 따라서 우리는 이를 지원하기 위해 검색 증강 생성(Retrieval-Augmented Generation, RAG) 기술을 사용합니다. 이 기술은 정보를 찾아 새로운 텍스트를 생성하는 방식을 결합하여 DBRX를 더욱 똑똑하게 만듭니다. Databricks에서 구현된 RAG 및 DBRX는 머신 러닝을 보다 쉽고 효과적으로 만들어줍니다.\n\n","ogImage":{"url":"/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_0.png"},"coverImage":"/assets/img/2024-05-23-DatabricksDBRXforreal-timewithoutfine-tuning_0.png","tag":["Tech"],"readingTime":6},{"title":"넷플릭스 데이터 쇄도를 탐색하며 효과적인 데이터 관리의 필수성","description":"","date":"2024-05-23 14:00","slug":"2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement","content":"\n\nBy Vinay Kawade, Obi-Ike Nwoke, Vlad Sydorenko, Priyesh Narayanan, Shannon Heh, Shunfei Chen\n\n소개\n\n오늘날, 디지털 시대에 데이터가 전례없는 속도로 생성되고 있습니다. 예를 들어, Netflix를 살펴보겠습니다. 세계 각지의 Netflix 스튜디오에서 매년 수백 PB의 에셋이 생성됩니다. 콘텐츠는 텍스트와 이미지 시퀀스부터 소스 인코딩용 큰 IMF¹ 파일에 이르기까지 다양합니다. 때로는 생성된 프록시와 중간 파일도 있습니다. 스튜디오로부터 흘러나오는 이 방대한 데이터 양은 실질적인 통찰을 얻기 위한 효율적인 데이터 관리 전략에 대한 중대한 필요성을 강조합니다. 주목할 점은 모든 콘텐츠의 상당 부분이 미사용 상태로 남아있는 것입니다.\n\n본 기사에서, 저희 미디어 인프라스트럭처 플랫폼 팀은 생산 데이터를 효과적으로 관리하기 위한 해결책인 Garbage Collector의 개발을 개요로 설명합니다.\n\n<div class=\"content-ad\"></div>\n\nThe Magnitude of Data Generation\n\n매주 전 세계의 수백 개의 Netflix 스튜디오에서 약 2 페타바이트의 데이터가 생산됩니다. 이는 텍스트, 이미지, 이미지 시퀀스, IMF 등 다양한 소스로부터 생성된 복합 데이터입니다. 이 규모의 데이터는 엄청납니다. 이 정보를 효과적으로 총정리하고 분석하는 것이 더 어려워지고 있습니다. 또한 이로 인해 저장 비용이 크게 증가했습니다. 우리는 역대 기록적인 저장 비용 증가률인 매년 50% 증가를 보고 있습니다. 동시에 내부 연구에 따르면 적어도 데이터의 40%가 사용되지 않고 낭비되고 있다고 합니다.\n\n[이미지]\n\n효과적인 데이터 관리의 필요성\n\n<div class=\"content-ad\"></div>\n\n데이터가 계속해서 증가함에 따라, 특히 스튜디오에서 수집된 데이터는 일반적으로 업로드 - 읽기 - 재업로드 - 미해당을 따르는 수명주기를 거칩니다. 효율적으로 관리하는 작업은 점점 중요성을 갖게 됩니다. Netflix에서는 미디어 인프라 및 저장 플랫폼 팀이 사용자 조치 또는 사전 구성된 수명 주기 정책에 따라 파일 객체를 모니터링하고 정리하는 확장 가능하고 비동기적인 Garbage Collector (GC)를 사용한 데이터 수명주기 관리 솔루션을 개발했습니다. GC는 팀의 Baggins 서비스의 구성 요소로 S3 위에 미디어 특정 사용 사례에 맞춘 내부 추상화 계층입니다.\n\n아키텍처\n\n상위 수준에서 수명주기 관리자를 설계하여 안전하게 삭제하거나 더 차가운 저장소로 이동할 수 있는 데이터를 수동으로 모니터링하고 제거합니다. Garbage Collection에 대해 \"표시 및 정리\"의 관점을 취합니다.\n\n먼저, 우리는 모든 바이트를 AWS의 Simple Storage Service (S3)에 저장합니다. 그러나, S3의 각 파일에 대해 Baggins에 각 파일의 일부 메타데이터를 유지합니다. 이 메타데이터는 카산드라 데이터베이스에 저장되며 파일의 메타해시 목록 체크섬인 SHA-1, MD5 및 전체 파일에 대한 XXHash, 클라이언트 측 암호화된 객체를 위한 암호화 키와 같은 여러 필드가 포함되어 있습니다. S3에서 이러한 파일과 상호 작용하는 수백 개의 내부 넷플릭스 응용 프로그램이 있으며, 종종 여러 프록시, 파생물, 클립 등을 생성합니다. 이러한 응용 프로그램에는 프로모 미디어 생성, 마케팅 통신, 콘텐츠 인텔리전스, 자산 관리 플랫폼 등의 워크플로우가 포함됩니다. 이러한 응용 프로그램은 불필요한 파일을 필요할 때마다 삭제하거나 객체의 TTL을 사전 구성하여 파일이 자동으로 생성된 이후 일정 간격마다 삭제할 수 있습니다. 이 기간은 7일, 15일, 30일, 60일 또는 180일로 설정할 수 있습니다. 삭제 API를 통해 데이터베이스에서 해당 객체를 소프트 삭제로 표시하고 S3에서 해당 객체에 대한 액세스를 중지합니다. 이 소프트 삭제는 99% 분위수에서 삭제 API의 초당 15밀리초 미만의 초저지연을 유지합니다.\n\n<div class=\"content-ad\"></div>\n\n위에서 언급한 것처럼, 우리는 얼마 지나지 않아 '하드 삭제' 문제에 직면하게 될 것입니다. 이것은 삭제된 파일과 관련된 모든 흔적이 우리 시스템에서 제거되어야 함을 의미합니다. 이는 S3에서 바이트를 정리하고, Elastic Search에서 수행된 모든 색인을 삭제하며, 마지막으로는 Cassandra DB에서 모든 메타데이터를 삭제하는 것을 포함합니다. 우리는 다음과 같은 요구 사항을 가지고 시작했습니다.\n\n- 삭제 API(소프트 삭제)를 낮은 대기 시간으로 유지합니다.\n- 24시간 내내 수동 정리를 수행합니다.\n- 온라인 데이터베이스에 영향을 미치기 시작하면 정리를 제한합니다.\n- 정리가 급격히 증가해도 쉽게 확장할 수 있습니다.\n- 매일 정리를 편리하게 시각화합니다.\n- 아카이빙과 같은 다른 데이터 최적화 작업을 실행하기 위한 일반적인 프레임워크를 구축합니다.\n\n시스템 하의 구조\n\n다음은 시스템의 고수준 아키텍처입니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_1.png\" />\n\n클라이언트 관점에서는 ~15밀리초의 대기 시간을 소비하는 삭제 API를 간단히 호출합니다. 그러나 이후에는 이 객체 및 모든 메타데이터를 적절하게 정리하고 모든 위치에서 제대로 정리하는 일이 많이 발생합니다. 이 다이어그램은 처음에 약간 복잡해 보일 수 있지만, 화살표를 추적하면 이해가 쉬워지고 전체 그림이 명확해집니다. 위의 주기는 매일 수행되며, 수동 및 자동화된 방식으로 작동하여 그 날과 지금까지 누적된 삭제 대상 데이터와 백로그를 정리합니다.\n\n- 객체 키 가져오기: 모든 S3 객체는 Cassandra 데이터베이스에 해당 레코드가 있습니다. Cassandra 메타데이터 테이블의 항목들은 주요 참조 지점으로 기능합니다. 대응하는 행에 삭제 표시자를 추가하여 해당 행의 객체 키를 소프트 삭제하도록 표시합니다. 동일한 데이터베이스에는 버킷 수준 구성 정보를 보유하는 테이블도 있으며, 버킷 수준 사전 구성 TTL과 같은 정보를 포함합니다. 그러나 Cassandra는 파티션 키를 제공하지 않으면 TTL이 지정되거나 소프트로 삭제된 행을 필터링하는 것을 허용하지 않습니다. 또한, 메타데이터는 여러 테이블에 분산되어 있으며 정규화가 필요합니다. 여기서 Casspactor와 Apache Iceberg가 필요합니다.\n- Casspactor: 이는 Netflix의 내부 도구로, Cassandra 테이블을 Iceberg로 내보내는 데 사용됩니다. Casspactor는 Cassandra의 백업 복사본을 기반으로 작동하며 온라인 데이터베이스에 대한 지연시간을 발생시키지 않습니다. Iceberg는 Netflix에서 만들어진 오픈 소스 데이터 웨어하우스 솔루션으로, 구조화된/구조화되지 않은 데이터에 대한 SQL과 유사한 액세스를 제공합니다. TTL이 지정된/소프트 삭제된 행을 얻기 위해 Cassandra를 쿼리하는 대신 Iceberg의 복사된 행을 대상으로 쿼리할 수 있게 되었습니다.\n- 우리는 Netflix의 Workflow Orchestration 프레임워크 Maestro를 사용하여 매일 Iceberg를 호출하여 그 날 정리 작업 가능한 키를 필터링하는 일일 워크플로우를 설정합니다. 이 흐름은 버킷 수준 TTL 구성 및 객체 수준 삭제 표시자를 고려합니다.\n- Maestro 워크플로우는 다른 임시 Iceberg 테이블에 결과를 집계하여 이후 작업의 소스로 사용합니다.\n- 그 날 삭제해야 할 모든 관심 있는 행이 준비된 후, 우리는 데이터 이동 및 처리를 위한 홈그로운 솔루션을 사용하여 이를 모두 Kafka 큐로 내보냅니다. Iceberg 커넥터와 Kafka 싱크로 구성된 Data Mesh 파이프라인을 만들었습니다.\n- 우리는 이 Kafka 주제를 청취하는 몇 개의 Garbage Collector (GC) Worker를 실행하고 각 행에 대해 삭제 작업을 수행합니다. 이러한 작업은 수평 확장 가능하며 Kafka 파이프라인에서 발생하는 스파이크나 백로그에 기반해 자동으로 확장됩니다.\n- GC 워커는 객체의 완전한 정리를 처리합니다. 먼저 S3에서 모든 바이트를 삭제합니다. 이는 AWS S3에서 부과하는 5TB 최대 파일 크기 제한을 초과하는 경우 단일 파일 또는 다중 파일이 될 수 있습니다.\n- 그런 다음 우리는 로컬 Elastic Search에서 이 소프트 삭제된 파일에 대한 모든 참조를 정리합니다.\n- 마지막으로, 근무자들은 온라인 Cassandra 데이터베이스에서 이 항목을 제거하여 루프를 완료합니다. GC 워커의 자동 스케일링도 현재 부하 및 지연 시간에 대한 Cassandra 데이터베이스의 입력을 받습니다. 이러한 매개 변수들은 데이터 삭제 속도에 대한 제어된 속도 조절에 반영됩니다.\n- 마지막 단계는 매일 삭제하는 파일 수와 크기에 대한 세부 내용을 제공하는 대시보드를 강화합니다. 이를 위해 Apache Superset을 사용합니다. 이를 통해 AWS 송장과 미래 비용을 예측하는 데 사용할 수 있는 상당한 데이터를 확보할 수 있습니다.\n\n저장 통계\n\n<div class=\"content-ad\"></div>\n\n이렇게 우리 대시보드가 어떻게 보이는지 알려드릴게요,\n\n![대시보드 스냅샷](/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_2.png)\n\n![대시보드 스냅샷](/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_3.png)\n\n필수 통찰력\n\n<div class=\"content-ad\"></div>\n\n- 데이터 정리 전략 수립을 우선시하세요. 데이터 정리 프로세스는 초기 설계의 중요한 부분이어야 합니다. 후속 고려 사항이 아닙니다. 시간이 흐를수록 데이터 정리는 적절히 관리되지 않으면 압도적인 작업으로 번질 수 있습니다.\n- 지출을 지속적으로 모니터링하고 기록하세요. 비용 없는 부분은 없습니다. 데이터의 각 바이트는 금전적 영향을 가지고 있습니다. 따라서 저장된 모든 데이터에 대한 포괄적인 계획이 필수적입니다. 이는 특정 기간 이후 데이터를 삭제하거나 사용되지 않을 때 더 비용 효율적인 저장 계층으로 이전하거나, 적어도 미래 참조 및 의사 결정을 위해 무기한 보유를 위한 사유를 유지하는 것을 포함할 수 있습니다.\n- 디자인은 다양한 작업과 환경에서 사용할 수 있는 유연성을 가져야 합니다. 활성 데이터부터 비활성 데이터 보관, 그리고 중간에 있는 모든 것까지 관리할 수 있어야 합니다.\n\n**결론**\n\n지수적인 데이터 증가 시대를 계속해서 탐색함에 따라 효과적인 데이터 관리의 필요성은 지나치게 강조될 수 없습니다. 데이터의 양을 다루는 것뿐만 아니라 품질과 관련성을 이해하는 것입니다. 포괄적인 데이터 관리 전략에 투자하는 기ꢣ치는 새로운 데이터 중심 환경에서 선도할 기업들이 될 것입니다.\n\n용어\n\n<div class=\"content-ad\"></div>\n\nIMF (Interoperable Master Format): 이것은 오디오와 비디오 마스터 파일의 디지털 전송과 저장에 사용되는 표준화된 형식입니다. 더 많은 정보를 원하시면 이 개요를 방문해보세요.\n\nMHL (미디어 해시 목록): 이것은 미디어 파일에서 체크섬을 보존하기 위해 전송 중에 그리고 정적 상태에서 저장하는 데 사용되는 표준입니다. 더 많은 정보를 원하시면 https://mediahashlist.org 를 방문해보세요.\n\nPB (페타바이트): 디지털 정보 저장의 단위로, 천 테라바이트 또는 백만 기가바이트에 해당합니다.\n\nTTL (Time-to-Live): 이 용어는 데이터가 폐기되기 전에 저장되는 기간을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n감사의 말씀\n\n마에스트로 팀, 캐스팩터 팀, 아이스버그 팀의 업무에 기여한 동료인 엠리 쇼, 앙쿠르 케트라팔, 에샤 팔타, 빅터 예레비치, 미나크시 진달, 페이지 후, 동동 우, 챈텔 양, 그레고리 알몬드, 아비 칸다사미, 그 외 훌륭한 동료들께 특별한 감사를 전합니다.","ogImage":{"url":"/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_0.png"},"coverImage":"/assets/img/2024-05-23-NavigatingtheNetflixDataDelugeTheImperativeofEffectiveDataManagement_0.png","tag":["Tech"],"readingTime":7},{"title":"데이터 안전 보장 암호화되지 않은 RDS 데이터베이스를 암호화된 데이터베이스로 이전하기","description":"","date":"2024-05-23 13:59","slug":"2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne","content":"\n![태그](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_0.png)\n\nAmazon RDS DB 인스턴스에 대한 암호화는 생성 시에만 활성화할 수 있습니다. 기존 인스턴스를 암호화하려면 스냅샷을 생성한 후 해당 스냅샷의 암호화된 복사본을 만들어 새로운 암호화된 인스턴스로 복원합니다. 다운타임이 허용되는 경우에는 새 인스턴스로 애플리케이션을 전환하십시오. 최소한의 다운타임을 위해 AWS 데이터베이스 마이그레이션 서비스(AWS DMS)를 사용하여 데이터를 지속적으로 마이그레이션하고 복제함으로써 새로운 암호화된 데이터베이스로의 원활한 전환을 허용할 수 있습니다.\n\n## 1. 암호화 상태 확인\n\n첫 번째 단계는 현재 사용 중인 RDS 데이터베이스가 암호화되었는지 확인하는 것입니다. AWS 관리 콘솔에 로그인하고 RDS 서비스로 이동합니다. 대상 데이터베이스를 찾고 \"구성\" 탭을 클릭합니다. \"암호화\" 섹션을 찾아보십시오. 만약 \"활성화되지 않음\"으로 표시된다면 데이터베이스가 암호화되지 않았음을 의미합니다.\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_1.png)\n\n## 2. 스냅샷 생성\n\n암호화하려는 인스턴스의 DB 스냅샷을 만듭니다. 스냅샷을 만드는 데 걸리는 시간은 데이터베이스의 크기에 따라 다릅니다. 이제 왼쪽 메뉴에서 \"스냅샷\" 옵션으로 이동하고 \"스냅샷 촬영\"을 클릭합니다. 쉽죠?\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_2.png)\n\n<div class=\"content-ad\"></div>\n\n이제 스냅샷을 만들 데이터베이스를 선택해야 해요:\n\n![image](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_3.png)\n\n이를 \"UnencryptedSnapshot\"이라고 이름 짓을 수 있어요. 실제 데이터베이스의 사본이 될 거에요. 스냅샷이 생성되기를 기다려 주세요. 제 경우에는 약 2분이 걸렸어요.\n\n## 3. 사본 암호화\n\n<div class=\"content-ad\"></div>\n\n이제 생성한 DB 스냅샷을 선택해 주세요. 작업에서 '스냅샷 복사'를 선택하세요. 대상 AWS 지역 및 DB 스냗샷 사본의 이름을 해당 필드에 제공해 주세요. '암호화 사용' 확인란을 선택하세요. 마스터 키로는 DB 스냅샷 사본을 암호화하는 데 사용할 KMS 키 식별자를 지정하세요. '스냅샷 복사'를 선택하세요.\n\n그리고 보시다시피 제가 암호화를 활성화했으며 (기본) aws/rds를 AWS KMS 키로 선택했습니다.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_4.png)\n\n두 번째 스냅샷이 완료될 때까지 다시 기다릴 것입니다. (참고: 5분이 걸렸습니다)\n\n<div class=\"content-ad\"></div>\n\n## 4. 암호화된 데이터베이스 복원\n\n스냅샷을 사용할 수 있는 상태가 되면, 데이터베이스를 복원해야 합니다. \"암호화된 스냅샷\"을 선택한 후 \"작업\"을 클릭한 다음 \"스냅샷 복원\"을 선택하세요. DB 인스턴스 식별자에는 새로운 DB 인스턴스를 위한 고유한 이름을 제공하세요. 동일한 구성을 유지하겠으며 필요에 따라 편집할 수도 있습니다. 모든 옵션을 확인한 후 복원을 클릭하세요.\n\n지금은 DB가 생성 중인 상태입니다:\n\n![image](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_5.png)\n\n<div class=\"content-ad\"></div>\n\n## 5. DMS를 사용한 데이터 마이그레이션\n\n마지막 단계는 DMS로 이동하여 작업을 만들어야 합니다. 그러기 전에 소스 엔드포인트, 대상 엔드포인트 및 복제 인스턴스를 만들어야 합니다.\n\n먼저 복제 인스턴스에서 시작해야 합니다. 왼쪽 메뉴 탭에서 '복제 인스턴스'를 선택하고 \"복제 인스턴스 생성\"을 선택하세요.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_6.png)\n\n<div class=\"content-ad\"></div>\n\n다른 구성에 대한 선호도를 포함할 수 있습니다. 인스턴스가 생성되었으므로 엔드포인트로 진행할 수 있습니다.\n\n소스 엔드포인트의 경우 DMS 콘솔에서 \"엔드포인트\"를 선택하고 \"엔드포인트 생성\"을 해야 합니다.\n\n아래 이미지에서 보이는 대로 소스 엔드포인트로 비암호화된 RDS를 선택해야 합니다:\n\n![image](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_7.png)\n\n<div class=\"content-ad\"></div>\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_8.png)\n\n수동으로 자격 증명을 추가하는 경우, 비밀번호를 추가해야할 것입니다. 비밀번호를 추가하려면 \"검색\"을 클릭하면 모든 데이터베이스 정보를 찾을 수 있는 시크릿 매니저를 확인해야 합니다.\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_9.png)\n\n이번에도 대상 엔드포인트에 대해 동일한 단계를 수행할 것입니다. 새 데이터베이스를 선택하기만 하면 됩니다. 이제 두 개의 엔드포인트가 준비되었으므로 작업을 생성할 수 있습니다:\n\n\n<div class=\"content-ad\"></div>\n\n아래는 Markdown 형식의 텍스트입니다.\n\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_10.png)\n\n작업을 만들려면 왼쪽 메뉴에서 \"데이터 마이그레이션 작업\"으로 이동하여 작업을 만들어야 합니다:\n\n![이미지](/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_11.png)\n\n소스, 대상, 복제 인스턴스 및 마이그레이션 유형을 포함한 설정을 올바르게 구성하고, \"기존 데이터 마이그레이션 및 지속적인 변경 복제\"를 선택해야 합니다:\n\n\n<div class=\"content-ad\"></div>\n\n아래의 표태그를 Markdown 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\n","ogImage":{"url":"/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_0.png"},"coverImage":"/assets/img/2024-05-23-SecuringYourDataMigratinganUnencryptedRDSDatabasetoanEncryptedOne_0.png","tag":["Tech"],"readingTime":4},{"title":"REST API를 API Gateway, Lambda, DynamoDB, Cognito를 사용하여 배포하는 단계별 가이드  Terraform","description":"","date":"2024-05-23 13:55","slug":"2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform","content":"\n\n![image](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_0.png)\n\n# 소개\n\n우리는 다양한 상황에서 사용할 수 있는 실전 프로젝트를 만들고 싶습니다. 실제 세계에서 매우 일반적인 것으로, 거의 모든 애플리케이션이 모듈식 레고 블록으로 구성된 마이크로서비스에 기반을 두고 있습니다.\n\n특히, 목표는 API 게이트웨이에 호스팅된 API를 만들고, 백엔드는 람다에, 데이터베이스는 DynamoDB에 있는 것입니다. 람다 함수에는 DynamoDB 테이블에서 CRUD 작업 (CREATE, READ, UPDATE, DELETE)을 수행하는 로직이 포함될 것입니다. 그리고 추가로 몇 가지 경로에 대한 공개 액세스를 제한하기 위해 Amazon Cognito를 사용한 인증을 추가할 것입니다. 왜냐하면 데이터베이스에 대한 쓰기 작업은 위험하기 때문입니다.\n\n\n<div class=\"content-ad\"></div>\n\n시칠리아 섬에서 태어났기 때문에 신기한 섬리아 요리 목록을 관리할 수 있는 간단한 API를 생성할 것입니다.\n\n모든 소스 코드는 여기에서 확인하실 수 있습니다:\n\n# 단계 1: 공급자, AWS 지역, S3 백엔드 설정\n\n첫 번째 단계는 AWS를 제공자로 사용하도록 지정하는 것입니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# provider.tf\n\nprovider \"aws\" {\n  region = var.aws_region\n}\n```\n\n우리는 하나의 변수만 정의하고 있어요.\n\n```js\n# variables.tf\n\nvariable \"aws_region\" {\n  default   = \"eu-west-3\"\n  type      = string\n}\n```\n\n그런 다음 AWS 관리 콘솔로 이동해서 “S3” AWS 서비스로 이동하여 나중에 사용할 Terraform 상태 파일을 저장할 S3 버킷을 생성하세요. \"key\" 속성으로 지정된 경로에 생성할 거에요. 저는 \"my-api-gateway-lambda-terraform-state\"라고 이름지었어요. 모든 옵션을 기본값으로 남겨두세요.```\n\n<div class=\"content-ad\"></div>\n\n만들었다면, Terraform 구성에서 명시할 것입니다:\n\n```js\n# backend.tf\n\nterraform {\n  backend \"s3\" {\n    bucket = \"my-api-gateway-lambda-terraform-state\"\n    region = var.aws_region\n    key    = \"API-Gateway/terraform.tfstate\"\n  }\n  required_version = \">= 0.13.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 2.7.0\"\n    }\n  }\n}\n```\n\n## 단계 2: Lambda IAM 역할 생성\n\nLambda 함수가 DynamoDB 테이블에서 작업을 수행하려면 해당 권한이 있어야 합니다. 따라서 IAM 역할을 생성해야 합니다.\n\n<div class=\"content-ad\"></div>\n\n먼저 IAM 역할의 Assume Role 정책(Trust Relationship이라고도 함)을 명시적으로 지정해야 합니다. 이는 어떤 리소스 또는 서비스가 원하는 역할을 가져갈 수 있는 지를 나타내는데, 이 경우에는 Lambda 함수입니다. AWS 서비스에 권한을 제공할 때 이 단계는 항상 필수적입니다. 이 정책의 형식은 다음과 같습니다:\n\n```js\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"Service\": \"lambda.amazonaws.com\" # AWS 서비스의 이름\n        },\n        \"Action\": \"sts:AssumeRole\"\n    }]\n}\n```\n\n기본적으로 \"내 계정의 모든 Lambda 함수가 이 역할을 가져올 수 있습니다\"라고 말하고 있습니다.\n\n또한 Lambda에게 함수 실행과 관련된 로그 작성을 위해 필요한 최소한의 권한을 부여해야 합니다. 이 권한들은 이미 AWS에서 제공하는 AWSLambdaBasicExecutionRole이라는 IAM 서비스 역할에 정의되어 있습니다. 따라서 이 서비스 역할을 새 IAM 역할에 연결하여 Lambda에 할당할 것입니다.\n\n<div class=\"content-ad\"></div>\n\n파일 \"iam.tf\"를 생성하고 다음 코드를 붝어 주세요:\n\n```js\n# iam.tf\n\n# 역할 가정 정책\ndata \"aws_iam_policy_document\" \"AWSLambdaTrustPolicy\" {\n  statement {\n    actions    = [\"sts:AssumeRole\"]\n    effect     = \"Allow\"\n    principals {\n      type        = \"Service\"\n      identifiers = [\"lambda.amazonaws.com\"]\n    }\n  }\n}\n\n# IAM 역할 정의 및 가정 역할 정책 첨부\nresource \"aws_iam_role\" \"terraform_function_role\" {\n  name               = \"terraform_function_role\"\n  assume_role_policy = data.aws_iam_policy_document.AWSLambdaTrustPolicy.json\n}\n\n# 방금 정의한 IAM 역할에 IAM 서비스 역할 첨부\n# AWSLambdaBasicExecutionRole은 람다 함수에 최소한의 권한을 부여합니다\n# (실행에 대한 로그 작성, 오류, 디버깅 등)\nresource \"aws_iam_role_policy_attachment\" \"terraform_lambda_policy\" {\n  role       = aws_iam_role.terraform_function_role.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n}\n```\n\n또한, 이외에도 말씀드린대로 Lambda 함수가 생성할 DynamoDB 테이블에 액세스해야 합니다. 이를 위해 DynamoDB 테이블에서 수행할 모든 작업이 명시적으로 허용된 JSON 정책 문서를 작성합니다:\n\n```js\n// lambda_dynamodb_policy.json\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"ListAndDescribe\",\n          \"Effect\": \"Allow\",\n          \"Action\": [\n              \"dynamodb:List*\",\n              \"dynamodb:DescribeReservedCapacity*\",\n              \"dynamodb:DescribeLimits\",\n              \"dynamodb:DescribeTimeToLive\"\n          ],\n          \"Resource\": \"*\"\n      },\n      {\n          \"Sid\": \"SpecificTable\",\n          \"Effect\": \"Allow\",\n          \"Action\": [\n              \"dynamodb:BatchGet*\",\n              \"dynamodb:DescribeStream\",\n              \"dynamodb:DescribeTable\",\n              \"dynamodb:Get*\",\n              \"dynamodb:Query\",\n              \"dynamodb:Scan\",\n              \"dynamodb:BatchWrite*\",\n              \"dynamodb:CreateTable\",\n              \"dynamodb:Delete*\",\n              \"dynamodb:Update*\",\n              \"dynamodb:PutItem\"\n          ],\n          \"Resource\": \"arn:aws:dynamodb:*:*:table/dishes\" // DynamoDB 테이블\n      }\n  ]\n}\n```\n\n<div class=\"content-ad\"></div>\n\nLambda 함수가 DynamoDB 테이블에서 CRUD 작업(Get, Put, Update, Delete, Scan, Query 및 기타 작업)을 수행할 수 있는 정책을 정의하고, \"dishes\"라는 이름의 테이블을 사용할 것입니다.\n\n\"iam.tf\" 파일에서는 JSON 문서에 정의된 정책을 IAM 역할에 부착하도록 다음과 같이 작성합니다:\n\n```js\n# iam.tf\n\n# DynamoDB에 액세스하기 위한 IAM 역할에 사용자 지정 정책 부착\nresource \"aws_iam_role_policy\" \"lambda_dynamodb_policy\" {\n  name   = \"lambda_dynamodb_policy\"\n  role   = aws_iam_role.lambda-iam-role.name\n  policy = file(\"${path.module}/lambda_dynamodb_policy.json\")\n}\n```\n\n# 단계 3: Lambda 코드 설정하기\n\n<div class=\"content-ad\"></div>\n\nLambda 함수를 설정하는 것이 다음 단계입니다. 이 함수는 DynamoDB 테이블과 상호 작용할 것입니다. 이 함수는 Python으로 작성될 것입니다.\n\nLambda 함수 안에서는 각각의 작업에 대한 메서드를 정의하고 API Gateway로 응답을 반환할 수 있습니다. 우리는 함수의 코드를 Python으로 작성할 것입니다.\n\n우선적으로 Lambda를 테스트할 때 무슨 일이 벌어지는지 볼 수 있도록 로거를 설정합니다. 그런 다음 lambda_handler() 안에서는 AWS 서비스와 상호 작용하기 위해 사용되는 boto3 라이브러리를 활용하여 DynamoDB 클라이언트를 선언합니다. Lambda를 트리거하는 이벤트는 API Gateway에서 오는 HTTP 요청입니다. 우리는 이를 통해 HTTP 메소드를 읽고 Lambda가 DynamoDB 테이블에서 수행해야 하는 작업을 구별할 수 있습니다.\n\nREST API는 트리 구조로 구성되어 있으며, 우리는 이를 다음과 같이 구조화하고 싶습니다:\n\n<div class=\"content-ad\"></div>\n\n\n![Image](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_1.png)\n\n우리는 다음을 할 수 있기를 원합니다:\n\n- GET dishes/ : 모든 요리 항목 검색\n- GET dish/'dishId' : ID로 특정 요리 항목 검색\n- POST dish/ : 새로운 요리 항목 저장\n- PATCH dish/ : 특정 요리 항목의 속성 업데이트\n- DELETE dish/ : 테이블에서 요리 항목 삭제\n\n테이블의 모든 항목을 가져 오기 위해 재귀 함수 recursive_scan을 활용하며, 이 함수는 DynamoDB 테이블에서 레코드를 효율적으로 스캔하는 데 사용됩니다.\n\n\n<div class=\"content-ad\"></div>\n\n표준 DynamoDB의 scan 작업은 테이블의 모든 항목을 읽는 표준 접근 방식입니다. 그러나 DynamoDB의 분산 특성과 확장성 때문에, 단일 scan 작업으로는 특히 테이블이 큰 경우 모든 항목을 한 번에 검색하지 못할 수 있습니다. DynamoDB는 결과를 페이지별로 반환하며, 다음 결과 페이지가 시작되는 위치를 나타내는 토큰(LastEvaluatedKey)과 함께 항목의 하위 집합을 반환합니다.\n\nrecursive_scan 메서드는 모든 페이지의 결과를 재귀적으로 가져와서 더 이상 페이지가 남아있지 않을 때까지(응답에 LastEvaluatedKey가 없을 때) 검색 프로세스를 최적화합니다. 이를 통해 페이지 수에 관계없이 DynamoDB 테이블의 모든 항목을 효율적으로 검색할 수 있습니다.\n\n```js\ndef recursive_scan(scan_params, items):\n    response = table.scan(**scan_params)\n    items += response['Items']\n    if 'LastEvaluatedKey' in response:\n        scan_params['ExclusiveStartKey'] = response['LastEvaluatedKey']\n        recursive_scan(scan_params, items)\n    return items\n```\n\n여기에 Lambda 함수의 전체 코드가 있습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# lambda_code.py\n\nimport json\nimport logging\nimport boto3\nfrom decimal import Decimal\nfrom botocore.exceptions import ClientError\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('dishes')\n\ndish_path = '/dish'\ndishes_path = '/dishes'\n\ndef lambda_handler(event, context):\n\n    logger.info('API event: {}'.format(event))\n\n    response = None\n\n    try:\n        http_method = event.get('httpMethod')\n        path = event.get('path')\n\n        if http_method == 'GET' and path == dishes_path:\n            response = get_all_dishes()\n\n        elif http_method == 'GET' and path == dish_path:\n            dish_id = event['queryStringParameters']['dish_id']\n            response = get_dish(dish_id)\n\n        elif http_method == 'POST' and path == dish_path:\n            body = json.loads(event['body'])\n            response = save_dish(body)\n\n        elif http_method == 'PATCH' and path == dish_path:\n            body = json.loads(event['body'])\n            response = update_dish(body['dish_id'], body['update_key'], body['update_value'])\n\n        elif http_method == 'DELETE':\n            body = json.loads(event['body'])\n            response = delete_dish(body['dish_id'])\n\n        else:\n            response = generate_response(404, '리소스를 찾을 수 없습니다.')\n\n    except ClientError as e:\n        logger.error('오류: {}'.format(e))\n        response = generate_response(404, e.response['Error']['Message'])\n\n    return response\n\ndef get_dish(dish_id):\n    try:\n        response = table.get_item(Key={'dish_id': dish_id})\n        item = response['Item']\n        logger.info('항목 조회: {}'.format(item))\n        return generate_response(200, item)\n    except ClientError as e:\n        logger.error('오류: {}'.format(e))\n        return generate_response(404, e.response['Error']['Message'])\n\ndef get_all_dishes():\n    try:\n        scan_params = {\n            'TableName': table.name\n        }\n        items = recursive_scan(scan_params, [])\n        logger.info('모든 항목 조회: {}'.format(items))\n        return generate_response(200, items)\n    except ClientError as e:\n        logger.error('오류: {}'.format(e))\n        return generate_response(404, e.response['Error']['Message'])\n\ndef recursive_scan(scan_params, items):\n    response = table.scan(**scan_params)\n    items += response['Items']\n    if 'LastEvaluatedKey' in response:\n        scan_params['ExclusiveStartKey'] = response['LastEvaluatedKey']\n        recursive_scan(scan_params, items)\n    return items\n\ndef save_dish(item):\n    try:\n        table.put_item(Item=item)\n        logger.info('항목 저장: {}'.format(item))\n        body = {\n            '작업': '저장',\n            '메시지': '성공',\n            '항목': item\n        }\n        return generate_response(200, body)\n    except ClientError as e:\n        logger.error('오류: {}'.format(e))\n        return generate_response(404, e.response['Error']['Message'])\n\ndef update_dish(dish_id, update_key, update_value):\n    try:\n        response = table.update_item(\n            Key={'dish_id': dish_id},\n            UpdateExpression=f'SET {update_key} = :value',\n            ExpressionAttributeValues={':value': update_value},\n            ReturnValues='UPDATED_NEW'\n        )\n        logger.info('항목 업데이트: {}'.format(response))\n        body = {\n            '작업': '업데이트',\n            '메시지': '성공',\n            '항목': response\n        }\n        return generate_response(200, response)\n    except ClientError as e:\n        logger.error('오류: {}'.format(e))\n        return generate_response(404, e.response['Error']['Message'])\n\ndef delete_dish(dish_id):\n    try:\n        response = table.delete_item(\n            Key={'dish_id': dish_id},\n            ReturnValues='ALL_OLD'\n        )\n        logger.info('항목 삭제: {}'.format(response))\n        body = {\n            '작업': '삭제',\n            '메시지': '성공',\n            '항목': response\n        }\n        return generate_response(200, body)\n    except ClientError as e:\n        logger.error('오류: {}'.format(e))\n        return generate_response(404, e.response['Error']['Message'])\n\nclass DecimalEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Decimal):\n            # 정수 또는 소수인지 확인합니다\n            if obj % 1 == 0:\n                return int(obj)\n            else:\n                return float(obj)\n        # 기본 클래스의 default 메서드가 TypeError를 발생시키도록 합니다\n        return super(DecimalEncoder, self).default(obj)\n\ndef generate_response(status_code, body):\n    return {\n        'statusCode': status_code,\n        'headers': {\n            'Content-Type': 'application/json',\n        },\n        'body': json.dumps(body, cls=DecimalEncoder)\n    }\n```\n\n그런 다음, “lambda.tf” Terraform 파일에는 Lambda 코드를 압축하는 데이터 블록을 정의하고 해당 Lambda 자체에 대한 리소스 블록이 있습니다:\n\n```js\ndata \"archive_file\" \"lambda_code\" {\n  type        = \"zip\"\n  source_file = \"${path.module}/lambda_code.py\"\n  output_path = \"${path.module}/lambda_code.zip\"\n}\n\nresource \"aws_lambda_function\" \"my-lambda-function\" {\n  filename      = \"${path.module}/lambda_code.zip\"\n  function_name = \"api-gateway-lambda\"\n  role          = aws_iam_role.lambda-iam-role.arn\n  handler       = \"lambda_code.lambda_handler\"\n  runtime       = \"python3.12\"\n\n  source_code_hash = data.archive_file.lambda_code.output_base64sha256\n}\n```\n\n# 단계 3: DynamoDB 설정\n\n\n<div class=\"content-ad\"></div>\n\n시칠리아 요리 테이블을 만드는 시간이 왔습니다!\n\n![Sicilian Dishes](https://miro.medium.com/v2/resize:fit:480/1*WMK7Qze__kL4gO7lXQ5LKg.gif)\n\n`database.tf` 파일을 생성하고 다음 코드를 붙여넣으세요:\n\n```js\n# 다이나모DB 테이블 정의\nresource \"aws_dynamodb_table\" \"my_dynamodb_table\" {\n  name         = \"dishes\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"dish_id\"\n\n  attribute {\n    name = \"dish_id\"\n    type = \"S\"\n  }\n\n  tags = {\n    Name = \"dishes-table\"\n  }\n}\n\nlocals {\n  json_data = file(\"${path.module}/dishes.json\")\n  dishes    = jsondecode(local.json_data)\n}\n\n# 각 요리별로 다이나모DB 테이블에 새 항목 생성\nresource \"aws_dynamodb_table_item\" \"dishes\" {\n  for_each   = local.dishes\n  table_name = aws_dynamodb_table.my_dynamodb_table.name\n  hash_key   = aws_dynamodb_table.my_dynamodb_table.hash_key\n  item       = jsonencode(each.value)\n}\n```\n\n<div class=\"content-ad\"></div>\n\nJSON 문서에서 데이터를 가져 오므로 \"dishes.json\" 파일을 만드십시오. 각 속성에 대해 유형을 지정합니다 (S = 문자열, N = 숫자, L = 목록).\n\n```js\n// dishes.json\n\n{\n    \"Item1\": {\n        \"dish_id\": {\n            \"S\": \"1\"\n        },\n        \"name\": {\n            \"S\": \"아란치니\"\n        },\n        \"description\": {\n            \"S\": \"치즈, 완두 및 고기가 들어있는 튀긴 쌀 공예볼\"\n        },\n        \"price\": {\n            \"N\": \"8.99\"\n        },\n        \"ingredients\": {\n            \"L\": [\n                {\"S\": \"쌀\"},\n                {\"S\": \"치즈\"},\n                {\"S\": \"완두\"},\n                {\"S\": \"고기\"},\n                {\"S\": \"빵 가루\"}\n            ]\n        }\n    },\n    \"Item2\": {\n        \"dish_id\": {\n            \"S\": \"2\"\n        },\n        \"name\": {\n            \"S\": \"카놀리\"\n        },\n        \"description\": {\n            \"S\": \"튜브 모양의 튀겨진 페이스트리 도우로 구운 쉘에 달콤하고 부드러운 필링을 채운 시칠리아 디저트\"\n        },\n        \"price\": {\n            \"N\": \"5.99\"\n        },\n        \"ingredients\": {\n            \"L\": [\n                {\"S\": \"밀가루\"},\n                {\"S\": \"리코타 치즈\"},\n                {\"S\": \"설탕\"},\n                {\"S\": \"초콜릿 칩\"}\n            ]\n        }\n    },\n    \"Item3\": {\n        \"dish_id\": {\n            \"S\": \"3\"\n        },\n        \"name\": {\n            \"S\": \"파스타 알라 노르마\"\n        },\n        \"description\": {\n            \"S\": \"토마토 소스, 튀긴 가지, 갈은 리코타 샐라타 치즈 및 바질이 들어간 파스타\"\n        },\n        \"price\": {\n            \"N\": \"12.99\"\n        },\n        \"ingredients\": {\n            \"L\": [\n                {\"S\": \"파스타\"},\n                {\"S\": \"토마토 소스\"},\n                {\"S\": \"가지\"},\n                {\"S\": \"리코타 치즈\"},\n                {\"S\": \"바질\"}\n            ]\n        }\n    },\n    \"Item4\": {\n        \"dish_id\": {\n            \"S\": \"4\"\n        },\n        \"name\": {\n            \"S\": \"카사타\"\n        },\n        \"description\": {\n            \"S\": \"과일 주스 또는 리큐르로 적시한 둥근 스펀지 케이크로 리코타 치즈, 설탕이 묻혔고 카놀리 크림과 유사한 초콜릿 또는 바닐라 필링이 층층이 쌓인 시칠리아 케이크\"\n        },\n        \"price\": {\n            \"N\": \"15.99\"\n        },\n        \"ingredients\": {\n            \"L\": [\n                {\"S\": \"스펀지 케이크\"},\n                {\"S\": \"과일 주스\"},\n                {\"S\": \"리큐르\"},\n                {\"S\": \"리코타 치즈\"},\n                {\"S\": \"설탕\"},\n                {\"S\": \"초콜릿\"},\n                {\"S\": \"바닐라\"}\n            ]\n        }\n    }\n}\n```\n\n# 단계 4: API Gateway 설정\n\n이제 API 게이트웨이를 설정 할 시간입니다. API 게이트웨이는 프록시 역할을합니다. 클라이언트에서 Lambda 함수로 오는 HTTP 요청을 전달하며이 \"트릭\"을 사용하여 원래의 HTTP 요청이 전송됩니다 (GET, POST 등)\n\n<div class=\"content-ad\"></div>\n\n먼저 API 게이트웨이 REST API를 설정하고 두 가지 API 리소스를 만듭니다. 각각의 경로(/dishes 및 /dish)를 위한 한 가지씩:\n\n```js\n# api_gateway.tf\n\n# API 게이트웨이\nresource \"aws_api_gateway_rest_api\" \"API-gw\" {\n  name        = \"lambda_rest_api\"\n  description = \"시칠리아 요리를 위한 REST API입니다.\"\n  endpoint_configuration {\n    types = [\"REGIONAL\"]\n  }\n}\n\n# \"/dishes\" 경로를 위한 API 리소스\nresource \"aws_api_gateway_resource\" \"API-resource-dishes\" {\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n  parent_id   = aws_api_gateway_rest_api.API-gw.root_resource_id\n  path_part   = \"dishes\"\n}\n\n# \"/dish\" 경로를 위한 API 리소스\nresource \"aws_api_gateway_resource\" \"API-resource-dish\" {\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n  parent_id   = aws_api_gateway_rest_api.API-gw.root_resource_id\n  path_part   = \"dishes\"\n}\n```\n\n우리가 원하는 API 엔드포인트는 다음과 같습니다:\n\n- GET /dishes: 모든 시칠리아 요리의 목록을 가져옵니다.\n- GET /dishes/'dishId': ID에 따라 특정 요리의 세부 정보를 가져옵니다.\n- POST /dishes: 새로운 시칠리아 요리를 데이터베이스에 추가합니다.\n- PATCH /dishes/'dishId': 특정 요리의 세부 정보를 업데이트합니다.\n- DELETE /dishes/'dishId': 데이터베이스에서 시칠리아 요리를 삭제합니다.\n\n<div class=\"content-ad\"></div>\n\n각 HTTP 메서드에 대해 아래와 같이 몇 가지 블록을 정의합니다:\n\n- Method (HTTP 메서드 지정)\n- Integration (Lambda와 통합)\n- Method response\n- Integration response\n\n![이미지](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_2.png)\n\n“REST API”는 만들 API Gateway 객체 모두를 담고 있는 컨테이너입니다.\n\n<div class=\"content-ad\"></div>\n\nAPI Gateway에 들어오는 모든 요청은 다음과 일치해야 합니다:\n\n- 구성된 리소스 (특정한 /dish 또는 다른 /dishes)\n- HTTP 메서드\n\nAPI 게이트웨이 리소스의 각 메서드는 Lambda 함수로 들어오는 요청이 보내지는 통합을 가지고 있습니다. \"AWS_PROXY\" 통합 유형은 API 게이트웨이가 AWS Lambda API를 호출하여 Lambda 함수의 \"invocation\"을 생성하도록합니다. 그런 다음 우리는 메서드 응답(관련된 상태 코드로) 및 통합 응답을 구성합니다.\n\n```js\n# . . .\n\n# Lambda 함수를 트리거하는 API 게이트웨이 정의\nresource \"aws_api_gateway_rest_api\" \"API-gw\" {\n  name        = \"lambda_rest_api\"\n  description = \"이것은 시칠리아 요리를 위한 REST API입니다.\"\n  endpoint_configuration {\n    types = [\"REGIONAL\"]\n  }\n}\n\nresource \"aws_api_gateway_resource\" \"API-resource-dish\" {\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n  parent_id   = aws_api_gateway_rest_api.API-gw.root_resource_id\n  path_part   = \"dish\"\n}\n\nresource \"aws_api_gateway_resource\" \"API-resource-dishes\" {\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n  parent_id   = aws_api_gateway_rest_api.API-gw.root_resource_id\n  path_part   = \"dishes\"\n}\n\n#####################################################################################################\n########################### GET ALL /dishes #########################################################\n#####################################################################################################\n\nresource \"aws_api_gateway_method\" \"GET_all_method\" {\n  rest_api_id   = aws_api_gateway_rest_api.API-gw.id\n  resource_id   = aws_api_gateway_resource.API-resource-dishes.id\n  http_method   = \"GET\"\n  authorization = \"NONE\"\n}\n\n. . .\n```\n\n<div class=\"content-ad\"></div>\n\n다음으로 API 게이트웨이가 람다 함수를 호출할 수 있도록 설정해야 합니다:\n\n```js\n# . . .\n\n# API 게이트웨이가 람다에 접근할 수 있도록 허용\nresource \"aws_lambda_permission\" \"apigw\" {\n  statement_id  = \"AllowAPIGatewayInvoke\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.my-lambda-function.function_name\n  principal     = \"apigateway.amazonaws.com\"\n\n  # 여기서 /*/* 부분은 \"REST API\" 내에서 어떤 리소스의 어떤 메서드에서도 접근할 수 있게 합니다.\n  source_arn = \"${aws_api_gateway_rest_api.API-gw.execution_arn}/*/*\"\n}\n```\n\n그런 다음 API 배포를 구성하고 \"prod\"라는 스테이지를 생성하여, API 게이트웨이 URL이 \"/prod/dishes\"와 같은 형태가 됩니다:\n\n```js\n# . . .\n\n# 배포\nresource \"aws_api_gateway_deployment\" \"example\" {\n\n  depends_on = [\n    aws_api_gateway_integration.GET_one_lambda_integration,\n    aws_api_gateway_integration.GET_all_lambda_integration,\n    aws_api_gateway_integration.PATCH_lambda_integration,\n    aws_api_gateway_integration.POST_lambda_integration,\n    aws_api_gateway_integration.DELETE_lambda_integration\n  ]\n\n  triggers = {\n    redeployment = sha1(jsonencode([\n      aws_api_gateway_resource.API-resource-dish,\n      aws_api_gateway_method.GET_one_method,\n      aws_api_gateway_integration.GET_one_lambda_integration,\n      aws_api_gateway_method.GET_all_method,\n      aws_api_gateway_integration.GET_all_lambda_integration,\n      aws_api_gateway_method.POST_method,\n      aws_api_gateway_integration.POST_lambda_integration,\n      aws_api_gateway_method.PATCH_method,\n      aws_api_gateway_integration.PATCH_lambda_integration,\n      aws_api_gateway_method.DELETE_method,\n      aws_api_gateway_integration.DELETE_lambda_integration\n    ]))\n  }\n\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n}\n\n# 배포 스테이지\nresource \"aws_api_gateway_stage\" \"my-prod-stage\" {\n  deployment_id = aws_api_gateway_deployment.example.id\n  rest_api_id   = aws_api_gateway_rest_api.API-gw.id\n  stage_name    = \"prod\"\n\n  depends_on = [aws_cloudwatch_log_group.rest-api-logs]\n}\n```\n\n<div class=\"content-ad\"></div>\n\n백그라운드에서 요청을 보낼 때 무엇이 일어나는지 기록하기 위해 CloudWatch 로그 그룹을 설정하고 있습니다. CloudWatch LogGroup의 이름은 API-Gateway-Execution-Logs\\_'YOUR_API_ID'/'YOUR_STAGE_NAME' 형식이어야 합니다.\n\n그런 다음 API Gateway 스테이지 수준 실행 로깅을 설정하기 위해 \"method_settings\" 리소스를 사용합니다.\n\n```js\n# . . .\n\n# 디버깅 목적의 CloudWatch 로그 그룹\nresource \"aws_cloudwatch_log_group\" \"rest-api-logs\" {\n  name              = \"API-Gateway-Execution-Logs_${aws_api_gateway_rest_api.API-gw.id}/prod\"\n  retention_in_days = 7\n}\n\n# 메서드 설정\nresource \"aws_api_gateway_method_settings\" \"my_settings\" {\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n  stage_name  = aws_api_gateway_stage.my-prod-stage.stage_name\n  method_path = \"*/*\"\n  settings {\n    logging_level = \"INFO\"\n    data_trace_enabled = true\n    metrics_enabled = true\n  }\n}\n```\n\n다음으로 CORS 모듈을 정의합니다. AWS 문서는 CORS 및 통합 및 통합 응답과 관련된 모든 뉘앙스를 잘 설명하고 있으므로 여기에 링크만 첨부하겠습니다.\n\n<div class=\"content-ad\"></div>\n\n```js\n# . . .\n\nmodule \"cors\" {\n  source = \"./modules/cors\"\n\n  api_id            = aws_api_gateway_rest_api.API-gw.id\n  api_resource_id   = aws_api_gateway_resource.API-resource-dish.id\n  allow_credentials = true\n}\n```\n\n![Image](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_3.png)\n\n이 구조를 따라가서 각 파일에 다음 코드를 붙여넣으세요.\n\ncors.tf:\n\n\n\n<div class=\"content-ad\"></div>\n\n```js\n# cors.tf\n\nresource \"aws_api_gateway_method\" \"_\" {\n  rest_api_id   = var.api_id\n  resource_id   = var.api_resource_id\n  http_method   = \"OPTIONS\"\n  authorization = \"NONE\"\n}\n\n# aws_api_gateway_integration._\nresource \"aws_api_gateway_integration\" \"_\" {\n  rest_api_id = var.api_id\n  resource_id = var.api_resource_id\n  http_method = aws_api_gateway_method._.http_method\n\n  type = \"MOCK\"\n\n  request_templates = {\n    \"application/json\" = \"{ \\\"statusCode\\\": 200 }\"\n  }\n}\n\n# aws_api_gateway_integration_response._\nresource \"aws_api_gateway_integration_response\" \"_\" {\n  rest_api_id = var.api_id\n  resource_id = var.api_resource_id\n  http_method = aws_api_gateway_method._.http_method\n  status_code = 200\n\n  response_parameters = local.integration_response_parameters\n\n  depends_on = [\n    aws_api_gateway_integration._,\n    aws_api_gateway_method_response._,\n  ]\n}\n\n# aws_api_gateway_method_response._\nresource \"aws_api_gateway_method_response\" \"_\" {\n  rest_api_id = var.api_id\n  resource_id = var.api_resource_id\n  http_method = aws_api_gateway_method._.http_method\n  status_code = 200\n\n  response_parameters = local.method_response_parameters\n\n  response_models = {\n    \"application/json\" = \"Empty\"\n  }\n\n  depends_on = [\n    aws_api_gateway_method._,\n  ]\n}\n```\n\nheaders.tf:\n\n```js\n# headers.tf\n\nlocals {\n  headers = tomap({\n     \"Access-Control-Allow-Headers\"= \"'${join(\",\", var.allow_headers)}'\",\n    \"Access-Control-Allow-Methods\"= \"'${join(\",\", var.allow_methods)}'\",\n    \"Access-Control-Allow-Origin\"= \"'${var.allow_origin}'\",\n    \"Access-Control-Max-Age\"= \"'${var.allow_max_age}'\",\n    \"Access-Control-Allow-Credentials\"= var.allow_credentials ? \"'true'\" : \"\"\n  })\n\n  # Pick non-empty header values\n  header_values = compact(values(local.headers))\n\n  # Pick names that from non-empty header values\n  header_names = matchkeys(\n    keys(local.headers),\n    values(local.headers),\n    local.header_values\n  )\n\n  # Parameter names for method and integration responses\n  parameter_names = formatlist(\"method.response.header.%s\", local.header_names)\n\n  # Map parameter list to \"true\" values\n  true_list = split(\"|\",\n    replace(join(\"|\", local.parameter_names), \"/[^|]+/\", \"true\")\n  )\n\n  # Integration response parameters\n  integration_response_parameters = zipmap(\n    local.parameter_names,\n    local.header_values\n  )\n\n  # Method response parameters\n  method_response_parameters = zipmap(\n    local.parameter_names,\n    local.true_list\n  )\n}\n```\n\nvariables.tf:\n\n<div class=\"content-ad\"></div>\n\n```json\n변수 \"api_id\" {\n  설명 = \"API 식별자\"\n}\n\n# var.api_resource_id\n변수 \"api_resource_id\" {\n  설명 = \"API 리소스 식별자\"\n}\n\n# -----------------------------------------------------------------------------\n# Variables: CORS-related\n# -----------------------------------------------------------------------------\n\n# var.allow_headers\n변수 \"allow_headers\" {\n  설명 = \"허용 헤더\"\n  유형 = list(string)\n\n  기본값 = [\n    \"Authorization\",\n    \"Content-Type\",\n    \"X-Amz-Date\",\n    \"X-Amz-Security-Token\",\n    \"X-Api-Key\",\n  ]\n}\n\n# var.allow_methods\n변수 \"allow_methods\" {\n  설명 = \"허용 메소드\"\n  유형 = list(string)\n\n  기본값 = [\n    \"OPTIONS\",\n    \"HEAD\",\n    \"GET\",\n    \"POST\",\n    \"PUT\",\n    \"PATCH\",\n    \"DELETE\",\n  ]\n}\n\n# var.allow_origin\n변수 \"allow_origin\" {\n  설명 = \"허용 출처\"\n  유형 = string\n  기본값 = \"*\"\n}\n\n# var.allow_max_age\n변수 \"allow_max_age\" {\n  설명 = \"응답 캐싱 시간을 허용\"\n  유형 = string\n  기본값 = \"7200\"\n}\n\n# var.allowed_credentials\n변수 \"allow_credentials\" {\n  설명 = \"자격 증명 허용\"\n  기본값 = false\n}\n```\n\n마지막으로 \"outputs.tf\" 파일에 아래와 같이 API Gateway를 적용한 후의 호출 URL을 출력하는 블록을 선언하세요:\n\n```json\n# 테스트 API Gateway URL\noutput \"api_gateway_url\" {\n  value = aws_api_gateway_deployment.example.invoke_url\n}\n```\n\n# 단계 5: 코그니토로 인증 추가하기\n\n`\n\n<div class=\"content-ad\"></div>\n\n다이나모DB 테이블의 작업(POST, PATCH, DELETE)은 위험할 수 있습니다. REST API를 공개적으로 노출하고 싶지 않으므로 일부 HTTP 엔드포인트에 대한 액세스를 제한하고 싶습니다. 따라서 인증을 구현하고자 하는데, 첫 번째 단계는 Cognito 사용자 풀을 생성하는 것입니다.\n\n```js\n# authentication.tf\n\nresource \"aws_cognito_user_pool\" \"pool\" {\n  name = \"mypool\"\n}\n```\n\n응용 프로그램이 사용자 풀에 액세스할 수 있도록하려면 사용자 풀 클라이언트를 정의해야 합니다. 우리는 기본 사용자 정보(email, openid, profile)에 대한 허용된 OAuth 플로 및 사용자 스코프를 명시하고 있습니다. 클라이언트 시크릿을 생성하지 않습니다. 또한 관리자 및 사용자 비밀번호 인증이 모두 허용됩니다. Cognito가 식별 제공자입니다. 그런 다음 OAuth 2.0 인증 서버가 사용자를 성공적으로 인증한 후 사용자를 리디렉션해야 할 위치 및 로그아웃 후 리디렉션할 위치가 정의됩니다. 어쨌든 이 프로젝트에 대해서는 이렇게까지 자세히 묘사하는 것은 그리 중요하지 않습니다.\n\n```js\n# authentication.tf\n\nresource \"aws_cognito_user_pool_client\" \"client\" {\n  name = \"client\"\n  allowed_oauth_flows_user_pool_client = true\n  generate_secret = false\n  allowed_oauth_scopes = [\"aws.cognito.signin.user.admin\",\"email\", \"openid\", \"profile\"]\n  allowed_oauth_flows = [\"implicit\", \"code\"]\n  explicit_auth_flows = [\"ADMIN_NO_SRP_AUTH\", \"USER_PASSWORD_AUTH\"]\n  supported_identity_providers = [\"COGNITO\"]\n\n  user_pool_id = aws_cognito_user_pool.pool.id\n  callback_urls = [\"https://example.com\"]\n  logout_urls = [\"https://example.com\"]\n}\n```\n\n<div class=\"content-ad\"></div>\n\n유저 풀 내에서 API 액세스를 테스트하기 위해 유저도 생성합니다.\n\n```js\n# authentication.tf\n\nresource \"aws_cognito_user\" \"example\" {\n  user_pool_id = aws_cognito_user_pool.pool.id\n  username = \"mattia\"\n  password = \"Test@123\"\n}\n```\n\n이 구성을 적용하여 모든 리소스가 올바르게 생성되었는지 확인해보세요 (유저 풀, 유저 풀 클라이언트 및 유저). AWS 관리 콘솔에서 “Amazon Cognito”로 이동하여 “User pools”를 선택하고 방금 생성한 풀을 클릭합니다. User pool ID를 메모하세요.\n\n<img src=\"/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_4.png\" />\n\n<div class=\"content-ad\"></div>\n\n새 사용자도 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_5.png)\n\n“앱 통합”을 클릭하고 “앱 클라이언트 및 분석”으로 내려가면 우리가 만든 클라이언트도 확인할 수 있습니다. 클라이언트 ID를 메모해 두세요.\n\n![이미지](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_6.png)\n\n<div class=\"content-ad\"></div>\n\n인증을 실제로 구현하고 애플리케이션의 일부 API 엔드포인트에 대한 공개 액세스를 제한하려면 먼저 Cognito 사용자 풀 내에서 Authorizer를 정의해야 합니다. Authorizer가 활성화되면 Lambda가 트리거되기 전에 수신된 요청 토큰이 먼저 이 Cognito 사용자 풀과 대조되어야 합니다. 따라서 \"api_gateway.tf\"에서 Authorizer를 정의합니다:\n\n```js\n# api_gateway.tf\n\nresource \"aws_api_gateway_authorizer\" \"demo\" {\n  name = \"my_apig_authorizer2\"\n  rest_api_id = aws_api_gateway_rest_api.API-gw.id\n  type = \"COGNITO_USER_POOLS\"\n  provider_arns = [aws_cognito_user_pool.pool.arn]\n}\n```\n\n기억하시나요? HTTP 메서드의 리소스 블록을 정의할 때 \"authorization\"을 \"NONE\"으로 설정했던 것을요. 이제 이 값을 변경하여 \"COGNITO_USER_POOLS\"로 설정하고 Authorizer ID를 지정하려고 합니다. 예를 들어 POST HTTP 메서드의 경우:\n\n```js\n# api_gateway.tf\n\n#####################################################################################################\n########################### POST /dish #########################################################\n#####################################################################################################\n\nresource \"aws_api_gateway_method\" \"POST_method\" {\n  rest_api_id   = aws_api_gateway_rest_api.API-gw.id\n  resource_id   = aws_api_gateway_resource.API-resource-dish.id\n  http_method   = \"POST\"\n  # authorization = \"NONE\" // 주석 처리\n  authorization = \"COGNITO_USER_POOLS\"\n  authorizer_id = aws_api_gateway_authorizer.demo.id\n}\n```\n\n<div class=\"content-ad\"></div>\n\n위의 모든 중요한 엔드포인트(POST, PATCH, DELETE)에 대해 이 작업을 수행하십시오.\n\n더불어 이 구성을 적용하면 Postman으로 새 요청을 보내면 401 Unauthorized가 반환될 것입니다:\n\n![image](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_7.png)\n\n이제 HTTP 요청을 제출할 때 인가를 받기 위해 액세스 토큰을 제공해야 합니다. 액세스 토큰을 생성하려면 이전에 기록한 정보를 사용하여 \"aws cognito-idp\" 명령을 사용할 것입니다:\n\n<div class=\"content-ad\"></div>\n\n```md\naws cognito-idp admin-initiate-auth --user-pool-id <USER_POOL_ID> --client-id <CLIENT_ID> --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME=mattia,PASSWORD=Test@123\n```\n\n위 명령어에서 User Pool ID, User Pool client ID, 그리고 이전에 정의한 테스트 사용자의 사용자 이름과 암호를 교체해야 합니다.\n\n우리는 Cognito 사용자 풀에 대한 테스트 사용자를 인증하고, 그 결과로 액세스 토큰을 받습니다. 위 명령어의 출력은 아래와 유사합니다:\n\n```md\n{\n\"ChallengeParameters\": {},\n\"AuthenticationResult\": {\n\"AccessToken\": <ACCESS_TOKEN>,\n\"ExpiresIn\": 3600,\n\"TokenType\": \"Bearer\",\n\"RefreshToken\": <REFRESH_TOKEN>,\n\"IdToken\": <ID_TOKEN> # ID 토큰의 값을 복사하세요\n}\n}\n```\n\n<div class=\"content-ad\"></div>\n\n이 토큰들을 테스트하려면 ID 토큰의 값을 복사하고 AWS 관리 콘솔에서 \"API Gateway\"로 이동한 다음, API를 선택하고 왼쪽에 있는 \"Authorizers\"를 클릭하세요:\n\n![API Gateway Authorizer Test](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_8.png)\n\n이제 Authorizer를 클릭하세요. 그런 다음 Authorizer 테스트 섹션에 이전에 복사한 ID 토큰을 붙여넣고 \"Test authorizer\" 버튼을 클릭하세요. 파란 상자 안의 그것과 같은 출력이 있어야 합니다:\n\n![Authorizer Test Output](/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_9.png)\n\n<div class=\"content-ad\"></div>\n\n\"토큰 만료 날짜인 'exp' 필드가 있는 것을 확인할 수 있습니다. 제 경우에는 유효합니다.\n\n이제 Postman으로 돌아가서 \"Headers\" 탭으로 이동하여 새 필드를 만들고 키를 \"Authorization\"로 선택한 후 값 필드에 다음 형식으로 ID 토큰을 지정하세요:\n\n```js\nBearer <ID_TOKEN>\n```\n\n<img src=\"/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_10.png\" />\"\n\n<div class=\"content-ad\"></div>\n\n요청을 보내시면 지금은 200 상태 코드를 받게 될 거에요. 모든 것이 잘 되고 있어요.\n\n# 결론\n\n이 프로젝트를 좋아해 주셨으면 좋겠고, 다음에 또 만나요! 궁금한 점 있으면 언제나 물어봐 주세요!\n","ogImage":{"url":"/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_0.png"},"coverImage":"/assets/img/2024-05-23-AStep-by-StepGuideOnDeployingRESTAPIusingAPIGatewayLambdaDynamoDBCognitoTerraform_0.png","tag":["Tech"],"readingTime":31},{"title":"Amazon Redshift의 디자인을 이해하는 데 다시 8시간을 보냈어요 내가 발견한 것은 여기 있어요","description":"","date":"2024-05-23 13:50","slug":"2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound","content":"\n## 레드시프트 학술 논문으로부터의 모든 통찰: 2022년에 새롭게 태어난 아마존 레드시프트\n\n- 역사와 배경\n- 고수준 아키텍처\n- 쿼리의 생애\n- 코드 생성\n- 컴파일 서비스\n- 저장\n- 컴퓨팅\n- 통합\n\n# 소개\n\n나이가 들수록 많은 것들을 잘못 알고 있었다는 것을 깨달았습니다. 아마존 레드시프트에 대해 잘못 생각한 것이 하나입니다. 레드시프트에 갇혀 거의 일 년을 보낸 후 구글 빅쿼리를 처음 사용했을 때, 빅쿼리가 5배 이상 더 발전된 기술이고(특히 빅쿼리의 서버리스 경험 때문에), 레드시프트보다 더 진보했다고 스스로에게 이야기했습니다. 그 인상은 세 년간 지속되었습니다. 돌이켜보면, 나 자신을 비웃으며 왜 그렇게 순진했는지 의문을 제기합니다.\n\n<div class=\"content-ad\"></div>\n\n우수한 제품인 BigQuery, Redshift 또는 Snowflake와 같은 데이터베이스는 각각 하드웨어 제약 조건을 다루고 시스템 디자인 문제를 해결하는 고유한 방식이 있습니다. 어떤 데이터베이스가 더 빠른지 비교하는 대신에, 나는 그들의 내부 구현을 살펴가면서 가치 있는 것들을 배우는 것을 좋아해요. 이 기사는 Amazon Redshift에 대해 심층적으로 탐구한 결과물입니다 — 이전에 내가 무시했던 OLAP 시스템입니다.\n\n이 기사에서는 학술 논문 \"Amazon Redshift Re-invented (2022)\"에서 대부분의 자료를 사용할 것이며, 추가 참고 문서는 기사 끝에 포함될 것입니다.\n\n# 역사\n\nAmazon Redshift는 클라우드를 위해 설계된 열 지향적 대규모 병렬 처리 데이터 웨어하우스입니다. 이 시스템은 대규모 병렬 처리 (MPP) 데이터 웨어하우스 회사 ParAccel에서 기술을 기반으로 구축되었으며, 이후 Actian에 인수되었습니다. 이 시스템은 이전 버전인 PostgreSQL 8.0.2를 기반으로 구축되었으며, Redshift는 그 버전에 변경을 가했습니다. 초기 프리뷰 베타판은 2012년 11월에 출시되었고, 전체 버전은 2013년 2월 15일에 제공되었습니다.\n\n<div class=\"content-ad\"></div>\n\n# High-level architecture\n\n![img](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_0.png)\n\nRedshift 클러스터는 쿼리 실행을 처리하기 위해 여러 컴퓨팅 인스턴스로 구성됩니다. 각 클러스터는 단일 코디네이터 노드(=리더)와 여러 워커 노드를 가지고 있습니다.\n\n데이터는 Amazon S3를 기반으로 하는 Redshift 관리 스토리지(RMS)에 저장됩니다. Redshift가 쿼리를 처리할 때, 데이터는 압축된 컬럼 지향 형식으로 로컬 SSD에 있는 컴퓨팅 노드에 캐싱됩니다. (저의 제한된 지식으로는 이것이 Snowflake 저장 계층과 유사하다고 생각됩니다.)\n\n<div class=\"content-ad\"></div>\n\n테이블 데이터는 여러 버킷으로 분할되어 모든 워커 노드에 분산됩니다. Redshift는 데이터의 특성에 기반하여 파티션 스키마를 적용할 수도 있고, 사용자가 명시적으로 라운드로빈 또는 해시와 같은 원하는 파티션 스키마를 선언할 수도 있습니다.\n\n컴퓨팅과 스토리지 외에도 Redshift에는 다음과 같은 구성 요소가 있습니다 :\n\n- AQUA는 FPGA를 활용하여 쿼리 성능을 가속화하는 레이어입니다.\n- Compilation-As-A-Service는 생성된 코드(쿼리에서)를 위한 캐싱 서비스입니다.\n- Amazon Redshift Spectrum을 사용하면 Redshift에서 S3의 데이터를 직접 쿼리할 수 있습니다.\n\n# 쿼리의 생명주기\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_1.png\" />\n\n아키텍처 구성 요소를 자세히 살펴보기 전에, Redshift 쿼리의 여정을 간단히 살펴보겠습니다:\n\n- 쿼리는 먼저 리더 노드에 \"안녕\"이라고 말합니다. 여기서 구문 분석, 재작성 및 최적화됩니다.\n- Redshift는 클러스터의 토폴로지를 사용하여 최적의 계획을 선택합니다. 계획 프로세스는 데이터 분포 정보도 활용하여 데이터 이동을 줄입니다.\n- 계획 단계 후 Redshift는 실행 단계로 이동합니다. 계획은 개별 실행 단위로 분할됩니다. 각 단위는 이전 단위의 중간 출력을 사용합니다. Redshift는 각 단위에 대해 최적화된 C++ 코드를 생성 및 컴파일하고 이 코드를 네트워크를 통해 컴퓨트 노드로 전송합니다.\n- 열 지향 데이터는 로컬 SSD에서 스캔되거나 Redshift 관리 스토리지에서 공급됩니다.\n\nRedshift 실행 엔진은 성능을 향상시키기 위해 여러 최적화 기술을 적용합니다:\n\n<div class=\"content-ad\"></div>\n\n- zone-maps를 사용하는 것은 작은 해시 테이블이며 각 데이터 블록의 최소-최대 값을 저장합니다. (Snowflake와 BigQuery도 이렇게 합니다.)\n- 스캔 연산은 Vectorization 및 SIMD(단일 명령, 다중 데이터) 처리를 사용합니다.\n- 가벼운 압축 형식입니다.\n- 블룸 필터\n- 프리패칭\n- Redshift의 AZ64 압축.\n\n제가 Redshift 구성 요소에 대해 자세히 설명할 때 이러한 기술들을 다시 볼 수도 있습니다.\n\n# 코드 생성\n\nOLAP(On-Line Analytical Processing) 세계에서 쿼리 성능을 향상시키는 두 가지 주요 방법은 벡터화(Vectorization)와 코드 특수화(Code Specialization)입니다.\n\n<div class=\"content-ad\"></div>\n\nVectorization의 주요 아이디어는 하나의 레코드를 처리하는 대신, 엔진이 일괄(벡터) 값으로 처리한다는 것입니다.\n\n후자의 방식에서 엔진은 각 쿼리에 대해 코드를 생성하여 CPU 명령을 줄입니다. 코드 특수화를 적용하지 않는 시스템에서 각 연산자는 데이터 유형을 확인하고 입력 데이터 유형에 적합한 함수를 선택하기 위해 조건 블록(switch)을 통과해야 합니다. 코드 생성 방식은 실행 중에 해당 쿼리의 모든 연산자를 생성하기 때문에 이러한 과정을 피합니다.\n\nRedshift는 코드 생성 방식을 적용했습니다. 시스템은 쿼리 계획과 실행 스키마에 특정한 C++ 코드를 생성합니다. 생성된 코드는 컴파일되고, 바이너리가 실행을 위해 컴퓨팅 노드로 전달됩니다. 각 컴파일된 파일은 물리적 쿼리 계획의 일부인 세그먼트라고 합니다.\n\n<div class=\"content-ad\"></div>\n\n코드 생성을 적용했음에도 불구하고 Redshift는 생성된 코드에 SIMD-벡터화된 데이터 스캔 레이어를 추가합니다. 벡터화된 스캔 함수는 미리 컴파일되며 (실시간으로 생성되는 것이 아닌) Switch 문으로 모든 데이터 유형을 처리합니다. 이는 Redshift가 더 나은 데이터 스캔 성능을 달성하고 각 쿼리에 대해 컴파일해야 하는 내장 코드 양을 줄이는 데 도움이 됩니다.\n\n# 컴파일 서비스\n\n위 섹션에서 알 수 있듯이 Redshift는 쿼리 실행을 위해 컴파일된 최적화된 객체를 사용할 것입니다. 이러한 객체는 로컬 클러스터 캐시에 캐싱되므로 동일하거나 유사한 쿼리가 실행될 때 마다 컴파일된 객체가 재사용되어 실행 시간이 더 빨라집니다. Redshift가 쿼리를 컴파일할 필요가 없어지기 때문입니다. 이 전략은 필요한 컴파일된 객체가 로컬 캐시에 있는 경우에만 성능을 향상시킵니다. 그렇지 않은 경우 Redshift는 코드를 생성해야 하므로 지연이 발생합니다.\n\n<img src=\"/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_3.png\" />\n\n<div class=\"content-ad\"></div>\n\n2020년에 Redshift는 컴파일 서비스를 소개했어요 (만약 마일스톤에 관해 틀린 정보를 전달했다면 이를 수정해 주세요). 이 서비스는 클러스터 리소스 대신 별도의 리소스를 사용해요. 컴파일 서비스는 컴파일된 객체를 외부 캐시에 캐싱하여 Redshift가 여러 클러스터에 대해 캐시된 객체를 제공할 수 있게 해줘요.\n\n또한, 컴파일 서비스는 외부 컴파일 서비스의 병렬성을 활용하여 원하는 객체가 로컬 캐시나 외부 캐시에 없을 경우 코드를 더 빨리 컴파일할 수 있어요.\n\nRedshift 뒤에 있는 사람들은 다음을 관찰했어요:\n\n# CPU 친화적인 인코딩\n\n<div class=\"content-ad\"></div>\n\nRedshift은 디스크에 압축된 데이터를 저장합니다. LZO 및 ZSTD와 같은 일반적인 압축 알고리즘 외에도 Redshift는 AZ64 알고리즘과 같은 최적화된 유형별 알고리즘을 지원합니다. 이 알고리즘은 숫자 및 날짜/시간 데이터 유형을 다루는 것으로, 2019년에 Amazon에서 소개되었습니다. AZ64은 높은 압축 비율을 달성하고 성능을 향상시키도록 설계되었습니다. AZ64는 ZSTD와 비슷한 압축률을 달성하지만 빠른 압축 해제 속도를 갖추고 있습니다.\n\n여기서 언급해야 할 멋진 점은 사용자가 AUTO 옵션(레드시프트가 데이터에 대한 압축을 자동으로 정의하도록 하는 옵션) 외에도 열 단위로 명시적으로 압축 체계를 정의할 수 있다는 것입니다. 게다가 한번 정의한 후에는 ALTER TABLE 절을 사용하여 압축 체계를 변경할 수 있습니다. 이것은 흥미로운 기능이라고 생각합니다. 사용자가 데이터에 대해 가장 이해하고 있으므로 유연한 압축 옵션을 허용하는 것이 데이터가 어떻게 저장되는지에 대한 더 나은 통제를 제공할 수 있을 것입니다. 그에 상응하여 더 많은 권한은 더 큰 책임을 의미합니다. 조심하지 않으면 나쁜 (압축) 선택이 성능과 비용에 악영향을 줄 수 있습니다. 내가 아는 한, Google은 BigQuery에서 이 기능을 허용하지 않습니다. Snowflake가 이를 지원하는지 알지 못하니 알고 계시면 의견을 남겨주세요.\n\n# 적응형 실행\n\nRedshift의 쿼리 엔진은 실행 통계에 따라 생성된 코드나 실행 속성을 조정하여 성능을 향상시키는 런타임 결정을 내립니다. 실행 중에 Bloom 필터를 사용하는 것은 Redshift의 동적 최적화의 대담한 예입니다.\n\n<div class=\"content-ad\"></div>\n\n# Amazon Redshift을 위한 AQUA\n\n<img src=\"/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_4.png\" />\n\nAdvanced Query Accelerator (AQUA)는 2021년에 Redshift에 의해 소개된 멀티 테넌시 서비스입니다. 이는 Redshift Managed Storage를 위한 캐싱 레이어 역할을 하며 복잡한 스캔 및 집계를 가속화합니다.\n\nAQUA는 지역 서비스인 Amazon S3에서 데이터를 가져오는 레이턴시를 피하고 Redshift의 캐시 스토리지에 데이터를 채우는 필요성을 줄이기 위해, 클러스터의 핫 데이터(여러 번 액세스되는 데이터)를 로컬 SSD에 캐싱합니다. Redshift는 입력 쿼리로부터 해당 스캔 및 집계 작업을 감지하고 이를 AQUA로 푸시하여 캐시된 데이터로 처리합니다.\n\n<div class=\"content-ad\"></div>\n\n아마존의 사람들은 AWS의 Nitro ASIC를 활용한 커스텀 서버를 설계하여 압축 및 암호화를 가속화하고, FPGAs를 사용하여 필터링 및 집합 연산의 실행 속도를 향상시키도록 했습니다.\n\n# 쿼리 재작성 프레임워크\n\nRedshift는 또한 두 가지 목표를 가진 새로운 쿼리 재작성 프레임워크(QRF)를 소개했습니다:\n\n- 연합, 조인 및 집계와 같은 연산의 실행 순서를 최적화하기 위한 재작성 규칙.\n- 점진적 재료화 뷰 쿼리 및 유지 관리를 위한 스크립트 작성. (곧 다룰 예정입니다)\n\n<div class=\"content-ad\"></div>\n\n# 스토리지\n\n이 섹션에서는 Redshift의 스토리지 레이어인 Redshift Managed Storage부터 동시성 제어까지 살펴볼 것입니다.\n\n# Redshift Managed Storage (RMS)\n\nRA3 클러스터 유형이나 Redshift 서버리스를 선택할 때, 데이터는 RMS에 저장됩니다. 이 저장 레이어는 Amazon S3를 기반으로 하며, 특정 년도 동안 다중 존에서 99.999999999%의 내구성과 99.99%의 가용성을 달성합니다. RMS를 사용하면 데이터가 컴퓨팅 노드에서 분리되어 저장되므로 고객은 컴퓨팅과 스토리지를 독립적으로 확장하고 비용을 지불할 수 있습니다. RMS는 S3를 기반으로 하기 때문에 데이터 블록 온도 및 블록화와 같은 최적화를 사용하여 높은 성능을 제공합니다.\n\n<div class=\"content-ad\"></div>\n\nRMS는 고대역 네트워킹을 제공하는 AWS Nitro 시스템 위에서 구축되었어요. RMS는 티어-1 캐시로 높은 성능의 SSD 기반 로컬 스토리지를 사용하고 있어요. Redshift는 자동으로 미세하게 데이터를 제거하고 지능적으로 데이터를 미리 가져오는 기술을 활용하여 로컬 SSD에서 최고의 성능을 얻으면서 S3의 무제한 확장성을 달성하고 있어요.\n\n![이미지](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_5.png)\n\nRMS는 S3로부터 데이터 접근을 향상시키기 위해 데이터 블록을 메모리에 넣고 로컬 SSD에 캐시하는 미리가져오기 메커니즘을 사용하고 있어요. RMS는 관련 블록이 로컬에서 사용 가능하도록 유지하기 위해 캐시 대체를 조정하면서 모든 블록에 대한 액세스를 추적해요. 로컬 SSD 위의 캐시 레이어인 메모리 디스크 캐시 크기는 쿼리의 성능과 메모리 요구 사항을 균형있게 조정할 수 있도록 동적으로 변경될 수 있어요.\n\n테이블의 데이터는 데이터 슬라이스로 분할되어 논리적인 데이터 블록 체인으로 저장됩니다. 각 블록(크기 1MB)에는 식별, 테이블 소유권 또는 슬라이스 정보와 같은 정보가 포함된 헤더가 있어요. 블록은 메모리 내 구조인 슈퍼 블록을 사용하여 색인화돼요. 논문에 따르면 슈퍼 블록은 많은 파일 시스템과 유사한 특성을 가진 색인 구조입니다. 쿼리는 슈퍼 블록을 스캔하기 위해 존 맵을 사용하여 필요한 데이터 블록을 가져와요. 게다가 슈퍼 블록은 실행 중인 쿼리에 의해 처리된 데이터 블록을 위한 쿼리 추적 정보도 포함하고 있어요.\n\n<div class=\"content-ad\"></div>\n\nRMS는 Amazon S3로 트랜잭션을 동기화하여 여러 클러스터가 일관된 데이터에 액세스할 수 있게 해줍니다. 데이터는 쓰기 요청을 일괄 처리함으로써 다른 가용 영역에 걸쳐 S3에 쓰여집니다. 동시 클러스터는 동시 쓰기를 위해 필요할 때 요청되며 읽기는 스냅샷 분리에 의존합니다.\n\n메인 클러스터에서 데이터가 삭제되면 Redshift는 해당 데이터가 더 이상 쿼리에 필요하지 않음을 보장하고, 이 데이터를 개체 저장소의 가비지 컬렉터용으로 표시합니다. 데이터가 Amazon S3에 백업되어 있기 때문에 SSD가 고장나도 데이터는 손실되지 않습니다.\n\nAmazon S3는 또한 데이터 스냅샷을 저장합니다. 이러한 스냅샷은 복원 지점으로 작용합니다. Redshift는 전체 클러스터 데이터뿐만 아니라 개별 테이블의 데이터를 복원하는 것도 지원합니다. Amazon S3는 데이터 공유와 머신 러닝의 진실의 원천으로도 기능합니다.\n\n# 데이터 메타데이터 분리\n\n<div class=\"content-ad\"></div>\n\n메타데이터와 데이터를 분리하면 Elastic Resize 및 Cross Instance Restore와 같은 프로세스를 구현하기가 더 쉬워집니다. 이 둘 다 메타데이터를 한 클러스터 구성에서 다른 클러스터 구성으로 이동해야 합니다.\n\nElastic Resize는 고객이 클러스터의 노드를 추가하여 성능을 향상시키거나 노드를 제거하여 비용을 절약할 수 있는 기능입니다. Cross-Instance Restore를 통해 사용자는 하나의 인스턴스 유형 클러스터에서 가져온 스냅샷을 다른 인스턴스 유형이나 다른 노드 수의 클러스터로 복원할 수 있습니다.\n\n이 프로세스의 구현 세부 정보는 다음과 같습니다:\n\n- 데이터의 사본이 Amazon S3에 저장되도록 보장합니다.\n- 재구성하기 전 Redshift는 클러스터의 데이터를 고려합니다. 데이터 이동을 최소화하는 재구성 계획을 수립하여 균형 잡힌 클러스터를 생성합니다.\n- 재구성하기 전에 Redshift는 데이터에 대한 카운트와 체크섬을 기록하고 완료 후 정확성을 검증합니다.\n- 복원의 경우 Redshift는 테이블 수, 블록 수, 행 수, 사용된 바이트 및 데이터 분포의 카운트를 기록하고 스냅샷과 함께 저장합니다. 복원 후에 카운트와 체크섬을 검증하고 새 쿼리를 수락하기 전에 확인합니다.\n\n<div class=\"content-ad\"></div>\n\n다음 논문을 따르면:\n\n## 로컬 저장소의 한계를 넘어서\n\nRedshift는 무한한 확장성을 제공하기 위해 Amazon S3를 활용하며, 로컬 메모리와 SSD를 캐시로 사용합니다. (Snowflake와 같이).\n\n클러스터는 각 데이터 블록의 액세스 횟수에 따라 작업 데이터 세트를 로컬로 유지합니다. Tiered 캐시는 이 정보를 추적하는 역할을 합니다. 캐시는 두 단계로 구성됩니다:\n\n<div class=\"content-ad\"></div>\n\n\n![image](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_6.png)\n\n- Low level: This level stores cold data blocks. Every time the query accesses a data block, the system increases the block’s reference count.\n- High level: the cold blocks become hot (after being accessed multiple times), and the policy promotes data blocks to a high level.\n\nDuring eviction, the reference count of each block is decremented. When the reference count reaches zero, the block will be moved down to the low level or entirely evicted from the cache.\n\n(Sounds like Python object’s reference count, huh?)\n\n\n<div class=\"content-ad\"></div>\n\nRMS는 클러스터 재구성 후 로컬 SSD에 데이터를 다시 채우는 데 티어별 저장소 캐시를 사용합니다 (예 : Elastic Resize). 이와 같은 시나리오에서 컴퓨팅 노드는 고객 쿼리에서 가장 자주 액세스될 것으로 예상되는 데이터 블록을 로컬 디스크에 채웁니다.\n\n마지막으로, Redshift에는 메모리 내에서 가장 빈번하게 액세스되는 가장 뜨거운 블록을 유지하는 동적 디스크 캐시라는 또 다른 캐시 레이어가 있습니다. 또한 특정 쿼리에서 임시 블록을 저장합니다. 이 캐시는 메모리가 사용 가능할 때 자동으로 확장되고 시스템이 메모리 부족 상태가 되면 자동으로 축소됩니다.\n\n# 점진적 커밋\n\n비용을 절감하기 위해 RMS는 마지막 커밋과 비교하여 데이터 변경 사항만 캡처합니다. 이러한 변경 사항은 나중에 커밋 로그에 업데이트됩니다. Redshift의 로그 기반 커밋 프로토콜은 인메모리 구조를 영구 구조(디스크)로부터 분리하며, 각 슈퍼블록은 변경 사항의 로그입니다. 논문에서 가져온 내용:\n\n<div class=\"content-ad\"></div>\n\n이 로그 구조화된 메타데이터는 일관된 데이터에 접근하여 로그를 적용함으로써 동시성 확장 및 데이터 공유와 같은 기능의 비용을 줄입니다.\n\n# 동시성 제어\n\nRedshift는 다중 버전 동시성 제어를 구현하여 읽기 프로세스가 다른 읽기 요청으로 블록되는 것을 방지합니다. 쓰기 요청은 다른 동시 쓰기 요청에 의해만 차단될 수 있습니다.\n\n각 트랜잭션은 트랜잭션이 시작되기 전에 모든 커밋된 트랜잭션에 의해 설정된 데이터베이스의 일관된 스냅샷을 볼 수 있습니다. 논문에서 Amazon은 스냅샷 격리 위에 Serial Safety Net (SSN)을 기반으로 한 새로운 디자인을 사용하여 엄격한 직렬화를 메모리 효율적인 방식으로 보장하는데 사용했습니다. 이 디자인은 이전에 커밋된 트랜잭션의 요약 정보만 사용하므로 엄격한 직렬화를 보장할 수 있습니다.\n\n<div class=\"content-ad\"></div>\n\n# 계산\n\n아마존을 따라가는 레드시프트는 매주 수십억 개의 쿼리를 처리합니다. 사용자는 필요에 따라 계산 성능을 조절할 수 있는 다음 옵션 중 하나를 선택할 수 있습니다:\n\n# 클러스터 크기 확장\n\n![image](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_7.png)\n\n<div class=\"content-ad\"></div>\n\n이 설정을 통해 고객은 필요에 따라 클러스터에서 컴퓨팅 노드를 추가하거나 제거할 수 있습니다. 데이터를 섞는 대신, Elastic Resize는 데이터 파티션 할당(메타데이터만)을 분배하여 데이터 파티션을 노드 간에 조직화되고 균형 있게 유지합니다. 크기를 조정한 후, 컴퓨팅 노드의 로컬 캐시(SSD)는 할당 정보에 따라 S3에서 데이터를 받아 채웁니다. (Redshift는 핫 데이터에 우선순위를 둠)\n\n그러나 이로 인해 잠재적인 문제가 발생할 수 있습니다. 크기를 조정한 후, 각 노드가 담당하는 데이터의 수가 크기를 조정하기 이전과 다를 수 있으며, 이는 일관되지 않은 쿼리 성능을 초래할 수 있습니다. Redshift는 이를 다루기 위해 컴퓨팅 병렬성(작업자, 프로세스, 스레드 수)을 데이터 파티션과 분리합니다:\n\n![이미지](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_8.png)\n\n- 컴퓨팅 병렬성이 `데이터 파티션의 수와 같은 경우, 개별 컴퓨팅 프로세스는 여러 데이터 파티션에서 작업합니다.\n- 컴퓨팅 병렬성이 `데이터 파티션의 수와 다른 경우, 여러 컴퓨팅 프로세스가 개별 데이터 파티션에서 작업을 공유합니다.\n\n<div class=\"content-ad\"></div>\n\nRedshift은 공유 가능한 작업 단위 덕분에 이것을 달성합니다.\n\n## 동시성 스케일링\n\n이 구성은 OLAP 시스템의 전형적인 도전 과제 중 하나인 동시성을 다루는 데 도움을 줍니다. 사용자가 Redshift에서 더 많은 동시성 기능을 필요로 할 때 동적으로 확장됩니다.\n\n![이미지](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_9.png)\n\n<div class=\"content-ad\"></div>\n\nConcurrency Scaling을 사용하면 사용자는 쿼리를 제출하기 위한 단일 클러스터 활성 엔드포인트를 유지합니다. Redshift는 리소스가 완전히 활용되고 새로운 쿼리가 지속적으로 발생하는 것을 감지하면 자동으로 추가적인 Concurrency Scaling 컴퓨팅 클러스터를 추가합니다. 대기 중인 쿼리는 이러한 클러스터로 라우팅되어 처리됩니다. 또한 추가된 클러스터는 S3로부터 데이터를 로컬 디스크에 채웁니다.\n\n# 컴퓨팅 분리\n\nRedshift는 고객이 다른 Redshift 컴퓨팅 클러스터 및 AWS 계정 간에 데이터를 공유할 수 있도록 합니다. 컴퓨팅 클러스터는 단일 데이터 원본에 액세스할 수 있으며 ETL 파이프라인을 개발하거나 데이터 복사 비용을 부담할 필요가 없습니다.\n\n사용자는 스키마와 테이블부터 사용자 정의 함수(UDF)까지 다양한 수준에서 데이터를 공유할 수 있습니다. 다른 사람과 데이터를 공유하려면 데이터의 소유자(생산자)가 먼저 데이터 공유를 생성하고 사용자에게 액세스를 부여합니다. Redshift는 IAM 정책과 메타데이터를 사용하여 인증 및 권한 부여를 구현합니다.\n\n<div class=\"content-ad\"></div>\n\n고객은 메타데이터 요청을 사용하여 공유된 객체에 대한 쿼리를 공유합니다. 공유 데이터에 액세스하는 것이 인가된 고객만 서비스를 제공받을 수 있습니다. 각 요청은 디렉토리 서비스 및 프록시 레이어를 통해 전달됩니다. 프록시는 요청을 인증하고 승인하며 메타데이터 요청을 적절한 프로듀서로 라우팅합니다. 고객 측에서 메타데이터를 수신한 후에는 RMS에서 원하는 데이터를 읽고 쿼리를 처리합니다. 공유 데이터를 쿼리할 때의 캐시 프로세스는 변경되지 않습니다: 공유 데이터는 클러스터에 캐시되며 이후의 쿼리에서는 로컬로 데이터를 읽어옵니다.\n\n## 자동 튜닝 및 운영\n\n첫날부터 Redshift는 전통적인 데이터 웨어하우징 시스템(로컬 서버 및 데이터 센터 구축)과 비교하여 많은 측면을 간소화했습니다. 그럼에도, 일부 유지 관리 및 튜닝 작업은 경험 많은 데이터베이스 관리자가 필요합니다: 사용자는 성능 향상을 위해 명시적으로 vacuum 프로세스를 예약하거나 분산 또는 정렬 키를 결정해야 합니다.\n\n현재 Redshift는 고객 워크로드에 성능 영향을 미치지 않고 비주요 프로세스인 vacuuming, analyzing 또는 materialized views 새로고침을 자동으로 실행합니다. Redshift는 사용자 워크로드를 관찰하고 분석하여 성능 향상 기회를 식별하며, 예를 들어 워크로드에 대한 최적의 분산 및 정렬 키를 자동으로 지정하여 적용합니다.\n\n<div class=\"content-ad\"></div>\n\n또한, Redshift는 고급 예측 메커니즘을 활용하여 웜 풀을 통해 필요할 때 추가 노드를 가능한 빨리 사용할 수 있습니다. 이는 Snowflake의 사전 웜 워커 풀과 매우 유사하며, 노드 장애 또는 동시성 스케일링으로 인한 쿼리 대기 시간 및 다운타임을 줄입니다. 마지막으로, Amazon Redshift는 사용자 개입 없이 실행 및 스케일링이 쉬운 서버리스 옵션(예: Google BigQuery)을 제공합니다.\n\n# 자동 테이블 최적화\n\n분배 및 정렬 키와 같은 속성을 최적화하여 작업 부하의 성능을 최적화할 수 있습니다. 분배 키는 테이블 데이터가 클러스터 전체에 분배되는 방식을 나타내는 속성으로, 시스템이 병렬 리소스를 효율적으로 할당할 수 있도록 도와줍니다. 정렬 키는 하나 이상의 열을 기반으로 데이터를 정리하여 존 맵 인덱싱을 활용합니다. 존 맵은 데이터 단위의 최소값 및 최대값을 저장하는 인덱싱 구조로, 불필요한 데이터를 건너뛰는 데 매우 유용하며 데이터 정렬은 존 맵을 효율적으로 활용할 수 있습니다. (이건 BigQuery 클러스터링과 비슷하게 들리나요?)\n\n![Amazon Redshift 디자인을 이해하는 데 또 8시간을 보냈어요. 여기서 발견한 것들](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_10.png)\n\n<div class=\"content-ad\"></div>\n\n과거에는 이러한 키들이 사용자에 의해 명시적으로 정의되었습니다. 지금은 Redshift가 자동으로 Automatic Table Optimization (ATO)을 통해 이 프로세스를 처리합니다. ATO는 워크로드를 분석하여 최적의 분배 및 정렬 키를 추천합니다. 추천을 생성하기 위해 최적화된 쿼리 계획, 기본치, 및 예측 선택도와 같은 쿼리 실행 메타데이터를 주기적으로 수집합니다.\n\n목표와 함께 추천된 키들:\n\n- 분배 키: 데이터 이동 비용을 최소화하기 위해 시스템은 특정 워크로드의 모든 테이블을 조사하여 추천해야 합니다.\n- 정렬 키: 디스크에서 읽어야 하는 데이터 양을 줄이기 위해.\n\n추천을 받은 후, Redshift는 고객들에게 두 가지 옵션을 제공합니다:\n\n<div class=\"content-ad\"></div>\n\n- 콘솔을 통한 수동 적용.\n- 자동 백그라운드 워커가 권장 사항을 적용합니다. 이 워커는 설정을 점진적으로 적용하고 클러스터가 너무 바쁘지 않을 때에만 작업을 실행합니다.\n\n# 자동 워크로드 관리\n\nRedshift의 자동 워크로드 관리자(AutoWLM)는 입장 제어, 스케줄링 및 자원 할당을 담당합니다. 쿼리를 수신한 후 AutoWLM은 실행 계획과 최적화된 통계를 벡터 형식으로 변환합니다. 그런 다음 Redshift는 해당 벡터를 ML 모델로 넣어 컴파일 및 실행 시간을 예측합니다. Redshift는 모델의 결과를 사용하여 예측된 실행 시간을 기반으로 쿼리를 대기열에 넣습니다. 추정된 메모리 요구량(모델에 의해 예측됨)이 사용 가능한 메모리 풀에서 충족될 때만 쿼리가 실행 단계로 진행됩니다. 또한 자원 이용률이 너무 높다고 감지될 때 AutoWLM은 병행성 비율을 제한하여 쿼리 대기 시간을 피합니다.\n\nAutoWLM은 스케줄링에 가중 라운드로빈 메커니즘을 활용하여 우선순위가 더 높은 쿼리를 우선적으로 더 자주 스케줄링합니다. 또한 SLA를 준수해야 하는 높은 우선순위 쿼리는 하드웨어 자원의 더 큰 할당을 받습니다. Redshift는 서로 다른 우선순위를 갖는 쿼리가 동시에 실행될 때 CPU 및 I/O를 지수함수적으로 감소하는 부분으로 분할합니다. 이는 높은 우선순위 쿼리를 낮은 우선순위 쿼리보다 지수함수적으로 빠르게 부스트합니다.\n\n<div class=\"content-ad\"></div>\n\n하위 우선순위 쿼리 뒤에 높은 우선순위 쿼리가 오면 AutoWLM은 상위 우선순위 쿼리에 공간을 확보하기 위해 하위 우선순위 쿼리에서 리소스를 회수합니다. 낮은 우선순위 쿼리에서 리소스 고갈을 방지하기 위해 시스템에 의해 리소스가 회수되는 확률이 각 빼앗애질 때마다 줄어듭니다. 결과적으로 리소스가 고갈되면 Redshift는 상위 우선순위 리소스를 제공하기 위해 하위 우선순위 쿼리를 대기열에 넣습니다.\n\n# 쿼리 예측 프레임워크\n\n위 섹션에서 언급했듯이 AutoWLM은 메모리 소비량이나 실행 시간과 같은 메트릭을 예측하기 위해 머신 러닝 모델을 사용합니다. Redshift의 쿼리 예측 프레임워크는 이러한 모델을 유지하는 역할을 합니다. 이 프레임워크는 Redshift 클러스터에서 실행되며 데이터를 수집하고 XGBoost 모델로 훈련을 하며 필요할 때 결과를 출력합니다. 클러스터에서 프레임워크를 실행함으로써 변화하는 워크로드에 빠르게 적응할 수 있습니다. 위에서 언급한 코드 컴파일 서비스도 최적화를 위해 쿼리 예측 프레임워크를 사용합니다.\n\n# 자료화된 뷰\n\n<div class=\"content-ad\"></div>\n\n\n![Image description](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_11.png)\n\nSQL 뷰와 Materialize View (MV)은 쿼리 결과를 테이블처럼 나타내는 방법을 제공합니다. View와는 달리, MV는 데이터를 디스크에 물리적으로 유지하므로 MV에서 데이터를 쿼리할 때 실행 시간이 빨라집니다. Redshift는 MV 관리를 다음과 같이 자동화합니다:\n\n- 기본 테이블의 변경 사항을 반영하여 필터, 선택, 그룹화 및 조인을 점진적으로 유지합니다.\n- 유지 관리 시간을 자동화합니다: Redshift는 어떤 MV를 새로 고쳐야 하는지 감지합니다. 이는 쿼리 워크로드에서 MV의 유틸리티 및 MV 새로 고침 비용 두 가지 요소를 사용하여 수행됩니다.\n- MV를 통해 쿼리를 자동으로 다시 작성하여 최적의 성능을 달성합니다. 점진적 유지 관리 및 쿼리 다시 작성은 \"쿼리 다시 작성 프레임워크\" 섹션에서 언급된 프레임워크를 사용합니다.\n\n# 스마트 웜 풀\n\n\n<div class=\"content-ad\"></div>\n\n클라우드 시대에는 하드웨어 고장이 더 이상 예외가 아닙니다. 머신 고장으로 인한 성능 저하를 방지하기 위해 Redshift는 스마트 웜 풀 아키텍처(컴퓨팅 머신이 서비스되기 전에 미리 웜업됨)를 사용합니다. 이 아키텍처는 많은 프로세스에서 효율성을 제공합니다: 실패한 노드 교체, 클러스터 재개, 자동 동시성 스케일링...\n\n![image](/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_12.png)\n\n웜 풀은 사전 설치된 소프트웨어와 네트워킹 구성을 갖춘 EC2 컴퓨트 인스턴스 그룹입니다. 각 지역마다 AWS 가용 영역마다 별도의 웜 풀이 위치해 있습니다. 작업을 낮은 지연 시간으로 유지하기 위해서는 웜 풀에서 노드를 획득할 때 높은 히트율이 필요합니다. Redshift는 머신러닝 모델을 사용하여 특정 시점에서 필요한 EC2 인스턴스 수를 예측합니다. 시스템은 각 지역과 가용 영역에서 웜 풀을 동적으로 조정하여 인프라 비용을 절약합니다.\n\n# 통합\n\n<div class=\"content-ad\"></div>\n\n내장형인 AWS Redshift는 Amazon 클라우드 서비스 생태계의 혜택을 크게 누립니다. 여기에는 Redshift의 통합 옵션 몇 가지가 있습니다.\n\n- Amazon Redshift Spectrum을 사용하면 S3에서 데이터를 직접 쿼리할 수 있습니다. 이 기능은 대규모 Scale-Out 처리를 제공하여 Parquet, Text, ORC 및 AVRO 형식의 데이터를 스캔하고 집계합니다.\n- Amazon Sagemaker와 함께하는 Redshift ML을 사용하면 SQL을 사용하여 기계 학습 모델을 학습하고 예측하는 것이 쉬워집니다. Redshift ML은 Amazon SageMaker를 활용하여 모델을 학습한 뒤 SQL 함수로 노출하고 사용자는 SQL을 사용하여 직접 사용할 수 있습니다\n- Redshift Federated Query를 사용하면 Redshift가 고객의 OLTP 데이터베이스 (Postgres, MySQL 등)에 직접 연결하여 데이터를 가져올 수 있습니다. 이 편리한 기능으로 ETL 파이프라인을 통해 OLTP 출처에서 데이터를 추출할 필요가 없어집니다.\n- 슈퍼 스키마리스 처리: SUPER 반구조화 형식은 스키마가 없는 중첩 데이터를 포함할 수 있습니다. SUPER 형식의 값은 Redshift 문자열 및 숫자 스칼라, 배열 및 구조체로 구성될 수 있습니다. 사용자는 SUPER 유형을 사용할 때 미리 스키마를 정의할 필요가 없습니다. Redshift의 동적 형식 지정은 중첩 데이터를 감지할 수 있습니다.\n- Lambda와 함께하는 Redshift: Redshift는 AWS Lambda 코드를 지원하는 사용자 정의 함수(UDF)를 지원합니다. Lambda UDF는 Java, Go, PowerShell, Node.js, C#, Python 및 Ruby로 작성할 수 있습니다.\n\n# 마무리\n\n조금 긴 글이었죠? 이 글이 제 가장 긴 글입니다.\n\n<div class=\"content-ad\"></div>\n\n제목에 언급된 \"8시간\"에도 불구하고, 이 블로그를 마무리하는 데 거의 일주일이 걸렸어요. Redshift 논문은 많은 새로운 내용을 제공해서 조금 힘들었죠. 게다가, 이번에는 코드 특화 접근 방식을 선택한 OLAP 시스템을 연구한 것이 처음이라서 (이전에 공부한 시스템들은 모두 벡터화를 사용했어요: BigQuery, Snowflake, DuckDB).\n\n코드 특화 외에도, Redshift의 주요 기능으로는 컴파일 서비스, Redshift 관리 스토리지, 그리고 데이터베이스 운영을 위해 머신 러닝을 적용한 것이 언급할만해요. 게다가, Redshift의 웜 풀 아키텍처는 Snowflake의 사전 웜 풀과 유사한데, 두 솔루션이 모두 컴퓨팅 스케일링의 지연 시간을 최소화하려고 노력하지만, Redshift의 경우는 머신 러닝 모델을 활용해서 작동한다는 점이 다르죠.\n\n지금은 Redshift가 백그라운드 작업에 머신 러닝을 명시적으로 사용한다는 유일한 시스템인 것으로 보여요. Snowflake나 BigQuery는 이를 언급하지 않았죠. (제가 놓친 부분이 있다면 알려주세요)\n\n이제 이만 헤어집니다. 다음 주에 또 만나요!\n\n<div class=\"content-ad\"></div>\n\n# 참고 자료\n\n논문: Amazon Redshift Re-invented — 2022\n\n문서: Amazon Redshift 공식 문서\n\nPowerPoint 프레젠테이션: Amazon Redshift에 대한 심층 학습 및 모범 사례\n\n<div class=\"content-ad\"></div>\n\n내 뉴스레터는 매주 블로그 형식의 이메일로, 내가 더 똑똑한 사람들로부터 배운 것들을 정리해 두는 공간입니다.\n\n그러니 나와 함께 배우고 성장하고 싶다면 여기에서 구독해 주세요: https://vutr.substack.com.\n","ogImage":{"url":"/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_0.png"},"coverImage":"/assets/img/2024-05-23-Ispentanother8hoursunderstandingthedesignofAmazonRedshiftHereswhatIfound_0.png","tag":["Tech"],"readingTime":19},{"title":"유튜브에서 배우는 LLMs 사용하기","description":"","date":"2024-05-23 13:47","slug":"2024-05-23-UsingLLMstoLearnFromYouTube","content":"\n\n![Using LLMs to Learn From YouTube](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_0.png)\n\n# 소개\n\n팟캐스트나 시청하고 싶은 비디오를 만나본 적이 있나요? 하지만 길이 때문에 시간을 내기 어려워한 적이 있나요? 이러한 형태의 내용의 특정 부분을 다시 참조할 수 있는 쉬운 방법을 바란 적이 있나요?\n\n저는 The Diary of a CEO와 같은 인기 팟캐스트의 YouTube 비디오들에 관해 많은 시간을 할애하기 어려운 문제에 직면해왔습니다. 사실 이러한 팟캐스트에서 다루는 많은 정보들은 빠른 구글 검색을 통해 손쉽게 찾을 수 있습니다. 그러나 저자가 열정적으로 어떤 것에 대한 견해를 표현하거나 성공한 기업가의 경험을 그들의 관점에서 듣는 것은 훨씬 더 통찰력과 명확함을 제공합니다.\n\n\n<div class=\"content-ad\"></div>\n\n해당 문제로 동기부여 받으면서 LLM(언어 모델)을 기반으로 한 애플리케이션 및 개발에 대해 배우고 싶어졌습니다. 그래서 YouTube 동영상의 내용에 관한 질문을 할 수 있는 챗봇을 구축하기로 결정했습니다. 이 프레임워크인 RAG(Retrieval Augmented Generation)를 사용했습니다. 이후에, 이 어플리케이션을 LangChain, Pinecone, Flask, React를 사용하여 개발하고 AWS에 배포하는 경험에 대해 이야기하겠습니다:\n\n# 백엔드\n\nLLM이 사용자 정의 질문에 답변을 생성하는 소스로 YouTube 동영상의 대본을 사용할 것입니다. 이를 용이하게 하기 위해, 백엔드는 이를 실시간으로 검색하고 적절히 저장하여 사용할 수 있는 방법과 답변 생성에 사용할 수 있게 하는 방법이 필요합니다. 또한 사용자가 나중에 참조할 수 있도록 채팅 기록을 저장하는 방법도 있으면 좋겠습니다. 이제 이러한 요구 사항을 모두 충족시키기 위해 백엔드를 개발하는 방법에 대해 살펴보겠습니다.\n\n## 응답 생성\n\n<div class=\"content-ad\"></div>\n\n이 대화형 질문 응답 도구는 관련 컨텍스트와 채팅 기록을 모두 고려하여 질문에 대한 응답을 생성할 수 있어야 합니다. 이는 아래에 설명된대로 대화 기억을 갖춘 데이터 검색 확장 생성을 사용하여 달성할 수 있습니다:\n\n![image](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_1.png)\n\n불분명한 부분을 명확히 하기 위해 다음과 같은 단계가 포함됩니다:\n\n- 질문 요약: 현재 질문과 채팅 기록을 적절한 프롬프트를 사용하여 단독 질문으로 요약합니다. 이를 위해 LLM에게 요청합니다.\n- 의미 검색: 다음으로, 이러한 축약된 질문과 가장 관련성 있는 YouTube 대본 청크를 검색해야 합니다. 대본 자체는 단어 및 구의 수치적 표현인 임베딩으로 저장되어 있습니다. 이 임베딩은 내용과 의미를 포착하는 임베딩 모델에 의해 학습되었습니다. 의미 검색 중에는 임베딩이 축약된 질문의 임베딩과 가장 유사한 각 대본 구성 요소를 검색합니다.\n- 컨텍스트 인식 생성: 이러한 검색된 대본 청크는 다시 LLM에게 다른 프롬프트로 사용되어 축약된 질문에 대답하도록 요청됩니다. 축약된 질문을 사용하면 생성된 답변이 현재 질문 및 채팅 중 사용자가 이전에 물었던 질문과 관련이 있는지 보장됩니다.\n\n<div class=\"content-ad\"></div>\n\n## 데이터 파이프라인\n\n이미 설명된 프로세스의 구현 단계에 들어가기 전에, YouTube 비디오 트랜스크립트 자체에 집중해 보겠습니다. 언급했듯이, RAG 프로세스의 시맨틱 검색 단계에서 효율적으로 검색하고 검색될 수 있도록 이들을 임베딩으로 저장해야 합니다. 이제 이를 위한 소스, 검색 방법 및 저장 방법을 살펴보겠습니다.\n\n- 소스: YouTube는 비디오 ID 및 자동 생성된 트랜스크립트와 같은 메타데이터에 액세스할 수 있도록 Data API를 제공합니다. 첫 번째로, 다양한 돈 전문가와 기업가가 개인 재무, 투자, 성공적인 비즈니스 구축에 대해 논의하는 The Diary of a CEO 팟캐스트 플레이리스트를 선택했습니다.\n- 검색: YouTube 비디오의 비디오 ID와 같은 메타데이터를 검색하는 데 책임 있는 한 클래스와, youtube-transcript-API Python 패키지를 사용하여 비디오 트랜스크립트를 검색하는 데 책임 있는 다른 클래스를 활용합니다. 이 트랜스크립트는 그들의 원시 형태로 JSON 파일로 저장됩니다.\n- 저장: 그런 다음, 트랜스크립트를 임베딩으로 변환하고 벡터 데이터베이스에 저장해야 합니다. 그러나 이 단계의 사전 조건은 각 질문에 가장 관련성 높은 텍스트 세그먼트를 얻는 동시에 LLM 프롬프트의 길이를 최소화하기 위해 이를 청크로 분할해야 한다는 것입니다. 이 요구 사항을 충족시키기 위해 본 요구 사항을 만족시키기 위해 본 사용자 정의 S3JsonFileLoader 클래스를 정의하고(상자 밖의 문제로 인해 일부 문제가 발생함), 텍스트 분할 객체를 사용하여 트랜스크립트를 로드할 때 분할합니다. 그런 다음, 오픈AI의 gpt-3.5-turbo 모델이 예상하는 임베딩으로 트랜스크립트 청크를 저장하기 위해 LangChain의 인터페이스를 이용하여 선택한 Vectorstore인 Pinecone Vectorstore에 연결합니다:\n\n```js\nimport os\n\nimport pinecone\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Pinecone\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom chatytt.embeddings.s3_json_document_loader import S3JsonFileLoader\n\n# 트랜스크립트를 청크로 나누기 위한 스플리터 정의\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=pre_processing_conf[\"recursive_character_splitting\"][\"chunk_size\"],\n    chunk_overlap=pre_processing_conf[\"recursive_character_splitting\"][\n        \"chunk_overlap\"\n    ],\n)\n\n# 트랜스크립트 로드 및 분할\nloader = S3JsonFileLoader(\n    bucket=\"s3_bucket_name\",\n    key=\"transcript_file_name\",\n    text_splitter=text_splitter,\n)\ntranscript_chunks = loader.load(split_doc=True)\n\n# 관련 Pinecone 벡터스토어에 연결\npinecone.init(\n    api_key=os.environ.get(\"PINECONE_API_KEY\"),\n    environment=\"gcp-starter\"\n)\npinecone_index = pinecone.Index(os.environ.get(\"INDEX_NAME\"), pool_threads=4)\n\n# Pinecone에 트랜스크립트 청크를 임베딩으로 저장\nvector_store = Pinecone(\n    index=pinecone_index,\n    embedding=OpenAIEmbeddings(),\n    text_key=\"text\",\n)\nvector_store.add_documents(documents=transcript_chunks)\n```\n\n<div class=\"content-ad\"></div>\n\n우리는 주기적으로 실행되도록 구성된 워크플로를 이용하여 이러한 단계를 자동화하기 위해 몇 가지 AWS 서비스를 활용할 수도 있습니다. 저는 위에서 언급한 세 가지 단계마다 별도의 AWS Lamba 함수(필요한 리소스를 런타임에 필요에 따라 프로비저닝 및 활용하는 서버리스 컴퓨팅 형태)를 구현하고, 이러한 함수의 실행 순서를 AWS Step Functions(서버리스 오케스트레이션 도구)를 사용하여 정의합니다. 그런 다음, 이 워크플로는 매주 한 번 실행되도록 설정한 Amazon EventBridge 일정에 의해 실행됩니다:\n\n![이미지](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_2.png)\n\n## RAG 구현\n\n이제 우리가 선택한 재생 목록의 대본이 주기적으로 검색되고 임베딩으로 변환되어 저장되고 있는데, 이제 응용 프로그램의 핵심 백엔드 기능 구현으로 이동할 수 있습니다. 즉, 사용자 정의 질문에 대한 답변을 생성하는 프로세스를 구현해야 합니다. 다행히 LangChain은 이 작업을 완벽하게 수행하는 ConversationalRetrievalChain을 기본 제공합니다! 필요한 것은 쿼리, 채팅 기록, 대본 청크를 검색하는 데 사용할 수 있는 벡터 저장소 개체 및 선택한 LLM을 이 체인에 전달하는 것뿐입니다:\n\n<div class=\"content-ad\"></div>\n\n```js\nimport pinecone\nfrom langchain.vectorstores import Pinecone\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# 세맨틱 검색을 수행할 벡터 저장소를 정의합니다. 이를 위해 Pinecone 벡터 데이터베이스에 대해 세맨틱 검색을 수행합니다.\npinecone.init(\n    api_key=os.environ.get(\"PINECONE_API_KEY\"), environment=\"gcp-starter\"\n)\npinecone_index = pinecone.Index(os.environ.get(\"INDEX_NAME\"), pool_threads=4)\nvector_store = Pinecone(\n    index=pinecone_index,\n    embedding=OpenAIEmbeddings(),\n    text_key=\"text\",\n)\n\n# RAG에서 대화 기억을 갖고 있는 단계를 수행할 검색 연쇄를 정의합니다.\nchain = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(), retriever=vector_store.as_retriever()\n)\n\n# 현재 질문과 채팅 이력을 전달하여 체인을 호출합니다.\nresponse = chain({\"question\": query, \"chat_history\": chat_history})[\"answer\"]\n```\n\n## 채팅 이력 저장\n\n백엔드는 이제 질문에 대한 답변을 생성할 수 있지만 사용자가 이전 채팅 내용을 참조할 수 있도록 채팅 이력을 저장하고 검색할 수도 있으면 좋을 것입니다. 이는 동일한 항목에 대한 다른 사용자의 액세스 패턴을 알려진 대로 처리하는 NoSQL 데이터베이스인 DynamoDB를 사용하기로 결정했습니다. 이 데이터베이스는 이러한 형식의 비구조적 데이터를 처리하는 빠른 속도와 비용 효율성으로 알려져 있습니다. 추가로 boto3 SDK는 데이터베이스와 상호 작용을 단순화하여 저장 및 검색에 몇 가지 함수만 필요합니다:\n\n```js\nimport os\nimport time\nfrom typing import List, Any\n\nimport boto3\n\ntable = boto3.resource(\"dynamodb\").Table(os.environ.get(\"CHAT_HISTORY_TABLE_NAME\"))\n\ndef fetch_chat_history(user_id: str) -> str:\n    response = table.get_item(Key={\"UserId\": user_id})\n    return response[\"Item\"]\n\n\ndef update_chat_history(user_id: str, chat_history: List[dict[str, Any]]):\n    chat_history_update_data = {\n        \"UpdatedTimestamp\": {\"Value\": int(time.time()), \"Action\": \"PUT\"},\n        \"ChatHistory\": {\"Value\": chat_history, \"Action\": \"PUT\"},\n    }\n    table.update_item(\n        Key={\"UserId\": user_id}, AttributeUpdates=chat_history_update_data\n    )\n\n\ndef is_new_user(user_id: str) -> bool:\n    response = table.get_item(Key={\"UserId\": user_id})\n    return response.get(\"Item\") is None\n\n\ndef create_chat_history(user_id: str, chat_history: List[dict[str, Any]]):\n    item = {\n        \"UserId\": user_id,\n        \"CreatedTimestamp\": int(time.time()),\n        \"UpdatedTimestamp\": None,\n        \"ChatHistory\": chat_history,\n    }\n    table.put_item(Item=item)\n```\n\n<div class=\"content-ad\"></div>\n\n## API를 통해 로직 노출하기\n\n이제 모든 핵심 기능을 다루었지만 사용자가 상호작용하는 앱의 클라이언트 측은 이러한 프로세스를 트리거하고 활용하는 방법이 필요합니다. 이를 용이하게하기 위해 Flask API 내의 각 로직 조각(응답 생성, 채팅 기록 저장, 채팅 기록 검색)은 개별 엔드포인트를 통해 노출되며, 프론트 엔드에서 호출될 것입니다:\n\n```js\nfrom dotenv import load_dotenv\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\n\nfrom chatytt.chains.standard import ConversationalQAChain\nfrom chatytt.vector_store.pinecone_db import PineconeDB\nfrom server.utils.chat import parse_chat_history\nfrom server.utils.dynamodb import (\n    is_new_user,\n    fetch_chat_history,\n    create_chat_history,\n    update_chat_history,\n)\n\nload_dotenv()\napp = Flask(__name__)\n\n# 서버와 클라이언트가 각각 호스팅되므로 Cross Origin Resource Sharing을 활성화\nCORS(app)\n\npinecone_db = PineconeDB(index_name=\"youtube-transcripts\", embedding_source=\"open-ai\")\nchain = ConversationalQAChain(vector_store=pinecone_db.vector_store)\n\n@app.route(\"/get-query-response/\", methods=[\"POST\"])\ndef get_query_response():\n    data = request.get_json()\n    query = data[\"query\"]\n\n    raw_chat_history = data[\"chatHistory\"]\n    chat_history = parse_chat_history(raw_chat_history)\n    response = chain.get_response(query=query, chat_history=chat_history)\n\n    return jsonify({\"response\": response})\n\n\n@app.route(\"/get-chat-history/\", methods=[\"GET\"])\ndef get_chat_history():\n    user_id = request.args.get(\"userId\")\n\n    if is_new_user(user_id):\n        response = {\"chatHistory\": []}\n        return jsonify({\"response\": response})\n\n    response = {\"chatHistory\": fetch_chat_history(user_id=user_id)[\"ChatHistory\"]}\n\n    return jsonify({\"response\": response})\n\n\n@app.route(\"/save-chat-history/\", methods=[\"PUT\"])\ndef save_chat_history():\n    data = request.get_json()\n    user_id = data[\"userId\"]\n\n    if is_new_user(user_id):\n        create_chat_history(user_id=user_id, chat_history=data[\"chatHistory\"])\n    else:\n        update_chat_history(user_id=user_id, chat_history=data[\"chatHistory\"])\n\n    return jsonify({\"response\": \"chat saved\"})\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=8080)\n```\n\n마지막으로, 세 개의 엔드포인트를 하나의 함수로 래핑하여 AWS Lambda를 사용하고, API Gateway 리소스에 의해 트리거되는 방식으로 요청을 올바른 엔드포인트로 보내도록 하는 방법을 설명합니다. 이제 이 설정의 흐름은 다음과 같습니다:\n\n<div class=\"content-ad\"></div>\n\n아래는 이 전체에 사용되는 각 섹션에 대한 전용 기능 컴포넌트를 활용하여 채팅 봇 애플리케이션에서 기대할 수 있는 모든 일반 요구 사항을 다룹니다:\n\n<div class=\"content-ad\"></div>\n\n- 사용자 입력을 보내는 채팅 버튼이 있는 컨테이너입니다.\n- 사용자 입력 및 답변이 표시되는 채팅 피드가 있습니다.\n- 채팅 기록, 새로운 채팅 버튼 및 채팅 저장 버튼이 있는 사이드바가 있습니다.\n\n이 컴포넌트들 간의 상호작용과 데이터 흐름은 아래와 같이 설명됩니다:\n\n![Components Interaction](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_4.png)\n\n세 가지 엔드포인트로의 API 호출 및 클라이언트 측에서 관련 변수의 상태 변경은 각각의 기능 컴포넌트에서 정의됩니다:\n\n<div class=\"content-ad\"></div>\n\n- 질문마다 생성된 답변을 가져오는 논리:\n\n```js\nimport React from \"react\";\nimport {chatItem} from \"./LiveChatFeed\";\n\ninterface Props {\n    setCurrentChat: React.SetStateAction<any>\n    userInput: string\n    currentChat: Array<chatItem>\n    setUserInput: React.SetStateAction<any>\n}\n\nfunction getCurrentChat({setCurrentChat, userInput, currentChat, setUserInput}: Props){\n    // 현재 채팅은 라이브 채팅 피드에 표시됩니다. LLM 응답을 기다리기 전에\n    // 사용자 질문을 제공하기 위해 API에 대답을 요청하기 전에 복사하여\n    // 별도 변수에 전달하고 현재 채팅에 추가합니다.\n    const userInputText = userInput\n    setUserInput(\"\")\n    setCurrentChat([\n        ...currentChat,\n        {\n            \"text\": userInputText,\n            isBot: false\n        }\n    ])\n\n    // 포스트 요청을 위한 API 페이로드 생성\n    const options = {\n        method: 'POST',\n        headers: {\n            \"Content-Type\": 'application/json',\n            'Accept': 'application/json'\n        },\n        body: JSON.stringify({\n            query: userInputText,\n            chatHistory: currentChat\n        })\n    }\n\n    // 엔드포인트에 핑을 보내 응답을 기다린 후 현재 채팅에 추가하여\n    // 라이브 채팅 피드에 표시\n    fetch(`${import.meta.env.VITE_ENDPOINT}get-query-response/`, options).then(\n        (response) => response.json()\n    ).then(\n        (data) => {\n            setCurrentChat([\n                ...currentChat,\n                {\n                    \"text\": userInputText,\n                    \"isBot\": false\n                },\n                {\n                    \"text\": data.response,\n                    \"isBot\": true\n                }\n            ])\n        }\n    )\n}\n\nexport default getCurrentChat\n```\n\n2. 사용자가 채팅 저장 버튼을 클릭할 때 채팅 기록을 저장하는 방법:\n\n```js\nimport React, {useState} from \"react\";\nimport {chatItem} from \"./LiveChatFeed\";\nimport saveIcon from \"../assets/saveicon.png\"\nimport tickIcon from \"../assets/tickicon.png\"\n\ninterface Props {\n    userId: String\n    previousChats: Array<Array<chatItem>>\n}\n\nfunction SaveChatHistoryButton({userId, previousChats}: Props){\n    // 현재 채팅이 저장되었는지 여부를 결정하는 상태 정의합니다.\n    const [isChatSaved, setIsChatSaved] = useState(false)\n\n    // 채팅 기록을 저장하는 PUT 요청 페이로드 생성\n    const saveChatHistory = () => {\n        const options = {\n            method: 'PUT',\n            headers: {\n                \"Content-Type\": 'application/json',\n                'Accept': 'application/json'\n            },\n            body: JSON.stringify({\n                \"userId\": userId,\n                \"chatHistory\": previousChats\n            })\n        }\n\n        // 채팅 기록이 성공적으로 저장되면 API에 핑 보내고\n        // isChatSaved 상태를 true로 설정합니다\n        fetch(`${import.meta.env.VITE_ENDPOINT}save-chat-history/`, options).then(\n            (response) => response.json()\n        ).then(\n            (data) => {\n                setIsChatSaved(true)\n            }\n        )\n    }\n\n    // isChatSaved 상태 값에 따라 저장된 채팅 버튼에 동적으로 텍스트 표시\n    return (\n        <button\n            className=\"save-chat-history-button\"\n            onClick={() => {saveChatHistory()}\n        > <img className={isChatSaved?\"tick-icon-img\":\"save-icon-img\"} src={isChatSaved?tickIcon:saveIcon}/>\n        {isChatSaved?\"Chats Saved\":\"Save Chat History\"}\n        </button>\n    )\n}\n\nexport default SaveChatHistoryButton\n```\n\n<div class=\"content-ad\"></div>\n\n3. 앱이 처음으로 로드될 때 채팅 기록을 검색합니다:\n\n```js\nimport React from \"react\";\nimport {chatItem} from \"./LiveChatFeed\";\n\ninterface Props {\n    userId: String\n    previousChats: Array<Array<chatItem>>\n    setPreviousChats: React.SetStateAction<any>\n}\n\nfunction getUserChatHistory({userId, previousChats, setPreviousChats}: Props){\n    // GET 요청을 위한 페이로드 생성\n    const options = {\n            method: 'GET',\n            headers: {\n                \"Content-Type\": 'application/json',\n                'Accept': 'application/json'\n            }\n        }\n\n        // GET 요청이므로 사용자 ID를 쿼리 매개변수로 전달합니다.\n        // API에서 반환된 채팅 기록으로 이전 채팅 상태를 설정합니다.\n        fetch(`${import.meta.env.VITE_ENDPOINT}get-chat-history/?userId=${userId}`, options).then(\n            (response) => response.json()\n        ).then(\n            (data) => {\n            if (data.response.chatHistory.length > 0) {\n                setPreviousChats(\n                        [\n                            ...previousChats,\n                            ...data.response.chatHistory\n                        ]\n                    )\n                }\n            }\n        )\n}\n\nexport default getUserChatHistory\n```\n\nUI 자체에 대해, ChatGPT의 인터페이스와 매우 유사한 것을 선택했습니다. 중앙의 채팅 피드 구성 요소와 채팅 기록과 같은 지원 콘텐츠를 담은 사이드바가 있습니다. 사용자를 위한 편안함 기능으로는 가장 최근에 생성된 채팅 항목으로 자동 스크롤링 및 로그인시 이전 채팅이 로드되는 등이 있습니다. 최종 UI 모습은 아래와 같습니다:\n\n이제 완전히 기능하는 UI가 준비되었으니 온라인 사용을 위해 호스팅해야 합니다. 저는 AWS Amplify를 사용하여 이를 수행하기로 선택했습니다. Amplify는 리소스 프로비저닝 및 웹 애플리케이션 호스팅을 처리하는 완전 관리형 웹 호스팅 서비스 중 하나입니다. 앱의 사용자 인증은 Amazon Cognito에 의해 관리되며 사용자 회원가입, 로그인, 자격 증명 저장 및 관리를 제공합니다.\n\n<div class=\"content-ad\"></div>\n\n<img src=\"/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_5.png\" />\n\n# ChatGPT 응답과 비교\n\n앱을 구축하는 과정을 논의했으니, 몇 가지 질문에 대한 생성된 응답을 심도 있게 살펴보고, 이를 ChatGPT\\*에 제시된 동일한 질문과 비교해 보겠습니다.\n\n저희 애플리케이션에서 사용되는 LLM에 제시된 기본 프롬프트에는 시맨틱 검색 단계에서 검색된 추가 컨텍스트(관련 트랜스크립트 청킹 형태)가 포함될 것이기 때문에 이러한 비교는 본질적으로 \"부당\"한 것이라는 점에 유의하십시오. 그러나, RAG를 사용하여 생성된 프롬프트와 같은 기본 LLM에 의해 생성된 응답과 어떤 차이가 있는지 정성적으로 평가할 수 있게 해줄 것입니다.\n\n<div class=\"content-ad\"></div>\n\n\\*All ChatGPT responses are from gpt-3.5, since this was the model used in the application.\n\n## 예시 1:\n\nSteven Bartlett가 금융 작가이자 투자자인 Morgan Housel과의 대화를 나누는 이 비디오의 내용에 대해 알고 싶습니다. 비디오 제목을 보면 집 구입에 반대하는 것으로 보이지만, 시간이 없어서 전체 내용을 확인할 수 없다고 가정해 봅시다. 여기 저는 이에 대해 애플리케이션과의 대화 스니펫을 보여드립니다:\n\n<img src=\"/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_6.png\" />\n\n<div class=\"content-ad\"></div>\n\n표 태그를 Markdown 형식으로 변경해주세요.\n\n<div class=\"content-ad\"></div>\n\n## 예시 2\n\n아래는 수익화 및 인수 전문가인 Alex Hormozi와의 이 토론에 대한 채팅에서 일부 스니펫이 있습니다. 비디오 제목으로부터, 그는 비즈니스를 성공적으로 확장하는 데에 대해 잘 알고 있다는 것 같아, 그에 대해 더 많은 세부 정보를 요청했습니다:\n\n![이미지](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_8.png)\n\n이것은 합리적인 대답으로 보이지만, 동일한 질문 라인에서 더 많은 정보를 추출할 수 있는지 확인해 보겠습니다.\n\n<div class=\"content-ad\"></div>\n\n![LLM 이미지1](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_9.png)\n\n![LLM 이미지2](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_10.png)\n\nYouTube 트랜스크립트에서 LLM이 추출할 수 있는 세부 수준에 주목해보세요. 위의 내용은 비디오의 17:00-35:00 타임스탬프 주변 15~20분 부분에서 찾을 수 있습니다.\n\nChatGPT에 같은 질문을 한 결과, 기업가에 관한 일반적인 답변만 제공되었지만 비디오 트랜스크립트 내용을 통해 사용할 수 있는 세부 정보가 부족합니다.\n\n<div class=\"content-ad\"></div>\n\n\n![사용 예시 이미지](/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_11.png)\n\n# 배포\n\n마지막으로 논의할 주제는 AWS에서 각 구성 요소를 배포하는 과정입니다. 데이터 파이프라인, 백엔드 및 프론트엔드는 각각 자체 CloudFormation 스택(여러 AWS 리소스의 모음) 내에 포함되어 있습니다. 이렇게 각각을 격리된 상태에서 배포할 수 있도록 해줌으로써, 개발 중에 전체 앱이 불필요하게 다시 배포되지 않도록 합니다. 저는 AWS SAM (서버리스 애플리케이션 모델)을 사용하여 각 구성 요소에 대한 인프라를 코드로 배포하는데 활용하고 있습니다. SAM 템플릿 사양 및 CLI를 활용합니다:\n\n- SAM 템플릿 사양 - AWS CloudFormation을 확장하는 용도로 사용되는 간결한 구문으로, AWS 리소스의 모음을 정의하고 구성하는 데 사용됩니다. 리소스 간 상호 작용 방식 및 필요한 권한을 지정합니다.\n- SAM CLI - SAM 템플릿에 정의된 리소스를 빌드하고 배포하는 데 사용되는 명령줄 도구입니다. 애플리케이션 코드 및 종속성의 패키징, SAM 템플릿을 CloudFormation 구문으로 변환하고 CloudFormation에서 개별 스택으로 템플릿을 배포하는 작업을 처리합니다.\n\n\n<div class=\"content-ad\"></div>\n\n위 글 전체 템플릿(자원 정의)을 모두 포함하는 대신, 우리가 이야기한 각 서비스에 대해 특정 관심 영역을 강조하겠습니다.\n\nAWS 리소스에 민감한 환경 변수 전달하기:\n\n유튜브 데이터 API, OpenAI API, Pinecone API와 같은 외부 구성 요소는 애플리케이션 전체에서 많이 의존합니다. 이러한 값을 CloudFormation 템플릿에 하드코딩하고 '매개변수'로 전달하는 것이 가능하지만, 더 안전한 방법은 AWS SecretsManager에서 각각의 비밀을 만들고 다음과 같이 해당 템플릿에서 이 비밀을 참조하는 것입니다:\n\n```js\nParameters: YoutubeDataAPIKey: Type: String;\nDefault: \"{resolve:secretsmanager:youtube-data-api-key:SecretString:youtube-data-api-key}\";\nPineconeAPIKey: Type: String;\nDefault: \"{resolve:secretsmanager:pinecone-api-key:SecretString:pinecone-api-key}\";\nOpenaiAPIKey: Type: String;\nDefault: \"{resolve:secretsmanager:openai-api-key:SecretString:openai-api-key}\";\n```\n\n<div class=\"content-ad\"></div>\n\n람다 함수 정의:\n\n이 서버리스 코드 단위들은 데이터 파이프라인의 중심을 형성하며 웹 애플리케이션의 백엔드로의 진입점 역할을 합니다. SAM을 사용하여 이들을 배포하는 것은 호출될 때 함수가 실행해야 할 코드의 경로를 정의하는 것과 필요한 권한 및 환경 변수를 함께 지정하는 것만으로 간단합니다. 다음은 데이터 파이프라인에서 사용되는 함수 중 하나의 예시입니다:\n\n```js\nFetchLatestVideoIDsFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: ../code_uri/.\n      Handler: chatytt.youtube_data.lambda_handlers.fetch_latest_video_ids.lambda_handler\n      Policies:\n        - AmazonS3FullAccess\n      Environment:\n        Variables:\n          PLAYLIST_NAME:\n            Ref: PlaylistName\n          YOUTUBE_DATA_API_KEY:\n            Ref: YoutubeDataAPIKey\n```\n\nAmazon States 언어로 데이터 파이프라인의 정의를 검색하는 것:\n\n<div class=\"content-ad\"></div>\n\n개별 람다 함수에 Step Functions을 오케스트레이터로 사용하려면, Amazon States Language에서 각 함수가 실행되어야 하는 순서 및 최대 재시도 횟수와 같은 구성을 정의해야 합니다. 이 작업을 간단하게 수행하는 방법은 Step Functions 콘솔의 Workflow Studio를 사용하여 워크플로우를 다이어그램으로 만든 다음, 해당 워크플로우의 자동 생성된 ASL 정의를 적절히 수정할 수 있는 시작점으로 활용하는 것입니다. 그러면 이를 CloudFormation 템플릿에 링크하여 해당 위치에 정의하는 대신에 사용할 수 있습니다:\n\n```js\nEmbeddingRetrieverStateMachine:\n  Type: AWS::Serverless::StateMachine\n  Properties:\n    DefinitionUri: statemachine/embedding_retriever.asl.json\n    DefinitionSubstitutions:\n      FetchLatestVideoIDsFunctionArn: !GetAtt FetchLatestVideoIDsFunction.Arn\n      FetchLatestVideoTranscriptsArn: !GetAtt FetchLatestVideoTranscripts.Arn\n      FetchLatestTranscriptEmbeddingsArn: !GetAtt FetchLatestTranscriptEmbeddings.Arn\n    Events:\n      WeeklySchedule:\n        Type: Schedule\n        Properties:\n          Description: Schedule to run the workflow once per week on a Monday.\n          Enabled: true\n          Schedule: cron(0 3 ? * 1 *)\n    Policies:\n    - LambdaInvokePolicy:\n        FunctionName: !Ref FetchLatestVideoIDsFunction\n    - LambdaInvokePolicy:\n        FunctionName: !Ref FetchLatestVideoTranscripts\n    - LambdaInvokePolicy:\n        FunctionName: !Ref FetchLatestTranscriptEmbeddings\n```\n\n이 포스트에서 논의한 데이터 파이프라인을 위해 사용된 ASL 정의는 여기에서 확인할 수 있습니다.\n\nAPI 리소스 정의:\n\n<div class=\"content-ad\"></div>\n\n웹 애플리케이션용 API가 프론트엔드와 별도로 호스팅될 예정이므로 API 리소스를 정의할 때 CORS (Cross-Origin Resource Sharing) 지원을 활성화해야 합니다:\n\n```js\nChatYTTApi: Type: AWS::Serverless::Api;\nProperties: StageName: Prod;\nCors: AllowMethods: \"'*'\";\nAllowHeaders: \"'*'\";\nAllowOrigin: \"'*'\";\n```\n\n위 설정을 통해 두 리소스가 서로 자유롭게 통신할 수 있게 됩니다. 람다 함수를 통해 액세스할 수 있는 여러 엔드포인트는 다음과 같이 정의할 수 있습니다:\n\n```js\nChatResponseFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Runtime: python3.9\n      Timeout: 120\n      CodeUri: ../code_uri/.\n      Handler: server.lambda_handler.lambda_handler\n      Policies:\n        - AmazonDynamoDBFullAccess\n      MemorySize: 512\n      Architectures:\n        - x86_64\n      Environment:\n        Variables:\n          PINECONE_API_KEY:\n            Ref: PineconeAPIKey\n          OPENAI_API_KEY:\n            Ref: OpenaiAPIKey\n      Events:\n        GetQueryResponse:\n          Type: Api\n          Properties:\n            RestApiId: !Ref ChatYTTApi\n            Path: /get-query-response/\n            Method: post\n        GetChatHistory:\n          Type: Api\n          Properties:\n            RestApiId: !Ref ChatYTTApi\n            Path: /get-chat-history/\n            Method: get\n        UpdateChatHistory:\n          Type: Api\n          Properties:\n            RestApiId: !Ref ChatYTTApi\n            Path: /save-chat-history/\n            Method: put\n```\n\n<div class=\"content-ad\"></div>\n\n리액트 앱 리소스를 정의하는 방법:\n\nAWS Amplify를 사용하면 관련 Github 저장소에 대한 참조와 적절한 액세스 토큰을 사용하여 애플리케이션을 빌드하고 배포할 수 있습니다:\n\n```js\nAmplifyApp:\n    Type: AWS::Amplify::App\n    Properties:\n      Name: amplify-chatytt-client\n      Repository: <https://github.com/suresha97/ChatYTT>\n      AccessToken: '{resolve:secretsmanager:github-token:SecretString:github-token}'\n      IAMServiceRole: !GetAtt AmplifyRole.Arn\n      EnvironmentVariables:\n        - Name: ENDPOINT\n          Value: !ImportValue 'chatytt-api-ChatYTTAPIURL'\n```\n\n한 번 저장소 자체에 액세스할 수 있으면, Amplify는 앱을 구축하고 배포하는 방법에 대한 지침이 포함된 구성 파일을 찾습니다:\n\n<div class=\"content-ad\"></div>\n\n```yaml\nversion: 1\nfrontend:\n  phases:\n    preBuild:\n      commands:\n        - cd client\n        - npm ci\n    build:\n      commands:\n        - echo \"VITE_ENDPOINT=$ENDPOINT\" >> .env\n        - npm run build\n  artifacts:\n    baseDirectory: ./client/dist\n    files:\n      - \"**/*\"\n  cache:\n    paths:\n      - node_modules/**/*\n```\n\n덤으로, 계속적인 배포 프로세스를 자동화하기 위해 모니터링할 브랜치 리소스를 정의하여 추가 커밋이 발생할 때 앱을 자동으로 재배포할 수도 있습니다:\n\n```yaml\nAmplifyBranch:\n  Type: AWS::Amplify::Branch\n  Properties:\n    BranchName: main\n    AppId: !GetAtt AmplifyApp.AppId\n    EnableAutoBuild: true\n```\n\n이렇게 최종 배포가 완료되면, AWS Amplify 콘솔에서 제공된 링크를 통해 누구에게나 접근 가능합니다. 이렇게 접근한 앱의 데모 기록은 다음에서 확인할 수 있습니다:```\n\n<div class=\"content-ad\"></div>\n\n# 결론\n\n높은 수준에서 다음 단계를 다뤘습니다:\n\n- 콘텐츠 수집 및 저장을 위한 데이터 파이프라인 구축.\n- 대화 기억을 활용한 검색 증강 생성을 수행하는 백엔드 서버 구성.\n- 생성된 답변 및 채팅 기록을 제공하는 사용자 인터페이스 설계.\n- 이러한 구성 요소를 연결하고 배포하여 가치를 제공하고 시간을 절약하는 솔루션을 생성하는 방법.\n\n이러한 응용프로그램이 학습 및 개발 목적의 YouTube 비디오 소비를 최적화하고 어떤 방식으로든 간소화할 수 있는 방법을 알아보았습니다. 그러나 이러한 방법은 동일하게 직장에서 내부 사용이나 고객을 위한 솔루션을 확장하는 데 쉽게 적용할 수 있습니다. 이것이 LLMs의 인기와 특히 RAG 기술이 많은 조직에서 큰 주목을 받는 이유입니다.\n\n<div class=\"content-ad\"></div>\n\n마크다운 형식으로 표 태그를 변경해보세요.\n\n# 감사의 글\n\nDiary of a CEO 팀에게 이 프로젝트와 이 글 작성 중에 이 플레이리스트의 비디오 대본을 사용할 수 있는 허락을 받아 감사의 말씀을 전합니다.\n\n모든 이미지는 별도 명시가 없는 한 저자가 찍은 것입니다.\n","ogImage":{"url":"/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_0.png"},"coverImage":"/assets/img/2024-05-23-UsingLLMstoLearnFromYouTube_0.png","tag":["Tech"],"readingTime":23}],"page":"21","totalPageCount":68,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true}