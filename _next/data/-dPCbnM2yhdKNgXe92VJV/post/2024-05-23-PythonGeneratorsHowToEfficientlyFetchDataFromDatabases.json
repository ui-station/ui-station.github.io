{"pageProps":{"post":{"title":"파이썬 제너레이터 데이터베이스에서 효율적으로 데이터를 가져오는 방법","description":"","date":"2024-05-23 14:16","slug":"2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases","content":"\n![image](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png)\n\n# 온디맨드 코스 | 추천\n\n몇몇 독자들이 데이터 엔지니어로 성장하는 데 도움이 될 온디맨드 코스를 요청했습니다. 제가 추천하는 3가지 좋은 자원은 다음과 같습니다:\n\n- 데이터 엔지니어링 나노디그리 (UDACITY)\n- 아파치 카프카 & 아파치 스파크를 이용한 데이터 스트리밍 나노디그리 (UDACITY)\n- 파이스파크를 이용한 스파크 및 파이썬 빅데이터 과정 (UDEMY)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아직 Medium 회원이 아니신가요? Medium이 제공하는 모든 것에 액세스하려면 매월 $5로 가입하는 것을 고려해보세요!\n\n# 소개\n\n데이터 엔지니어로써 우리는 종종 운영 데이터베이스에서 특히 큰 데이터 집합을 가져와서 일련의 변환을 수행한 후에 분석 데이터베이스나 S3 버킷과 같은 클라우드 객체 저장소에 기록해야 하는 상황에 직면합니다.\n\n이 경우 Airflow 인스턴스에서 사용 가능한 메모리의 큰 부분을 사용하여 데이터 팀의 다른 동료들의 작업에 영향을 주지 않고 작업을 수행하는 방법을 찾아야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬 생성기가 메모리 피크를 피하면서 데이터베이스에서 데이터를 효율적으로 가져올 때 사용될 수 있는 좋은 옵션이 될 수 있습니다.\n\n실제로, 본 자습서에서는 생성기를 사용하는 것이 데이터 엔지니어에게 현명한 접근 방식인 두 가지 실용적인 사용 사례를 살펴볼 것입니다. 이를 위해 Docker 컨테이너를 구동하여 실제 엔드 투 엔드 데이터 워크플로를 시뮬레이션하기 위해 세 가지 서비스(포스트그레스 데이터베이스, 주피터 노트북 및 MinIO)를 실행할 것입니다.\n\n# 파이썬에서 생성기의 장점\n\n파이썬에서 표준 함수는 단일 값 계산 후 종료되지만, 생성기는 필요에 따라 일시 중지하고 다시 시작하면서 시간이 지남에 따라 값 시퀀스를 생성할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제너레이터는 값을 시퀀스로 생성하기 위해 return 대신 yield 문을 사용하는 특별한 함수입니다. 값은 한 번에 하나씩 생성되며 전체 시퀀스를 메모리에 저장할 필요가 없습니다.\n\n제너레이터 함수가 호출되면 제너레이터에 의해 생성된 값의 시퀀스를 반복할 수 있는 이터레이터 객체가 반환됩니다.\n\n예를 들어, 0부터 입력 변수 n 사이의 숫자들의 제곱을 생성하는 squares_generator(n) 함수를 만들어 봅시다:\n\n```js\ndef squares_generator(n):\n  num = 0\n  while num < n:\n    yield num * num\n    num += 1\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n함수를 호출하면 이터레이터만 반환됩니다:\n\n```js\nsquares_generator(n)\n\n#출력:\n# <generator object squares_generator at 0x10653bdd0>\n```\n\n모든 값의 시퀀스를 가져오려면 제너레이터 함수를 루프 안에서 호출해야합니다:\n\n```js\nfor num in squares_generator(5):\n  print(num)\n\n#출력:\n0\n1\n4\n9\n16\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n더 효율적이고 세련된 옵션은 함수 대신 한 줄로 작성된 생성기 표현식을 만드는 것입니다:\n\n```js\nn = 5\ngenerator_exp = (num * num for num in range(n))\n```\n\n이제 값을 next() 메서드를 사용하여 직접 접근할 수 있습니다:\n\n```js\nprint(next(generator_exp)) # 0\nprint(next(generator_exp)) # 1\nprint(next(generator_exp)) # 4\nprint(next(generator_exp)) # 9\nprint(next(generator_exp)) # 16\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 볼 수 있듯이, 제너레이터 함수에서 값이 반환되는 방식은 일반적인 파이썬 함수와는 즉각적으로 직관적이지 않습니다. 아마도 그것이 많은 데이터 엔지니어들이 발생해야 할 정도로 제너레이터를 사용하지 않는 이유일 것입니다.\n\n다음 섹션에서 두 가지 일반적인 사용 사례를 설명해보겠습니다.\n\n# 목표 및 설정\n\n이 자습서의 목표는 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Postgres DB로부터 데이터를 가져와서 pandas 데이터프레임으로 저장합니다.\n- pandas 데이터프레임을 Parquet 형식으로 S3 버킷에 씁니다.\n\n각 목표는 일반 함수와 제너레이터 함수를 사용하여 모두 달성될 것입니다.\n\n이러한 워크플로우를 시뮬레이션하기 위해 세 가지 서비스가 있는 도커 컨테이너를 실행합니다:\n\n- Postgres DB = 데이터를 가져올 소스 운영 데이터베이스로 사용될 서비스입니다. Docker-compose가 mainDB를 생성하고 transactions이라는 테이블에 5백만 개의 모의 레코드를 삽입하는 작업을 수행합니다. 참고: 이 튜토리얼을 위한 자료를 준비하는 동안, 더 큰 데이터셋을 시뮬레이션하기 위해 5천만 개, 1억 개의 행을 시도해 보았지만 Docker 서비스의 성능에 영향을 미쳤습니다.\n- MinIO = AWS S3 버킷을 시뮬레이션하는 데 사용될 서비스로, awswrangler 패키지를 사용하여 pandas 데이터프레임을 Parquet 형식으로 쓸 때 도움이 될 것입니다.\n- Jupyter Notebook = 익숙한 컴파일러를 통해 Python 코드 조각을 대화식으로 실행하는 데 사용될 서비스입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금까지 설명한 내용을 시각적으로 보여주는 그래프입니다:\n\n![그래프](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_1.png)\n\n첫 번째 단계로는 프로젝트의 GitHub 리포지토리를 복제하고 해당 폴더로 이동합니다:\n\n```js\ngit clone git@github.com:anbento0490/projects.git &&\ncd fetch_data_with_python_generators\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러면 세 가지 서비스를 시작하는 도커 컴포즈를 실행할 수 있어요:\n\n```js\ndocker compose up -d\n\n[+] Running 5/5\n ⠿ Network shared-network                 Created                                                 0.0s\n ⠿ Container jupyter-notebooks            Started                                                 1.0s\n ⠿ Container minio                        Started                                                 0.7s\n ⠿ Container postgres-db                  Started                                                 0.9s\n ⠿ Container mc                           Started                                                 1.1s\n```\n\n最終적으로 확인할 수 있어요:\n\n- 포스트그레스 데이터베이스에 transactions 테이블이 생성되었고 5백만 개의 레코드가 포함되어 있어요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndocker exec -it postgres-db /bin/bash\n\nroot@9632469c70e0:/# psql -U postgres\n\npsql (13.13 (Debian 13.13-1.pgdg120+1))\n도움말을 보려면 \"help\"를 입력하세요.\n\npostgres=# \\c mainDB\n데이터베이스 \"mainDB\"에 사용자 \"postgres\"로 연결되었습니다.\n\nmainDB=# select count(*) from transactions;\n  count\n---------\n 5000000\n(1 로우)\n```\n\n- MinIO UI는 localhost:9001 포트에서 접속할 수 있습니다. 자격 증명을 요청 받을 때 (관리자 및 비밀번호를 입력)를 사용하고 generators-test-bucket이라는 빈 버킷이 생성되었습니다:\n\n![image](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_2.png)\n\n- Jupyter Notebook UI는 localhost:8889에서 접근할 수 있으며 아래에 토큰을 검색하여 액세스할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```bash\ndocker exec -it jupyter-notebooks /bin/bash\n\nroot@eae08d1f4bf6:~# jupyter server list\n\n현재 실행 중인 서버:\nhttp://eae08d1f4bf6:8888/?token=8a45d846d03cf0c0e4584c3b73af86ba5dk9e83c8ac47ee7 :: /home/jovyan\n```\n\n![Python Generators](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_3.png)\n\n좋아요! Jupyter에서 몇 가지 코드를 실행할 준비가 모두 끝났어요.\n\n하지만 그 전에 MinIO의 버킷과 상호 작용하려면 새로운 access_key와 secret_access_key를 생성해야 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_4.png\" />\n\n알림: MinIO 버킷의 가장 멋진 기능 중 하나는 AWS S3 버킷처럼 상호 작용할 수 있다는 것입니다 (예: boto3, awswrangler 등을 사용하여). 하지만 이러한 기능은 비용이 발생하지 않으며, 로컬 환경에만 존재하므로 비밀을 노출할 걱정이 없습니다. 컨테이너가 중지될 때까지 유지되지 않으므로 데이터가 계속 유지되지 않습니다.\n\n이제 생성기 노트북에서 다음 코드를 실행해 봅시다 (비밀 정보를 꼭 교체해주세요):\n\n```python\nimport psycopg2\nimport pandas as pd\nimport boto3\nimport awswrangler as wr\n\n#######################################################\n######## PG DB에 연결하고 커서 생성 #######\nconnection = psycopg2.connect(user=\"postgres\",\n                              password=\"postgres\",\n                              port=\"5432\",\n                              database=\"mainDB\")\ncursor = connection.cursor()\n\nquery = \"select * from transactions;\"\n\n#######################################################\n######## MINIO 버킷에 연결 ###################\n\nboto3.setup_default_session(aws_access_key_id='your_access_key',\n                            aws_secret_access_key='your_secret_key')\n\nbucket = 'generators-test-bucket'\nfolder_gen = 'data_gen'\nfolder_batch = 'data_batch'\nparquet_file_name = 'transactions'\nbatch_size = 1000000\n\nwr.config.s3_endpoint_url = 'http://minio:9000'\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 mainDB에 연결하고 쿼리를 실행하기 위한 커서를 만듭니다. 또한 generators-test-bucket와 상호 작용하기 위한 기본 세션이 설정됩니다.\n\n# 사용 사례 #1: 데이터베이스에서 읽기\n\n데이터 엔지니어로서 데이터베이스 또는 외부 서비스에서 대규모 데이터 세트를 Python 파이프라인으로 가져올 때, 다음 사항 사이의 균형을 찾아야 합니다:\n\n- 메모리: 한꺼번에 전체 데이터 세트를 가져오면 OOM 오류가 발생하거나 전체 인스턴스/클러스터의 성능에 영향을 줄 수 있습니다.\n- 속도: 행을 하나씩 가져오는 것도 비싼 I/O 네트워크 작업을 초래할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 방법 #1: 일괄적으로 데이터 가져오기\n\n실무에서 자주 사용하는 합리적인 절충안은 사용 가능한 메모리와 데이터 파이프라인의 속도 요구 사항에 따라 배치로 데이터를 가져오는 것입니다:\n\n```js\n# 1.1. 배치를 사용하여 DF 생성\ndef create_df_batch(cursor, batch_size):\n\n    print('생성 중...')\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n\n    df = pd.DataFrame(columns=colnames)\n    cursor.execute(query)\n\n    while True:\n        rows = cursor.fetchmany(batch_size)\n        if not rows:\n            break\n        # 일부 변환\n        batch_df = pd.DataFrame(data = rows, columns=colnames)\n        df = pd.concat([df, batch_df], ignore_index=True)\n\n    print('DF 생성 완료!\\n')\n\n    return df\n```\n\n위 코드는 다음을 수행합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 빈 df를 생성;\n- 쿼리를 실행하고 전체 결과를 커서 객체에 캐싱;\n- while 루프를 초기화하여 매 반복마다 지정된 배치 크기(이 경우 1백만 행)와 동일한 행 수를 가져와 이 데이터를 사용하여 배치\\_df를 생성합니다.\n- 최종적으로 배치\\_df가 주 df에 추가됩니다. 전체 데이터셋이 통과될 때까지 이 프로세스가 반복됩니다.\n\n분명히 말하자면, 이것은 기본적인 예시이며, 단순히 한 번에 한 배치씩 df를 생성하는 것 외에도 while 루프의 일부로 다른 많은 작업(필터링, 정렬, 집계, 데이터를 다른 위치로 쓰기 등)을 수행할 수 있었습니다.\n\n노트북에서 함수를 실행하면 다음과 같이 결과를 얻을 수 있습니다:\n\n```js\n%%time\ndf_batch = create_df_batch(cursor, batch_size)\ndf_batch.head()\n\n결과:\n\n생성 중...\nDF 생성 완료!\n\nCPU 시간: 사용자 9.97초, 시스템: 13.7초, 총: 23.7초\n실제 시간: 25초\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Python Generators](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_5.png)\n\n## Method #2: Using Generators\n\nA less common -but powerful- strategy for data engineers is to fetch data as a stream using generators:\n\n```python\n# AUXILIARY FUNCTION\ndef generate_dataset(cursor):\n\n    cursor.execute(query)\n\n    for row in cursor.fetchall():\n        # some transformation\n        yield row\n\n# 2.1. CREATE DF USING GENERATORS\ndef create_df_gen(cursor):\n    print('Creating pandas DF using generator...')\n\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n\n    df = pd.DataFrame(data=generate_dataset(cursor), columns=colnames)\n\n    print('DF successfully created!\\n')\n\n    return df\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 코드 스니펫에서는 쿼리를 실행하고 행을 시퀀스로 반환하는 'generate_dataset' 보조 함수를 생성합니다. 이 함수는 'pd.DataFrame()' 절의 데이터 인수에 직접 전달되며, 내부적으로 모든 검색된 레코드를 순회하고 행이 소진될 때까지 요소를 생성합니다.\n\n다시 말하지만, 이 예제는 매우 기본적이며(주로 설명 목적으로), 보조 함수 내에서 어떤 종류의 필터링이나 변환을 수행할 수 있습니다. 함수를 실행하면 다음과 같은 결과가 나옵니다:\n\n```js\n%%time\ndf_gen = create_df_gen(cursor)\ndf_gen.head()\n\n팬더스 데이터프레임 생성 중...\nDF가 성공적으로 생성되었습니다!\n\nCPU 소요 시간: 사용자 9.04초, 시스템 2.1초, 총 11.1초\n실제 시간: 14.4초\n```\n\n<img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_6.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 가지 방법 모두 데이터 프레임이 반환되기 때문에 메모리 사용량이 동일할 것 같지만, 이는 사실이 아닙니다. 데이터 프레임이 생성되는 동안 데이터 처리 방식이 다르기 때문입니다:\n\n- 방법 #1의 경우, 데이터 교환 과정이 다소 비효율적으로 이루어지고 네트워크를 통해 데이터가 교환되어 더 높은 최대 메모리가 발생합니다.\n- 방법 #2의 경우, 필요할 때만 값을 계산하고 하나씩 처리하기 때문에 더 작은 메모리 공간을 사용합니다.\n\n# 사용 사례 #2: 클라우드 객체 저장소에 쓰기\n\n가끔 데이터 엔지니어는 데이터베이스에 저장된 대량의 데이터를 가져와서 이러한 레코드를 외부(예: 규제기관, 감사인, 파트너)와 공유해야 할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적인 해결책은 클라우드 객체 저장소를 생성하는 것입니다. 데이터가 전달되어 제 3자(적절한 액세스 권한이 부여된)가 데이터를 읽고 자신의 시스템으로 복사할 수 있게 합니다.\n\n사실, 우리는 데이터가 parquet 형식으로 작성될 버킷인 generators-test-bucket을 생성했습니다. 이는 awswrangler 패키지를 활용하여 데이터가 저장될 것입니다.\n\nawswrangler의 장점은 pandas 데이터프레임과 매우 잘 작동하며 데이터 집합 구조를 유지한 채로 데이터프레임을 parquet 형식으로 변환할 수 있다는 것입니다.\n\n## 방법 #1: 일괄 처리를 사용하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n첫 번째 사용 사례의 경우, 일반적으로 데이터를 일괄적으로 가져와서 쓰는 것이 일반적이며 전체 데이터 집합이 순회될 때까지 계속됩니다 :\n\n```js\n# 1.2 WRITING DF TO MINIO BUCKET IN PARQUET FORMAT USING BATCHES\ndef write_df_to_s3_batch(cursor, bucket, folder, parquet_file_name, batch_size):\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n    cursor.execute(query)\n    batch_num = 1\n    while True:\n        rows = cursor.fetchmany(batch_size)\n        if not rows:\n            break\n        print(f\"Writing DF batch #{batch_num} to S3 bucket...\")\n        wr.s3.to_parquet(df= pd.DataFrame(data = rows, columns=colnames),\n                         path=f's3://{bucket}/{folder}/{parquet_file_name}',\n                         compression='gzip',\n                         mode = 'append',\n                         dataset=True)\n        print('Batch successfully written to S3 bucket!\\n')\n        batch_num += 1\n```\n\nwrite_df_to_s3_batch() 함수를 실행하면 각각 100만 개의 레코드를 포함하는 5개의 파케이 파일이 해당 버킷에 생성됩니다 :\n\n```js\nwrite_df_to_s3_batch(cursor, bucket, folder_batch, parquet_file_name, batch_size)\n\nWriting DF batch #1 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #2 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #3 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #4 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #5 to S3 bucket...\nBatch successfully written to S3 bucket!\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_7.png\" />\n\n## 방법 #2: 제너레이터 사용하기\n\n대안으로, 제너레이터를 활용하여 데이터를 추출하고 버킷에 작성할 수 있습니다. 제너레이터는 데이터를 가져오고 이동하는 동안 메모리 비효율성을 야기하지 않으므로 전체 DataFrame을 한 번에 쓰기를 결정할 수도 있습니다:\n\n```js\n# 2.2 GENERATOR를 사용하여 PARQUET 형식으로 DF를 MINIO 버킷에 쓰기\ndef write_df_to_s3_gen(cursor, bucket, folder, parquet_file_name):\n    print('DF를 S3 버킷에 쓰는 중...')\n\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n\n    wr.s3.to_parquet(df=pd.DataFrame(data=generate_dataset(cursor), columns=colnames),\n             path=f's3://{bucket}/{folder}/{parquet_file_name}',\n             compression='gzip',\n             mode='append',\n             dataset=True)\n    print('데이터가 성공적으로 S3 버킷에 쓰여졌습니다!\\n')\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndef wirte_df_to_s3_gen(cursor, bucket, folder_gen, parquet_file_name):\n\nWriting DF to S3 bucket...\nData successfully written to S3 bucket!\n```\n\n![Python Generators](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_8.png)\n\n# 결론\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적인 Python 함수보다 직관성이 떨어지는 제너레이터는 메모리를 적게 차지하면서도 좋은 성능을 제공하기 때문에 덜 사용되지만 이점이 많습니다.\n\n실제로 이 자습서에서는 데이터 엔지니어가 Python 제너레이터를 활용해 데이터베이스에서 데이터를 효율적으로 검색하는 방법을 연구하기 위해 세 가지 로컬 서비스(포스트그레스DB, 주피터 노트북, MinIO)를 도커를 통해 구동하여 데이터를 일괄로 처리하는 대신 데이터를 효율적으로 가져올 수 있는 두 가지 실제 예시를 공유했습니다.\n","ogImage":{"url":"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png"},"coverImage":"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png","tag":["Tech"],"readingTime":19},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png\" alt=\"image\"></p>\n<h1>온디맨드 코스 | 추천</h1>\n<p>몇몇 독자들이 데이터 엔지니어로 성장하는 데 도움이 될 온디맨드 코스를 요청했습니다. 제가 추천하는 3가지 좋은 자원은 다음과 같습니다:</p>\n<ul>\n<li>데이터 엔지니어링 나노디그리 (UDACITY)</li>\n<li>아파치 카프카 &#x26; 아파치 스파크를 이용한 데이터 스트리밍 나노디그리 (UDACITY)</li>\n<li>파이스파크를 이용한 스파크 및 파이썬 빅데이터 과정 (UDEMY)</li>\n</ul>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>아직 Medium 회원이 아니신가요? Medium이 제공하는 모든 것에 액세스하려면 매월 $5로 가입하는 것을 고려해보세요!</p>\n<h1>소개</h1>\n<p>데이터 엔지니어로써 우리는 종종 운영 데이터베이스에서 특히 큰 데이터 집합을 가져와서 일련의 변환을 수행한 후에 분석 데이터베이스나 S3 버킷과 같은 클라우드 객체 저장소에 기록해야 하는 상황에 직면합니다.</p>\n<p>이 경우 Airflow 인스턴스에서 사용 가능한 메모리의 큰 부분을 사용하여 데이터 팀의 다른 동료들의 작업에 영향을 주지 않고 작업을 수행하는 방법을 찾아야 합니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>파이썬 생성기가 메모리 피크를 피하면서 데이터베이스에서 데이터를 효율적으로 가져올 때 사용될 수 있는 좋은 옵션이 될 수 있습니다.</p>\n<p>실제로, 본 자습서에서는 생성기를 사용하는 것이 데이터 엔지니어에게 현명한 접근 방식인 두 가지 실용적인 사용 사례를 살펴볼 것입니다. 이를 위해 Docker 컨테이너를 구동하여 실제 엔드 투 엔드 데이터 워크플로를 시뮬레이션하기 위해 세 가지 서비스(포스트그레스 데이터베이스, 주피터 노트북 및 MinIO)를 실행할 것입니다.</p>\n<h1>파이썬에서 생성기의 장점</h1>\n<p>파이썬에서 표준 함수는 단일 값 계산 후 종료되지만, 생성기는 필요에 따라 일시 중지하고 다시 시작하면서 시간이 지남에 따라 값 시퀀스를 생성할 수 있습니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>제너레이터는 값을 시퀀스로 생성하기 위해 return 대신 yield 문을 사용하는 특별한 함수입니다. 값은 한 번에 하나씩 생성되며 전체 시퀀스를 메모리에 저장할 필요가 없습니다.</p>\n<p>제너레이터 함수가 호출되면 제너레이터에 의해 생성된 값의 시퀀스를 반복할 수 있는 이터레이터 객체가 반환됩니다.</p>\n<p>예를 들어, 0부터 입력 변수 n 사이의 숫자들의 제곱을 생성하는 squares_generator(n) 함수를 만들어 봅시다:</p>\n<pre><code class=\"hljs language-js\">def <span class=\"hljs-title function_\">squares_generator</span>(n):\n  num = <span class=\"hljs-number\">0</span>\n  <span class=\"hljs-keyword\">while</span> num &#x3C; <span class=\"hljs-attr\">n</span>:\n    <span class=\"hljs-keyword\">yield</span> num * num\n    num += <span class=\"hljs-number\">1</span>\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>함수를 호출하면 이터레이터만 반환됩니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">squares_generator</span>(n)\n\n#출력:\n# &#x3C;generator object squares_generator at <span class=\"hljs-number\">0x10653bdd0</span>>\n</code></pre>\n<p>모든 값의 시퀀스를 가져오려면 제너레이터 함수를 루프 안에서 호출해야합니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">for</span> num <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">squares_generator</span>(<span class=\"hljs-number\">5</span>):\n  <span class=\"hljs-title function_\">print</span>(num)\n\n#출력:\n<span class=\"hljs-number\">0</span>\n<span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">4</span>\n<span class=\"hljs-number\">9</span>\n<span class=\"hljs-number\">16</span>\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>더 효율적이고 세련된 옵션은 함수 대신 한 줄로 작성된 생성기 표현식을 만드는 것입니다:</p>\n<pre><code class=\"hljs language-js\">n = <span class=\"hljs-number\">5</span>\ngenerator_exp = (num * num <span class=\"hljs-keyword\">for</span> num <span class=\"hljs-keyword\">in</span> <span class=\"hljs-title function_\">range</span>(n))\n</code></pre>\n<p>이제 값을 next() 메서드를 사용하여 직접 접근할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">print</span>(<span class=\"hljs-title function_\">next</span>(generator_exp)) # <span class=\"hljs-number\">0</span>\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-title function_\">next</span>(generator_exp)) # <span class=\"hljs-number\">1</span>\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-title function_\">next</span>(generator_exp)) # <span class=\"hljs-number\">4</span>\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-title function_\">next</span>(generator_exp)) # <span class=\"hljs-number\">9</span>\n<span class=\"hljs-title function_\">print</span>(<span class=\"hljs-title function_\">next</span>(generator_exp)) # <span class=\"hljs-number\">16</span>\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>우리가 볼 수 있듯이, 제너레이터 함수에서 값이 반환되는 방식은 일반적인 파이썬 함수와는 즉각적으로 직관적이지 않습니다. 아마도 그것이 많은 데이터 엔지니어들이 발생해야 할 정도로 제너레이터를 사용하지 않는 이유일 것입니다.</p>\n<p>다음 섹션에서 두 가지 일반적인 사용 사례를 설명해보겠습니다.</p>\n<h1>목표 및 설정</h1>\n<p>이 자습서의 목표는 다음과 같습니다:</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>Postgres DB로부터 데이터를 가져와서 pandas 데이터프레임으로 저장합니다.</li>\n<li>pandas 데이터프레임을 Parquet 형식으로 S3 버킷에 씁니다.</li>\n</ul>\n<p>각 목표는 일반 함수와 제너레이터 함수를 사용하여 모두 달성될 것입니다.</p>\n<p>이러한 워크플로우를 시뮬레이션하기 위해 세 가지 서비스가 있는 도커 컨테이너를 실행합니다:</p>\n<ul>\n<li>Postgres DB = 데이터를 가져올 소스 운영 데이터베이스로 사용될 서비스입니다. Docker-compose가 mainDB를 생성하고 transactions이라는 테이블에 5백만 개의 모의 레코드를 삽입하는 작업을 수행합니다. 참고: 이 튜토리얼을 위한 자료를 준비하는 동안, 더 큰 데이터셋을 시뮬레이션하기 위해 5천만 개, 1억 개의 행을 시도해 보았지만 Docker 서비스의 성능에 영향을 미쳤습니다.</li>\n<li>MinIO = AWS S3 버킷을 시뮬레이션하는 데 사용될 서비스로, awswrangler 패키지를 사용하여 pandas 데이터프레임을 Parquet 형식으로 쓸 때 도움이 될 것입니다.</li>\n<li>Jupyter Notebook = 익숙한 컴파일러를 통해 Python 코드 조각을 대화식으로 실행하는 데 사용될 서비스입니다.</li>\n</ul>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>지금까지 설명한 내용을 시각적으로 보여주는 그래프입니다:</p>\n<p><img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_1.png\" alt=\"그래프\"></p>\n<p>첫 번째 단계로는 프로젝트의 GitHub 리포지토리를 복제하고 해당 폴더로 이동합니다:</p>\n<pre><code class=\"hljs language-js\">git clone git@github.<span class=\"hljs-property\">com</span>:anbento0490/projects.<span class=\"hljs-property\">git</span> &#x26;&#x26;\ncd fetch_data_with_python_generators\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>그러면 세 가지 서비스를 시작하는 도커 컴포즈를 실행할 수 있어요:</p>\n<pre><code class=\"hljs language-js\">docker compose up -d\n\n[+] <span class=\"hljs-title class_\">Running</span> <span class=\"hljs-number\">5</span>/<span class=\"hljs-number\">5</span>\n ⠿ <span class=\"hljs-title class_\">Network</span> shared-network                 <span class=\"hljs-title class_\">Created</span>                                                 <span class=\"hljs-number\">0.</span>0s\n ⠿ <span class=\"hljs-title class_\">Container</span> jupyter-notebooks            <span class=\"hljs-title class_\">Started</span>                                                 <span class=\"hljs-number\">1.</span>0s\n ⠿ <span class=\"hljs-title class_\">Container</span> minio                        <span class=\"hljs-title class_\">Started</span>                                                 <span class=\"hljs-number\">0.</span>7s\n ⠿ <span class=\"hljs-title class_\">Container</span> postgres-db                  <span class=\"hljs-title class_\">Started</span>                                                 <span class=\"hljs-number\">0.</span>9s\n ⠿ <span class=\"hljs-title class_\">Container</span> mc                           <span class=\"hljs-title class_\">Started</span>                                                 <span class=\"hljs-number\">1.</span>1s\n</code></pre>\n<p>最終적으로 확인할 수 있어요:</p>\n<ul>\n<li>포스트그레스 데이터베이스에 transactions 테이블이 생성되었고 5백만 개의 레코드가 포함되어 있어요:</li>\n</ul>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">docker exec -it postgres-db /bin/bash\n\nroot@9632469<span class=\"hljs-attr\">c70e0</span>:/# psql -U postgres\n\npsql (<span class=\"hljs-number\">13.13</span> (<span class=\"hljs-title class_\">Debian</span> <span class=\"hljs-number\">13.13</span>-<span class=\"hljs-number\">1.</span>pgdg120+<span class=\"hljs-number\">1</span>))\n도움말을 보려면 <span class=\"hljs-string\">\"help\"</span>를 입력하세요.\n\npostgres=# \\c mainDB\n데이터베이스 <span class=\"hljs-string\">\"mainDB\"</span>에 사용자 <span class=\"hljs-string\">\"postgres\"</span>로 연결되었습니다.\n\nmainDB=# select <span class=\"hljs-title function_\">count</span>(*) <span class=\"hljs-keyword\">from</span> transactions;\n  count\n---------\n <span class=\"hljs-number\">5000000</span>\n(<span class=\"hljs-number\">1</span> 로우)\n</code></pre>\n<ul>\n<li>MinIO UI는 localhost:9001 포트에서 접속할 수 있습니다. 자격 증명을 요청 받을 때 (관리자 및 비밀번호를 입력)를 사용하고 generators-test-bucket이라는 빈 버킷이 생성되었습니다:</li>\n</ul>\n<p><img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_2.png\" alt=\"image\"></p>\n<ul>\n<li>Jupyter Notebook UI는 localhost:8889에서 접근할 수 있으며 아래에 토큰을 검색하여 액세스할 수 있습니다:</li>\n</ul>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-bash\">docker <span class=\"hljs-built_in\">exec</span> -it jupyter-notebooks /bin/bash\n\nroot@eae08d1f4bf6:~<span class=\"hljs-comment\"># jupyter server list</span>\n\n현재 실행 중인 서버:\nhttp://eae08d1f4bf6:8888/?token=8a45d846d03cf0c0e4584c3b73af86ba5dk9e83c8ac47ee7 :: /home/jovyan\n</code></pre>\n<p><img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_3.png\" alt=\"Python Generators\"></p>\n<p>좋아요! Jupyter에서 몇 가지 코드를 실행할 준비가 모두 끝났어요.</p>\n<p>하지만 그 전에 MinIO의 버킷과 상호 작용하려면 새로운 access_key와 secret_access_key를 생성해야 합니다:</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_4.png\">\n<p>알림: MinIO 버킷의 가장 멋진 기능 중 하나는 AWS S3 버킷처럼 상호 작용할 수 있다는 것입니다 (예: boto3, awswrangler 등을 사용하여). 하지만 이러한 기능은 비용이 발생하지 않으며, 로컬 환경에만 존재하므로 비밀을 노출할 걱정이 없습니다. 컨테이너가 중지될 때까지 유지되지 않으므로 데이터가 계속 유지되지 않습니다.</p>\n<p>이제 생성기 노트북에서 다음 코드를 실행해 봅시다 (비밀 정보를 꼭 교체해주세요):</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> psycopg2\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> boto3\n<span class=\"hljs-keyword\">import</span> awswrangler <span class=\"hljs-keyword\">as</span> wr\n\n<span class=\"hljs-comment\">#######################################################</span>\n<span class=\"hljs-comment\">######## PG DB에 연결하고 커서 생성 #######</span>\nconnection = psycopg2.connect(user=<span class=\"hljs-string\">\"postgres\"</span>,\n                              password=<span class=\"hljs-string\">\"postgres\"</span>,\n                              port=<span class=\"hljs-string\">\"5432\"</span>,\n                              database=<span class=\"hljs-string\">\"mainDB\"</span>)\ncursor = connection.cursor()\n\nquery = <span class=\"hljs-string\">\"select * from transactions;\"</span>\n\n<span class=\"hljs-comment\">#######################################################</span>\n<span class=\"hljs-comment\">######## MINIO 버킷에 연결 ###################</span>\n\nboto3.setup_default_session(aws_access_key_id=<span class=\"hljs-string\">'your_access_key'</span>,\n                            aws_secret_access_key=<span class=\"hljs-string\">'your_secret_key'</span>)\n\nbucket = <span class=\"hljs-string\">'generators-test-bucket'</span>\nfolder_gen = <span class=\"hljs-string\">'data_gen'</span>\nfolder_batch = <span class=\"hljs-string\">'data_batch'</span>\nparquet_file_name = <span class=\"hljs-string\">'transactions'</span>\nbatch_size = <span class=\"hljs-number\">1000000</span>\n\nwr.config.s3_endpoint_url = <span class=\"hljs-string\">'http://minio:9000'</span>\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>이것은 mainDB에 연결하고 쿼리를 실행하기 위한 커서를 만듭니다. 또한 generators-test-bucket와 상호 작용하기 위한 기본 세션이 설정됩니다.</p>\n<h1>사용 사례 #1: 데이터베이스에서 읽기</h1>\n<p>데이터 엔지니어로서 데이터베이스 또는 외부 서비스에서 대규모 데이터 세트를 Python 파이프라인으로 가져올 때, 다음 사항 사이의 균형을 찾아야 합니다:</p>\n<ul>\n<li>메모리: 한꺼번에 전체 데이터 세트를 가져오면 OOM 오류가 발생하거나 전체 인스턴스/클러스터의 성능에 영향을 줄 수 있습니다.</li>\n<li>속도: 행을 하나씩 가져오는 것도 비싼 I/O 네트워크 작업을 초래할 수 있습니다.</li>\n</ul>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>방법 #1: 일괄적으로 데이터 가져오기</h2>\n<p>실무에서 자주 사용하는 합리적인 절충안은 사용 가능한 메모리와 데이터 파이프라인의 속도 요구 사항에 따라 배치로 데이터를 가져오는 것입니다:</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-number\">1.1</span>. 배치를 사용하여 <span class=\"hljs-variable constant_\">DF</span> 생성\ndef <span class=\"hljs-title function_\">create_df_batch</span>(cursor, batch_size):\n\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'생성 중...'</span>)\n    colnames = [<span class=\"hljs-string\">'transaction_id'</span>,\n                <span class=\"hljs-string\">'user_id'</span>,\n                <span class=\"hljs-string\">'product_name'</span>,\n                <span class=\"hljs-string\">'transaction_date'</span>,\n                <span class=\"hljs-string\">'amount_gbp'</span>]\n\n    df = pd.<span class=\"hljs-title class_\">DataFrame</span>(columns=colnames)\n    cursor.<span class=\"hljs-title function_\">execute</span>(query)\n\n    <span class=\"hljs-keyword\">while</span> <span class=\"hljs-title class_\">True</span>:\n        rows = cursor.<span class=\"hljs-title function_\">fetchmany</span>(batch_size)\n        <span class=\"hljs-keyword\">if</span> not <span class=\"hljs-attr\">rows</span>:\n            <span class=\"hljs-keyword\">break</span>\n        # 일부 변환\n        batch_df = pd.<span class=\"hljs-title class_\">DataFrame</span>(data = rows, columns=colnames)\n        df = pd.<span class=\"hljs-title function_\">concat</span>([df, batch_df], ignore_index=<span class=\"hljs-title class_\">True</span>)\n\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'DF 생성 완료!\\n'</span>)\n\n    <span class=\"hljs-keyword\">return</span> df\n</code></pre>\n<p>위 코드는 다음을 수행합니다:</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>빈 df를 생성;</li>\n<li>쿼리를 실행하고 전체 결과를 커서 객체에 캐싱;</li>\n<li>while 루프를 초기화하여 매 반복마다 지정된 배치 크기(이 경우 1백만 행)와 동일한 행 수를 가져와 이 데이터를 사용하여 배치_df를 생성합니다.</li>\n<li>최종적으로 배치_df가 주 df에 추가됩니다. 전체 데이터셋이 통과될 때까지 이 프로세스가 반복됩니다.</li>\n</ul>\n<p>분명히 말하자면, 이것은 기본적인 예시이며, 단순히 한 번에 한 배치씩 df를 생성하는 것 외에도 while 루프의 일부로 다른 많은 작업(필터링, 정렬, 집계, 데이터를 다른 위치로 쓰기 등)을 수행할 수 있었습니다.</p>\n<p>노트북에서 함수를 실행하면 다음과 같이 결과를 얻을 수 있습니다:</p>\n<pre><code class=\"hljs language-js\">%%time\ndf_batch = <span class=\"hljs-title function_\">create_df_batch</span>(cursor, batch_size)\ndf_batch.<span class=\"hljs-title function_\">head</span>()\n\n결과:\n\n생성 중...\n<span class=\"hljs-variable constant_\">DF</span> 생성 완료!\n\n<span class=\"hljs-variable constant_\">CPU</span> 시간: 사용자 <span class=\"hljs-number\">9.97</span>초, 시스템: <span class=\"hljs-number\">13.7</span>초, 총: <span class=\"hljs-number\">23.7</span>초\n실제 시간: <span class=\"hljs-number\">25</span>초\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p><img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_5.png\" alt=\"Python Generators\"></p>\n<h2>Method #2: Using Generators</h2>\n<p>A less common -but powerful- strategy for data engineers is to fetch data as a stream using generators:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># AUXILIARY FUNCTION</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">generate_dataset</span>(<span class=\"hljs-params\">cursor</span>):\n\n    cursor.execute(query)\n\n    <span class=\"hljs-keyword\">for</span> row <span class=\"hljs-keyword\">in</span> cursor.fetchall():\n        <span class=\"hljs-comment\"># some transformation</span>\n        <span class=\"hljs-keyword\">yield</span> row\n\n<span class=\"hljs-comment\"># 2.1. CREATE DF USING GENERATORS</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_df_gen</span>(<span class=\"hljs-params\">cursor</span>):\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'Creating pandas DF using generator...'</span>)\n\n    colnames = [<span class=\"hljs-string\">'transaction_id'</span>,\n                <span class=\"hljs-string\">'user_id'</span>,\n                <span class=\"hljs-string\">'product_name'</span>,\n                <span class=\"hljs-string\">'transaction_date'</span>,\n                <span class=\"hljs-string\">'amount_gbp'</span>]\n\n    df = pd.DataFrame(data=generate_dataset(cursor), columns=colnames)\n\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'DF successfully created!\\n'</span>)\n\n    <span class=\"hljs-keyword\">return</span> df\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>위의 코드 스니펫에서는 쿼리를 실행하고 행을 시퀀스로 반환하는 'generate_dataset' 보조 함수를 생성합니다. 이 함수는 'pd.DataFrame()' 절의 데이터 인수에 직접 전달되며, 내부적으로 모든 검색된 레코드를 순회하고 행이 소진될 때까지 요소를 생성합니다.</p>\n<p>다시 말하지만, 이 예제는 매우 기본적이며(주로 설명 목적으로), 보조 함수 내에서 어떤 종류의 필터링이나 변환을 수행할 수 있습니다. 함수를 실행하면 다음과 같은 결과가 나옵니다:</p>\n<pre><code class=\"hljs language-js\">%%time\ndf_gen = <span class=\"hljs-title function_\">create_df_gen</span>(cursor)\ndf_gen.<span class=\"hljs-title function_\">head</span>()\n\n팬더스 데이터프레임 생성 중...\n<span class=\"hljs-variable constant_\">DF</span>가 성공적으로 생성되었습니다!\n\n<span class=\"hljs-variable constant_\">CPU</span> 소요 시간: 사용자 <span class=\"hljs-number\">9.04</span>초, 시스템 <span class=\"hljs-number\">2.1</span>초, 총 <span class=\"hljs-number\">11.1</span>초\n실제 시간: <span class=\"hljs-number\">14.4</span>초\n</code></pre>\n<img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_6.png\">\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>두 가지 방법 모두 데이터 프레임이 반환되기 때문에 메모리 사용량이 동일할 것 같지만, 이는 사실이 아닙니다. 데이터 프레임이 생성되는 동안 데이터 처리 방식이 다르기 때문입니다:</p>\n<ul>\n<li>방법 #1의 경우, 데이터 교환 과정이 다소 비효율적으로 이루어지고 네트워크를 통해 데이터가 교환되어 더 높은 최대 메모리가 발생합니다.</li>\n<li>방법 #2의 경우, 필요할 때만 값을 계산하고 하나씩 처리하기 때문에 더 작은 메모리 공간을 사용합니다.</li>\n</ul>\n<h1>사용 사례 #2: 클라우드 객체 저장소에 쓰기</h1>\n<p>가끔 데이터 엔지니어는 데이터베이스에 저장된 대량의 데이터를 가져와서 이러한 레코드를 외부(예: 규제기관, 감사인, 파트너)와 공유해야 할 수 있습니다.</p>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>일반적인 해결책은 클라우드 객체 저장소를 생성하는 것입니다. 데이터가 전달되어 제 3자(적절한 액세스 권한이 부여된)가 데이터를 읽고 자신의 시스템으로 복사할 수 있게 합니다.</p>\n<p>사실, 우리는 데이터가 parquet 형식으로 작성될 버킷인 generators-test-bucket을 생성했습니다. 이는 awswrangler 패키지를 활용하여 데이터가 저장될 것입니다.</p>\n<p>awswrangler의 장점은 pandas 데이터프레임과 매우 잘 작동하며 데이터 집합 구조를 유지한 채로 데이터프레임을 parquet 형식으로 변환할 수 있다는 것입니다.</p>\n<h2>방법 #1: 일괄 처리를 사용하기</h2>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>첫 번째 사용 사례의 경우, 일반적으로 데이터를 일괄적으로 가져와서 쓰는 것이 일반적이며 전체 데이터 집합이 순회될 때까지 계속됩니다 :</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-number\">1.2</span> <span class=\"hljs-variable constant_\">WRITING</span> <span class=\"hljs-variable constant_\">DF</span> <span class=\"hljs-variable constant_\">TO</span> <span class=\"hljs-variable constant_\">MINIO</span> <span class=\"hljs-variable constant_\">BUCKET</span> <span class=\"hljs-variable constant_\">IN</span> <span class=\"hljs-variable constant_\">PARQUET</span> <span class=\"hljs-variable constant_\">FORMAT</span> <span class=\"hljs-variable constant_\">USING</span> <span class=\"hljs-variable constant_\">BATCHES</span>\ndef <span class=\"hljs-title function_\">write_df_to_s3_batch</span>(cursor, bucket, folder, parquet_file_name, batch_size):\n    colnames = [<span class=\"hljs-string\">'transaction_id'</span>,\n                <span class=\"hljs-string\">'user_id'</span>,\n                <span class=\"hljs-string\">'product_name'</span>,\n                <span class=\"hljs-string\">'transaction_date'</span>,\n                <span class=\"hljs-string\">'amount_gbp'</span>]\n    cursor.<span class=\"hljs-title function_\">execute</span>(query)\n    batch_num = <span class=\"hljs-number\">1</span>\n    <span class=\"hljs-keyword\">while</span> <span class=\"hljs-title class_\">True</span>:\n        rows = cursor.<span class=\"hljs-title function_\">fetchmany</span>(batch_size)\n        <span class=\"hljs-keyword\">if</span> not <span class=\"hljs-attr\">rows</span>:\n            <span class=\"hljs-keyword\">break</span>\n        <span class=\"hljs-title function_\">print</span>(f<span class=\"hljs-string\">\"Writing DF batch #{batch_num} to S3 bucket...\"</span>)\n        wr.<span class=\"hljs-property\">s3</span>.<span class=\"hljs-title function_\">to_parquet</span>(df= pd.<span class=\"hljs-title class_\">DataFrame</span>(data = rows, columns=colnames),\n                         path=f<span class=\"hljs-string\">'s3://{bucket}/{folder}/{parquet_file_name}'</span>,\n                         compression=<span class=\"hljs-string\">'gzip'</span>,\n                         mode = <span class=\"hljs-string\">'append'</span>,\n                         dataset=<span class=\"hljs-title class_\">True</span>)\n        <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'Batch successfully written to S3 bucket!\\n'</span>)\n        batch_num += <span class=\"hljs-number\">1</span>\n</code></pre>\n<p>write_df_to_s3_batch() 함수를 실행하면 각각 100만 개의 레코드를 포함하는 5개의 파케이 파일이 해당 버킷에 생성됩니다 :</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-title function_\">write_df_to_s3_batch</span>(cursor, bucket, folder_batch, parquet_file_name, batch_size)\n\n<span class=\"hljs-title class_\">Writing</span> <span class=\"hljs-variable constant_\">DF</span> batch #<span class=\"hljs-number\">1</span> to <span class=\"hljs-variable constant_\">S3</span> bucket...\n<span class=\"hljs-title class_\">Batch</span> successfully written to <span class=\"hljs-variable constant_\">S3</span> bucket!\n\n<span class=\"hljs-title class_\">Writing</span> <span class=\"hljs-variable constant_\">DF</span> batch #<span class=\"hljs-number\">2</span> to <span class=\"hljs-variable constant_\">S3</span> bucket...\n<span class=\"hljs-title class_\">Batch</span> successfully written to <span class=\"hljs-variable constant_\">S3</span> bucket!\n\n<span class=\"hljs-title class_\">Writing</span> <span class=\"hljs-variable constant_\">DF</span> batch #<span class=\"hljs-number\">3</span> to <span class=\"hljs-variable constant_\">S3</span> bucket...\n<span class=\"hljs-title class_\">Batch</span> successfully written to <span class=\"hljs-variable constant_\">S3</span> bucket!\n\n<span class=\"hljs-title class_\">Writing</span> <span class=\"hljs-variable constant_\">DF</span> batch #<span class=\"hljs-number\">4</span> to <span class=\"hljs-variable constant_\">S3</span> bucket...\n<span class=\"hljs-title class_\">Batch</span> successfully written to <span class=\"hljs-variable constant_\">S3</span> bucket!\n\n<span class=\"hljs-title class_\">Writing</span> <span class=\"hljs-variable constant_\">DF</span> batch #<span class=\"hljs-number\">5</span> to <span class=\"hljs-variable constant_\">S3</span> bucket...\n<span class=\"hljs-title class_\">Batch</span> successfully written to <span class=\"hljs-variable constant_\">S3</span> bucket!\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_7.png\">\n<h2>방법 #2: 제너레이터 사용하기</h2>\n<p>대안으로, 제너레이터를 활용하여 데이터를 추출하고 버킷에 작성할 수 있습니다. 제너레이터는 데이터를 가져오고 이동하는 동안 메모리 비효율성을 야기하지 않으므로 전체 DataFrame을 한 번에 쓰기를 결정할 수도 있습니다:</p>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-number\">2.2</span> <span class=\"hljs-variable constant_\">GENERATOR</span>를 사용하여 <span class=\"hljs-variable constant_\">PARQUET</span> 형식으로 <span class=\"hljs-variable constant_\">DF</span>를 <span class=\"hljs-variable constant_\">MINIO</span> 버킷에 쓰기\ndef <span class=\"hljs-title function_\">write_df_to_s3_gen</span>(cursor, bucket, folder, parquet_file_name):\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'DF를 S3 버킷에 쓰는 중...'</span>)\n\n    colnames = [<span class=\"hljs-string\">'transaction_id'</span>,\n                <span class=\"hljs-string\">'user_id'</span>,\n                <span class=\"hljs-string\">'product_name'</span>,\n                <span class=\"hljs-string\">'transaction_date'</span>,\n                <span class=\"hljs-string\">'amount_gbp'</span>]\n\n    wr.<span class=\"hljs-property\">s3</span>.<span class=\"hljs-title function_\">to_parquet</span>(df=pd.<span class=\"hljs-title class_\">DataFrame</span>(data=<span class=\"hljs-title function_\">generate_dataset</span>(cursor), columns=colnames),\n             path=f<span class=\"hljs-string\">'s3://{bucket}/{folder}/{parquet_file_name}'</span>,\n             compression=<span class=\"hljs-string\">'gzip'</span>,\n             mode=<span class=\"hljs-string\">'append'</span>,\n             dataset=<span class=\"hljs-title class_\">True</span>)\n    <span class=\"hljs-title function_\">print</span>(<span class=\"hljs-string\">'데이터가 성공적으로 S3 버킷에 쓰여졌습니다!\\n'</span>)\n</code></pre>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">wirte_df_to_s3_gen</span>(<span class=\"hljs-params\">cursor, bucket, folder_gen, parquet_file_name</span>):\n\nWriting DF to S3 bucket...\nData successfully written to S3 bucket!\n</code></pre>\n<p><img src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_8.png\" alt=\"Python Generators\"></p>\n<h1>결론</h1>\n<!-- ui-station 사각형 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>일반적인 Python 함수보다 직관성이 떨어지는 제너레이터는 메모리를 적게 차지하면서도 좋은 성능을 제공하기 때문에 덜 사용되지만 이점이 많습니다.</p>\n<p>실제로 이 자습서에서는 데이터 엔지니어가 Python 제너레이터를 활용해 데이터베이스에서 데이터를 효율적으로 검색하는 방법을 연구하기 위해 세 가지 로컬 서비스(포스트그레스DB, 주피터 노트북, MinIO)를 도커를 통해 구동하여 데이터를 일괄로 처리하는 대신 데이터를 효율적으로 가져올 수 있는 두 가지 실제 예시를 공유했습니다.</p>\n</body>\n</html>\n"},"__N_SSG":true}