{"pageProps":{"posts":[{"title":"쿠버네티스 이스티오 앰비언트 메쉬 투어  1부 설정 및 Z터널 사용 방법","description":"","date":"2024-06-23 23:14","slug":"2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel","content":"\n피츠오는 앰비언트 모드로 전환하고 있어요 — 사이드카 없는 모델로 말이죠. 마침내 우리는 CPU와 메모리 소비가 많은 사이드카를 버릴 수 있게 되었어요!\n\n다가오는 장마시즌을 맞아, 저는 피츠오 앰비언트 메시에 간접적으로 참여해보기로 했어요.\n\n이 블로그를 사용하여 여행을 문서화하고, 비슷한 생각을 가진 피츠오 사용자들이 함께 따라올 수 있게 도와볼 거예요 :P.\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 나름대로 번역해보겠습니다:\n\n안녕하세요! Part 1에서는 주변 메시 실험을 위해 로컬 클러스터 설정을 완벽하게 진행할 예정입니다. 그런 다음 메시의 ztunnel 구성 요소를 탐험할 것입니다.\n\nPart 2에서는 L4 인증 정책에 대해 이야기하고 waypoint 프록시를 시작하는 방법을 살펴볼 것입니다.\n\n# Ambient란 무엇인가요?\n\nIstio Ambient Mesh는 사이드카 없이 Istio의 새로운 데이터플레인 모드입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적인 Istio 모드에서는 모든 응용 프로그램 Pod이 envoy 프록시로 주입되었다는 것을 기억하십시오. 그러나 이 새로운 모드에서는 응용 프로그램 Pod이 건드리지 않을 거에요 :) 그리고 그들은 자신의 응용 프로그램 컨테이너만 가지게 될 거에요.\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_1.png)\n\n가장 먼저 떠오르는 큰 장점 중 하나는 인프라 비용이 크게 절감된다는 것입니다. 컴퓨팅 코어 및 메모리 관점에서 얼마나 많은 돈을 절약하고 있는지 상상해보세요 !!\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_2.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Ambient Architecture\n\n과거와 현재의 Ambient Architecture를 상상해 보는 시간입니다. 데이터 평면의 트래픽 흐름 경로에서 그들이 어떻게 다른지 살펴봅시다.\n\n## Sidecar 모드\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 전통적인 사이드카 모델이며 각 서비스 팟에는 애플리케이션 컨테이너와 Envoy 사이드카가 결합되어 있습니다. 애플리케이션으로부터 오고 가는 모든 트래픽은 사이드카에 의해 가로채집니다.\n\n## Ambient 모드, ztunnel 사용\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_4.png)\n\n이 새로운 ambient 모드에서 애플리케이션 팟은 사이드카가 없는 독립적인 팟입니다. 그러나 이 경우에는 클러스터의 각 Kubernetes 노드마다 데몬셋 팟이 실행될 것입니다 - 강력한 ztunnel (제로 트러스트 터널). 노드 내 팟간의 모든 트래픽은 ztunnel에 의해 가로채집됩니다. Ztunnel은 각 노드당 L4 프록시입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Ambient mode, with ztunnel + waypoint\n\n![Image](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_5.png)\n\nZtunnel은 L4 프록시가 필요한 워크로드 간 네트워킹에 충분합니다. http 헤더 기반 라우팅, L7 권한 부여와 같은 L7 요구 사항을 충족시키기 위해 waypoint 프록시라는 워크로드를 배포합니다. 이는 애플리케이션당 envoy 팟으로, 동일한 노드 또는 다른 노드에서 실행될 수 있습니다.\n\n이 경우 ztunnel에서 생성된 트래픽은 waypoint 프록시에 도달하고, waypoint는 그것을 목적지 ztunnel로 전달합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러므로, 어플리케이션이 L7 처리를 요구하지 않는 경우 waypoint를 없애고 ztunnel만 사용할 수 있습니다. 이전 방식에서는, 우리가 L4 요구사항만 가지고 있더라도 envoy sidecars를 반드시 사용해야 했습니다.\n\n# 클러스터 설정 깊이에 대한 정보\n\n이상적으로 지원되는 환경에서 사용할 수 있는 유효한 CNI 설정으로 Kind 클러스터를 설정하는 데 많은 조정이 필요했습니다. 현재는 어떤 공개 GKE/AKS/EKS k8s 클러스터에 대한 실험을 할 수 있는 권한이 없습니다.\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러므로 로컬 Kind 클러스터를 홈 설정하겠습니다.\n\n## 시스템 사양\n\n- Mac M1 (Apple-Silicon 아키텍처)를 사용하며 기본 구성으로 진행됩니다.\n\n시스템 아키텍처(Linux/Windows/Apple Intel)를 고려하여 비슷하게 진행할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Rancher Desktop\n\n저희 k8s 클러스터를 위해 도커 런타임이 필요한데, 저는 SUSE의 Rancher Desktop을 사용할 예정입니다 — https://rancherdesktop.io/.\n\n다른 대안으로는 Docker Desktop이 있습니다 — https://www.docker.com/products/docker-desktop/\n\n- Kind\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저는 Multi-Node K8s 클러스터가 필요하다고 생각해요. 그래서 우리가 사용할 최상의 도구는 Kind를 이용해 클러스터를 부트스트랩하는 거죠. 여기 https://kind.sigs.k8s.io/docs/user/quick-start 에서 보다 자세한 정보를 얻을 수 있어요.\n\n이를 통해 우리는 k8s 노드에 ssh로 접속하고 노드 구성을 실험해볼 수도 있을 거에요.\n\n그리고 블로그에 나온대로 istio-ambient 프로필을 설치한 후에는 Kind 노드에서 문제가 발생할 수 있어요. 여기 https://kind.sigs.k8s.io/docs/user/known-issues/#pod-errors-due-to-too-many-open-files 에서 문제를 해결할 수 있어요. 영향을 받는 노드의 sysctl.conf를 수정한 후에는 istio-system pods가 문제없이 생성될 거에요.\n\n<img src=\"/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_7.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- CNI(Container Networking Interface)\n\n\"Istio Ambient Mode\"은 Calico, Cilium 등과 같은 CNI와 함께 지원됩니다. Kind 클러스터의 기본 CNI는 \"kindnetd\"이며, 저는 직접 테스트해 본 결과 ztunnel 팟이 실행되지 않았습니다. 그래서 다른 CNI가 필요했습니다.\n\n- Cilium CNI도 Mac M1에서 제대로 로드되지 않았습니다. 즉, cilium 데몬셋 팟이 실행되지 않았습니다.\n- 마침내 Calico CNI로 Kind를 설정하는 방법을 찾았습니다. Kind에서 기본 CNI를 비활성화하고 다음과 같이 진행합니다: [https://docs.tigera.io/calico/latest/getting-started/kubernetes/kind](https://docs.tigera.io/calico/latest/getting-started/kubernetes/kind)\n- 이 설정에서 마지막 단계를 따를 수도 있습니다: [https://alexbrand.dev/post/creating-a-kind-cluster-with-calico-networking/](https://alexbrand.dev/post/creating-a-kind-cluster-with-calico-networking/)\n\n# Istio 설치\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 클러스터 개요\n\n이스티오를 시작하기 전에 클러스터 스펙을 살펴봅시다.\n\n다음은 Kind 클러스터 설정 스크립트입니다.\n\n마스터 노드 1개와 워커 노드 2개가 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ kg 노드\n이름                    상태     역할           생성 시간    버전\nambient-control-plane   준비     제어 플레인    117분     v1.30.0\nambient-worker          준비     <없음>         116분     v1.30.0\nambient-worker2         준비     <없음>         116분     v1.30.0\n```\n\n또한 Calico CNI 데몬세트가 실행 중인지 확인합니다. 클러스터 CoreDNS도 준비되어 있는지 확인합니다.\n\n```js\n(⎈|kind-ambient:kube-system)➜  ~ kgpo\n이름                                            준비     상태      재시작      생성 시간\ncalico-kube-controllers-564985c589-xmsrp        1/1     실행 중  1 (96분 전)  117분\ncalico-node-796kq                               1/1     실행 중  0             116분\ncalico-node-l6fsg                               1/1     실행 중  0             116분\ncalico-node-zkckp                               1/1     실행 중  0             116분\ncoredns-7db6d8ff4d-h88q6                        1/1     실행 중  0             118분\ncoredns-7db6d8ff4d-rtncd                        1/1     실행 중  0             118분\n```\n\n## Istio Ambient 프로필 설치\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n설치는 간단하게 진행됩니다. 여기에서 찾을 수 있어요: [https://istio.io/v1.20/docs/ops/ambient/getting-started/](https://istio.io/v1.20/docs/ops/ambient/getting-started/)\n\n```js\nsabuj $ istioctl install --set profile=ambient --set \"components.ingressGateways[0].enabled=true\" --set \"components.ingressGateways[0].name=istio-ingressgateway\" --skip-confirmation\n✔ Istio core가 설치되었습니다\n✔ Ztunnel이 설치되었습니다\n✔ Istiod가 설치되었습니다\n✔ CNI가 설치되었습니다\n✔ 인그레스 게이트웨이가 설치되었습니다\n✔ 설치가 완료되었습니다\n이 설치를 주입과 유효성 검사를 위한 기본 설정으로 지정합니다.\n```\n\n- 내가 사용 중인 Istio 버전은 다음과 같아요.\n\n```js\n(⎈|kind-ambient:kube-system)➜  ~ istioctl version\nclient version: 1.15.0\ncontrol plane version: 1.22.1\ndata plane version: 1.22.1 (4 프록시)\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Ztunnel은 각 노드에서 실행되는 데몬세트입니다 — 이 경우 3개의 노드가 있습니다. istio-system 네임스페이스에서 ztunnel이 제대로 실행 중인지 확인해 봅시다.\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ kgpo -owide\nNAME                                    READY   STATUS    RESTARTS   AGE    IP               NODE                    NOMINATED NODE   READINESS GATES\nistio-cni-node-7q6z7                    1/1     Running   0          117m   172.18.0.2       ambient-worker2         <none>           <none>\nistio-cni-node-fg6n4                    1/1     Running   0          117m   172.18.0.4       ambient-worker          <none>           <none>\nistio-cni-node-gwvl8                    1/1     Running   0          117m   172.18.0.3       ambient-control-plane   <none>           <none>\nistio-ingressgateway-6f48dfb7db-862sm   1/1     Running   0          117m   192.168.184.70   ambient-worker          <none>           <none>\nistiod-6875bc5c58-n4j7d                 1/1     Running   0          118m   192.168.246.2    ambient-worker2         <none>           <none>\nztunnel-62hp8                           1/1     Running   0          117m   192.168.246.3    ambient-worker2         <none>           <none>\nztunnel-fv5f8                           1/1     Running   0          117m   192.168.184.71   ambient-worker          <none>           <none>\nztunnel-gs52c                           1/1     Running   0          117m   192.168.208.1    ambient-control-plane   <none>           <none>\n\n(⎈|kind-ambient:istio-system)➜  ~ kg ds\nNAME             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\nistio-cni-node   3         3         3       3            3           kubernetes.io/os=linux   119m\nztunnel          3         3         3       3            3           kubernetes.io/os=linux   118m\n```\n\n정말로 3개의 파드가 있네요 !\n\n- 비슷하게 Istio-cni는 또 다른 설치된 데몬세트입니다.\n- Istio 제어 평면인 istiod도 실행 중입니다.\n- 우리는 공개 트래픽을 위해 기본 istio ingressgateway도 설치했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 작업량 설정\n\n이 프로젝트의 모든 매니페스트 파일은 여기에서 찾을 수 있습니다: https://github.com/JanaSabuj/istio-ambient-mesh-exploration\n\n## 안건\n\n- 우리는 앰비언트 프로필 없이 앱을 설정하고, 일반 Istio crds를 사용할 것입니다. 그 후에 앱을 호출할 예정입니다. i) 인그레스 ii) 디버그 클라이언트 pod\n- 그 후에, Istio 데이터 평면 모드를 앰비언트로 전환하고, ztunnel pod를 통해 흐르는 트래픽 경로의 변화를 관찰할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 네임스페이스\n\nambient-demo라는 네임스페이스를 만들어 보겠습니다. 이 네임스페이스에서 앱을 호스팅할 예정입니다.\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ k create ns ambient-demo\n\n(⎈|kind-ambient:istio-system)➜  ~ kg ns\nNAME                 STATUS   AGE\ndefault              Active   132m\nistio-system         Active   125m\nkube-node-lease      Active   132m\nkube-public          Active   132m\nkube-system          Active   132m\nlocal-path-storage   Active   132m\n```\n\n## 애플리케이션\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼 배포, 서비스 및 서비스 어카운트 yaml을 사용하여 애플리케이션을 배포합니다.\n\n클러스터에서는 다음과 같이 보입니다.\n\n```bash\n(⎈|kind-ambient:ambient-demo)➜  ~ kg all\nNAME                          READY   STATUS    RESTARTS   AGE\npod/httpbin-6f4dc97cb-5dpz9   1/1     Running   0          3m21s\npod/httpbin-6f4dc97cb-swdlb   1/1     Running   0          3m31s\n\nNAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/httpbin   ClusterIP   10.96.157.221   <none>        80/TCP    143m\n\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/httpbin   2/2     2            2           143m\n\nNAME                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/httpbin-6f4dc97cb    2         2         2       3m31s\n```\n\n## Istio 구성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n외부 세계에 노출하기 위해 Istio Gateway 및 Istio VirtualService를 생성하여 Istio Ingress를 통해 액세스할 수 있게 만듭니다.\n\n- 우리는 Istio Ingress 파드에 8081 포트를 통해 게이트웨이를 추가하고 있습니다.\n- 그 다음으로, VirtualService를 통해 ambient-demo 네임스페이스에서 실행되는 httpbin 서비스로 라우팅합니다.\n\n## Istio Ingress를 통한 확인\n\n이제 우리의 응용 프로그램에 액세스할 수 있는지 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ kgs\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.96.77.172   <pending>     15021:30807/TCP,80:30587/TCP,443:31004/TCP   147m\n```\n\nKind가 인그레스 서비스를 위한 외부 IP를 제공하지 않았기 때문에, 원하는 리스너 8081에서 인그레스 파드를 포트포워딩하여 외부 액세스를 시뮬레이션할 것입니다.\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ kpf istio-ingressgateway-6f48dfb7db-862sm 8081\nForwarding from 127.0.0.1:8081 -> 8081\nForwarding from [::1]:8081 -> 8081\n```\n\n브라우저에서 127.0.0.1:8081을 입력하여 애플리케이션을 확인할 수 있습니다!\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nhttp://127.0.0.1:8081/\n\nhttpbin.org\n 0.9.2\n[ Base URL: 127.0.0.1:8081/ ]\n간단한 HTTP 요청 및 응답 서비스입니다.\n\n로컬에서 실행: $ docker run -p 80:80 kennethreitz/httpbin\n\n개발자 - 웹사이트\n개발자에게 이메일 보내기\n```\n\n- 또 다른 확인 방법은 로컬에서 curl을 통해 하는 것입니다. 우리는 요청이 istio-envoy 파드에 의해 처리된 것을 확인합니다.\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ curl 127.0.0.1:8081 -v\n*   Trying 127.0.0.1:8081...\n* Connected to 127.0.0.1 (127.0.0.1) port 8081\n> GET / HTTP/1.1\n> Host: 127.0.0.1:8081\n> User-Agent: curl/8.6.0\n> Accept: */*\n>\n< HTTP/1.1 200 OK\n< server: istio-envoy\n< date: Sun, 16 Jun 2024 11:52:13 GMT\n< content-type: text/html; charset=utf-8\n< content-length: 9593\n< access-control-allow-origin: *\n< access-control-allow-credentials: true\n< x-envoy-upstream-service-time: 271\n<\n<!DOCTYPE html>\n<html lang=\"en\">\n...\n```\n\n## 디버그 클라이언트 파드를 통한 확인\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 메쉬 내부 호출을 테스트해보려고 합니다. 따라서 각 노드마다 클라이언트 팟이 하나씩 있는 것이 좋습니다. 이를 위해 디버거 데몬세트 클라이언트 워크로드를 설정할 수 있습니다 — https://github.com/digitalocean/doks-debug\n\n사용한 수정된 Manifest: https://gist.github.com/JanaSabuj/a4dd2504752b8c2b30d2d2d05320f7ef\n\n저는 이 데몬세트를 우리 ambient-demo 네임스페이스에 배포했습니다.\n\n```js\n(⎈|kind-ambient:ambient-demo)➜  ~ kg ds\nNAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndoks-debug   3         3         3       3            3           <none>          89m\n\n(⎈|kind-ambient:ambient-demo)➜  ~ kgpo -owide\nNAME                      READY   STATUS    RESTARTS   AGE   IP               NODE                    NOMINATED NODE   READINESS GATES\ndoks-debug-j9mm5          1/1     Running   0          90m   192.168.246.5    ambient-worker2         <none>           <none>\ndoks-debug-rdhgq          1/1     Running   0          90m   192.168.184.73   ambient-worker          <none>           <none>\ndoks-debug-v7cld          1/1     Running   0          90m   192.168.208.3    ambient-control-plane   <none>\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테스트해 보기 위해 디버그 팟으로 진입하여 k8s fqdn httpbin.ambient-demo.svc.cluster.local을 curl을 시도해 보았습니다.\n\n200 OK를 반환하고 있습니다.\n\n```js\n(⎈|kind-ambient:ambient-demo)➜  ~ k exec -it doks-debug-j9mm5 -- /bin/bash\n\nroot@doks-debug-j9mm5:~# curl httpbin.ambient-demo.svc.cluster.local -v\n*   Trying 10.96.157.221:80...\n* Connected to httpbin.ambient-demo.svc.cluster.local (10.96.157.221) port 80 (#0)\n> GET / HTTP/1.1\n> Host: httpbin.ambient-demo.svc.cluster.local\n> User-Agent: curl/7.88.1\n> Accept: */*\n>\n< HTTP/1.1 200 OK\n< Server: gunicorn/19.9.0\n< Date: Sun, 16 Jun 2024 12:06:11 GMT\n< Connection: keep-alive\n< Content-Type: text/html; charset=utf-8\n< Content-Length: 9593\n< Access-Control-Allow-Origin: *\n< Access-Control-Allow-Credentials: true\n<\n```\n\n# Ambient Injection\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금까지 앰비언트 모드가 활성화되지 않았습니다. Istio 게이트웨이에서 추가 된 리스너로 인해 요청은 Istio 인그레스 파드에 도착한 다음 VirtualService를 통해 앱 파드로 라우팅됩니다.\n\n매쉬 내부 호출의 경우, 클라이언트와 서버 파드 사이에 직접적인 포드 간 통신이 이루어집니다.\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_8.png)\n\n앰비언트를 주입하는 동안 우리는 또한 다음 로그를 계속 추적할 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- istio-cni\n- ztunnel\n\nIstio가 네임스페이스 ambient-demo에 대한 Ambient Dataplane 모드를 활성화하도록 내부적으로 라우트, iptables 등을 설정했는지 확인하려면 아래 명령어를 사용해보세요.\n\n```js\n(⎈|kind-ambient:ambient-demo)➜\n~ kubectl label namespace ambient-demo istio.io/dataplane-mode=ambient\n```\n\n## 관찰된 로그\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- istio-cni\n\n```js\n(⎈|kind-ambient:ambient-demo)➜ ~ stern istio-cni -n istio-system\n\nistio-cni-node-7q6z7 install-cni 2024-06-16T10:13:59.243451Z info ambient Namespace ambient-demo is enabled in ambient mesh\nistio-cni-node-7q6z7 install-cni 2024-06-16T10:13:59.264500Z info ambient in pod mode - adding pod ambient-demo/httpbin-5bd875fbdd-84vs8 to ztunnel\nistio-cni-node-gwvl8 install-cni 2024-06-16T10:13:59.291505Z info ambient Namespace ambient-demo is enabled in ambient mesh\nistio-cni-node-fg6n4 install-cni 2024-06-16T10:13:59.281587Z info ambient Namespace ambient-demo is enabled in ambient mesh\nistio-cni-node-fg6n4 install-cni 2024-06-16T10:13:59.313668Z info ambient in pod mode - adding pod ambient-demo/httpbin-5bd875fbdd-dp4ct to ztunnel\n\nistio-cni-node-7q6z7 install-cni 2024-06-16T10:13:59.264500Z info ambient in pod mode - adding pod ambient-demo/httpbin-5bd875fbdd-84vs8 to ztunnel\nistio-cni-node-7q6z7 install-cni 2024-06-16T10:13:59.325907Z info iptables Running iptables-restore with the following input:\nistio-cni-node-7q6z7 install-cni * nat\nistio-cni-node-7q6z7 install-cni -N ISTIO_OUTPUT\nistio-cni-node-7q6z7 install-cni -A OUTPUT -j ISTIO_OUTPUT\nistio-cni-node-7q6z7 install-cni -A ISTIO_OUTPUT -d 169.254.7.127 -p tcp -m tcp -j ACCEPT\nistio-cni-node-7q6z7 install-cni -A ISTIO_OUTPUT -p tcp -m mark --mark 0x111/0xfff -j ACCEPT\nistio-cni-node-7q6z7 install-cni -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -j ACCEPT\nistio-cni-node-7q6z7 install-cni -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -p tcp -m mark ! --mark 0x539/0xfff -j REDIRECT --to-ports 15001\nistio-cni-node-7q6z7 install-cni COMMIT\nistio-cni-node-7q6z7 install-cni * mangle\nistio-cni-node-7q6z7 install-cni -N ISTIO_PRERT\nistio-cni-node-7q6z7 install-cni -N ISTIO_OUTPUT\nistio-cni-node-7q6z7 install-cni -A PREROUTING -j ISTIO_PRERT\nistio-cni-node-7q6z7 install-cni -A OUTPUT -j ISTIO_OUTPUT\nistio-cni-node-7q6z7 install-cni -A ISTIO_PRERT -m mark --mark 0x539/0xfff -j CONNMARK --set-xmark 0x111/0xfff\nistio-cni-node-7q6z7 install-cni -A ISTIO_PRERT -s 169.254.7.127 -p tcp -m tcp -j ACCEPT\nistio-cni-node-7q6z7 install-cni -A ISTIO_PRERT ! -d 127.0.0.1/32 -p tcp -i lo -j ACCEPT\nistio-cni-node-7q6z7 install-cni -A ISTIO_PRERT -p tcp -m tcp --dport 15008 -m mark ! --mark 0x539/0xfff -j TPROXY --on-port 15008 --tproxy-mark 0x111/0xfff\nistio-cni-node-7q6z7 install-cni -A ISTIO_PRERT -p tcp -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\nistio-cni-node-7q6z7 install-cni -A ISTIO_PRERT ! -d 127.0.0.1/32 -p tcp -m mark ! --mark 0x539/0xfff -j TPROXY --on-port 15006 --tproxy-mark 0x111/0xfff\nistio-cni-node-7q6z7 install-cni -A ISTIO_OUTPUT -m connmark --mark 0x111/0xfff -j CONNMARK --restore-mark --nfmask 0xffffffff --ctmask 0xffffffff\nistio-cni-node-7q6z7 install-cni COMMIT\nistio-cni-node-7q6z7 install-cni 2024-06-16T10:13:59.335115Z info Running command (with wait lock): iptables-restore --noflush -v --wait=30\nistio-cni-node-7q6z7 install-cni 2024-06-16T10:13:59.550687Z info ambient About to send added pod: 3ab72f78-8e2b-4e49-bc47-45fa4f90dbf7 to ztunnel: add:{uid:\"3ab72f78-8e2b-4e49-bc47-45fa4f90dbf7\" workload_info:{name:\"httpbin-5bd875fbdd-84vs8\" namespace:\"ambient-demo\" service_account:\"default\" trust_domain:\"cluster.local\"}\n```\n\n많은 로그가 생성되었습니다. 우리는 각 httpbin pod를 ambient-demo 네임스페이스에 추가하여 ztunnel을 통해 ambient 경로에 추가하고 있음을 확인할 수 있습니다.\n\n- ztunnel\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```plaintext\n(⎈|kind-ambient:ambient-demo)➜ ~ stern ztunnel -n istio-system\n\nztunnel-62hp8 istio-proxy 2024-06-16T10:13:59.559505Z 정보 inpod::statemanager pod WorkloadUid(\"3ab72f78-8e2b-4e49-bc47-45fa4f90dbf7\")가 netns를 수신하고 프록시를 시작합니다.\nztunnel-fv5f8 istio-proxy 2024-06-16T10:13:59.557545Z 정보 inpod::statemanager pod WorkloadUid(\"7f8be6a6-64f2-40e9-8926-6c3a618eb7d9\")가 netns를 수신하고 프록시를 시작합니다.\nztunnel-fv5f8 istio-proxy 2024-06-16T10:13:59.560458Z 정보 proxy::inbound 리스너가 구성되었으며 주소=[::]:15008 구성요소=\"inbound\" 투명=true\nztunnel-fv5f8 istio-proxy 2024-06-16T10:13:59.561604Z 정보 proxy::inbound_passthrough 리스너가 구성되었으며 주소=[::]:15006 구성요소=\"inbound plaintext\" 투명=true\nztunnel-fv5f8 istio-proxy 2024-06-16T10:13:59.561647Z 정보 proxy::outbound 리스너가 구성되었으며 주소=[::]:15001 구성요소=\"outbound\" 투명=true\nztunnel-62hp8 istio-proxy 2024-06-16T10:13:59.573883Z 정보 proxy::inbound 리스너가 구성되었으며 주소=[::]:15008 구성요소=\"inbound\" 투명=true\nztunnel-62hp8 istio-proxy 2024-06-16T10:13:59.586596Z 정보 proxy::inbound_passthrough 리스너가 구성되었으며 주소=[::]:15006 구성요소=\"inbound plaintext\" 투명=true\nztunnel-62hp8 istio-proxy 2024-06-16T10:13:59.586819Z 정보 proxy::outbound 리스너가 구성되었으며 주소=[::]:15001 구성요소=\"outbound\" 투명=true\nztunnel-fv5f8 istio-proxy 2024-06-16T10:13:59.811460Z 정보 xds::client:xds{id=14}가 응답을 수신했습니다. type_url=\"type.googleapis.com/istio.workload.Address\" 크기=2  삭제=0\nztunnel-62hp8 istio-proxy 2024-06-16T10:13:59.816352Z 정보 xds::client:xds{id=14}가 응답을 수신했습니다. type_url=\"type.googleapis.com/istio.workload.Address\" 크기=2  삭제=0\nztunnel-gs52c istio-proxy 2024-06-16T10:13:59.821310Z 정보 xds::client:xds{id=14}가 응답을 수신했습니다. type_url=\"type.googleapis.com/istio.workload.Address\" 크기=2  삭제=0\n```\n\nztunnel이 자체 내부 및 외부 리스너를 설정 중인 것으로 보입니다.\n\n## 트래픽 흐름 - Istio Ingress를 통해\n\n이전과 마찬가지로, istio-ingress pod로 포트 포워딩을 설정하고 localhost를 통해 액세스합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 개의 호출을 각각 2개의 httpbin 팟에 대응하도록 인그레스에서 호출을 추출하려고 합니다. 그리고 동시에 동일한 ztunnel 로그를 캡처하려고 합니다.\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ kpf istio-ingressgateway-6f48dfb7db-862sm 8081\nForwarding from 127.0.0.1:8081 -> 8081\n\n$ curl localhost:8081/\n$ curl localhost:8081/\n```\n\n```js\n(⎈|kind-ambient:istio-system)➜  ~ stern ztunnel -n istio-system\n\nztunnel-fv5f8 istio-proxy 2024-06-16T12:26:24.154500Z\ninfo access connection complete src.addr=192.168.184.70:52792\nsrc.workload=istio-ingressgateway-6f48dfb7db-862sm src.namespace=istio-system\nsrc.identity=\"spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\"\ndst.addr=192.168.184.74:80 dst.hbone_addr=192.168.184.74:80\ndst.service=httpbin.ambient-demo.svc.cluster.local\ndst.workload=httpbin-6f4dc97cb-swdlb dst.namespace=ambient-demo\ndst.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/httpbin-sa\"\ndirection=\"inbound\" bytes_sent=51083 bytes_recv=4180 duration=\"2166ms\"\n\nztunnel-62hp8 istio-proxy 2024-06-16T12:28:40.267690Z\ninfo access connection complete src.addr=192.168.184.70:55036\nsrc.workload=istio-ingressgateway-6f48dfb7db-862sm src.namespace=istio-system\nsrc.identity=\"spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\"\ndst.addr=192.168.246.6:80 dst.hbone_addr=192.168.246.6:80\ndst.service=httpbin.ambient-demo.svc.cluster.local\ndst.workload=httpbin-6f4dc97cb-5dpz9 dst.namespace=ambient-demo\ndst.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/httpbin-sa\"\ndirection=\"inbound\" bytes_sent=41251 bytes_recv=2007 duration=\"2331ms\"\n```\n\n인그레스가 주변 데이터 플레인 경로에 떨어지지 않기 때문에 인그레스 팟에서의 호출은 주변 데이터 플레인 경로로 직접 ztunnel 팟으로 이어집니다. 한 번에 하나의 노드로 이동한 후 해당 내부 노드 팟으로 inbound됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 로그 캡처에서 확인한 내용입니다\n\n```js\ndirection = \"inbound\";\n```\n\n- 이는 주변 레이블이 붙은 네임스페이스 파드로의 트래픽이 항상 ztunnel을 통해 이동함을 확인합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\ndst.hbone_addr=192.168.184.74:80\ndst.hbone_addr=192.168.246.6:80\n\nhttpbin-6f4dc97cb-5dpz9 1/1 Running 0 56m 192.168.246.6 ambient-worker2 <none> <none>\nhttpbin-6f4dc97cb-swdlb 1/1 Running 0 56m 192.168.184.74 ambient-worker <none> <none>\n\n- 이것은 실제 httpbin pod ip에 해당하는 dest pod ip를 캡처합니다.\n\n## 트래픽 흐름 — Mesh 내부를 통해\n\n클라이언트 디버그 pod에 exec하여 httpbin 서비스로의 Mesh 내부 호출을 시도하고 동시에 ztunnel 로그를 캡쳐하여 동일한 동작을 확인해보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n(⎈|kind-ambient:ambient-demo)➜  ~ kgpo -owide\nNAME                      READY   STATUS    RESTARTS   AGE    IP               NODE                    NOMINATED NODE   READINESS GATES\ndoks-debug-j9mm5          1/1     Running   0          122m   192.168.246.5    ambient-worker2         <none>           <none>\ndoks-debug-rdhgq          1/1     Running   0          122m   192.168.184.73   ambient-worker          <none>           <none>\ndoks-debug-v7cld          1/1     Running   0          122m   192.168.208.3    ambient-control-plane   <none>           <none>\nhttpbin-6f4dc97cb-5dpz9   1/1     Running   0          56m    192.168.246.6    ambient-worker2         <none>           <none>\nhttpbin-6f4dc97cb-swdlb   1/1     Running   0          56m    192.168.184.74   ambient-worker          <none>           <none>\n```\n\n그냥 같은 노드에 있는 클라이언트와 앱 팟들을 연결하기 위해 —\n\n```js\n(⎈|kind-ambient:ambient-demo)➜  ~ kgpo -A -owide | grep \"ambient-worker \"\nambient-demo         doks-debug-rdhgq                                1/1     Running   0               133m    192.168.184.73   ambient-worker          <none>           <none>\nistio-system         ztunnel-fv5f8                                   1/1     Running   0               3h24m   192.168.184.71   ambient-worker          <none>           <none>\nambient-demo         httpbin-6f4dc97cb-swdlb                         1/1     Running   0               68m     192.168.184.74   ambient-worker          <none>           <none>\n\n\n(⎈|kind-ambient:ambient-demo)➜  ~ kgpo -A -owide | grep \"ambient-worker2\"\nkube-system          doks-debug-9nlh7                                1/1     Running   0               3h6m    192.168.246.4    ambient-worker2         <none>           <none>\nistio-system         ztunnel-62hp8                                   1/1     Running   0               3h23m   192.168.246.3    ambient-worker2         <none>           <none>\nambient-demo         httpbin-6f4dc97cb-5dpz9                         1/1     Running   0               67m     192.168.246.6    ambient-worker2         <none>           <none>\n```\n\ndebug pod인 doks-debug-rdhgq를 실행하기 위해 ambient-worker 노드에 스케줄된 상태로 들어가보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_10.png\" />\n\n```js\n테이블 1: 동일 노드 내의 클라이언트 및 서버\n---------\nztunnel-fv5f8 istio-proxy 2024-06-16T12:40:48.707707Z info access connection complete src.addr=192.168.184.73:56463 src.workload=doks-debug-rdhgq src.namespace=ambient-demo src.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/default\" dst.addr=192.168.184.74:80 dst.hbone_addr=192.168.184.74:80 dst.service=httpbin.ambient-demo.svc.cluster.local dst.workload=httpbin-6f4dc97cb-swdlb dst.namespace=ambient-demo dst.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/httpbin-sa\" direction  =\"inbound\" bytes_sent=9832 bytes_recv=84 duration=\"178ms\"\nztunnel-fv5f8 istio-proxy 2024-06-16T12:40:48.708070Z info access connection complete src.addr=192.168.184.73:34190 src.workload=doks-debug-rdhgq src.namespace=ambient-demo src.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/default\" dst.addr=192.168.184.74:15008 dst.hbone_addr=192.168.184.74:80 dst.service=httpbin.ambient-demo.svc.cluster.local dst.workload=httpbin-6f4dc97cb-swdlb dst.namespace=ambient-demo dst.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/httpbin-sa\" direction=\"outbound\" bytes_sent=84 bytes_recv=9832 duration=\"203ms\"\n```\n\n동일 노드 내의 클라이언트와 서버의 경우, 파드에서 ztunnel로 외부 패킷이 들어오면 동일한 ztunnel을 통해 대상 파드로 들어오는 내부 패킷으로 리디렉션됩니다.\n\n따라서, 동일 노드 내의 클라이언트 및 서버에서는 한 ztunnel이 외부 패킷과 내부 패킷을 받습니다. \"bound\"는 동일 노드 내의 응용 프로그램 파드로부터/받는 방향을 지정합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n캡처 2: 클라이언트 및 서버가 다른 노드에 있는 경우\n---------\nztunnel-62hp8 istio-proxy 2024-06-16T12:48:51.792527Z info access connection complete src.addr=192.168.184.73:53265 src.workload=doks-debug-rdhgq src.namespace=ambient-demo src.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/default\" dst.addr=192.168.246.6:80 dst.hbone_addr=192.168.246.6:80 dst.service=httpbin.ambient-demo.svc.cluster.local dst.workload=httpbin-6f4dc97cb-5dpz9 dst.namespace=ambient-demo dst.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/httpbin-sa\" direction=\"inbound\" bytes_sent=9832 bytes_recv=84 duration=\"60ms\"\nztunnel-fv5f8 istio-proxy 2024-06-16T12:48:51.793112Z info access connection complete src.addr=192.168.184.73:60162 src.workload=doks-debug-rdhgq src.namespace=ambient-demo src.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/default\" dst.addr=192.168.246.6:15008 dst.hbone_addr=192.168.246.6:80 dst.service=httpbin.ambient-demo.svc.cluster.local dst.workload=httpbin-6f4dc97cb-5dpz9 dst.namespace=ambient-demo dst.identity=\"spiffe://cluster.local/ns/ambient-demo/sa/httpbin-sa\" direction=\"outbound\" bytes_sent=84 bytes_recv=9832 duration=\"61ms\"\n```\n\n만약 클라이언트와 서버가 다른 노드에 있는 경우, 출발 트래픽은 동일한 노드인 ztunnel에서 외부 트래픽 패킷을 만나고, 그런 다음 목적지 파드의 노드의 ztunnel로 전송되어 들어오는 패킷으로 처리됩니다.\n\n# 결론\n\n이 실험을 통해 Istio Ambient Mesh에서 ztunnel을 통한 패킷 흐름을 시각화할 수 있었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_11.png)\n\n다음 파트에서는 ztunnel을 통해 적용할 수 있는 L4 인증 정책을 탐색할 것입니다.\n\n또한 Ambient에 있는 L7 프록시인 Waypoint Proxy에 대해 탐구할 것입니다.\n\n다른 기술 블로그는 여기에서 확인할 수 있습니다: [링크](https://janasabuj.github.io/posts/)\n","ogImage":{"url":"/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_0.png"},"coverImage":"/assets/img/2024-06-23-TouringtheKubernetesIstioAmbientMeshPart1SetupZTunnel_0.png","tag":["Tech"],"readingTime":35},{"title":"클라우드 제공 로드밸런서 없이 Kong과 Gateway를 사용하여 Kubernetes 서비스에 접근하는 방법","description":"","date":"2024-06-23 23:10","slug":"2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer","content":"\n## 이 기사에서는 쿠버네티스 클러스터 내에서 Kong을 API 게이트웨이로 배포하여 서비스에 관리된 액세스를 제공하는 방법을 살펴봅니다. 이를 클라우드 서비스에서 수행합니다. 이 클라우드 서비스는 쿠버네티스 호환 외부 로드 밸런서 서비스를 제공하지 않습니다. 또한 Ingress 리소스 대신 최신 Kubernetes Gateway를 사용합니다.\n\n저와 이전 기사 중 한 가지 이상을 따르신 분들은 저의 이전 기사들 중 하나를 따라오셨을 것입니다. 저는 신뢰할 수 있고 비용 효율적이지만 제한된 범위의 서비스를 제공하는 호주 클라우드 제공자인 Binary Lane을 사용합니다.\n\n제한된 범위의 서비스만 제공하는 것은 모든 쿠버네티스 작업을 직접해야 하므로, 배우고 솔루션의 작동 방식을 제어할 수 있는 기회를 제공합니다. 또한 어떤 클라우드 공급 업체에도 얽매이지 않을 수 있습니다. 또한 비용 효율적입니다.\n\n이 기사에서는 Kubernetes 클러스터에 Kong API 게이트웨이를 추가하여 서비스에 액세스하는 방법을 살펴봅니다. 할 일이 꽤 많기 때문에 이 기사는 좀 길지만, API 게이트웨이의 역할에 대한 이론 부분을 별도의 기사로 분리했습니다. API 게이트웨이의 역할을 이해하지 못하신다면 먼저 해당 기사를 읽는 것을 권장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKong은 신뢰할 만한 엔터프라이즈급 API 게이트웨이이지만 설정하기가 매우 까다로울 수 있습니다. 이 기사의 끝에서 문제를 디버그하는 방법에 대한 일부 힌트를 제공하겠습니다. 이 기사에서 설계를 조정하는 경우, 이름과 포트를 올바르게 구성했는지 확인하세요.\n\n# 네트워크 디자인\n\nKubernetes 네트워킹은 복잡한 주제이며 여기서 다루기 어렵지만, 고수준에서 네트워크 디자인에 대해 생각해야 합니다.\n\n![네트워크 디자인](/assets/img/2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이전 기사를 따라오셨다면, 이제 이진 레인(또는 다른 클라우드 제공업체) 서버에 Kubernetes 클러스터가 설치되어 있어야 합니다. 인터넷에서 접근이 불가능한 가상 사설 클라우드 (VPC) 개인 서브넷에 세 개의 노드가 설치되어 있을 것입니다. 이러한 노드들은 인터넷을 통해 접속이 가능한 전용 VPN을 통해서만 연결됩니다(위에는 표시되지 않음). 인터넷 및 VPC 인터페이스를 모두 가지고 있는 게이트웨이 서버가 있어서, 인터넷에서 클러스터로 들어오는 접속(inress)과 클러스터에서 인터넷으로 나가는 접속(egress)을 제공합니다.\n\n기본 네트워크 토폴로지가 이제 갖추어 졌습니다. 이제 우리는 서비스가 외부 세계에 제공하는 API를 관리할 수 있기를 원합니다. 이 작업은 Kong Gateway API를 통해 수행됩니다.\n\n# 인터넷에서 연결 설정하기 (인그레스)\n\nAWS, 구글 클라우드 또는 Azure와 같은 풀 서비스 제공업체를 사용하면, LoadBalancer 유형의 Kubernetes 서비스를 사용하여 인터넷 연결이 자동으로 생성되는 방식으로 Kubernetes를 설정할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n바이너리 레인에는 로드 밸런서 서비스가 있지만 쿠버네티스를 통해 관리할 수는 없으므로 로드 밸런서를 직접 생성하고 구성하거나 고유한 인그레스 포인트를 만들어야 합니다. 특정 클라우드 제공 업체의 기능에 구속되지 않기 위해, 저는 개인적으로 내 gw 서버에서 NGINX 역방향 프록시를 실행하여 고유한 인그레스 포인트를 만드는 것을 선호합니다.\n\n이 아키텍처에서 gw 서버에서 실행되는 NGINX는 두 가지 기능을 수행합니다:\n\n- 유효한 요청을 모두 쿠버네티스 클러스터로 라우팅하여 Kong이 처리\n- 쿠버네티스 노드 간 요청을 로드 밸런싱\n\nKong이 NodePort 서비스로 노출될 것이므로 클러스터의 모든 노드에서 액세스할 수 있습니다. 이를 통해 NGINX가 노드 간 요청을 로드 밸런싱할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n서비스 자체가 사용 가능한 파드 전체에 무작위로 로드 밸런싱을 수행하므로 NGINX에 의한 로드 밸런싱이 노드 장애나 과부하 상황을 견딜 목적으로만 사용된다는 것을 유의한 점입니다. 서비스 로드 밸런싱에 대해 더 읽어보실 수 있습니다.\n\n본 솔루션에서 NGINX를 수동으로 구성된, 대체될 수 있는 외부 로드 밸런서로 간주하실 수 있습니다.\n\n# 쿠버네티스 서비스\n\n쿠버네티스 서비스는 하나 이상의 파드가 제공하는 서비스에 대한 액세스를 허용합니다. 이를 통해 파드가 종료되고 재예약되더라도 특정 노드에서 요청이 발생하더라도 서비스가 계속하여 요구에 따른 대로 요청을 라우팅하는 단일 접점으로 유지됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/assets/img/2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer_1.png)\n\n서비스를 사용함으로써 요청을 보낼 수 있는 단일하고 안정적인 IP 주소가 제공됩니다. 서비스는 사용 가능한 Pod들 사이에서 부하 분산 기능을 제공합니다. Kubernetes는 또한 클러스터의 DNS에 서비스에 대한 참조를 추가함으로써 서비스가 이름으로 액세스될 수 있게 합니다. 여러 가지 다른 변형이 등록됩니다:\n\n```js\n<서비스 이름>.<네임스페이스>.svc.local\n<서비스 이름>.<네임스페이스>.svc\n<서비스 이름>.<네임스페이스>\n<서비스 이름>\n```\n\n# API 게이트웨이\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nNGINX 게이트웨이와 쿠버네티스 서비스는 서비스에 대한 외부 인터페이스를 제공하는 데 도움이 되지만 기능이 제한적이며 수동으로 설정해야 합니다.\n\nAPI 게이트웨이는 이 문제를 해결하는 클러스터 구성 요소입니다. 이 게이트웨이는 솔루션의 일부로 구성되며 서비스 앞에 위치하여 여기서 설명하는 추가 기능을 제공합니다.\n\n![image](/assets/img/2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer_2.png)\n\nAPI 게이트웨이는 클러스터의 일부로 있기 때문에 클러스터 내 리소스의 변경에 따라 자동으로 구성될 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Kong API Gateway\n\n다양한 API 게이트웨이 기술이 있지만, 이 글에서는 무료 오픈 소스 솔루션인 Kong을 선택했습니다. 유료 엔터프라이즈 설치도 가능합니다. 여기에서 Kong에 대한 포괄적인 공식 문서를 찾을 수 있습니다.\n\nKong은 쿠버네티스 커뮤니티와 적극적으로 협력하여 클러스터 내에서 게이트웨이에 대한 새로운 표준을 정의하고 있습니다. 이로 인해 게이트웨이 자체와 혼동되어서는 안 되는 새로운 쿠버네티스 리소스 유형인 게이트웨이 API가 만들어졌습니다.\n\n이것이 Kong이 어떻게 작동하는지 대략적으로 설명했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer_3.png)\n\n클러스터 외부의 모든 클라이언트로부터 들어오는 트래픽이 Kong에 도달합니다. Kong은 구성 내의 규칙에 따라 요청을 처리하고 클러스터 내외의 적절한 서비스로 요청을 전달합니다.\n\nKong은 Kubernetes 리소스 매니페스트에서 정적으로 또는 데이터베이스에서 구성을 가져올 수 있습니다 (DB-less 설치). Kong은 이제 DB-less 설치를 새로운 설치에 사용할 것을 권장하며, 이를 따를 것입니다.\n\nKong은 성숙한 플러그인 기능을 갖추고 있습니다. 이를 통해 제3자가 Kong의 플러그인으로 기능 확장을 개발할 수 있습니다. 플러그인은 트래픽 흐름에 위치하여 속도 제한 및 인증과 같은 작업에 도움을 줄 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로 Kong을 관리하기 위해 플러그인, 구성 등을 관리할 수 있도록 Management UI를 제공합니다. Management UI는 Admin API를 통해 Kong과 상호 작용합니다.\n\nKong에 대해 상세한 문서를 살펴보면 여기서 다룰 수 있는 내용보다 더 많음을 알 수 있습니다. 그래서 제가 다루는 내용은 기본 사항에만 초점을 맞추겠습니다.\n\n## DB-less 설치\n\nDB-less 설치가 어떻게 작동하는지 이해하는 것이 중요하다고 생 생각합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 구성에서 Kong은 두 가지 구성요소를 설치합니다:\n\n- Kong Ingress Controller (KIC)\n- Kong Gateway\n\n게이트웨이는 프록시를 통해 모든 사용자 트래픽의 경로 지정을 처리합니다. Kong Ingress Controller (KIC)는 Kubernetes 리소스 정의 (예: HTTPRoute)에서 구성을 가져와서 프록시가 이해하는 규칙으로 변환하고 실시간으로 프록시에 규칙을 업로드합니다. 이러한 방식으로 Kubernetes 구성의 변경 사항이 자동으로 프록시에 적용됩니다.\n\nKIC는 내부 Kubernetes API를 사용하여 Kubernetes 클러스터에 대한 정보를 얻습니다. 이 API는 클러스터를 관리하는 데 사용되는 것으로, kubectl을 사용할 때 실제로는 Kubernetes API와 상호 작용합니다. 이 API를 통해 Kong 및 kubectl과 같은 애플리케이션은 클러스터에 대한 정보를 찾거나 변경할 수 있습니다. KIC는 이 API를 통해 백업 데이터베이스가 필요 없이 클러스터 리소스 파일과 Gateway를 동기화할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 테스트할 서비스\n\nKong 배포에 들어가기 전에 Kong을 통해 접근할 수 있는 서비스를 가지고 있어야 합니다. 어차피 API Gateway에 API가 없다면 그리 유용하지 않을 것이니까요!\n\n가장 간단한 방법은 NGINX를 웹 서버로 배포하고 정적 콘텐츠로 구성하는 것입니다. Kubernetes에서 이 작업을 한 적이 없다면 다른 기사 하나에서 그 방법을 읽어볼 수 있습니다.\n\n2개의 서비스를 생성하는 것을 제안합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 안녕하세요 world 1: 2개의 레플리카\n- 안녕하세요 world 2: 1개의 레플리카\n\n이들은 클러스터 내 파드에서 ClusterIP 서비스를 통해 접근할 수 있어야 합니다. 이러한 서비스를 설정하는 내 기사에서는 브라우저에서 서비스를 확인할 수 있도록 NodePort 서비스를 생성합니다. 이를 수행할 경우 내부 클러스터 IP 및 포트를 사용해야 합니다. 서비스 유형에 대한 자세한 내용은 다른 기사에서 확인할 수 있습니다.\n\n두 서비스가 올바르게 실행되고 Hello World HTML을 제공할 수 있는지 확인하세요.\n\n내가 여기서 설명하는 예제에서, 내 두 서비스는 다음과 같이 위치해 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- http://`node IP 주소`:30082 — 'Hello World 1 !!'라고 응답합니다.\n- http://`node IP 주소`:30082 — 'Hello World 2 !!'라고 응답합니다.\n\n어느 경로도 필요하지 않으며 경로를 추가하면 (예: http://`node IP 주소`:30082/world1) 404 오류가 발생합니다. 이 사실을 인식하지 못하면 나중에 문제가 될 수 있으므로 주의해야 합니다.\n\n이제 서비스가 실행 중이므로 Kong을 통해 액세스해 보겠습니다.\n\n# 쿠버네티스 게이트웨이 자원 생성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDB 미사용 모드에서는 Kong API 게이트웨이를 구성할 때 Kubernetes Ingress 또는 HTTPRoute 리소스를 생성합니다. 이러한 리소스를 클러스터에 적용하면 Kong이 프록시 구성 요소 내에서 라우팅 규칙을 정의하는 데 사용됩니다. 이를 통해 들어오는 트래픽이 서비스로 전달됩니다.\n\nIngress 리소스는 작동하지만 기능이 제한적입니다. Kubernetes 커뮤니티와 함께 Kong에서 개발한 새 Gateway 리소스를 사용하면 API를 더 정교하게 관리할 수 있습니다.\n\n우리는 Kong과 함께 Gateway 리소스를 사용할 것입니다. 이를 위해 먼저 GatewayClass 및 Gateway 리소스를 지원하는 새로운 Custom Resource Definitions (CRD)를 클러스터에 적용해야 합니다. 클러스터에서 kubectl을 실행하는 위치에서 다음 명령을 실행하여 이 작업을 수행할 수 있습니다. 저의 경우에는 제 k8s-master 서버에서 이를 실행합니다.\n\n```js\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n본 문서에서는 사용하지 않을 실험적 기능 몇 가지를 소개해드리겠습니다만, 참고용으로 여기에 추가해두었습니다.\n\n```js\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/experimental-install.yaml\n```\n\n이제 Kubernetes를 위해 GatewayClass 및 Gateway 두 리소스를 정의할 수 있습니다. 이들이 무엇을 하는 지에 대해 설명했으니, 이를 다시 반복하지는 않겠습니다. 간결함을 위해 해당 내용은 여기서 생략합니다.\n\n## GatewayClass\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKong 기술을 클러스터에 소개하는 GatewayClass를 정의할 것입니다. 다음 파일을 생성해주세요:\n\nkong-gw-class.yml\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: kong-class\n  annotations:\n    konghq.com/gatewayclass-unmanaged: \"true\"\nspec:\n  controllerName: konghq.com/kic-gateway-controller\n```\n\n이 파일에 대해 몇 가지 주의할 사항이 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 이것은 클러스터 수준 리소스이기 때문에 네임스페이스가 없습니다.\n- 주석은 솔루션별 옵션을 정의하며 Kong의 경우 konghq로 시작합니다.\n- konghq.com/gatewayclass-unmanaged 주석은 'true'(문자열)로 설정되어 있습니다. 왜냐하면 Kong이 오퍼레이터를 통해 자동으로 설정되는 것이 아니라 수동으로 설정되고 있기 때문입니다.(다른 옵션도 있으니 여기를 참조하세요)\n- 인그레스 컨트롤러는 Kong 인그레스 컨트롤러(konghq.com/kic-gateway-controller)이며 contollerName 필드에서 구성됩니다.\n\n이제 다음과 같이 클래스를 생성하세요:\n\n```js\nkubectl apply -f kong-gw-class.yml\n```\n\n이제 이 클래스를 사용하는 게이트웨이를 생성할 수 있습니다. 동일한 GatewayClass를 참조하는 여러 Gateway 인스턴스를 생성할 수 있다는 점을 유의하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 게이트웨이\n\n수동으로 설치된 Kong 게이트웨이의 경우 (우리가 생성중인 것과 같이), 다음 파일을 만들어야 합니다:\n\nkong-gw-gateway.yml\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: kong-gateway\n  namespace: kong\nspec:\n  gatewayClassName: kong-class\n  listeners:\n    - name: world-selector\n      hostname: worlds.com\n      port: 80\n      protocol: HTTP\n      allowedRoutes:\n        namespaces:\n          from: All\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파일에서 유의해야 할 몇 가지 사항이 또 있습니다:\n\n- 나중에 참조할 수 있는 이름이 있습니다 (kong-gateway)\n- GatewayClass는 위에서 생성한 GatewayClass의 이름을 의미합니다\n- 이 Gateway는 이 API Gateway의 진입점인 하나의 리스너만 정의합니다\n- 리스너에는 URL 호환성이 있는 고유한 이름이 지정됩니다\n- 리스너는 포트 80에 바인드됩니다\n- 호스트명은 일치 필드로 사용되며 옵션입니다\n- 이 리스너에 연결할 서비스(allowedRoutes)를 제어할 수 있으며 해당 서비스들은 네임스페이스를 통해 연결됩니다 - 동일한 네임스페이스를 기본으로 사용하여 다른 네임스페이스로 연결하기 위해 모든 네임스페이스로 변경됩니다\n- Gateway 사양은 게이트웨이가 HTTP를 통해 단일 포트(80)에서 수신하는 것을 예상합니다.\n\nGateways는 네임스페이스에 특정하며 API Gateway를 설치하기 전에 생성해야 합니다:\n\n```js\nkubectl create namespace kong\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 다음 명령을 사용하여 리소스를 만드세요:\n\n```js\nkubectl apply -f kong-gw-gateway.yml\n```\n\n이제 GatewayClass 및 Gateway 리소스가 정의되었으므로, 애플리케이션 자체를 설치하여 이 두 리소스의 구현을 형성할 수 있습니다.\n\n# Kong 설치\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지금은 Helm 차트를 사용하여 Kong을 설치할 것입니다. 만약 Helm이 없다면, Helm을 설치하는 방법은 여기에서 찾을 수 있습니다.\n\n## Kong CRDs\n\nKong을 설치하기 전에 Kong Custom Resource Definitions (CRDs)를 설치해야 합니다. 이 작업은 클러스터에서 kubectl을 실행하는 위치에서 다음 명령을 실행하여 수행할 수 있습니다. 제 경우에는 k8s-master 서버에서 이 작업을 수행하고 있습니다.\n\n```js\nkubectl apply -k https://github.com/Kong/kubernetes-ingress-controller/config/crd\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Kong Application\n\nKong을 Kubernetes 클러스터에 설치할 때, 두 가지 구성 요소가 설치됩니다:\n\n- Kong 인그레스 컨트롤러 (KIC) — 쿠버네티스 리소스 정의를 Kong 게이트웨이 구성으로 변환합니다.\n- Kong 게이트웨이 — Kong 인그레스 컨트롤러 (KIC)에 의해 삽입된 구성을 기반으로 서비스로의 라우팅을 담당합니다.\n\n먼저, 로컬 헬름에 Kong 저장소를 추가하십시오:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nhelm repo add kong https://charts.konghq.com\nhelm repo update\n```\n\n만약 다음 명령어로 Helm 차트를 검색하면:\n\n```js\nhelm search repo kong\n```\n\n두 개의 항목을 찾을 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n이름           차트 버전  앱 버전  설명\nkong/kong    2.33.3    3.5       클라우드 네이티브 인그레스 및 API 관리\nkong/ingress 0.10.2    3.4       콩 인그레스 컨트롤러 및 콩 게이트웨이 배포\n```\n\nDB 레스 구성을 사용할 것이므로 kong/ingress를 사용할 것입니다. 설치하기 전에 몇 가지 값을 재정의해야 합니다. 다음 파일을 만들어주세요:\n\nkong-values.yml\n\n```js\n#controller:\n#  ingressController:\n#    env:\n#      LOG_LEVEL: trace\n#      dump_config: true\n\ngateway:\n  admin:\n    http:\n      enabled: true\n  proxy:\n    type: NodePort\n    http:\n      enabled: true\n      nodePort: 32001\n    tls:\n      enabled: false\n#  ingressController:\n#    env:\n#      LOG_LEVEL: trace\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKIC 및 Kong Gateway를 부모 Helm 차트를 통해 설치하고 있기 때문에 이 두 애플리케이션의 구성은 각각 컨트롤러 및 게이트웨이 레이블 아래에 있습니다. 컨트롤러는 단순히 알림으로 남겨두었습니다.\n\n또한, 주석 처리된 여러 줄을 볼 수 있습니다. 이것들은 Pod 로그를 통해 무엇이 발생하는지 디버그하고 싶을 때 유용합니다.\n\nBinary Lane은 Kubernetes가 구성할 수 있는 로드 밸런서를 제공하지 않기 때문에 프록시 구성을 재정의하고 있습니다. Kubernetes에게 LoadBalancer 서비스 대신 NodePort 서비스를 설정하도록 지시하고 있습니다. 게이트웨이를 클러스터의 모든 노드에서 사용할 수 있도록 포트 32001에 노출하고 있습니다.\n\n이전에 kong 네임스페이스를 생성했으므로 이제 Kong을 설치할 준비가 되었습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nhelm install kong kong/ingress -f kong-values.yml -n kong\n```\n\n이제 설치가 예상대로 작동하는지 확인할 수 있습니다. 준비되는 데 1-2분 정도 걸릴 수 있습니다:\n\n```js\nkubectl get all -n kong\n```\n\n다음과 같은 결과를 얻어야 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n### NAME READY STATUS RESTARTS AGE\n\n- pod/kong-controller-68cddcbcb7-z46lh 1/1 Running 0 45s\n- pod/kong-gateway-687c5b78db-5qvgd 1/1 Running 0 45s\n\n### NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\n\n- service/kong-controller-validation-webhook ClusterIP 10.110.172.40 <none> 443/TCP 46s\n- service/kong-gateway-admin ClusterIP None <none> 8444/TCP 46s\n- service/kong-gateway-manager NodePort 10.100.254.169 <none> 8002:30698/TCP,8445:30393/TCP 46s\n- service/kong-gateway-proxy NodePort 10.96.24.196 <none> 80:32001/TCP 46s\n\n### NAME READY UP-TO-DATE AVAILABLE AGE\n\n- deployment.apps/kong-controller 1/1 1 1 45s\n- deployment.apps/kong-gateway 1/1 1 1 45s\n\n### NAME DESIRED CURRENT READY AGE\n\n- replicaset.apps/kong-controller-68cddcbcb7 1 1 1 45s\n- replicaset.apps/kong-gateway-687c5b78db 1 1 1 45s\n\nManagement UI 서비스가 NodePort를 통해 노출됩니다. 이는 관리 API를 볼 수 있는 것을 기대하고 작동하지 않을 것입니다. DB-less 설치를 하고 있기 때문에, 관리 UI의 유일한 사용은 설정을 확인하는 것뿐입니다.\n\n클러스터 내 노드에서 프록시 주소를 curl로 테스트할 수 있습니다:\n\n```js\ncurl localhost:32001\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n요청한 표는 Markdown 형식으로 변경해야 합니다.\n\n```json\n{\n  \"message\": \"해당 값으로 일치하는 경로가 없습니다.\",\n  \"request_id\": \"7fc9db053e3029105581890e81effe12\"\n}\n```\n\n요청 ID는 해당 거래에 고유하며 curl 명령을 다시 실행하면 다른 값을 볼 수 있습니다. 이는 Kong에서 추가되어 시스템을 통해 요청을 추적할 수 있게 합니다. 멋지죠?\n\n이제 새 API 게이트웨이를 구성하여 이전에 생성한 테스트 서비스로 요청을 라우트할 준비가 되었습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 라우트 추가하기\n\n게이트웨이 리소스에 라우트를 추가하려면 HTTPRoute 리소스를 사용합니다. 다른 수준의 라우팅을 위한 다른 리소스 유형도 있습니다. 이제 worlds.com/world1을 hello-world-1-svc에, worlds.com/world2를 hello-world-2-svc에 연결하기 위해 이러한 리소스 중 하나를 생성할 것입니다.\n\n저는 하나의 HTTPRoute 리소스를 설명하겠고, 다른 하나는 여러분에게 만들어 달라고 요청할 것입니다. 리소스 파일을 생성해주세요:\n\nhello-world-1-route.yml\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: example-1\n  annotations:\n    konghq.com/strip-path: \"true\"\nspec:\n  parentRefs:\n    - name: kong-gateway\n      namespace: kong\n  hostnames:\n    - worlds.com\n  rules:\n    - matches:\n        - path:\n            type: PathPrefix\n            value: /world1\n      backendRefs:\n        - name: hello-world-1-svc\n          port: 80\n          kind: Service\n```\n\n이 파일에서는 'true'로 설정된 Kong 특정 주석인 konghq.com/strip-path를 추가했습니다. 이는 수신된 일치하는 경로를 요청에서 southbound 서비스로 줄일 것입니다. 다른 줄에는 다음이 포함되어 있습니다:\n\n- 사용할 게이트웨이의 정의(ParentRefs에서)는 이름과 네임스페이스로 참조됩니다.\n- 게이트웨이에서 적절한 수신기에 일치시킬 호스트명에 대한 선택적 참조\n- 이 경로에 대해 들어오는 요청과 일치시키는 규칙\n- 요청을 이 일치에 대해 경로지정할 서비스를 정의하는 backendRefs(서비스의 내부 DNS 이름이름이며 포트는 서비스에 대한 매핑되지 않은 클러스터 IP 포트임을 주의하세요)\n\n이 경로에서 일치는 /world1의 접두사이며, 그 후 서비스로 전달되기 전에 제거됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 다음 라우트를 만듭니다:\n\n```js\nkubectl apply -f hello-world-1-route.yml\n```\n\nNodePort 서비스를 사용하여 게이트웨이를 만들었습니다. 이제 서비스를 테스트할 수 있습니다. NodePort 서비스는 클러스터의 모든 노드에서 사용할 수 있습니다. 보통 저는 k8s-master 노드를 사용하지만 다른 노드도 사용할 수 있습니다. 다음 명령어로 테스트할 수 있습니다:\n\n```js\ncurl -H \"Host: worlds.com\" <k8s-master IP 주소>:32001/world2\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n원하는 요청 라우팅을 위해 Host 헤더로 호스트명을 worlds.com으로 설정했습니다. 테스트 서비스 응답이 돌아오는 것을 확인할 수 있어야 합니다.\n\n이제 두 번째 HTTPRoute 리소스를 추가하여 두 번째 서비스의 요청을 관리할 수 있습니다.\n\n이제 클러스터 노드에서 서비스에 액세스할 수 있으므로 최종 단계 진행할 수 있습니다 - gw 서버 구성.\n\n# 인그레스 지점 구성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 AWS, Azure, 또는 Google Cloud에서 작업 중이었다면, Gateway 서비스를 LoadBalancer로 유지하고 자동으로 인그레스 포인트가 생성되도록 할 수 있었을 텐데 Binary Lane에서 작업 중이므로 직접 만들어야 합니다.\n\n제 글을 따라오셨다면 알겠지만, 저희는 클러스터로부터 인터넷으로의 인그레스 포인트로 작용하는 gw 서버가 있다는 것을 알고 계실 것입니다. 이 서버는 간단하게 NGINX를 사용하여 구성되어 있습니다.\n\n우리는 이것을 모든 요청을 라운드 로빈 로드 밸런서를 사용하여 클러스터 내 모든 노드로 경로를 설정하도록 구성할 것입니다.\n\ngw 서버에 로그인하고 root 사용자로 다음 파일을 업데이트하십시오 (``에 자신의 값으로 필드를 교체하는 것을 잊지 마세요):\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 worlds.conf 파일의 내용입니다.\n\n```js\nupstream k8s_cluster {\n  server <k8s-master>:32001;\n  server <k8s-node1>:32001;\n  server <k8s-node2>:32001;\n}\n\nserver {\n    listen 80;\n    listen [::]:80;\n\n    server_name worlds.com;\n\n    location / {\n        proxy_pass http://k8s_cluster;\n        include proxy_params;\n    }\n}\n```\n\n일반적으로 프록시 매개변수는 별도의 파일에 설정됩니다:\n\n`/etc/nginx/proxy_params`\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nproxy_set_header Host $http_host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n```\n\n이렇게 하면 Host 헤더 및 기타 세부 정보가 전달되어 라우팅이 효율적으로 작동할 수 있습니다.\n\n이제 사이트를 활성화하고 구성을 테스트한 다음 NGINX를 재시작하십시오:\n\n```js\nln -s /etc/nginx/sites-available/worlds.conf /etc/nginx/sites-enabled/\nnginx -t\nsystemctl restart nginx\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음과 같이 테스트할 수 있습니다 ( ' '필드를 귀하의 값으로 대체하세요):\n\n```js\ncurl -H \"Host: worlds.com\" <gw 서버 공인 IP 주소>/world1\n```\n\n서버로부터 응답을 받아야 합니다.\n\n축하합니다! 이제 콩(Kong)을 설치하고 서비스에 연결하도록 구성하는 데 성공했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Kong 디버깅\n\n만약 Kong에서 문제가 발생하면 디버깅하는 것이 어려울 수 있어요. 제가 Kong 설치 과정에서 발견한 몇 가지 지침을 공유해드릴게요:\n\n- GatewayClass, Gateway, 그리고 controller/gateway 포드에 kubectl describe를 사용해서 결과물을 주의깊게 살펴보세요. 이런 방법을 이용해 해결책을 찾을 때까지 곤란한 상황에 직면한 적이 있어요.\n- controller와 gateway 로그를 다음과 같이 확인해보세요:\n\n```js\nkubectl logs <pod name> -n kong\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- kong-values.yml 파일을 사용하여 로깅 레벨을 높이세요 (이전에 보여준 라인의 주석을 제거하세요)\n- NodePort 주소를 얻기 위해 kubectl get svc -n kong를 사용하여 관리 UI에 접속하세요 — HTTP 포트를 사용하고 Admin API를 포트 포워딩하세요 (서비스를 외부로 바인딩하기 위해 --address 옵션을 추가하세요):\n\n```js\nkubectl port--forward <게이트웨이 파드 이름> 8001:8001 --address <k8s-마스터 IP 주소>\n```\n\n- 포트 8001을 포워딩한 이후, Postman와 같은 REST API 도구를 사용하여 Admin UI에 접속하세요\n- 접속할 수 있는 디버그 포트가 있습니다:\n\n```js\nkubectl port-forward -n kong <컨트롤러 파드 이름>  10256:10256 --address <k8s-마스터 IP 주소>\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 요약\n\n이 글은 API Gateway인 Kong을 설치하기 위해 필요한 모든 단계를 함께 수행해야 하기 때문에 길었습니다.\n\n이 글에서 우리는:\n\n- 네트워크 토폴로지를 검토했습니다.\n- 서비스가 서비스에 접근하는 데 도움이 되는 방법을 살펐습니다.\n- Kong이 Kubernetes와 어떻게 작동하는지 살펐습니다.\n- 사용할 테스트 서비스를 생성했습니다.\n- GatewayClass 및 Gateway 리소스를 설치하고 구성했습니다.\n- Kong을 설치하고 구성했습니다.\n- 자체 외부 로드 밸런서를 구성했습니다.\n- API Gateway 설치 문제를 해결하는 방법을 고려했습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마침내 저희는 Kong API 게이트웨이를 통해 인터넷에서 저희의 테스트 서비스에 접속할 수 있었습니다.\n\n이 기사가 흥미롭게 여겨진다면 박수를 부탁드립니다. 이는 미래에 어떤 기사를 쓸지 판단하는 데 도움이 됩니다. 의견이 있으시면 댓글에 남겨주시기 바랍니다.\n","ogImage":{"url":"/assets/img/2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer_0.png"},"coverImage":"/assets/img/2024-06-23-UsingKongtoaccessKubernetesservicesusingaGatewayresourcewithnocloudprovidedLoadBalancer_0.png","tag":["Tech"],"readingTime":27},{"title":"도커 핵심 개념 완벽 정리","description":"","date":"2024-06-23 23:08","slug":"2024-06-23-DockerFundamentals","content":"\n도커는 개발자가 컨테이너에서 애플리케이션을 빌드, 배포 및 실행할 수 있도록 하는 플랫폼입니다. 컨테이너는 가벼우며 이식 가능하고 효율적이어서 현대적인 애플리케이션 개발과 배포에 인기가 많습니다.\n\nbuymeacoffee ☕ 👈 해당 링크를 클릭해 주세요\n\n## 컨테이너 & 가상 머신(VM)\n\n컨테이너와 가상 머신(VM)은 모두 애플리케이션에 대한 격리된 환경을 제공하지만, 그들은 크게 다릅니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- VMs: 전체 하드웨어 및 운영 체제를 가상화합니다. 무겁고 더 많은 자원을 사용합니다.\n- Containers: 운영 체제를 가상화하고 호스트 머신과 커널을 공유하지만 애플리케이션 프로세스를 격리합니다. 가벼우며 적은 자원을 사용합니다.\n\n## 전통적인 배포와의 도전 과제\n\n전통적인 배포 방법은 종종 다음과 같은 도전 과제에 직면합니다:\n\n- 의존성 충돌: 다른 애플리케이션이 서로 다른 라이브러리나 의존성 버전을 필요로 하여 충돌을 일으킬 수 있습니다.\n- 환경 일관성: 구성의 차이로 인해 응용 프로그램이 다양한 환경에서 다르게 동작할 수 있습니다.\n- 확장성: 여러 서버에 걸쳐 응용 프로그램을 확장하는 것은 복잡하고 자원을 많이 사용할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨테이너화는 애플리케이션과 그 종속성을 컨테이너에 캡슐화하여 일관성과 이식성을 환경 전반에 걸쳐 보장함으로써 이러한 도전 과제에 대응합니다.\n\n도커 아키텍처 이해하기\n\n도커는 클라이언트-서버 아키텍처를 따르며, 주요 구성 요소는 도커 클라이언트, 도커 데몬 및 도커 레지스트리입니다.\n\n![이미지](/assets/img/2024-06-23-DockerFundamentals_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커는 여러 주요 구성 요소를 사용하는 클라이언트-서버 아키텍처를 활용합니다:\n\n- 도커 클라이언트: 도커와 상호 작용하는 데 사용하는 명령줄 인터페이스(CLI) 도구입니다. 컨테이너를 빌드, 실행 및 관리할 수 있습니다. 이것은 도커를 원격으로 제어하는 것으로 생각할 수 있습니다.\n- 도커 엔진 (데몬): 이 소프트웨어는 시스템에서 실행되며 컨테이너의 빌드, 실행 및 배포를 관리합니다. 도커 클라이언트로부터 명령을 수신하고 그에 대해 행동에 옮깁니다.\n- 도커 호스트: 도커가 설치된 물리적인 머신(또는 가상 머신)입니다. 도커 컨테이너를 실행하는 데 필요한 리소스 및 환경을 제공합니다.\n\n도커 오브젝트 (이미지 및 컨테이너):\n\n- 도커 이미지: 도커 컨테이너를 만드는 데 필요한 지침이 포함된 청사진입니다. 어플리케이션을 실행하는 데 필요한 환경(운영 체제, 라이브러리, 응용 프로그램 코드)을 정의합니다. 이것은 어플리케이션 환경을 만드는 레시피로 상상할 수 있습니다.\n- 도커 컨테이너: 도커 이미지의 실행 중인 인스턴스입니다. 가벼우며 다른 컨테이너로부터 격리됩니다. 기본 호스트 시스템의 커널을 공유합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 플로우:\n\n일반적으로 도커 작업 흐름은 다음 단계로 진행됩니다:\n\n![도커 이미지 생성](/assets/img/2024-06-23-DockerFundamentals_1.png)\n\n- 빌드: 도커 파일로부터 도커 이미지를 생성합니다.\n- 푸시: 이미지를 중간 레지스트리나 도커 허브에 업로드합니다.\n- 풀: 레지스트리에서 이미지를 다운로드합니다.\n- 실행: 이미지로부터 컨테이너를 배포합니다.\n- 공유 (선택 사항): 이미지를 레지스트리(예: 도커 허브)에 푸시하여 다른 사람들과 공유할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도커 명령어:\n\n도커 명령어를 사용하면 도커 컨테이너를 쉽게 생성, 실행, 중지, 제거하고 관리할 수 있습니다. 이러한 명령어는 응용 프로그램을 컨테이너 환경에서 배포하고 관리하는 프로세스를 자동화하고 간소화하는 데 도움이 될 수 있습니다.\n\n![도커 기초 사항 이미지](/assets/img/2024-06-23-DockerFundamentals_2.png)\n\n가장 일반적으로 사용되는 도커 명령어를 살펴보면 도커 컨테이너를 효과적으로 관리하는 데 도움이 될 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- `docker run` – 새로운 이미지로부터 Docker 컨테이너를 시작하는 데 사용됩니다.\n- `docker ps` – 실행 중인 모든 Docker 컨테이너를 나열하는 데 사용됩니다.\n- `docker stop` – 실행 중인 컨테이너를 중지하는 데 사용됩니다.\n- `docker rm` – Docker 컨테이너를 제거하는 데 사용됩니다.\n- `docker images` – 현재 시스템에 있는 모든 Docker 이미지를 나열하는 데 사용됩니다.\n- `docker pull` – 레지스트리에서 Docker 이미지를 다운로드하는 데 사용됩니다.\n- `docker exec` – 실행 중인 컨테이너에서 명령을 실행하는 데 사용됩니다.\n- `docker-compose` – 여러 컨테이너로 구성된 Docker 애플리케이션을 관리하는 데 사용됩니다.\n\n# Docker 설치\n\nDocker를 설치하는 과정은 운영 체제에 따라 약간 다를 수 있습니다. 아래는 일반적인 플랫폼에 Docker를 설치하는 일반적인 단계입니다:\n\n# 1. Windows에 Docker 설치하기:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n1. Docker 웹사이트에서 Docker Desktop for Windows 설치 프로그램을 다운로드하고 실행해주세요 🐬.\n\n2. 설치를 완료하기 위해 화면 안내에 따라 진행해주세요. Docker Desktop은 Docker Engine을 포함한 필요한 구성 요소를 시스템에 설치합니다.\n\n3. 설치가 완료되면 Docker Desktop은 시스템 트레이에 나타나며 Docker 명령어는 명령 프롬프트(또는) PowerShell에서 실행할 수 있습니다.\n\n# 2. Linux(Ubuntu/Debian)에 Docker 설치하기:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n터미널을 열고 아래 명령을 하나씩 실행해주세요:\n\n```bash\n# 패키지 인덱스 업데이트\n\n# 필요한 종속성 설치\n\n# Docker GPG 키 추가\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 도커 저장소 추가\n\n# 패키지 인덱스 다시 업데이트\n\n# 도커 설치\n\n# 도커 서비스 시작\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 도커 명령을 sudo 없이 실행하려면 사용자를 'docker' 그룹에 추가하세요.\n\n## macOS에 도커 설치하기:\n\n1. 도커 웹사이트에서 도커 데스크톱 for Mac 설치 프로그램을 다운로드 🐬 받아 더블 클릭하여 설치를 시작합니다.\n\n2. 설치를 완료하기 위해 Docker.app 파일을 Applications 폴더로 드래그 앤 드롭하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 응용 프로그램 폴더에서 Docker Desktop을 실행하면 배경에서 실행됩니다.\n\n# 4. Docker 설치 확인:\n\nDocker를 설치한 후 터미널이나 명령 프롬프트에서 다음 명령을 실행하여 설치를 확인할 수 있습니다:\n\n![이미지](/assets/img/2024-06-23-DockerFundamentals_3.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 Docker가 정상적으로 설치되었다면, 버전 번호가 표시됩니다.\n\n![Docker version](/assets/img/2024-06-23-DockerFundamentals_4.png)\n\n## 첫 번째 Docker 컨테이너 실행하기\n\nNginx 이미지 다운로드: 터미널(또는) 명령 프롬프트에서 다음 명령을 사용하여 Docker 허브에서 공식 Nginx 이미지를 다운로드하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Docker Fundamentals](/assets/img/2024-06-23-DockerFundamentals_5.png)\n\nRun Nginx Container\nNow, run the Nginx container using the `docker run` command:\n\n- `-d`: Detached mode. The container will run in the background.\n- `-p 80:80`: Publishes port 80 from the container to port 80 on the host machine. This allows you to access the web server on your browser.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- **name**: `my_web_app` - 컨테이너에 사용자 정의 이름(\"my_web_app\")을 할당하여 쉽게 식별할 수 있도록 합니다.\n\n- **nginx**: 실행할 이미지의 이름(이 경우에는 공식 Nginx 이미지).\n\n웹 서버가 성공적으로 실행 중인 것을 나타내는 기본 Nginx 랜딩 페이지를 볼 수 있어야 합니다.\n\n![Nginx Landing Page](/assets/img/2024-06-23-DockerFundamentals_6.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n로컬 이미지 목록\n\n현재 실행 중인 컨테이너를 나열하려면:\n\n![DockerFundamentals_7](/assets/img/2024-06-23-DockerFundamentals_7.png)\n\n컨테이너를 중지하려면 다음 명령어를 사용할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그리고 컨테이너를 제거하고 싶다면 다음을 사용할 수 있어요:\n\nDocker와 Nginx를 사용하여 간단한 웹 애플리케이션을 성공적으로 만들었어요. 이미지를 제거한 후에도요.\n\n# 결론\n\nDocker를 사용한 컨테이너화는 응용 프로그램을 패키지화, 배포, 관리하는 강력하고 효율적인 방법을 제공해요. Docker의 기능과 도구를 활용하여 개발자들은 개발 프로세스를 간소화하고 응용 프로그램의 일관성을 보장하며 확장성과 이식성을 향상시킬 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리 블로그를 읽어 주셔서 감사합니다 🙏.\n","ogImage":{"url":"/assets/img/2024-06-23-DockerFundamentals_0.png"},"coverImage":"/assets/img/2024-06-23-DockerFundamentals_0.png","tag":["Tech"],"readingTime":10},{"title":"Kubernetes에서 서비스 및 네트워킹 이해하는 방법","description":"","date":"2024-06-23 23:06","slug":"2024-06-23-ServicesandNetworkinginKubernetes","content":"\n## 쿠버네티스 기본 개념 — 파트 (3.a)\n\n# 소개\n\n쿠버네티스는 조직이 컨테이너화된 애플리케이션을 배포, 확장 및 관리하는 방식을 혁신적으로 변화시켰습니다. 이 오케스트레이션 플랫폼의 핵심에는 컨테이너 간의 원활한 통신과 연결을 보장하는 복잡한 서비스 및 네트워킹 구성 요소들이 있습니다. 이 문서에서는 쿠버네티스 서비스와 네트워킹의 세계에 대해 깊이 있게 다루며 핵심 개념, 서비스 유형 및 이들이 컨테이너화된 애플리케이션의 전반적인 효율성에 어떻게 기여하는지 살펴보겠습니다.\n\n## 튜토리얼 흐름:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_0.png\" />\n\n서비스에 대해 이야기한 후 인그레스 컨트롤러를 자세히 살펴볼 예정입니다.\n\n# K8s의 서비스\n\n## 쿠버네티스 서비스란?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKubernetes에서 서비스는 포드의 논리적 집합과 해당 포드에 액세스하는 정책을 정의하는 추상화입니다. 서비스를 통해 응용 프로그램의 서로 다른 부분 간의 통신 및 발견이 가능해지며, 확장 가능하고 견고한 아키텍처를 제공할 수 있습니다.\n\n## Kubernetes에서 주요 서비스 유형은 무엇인가요?\n\n- NodePort 서비스: 각 노드의 IP에 정적 포트에서 서비스를 노출합니다. 이 포트를 통해 해당 포트의 어느 노드에서든 서비스에 외부 액세스할 수 있습니다.\n- ClusterIP 서비스: 기본 서비스 유형입니다. 클러스터 내부 IP에서 서비스를 노출합니다. 클러스터 내의 포드는 이 IP를 사용하여 서비스에 도달할 수 있습니다.\n- LoadBalancer 서비스: 클라우드 제공 업체의 로드 밸런서를 사용하여 서비스를 외부에 노출합니다. 로드 밸런싱이 필요한 외부 액세스가 필요한 응용 프로그램에 적합합니다.\n\n## NodePort 서비스란 무엇이며 예시를 들어 설명해주세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nNodePort은 클러스터의 각 노드에서 특정 포트로 서비스를 노출하는 서비스 유형입니다.\n\n이를 통해 클러스터 외부에서 해당 서비스에 외부 액세스가 가능해집니다.\n\nNodePort는 클러스터 내부에서 실행 중인 웹 응용 프로그램에 액세스하는 것과 같이 서비스를 외부 세계에 노출하는 데 일반적으로 사용됩니다.\n\nNodePort 서비스에 대한 주요 포인트:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 외부 액세스: NodePort는 각 노드의 포트를 서비스의 포트에 매핑하여 서비스를 외부에서 액세스할 수 있게 합니다.\n- 클러스터 IP: NodePort 서비스에는 클러스터 내에서 ClusterIP를 통해 내부적으로 액세스할 수 있는 기능도 있습니다.\n- 포트 매핑: NodePort 서비스를 생성하면 Kubernetes가 각 노드에 미리 정의된 범위(기본값은 30000~32767)에서 포트를 자동으로 할당합니다. 이 포트가 NodePort입니다.\n- 로드 밸런싱: NodePort는 정교한 로드 밸런싱을 제공하지는 않지만, 클러스터 내의 모든 노드에 도달하여 서비스에 외부 액세스할 수 있게 합니다.\n\n![서비스 및 네트워킹 in Kubernetes](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_1.png)\n\n![서비스 및 네트워킹 in Kubernetes](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_2.png)\n\n이 예제의 구성 요소를 자세히 살펴보겠습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- metadata: 서비스의 이름을 지정합니다.\n- spec.selector: 서비스가 대상이 될 파드를 결정하는 셀렉터를 정의합니다. 이 경우에는 app: my-app 라벨이 있는 파드를 대상으로 합니다.\n- spec.ports: 포트 구성을 지정합니다. 이 예에서는 서비스가 포트 80에서 수신하고 포트 8080으로 트래픽을 전달합니다.\n- spec.type: NodePort: 이것이 NodePort 서비스임을 나타냅니다.\n- kubectl apply -f demoNodeport.yaml 명령을 사용하여 이 YAML 구성을 적용하면, Kubernetes가 teva-nodeport-service 라는 이름의 NodePort 서비스를 생성합니다.\n\n하나의 노드에 여러 개의 파드가 있는 경우\n\n![서비스 및 쿠버네티스 네트워킹](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_3.png)\n\n여러 노드에 파드가 있는 경우\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_4.png)\n\ncurl http://192.169.1.4:30008 또는 curl http://192.169.1.6:30008 또는 curl http://192.169.1.9:30008\n\n## ClusterIP 서비스란 무엇이며 예시를 들어 설명해주세요?\n\nClusterIP 서비스는 내부 IP 주소를 노출하고 클러스터 내에서만 접근할 수 있도록 만드는 종류의 서비스입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이는 클러스터 외부에서 해당 서비스에 접근할 수 없음을 의미합니다.\n\nClusterIP 서비스는 주로 클러스터 내의 다른 구성 요소 또는 마이크로서비스 간 통신에 사용됩니다.\n\nClusterIP 서비스에 대한 주요 포인트:\n\n- 내부 IP 주소: ClusterIP 서비스는 쿠버네티스 클러스터 내에서만 접근할 수 있는 내부 IP 주소가 할당됩니다.\n- Pod 선택기: 선별기를 기반으로 한 일련의 파드와 연관되어 있습니다. 서비스는 지정된 레이블 선택기와 일치하는 파드로 트래픽을 전달합니다.\n- 로드 밸런싱: ClusterIP 서비스는 해당 서비스와 관련된 파드 간의 기본적인 로드 밸런싱을 제공합니다. 들어오는 트래픽은 선택된 파드 사이에 분산됩니다.\n- 클러스터 내 통신: 이러한 서비스는 같은 쿠버네티스 클러스터 내에서 실행되는 응용 프로그램 또는 마이크로서비스의 서로 다른 부분 간 통신에 적합합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 변경하영 표시한 것입니다.\n\n![ClusterIP implementation](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_5.png)\n\nExample Sample ClusterIP implementation.\n\n![ClusterIP YAML example](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_6.png)\n\nThis YAML defines a ClusterIP service named “backend-service” that selects pods labeled with “app: backend.”\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n서비스는 포트 8080에서 수신하고 해당 포트 8080으로 팟으로 트래픽을 전달합니다.\n\n![Image 1](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_7.png)\n\n![Image 2](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_8.png)\n\n이 예시의 구성 요소를 살펴보겠습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- metadata: 서비스의 이름을 지정합니다.\n- spec.selector: 서비스가 대상이 되는 팟을 결정하는 셀렉터를 정의합니다. 이 경우, app: frontend 라벨을 가진 팟을 대상으로 합니다.\n- spec.ports: 포트 구성을 지정합니다. 이 예에서는 서비스가 포트 80에서 수신하고 포트 8080으로 트래픽을 전달합니다.\n- spec.type: ClusterIP: 이것이 ClusterIP 서비스임을 나타냅니다.\n- kubectl apply -f `filename`.yaml을 사용하여이 YAML 구성을 적용하면 Kubernetes가 example-clusterip-service라는 ClusterIP 서비스를 생성합니다.\n- 이 서비스는 클러스터 내에서 ClusterIP 주소를 사용하여 접근할 수 있습니다.\n- 동일한 Kubernetes 클러스터 내의 다른 팟들은 이 서비스와 통신하기 위해 ClusterIP 및 포트를 사용할 수 있습니다 (예: example-frontend-service: 80).\n\n## 로드 밸런서 서비스란 무엇이며 예시를 들어 설명하세요?\n\nKubernetes(K8s)에서 로드 밸런서는 서비스를 외부 세계에 노출시키고 외부 로드 밸런서를 자동으로 프로비저닝하는 유형의 서비스입니다.\n\n이 로드 밸런서는 들어오는 네트워크 트래픽을 여러 노드에 분산하여 응용 프로그램의 고가용성과 신뢰성을 보장합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n로드밸런서 서비스에 대한 주요 포인트:\n\n- 외부 액세스: 로드밸런서 서비스는 클라우드 제공 업체의 로드 밸런서를 프로비저닝하여 서비스를 외부에서 접근 가능하게 만듭니다. 외부 로드 밸런서에는 일반적으로 공개 IP 주소가 있으며 귀하의 응용 프로그램을 실행하는 노드로 트래픽을 분배할 수 있습니다.\n- 자동 프로비저닝: 로드밸런서 서비스를 만들면 Kubernetes가 클라우드 제공 업체와 통신하여 자동으로 로드 밸런서를 프로비저닝합니다. 이 프로세스의 구체적인 내용은 클라우드 제공 업체에 따라 다릅니다.\n- NodePort 및 ClusterIP: 로드밸런서 서비스에는 NodePort 및 ClusterIP도 있습니다. NodePort는 클러스터 외부에서 서비스에 액세스할 수 있게 하며, ClusterIP는 클러스터 내에서 내부 액세스를 허용합니다.\n- 자동 스케일링: 외부 로드 밸런서는 여러 노드에 트래픽을 분산하여 확장성과 내결함성을 제공하기 위해 수평으로 확장할 수 있습니다.\n\n![로드밸런서 서비스 이미지](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_9.png)\n\n이 예에서는 세 개의 백엔드 팟 복제본을 만드는 간단한 배포(backend-deployment)를 정의합니다. 이 팟은 선택기 일치를 위해 app: backend로 레이블이 지정됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_10.png)\n\n백엔드 서비스는 백엔드 팟과 연관된 로드 밸런서 서비스입니다. 포트 80을 노출하고 트래픽을 포트 8080에서 팟으로 전달합니다.\n\n이제 배포 및 서비스를 생성하기 위해 YAML 파일 두 개를 적용하세요:\n\n![이미지](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_11.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n서비스에 액세스하는 방법: 로드 밸런서 서비스가 프로비저닝되면 외부 IP 주소를 사용하여 서비스에 외부에서 액세스할 수 있습니다.\n\n클라우드 제공업체에 따라 IP 주소가 할당되는 데 시간이 걸릴 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_12.png)\n\n\"EXTERNAL-IP\" 필드를 찾아 할당된 후에 서비스에 액세스할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_13.png)\n\n# 인그레스 네트워킹\n","ogImage":{"url":"/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_0.png"},"coverImage":"/assets/img/2024-06-23-ServicesandNetworkinginKubernetes_0.png","tag":["Tech"],"readingTime":10},{"title":"데이터 엔지니어링을 위한 Kubernetes 처음부터 끝까지 완벽 가이드","description":"","date":"2024-06-23 23:03","slug":"2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide","content":"\n최근 몇 년간 데이터 엔지니어링이 크게 발전하였는데, Kubernetes가 이 분야에서 중요한 기술로 부상하였습니다. Kubernetes는 확장 가능하고 효율적인 애플리케이션 배포를 포함한 데이터 파이프라인 및 워크플로우의 효율적인 구축과 관리를 지원하는 오픈 소스 컨테이너 오케스트레이션 플랫폼입니다. 이 글에서는 Docker에서 Kubernetes 설정, kubectl로 클러스터 관리, Kubernetes 대시보드 배포, 그리고 Helm 차트를 사용해 Apache Airflow를 실행하는 방법에 대해 알아보겠습니다.\n\n![KubernetesforDataEngineeringAnEnd-to-EndGuide_0](/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_0.png)\n\n# Kubernetes란 무엇인가요?\n\nKubernetes는 K8s로 약칭되며, 애플리케이션 컨테이너를 자동으로 배포, 확장 및 운영하기 위해 설계된 오픈 소스 플랫폼입니다. 원래 구글에서 개발되었으며 현재는 Cloud Native Computing Foundation에서 유지보수하고 있습니다. Kubernetes는 견고한 기능과 광범위한 커뮤니티 지원으로 컨테이너 오케스트레이션의 표준으로 자리매깁니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 쿠버네티스의 핵심 개념\n\n쿠버네티스를 이해하기 위해 그 핵심 개념을 살펴보겠습니다:\n\n- 컨테이너: 쿠버네티스는 컨테이너화를 기반으로 구축되어 있습니다. 컨테이너는 응용 프로그램을 모든 종속성과 실행 환경과 함께 패키징합니다. 이는 다양한 개발, 테스트 및 프로덕션 환경에서 일관성을 유지합니다.\n- 파드: 쿠버네티스에서 가장 작은 배포 가능한 단위입니다. 파드는 하나 이상의 컨테이너를 포함할 수 있으며, 이 컨테이너들은 저장소, 네트워크 및 실행 방법에 대한 사양을 공유합니다. 파드는 순간적이고 임시적입니다.\n- 노드: 쿠버네티스에서의 워커 머신으로서, 물리적 또는 가상 머신이 될 수 있습니다. 각 노드는 파드를 실행하고, 마스터 노드에 의해 관리됩니다. 노드에는 파드를 실행하기 위한 필요한 서비스가 포함되어 있으며, 컨트롤 플레인에 의해 관리됩니다.\n- 컨트롤 플레인: 쿠버네티스 노드를 제어하는 프로세스들의 모음입니다. 모든 작업 할당은 여기에서 시작됩니다. 컨트롤 플레인의 구성 요소들은 클러스터에 대한 전역 결정(예: 스케줄링)을 내리고, 클러스터 이벤트(예: 배포의 레플리카 필드가 충족되지 않았을 때 새로운 파드 시작)를 감지하고 대응합니다.\n- 서비스: 쿠버네티스 서비스는 논리적인 일련의 파드와 그에 대한 액세스 정책을 정의하는 추상화 계층입니다. 이는 일부 파드에 네트워크 액세스를 제공하는 데 자주 사용됩니다.\n- 배포: 파드 및 레플리카셋에 대한 선언적 업데이트를 관리하는 고수준 개념입니다. 배포는 파드에 대한 템플릿을 사용하고 파드 수, 롤링 업데이트 전략 및 원하는 상태에 대한 제어 매개변수를 확장합니다.\n\n만일 전체 비디오에 관심이 있으시다면, [여기](#)를 클릭해서 보실 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 데이터 엔지니어링에서 Kubernetes 이해하기\n\n실제 측면에 들어가기 전에, 데이터 엔지니어링에서 Kubernetes가 왜 게임 체인저인지 이해하는 것이 중요합니다:\n\n- 확장성: Kubernetes는 수요에 따라 데이터 처리 워크로드를 자동으로 확장할 수 있습니다.\n- 내구성: 실패를 효과적으로 관리하여 데이터 파이프라인의 높은 가용성을 보장합니다.\n- 자원 최적화: Kubernetes는 기본 리소스의 사용을 최적화하여 비용을 절감합니다.\n- 이식성과 일관성: 서로 다른 배포 플랫폼에서도 일관된 환경을 제공합니다.\n\n# 연결 기술은 어떨까요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리 Kubernetes 클러스터를 관리하고 제어하며 데이터 엔지니어링 프로세스를 용이하게 하기 위해 Kubernetes와 함께 사용될 다른 기술들에 대해 이야기해보려고 해요. 이 기술들은 Docker, Kubectl, Helm 등을 포함합니다.\n\n## Docker\n\nDocker는 컨테이너 내에서 애플리케이션을 개발, 배포 및 실행하는 플랫폼입니다. 컨테이너화는 애플리케이션을 해당 애플리케이션만을 위한 자체 운영 환경과 함께 컨테이너에 묶는 완전한 가상화의 가벼운 대안입니다.\n\n- 컨테이너: 애플리케이션의 코드, 구성 및 종속성을 하나의 객체로 패키징하는 표준 방법을 제공합니다.\n- 이미지: 코드, 런타임, 라이브러리, 환경 변수 및 구성 파일 등 애플리케이션을 실행하는 데 필요한 모든 것이 포함된 실행 가능한 패키지입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 쿠버네티스 클러스터 관리\n\n쿠버네티스 클러스터를 관리하는 것은 각각이 클러스터의 라이프사이클 및 운영에 특정 목적을 제공하는 다양한 도구와 기술을 포함합니다. 저는 쿠버네티스 클러스터를 제어하고 관리하는 데 널리 사용되는 세 가지 주요 기술을 강조하기로 결정했습니다.\n\n## 1. kubectl\n\n- 목적: kubectl은 쿠버네티스 API와 상호 작용하는 명령줄 도구입니다. 이는 쿠버네티스 클러스터를 관리하는 주요 인터페이스입니다.\n- 기능: 애플리케이션을 배포하고 클러스터 리소스를 검사하고 관리하며 로그를 보고 팟에서 명령을 실행하는 등의 기능을 제공합니다.\n- 사용법: kubectl 명령은 간단합니다. 예를 들어, kubectl get pods는 현재 네임스페이스의 모든 팟을 나열하거나 kubectl apply -f deployment.yaml은 파일에서 구성을 적용하는 것과 같은 명령입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2. 미니큐브\n\n- 목적: 미니큐브는 쿠버네티스를 로컬에서 실행할 수 있게 해주는 도구입니다. 로컬 머신에서 단일 노드 쿠버네티스 클러스터를 생성합니다.\n- 사용 사례: 개발 및 테스트 목적으로 이상적입니다. 개발자들이 자신의 머신에서 쿠버네티스 환경에서 애플리케이션을 테스트할 수 있도록 합니다.\n- 특징: DNS, 노드 포트, 구성 맵 및 시크릿, 대시보드, 컨테이너 런타임 등 다양한 쿠버네티스 기능을 지원합니다.\n\n## 3. 쿠버엠톰\n\n- 목적: 쿠버엠톰은 쿠버네티스 클러스터를 생성하고 관리하기 위해 kubeadm init 및 kubeadm join을 제공하는 도구입니다.\n- 기능: 쿠버네티스 클러스터의 부트스트래핑, 제어 플레인 설정, 토큰 관리, kubeadm 설정 등을 처리합니다.\n- 사용법: 일반 클러스터 설정 과정을 간소화하여, 쿠버네티스에 처음 입문하는 사람들도 접근하기 쉽게 만듭니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 문서에서는 kubectl을 사용할 것입니다. 귀하의 운영 체제 및 버전을 선택하여 기기에 설치할 수 있습니다.\n\n만약 minikube 또는 kubeadm을 사용하여 Kubernetes 클러스터를 관리하고 싶다면, 각각 여기와 여기의 빠른 시작 가이드를 따를 수 있습니다.\n\n설치가 완료된 후에는 다음을 실행하여 설치를 확인할 수 있습니다.\n\n```js\nkubectl version\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제 경우에는 kubectl 버전 1.29.1을 실행 중입니다. 당신이 프로세스를 실행하는 시기에 따라 더 높을 수 있습니다.\n\n![이미지](/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_1.png)\n\n# Docker에서 Kubernetes 설정하기\n\nDocker에서 Kubernetes를 설정하는 것은 Docker가 컨테이너화된 애플리케이션을 실행하는 로컬 Kubernetes 클러스터를 생성하는 과정을 포함합니다. 이 설정은 개발 및 테스트 목적으로 특히 유용합니다. Docker에서 Kubernetes를 설정하는 단계별 가이드는 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 준비 사항\n\n- 도커: 시스템에 도커가 설치되어 있는지 확인해주세요. Windows 및 Mac용 도커 데스크톱은 쿠버네티스 지원이 기본 내장되어 있습니다.\n- 하드웨어 요구 사항: 여러 컨테이너를 실행하기 위한 충분한 CPU, 메모리 및 저장 공간이 필요합니다.\n\n# 설정 단계\n\n## 1. 도커 데스크톱 설치\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다운로드 및 설치: 공식 Docker 웹사이트에서 Docker Desktop을 다운로드하세요. 제공된 지침을 따라 컴퓨터에 설치해주세요.\n- Kubernetes 활성화: Docker Desktop에는 로컬 컴퓨터에서 실행되는 독립형 Kubernetes 서버가 포함되어 있습니다. 이를 활성화하기 전에 초기 기본 구성에서 리소스를 약간 늘려주어야 합니다.\n\n![이미지](/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_2.png)\n\nDocker Desktop에서 Kubernetes 활성화 방법:\n\n- Docker Desktop 설정을 엽니다.\n- Kubernetes 섹션을 찾습니다.\n- “Kubernetes 활성화”란에 체크합니다.\n- 변경사항을 저장하려면 “적용 및 다시 시작”을 클릭합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 변경하겠습니다.\n\n![이미지](/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_3.png)\n\n## 2. 설치 확인\n\n- 도커 확인: 터미널이나 명령 프롬프트를 열고 docker --version을 실행하여 도커가 올바르게 설치되었는지 확인합니다.\n\n## 3. 쿠버네티스 컨텍스트 구성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 쿠버네티스는 서로 다른 클러스터에 액세스하기 위해 컨텍스트를 사용합니다. 도커 데스크탑은 docker-desktop이라는 컨텍스트를 설정합니다.\n- 이 컨텍스트로 전환하려면 kubectl config use-context docker-desktop을 사용하십시오.\n\n# PC에 헬름 차트 설치하는 방법\n\n## macOS용:\n\n- Homebrew: Homebrew를 설치한 경우, 간단히 다음을 실행할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nbrew install helm\n```\n\n## Windows 사용자분들을 위해:\n\n- Chocolatey를 사용하는 경우, 다음 명령어를 실행하여 Helm을 설치할 수 있습니다:\n\n```js\nchoco install kubernetes-helm\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 리눅스용:\n\n- 스크립트: Helm은 리눅스 사용자를 위한 자동화된 스크립트를 제공합니다. 다음을 실행하세요:\n\n```js\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n```\n\n만약 다른 버전의 운영 체제, 패키지 관리자를 원하시거나 소스에서 직접 빌드하고 싶다면 공식 가이드의 지침을 따를 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n헬름이 컴퓨터에 설치되면 다음을 실행하여 설치를 확인할 수 있습니다:\n\n```js\nhelm version\n```\n\n아래 스크린샷과 유사한 내용이 표시됩니다. 저의 경우에는 헬름 3.14.0을 실행하고 있습니다.\n\n<img src=\"/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_4.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Kubernetes 대시보드 배포하기\n\nKubernetes 대시보드는 Kubernetes 클러스터를 관리하는 사용자 친화적인 웹 기반 인터페이스를 제공합니다. 클러스터 리소스와 애플리케이션을 보고 관리할 수 있으며 기본적인 문제 해결 기능도 제공합니다. Kubernetes 대시보드를 배포하는 방법은 다음과 같습니다:\n\n## 단계 1: 대시보드 배포하기\n\n- 배포 명령 실행: Kubernetes 대시보드를 배포하려면 kubectl을 사용하여 yaml 구성을 배포하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl apply -f https://raw.githubusercontent.com/airscholar/Kubernetes-For-DataEngineering/main/k8s/recommended-dashboard.yaml\n```\n\n이 명령은 Kubernetes 대시보드의 GitHub 저장소에서 권장 배포 구성을 다운로드하고 적용합니다. 보게 될 내용은 다음과 같아야 합니다.\n\n# 단계 2: 대시보드에 액세스하기\n\n- 프록시 시작: Kubernetes 대시보드는 프록시 서버를 통해 액세스됩니다. 다음 몤령을 사용하여 프록시를 시작합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl proxy\n```\n\n이 명령어를 실행하면 대시보드를 로컬 머신에서 URL을 통해 접속할 수 있습니다.\n\n![image](/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_5.png)\n\n2. URL에 접속하세요: 웹 브라우저를 열고 다음 URL로 이동하세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n```\n\n이 URL은 Kubernetes 대시보드 인터페이스로 이동합니다.\n\n# 단계 3: 대시보드 인증하기\n\n- Bearer 토큰 받기: 대시보드에 로그인하려면 Bearer 토큰을 생성해야 합니다. 다음 단계를 따라 서비스 계정을 생성하고 토큰을 받을 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다음 내용으로 dashboard-adminuser.yaml이라는 파일을 생성하세요:\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n```\n\n- 다음 내용으로 dashboard-clusterrole.yaml이라는 파일을 생성하세요:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: admin-user\n    namespace: kubernetes-dashboard\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 그리고 마지막으로 아래 내용을 가진 dashboard-secret.yaml 파일을 생성해주세요.\n\n```js\napiVersion: v1\nkind: Secret\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"admin-user\"\ntype: kubernetes.io/service-account-token\n```\n\n이 구성을 적용하려면 다음 명령을 실행해야합니다.\n\n```js\nkubectl apply -f dashboard-adminuser.yaml\nkubectl apply -f dashboard-clusterrole.yaml\nkubectl apply -f dashboard-secret.yaml\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 대시보드에 액세스할 때 사용할 토큰을 생성하는 방법입니다:\n\n```js\nkubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d\n```\n\n만약 CLI를 사용하는 것을 선호한다면, 대신에 다음 명령어를 실행할 수도 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- kube-system 네임스페이스에 서비스 계정을 생성해 주세요:\n\n```js\nkubectl create serviceaccount admin-user -n kubernetes-dashboard\n```\n\n- 서비스 계정을 클러스터 관리자 역할에 바인딩해 주세요:\n\n```js\nkubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin-user\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다음 명령을 사용하여 비밀을 가져오세요:\n\n```js\nkubectl get secret $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -n kubernetes-dashboard -o jsonpath=\"{.data.token}\" | base64 --decode\n```\n\n그런 다음 대시보드에 로그인하려면 토큰을 사용하세요:\n\n## 단계 4: 대시보드 사용하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 대시보드 탐색하기: 로그인하면 배포, 서비스 및 팟과 같은 Kubernetes 자원을 보거나 관리할 수 있습니다.\n- 자원 생성 및 수정: 대시보드 UI를 통해 직접 새로운 자원을 생성하거나 기존 자원을 수정할 수 있습니다.\n- 클러스터 및 애플리케이션 성능 모니터링: 대시보드를 통해 클러스터 전체 및 CPU 및 메모리 사용량을 포함한 개요를 제공합니다.\n\n모든 소셜 미디어 플랫폼에서 팔로우를 눌러주시고 지지를 보여주기 위해 박수를 보내고 댓글을 달아주세요.\n\n- Github: airscholar\n- Twitter: @YusufOGaniyu\n- LinkedIn: Yusuf Ganiyu\n- Youtube: CodeWithYu\n\n이제 Kubernetes 대시보드를 성공적으로 배포했으므로, 차례로 Kubernetes 클러스터에 Apache Airflow를 배포할 준비를 해야 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 쿠버네티스 클러스터에 Apache Airflow 배포하기\n\n쿠버네티스 클러스터에 Apache Airflow를 배포하는 것은 강력한 스케줄링 및 워크플로우 관리 기능을 확장 가능한 환경에서 활용하는 훌륭한 방법입니다. Apache Airflow를 쿠버네티스에 배포하는 가장 효율적인 방법은 Helm을 사용하는 것입니다. Helm은 쿠버네티스 응용 프로그램의 설치 및 관리를 간소화하는 패키지 매니저입니다. 이제 함께 Apache Airflow를 쿠버네티스에 배포하는 과정을 살펴보겠습니다.\n\n## 요구 사항\n\n다음 요구 사항은 문제 없이 진행하기 위해 필요합니다. 이전 부분을 건너 뛰었다면, 계속 진행하기 전에 모든 것이 잘 작동하는지 다시 확인하는 것이 좋습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 실행 중인 Kubernetes 클러스터가 필요합니다.\n- 로컬 머신에 Helm이 설치되어 있어야 합니다 (필요한 경우 Helm 설치 방법은 이전 지침을 참조하십시오).\n- Kubernetes 클러스터와 통신할 수 있도록 kubectl이 구성되어 있어야 합니다.\n\n## 단계 1: Airflow Helm Chart 저장소 추가\n\n먼저, Apache Airflow 공식 Helm 차트 저장소를 Helm 설치에 추가해야 합니다:\n\n```sh\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 명령어는 Airflow 리포지토리를 추가하고 로컬 Helm 차트 리포지토리 인덱스를 업데이트합니다.\n\n# 단계 2: Apache Airflow 설치\n\n다음 명령어로 Apache Airflow를 Helm 차트를 사용하여 설치할 수 있습니다:\n\n```js\nhelm install airflow apache-airflow/airflow --namespace airflow --create-namespace --debug\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- airflow은 릴리스 이름으로, 원하는대로 변경할 수 있습니다.\n- --namespace airflow는 Airflow를 설치할 Kubernetes 네임스페이스를 지정합니다. 네임스페이스가 존재하지 않는 경우, --create-namespace를 사용하여 생성하고 --debug를 사용하면 배포 단계를 볼 수 있습니다.\n\n이 명령은 웹 서버, 스케줄러 등 필요한 모든 구성 요소와 함께 Airflow를 배포합니다. 디버그 플래그가 있으면 Apache Airflow 릴리스를 배포하는 데 사용된 세부 정보, 구성 및 해당 값들을 볼 수 있습니다.\n\n배포 중에는 대기 중인 작업 및 해당 상태를 쿠버네티스 클러스터에서 볼 수 있습니다. 프로세스가 완료되면 작업은 대기열에서 제거되어 더 이상 보이지 않게 됩니다.\n\n재미있게도 Apache Airflow 릴리스는 완료되었지만 보통 localhost:8080으로 접근해도 여전히 접속할 수 없습니다. 이를 가능하게 하고 Apache Airflow 파드에 대한 연결을 처리하려면 아래 명령을 실행한 다음 UI에서 Apache Airflow 배포에 액세스해야 합니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflow\n```\n\n또한, 쿠버네티스 클러스터에서 Airflow 배포의 건강 상태를 확인하고 모니터링하는 것이 중요합니다. 모든 것이 잘 되고 준비되어 있는지 확인하기 위해 위의 네임스페이스를 airflow로 변경해주시면 워크로드 세부 정보를 볼 수 있습니다.\n\n이제 관리자 사용자 이름과 관리자 비밀번호로 Airflow UI에 액세스할 수 있습니다.\n\n화면 상단에 동적 웹서버 비밀 키에 관한 경고가 있는 경우 정적 웹서버 비밀 키를 사용하는 것이 좋습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 동적 키 대신 정적 키 사용하는 이유\n\n개발 환경에서는 동적 웹 서버 비밀 키를 사용해도 상관없지만, 제품 환경에서는 정적 웹 서버 비밀 키를 사용하는 것이 강력히 권장됩니다. 그 이유는 다음과 같습니다:\n\n- 응용 프로그램 인스턴스 간의 일관성: 특히 쿠버네티스로 관리되는 제품 환경에서는 응용 프로그램이 여러 인스턴스로 확장될 수 있습니다. 각 인스턴스가 자체 동적 비밀 키를 생성하는 경우 세션, 쿠키 또는 기타 암호화된 데이터 처리에 일관성이 무너질 수 있습니다. 정적 키는 응용 프로그램의 모든 인스턴스가 데이터를 일관적으로 읽고 쓸 수 있도록 보장합니다.\n- 다시 시작 시 지속성: 웹 서버가 다시 시작될 때 새로운 동적 비밀 키를 생성하면 이전 키로 암호화된 모든 세션 및 쿠키가 무효화됩니다. 이는 갑자기 로그아웃되거나 세션 데이터를 잃어버린 사용자에게 혼란을 줄 수 있습니다. 정적 키는 다시 시작할 때 동일하게 유지되어 세션의 연속성을 유지합니다.\n- 보안 최상의 사례: 역설적으로 보일 수 있지만, 안전하게 저장된 정적 비밀 키를 사용하는 것이 동적으로 생성하는 것보다 보안상 더 안전할 수 있습니다. 완전히 무작위이고 안전한 키를 생성하는 것은 쉽지 않습니다. 잘못 생성된 동적 키가 선정된 정적 키보다 덜 안전할 수 있습니다.\n- 구성 관리: 정적 키는 안전한 구성 관리 관행을 통해 관리할 수 있습니다. 이는 HashiCorp Vault, AWS Secrets Manager 또는 Kubernetes Secrets와 같은 비밀 관리 시스템에 키를 저장하는 것을 포함합니다. 이렇게 하면 키가 코드나 안전하지 않은 구성에서 노출되지 않고 액세스를 엄격하게 제어할 수 있습니다.\n- 감사 및 규정 준수: 많은 규정 준수 환경에서는 비밀 키에 대한 감사 및 액세스 제어가 필요합니다. 정적 키를 사용하면 이러한 제어를 구현하고 누가 액세스 권한을 가지고 있는지 추적하는 것이 더 쉬워집니다.\n\n이를 해결하기 위해, Apache Airflow 릴리스에 웹 서버 비밀 키를 포함하여 재구성하겠습니다. 이를 위해 원하는 구성을 덮어쓸 values.yaml 파일을 생성할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n키를 생성하는 다양한 방법이 있지만 가장 인기 있는 두 가지 방법을 강조하겠습니다:\n\n- 파이썬 암호화 라이브러리를 사용하여 키를 생성하는 간단한 코드를 작성할 수 있습니다.\n\n- 먼저, cryptography 라이브러리가 설치되어 있는지 확인하십시오:\n\n```js\npip install cryptography\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 그럼 당신의 Python 환경에서 다음 한 줄을 사용할 수 있어요:\n\n```js\npython -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\"\n```\n\n2. airflow 페르네 키 사용:\n\n```js\n echo Fernet Key: $(kubectl get secret --namespace airflow\n airflow-fernet-key -o jsonpath=\"{.data.fernet-key}\" | base64 --decode)\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한번 생성되면 다음과 같이 보일 것입니다. 이 값들로 values.yaml 파일을 업데이트할 수 있어요.\n\n```js\nfernetKey: aERBZE5MN3E0TjRjU2xzQWxCdTNIUks0WGFTZThoWXc=\nwebserverSecretKey: aERBZE5MN3E0TjRjU2xzQWxCdTNIUks0WGFTZThoWXc=\n```\n\n그런 다음 helm 차트 배포를 다시 업데이트하세요:\n\n```js\nhelm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace -f values.yaml\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nUI에서 변경 사항을 미리보기하면 경고 메시지가 사라진 것을 확인할 수 있습니다.\n\n# 쿠버네티스에서 Airflow의 DAG 연결\n\n쿠버네티스에 Apache Airflow를 성공적으로 배포한 후에 해야 할 다음 단계는 DAG를 해당 Airflow에 연결하는 것입니다.\n\n우리의 경우, GitHub 저장소에서 DAG를 Airflow에 연결하게 됩니다. 거기에 Airflow DAG 코드를 작성하고 커밋한 후에 해당 커밋으로부터 URL을 얻어 Kubernetes와 동기화할 것입니다. 아래에 제시된 대로 진행됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nHere is the translation into Korean:\n\n```python\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'datamasterylab.com',\n    'start_date': datetime(2024, 1, 25),\n    'catchup': False\n}\n\ndag = DAG(\n    'hello_world',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1)\n)\n\nt1 = BashOperator(\n    task_id='hello_world',\n    bash_command='echo \"Hello World\"',\n    dag=dag\n)\n\nt2 = BashOperator(\n    task_id='hello_dml',\n    bash_command='echo \"Hello Data Mastery Lab\"',\n    dag=dag\n)\n\nt1 >> t2\n\n```\n\n```python\nfrom datetime import datetime, timedelta\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\n\ndef get_data(**kwargs):\n    import requests\n    import pandas as pd\n\n    url = 'https://raw.githubusercontent.com/airscholar/ApacheFlink-SalesAnalytics/main/output/new-output.csv'\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        df = pd.read_csv(url, header=None, names=['Category', 'Price', 'Quantity'])\n\n        # 데이터프레임을 JSON 문자열로 변환하여 xcom으로 전달\n        json_data = df.to_json(orient='records')\n\n        kwargs['ti'].xcom_push(key='data', value=json_data)\n    else:\n        raise Exception(f'데이터를 가져오는 데 실패했습니다. HTTP 상태 코드: {response.status_code}')\n\n\ndef preview_data(**kwargs):\n    import pandas as pd\n    import json\n\n    output_data = kwargs['ti'].xcom_pull(key='data', task_ids='get_data')\n    print(output_data)\n    if output_data:\n        output_data = json.loads(output_data)\n    else:\n        raise ValueError('XCom으로부터 데이터를 받지 못했습니다.')\n\n    # JSON 데이터에서 데이터프레임 생성\n    df = pd.DataFrame(output_data)\n\n    # 총 판매량 계산\n    df['Total'] = df['Price'] * df['Quantity']\n\n    df = df.groupby('Category', as_index=False).agg({'Quantity': 'sum', 'Total': 'sum'})\n\n    # 총 판매량을 기준으로 정렬\n    df = df.sort_values(by='Total', ascending=False)\n\n    print(df[['Category', 'Total']].head(20))\n\n\ndefault_args = {\n    'owner': 'datamasterylab.com',\n    'start_date': datetime(2024, 1, 25),\n    'catchup': False\n}\n\ndag = DAG(\n    'fetch_and_preview',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1)\n)\n\nget_data_from_url = PythonOperator(\n    task_id='get_data',\n    python_callable=get_data,\n    dag=dag\n)\n\npreview_data_from_url = PythonOperator(\n    task_id='preview_data',\n    python_callable=preview_data,\n    dag=dag\n)\n\nget_data_from_url >> preview_data_from_url\n\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위에서 commited된 저장소는 다음과 같습니다:\n\n```js\nhttps://github.com/airscholar/Kubernetes-For-DataEngineering.git\n```\n\n이제 우리의 values.yaml 파일로 돌아가서 저장소와 동기화할 값을 업데이트해봅시다.\n\n```js\nfernetKey: aERBZE5MN3E0TjRjU2xzQWxCdTNIUks0WGFTZThoWXc=\nwebserverSecretKey: aERBZE5MN3E0TjRjU2xzQWxCdTNIUks0WGFTZThoWXc=\n\ndags:\n  gitSync:\n    enabled: true\n    repo: https://github.com/airscholar/Kubernetes-For-DataEngineering.git\n    branch: main\n    rev: HEAD\n    depth: 1\n    maxFailures: 0\n    subPath: \"dags\"\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하루 마무리로 우리는 구성을 다시 적용하고 helm 차트로 릴리스를 업데이트할 거에요!\n\n```js\nhelm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace -f values.yaml\n```\n\nUI에서 그 모습을 확인해보죠.\n\n마지막으로, 이 DAG들을 트리거하고 해당 로그에서 출력을 확인해봅시다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 이제 마무리입니다!\n\n아래 주제 중에 관심이 있는 분은:\n\n- Python\n- 데이터 엔지니어링\n- 데이터 분석\n- 데이터 과학\n- SQL\n- 클라우드 플랫폼 (AWS/GCP/Azure)\n- 머신러닝\n- 인공지능\n\n제 모든 플랫폼을 좋아요와 팔로우 해주세요:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- Github: airscholar\n- Twitter: @YusufOGaniyu\n- LinkedIn: Yusuf Ganiyu\n- Youtube: CodeWithYu\n- Medium: Yusuf Ganiyu\n\n나는 LinkedIn, X, Medium 및 YouTube에서 매일 콘텐츠를 공유합니다.\n\ndatamasterylab.com에서 더 많은 코스를 확인하실 수 있습니다.\n\n# 자료들\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n깃허브 풀 코드\n\n유튜브 비디오\n\n엔드 투 엔드 데이터 엔지니어링 재생 목록\n\n# 스택데믹\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n끝까지 읽어 주셔서 감사합니다. 떠나시기 전에:\n\n- 축소 버튼을 클릭해 주시고 작가를 팔로우해 주세요! 👏\n- X사의 팔로우 | LinkedIn | YouTube | Discord\n- 다른 플랫폼도 방문해 보세요: In Plain English | CoFeed | Venture\n","ogImage":{"url":"/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_0.png"},"coverImage":"/assets/img/2024-06-23-KubernetesforDataEngineeringAnEnd-to-EndGuide_0.png","tag":["Tech"],"readingTime":28},{"title":"쿠버네티스 명령어 명령형 vs 선언형 비교","description":"","date":"2024-06-23 23:01","slug":"2024-06-23-ImperativevsDeclarativeKubernetesCommands","content":"\n쿠버네티스 명령줄 도구 인 kubectl을 사용하면 쿠버네티스 클러스터에 대해 명령을 실행할 수 있습니다. kubectl을 사용하여 응용 프로그램을 배포하고 클러스터 리소스를 검사하고 관리하며 로그를 볼 수 있습니다.\n\nkubectl 도구는 세 가지 유형의 객체 관리를 지원합니다.\n\n- 명령형 명령\n- 명령형 객체 구성\n- 선언적 객체 구성\n\n![이미지](/assets/img/2024-06-23-ImperativevsDeclarativeKubernetesCommands_0.png)\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 명령문\n\n명령문은 클러스터의 객체에 직접적으로 작용하며, 이러한 명령문은 객체의 상태를 즉시 변경합니다.\n\n이러한 명령문의 예시:\n\n- **kubectl create** — 이 명령어는 객체(e.g. 배포(Deployment), 레플리카셋(ReplicaSet) 등)를 생성하는 데 사용됩니다.\n- **kubectl run** — 이 명령어는 포드(Pod)를 생성하는 데 사용됩니다.\n- **kubectl expose** — 이 명령어는 배포(Deployment) 또는 레플리카셋(ReplicaSet)에 대한 서비스를 생성하는 데 사용됩니다.\n- **kubectl scale** — 이 몤령어는 배포(Deployment) 또는 레플리카셋(ReplicaSet)에서 레플리카의 수를 확장하거나 축소하는 데 사용됩니다.\n- **kubectl delete** — 이 명령어는 객체(e.g. 배포(Deployment), 포드(Pod) 등)를 삭제하는 데 사용됩니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예시\n\n첫 번째 명령은 nginx 컨테이너를 실행하는 Pod 객체를 생성합니다. 두 번째 명령\n\nnginx 컨테이너를 실행하는 Pod 객체를 생성합니다:\n\n```js\nkubectl run nginx --image=nginx\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nnginx 컨테이너를 실행하는 ReplicaSets가 있는 Deployment 객체를 만들어보세요:\n\n```js\nkubectl create deployment nginx --image nginx\n```\n\nImperative 명령어들은 보통 사용하기 쉽습니다. 학습이나 테스트 프로젝트에는 훌륭한데, Git과 같은 버전 관리 시스템에서 시스템 변경사항을 추적할 수 없기 때문에 프로덕션 시스템에서는 일반적으로 피해야합니다.\n\n## 명령어를 사용한 객체 구성\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n명령형 객체 구성에서는 kubectl 명령어를 사용하여 작업(생성, 대체 등), 선택적 플래그 및 하나 이상의 파일 이름을 지정합니다.\n\n예시\n\n구성 파일에 정의된 객체를 생성합니다:\n\n```js\nkubectl create -f config.yaml\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 구성 파일에서 정의된 객체를 삭제합니다:\n\n```js\nkubectl delete -f config1.yaml -f config2.yaml\n```\n\n## 선언적 객체 구성\n\n명령형 명령어와는 달리 객체에 대한 작업을 수행하기 위해 정확한 단계를 올바른 순서대로 수행해야 하는 절차적 방법과 달리, 선언적 접근 방식은 선언적 매니페스트 파일에서 객체의 원하는 상태를 선언하고 Kubernetes가 kubectl applycommand를 사용하여 객체의 원하는 상태를 달성하도록 관리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예시\n\nconfigs 디렉토리에 있는 모든 객체 구성 파일을 처리하고 라이브 객체를 생성하거나 패치합니다. 무엇이 변경될 것인지 먼저 확인하고 적용할 수 있습니다:\n\n```js\nkubectl diff -f configs/\nkubectl apply -f configs/\n```\n\n디렉토리를 재귀적으로 처리합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl diff -R -f configs/\nkubectl apply -R -f configs/\n```\n\n딱지 복제 파일은 YAML 또는 JSON으로 작성되며 Kubernetes 객체의 원하는 상태를 정의합니다. 선언적 매니페스트가 클러스터에 적용되면 Kubernetes는 객체의 현재 상태와 원하는 상태를 비교하여 원하는 상태를 달성하기 위해 필요한 변경을 수행합니다.\n\n다음은 선언적 매니페스트 예시입니다 — configs/nginx-deployment.yaml\n\n```js\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx-container\n          image: nginx:latest\n          ports:\n            - containerPort: 8080\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 manifest는 nginx:latest Docker 이미지를 기반으로 하는 컨테이너의 복제본을 두 개 실행해야 한다는 배포를 설명합니다.\n\n이렇게 하면, 우리는 많은 선언적 manifest 파일들을 직접적으로 구성에 추가하고, 모두를 하나의 kubectl apply -f configs/ 명령어로 적용할 수 있습니다.\n\n일반적으로 선언적 접근 방식은 변경 사항을 버전 관리 시스템에서 추적할 수 있게 하며, 코드 리뷰를 가능하게 하고 변경 사항을 CI/CD 파이프라인에서 자동으로 적용하는 것을 가능하게 합니다.\n\n이 글이 마음에 드셨다면 팔로우 버튼을 눌러 주세요. 더 이상의 이와 유사한 글을 읽고 싶으시다면요\n","ogImage":{"url":"/assets/img/2024-06-23-ImperativevsDeclarativeKubernetesCommands_0.png"},"coverImage":"/assets/img/2024-06-23-ImperativevsDeclarativeKubernetesCommands_0.png","tag":["Tech"],"readingTime":5},{"title":"K8sGPT  Ollama 무료 Kubernetes 자동 진단 솔루션 사용법","description":"","date":"2024-06-23 23:00","slug":"2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution","content":"\n![Kubernetes Automated Diagnosis Tool: k8sgpt-operator](/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png)\n\n주말에 블로그 초고를 확인했더니 이 글이 있었어요. 한 해 전 'Kubernetes Automated Diagnosis Tool: k8sgpt-operator'를 쓸 때의 기억이 떠오르네요. 처음에는 K8sGPT + LocalAI를 써보려 했지만, Ollama로 시도해보니 더 사용하기 편리했어요. 게다가 Ollama는 OpenAI API를 지원하기도 해서 Ollama로 바꾸기로 결정했죠.\n\nk8sgpt-operator를 소개하는 글을 게시한 후 몇몇 독자들이 OpenAI를 사용하기 위한 높은 진입 장벽을 언급했어요. 이 문제는 정말 어려운 문제이지만 극복할 수 있는 문제에요. 하지만 이 글은 그 문제를 해결하는 게 아니라 OpenAI 대안인 Ollama를 소개하기 위한 글이에요. 작년 말에 k8sgpt는 CNCF Sandbox에 들어갔어요.\n\n# 1. Ollama 설치하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Ollama](/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_1.png)\n\nOllama은 로컬이나 클라우드에서 쉽게 설치하고 실행할 수 있는 여러 대형 모델을 지원하는 오픈 소스 대형 모델 도구입니다. 매우 사용하기 편리하며 간단한 명령어로 실행할 수 있습니다. macOS에서는 homebrew를 사용하여 다음 명령어로 쉽게 설치할 수 있습니다:\n\n```js\nbrew install ollama\n```\n\n최신 버전은 0.1.44입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nollama -v\n경고: 실행 중인 Ollama 인스턴스에 연결할 수 없습니다\n경고: 클라이언트 버전은 0.1.44입니다\n```\n\n리눅스에서는 공식 스크립트로도 설치할 수 있습니다.\n\n```js\ncurl -sSL https://ollama.com/install.sh | sh\n```\n\nOllama를 시작하고 컨테이너나 K8s 클러스터에서 접근할 수 있도록 환경 변수를 통해 수신 주소를 0.0.0.0으로 설정하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nOLLAMA_HOST=0.0.0.0 ollama start\n```\n\n```js\n...\ntime=2024-06-16T07:54:57.329+08:00 level=INFO source=routes.go:1057 msg=\"127.0.0.1:11434에서 수신 대기 중 (버전 0.1.44)\"\ntime=2024-06-16T07:54:57.329+08:00 level=INFO source=payload.go:30 msg=\"임베디드 파일 추출 중\" dir=/var/folders/9p/2tp6g0896715zst_bfkynff00000gn/T/ollama1722873865/runners\ntime=2024-06-16T07:54:57.346+08:00 level=INFO source=payload.go:44 msg=\"동적 LLM 라이브러리 [metal]\"\ntime=2024-06-16T07:54:57.385+08:00 level=INFO source=types.go:71 msg=\"추론 계산 중\" id=0 library=metal compute=\"\" driver=0.0 name=\"\" total=\"21.3 GiB\" available=\"21.3 GiB\"\n```\n\n# 2. 큰 모델 다운로드 및 실행하기\n\n4월에 Meta에서 오픈 소스로 공개된 인기 있는 큰 모델 중 하나인 Llama3가 있습니다. Llama3에는 8B와 70B 두 가지 버전이 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n맥OS에서 실행 중이고, 8B 버전을 선택했어요. 8B 버전은 4.7GB이며, 빠른 인터넷 연결로 다운로드하면 3-4분이 소요돼요.\n\n```js\nollama run llama3\n```\n\n제 M1 Pro에서 32GB 메모리를 사용하고 있는데, 실행하는 데 약 12초 정도 걸려요.\n\n```js\ntime=2024-06-17T09:30:25.070+08:00 level=INFO source=server.go:572 msg=\"llama runner started in 12.58 seconds\"\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n각 쿼리마다 약 14초가 소요됩니다.\n\n```js\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n```js\n....\n\"total_duration\":14064009500,\"load_duration\":1605750,\"prompt_eval_duration\":166998000,\"eval_count\":419,\"eval_duration\":13894579000}\n```\n\n# 3. K8sGPT CLI 백엔드 구성하기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 k8sgpt-operator를 테스트하려면 이 단계를 건너뛸 수 있어요.\n\nk8sgpt의 백엔드로 Ollama REST API를 사용할 거에요. 이 API는 추론 제공자로 기능하며, backend 유형은 localai로 선택했어요. LocalAI는 OpenAI API와 호환되며, 실제 제공자는 여전히 Llama를 실행하는 Ollama일 거예요.\n\n```js\nk8sgpt auth add --backend localai --model llama3 --baseurl http://localhost:11434/v1\n```\n\n이를 기본 제공자로 설정하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nk8sgpt auth default --provider localai\nlocalai로 기본 제공자가 설정되었습니다.\n\n테스트 중:\n\nimage-not-exist 이미지를 사용하여 k8s 내에서 Pod를 생성합니다.\n\nkubectl get po k8sgpt-test\n이름          준비     상태         다시 시작     나이\nk8sgpt-test   0/1     ErrImagePull   0          6초\n\n<!-- ui-station 사각형 -->\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n에러를 분석하려면 k8sgpt를 사용해보세요.\n\nk8sgpt analyze --explain --filter=Pod --namespace=default --output=json\n\n{\n  \"provider\": \"localai\",\n  \"errors\": null,\n  \"status\": \"ProblemDetected\",\n  \"problems\": 1,\n  \"results\": [\n    {\n      \"kind\": \"Pod\",\n      \"name\": \"default/k8sgpt-test\",\n      \"error\": [\n        {\n          \"Text\": \"Back-off pulling image \\\"image-not-exist\\\"\",\n          \"KubernetesDoc\": \"\",\n          \"Sensitive\": []\n        }\n      ],\n      \"details\": \"Error: Back-off pulling image \\\"image-not-exist\\\"\\n\\nSolution: \\n1. Check if the image exists on Docker Hub or your local registry.\\n2. If not, create the image using a Dockerfile and build it.\\n3. If the image exists, check the spelling and try again.\\n4. Verify the image repository URL in your Kubernetes configuration file (e.g., deployment.yaml).\",\n      \"parentObject\": \"\"\n    }\n  ]\n}\n\n# 4. k8sgpt-operator 배포 및 설정하기\n\n<!-- ui-station 사각형 -->\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nk8sgpt-operator은 클러스터 내에서 k8sgpt를 자동화할 수 있습니다. Helm을 사용하여 쉽게 설치할 수 있어요.\n\n```\n\nhelm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt --create-namespace\n\nk8sgpt-operator는 K8sGPT를 구성하고 분석 결과를 출력하는 Result를 위한 두 가지 CRD를 제공합니다.\n\nkubectl api-resources | grep -i gpt\nk8sgpts core.k8sgpt.ai/v1alpha1 true K8sGPT\nresults core.k8sgpt.ai/v1alpha1 true Result\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nOllama의 IP 주소를 baseUrl로 사용하여 K8sGPT를 구성하세요.\n\n```js\nkubectl apply -n k8sgpt -f - << EOF\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-ollama\nspec:\n  ai:\n    enabled: true\n    model: llama3\n    backend: localai\n    baseUrl: http://198.19.249.3:11434/v1\n  noCache: false\n  filters: [\"Pod\"]\n  repository: ghcr.io/k8sgpt-ai/k8sgpt\n  version: v0.3.8\nEOF\n```\n\nK8sGPT CR을 생성한 후, 연산자(operator)가 이를 위한 파드를 자동으로 만듭니다. result CR을 확인하면 동일한 결과가 표시됩니다.\n\n```js\nkubectl get result -n k8sgpt -o jsonpath='{.items[].spec}' | jq .\n{\n  \"backend\": \"localai\",\n  \"details\": \"Error: Kubernetes is unable to pull the image \\\"image-not-exist\\\" due to it not existing.\\n\\nSolution: \\n1. Check if the image actually exists.\\n2. If not, create the image or use an alternative one.\\n3. If the image does exist, ensure that the Docker daemon and registry are properly configured.\",\n  \"error\": [\n    {\n      \"text\": \"Back-off pulling image \\\"image-not-exist\\\"\"\n    }\n  ],\n  \"kind\": \"Pod\",\n  \"name\": \"default/k8sgpt-test\",\n  \"parentObject\": \"\"\n}\n```\n","ogImage":{"url":"/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png"},"coverImage":"/assets/img/2024-06-23-K8sGPTOllamaAFreeKubernetesAutomatedDiagnosticSolution_0.png","tag":["Tech"],"readingTime":9},{"title":"GKE 멀티 클러스터 서비스MCS 처음에는 마법 같은 느낌이 드는 이유","description":"","date":"2024-06-23 22:58","slug":"2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst","content":"\nMulti-Cluster Services (MCS)은 한 GKE 클러스터에서 워크로드 간 통신을 허용하는 일반적인 문제에 대한 해결책을 제공합니다. 이 서비스는 이 글에서 MCS가 어떻게 구성되는지, 어떻게 구축되는지, 그리고 클러스터와 주변 인프라 수준에서 어떤 구성 요소가 관련되어 있는지 살펴볼 것입니다. 이를 통해 서비스에 대한 심층적인 이해를 얻고 문제 해결에 대한 신뢰를 높일 수 있을 것입니다.\n\n만약 플릿 및 MCS에 대한 일반 소개를 원한다면 Kishore Jagannath의 훌륭한 두 부분 블로그 포스트를 추천합니다.\n\n이 블로그 포스트에서는 조금 다른 방식으로 MCS의 구성 요소를 분석할 것입니다. 운이 좋다면 이런 과정이 기술이 마법처럼 느껴질 때의 어린 시절 추억을 떠올리게 할지도 모릅니다. 그러나 다행히 이번에는 가족용 계산기를 조금 복잡하게 살펴보고 고쳤다가 다시 조립했을 때 처럼 부모님께서 화를 내지 않을 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 우선 순위를 정하자 — MCS를 사용해야 할 때\n\nMCS의 내부 작동 방식을 더 깊이 들어가기 전에, 교차 클러스터 통신 영역을 탐색하기 전에 먼저 한 걸음 물러나보겠습니다. 아래 다이어그램에서 볼 수 있듯이, MCS는 쿠버네티스 또는 구체적으로 GKE 클러스터에서 실행되는 서비스를 사용하는 것을 허용하기 위한 여러 가능한 솔루션 중 하나에 불과합니다.\n\nMCS는 같은 플리트 내의 다른 클러스터에서 실행되는 팟에 백업된 서비스와 통신할 수 있도록 하는 작업에 집중하는 비교적 간단한 해결책입니다. 동일한 교차 클러스터 통신은 GKE의 Service Mesh나 Istio와 같은 서비스 메시의 다중 클러스터 기능이나 Cilium과 같은 네트워크 수준의 도구를 사용하여 구현할 수도 있습니다. 이미 이러한 방식 중 하나를 사용하고 있거나 교차 클러스터 통신 상단에 트래픽 관리, 투명 인증 또는 텔레미트리와 같은 기능을 사용할 계획이라면, 아마도 MCS는 너무 단순해서 당신의 사용 사례에 적합하지 않을 것이며, 대신 서비스 메시를 사용하는 것이 좋을 것입니다.\n\n<img src=\"/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_1.png\" />\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# MCS 데모 준비 중\n\n이 블로그 포스트에서 실제 탐구를 따라하고 싶다면, MCS와 놀 수 있도록 필요한 API를 활성화하고 GKE autopilot 클러스터 두 개를 생성하는 단계를 수행할 수 있습니다. 대신 표준 GKE 클러스터를 사용하고 싶다면, 여기 제공된 예시도 잘 작동할 것입니다.\n\n```js\nexport PROJECT_ID=<여기에 프로젝트 ID 입력>\n\ngcloud services enable \\\n    compute.googleapis.com \\\n    container.googleapis.com \\\n    multiclusterservicediscovery.googleapis.com \\\n    gkehub.googleapis.com \\\n    cloudresourcemanager.googleapis.com \\\n    trafficdirector.googleapis.com \\\n    dns.googleapis.com \\\n    --project=$PROJECT_ID\n\ngcloud container clusters create-auto \"test-us-cluster\" \\\n  --region \"us-central1\" --enable-master-authorized-networks \\\n  --network \"default\" --subnetwork \"default\" \\\n  --services-ipv4-cidr 10.99.0.0/20 \\\n  --async --project \"$PROJECT_ID\"\n\ngcloud container clusters create-auto \"test-eu-cluster\" \\\n  --region \"europe-west1\" --enable-master-authorized-networks \\\n  --network \"default\" --subnetwork \"default\" \\\n  --services-ipv4-cidr 10.99.16.0/20 \\\n  --async --project \"$PROJECT_ID\"\n```\n\n클러스터가 준비되면, 피트에서 다중 클러스터 기능을 활성화하고 새로 생성된 클러스터를 피트에 추가할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ngcloud container fleet multi-cluster-services enable --project $PROJECT_ID\n\ngcloud container fleet memberships register test-us-cluster \\\n   --gke-cluster us-central1/test-us-cluster \\\n   --enable-workload-identity \\\n   --project $PROJECT_ID\n\ngcloud container fleet memberships register test-eu-cluster \\\n   --gke-cluster europe-west1/test-eu-cluster \\\n   --enable-workload-identity \\\n   --project $PROJECT_ID\n```\n\n우리 클러스터 상황을 확인해보죠. 이를 위해 europe-west1 지역의 클러스터에 연결해봅시다:\n\n```js\ngcloud container clusters get-credentials test-eu-cluster --region europe-west1 --project $PROJECT_ID\n```\n\nMCS의 흔적이 이미 존재하는지 확인하기 위해 우리의 네임스페이스 리소스를 나열해보겠습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl get ns\n```\n\n네임스페이스 목록에는 \"gke-mcs\"라는 새로 생성된 네임스페이스가 포함되어 있어야 합니다. 해당 네임스페이스 이름은 이미 플릿에서 활성화된 MCS 기능과 관련이 있을 가능성이 높으며, 또한 해당 네임스페이스가 만들어진 시간은 클러스터를 플릿에 등록했을 때와 일치합니다.\n\n\"gke-mcs\" 네임스페이스를 좀 더 자세히 알아보고 이미 실행 중인 것이 있는지 확인해 봅시다:\n\n```js\nkubectl get all -n gke-mcs # 이 명령은 gke-mcs-importer에 대한 배포를 보여줍니다.\n\nkubectl logs -n gke-mcs -l k8s-app=gke-mcs-importer --tail -1 # Importer의 로그를 얻기 위해\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n로그에서는 수입 업체가 트래픽 디렉터 API에 액세스할 권한이 아직 부여되지 않았기 때문에 권한 오류가 발생할 수 있습니다:\n\n```js\n핸들러 오류: 스트림을 통해 ADS 응답 수신 중: 권한이 거부되었습니다:\nrpc 오류: 코드 = PermissionDenied desc = 권한\n'trafficdirector.networks.getConfigs'이 자원에 대해 거부되었습니다\n'//trafficdirector.googleapis.com/projects/...' (또는 존재하지 않을 수 있음).\n```\n\n수입 업체의 워크로드 ID를 사용하여 Cloud DNS에서 정보를 가져오는 데 사용되는 네트워크 뷰어 역할을 할당함으로써 이 문제를 해결할 수 있습니다:\n\n```js\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member \"serviceAccount:$PROJECT_ID.svc.id.goog[gke-mcs/gke-mcs-importer]\" \\\n    --role \"roles/compute.networkViewer\"\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 로그 문을 다시 실행하면 예상대로 폴링이 작동하지만 우리를 위해 생성된 존이 없다는 것을 확인할 것입니다. 이제 클러스터는 다중 클러스터 서비스를 배포할 준비가 되었습니다.\n\n# 데모 응용 프로그램 배포\n\n두 GKE 클러스터 간의 컨텍스트 전환을 쉽게하기 위해 먼저 클러스터 컨텍스트의 이름을 변경합니다. 또는 클라우드 셸에 미리 설치된 kubectx 단축키를 사용할 수도 있습니다.\n\n```js\ngcloud container clusters get-credentials test-us-cluster --region us-central1 --project $PROJECT_ID\nkubectl config rename-context \"$(kubectl config current-context)\" mcs-us\n\ngcloud container clusters get-credentials test-eu-cluster --region europe-west1 --project $PROJECT_ID\nkubectl config rename-context \"$(kubectl config current-context)\" mcs-eu\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 예시에서는 지나치게 화려한 애플리케이션이 필요하지 않습니다. 결국, 다른 GKE 클러스터에서 실행 중인 특정 서비스에 도달할 수 있는지 여부를 보여주기만 원합니다. 데모 목적으로, 우리의 EU 클러스터에서 기존의 hello-web 예제를 배포하고 클러스터 내에서 전통적인 ClusterIP 서비스로 노출합니다.\n\n```js\nkubectl create ns shared-services --context mcs-eu\n\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/kubernetes-engine-samples/main/quickstarts/hello-app/manifests/helloweb-deployment.yaml -n shared-services  --context mcs-eu\n\nkubectl expose deployment/helloweb --port 8080 -n shared-services --context mcs-eu\n```\n\n서비스가 생성되면 클러스터 내에서 기대대로 자동으로 생성된 k8s cluster.local DNS 이름을 사용하여 성공적으로 호출할 수 있습니다:\n\n```js\nkubectl run test-curl --image=curlimages/curl -it --rm --pod-running-timeout=4m --context mcs-eu -- curl -v http://helloweb.shared-services.svc.cluster.local:8080\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n흥미로운 사이드 노트와 나중에 중요한 세부 정보는, 우리의 GKE 클러스터가 클라우드 DNS를 사용한다는 것입니다. 따라서 Google Cloud 콘솔의 Cloud DNS UI에서 우리 서비스를 위해 자동으로 생성된 A 레코드도 볼 수 있습니다. DNS 존에는 명시적으로 이 Cloud DNS 존이 특정 GKE 클러스터에서만 사용 가능하다고 나와 있습니다. 이 DNS 존은 어떤 VPC에도 첨부되어 있지 않기 때문에 전형적인 사설 존은 아닙니다.\n\n![그림](/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_2.png)\n\nMCS의 목표는 미국 클러스터에서 이 서비스를 사용할 수 있도록 하는 것입니다. 서비스를 노출하기 전에 기본 서비스에서 어떤 일이 발생하는지 살펴봅시다. 아마도 예상하시겠지만, EU 클러스터에서 사용한 DNS 레코드는 미국 클러스터에서 사용할 수 없으며, 심지어 서비스 IP도 미국 클러스터에서 도달할 수 없습니다. 클러스터를 생성할 때 RFC 1918 범위를 사용했지만, 미국 클러스터에서는 이러한 클러스터를 호출할 수 없기 때문입니다.\n\n```js\n# 호스트 이름을 해결할 수 없다는 오류로 실패합니다\nkubectl run test-curl --image=curlimages/curl -it --rm --pod-running-timeout=4m --context mcs-us -- curl -v http://helloweb.shared-services.svc.cluster.local:8080\n\n# 타임아웃으로 실패합니다\nEU_SERVICE_IP=\"$(kubectl get svc -l app=hello -n shared-services --context mcs-eu -ojsonpath='{.items[*].spec.clusterIP}')\"\nkubectl run test-curl --image=curlimages/curl -it --rm --pod-running-timeout=4m --context mcs-us -- curl -v \"http://$EU_SERVICE_IP:8080\"\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희가 겪고 있는 문제는 미국 클러스터의 작업량이 EU 클러스터의 작업량에 도달하지 못하는 것이 아니라, 아래에서 직접 Pod IP를 호출하여 증명할 수 있는 것입니다. 네트워크 수준에서는 VPC 네이티브 클러스터 네트워킹을 사용하여 VPC 내에서 Pod IP가 경로 지정될 수 있기 때문에 이 작업이 가능합니다. 아래 예시에서는 양 쪽 클러스터의 API 서버에 연결하여 이를 활용하여 서로간의 교차 클러스터 통신을 증명하는 데 사용합니다.\n\n```js\nEU_POD_IP=\"$(kubectl get po -l app=hello -n shared-services --context mcs-eu -ojsonpath='{.items[*].status.podIP}')\"\n\nkubectl run test-curl --image=curlimages/curl -it --rm --pod-running-timeout=4m --context mcs-us -- curl -v http://$EU_POD_IP:8080\n```\n\n물론 포드 IP를 명시적으로 Kubernetes API에 쿼리하는 것은 확장 가능한 해결책이 아닙니다. 이것이 클러스터 간에 서비스 발견을 자동화하기 위해 MCS로 전환해야 하는 이유입니다.\n\n# MCS로 서비스 내보내기\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMCS에서 실행 중인 서비스를 EU 클러스터에서 내보내려면 EU 클러스터에 ServiceExport 리소스를 만듭니다. 동시에 해당 서비스를 가져올 US 클러스터의 이름 공간을 만듭니다:\n\n```js\nkubectl create ns shared-services --context mcs-us\n\nkubectl apply --context mcs-eu -f - <<EOF\napiVersion: net.gke.io/v1\nkind: ServiceExport\nmetadata:\n namespace: shared-services\n name: helloweb\nEOF\n```\n\n이제 방금 내보낸 서비스와 관련된 로그 항목이 포함된 MCS 가져오기자 로그를 US 클러스터에서 살펴볼 수 있습니다:\n\n```js\nkubectl logs -n gke-mcs -l k8s-app=gke-mcs-importer --tail=25 --context mcs-us\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n미국 클러스터가 EU에서 실행 중인 작업에 대한 정보를 수신했음을 나타내고 해당 정보를 자동으로 사용하는 엔드포인트가 만들어 졌음을 나타내고 있습니다:\n\n```js\nADS 응답을 받음 (europe-west1-d), 유형: type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\neurope-west1-d에서 1개의 negs로부터 업데이트\n엔드포인트 \"gke-mcs-...\" 생성 중\n```\n\nMCS importer를 통해 미국 클러스터에서 자동으로 생성된 리소스를 살펴봅시다.\n\n```js\nkubectl get ServiceImport -n shared-services --context mcs-us\n\nkubectl get Service -n shared-services --context mcs-us\n\nkubectl get Endpoints -n shared-services --context mcs-us\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 명령어에서 나열된 엔드포인트들은 EU 클러스터에서 실행 중인 워크로드의 pod ip를 포함하고 있습니다. 이는 ServiceExport 리소스가 이전 섹션에서 수동으로 제공한 교차 클러스터 리소스 식별을 자동화했다는 것을 의미합니다. 이제 위에서 찾은 서비스와 엔드포인트를 테스트하고 미국 클러스터 내에서부터 호출해 봅시다:\n\n```js\nSVC_NAME=$(kubectl get service -o=jsonpath='{.items[?(@.metadata.annotations.net\\.gke\\.io/service-import==\"helloweb\")].metadata.name}' -n shared-services --context mcs-us)\n\n\nkubectl run test-curl --image=curlimages/curl -it --rm \\\n  --pod-running-timeout=4m --context mcs-us -- \\\n  curl -v http://$SVC_NAME.shared-services.svc.cluster.local:8080\n```\n\n작동이 잘 되었네요. 이번에는 EU 클러스터의 API 서버와 통신하여 워크로드를 실행 중인 pod의 IP를 찾아낼 필요가 없었습니다. 왜냐하면 Service Import가 이미 해당 정보를 동기화했기 때문입니다. 유일하게 해결해야 할 문제는 \"gke-mcs-`해시`\" 형식으로 자동 생성된 MCS importer의 서비스 이름이 사전에 쉽게 알려지지 않는다는 것입니다. 위 예에서도 다시 API 서버를 사용하여 올바른 서비스 이름을 검색했습니다. 실제 사용 사례에서는 워크로드가 원격 서비스를 호출하기 전에 쿠버네티스 API 서버에 문의하는 것을 원치 않는 것은 당연합니다. 이는 추상화를 깨뜨리고 파드의 서비스 계정에 불필요한 권한이 필요하게 됩니다.\n\n클러스터셋 로컬 호스트 이름의 호기심스러운 사례#\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자동 생성되는 기억하기 어려운 서비스 이름 문제를 해결하기 위해 MCS가 제공하는 편리한 DNS 기반 솔루션을 사용할 수 있습니다. 각 가져온 서비스에 대해 \"SERVICE_EXPORT_NAME.NAMESPACE.svc.clusterset.local\" 형식의 DNS 항목을 만듭니다. 이 호스트 이름을 사용하여 생성된 서비스 이름을 식별하는 추가 단계 없이 서비스를 호출할 수 있는 결정론적인 방법이 생겼습니다. 가져온 서비스의 호스트 이름을 작성하고 클러스터 중 하나에서 팟 내부에서 호출할 수 있습니다:\n\n```js\nkubectl run test-curl --image=curlimages/curl -it --rm \\\n  --pod-running-timeout=4m --context mcs-eu -- \\\n  curl http://helloweb.shared-services.svc.clusterset.local:8080\n\nkubectl run test-curl --image=curlimages/curl -it --rm \\\n  --pod-running-timeout=4m --context mcs-us -- \\\n  curl http://helloweb.shared-services.svc.clusterset.local:8080\n```\n\n참고: 위 요청 중 하나에 대해 오류가 발생하면 DNS 캐싱 때문일 수 있습니다. 관리 DNS 존이 클러스터셋 로컬 DNS A 레코드를 나열하면 결국 팟에서 인식할 수 있습니다.\n\n처음에 예상하지 못한 흥미로운 점 중 하나는 clusterset.local DNS 존이 Cloud DNS UI에 노출되지 않는다는 것입니다. 하지만 우리 호스트 이름을 위해 레코드 세트가 있는 하부 관리 DNS 존이 있음을 다음 명령어를 실행하여 확인할 수 있습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ngcloud dns managed-zones list --location=us-central1-b\n\ngcloud dns managed-zones describe <name of the zone from above> --location=us-central1-b\n\ngcloud dns record-sets list --location=us-central1-b --zone <name of the zone from above\n```\n\n만약 클라우드 콘솔에서 clusterset.local 호스트명을 보고 싶다면 Traffic Director로 이동할 수 있어요. GCP 콘솔에서 Traffic Director UI를 열면 라우팅 규칙 맵 탭에서 우리의 플릿과 연결된 정방향 규칙 및 helloweb 서비스가 연결된 서비스로 나열된 라우팅 규칙을 볼 수 있어요. 또한 이에 연결된 정방향 규칙도 표시돼요.\n\n<img src=\"/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_3.png\" />\n\n라우팅 규칙의 이름을 클릭하면 정방향 규칙과 라우팅 규칙에서 사용된 호스트명 목록을 볼 수 있어요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_4.png\" />\n\n# [옵션 부분] 계산기를 해체하고 다시 조립하는\n\n만약 그게 맞다면, 우리의 호스트 이름이 실제로 전달 규칙에서 처리되고 내부 로드 밸런서의 경우처럼 연결된 NEG로 보내진다고 가정하는 유혹을 느낄 수 있습니다. 이것이 맞다면, 우리는 미국 클러스터의 쿠버네티스 리소스가 EU 서비스와 통신하기 위해서는 서비스와 엔드포인트에 대한 Kubernetes 리소스가 필요하지 않을 것입니다. 여기서 우리는 다시 시작점으로 돌아와 계산기를 해체해 우리의 이해를 확인하는 유사성을 다시 살펴보는 지점에 도달합니다.\n\n이 가정을 확인하기 위해 미국 클러스터의 shared-services 네임스페이스를 삭제해볼 수 있습니다. 이렇게 하면 우리가 이전에 살펴본 서비스와 엔드포인트를 포함한 모든 네임스페이스 리소스를 삭제합니다. 마지막으로 미국 클러스터의 파드에서 curl을 다시 실행하고 싶을 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl delete ns shared-services --context mcs-us\n\nkubectl run test-curl --image=curlimages/curl -it --rm \\\n  --pod-running-timeout=4m --context mcs-us -- \\\n  curl http://helloweb.shared-services.svc.clusterset.local:8080\n```\n\n위의 curl 명령은 실패할 것이며, MCS Importer가 shared-services 네임스페이스에 생성한 리소스가 실제로 필요했음을 확인합니다. 이것은 클러스터 외부의 라우트 규칙에서 호스트 이름이 구성되어 있더라도 해당된다는 것을 의미합니다. 계산기를 되돌려 놓고 MCS Importer가 리소스를 다시 생성할 수 있도록 shared-services 네임스페이스를 재생성합시다.\n\n```js\nkubectl create ns shared-services --context mcs-us\n```\n\nImporter가 서비스 및 엔드포인트 리소스를 다시 생성한 후, 우리는 버보즈 출력으로 curl 명령을 다시 실행합니다. 이렇게 하면 서비스를 삭제했을 때 위와 같은 이유로 실패했던 것을 확인할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nkubectl run test-curl --image=curlimages/curl -it --rm \\\n  --pod-running-timeout=4m --context mcs-us -- \\\n  curl http://helloweb.shared-services.svc.clusterset.local:8080\n```\n\n여기서 볼 수 있듯이 호스트 이름이 우리 서비스의 클러스터 IP로 해석되어 MCS에 접근하기 위해 서비스 리소스가 여전히 필요합니다.\n\n# 결론\n\n이 모든 실험과 탐구를 통해 MCS에 대한 우리의 이해를 완성할 수 있습니다. 우리는 지금 관련된 구성 요소를 이해하고, 처음에는 다소 마술적으로 보였던 기능을 가능하게 하는 것에 대해 더 나은 그림을 그릴 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![GKE Multi-Cluster Services](/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_5.png)\n\n이 게시물에서는 MCS를 실제로 경험하면서 뿐만 아니라 여러 가지를 부수고 다시 조립하는 과정으로 탐구했습니다. 우리는 한 클러스터에 서비스를 생성하고, 그런 다음 MCS를 단계별로 구축하고 워크로드에서 서비스를 소비하는 방식으로 다른 클러스터에서 실행되는 워크로드 사이의 통신을 가능하게 하는 리소스를 살펴봄으로써 MCS를 살펴보았습니다.\n\n자신만의 멀티 클러스터 서비스 여정을 계속해 보고 싶다면 문서에서의 MCS 예제를 살펴보고, 멀티 클러스터 통신 위에 추가 기능을 제공하는 서비스 메시와 같은 대체 구현도 고려해보세요.\n","ogImage":{"url":"/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_0.png"},"coverImage":"/assets/img/2024-06-23-GKEMulti-ClusterServicesMCSFeelslikemagicatfirst_0.png","tag":["Tech"],"readingTime":18},{"title":"M 시리즈 맥에서 Multipass로 로컬 클러스터 쉽게 만들기","description":"","date":"2024-06-23 22:57","slug":"2024-06-23-LocalClusterMadeEasywithMultipassonMacMchips","content":"\n<img src=\"/assets/img/2024-06-23-LocalClusterMadeEasywithMultipassonMacMchips_0.png\" />\n\n이 기사는 여러분의 기계에 k3s Kubernetes 환경을 설정하여 여러분의 POC를 테스트하고 CNCF 랜드스케이프의 더 많은 도구들을 탐색하는 방법을 보여줍니다.\n\n여러분의 Mac에서 K3S/K8S를 직접 실행할 수 없기 때문에 여러분은 Mac 위에 Linux 레이어를 설정해야 합니다. Mac M1에서 Linux VM을 설정하는 쉬운 방법은 Multipass를 사용하는 것입니다.\n\n왜 Multipass를 사용해야 하는지요?\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 번의 명령어로 즉시 Ubuntu VM을 얻을 수 있어요.\n\n먼저, Multipass를 설치해야 해요.\n\n```js\nbrew install --cask multipass\n```\n\n설치되면 메모리 및 디스크 공간을 지정하여 새 VM을 생성해보세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nmultipass launch --name k3s --memory 4G --disk 40G\n```\n\n우리는 심지어 VM에서 Mac 디렉터리를 마운트할 수도 있어요.\n\n```js\nmkdir ~/test/k8s\nmultipass mount ~/test/k8s k3s:~/k8s\n```\n\n호스트 디렉토리에서 변경 사항을 만들고 VM 내부의 클러스터에 변경 사항을 적용하려고 할 때 유용할 거예요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nVM 내부에서 설치 스크립트를 실행하여 k3s를 설치할 수 있어요.\n\n```js\nmultipass shell k3s\n\nubuntu@k3s:~$ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -\n```\n\nVM이 시작되면 VM 세부 정보를 확인할 수 있어요.\n\n```js\nmultipass info k3s\n\nName:           k3s\nState:          Running\nSnapshots:      0\nIPv4:           192.168.64.7\n                10.42.0.0\n                10.42.0.1\nRelease:        Ubuntu 24.04 LTS\nImage hash:     8263b4713896 (Ubuntu 24.04 LTS)\nCPU(s):         1\nLoad:           0.29 0.22 0.13\nDisk usage:     2.8GiB out of 38.7GiB\nMemory usage:   814.2MiB out of 3.8GiB\nMounts:         /Users/ibrahimmohamed/test/k8s => ~/k8s\n                    UID map: 501:default\n                    GID map: 20:default\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```shell\nK3S_IP=$(multipass info k3s | grep IPv4 | awk '{print $2}')\n\necho $K3S_IP\n\n192.168.64.7# kubeconfig 다운로드\n\nmultipass exec k3s cp /etc/rancher/k3s/k3s.yaml /home/ubuntu/k8s/\n\ncd ~/test/k8s\n\nsed -i '' \"s/127.0.0.1/${K3S_IP}/\" k3s.yaml\n\nexport KUBECONFIG=${PWD}/k3s.yaml\n```\n\n이제 kubeconfig이 있습니다:\n\n머신에 kubectl을 설치하세요:\n\n```shell\nbrew install kubectl\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```sh\nkubectl get nodes -o wide\n\nNAME   STATUS   ROLES                  AGE   VERSION        INTERNAL-IP    EXTERNAL-IP   OS-IMAGE           KERNEL-VERSION     CONTAINER-RUNTIME\nk3s    Ready    control-plane,master   12m   v1.29.5+k3s1   192.168.64.7   <none>        Ubuntu 24.04 LTS   6.8.0-35-generic   containerd://1.7.15-k3s1\n\n\nkubectl get pods -A\n\nNAMESPACE     NAME                                      READY   STATUS      RESTARTS   AGE\nkube-system   coredns-6799fbcd5-dc8nd                   1/1     Running     0          41m\nkube-system   local-path-provisioner-6c86858495-9q524   1/1     Running     0          41m\nkube-system   helm-install-traefik-crd-p4xhh            0/1     Completed   0          41m\nkube-system   metrics-server-54fd9b65b-vmhvc            1/1     Running     0          41m\nkube-system   helm-install-traefik-5snzg                0/1     Completed   1          41m\nkube-system   svclb-traefik-ae8c3cf6-hntgn              2/2     Running     0          40m\nkube-system   traefik-7d5f6474df-48vsc                  1/1     Running     0          40m\n```\n\nNow let's view it through Lens\n\n![Lens](/assets/img/2024-06-23-LocalClusterMadeEasywithMultipassonMacMchips_1.png)\n\nNow you are ready to run any POC on your local machine.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nk3s 실험이 끝나면 VM을 삭제할 수 있습니다.\n\n```js\nmultipass delete k3s\nmultipass purge\n```\n\n화이팅!\n","ogImage":{"url":"/assets/img/2024-06-23-LocalClusterMadeEasywithMultipassonMacMchips_0.png"},"coverImage":"/assets/img/2024-06-23-LocalClusterMadeEasywithMultipassonMacMchips_0.png","tag":["Tech"],"readingTime":5},{"title":"Docker Compose를 사용한 로컬 MongoDB Replica Set 설정 가이드 완벽한 방법","description":"","date":"2024-06-23 22:55","slug":"2024-06-23-TheonlylocalMongoDBreplicasetwithDockerComposeguideyoulleverneed","content":"\n![이미지](/assets/img/2024-06-23-TheonlylocalMongoDBreplicasetwithDockerComposeguideyoulleverneed_0.png)\n\n이 블로그 포스트에서는 MongoDB 레플리카 세트를 로컬에서 실행할 수 있는 다양한 Docker Compose 설정을 탐색해보려고 합니다. 레플리카 세트는 MongoDB의 강력한 기능인 트랜잭션, 변경 스트림 또는 oplog에 액세스하는 것과 같은 것들을 활용하려는 사람들에게 필수적입니다. 로컬에서 MongoDB 레플리카 세트를 실행하면 이러한 기능에 액세스할 뿐만 아니라 복제 메커니즘 및 일반적인 오류 허용성을 실험할 수 있는 일회용 샌드박스로도 작용합니다. 더 이상 기다리지 말고 시작해 봅시다!\n\n# 단일 노드 레플리카 세트 설정\n\n첫 번째 설정은 몇 초만에 MongoDB 단일 노드 레플리카 세트를 실행할 수 있는 준비된 Docker Compose 파일입니다. 클라우드 환경에서는 고가용성과 오류 허용성을 보장하기 위해 복수 노드가 필요할 것입니다. 그러나 로컬 개발에는 단일 노드 레플리카 세트가 충분하며 트랜잭션 및 변경 스트림에 액세스할 수 있습니다. 이는 로컬에서 MongoDB 인스턴스를 실행하는 데 필요한 CPU 및 메모리 리소스를 줄여 Google Chrome을 더 행복하게 만들어줍니다. rs0라는 이름의 단일 노드 레플리카 세트를 실행하는 docker-compose.yml 파일은 다음과 같습니다:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nversion: \"3.8\"\n\nservices:\n  mongo1:\n    image: mongo:7.0\n    command: [\"--replSet\", \"rs0\", \"--bind_ip_all\", \"--port\", \"27017\"]\n    ports:\n      - 27017:27017\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    healthcheck:\n      test: echo \"try { rs.status() } catch (err) { rs.initiate({_id:'rs0',members:[{_id:0,host:'host.docker.internal:27017'}]}) }\" | mongosh --port 27017 --quiet\n      interval: 5s\n      timeout: 30s\n      start_period: 0s\n      start_interval: 1s\n      retries: 30\n    volumes:\n      - \"mongo1_data:/data/db\"\n      - \"mongo1_config:/data/configdb\"\n\nvolumes:\n  mongo1_data:\n  mongo1_config:\n```\n\n여기에서 무슨 일이 벌어지고 있는지 이해해보도록 합시다. 먼저, 우리는 이 글 작성 시 최신 MongoDB Community Edition 이미지인 mongo:7.0을 사용하고 있습니다. rs0라는 레플리카 셋 이름을 지정하기 위해 --replSet 플래그를 사용하고 있습니다. --bind_ip_all 플래그는 MongoDB 인스턴스를 모든 IPv4 주소에 바인딩하기 위해 사용되었으며, --port 플래그는 MongoDB 인스턴스가 수신 대기할 포트를 지정하기 위해 사용되었습니다. 27017은 MongoDB의 기본 포트입니다. 컨테이너 포트 27017을 호스트 포트 27017로 매핑하여 호스트 머신에서 MongoDB 인스턴스에 연결할 수 있도록 하고 있습니다. extra_hosts 섹션은 host.docker.internal 호스트 이름을 호스트 머신의 IP 주소에 매핑하는 데 사용되고 있습니다.\n\nhealthcheck 기능은 우리의 설정에서 레플리카 셋을 초기화하기 위해 재사용되었습니다. 레플리카 셋은 rs.initiate() 명령을 사용하여 초기화되어야 하며(이것은 replSetInitiate 데이터베이스 명령의 동일한 것입니다), 이 작업은 MongoDB 인스턴스가 시작되는 동안 실패할 수 있으므로 healthcheck 기능을 사용하여 작업이 성공할 때까지 재시도하고 있습니다. Docker의 healthcheck은 시작 단계에서 조금 더 공격적일 수 있도록 허용해줍니다. 이것이 start_interval이 1초로만 설정되어 있는 이유입니다. 유감스럽게도 Docker Compose에서 start_interval이 아직 지원되지 않고 있지만, 이것은 사양의 일부입니다. 이 기능에 대한 진행 상황은 해당 GitHub 이슈에서 확인할 수 있습니다. 그 동안 우리는 5초로 일반 interval 값을 설정할 수 있으며, 과도하게 공격적이거나 너무 오랫동안 기다리는 중간 지점입니다. 그러나 start_interval이 구현되면 interval 값을 몇 분 동안 올릴 수 있을 것입니다.\n\n여기서 rs.status()를 사용한 이유는 레플리카 셋이 초기화되지 않았을 때 예외를 throw하기 때문에, 레플리카 셋이 초기화될 때까지 rs.initiate()를 호출하기에 편리합니다. 그 후에는 rs.status()를 주기적으로 호출하는 것은 부담이 되지 않습니다. 또한 여기서 healthcheck가 의도한 대로 작동하고 있다는 점에 유의하십시오. 왜냐하면 우리는 bash 명령이 처음으로 레플리카 셋을 초기화하거나 레플리카 셋이 이미 초기화되어 있는 경우에만 성공적인 종료 코드를 리턴할 것으로 기대하고 있기 때문입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로, 우리는 mongo1_data라는 Docker 볼륨에 데이터를 영속화합니다. 이것은 컨테이너가 중지될 때 데이터가 손실되지 않도록 하는 최선의 방법입니다. 또 다른 볼륨인 mongo1_config은 복제 세트 구성을 영속화하는 데 사용됩니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*4FJZGrr5m7VuvOk-SmYxcg.gif)\n\n이 단일 노드 MongoDB 복제 세트에 액세스하기 위한 연결 문자열은 다음과 같습니다:\n\n```js\nmongodb://127.0.0.1:27017/?replicaSet=rs0\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 쓰림 노드 복제본 설정\n\n이전에 말했듯이, 단일 노드 복제본은 로컬 개발에 충분합니다. 그러나 장애 허용성 및 고 가용성을 실험하려면 여러 노드가 필요합니다. 프로덕션 용도로 사용할 경우, MongoDB 문서에서는 적어도 세 개의 노드를 갖는 것을 권장합니다. 첫 번째 컨테이너는 기본 노드가 되고, 나머지 두 개의 컨테이너는 보조 노드가 됩니다. 다음은 rs0라는 세 개의 노드로 구성된 쓰림 복제본을 실행하기 위한 docker-compose.yml 파일입니다:\n\n```js\nversion: \"3.8\"\n\nservices:\n  mongo1:\n    image: mongo:7.0\n    command: [\"--replSet\", \"rs0\", \"--bind_ip_all\", \"--port\", \"27017\"]\n    ports:\n      - 27017:27017\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    healthcheck:\n      test: echo \"try { rs.status() } catch (err) { rs.initiate({_id:'rs0',members:[{_id:0,host:'host.docker.internal:27017',priority:1},{_id:1,host:'host.docker.internal:27018',priority:0.5},{_id:2,host:'host.docker.internal:27019',priority:0.5}]}) }\" | mongosh --port 27017 --quiet\n      interval: 5s\n      timeout: 30s\n      start_period: 0s\n      start_interval: 1s\n      retries: 30\n    volumes:\n      - \"mongo1_data:/data/db\"\n      - \"mongo1_config:/data/configdb\"\n\n  mongo2:\n    image: mongo:7.0\n    command: [\"--replSet\", \"rs0\", \"--bind_ip_all\", \"--port\", \"27018\"]\n    ports:\n      - 27018:27018\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    volumes:\n      - \"mongo2_data:/data/db\"\n      - \"mongo2_config:/data/configdb\"\n\n  mongo3:\n    image: mongo:7.0\n    command: [\"--replSet\", \"rs0\", \"--bind_ip_all\", \"--port\", \"27019\"]\n    ports:\n      - 27019:27019\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    volumes:\n      - \"mongo3_data:/data/db\"\n      - \"mongo3_config:/data/configdb\"\n\nvolumes:\n  mongo1_data:\n  mongo2_data:\n  mongo3_data:\n  mongo1_config:\n  mongo2_config:\n  mongo3_config:\n```\n\n이 구성에서 기본 노드를 중지하고 보조 노드가 새로운 기본 노드를 선택하는 방식을 확인할 수 있습니다. 이 설정에서 mongo1 컨테이너에는 다른 두 컨테이너보다 약간 더 높은 우선순위가 부여됩니다. 이는 복제본 세트가 완전히 기능할 때 mongo1 컨테이너가 기본 노드로 선출되도록 하는 것입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n보조 노드 중 하나를 중지하고 레플리카 세트가 계속 작동하는지 확인해 볼 수도 있어요. 모든 노드를 중지해 보고 레플리카 세트가 작동을 멈추는지도 확인할 수 있어요. 이는 내결함 허용성과 고가용성을 실험하는 좋은 방법이에요. 레플리카 세트 상태를 쿼리하고 어느 노드가 주 노드인지 확인하려면 rs.status() 몽고 쉘 명령어를 사용하세요.\n\n![image](https://miro.medium.com/v2/resize:fit:1400/1*w9Oxx6FtrIJ4SMj2ySOApA.gif)\n\n3개 노드 레플리카 세트 연결 문자열은:\n\n```js\nmongodb://127.0.0.1:27017,127.0.0.1:27018,127.0.0.1:27019/?replicaSet=rs0\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 연결 문제 해결하기\n\n![이미지](/assets/img/2024-06-23-TheonlylocalMongoDBreplicasetwithDockerComposeguideyoulleverneed_1.png)\n\n만약 MongoDB 복제본 세트에 연결하는 데 문제가 있다면 Docker가 실행 중인지 확인해주세요. 또한 host.docker.internal 호스트명이 호스트 머신의 IP 주소로 해석될 수 있도록도 확인해주세요.\n\nWindows에서는 호스트 파일에 \\*.docker.internal 호스트명을 자동으로 추가하는 설정이 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLinux에서 host.docker.internal을 해결할 수 없는 경우, host.docker.internal을 IP 주소 127.17.0.1로 매핑하기 위해 /etc/hosts 파일에 한 줄을 추가해야 합니다.\n\n# healthcheck에 대한 추가 사항\n\n여기에서 healthcheck을 사용하여 복제 세트를 초기화하는 장점은 docker-compose.yml 파일이 자체 포함되어 있습니다. 수동으로 복제 세트를 초기화하는 것을 선호하는 경우, healthcheck 섹션을 제거하고 rs.initiate() mongosh 명령을 사용하여 복제 세트를 초기화할 수 있습니다.\n\n```js\ndocker compose exec mongo1 mongosh --port 27017 --quiet --eval \"rs.initiate({...})\" --json relaxed\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 docker-compose.yml 파일을 사용할 모든 개발자 여러분께서는 적어도 한 번은 이 작업을 기억해야 합니다.\n","ogImage":{"url":"/assets/img/2024-06-23-TheonlylocalMongoDBreplicasetwithDockerComposeguideyoulleverneed_0.png"},"coverImage":"/assets/img/2024-06-23-TheonlylocalMongoDBreplicasetwithDockerComposeguideyoulleverneed_0.png","tag":["Tech"],"readingTime":8}],"page":"3","totalPageCount":110,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}