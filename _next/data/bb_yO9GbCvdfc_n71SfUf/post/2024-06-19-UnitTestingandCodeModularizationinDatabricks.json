{"pageProps":{"post":{"title":"단위 테스트 및 코드 모듈화를 위한 Databricks","description":"","date":"2024-06-19 12:22","slug":"2024-06-19-UnitTestingandCodeModularizationinDatabricks","content":"\n노트북은 Databricks에서 데이터를 다루는 인기 있는 방법입니다. 노트북 사용자는 데이터를 빠르게 읽고 변환하며 상호적으로 탐색할 수 있습니다. 게다가, 노트북을 공유하고 협업하는 것은 간단합니다. 그러나 프로젝트가 확장될수록 코드 중복을 방지하고 재사용성을 용이하게 하는 모듈화 기능이 필요해집니다.\n\n이를 달성하는 한 가지 방법은 공유 함수를 포함하는 노트북을 생성하고 각 노트북의 시작 부분에서 실행하는 것입니다. 또는 모듈을 만들어 일반적인 Python 개발과 유사한 Python import 명령어를 사용할 수 있습니다. 긴 코드 블록을 함수로 나누면 코드의 재사용을 촉진할 뿐만 아니라 테스트도 용이해집니다.\n\n![이미지](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png)\n\n## 모듈화\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬에서 모듈화란 프로그램을 작은 관리 가능한 모듈로 나누는 것을 말합니다. 파이썬에서 코드를 모듈화하는 것에는 여러 가지 이점이 있습니다:\n\n- 재사용성: 모듈은 다른 프로젝트에서 다시 사용할 수 있어 재작성이 필요하지 않습니다.\n- 유지보수성: 작은 중점적인 모듈로 인해 업데이트와 디버깅이 쉬워집니다.\n- 확장성: 프로젝트가 성장할 때 효율적인 확장이 가능합니다.\n- 협업: 다른 개발자들이 동시에 작업하기를 용이하게 합니다.\n- 테스트: 단위 테스트가 간소화되어 더 신뢰할 수 있는 코드를 작성할 수 있습니다.\n- 가독성: 특정 작업에 집중함으로써 코드 이해가 향상됩니다.\n\nDatabricks에서 모듈을 사용하기 위해서는 클래스 또는 함수를 포함한 파일들로 구성된 폴더와 **init**.py 파일을 생성해야 합니다. 이는 Databricks에 모듈임을 알려줍니다. 아래는 공통 모듈과 함께 공유 함수, 변환 로직을 포함한 변환 모듈, 그리고 테스트 데이터가 포함된 내 솔루션의 구조입니다.\n\n코드 구조:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n워크스페이스\n├── test_data\n│    └── testdata.csv\n├── common\n│    └── __init__.py\n│    └── utilis.py\n├── transform\n│    └── __init__.py\n│    └── operations.py\n├── test_utils.py\n├── test_tran.py\n├── test\n```\n\ntestdata.csv:\n\n```js\nentity,iso_code,date,indicator,value\nUnited States,USA,2022-04-17,Daily ICU occupancy,\nUnited States,USA,2022-04-17,Daily ICU occupancy per million,4.1\nUnited States,USA,2022-04-17,Daily hospital occupancy,10000\nUnited States,USA,2022-04-17,Daily hospital occupancy per million,30.3\nUnited States,USA,2022-04-17,Weekly new hospital admissions,11000\nUnited States,USA,2022-04-17,Weekly new hospital admissions per million,32.8\n```\n\nulits.py:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef mask_func(col_val):\n    if col_val is not None:\n        if len(col_val)>=16:\n            charList=list(col_val)\n            charList[4:12]='x'*8\n            return \"\".join(charList)\n        else:\n            return col_val\n    else:\n        return col_val\n```\n\noperations.py:\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, col\n\ndef deduplicate(df, uniq_col, orderby_col):\n    df = df.withColumn(\"rn\", row_number()\n        .over(Window.partitionBy(uniq_col)\n        .orderBy(col(orderby_col).desc())))\n\n    df = df.filter(col(\"rn\") == 1).drop(\"rn\")\n    return df\n\ndef clean_clients(df):\n    df = df.where(col(\"name\") != \"\").withColumn(\"timestamp\", col(\"timestamp\").cast(\"date\"))\n\n    return df\n```\n\n모듈에서 이러한 함수를 사용하려는 사람은 아래 예시와 같이 import 명령을 사용하여 노트북에 쉽게 추가할 수 있습니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![UnitTestingandCodeModularizationinDatabricks1](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_1.png)\n\nSimilarly, it’s possible to import transformation functions from the module and remove duplicated records from the DataFrame.\n\n![UnitTestingandCodeModularizationinDatabricks2](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_2.png)\n\n# Unit Testing in Databricks\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n소프트웨어 개발에서 단위 테스트는 코드의 정확성, 안정성, 유지 보수 가능성을 보장하는 중요한 요소입니다. 데이터 처리를 위해 노트북이 일반적으로 사용되는 Databricks에서는 단위 테스트가 더욱 중요해집니다.\n\nDatabricks에서 단위 테스트를 시작하려면 코드를 테스트할 수 있는 함수로 분해해야 합니다. 이 프로세스는 코드의 모듈성을 향상시키는 것뿐만 아니라 포괄적인 테스트 스위트를 작성하는 데 도움이 됩니다. Python에서 유닛 테스트를 수행하는 두 가지 인기있는 프레임워크인 Unittest와 pytest가 있습니다.\n\nUnittest 예시:\n\n```python\nimport unittest\n\nclass ExampleTestSuite(unittest.TestCase):\n\n    def test_import(self):\n        self.assertTrue(True)\n\n    def test_addition(self):\n        self.assertEqual(1 + 2, 3)\n\n    def test_subtraction(self):\n        self.assertNotEqual(1 - 2, 0)\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테이블 태그를 Markdown 형식으로 변경하세요.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 왜 단위 테스팅을 해야 할까요?\n\n처음 테스트 중에 코드가 정상적으로 작동하는 것처럼 보이더라도, 단위 테스트는 여러 가지 이유로 중요한 역할을 합니다:\n\n- 정확성 확인: 단위 테스트는 코드의 개별 단위 기능을 확인하여 다양한 조건에서 예상대로 작동하는지 확인합니다.\n- 초기 버그 탐지: 개발 과정 초기에 버그를 식별함으로써, 개발자는 이를 신속히 해결하여 시스템의 다른 부분으로 전파되는 가능성을 줄일 수 있습니다.\n- 리팩토링 및 유지보수: 단위 테스트는 코드 리팩토링 및 유지보수 과정에서 안전망 역할을 하며, 개발자가 확신을 갖고 변경을 가할 수 있으면서도 일관된 동작을 보장합니다.\n- 회귀 테스트: 단위 테스트는 회귀 테스트로 작용하여 새로운 변경사항이나 기능이 기존의 기능을 망가뜨리지 않도록 하여 시스템의 안정성을 유지합니다.\n\n마스킹 기능에 대한 단위 테스트의 간단한 예제를 살펴보겠습니다. 이 단위 테스트는 입력 숫자가 올바르게 마스킹되거나 None을 반환하는지를 확인하여, 함수가 변경되더라도 예상되는 동작이 유지되도록 합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\ntest_utils.py\n\n```python\nfrom common.utils import mask_func\n\ndef test_mask_func():\n    assert \"1234xxxxxxxx4568\" == mask_func(\"1234567891234568\")\n    assert mask_func(None) is None\n```\n\nETL(Extract, Transform, Load)과 같은 복잡한 프로세스의 경우, 데이터 변환 과정의 다양한 측면을 확인하는 데 개선된 테스트를 개발할 수 있습니다. 이러한 테스트에는 스키마 확인, 데이터프레임 비교, 행 수 유효성 검사 또는 특정 값의 존재 여부 확인이 포함될 수 있습니다.\n\ntest_tran.py:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport pytest\nfrom transform.operations import *\nfrom pyspark.testing.utils import assertDataFrameEqual\nfrom pyspark.sql import SparkSession\nfrom pyspark.testing import assertDataFrameEqual, assertSchemaEqual\nfrom pyspark.sql.types import *\nimport pandas as pd\n\n@pytest.fixture()\ndef spark():\n    return SparkSession.builder.appName(\"integrity-tests\").getOrCreate()\n\n@pytest.fixture()\ndef raw_input_df(spark):\n    df = pd.read_csv('test_data/testdata.csv')\n    return spark.createDataFrame(df)\n\n@pytest.fixture()\ndef test_df(spark):\n\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Los Angeles\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df = spark.createDataFrame(input_data, schema)\n\n    return df\n\ndef test_deduplicate(test_df, spark):\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df = spark.createDataFrame(input_data, schema)\n\n    df1 = deduplicate(test_df, \"Name\", \"timestamp\")\n    assertDataFrameEqual(df1, df)\n\ndef test_schema_deduplicated(test_df, spark):\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df_expected = spark.createDataFrame(input_data, schema)\n\n    test_df = deduplicate(test_df, \"Name\", \"timestamp\")\n    assertSchemaEqual(test_df.schema, df_expected.schema)\n\ndef test_clean_clients(test_df, spark):\n    df = clean_clients(test_df)\n    assert df.where(\"name == '' \").count() == 0\n\ndef test_readfromfile(raw_input_df):\n    assert raw_input_df.count() > 0\n```\n\n## Initializing Spark Session for Tests:\n\n테스트 파일은 주피터 노트북이 아니기 때문에, Spark 세션을 초기화하는 것이 필요합니다. 이를 위해서 `spark` 함수와 `fixture` 데코레이터를 사용해서 Spark 세션을 만들 수 있습니다. `fixture` 데코레이터는 자동으로 실행되며 각 테스트 함수에 해당하는 테스트 객체를 제공해주어 테스트 데이터의 생성 및 공유를 간편하게 할 수 있습니다.\n\n```js\n@pytest.fixture()\ndef spark():\n    return SparkSession.builder.appName(\"integrity-tests\").getOrCreate()\n```\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n선언했던 fixture 데코레이터의 동작 방식을 주목하는 것이 중요합니다. 이 기법을 사용하면 테스트 데이터를 원활하게 실행하고 전달할 수 있습니다. 이 기법을 이용하면 Spark 세션을 생성하여 테스트 데이터를 로드하고 이를 테스트 함수 사이에서 공유할 수 있습니다. 테스트 데이터는 목록을 기반으로 생성하거나 테스트 파일에서 로드할 수 있습니다.\n\n```js\n@pytest.fixture()\ndef raw_input_df(spark):\n df = pd.read_csv('test_data/testdata.csv')\n\n return spark.createDataFrame(df)\n```\n\n테스트용 데이터 원본으로 샘플 파일을 사용하는 것도 가능합니다. 그러나 워크스페이스에서 로드해야 하는 경우에는 Spark가 워크스페이스로부터 파일을 직접 로드하는 것을 지원하지 않기 때문에 Pandas를 사용해야 합니다.\n\n## 모듈의 지연 변경 사항 처리하기:\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모듈을 다룰 때 변경 내용을 구현하는 데 지연이 발생하는 것은 일반적입니다. 이 동작을 해결하기 위해 매직 함수를 사용할 수 있습니다:\n\n```js\n%load_ext autoreload\n\n%autoreload 2\n\n%aimport test_tran\n```\n\n# Databricks에서 단위 테스트 실행\n\nDatabricks에서 단위 테스트를 실행하려면 pytest 모듈을 호출하는 노트북을 생성해야 합니다. 아래는 지정된 저장소에서 테스트 실행을 트리거하는 코드 스니펫입니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n테스트 노트북:\n\n```js\n%pip install pytest\n```\n\n```js\nimport pytest\nimport sys\nimport os, sys\n\nrepo_name = \"<저장소 위치>\"\n\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nrepo_root = os.path.dirname(os.path.dirname(notebook_path))\n\nos.chdir(f\"/Workspace/{repo_root}/{repo_name}\")\nprint(os.getcwd())\n# 읽기 전용 파일 시스템에 pyc 파일을 쓰지 않도록 설정합니다.\nsys.dont_write_bytecode = True\n\n# pytest 실행.\nretcode = pytest.main([\".\", \"-v\", \"-p\", \"no:cacheprovider\"])\n\n# 테스트 실패가 있는 경우 셀 실행 실패 처리합니다.\nassert retcode == 0, \"pytest 호출에 실패했습니다. 자세한 내용은 로그를 확인하세요.\"\n```\n\npytest를 실행하면 현재 디렉토리와 서브디렉토리에서 이름이 test\\__.py 또는 _\\_test.py 패턴을 따르는 모든 파일을 자동으로 실행합니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n스크립트를 실행하면 다음과 비슷한 보고서가 표시됩니다:\n\n![보고서](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_4.png)\n\n# 요약\n\n요약하면 모듈화와 유닛 테스팅은 소프트웨어 개발에서 널리 사용되는 관행이며, 이러한 적용은 데이터 엔지니어링 활동에 매끄럽게 확장됩니다. 코드의 모듈화 및 유닛 테스트를 구현하여 데이터 처리 솔루션이 더욱 신뢰성 있고 유연해집니다. 모듈화는 코드 구성 요소의 더 나은 조직화와 재사용을 가능하게 하며, 유닛 테스트는 각 구성 요소가 다양한 조건에서 예상대로 동작하는지 확인합니다. 이러한 기술들이 함께 사용되면 데이터 엔지니어링 솔루션의 전체적인 견고성과 유지보수성에 기여하며, 마지막으로 데이터 처리 파이프라인 및 워크플로의 품질을 향상시킵니다.\n\n<!-- ui-station 사각형 -->\n\n<ins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 이 기사를 유익하게 여기셨다면, 'clap' 버튼을 클릭하거나 LinkedIn에서 좋아요를 표시해 주시면 감사하겠습니다. 여러분의 지원을 감사히 여깁니다. 궁금한 점이나 조언이 있으시다면 언제든 LinkedIn에서 연락해 주세요.\n","ogImage":{"url":"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png"},"coverImage":"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png","tag":["Tech"],"readingTime":13},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p>노트북은 Databricks에서 데이터를 다루는 인기 있는 방법입니다. 노트북 사용자는 데이터를 빠르게 읽고 변환하며 상호적으로 탐색할 수 있습니다. 게다가, 노트북을 공유하고 협업하는 것은 간단합니다. 그러나 프로젝트가 확장될수록 코드 중복을 방지하고 재사용성을 용이하게 하는 모듈화 기능이 필요해집니다.</p>\n<p>이를 달성하는 한 가지 방법은 공유 함수를 포함하는 노트북을 생성하고 각 노트북의 시작 부분에서 실행하는 것입니다. 또는 모듈을 만들어 일반적인 Python 개발과 유사한 Python import 명령어를 사용할 수 있습니다. 긴 코드 블록을 함수로 나누면 코드의 재사용을 촉진할 뿐만 아니라 테스트도 용이해집니다.</p>\n<p><img src=\"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png\" alt=\"이미지\"></p>\n<h2>모듈화</h2>\n<p></p>\n<p>파이썬에서 모듈화란 프로그램을 작은 관리 가능한 모듈로 나누는 것을 말합니다. 파이썬에서 코드를 모듈화하는 것에는 여러 가지 이점이 있습니다:</p>\n<ul>\n<li>재사용성: 모듈은 다른 프로젝트에서 다시 사용할 수 있어 재작성이 필요하지 않습니다.</li>\n<li>유지보수성: 작은 중점적인 모듈로 인해 업데이트와 디버깅이 쉬워집니다.</li>\n<li>확장성: 프로젝트가 성장할 때 효율적인 확장이 가능합니다.</li>\n<li>협업: 다른 개발자들이 동시에 작업하기를 용이하게 합니다.</li>\n<li>테스트: 단위 테스트가 간소화되어 더 신뢰할 수 있는 코드를 작성할 수 있습니다.</li>\n<li>가독성: 특정 작업에 집중함으로써 코드 이해가 향상됩니다.</li>\n</ul>\n<p>Databricks에서 모듈을 사용하기 위해서는 클래스 또는 함수를 포함한 파일들로 구성된 폴더와 <strong>init</strong>.py 파일을 생성해야 합니다. 이는 Databricks에 모듈임을 알려줍니다. 아래는 공통 모듈과 함께 공유 함수, 변환 로직을 포함한 변환 모듈, 그리고 테스트 데이터가 포함된 내 솔루션의 구조입니다.</p>\n<p>코드 구조:</p>\n<p></p>\n<pre><code class=\"hljs language-js\">워크스페이스\n├── test_data\n│    └── testdata.<span class=\"hljs-property\">csv</span>\n├── common\n│    └── __init__.<span class=\"hljs-property\">py</span>\n│    └── utilis.<span class=\"hljs-property\">py</span>\n├── transform\n│    └── __init__.<span class=\"hljs-property\">py</span>\n│    └── operations.<span class=\"hljs-property\">py</span>\n├── test_utils.<span class=\"hljs-property\">py</span>\n├── test_tran.<span class=\"hljs-property\">py</span>\n├── test\n</code></pre>\n<p>testdata.csv:</p>\n<pre><code class=\"hljs language-js\">entity,iso_code,date,indicator,value\n<span class=\"hljs-title class_\">United</span> <span class=\"hljs-title class_\">States</span>,<span class=\"hljs-variable constant_\">USA</span>,<span class=\"hljs-number\">2022</span>-<span class=\"hljs-number\">04</span>-<span class=\"hljs-number\">17</span>,<span class=\"hljs-title class_\">Daily</span> <span class=\"hljs-variable constant_\">ICU</span> occupancy,\n<span class=\"hljs-title class_\">United</span> <span class=\"hljs-title class_\">States</span>,<span class=\"hljs-variable constant_\">USA</span>,<span class=\"hljs-number\">2022</span>-<span class=\"hljs-number\">04</span>-<span class=\"hljs-number\">17</span>,<span class=\"hljs-title class_\">Daily</span> <span class=\"hljs-variable constant_\">ICU</span> occupancy per million,<span class=\"hljs-number\">4.1</span>\n<span class=\"hljs-title class_\">United</span> <span class=\"hljs-title class_\">States</span>,<span class=\"hljs-variable constant_\">USA</span>,<span class=\"hljs-number\">2022</span>-<span class=\"hljs-number\">04</span>-<span class=\"hljs-number\">17</span>,<span class=\"hljs-title class_\">Daily</span> hospital occupancy,<span class=\"hljs-number\">10000</span>\n<span class=\"hljs-title class_\">United</span> <span class=\"hljs-title class_\">States</span>,<span class=\"hljs-variable constant_\">USA</span>,<span class=\"hljs-number\">2022</span>-<span class=\"hljs-number\">04</span>-<span class=\"hljs-number\">17</span>,<span class=\"hljs-title class_\">Daily</span> hospital occupancy per million,<span class=\"hljs-number\">30.3</span>\n<span class=\"hljs-title class_\">United</span> <span class=\"hljs-title class_\">States</span>,<span class=\"hljs-variable constant_\">USA</span>,<span class=\"hljs-number\">2022</span>-<span class=\"hljs-number\">04</span>-<span class=\"hljs-number\">17</span>,<span class=\"hljs-title class_\">Weekly</span> <span class=\"hljs-keyword\">new</span> hospital admissions,<span class=\"hljs-number\">11000</span>\n<span class=\"hljs-title class_\">United</span> <span class=\"hljs-title class_\">States</span>,<span class=\"hljs-variable constant_\">USA</span>,<span class=\"hljs-number\">2022</span>-<span class=\"hljs-number\">04</span>-<span class=\"hljs-number\">17</span>,<span class=\"hljs-title class_\">Weekly</span> <span class=\"hljs-keyword\">new</span> hospital admissions per million,<span class=\"hljs-number\">32.8</span>\n</code></pre>\n<p>ulits.py:</p>\n<p></p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> udf\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StringType</span>\n\ndef <span class=\"hljs-title function_\">mask_func</span>(col_val):\n    <span class=\"hljs-keyword\">if</span> col_val is not <span class=\"hljs-title class_\">None</span>:\n        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-title function_\">len</span>(col_val)>=<span class=\"hljs-number\">16</span>:\n            charList=<span class=\"hljs-title function_\">list</span>(col_val)\n            charList[<span class=\"hljs-number\">4</span>:<span class=\"hljs-number\">12</span>]=<span class=\"hljs-string\">'x'</span>*<span class=\"hljs-number\">8</span>\n            <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">\"\"</span>.<span class=\"hljs-title function_\">join</span>(charList)\n        <span class=\"hljs-attr\">else</span>:\n            <span class=\"hljs-keyword\">return</span> col_val\n    <span class=\"hljs-attr\">else</span>:\n        <span class=\"hljs-keyword\">return</span> col_val\n</code></pre>\n<p>operations.py:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">window</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Window</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> row_number, col\n\ndef <span class=\"hljs-title function_\">deduplicate</span>(df, uniq_col, orderby_col):\n    df = df.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"rn\"</span>, <span class=\"hljs-title function_\">row_number</span>()\n        .<span class=\"hljs-title function_\">over</span>(<span class=\"hljs-title class_\">Window</span>.<span class=\"hljs-title function_\">partitionBy</span>(uniq_col)\n        .<span class=\"hljs-title function_\">orderBy</span>(<span class=\"hljs-title function_\">col</span>(orderby_col).<span class=\"hljs-title function_\">desc</span>())))\n\n    df = df.<span class=\"hljs-title function_\">filter</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"rn\"</span>) == <span class=\"hljs-number\">1</span>).<span class=\"hljs-title function_\">drop</span>(<span class=\"hljs-string\">\"rn\"</span>)\n    <span class=\"hljs-keyword\">return</span> df\n\ndef <span class=\"hljs-title function_\">clean_clients</span>(df):\n    df = df.<span class=\"hljs-title function_\">where</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"name\"</span>) != <span class=\"hljs-string\">\"\"</span>).<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"timestamp\"</span>, <span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"timestamp\"</span>).<span class=\"hljs-title function_\">cast</span>(<span class=\"hljs-string\">\"date\"</span>))\n\n    <span class=\"hljs-keyword\">return</span> df\n</code></pre>\n<p>모듈에서 이러한 함수를 사용하려는 사람은 아래 예시와 같이 import 명령을 사용하여 노트북에 쉽게 추가할 수 있습니다.</p>\n<p></p>\n<p><img src=\"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_1.png\" alt=\"UnitTestingandCodeModularizationinDatabricks1\"></p>\n<p>Similarly, it’s possible to import transformation functions from the module and remove duplicated records from the DataFrame.</p>\n<p><img src=\"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_2.png\" alt=\"UnitTestingandCodeModularizationinDatabricks2\"></p>\n<h1>Unit Testing in Databricks</h1>\n<p></p>\n<p>소프트웨어 개발에서 단위 테스트는 코드의 정확성, 안정성, 유지 보수 가능성을 보장하는 중요한 요소입니다. 데이터 처리를 위해 노트북이 일반적으로 사용되는 Databricks에서는 단위 테스트가 더욱 중요해집니다.</p>\n<p>Databricks에서 단위 테스트를 시작하려면 코드를 테스트할 수 있는 함수로 분해해야 합니다. 이 프로세스는 코드의 모듈성을 향상시키는 것뿐만 아니라 포괄적인 테스트 스위트를 작성하는 데 도움이 됩니다. Python에서 유닛 테스트를 수행하는 두 가지 인기있는 프레임워크인 Unittest와 pytest가 있습니다.</p>\n<p>Unittest 예시:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> unittest\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">ExampleTestSuite</span>(unittest.TestCase):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">test_import</span>(<span class=\"hljs-params\">self</span>):\n        self.assertTrue(<span class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">test_addition</span>(<span class=\"hljs-params\">self</span>):\n        self.assertEqual(<span class=\"hljs-number\">1</span> + <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">test_subtraction</span>(<span class=\"hljs-params\">self</span>):\n        self.assertNotEqual(<span class=\"hljs-number\">1</span> - <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">0</span>)\n</code></pre>\n<p></p>\n<p>테이블 태그를 Markdown 형식으로 변경하세요.</p>\n<p></p>\n<h2>왜 단위 테스팅을 해야 할까요?</h2>\n<p>처음 테스트 중에 코드가 정상적으로 작동하는 것처럼 보이더라도, 단위 테스트는 여러 가지 이유로 중요한 역할을 합니다:</p>\n<ul>\n<li>정확성 확인: 단위 테스트는 코드의 개별 단위 기능을 확인하여 다양한 조건에서 예상대로 작동하는지 확인합니다.</li>\n<li>초기 버그 탐지: 개발 과정 초기에 버그를 식별함으로써, 개발자는 이를 신속히 해결하여 시스템의 다른 부분으로 전파되는 가능성을 줄일 수 있습니다.</li>\n<li>리팩토링 및 유지보수: 단위 테스트는 코드 리팩토링 및 유지보수 과정에서 안전망 역할을 하며, 개발자가 확신을 갖고 변경을 가할 수 있으면서도 일관된 동작을 보장합니다.</li>\n<li>회귀 테스트: 단위 테스트는 회귀 테스트로 작용하여 새로운 변경사항이나 기능이 기존의 기능을 망가뜨리지 않도록 하여 시스템의 안정성을 유지합니다.</li>\n</ul>\n<p>마스킹 기능에 대한 단위 테스트의 간단한 예제를 살펴보겠습니다. 이 단위 테스트는 입력 숫자가 올바르게 마스킹되거나 None을 반환하는지를 확인하여, 함수가 변경되더라도 예상되는 동작이 유지되도록 합니다.</p>\n<p></p>\n<p>test_utils.py</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> common.utils <span class=\"hljs-keyword\">import</span> mask_func\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">test_mask_func</span>():\n    <span class=\"hljs-keyword\">assert</span> <span class=\"hljs-string\">\"1234xxxxxxxx4568\"</span> == mask_func(<span class=\"hljs-string\">\"1234567891234568\"</span>)\n    <span class=\"hljs-keyword\">assert</span> mask_func(<span class=\"hljs-literal\">None</span>) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>\n</code></pre>\n<p>ETL(Extract, Transform, Load)과 같은 복잡한 프로세스의 경우, 데이터 변환 과정의 다양한 측면을 확인하는 데 개선된 테스트를 개발할 수 있습니다. 이러한 테스트에는 스키마 확인, 데이터프레임 비교, 행 수 유효성 검사 또는 특정 값의 존재 여부 확인이 포함될 수 있습니다.</p>\n<p>test_tran.py:</p>\n<p></p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> pytest\n<span class=\"hljs-keyword\">from</span> transform.<span class=\"hljs-property\">operations</span> <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">testing</span>.<span class=\"hljs-property\">utils</span> <span class=\"hljs-keyword\">import</span> assertDataFrameEqual\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">testing</span> <span class=\"hljs-keyword\">import</span> assertDataFrameEqual, assertSchemaEqual\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> *\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n\n@pytest.<span class=\"hljs-title function_\">fixture</span>()\ndef <span class=\"hljs-title function_\">spark</span>():\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span>.<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">\"integrity-tests\"</span>).<span class=\"hljs-title function_\">getOrCreate</span>()\n\n@pytest.<span class=\"hljs-title function_\">fixture</span>()\ndef <span class=\"hljs-title function_\">raw_input_df</span>(spark):\n    df = pd.<span class=\"hljs-title function_\">read_csv</span>(<span class=\"hljs-string\">'test_data/testdata.csv'</span>)\n    <span class=\"hljs-keyword\">return</span> spark.<span class=\"hljs-title function_\">createDataFrame</span>(df)\n\n@pytest.<span class=\"hljs-title function_\">fixture</span>()\ndef <span class=\"hljs-title function_\">test_df</span>(spark):\n\n    schema = <span class=\"hljs-string\">\"name STRING, age INTEGER, city STRING, timestamp STRING\"</span>\n    input_data = [\n        (<span class=\"hljs-string\">\"John\"</span>, <span class=\"hljs-number\">25</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"Jane\"</span>, <span class=\"hljs-number\">30</span>, <span class=\"hljs-string\">\"Los Angeles\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"Jane\"</span>, <span class=\"hljs-number\">30</span>, <span class=\"hljs-string\">\"Chicago\"</span>, <span class=\"hljs-string\">\"20220101\"</span>),\n        (<span class=\"hljs-string\">\"Doe\"</span>, <span class=\"hljs-number\">40</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"\"</span>, <span class=\"hljs-number\">39</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n    ]\n    df = spark.<span class=\"hljs-title function_\">createDataFrame</span>(input_data, schema)\n\n    <span class=\"hljs-keyword\">return</span> df\n\ndef <span class=\"hljs-title function_\">test_deduplicate</span>(test_df, spark):\n    schema = <span class=\"hljs-string\">\"name STRING, age INTEGER, city STRING, timestamp STRING\"</span>\n    input_data = [\n        (<span class=\"hljs-string\">\"John\"</span>, <span class=\"hljs-number\">25</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"Jane\"</span>, <span class=\"hljs-number\">30</span>, <span class=\"hljs-string\">\"Chicago\"</span>, <span class=\"hljs-string\">\"20220101\"</span>),\n        (<span class=\"hljs-string\">\"Doe\"</span>, <span class=\"hljs-number\">40</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"\"</span>, <span class=\"hljs-number\">39</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n    ]\n    df = spark.<span class=\"hljs-title function_\">createDataFrame</span>(input_data, schema)\n\n    df1 = <span class=\"hljs-title function_\">deduplicate</span>(test_df, <span class=\"hljs-string\">\"Name\"</span>, <span class=\"hljs-string\">\"timestamp\"</span>)\n    <span class=\"hljs-title function_\">assertDataFrameEqual</span>(df1, df)\n\ndef <span class=\"hljs-title function_\">test_schema_deduplicated</span>(test_df, spark):\n    schema = <span class=\"hljs-string\">\"name STRING, age INTEGER, city STRING, timestamp STRING\"</span>\n    input_data = [\n        (<span class=\"hljs-string\">\"John\"</span>, <span class=\"hljs-number\">25</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"Jane\"</span>, <span class=\"hljs-number\">30</span>, <span class=\"hljs-string\">\"Chicago\"</span>, <span class=\"hljs-string\">\"20220101\"</span>),\n        (<span class=\"hljs-string\">\"Doe\"</span>, <span class=\"hljs-number\">40</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n        (<span class=\"hljs-string\">\"\"</span>, <span class=\"hljs-number\">39</span>, <span class=\"hljs-string\">\"New York\"</span>, <span class=\"hljs-string\">\"20210101\"</span>),\n    ]\n    df_expected = spark.<span class=\"hljs-title function_\">createDataFrame</span>(input_data, schema)\n\n    test_df = <span class=\"hljs-title function_\">deduplicate</span>(test_df, <span class=\"hljs-string\">\"Name\"</span>, <span class=\"hljs-string\">\"timestamp\"</span>)\n    <span class=\"hljs-title function_\">assertSchemaEqual</span>(test_df.<span class=\"hljs-property\">schema</span>, df_expected.<span class=\"hljs-property\">schema</span>)\n\ndef <span class=\"hljs-title function_\">test_clean_clients</span>(test_df, spark):\n    df = <span class=\"hljs-title function_\">clean_clients</span>(test_df)\n    assert df.<span class=\"hljs-title function_\">where</span>(<span class=\"hljs-string\">\"name == '' \"</span>).<span class=\"hljs-title function_\">count</span>() == <span class=\"hljs-number\">0</span>\n\ndef <span class=\"hljs-title function_\">test_readfromfile</span>(raw_input_df):\n    assert raw_input_df.<span class=\"hljs-title function_\">count</span>() > <span class=\"hljs-number\">0</span>\n</code></pre>\n<h2>Initializing Spark Session for Tests:</h2>\n<p>테스트 파일은 주피터 노트북이 아니기 때문에, Spark 세션을 초기화하는 것이 필요합니다. 이를 위해서 <code>spark</code> 함수와 <code>fixture</code> 데코레이터를 사용해서 Spark 세션을 만들 수 있습니다. <code>fixture</code> 데코레이터는 자동으로 실행되며 각 테스트 함수에 해당하는 테스트 객체를 제공해주어 테스트 데이터의 생성 및 공유를 간편하게 할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\">@pytest.<span class=\"hljs-title function_\">fixture</span>()\ndef <span class=\"hljs-title function_\">spark</span>():\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span>.<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">\"integrity-tests\"</span>).<span class=\"hljs-title function_\">getOrCreate</span>()\n</code></pre>\n<p></p>\n<p>선언했던 fixture 데코레이터의 동작 방식을 주목하는 것이 중요합니다. 이 기법을 사용하면 테스트 데이터를 원활하게 실행하고 전달할 수 있습니다. 이 기법을 이용하면 Spark 세션을 생성하여 테스트 데이터를 로드하고 이를 테스트 함수 사이에서 공유할 수 있습니다. 테스트 데이터는 목록을 기반으로 생성하거나 테스트 파일에서 로드할 수 있습니다.</p>\n<pre><code class=\"hljs language-js\">@pytest.<span class=\"hljs-title function_\">fixture</span>()\ndef <span class=\"hljs-title function_\">raw_input_df</span>(spark):\n df = pd.<span class=\"hljs-title function_\">read_csv</span>(<span class=\"hljs-string\">'test_data/testdata.csv'</span>)\n\n <span class=\"hljs-keyword\">return</span> spark.<span class=\"hljs-title function_\">createDataFrame</span>(df)\n</code></pre>\n<p>테스트용 데이터 원본으로 샘플 파일을 사용하는 것도 가능합니다. 그러나 워크스페이스에서 로드해야 하는 경우에는 Spark가 워크스페이스로부터 파일을 직접 로드하는 것을 지원하지 않기 때문에 Pandas를 사용해야 합니다.</p>\n<h2>모듈의 지연 변경 사항 처리하기:</h2>\n<p></p>\n<p>모듈을 다룰 때 변경 내용을 구현하는 데 지연이 발생하는 것은 일반적입니다. 이 동작을 해결하기 위해 매직 함수를 사용할 수 있습니다:</p>\n<pre><code class=\"hljs language-js\">%load_ext autoreload\n\n%autoreload <span class=\"hljs-number\">2</span>\n\n%aimport test_tran\n</code></pre>\n<h1>Databricks에서 단위 테스트 실행</h1>\n<p>Databricks에서 단위 테스트를 실행하려면 pytest 모듈을 호출하는 노트북을 생성해야 합니다. 아래는 지정된 저장소에서 테스트 실행을 트리거하는 코드 스니펫입니다.</p>\n<p></p>\n<p>테스트 노트북:</p>\n<pre><code class=\"hljs language-js\">%pip install pytest\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">import</span> pytest\n<span class=\"hljs-keyword\">import</span> sys\n<span class=\"hljs-keyword\">import</span> os, sys\n\nrepo_name = <span class=\"hljs-string\">\"&#x3C;저장소 위치>\"</span>\n\nnotebook_path = dbutils.<span class=\"hljs-property\">notebook</span>.<span class=\"hljs-property\">entry_point</span>.<span class=\"hljs-title function_\">getDbutils</span>().<span class=\"hljs-title function_\">notebook</span>().<span class=\"hljs-title function_\">getContext</span>().<span class=\"hljs-title function_\">notebookPath</span>().<span class=\"hljs-title function_\">get</span>()\nrepo_root = os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">dirname</span>(os.<span class=\"hljs-property\">path</span>.<span class=\"hljs-title function_\">dirname</span>(notebook_path))\n\nos.<span class=\"hljs-title function_\">chdir</span>(f<span class=\"hljs-string\">\"/Workspace/{repo_root}/{repo_name}\"</span>)\n<span class=\"hljs-title function_\">print</span>(os.<span class=\"hljs-title function_\">getcwd</span>())\n# 읽기 전용 파일 시스템에 pyc 파일을 쓰지 않도록 설정합니다.\nsys.<span class=\"hljs-property\">dont_write_bytecode</span> = <span class=\"hljs-title class_\">True</span>\n\n# pytest 실행.\nretcode = pytest.<span class=\"hljs-title function_\">main</span>([<span class=\"hljs-string\">\".\"</span>, <span class=\"hljs-string\">\"-v\"</span>, <span class=\"hljs-string\">\"-p\"</span>, <span class=\"hljs-string\">\"no:cacheprovider\"</span>])\n\n# 테스트 실패가 있는 경우 셀 실행 실패 처리합니다.\nassert retcode == <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">\"pytest 호출에 실패했습니다. 자세한 내용은 로그를 확인하세요.\"</span>\n</code></pre>\n<p>pytest를 실행하면 현재 디렉토리와 서브디렉토리에서 이름이 test__.py 또는 __test.py 패턴을 따르는 모든 파일을 자동으로 실행합니다.</p>\n<p></p>\n<p>스크립트를 실행하면 다음과 비슷한 보고서가 표시됩니다:</p>\n<p><img src=\"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_4.png\" alt=\"보고서\"></p>\n<h1>요약</h1>\n<p>요약하면 모듈화와 유닛 테스팅은 소프트웨어 개발에서 널리 사용되는 관행이며, 이러한 적용은 데이터 엔지니어링 활동에 매끄럽게 확장됩니다. 코드의 모듈화 및 유닛 테스트를 구현하여 데이터 처리 솔루션이 더욱 신뢰성 있고 유연해집니다. 모듈화는 코드 구성 요소의 더 나은 조직화와 재사용을 가능하게 하며, 유닛 테스트는 각 구성 요소가 다양한 조건에서 예상대로 동작하는지 확인합니다. 이러한 기술들이 함께 사용되면 데이터 엔지니어링 솔루션의 전체적인 견고성과 유지보수성에 기여하며, 마지막으로 데이터 처리 파이프라인 및 워크플로의 품질을 향상시킵니다.</p>\n<p></p>\n<p>만약 이 기사를 유익하게 여기셨다면, 'clap' 버튼을 클릭하거나 LinkedIn에서 좋아요를 표시해 주시면 감사하겠습니다. 여러분의 지원을 감사히 여깁니다. 궁금한 점이나 조언이 있으시다면 언제든 LinkedIn에서 연락해 주세요.</p>\n</body>\n</html>\n"},"__N_SSG":true}