<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>반복 신경망 시퀀스 모델링 소개 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="반복 신경망 시퀀스 모델링 소개 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="반복 신경망 시퀀스 모델링 소개 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling" data-gatsby-head="true"/><meta name="twitter:title" content="반복 신경망 시퀀스 모델링 소개 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-17 19:43" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/acd99c507555fdc6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/acd99c507555fdc6.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/RZIEBQ2aNAp_DXFVTV6eL/_buildManifest.js" defer=""></script><script src="/_next/static/RZIEBQ2aNAp_DXFVTV6eL/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">반복 신경망 시퀀스 모델링 소개</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="반복 신경망 시퀀스 모델링 소개" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 17, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png" alt="img"></p>
<p>많은 문제와 현상은 순차적으로 발생합니다. 대표적인 예로는 음성, 날씨 패턴, 시계열 등이 있습니다. 이러한 시스템들의 다음 위치는 이전 상태에 따라 달라집니다.</p>
<p>안타깝게도, 전통적인 신경망은 이러한 유형의 데이터를 처리하거나 예측할 수 없습니다. 왜냐하면 입력값을 독립적으로 분석하기 때문에 데이터가 실제로 순차적이라는 개념을 이해하지 못하기 때문입니다.</p>
<p>그렇다면, 이러한 유형의 데이터를 어떻게 예측할 수 있을까요?</p>
<p>"우리는 순환 신경망이라고 불리는 것으로 넘어갑니다!</p>
<p>표준 신경망에 익숙하지 않다면, 확인할 블로그 시리즈가 있어요! RNN으로 계속 진행하기 전에 이 일반적인 신경망이 어떻게 작동하는지 알아보는 것을 권장합니다.</p>
<h1>순환 신경망이란 무엇인가요?</h1>
<p>다음은 순환 신경망(RNNs)을 설명하는 다이어그램입니다:"</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_1.png" alt="RecurrentNeuralNetworksAnIntroductiontoSequenceModelling"></p>
<p>왼쪽에는 순환 뉴런이 있고, 오른쪽에는 시간에 따라 펼쳐진 순환 뉴런이 있습니다. RNN은 바닐라 피드포워드 신경망과 비슷해 보이지만, 이전 반복 실행에서 입력을 받는 중요한 차이점이 있습니다.</p>
<p>그래서 그들을 "순환"이라고 부르는 것입니다. 각 단계의 출력이 시간 안에 전파되어 다음 단계의 값을 계산하는 데 도움이 됩니다. 시스템에는 어떤 내재적 "기억"이 있어서 모델이 과거의 패턴을 추적할 수 있습니다.</p>
<p>예를 들어, Y_1을 예측할 때, X_1의 입력 및 이전 시간 단계 Y_0에서의 출력을 사용할 것입니다. Y_0가 Y_1에 영향을 미치기 때문에 Y_0가 Y_2에도간접적으로 영향을 줄 수 있다는 것을 알 수 있습니다. 이 알고리즘의 순환성을 명확하게 보여주는 사례입니다.</p>
<h1>숨겨진 상태</h1>
<p>문학 작품에서는 일반적으로 숨겨진 상태라는 개념을 볼 수 있습니다. 주로 순환 뉴런을 통해 전달되는 h로 표시됩니다.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_2.png" alt="이미지"></p>
<p>간단한 경우에는 숨겨진 상태가 셀의 출력인 경우도 있습니다. 즉, h=Y입니다. 그러나 우리가 나중에 살펴볼 것처럼, 장기 단기 메모리(LSTM) 및 게이트 순환 유닛(GRU)과 같은 보다 복잡한 셀의 경우에는 이것이 항상 참일 수 있는 것은 아닙니다.</p>
<p>따라서 각 뉴런으로 전달하는 것과 각 뉴런으로부터의 전달을 명시적으로 하는 것이 가장 좋습니다. 이것이 대부분의 문헌에서 위와 같이 표시되는 이유입니다.</p>
<h1>이론</h1>
<p>순환 뉴런의 각 숨겨진 상태는 다음과 같이 계산할 수 있습니다:</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_3.png" alt="Recurrent Neural Networks"></p>
<p>여기서:</p>
<ul>
<li>h_t는 시간 t에서의 은닉 상태입니다.</li>
<li>h_'t−1'는 이전 시간 단계의 은닉 상태입니다.</li>
<li>x_t는 시간 t에서의 입력 데이터입니다.</li>
<li>W_h는 은닉 상태에 대한 가중치 행렬입니다.</li>
<li>W_x는 입력 데이터에 대한 가중치 행렬입니다.</li>
<li>b_h는 은닉 상태에 대한 편향 벡터입니다.</li>
<li>σ는 활성화 함수로, 일반적으로 tanh 또는 sigmoid 함수를 사용합니다.</li>
</ul>
<p>그리고 각 순환 뉴런의 출력을 예측하는 방법은 다음과 같습니다:</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_4.png" alt="Recurrent Neurons"></p>
<p>위와 같이:</p>
<ul>
<li>y_t는 시간 t에서의 출력입니다.</li>
<li>W_y는 출력과 관련된 가중치 행렬입니다.</li>
<li>b_y는 출력 편향 벡터입니다.</li>
</ul>
<p>보시다시피 표기법과 변수 대부분은 일반 피드포워드 신경망과 유사합니다. 유일한 차이점은 숨겨진 상태의 전달로, 그것은 모델이 출력을 예측하는 데 사용할 다른 입력이나 특성으로 볼 수 있습니다.</p>
<p>각 숨겨진 층은 여러 반복 뉴런을 포함할 수 있으므로 각 후속 입력 뉴런에 숨겨진 상태의 벡터를 전달하게 됩니다. 이를 통해 네트워크는 데이터에서 더 복잡한 패턴을 포착하고 표현할 수 있습니다. 각 시간 단계에서 미니 신경망으로 생각할 수 있습니다.</p>
<h1>작업 예시</h1>
<p>우리는 RNN 내부에서 실제로 무슨 일이 일어나고 있는지 설명하기 위해 간단한 예제를 살펴볼 수 있습니다. 이 예제는 매우 단순한 시나리오일 것이지만, 알아야 할 주요 직관력을 설명해줄 것입니다. 실제로 현실에서는 어떤 문제도 이렇게 간단하지 않을 겁니다!</p>
<h2>설정</h2>
<p>1, 2, 3의 숫자 시퀀스가 있다고 가정해 보겠습니다. 이 시퀀스에서 다음 숫자인 4를 예측하기 위해 RNN을 훈련하려고 합니다.</p>
<p>당신의 RNN은 다음과 같은 구조를 가지고 있을 것입니다:</p>
<ul>
<li>하나의 입력 뉴런</li>
<li>하나의 은닉 뉴런</li>
<li>하나의 출력 뉴런</li>
</ul>
<p>가중치와 바이어스를 랜덤하게 초기화할 수 있습니다:</p>
<ul>
<li>W_x (입력에서 은닉으로의 가중치): 0.5</li>
<li>W_h (은닉에서 은닉으로의 가중치): 1.0</li>
<li>b_h (은닉 바이어스): 0</li>
<li>𝑏_𝑦 (출력 바이어스): 0</li>
</ul>
<p>위의 텍스트를 친근한 톤으로 한국어로 번역해 드리겠습니다.</p>
<p>다음 활성화 함수를 사용해주세요:</p>
<ul>
<li>은닉층: tanh</li>
<li>출력층: 없음 (identity/linear)</li>
</ul>
<p>초기 은닉 상태 값:</p>
<ul>
<li>h_0 = 0</li>
</ul>
<h2>Time Step 1 (Input: 1)</h2>
<p>다음은 첫 번째 숨겨진 상태입니다:</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_5.png" alt="첫 번째 숨겨진 상태"></p>
<p>그리고 출력은 다음과 같이 계산됩니다:</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_6.png" alt="Recurrent Neural Networks"></p>
<p>이 예시에서 출력 활성화 함수는 identity이므로 출력 값은 hidden state 값과 동일합니다. 그러나 많은 문제에서 항상 그런 것은 아니라는 것을 기억하세요.</p>
<h2>시간 단계 2 (입력: 2)</h2>
<p>이제 최근에 계산된 h_1 값 사용하여 시간 단계 2에서 다음 입력 값을 위한 위 과정을 반복할 수 있습니다.</p>
<p>아래는 Markdown 형식으로 테이블 태그를 바꿔본 것입니다.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_7.png" alt="이미지"></p>
<p>한번 더, 우리는 시간 단계 2에서 출력 값을 계산합니다:</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_8.png" alt="이미지"></p>
<h2>시간 단계 3 (입력: 3)</h2>
<p>마지막 입력 값과 세 번째 타임 스텝에서는 다음 이미지가 예측 모델을 보여줍니다:</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_9.png" alt="image"></p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_10.png" alt="image"></p>
<p>현재 모델은 다음 숫자를 0.984로 예측하고 있습니다. 실제 값인 4와는 분명히 멀리 떨어져 있습니다. 실제로는 더 많은 훈련 세트를 사용하여 시간을 거슬러 거슬러 역전파를 수행하여 매개 변수를 최적화할 것입니다. 이 내용은 다음 글에서 다룰 예정입니다!</p>
<p>다행히도 이 모든 계산과 최적화는 PyTorch와 TensorFlow와 같은 패키지를 통해 Python에서 수행됩니다. 제가 이 기사에서 이를 하는 방법의 예시를 나중에 보여 드리겠습니다!</p>
<h1>RNN의 종류</h1>
<p>위의 예는 많은 입력으로부터 하나의 RNN 프로세스의 논리적인 과정을 설명하고 있습니다. 우리는 여러 입력(1,2,3)으로 시작하여 시퀀스에서 다음 숫자를 예측하기 위해 노력하고 있는데, 이는 단일 값입니다.</p>
<p>그러나 다른 작업을 위한 여러 종류의 RNN이 있으며, 우리는 지금 그것들을 살펴볼 것입니다.</p>
<h2>일대일</h2>
<p>이것은 단일 예측을 내놓는 입력 세트가 하나인 전통적인 신경망입니다. 이것은 일반적인 지도 학습 문제를 해결하는 데 도움이 됩니다.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_11.png" alt="image"></p>
<h2>일대다</h2>
<p>하나의 입력이 여러 출력으로 이어집니다. 이미지 캡션을 만들거나 음악을 생성하는 데 사용할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_12.png" alt="image"></p>
<h2>Many-To-One</h2>
<p>여러 입력이 하나의 최종 출력을 생성합니다; 이 아키텍처는 감성 분석에 사용됩니다. 영화 리뷰를 제공하면 영화가 좋은지 나쁜지에 따라 +1 또는 -1을 할당합니다.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_13.png" alt="Many-To-Many"></p>
<p>This one gets an input at every step and produces an output at each step. This architecture is used for machine translation and also for problems like speech tagging.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_14.png" alt="Many-To-Many"></p>
<h2>인코더-디코더</h2>
<p>마지막으로, 인코더-디코더 네트워크를 사용할 수 있습니다. 이는 많은 개별 데이터를 입력으로 받아 하나의 데이터를 출력하는 네트워크와, 그로부터 다시 많은 개별 데이터를 출력으로 하는 네트워크로 구성됩니다. 이는 주로 한 언어로 된 문장을 다른 언어로 번역하는 데 사용됩니다.</p>
<p><img src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_15.png" alt="image"></p>
<h1>PyTorch 예시</h1>
<p>위는 PyTorch에서 간단한 RNN을 구현하는 간단한 예제입니다. 위에서 해결한 문제를 시연합니다. 입력이 1,2,3이고 순서에 따라 다음 숫자를 예측하려고 합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.<span class="hljs-property">nn</span> <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.<span class="hljs-property">optim</span> <span class="hljs-keyword">as</span> optim

# <span class="hljs-variable constant_">RNN</span> 모델 정의
<span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleRNN</span>(nn.<span class="hljs-property">Module</span>):
    def <span class="hljs-title function_">__init__</span>(self, input_size, hidden_size, output_size):
        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">SimpleRNN</span>, self).<span class="hljs-title function_">__init__</span>()
        self.<span class="hljs-property">hidden_size</span> = hidden_size
        self.<span class="hljs-property">rnn</span> = nn.<span class="hljs-title function_">RNN</span>(input_size, hidden_size, batch_first=<span class="hljs-title class_">True</span>)
        self.<span class="hljs-property">fc</span> = nn.<span class="hljs-title class_">Linear</span>(hidden_size, output_size)

    def <span class="hljs-title function_">forward</span>(self, x):
        x = x.<span class="hljs-title function_">unsqueeze</span>(-<span class="hljs-number">1</span>)
        h_0 = torch.<span class="hljs-title function_">zeros</span>(<span class="hljs-number">1</span>, x.<span class="hljs-title function_">size</span>(<span class="hljs-number">0</span>), self.<span class="hljs-property">hidden_size</span>)
        rnn_out, _ = self.<span class="hljs-title function_">rnn</span>(x, h_0)
        out = self.<span class="hljs-title function_">fc</span>(rnn_out[:, -<span class="hljs-number">1</span>, :])
        <span class="hljs-keyword">return</span> out

# 데이터셋
train = torch.<span class="hljs-title function_">tensor</span>([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], dtype=torch.<span class="hljs-property">float32</span>)
target = torch.<span class="hljs-title function_">tensor</span>([<span class="hljs-number">5</span>], dtype=torch.<span class="hljs-property">float32</span>)

# 모델 설정
input_size = <span class="hljs-number">1</span>
hidden_size = <span class="hljs-number">1</span>
output_size = <span class="hljs-number">1</span>
model = <span class="hljs-title class_">SimpleRNN</span>(input_size, hidden_size, output_size)

# 손실 및 옵티마이저
criterion = nn.<span class="hljs-title class_">MSELoss</span>()
optimizer = optim.<span class="hljs-title class_">Adam</span>(model.<span class="hljs-title function_">parameters</span>(), lr=<span class="hljs-number">0.01</span>)

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">1000</span>):
    optimizer.<span class="hljs-title function_">zero_grad</span>()
    output = <span class="hljs-title function_">model</span>(train.<span class="hljs-title function_">unsqueeze</span>(<span class="hljs-number">0</span>)).<span class="hljs-title function_">squeeze</span>()  # 배치 차원 추가 및 목표 형태와 일치하도록 압축
    loss = <span class="hljs-title function_">criterion</span>(output, target)
    loss.<span class="hljs-title function_">backward</span>()
    optimizer.<span class="hljs-title function_">step</span>()

# 다음 숫자 예측하는 함수
def <span class="hljs-title function_">predict</span>(model, input_seq):
    <span class="hljs-keyword">with</span> torch.<span class="hljs-title function_">no_grad</span>():
        input_seq = torch.<span class="hljs-title function_">tensor</span>(input_seq, dtype=torch.<span class="hljs-property">float32</span>).<span class="hljs-title function_">unsqueeze</span>(<span class="hljs-number">0</span>)
        output = <span class="hljs-title function_">model</span>(input_seq).<span class="hljs-title function_">squeeze</span>().<span class="hljs-title function_">item</span>()
    <span class="hljs-keyword">return</span> output

# 예제 테스트 세트
test = [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]
predicted = <span class="hljs-title function_">predict</span>(model, test)
<span class="hljs-title function_">print</span>(f<span class="hljs-string">'Input: {test}, Predicted Next Number: {predicted:.2f}'</span>)
</code></pre>
<p>1000번의 에폭 후 출력 결과는 5입니다! 이 경우에는 모델이 실제로 1000번의 역전파로 훈련되었기 때문에 성능이 우리가 위에서 손으로 계산한 예제보다 훨씬 좋습니다.</p>
<p>소스 코드는 저의 GitHub에서 확인하실 수 있습니다: (GitHub URL)</p>
<h1>장점 대비 단점</h1>
<p>이 모든 새롭게 습득한 정보를 바탕으로 RNN의 주요 장단점을 살펴보겠습니다:</p>
<h2>장점</h2>
<ul>
<li>이전 입력값의 형태를 기억할 수 있어서 순차적 데이터를 다루는 데 도움이 됩니다.</li>
<li>정확한 가중치와 편향이 모든 시간 단계에서 공유되어, 더 적은 매개변수와 더 나은 일반화를 이끌어냅니다.</li>
<li>재귀적 성격으로 인해 RNN은 가변 길이의 순차 데이터를 처리할 수 있습니다.</li>
</ul>
<h2>단점</h2>
<ul>
<li>RNN(순환 신경망)은 장기 기억 문제로 이어지는 사라지는 그래디언트 문제에서 상당히 고통받습니다.</li>
<li>각 시간 단계는 이전 단계의 출력에 의존하기 때문에 RNN은 병렬 처리할 수 없어 계산 효율이 떨어집니다.</li>
</ul>
<h1>요약</h1>
<p>RNN은 시퀀스 모델링에 매우 유용하며, 이전 실행의 정보와 메모리를 유지한 채 다음 예측으로 전파됩니다. 그들의 장점은 임의 길이의 입력을 처리할 수 있으며, 모델 크기가 이 입력 크기로 증가하지 않는다는 것입니다. 그러나 재귀적인 성격을 가지고 있기 때문에 병렬화할 수 없어 계산 효율이 낮으며, 사라지는 그래디언트 문제로 심각하게 고통받을 수 있습니다.</p>
<h1>또 다른 것!</h1>
<p>무료 뉴스레터 'Dishing the Data'를 갖고 있어요! 매주 더 나은 데이터 과학자가 되기 위한 조언과 분야에서의 경험을 나누고 있어요.</p>
<h1>저와 연결해보세요!</h1>
<ul>
<li>LinkedIn, X (트위터), 또는 인스타그램</li>
<li>기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 제 유튜브 채널!</li>
</ul>
<h1>참고 자료 및 더 읽을거리</h1>
<ul>
<li>Stanford RNN Cheatsheet</li>
<li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. September 2019. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649.</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"반복 신경망 시퀀스 모델링 소개","description":"","date":"2024-05-17 19:43","slug":"2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling","content":"\n\n![img](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png)\n\n많은 문제와 현상은 순차적으로 발생합니다. 대표적인 예로는 음성, 날씨 패턴, 시계열 등이 있습니다. 이러한 시스템들의 다음 위치는 이전 상태에 따라 달라집니다.\n\n안타깝게도, 전통적인 신경망은 이러한 유형의 데이터를 처리하거나 예측할 수 없습니다. 왜냐하면 입력값을 독립적으로 분석하기 때문에 데이터가 실제로 순차적이라는 개념을 이해하지 못하기 때문입니다.\n\n그렇다면, 이러한 유형의 데이터를 어떻게 예측할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"우리는 순환 신경망이라고 불리는 것으로 넘어갑니다!\n\n표준 신경망에 익숙하지 않다면, 확인할 블로그 시리즈가 있어요! RNN으로 계속 진행하기 전에 이 일반적인 신경망이 어떻게 작동하는지 알아보는 것을 권장합니다.\n\n# 순환 신경망이란 무엇인가요?\n\n다음은 순환 신경망(RNNs)을 설명하는 다이어그램입니다:\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![RecurrentNeuralNetworksAnIntroductiontoSequenceModelling](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_1.png)\n\n왼쪽에는 순환 뉴런이 있고, 오른쪽에는 시간에 따라 펼쳐진 순환 뉴런이 있습니다. RNN은 바닐라 피드포워드 신경망과 비슷해 보이지만, 이전 반복 실행에서 입력을 받는 중요한 차이점이 있습니다.\n\n그래서 그들을 \"순환\"이라고 부르는 것입니다. 각 단계의 출력이 시간 안에 전파되어 다음 단계의 값을 계산하는 데 도움이 됩니다. 시스템에는 어떤 내재적 \"기억\"이 있어서 모델이 과거의 패턴을 추적할 수 있습니다.\n\n예를 들어, Y_1을 예측할 때, X_1의 입력 및 이전 시간 단계 Y_0에서의 출력을 사용할 것입니다. Y_0가 Y_1에 영향을 미치기 때문에 Y_0가 Y_2에도간접적으로 영향을 줄 수 있다는 것을 알 수 있습니다. 이 알고리즘의 순환성을 명확하게 보여주는 사례입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 숨겨진 상태\n\n문학 작품에서는 일반적으로 숨겨진 상태라는 개념을 볼 수 있습니다. 주로 순환 뉴런을 통해 전달되는 h로 표시됩니다.\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_2.png)\n\n간단한 경우에는 숨겨진 상태가 셀의 출력인 경우도 있습니다. 즉, h=Y입니다. 그러나 우리가 나중에 살펴볼 것처럼, 장기 단기 메모리(LSTM) 및 게이트 순환 유닛(GRU)과 같은 보다 복잡한 셀의 경우에는 이것이 항상 참일 수 있는 것은 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 각 뉴런으로 전달하는 것과 각 뉴런으로부터의 전달을 명시적으로 하는 것이 가장 좋습니다. 이것이 대부분의 문헌에서 위와 같이 표시되는 이유입니다.\n\n# 이론\n\n순환 뉴런의 각 숨겨진 상태는 다음과 같이 계산할 수 있습니다:\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서:\n\n- h_t는 시간 t에서의 은닉 상태입니다.\n- h_'t−1'는 이전 시간 단계의 은닉 상태입니다.\n- x_t는 시간 t에서의 입력 데이터입니다.\n- W_h는 은닉 상태에 대한 가중치 행렬입니다.\n- W_x는 입력 데이터에 대한 가중치 행렬입니다.\n- b_h는 은닉 상태에 대한 편향 벡터입니다.\n- σ는 활성화 함수로, 일반적으로 tanh 또는 sigmoid 함수를 사용합니다.\n\n그리고 각 순환 뉴런의 출력을 예측하는 방법은 다음과 같습니다:\n\n![Recurrent Neurons](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위와 같이:\n\n- y_t는 시간 t에서의 출력입니다.\n- W_y는 출력과 관련된 가중치 행렬입니다.\n- b_y는 출력 편향 벡터입니다.\n\n보시다시피 표기법과 변수 대부분은 일반 피드포워드 신경망과 유사합니다. 유일한 차이점은 숨겨진 상태의 전달로, 그것은 모델이 출력을 예측하는 데 사용할 다른 입력이나 특성으로 볼 수 있습니다.\n\n각 숨겨진 층은 여러 반복 뉴런을 포함할 수 있으므로 각 후속 입력 뉴런에 숨겨진 상태의 벡터를 전달하게 됩니다. 이를 통해 네트워크는 데이터에서 더 복잡한 패턴을 포착하고 표현할 수 있습니다. 각 시간 단계에서 미니 신경망으로 생각할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 작업 예시\n\n우리는 RNN 내부에서 실제로 무슨 일이 일어나고 있는지 설명하기 위해 간단한 예제를 살펴볼 수 있습니다. 이 예제는 매우 단순한 시나리오일 것이지만, 알아야 할 주요 직관력을 설명해줄 것입니다. 실제로 현실에서는 어떤 문제도 이렇게 간단하지 않을 겁니다!\n\n## 설정\n\n1, 2, 3의 숫자 시퀀스가 있다고 가정해 보겠습니다. 이 시퀀스에서 다음 숫자인 4를 예측하기 위해 RNN을 훈련하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 RNN은 다음과 같은 구조를 가지고 있을 것입니다:\n\n- 하나의 입력 뉴런\n- 하나의 은닉 뉴런\n- 하나의 출력 뉴런\n\n가중치와 바이어스를 랜덤하게 초기화할 수 있습니다:\n\n- W_x (입력에서 은닉으로의 가중치): 0.5\n- W_h (은닉에서 은닉으로의 가중치): 1.0\n- b_h (은닉 바이어스): 0\n- 𝑏_𝑦 (출력 바이어스): 0\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 친근한 톤으로 한국어로 번역해 드리겠습니다.\n\n다음 활성화 함수를 사용해주세요:\n\n- 은닉층: tanh\n- 출력층: 없음 (identity/linear)\n\n초기 은닉 상태 값:\n\n- h_0 = 0\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Time Step 1 (Input: 1)\n\n다음은 첫 번째 숨겨진 상태입니다:\n\n![첫 번째 숨겨진 상태](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_5.png)\n\n그리고 출력은 다음과 같이 계산됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_6.png)\n\n이 예시에서 출력 활성화 함수는 identity이므로 출력 값은 hidden state 값과 동일합니다. 그러나 많은 문제에서 항상 그런 것은 아니라는 것을 기억하세요.\n\n## 시간 단계 2 (입력: 2)\n\n이제 최근에 계산된 h_1 값 사용하여 시간 단계 2에서 다음 입력 값을 위한 위 과정을 반복할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 테이블 태그를 바꿔본 것입니다.\n\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_7.png)\n\n한번 더, 우리는 시간 단계 2에서 출력 값을 계산합니다:\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_8.png)\n\n## 시간 단계 3 (입력: 3)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막 입력 값과 세 번째 타임 스텝에서는 다음 이미지가 예측 모델을 보여줍니다:\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_9.png)\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_10.png)\n\n현재 모델은 다음 숫자를 0.984로 예측하고 있습니다. 실제 값인 4와는 분명히 멀리 떨어져 있습니다. 실제로는 더 많은 훈련 세트를 사용하여 시간을 거슬러 거슬러 역전파를 수행하여 매개 변수를 최적화할 것입니다. 이 내용은 다음 글에서 다룰 예정입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히도 이 모든 계산과 최적화는 PyTorch와 TensorFlow와 같은 패키지를 통해 Python에서 수행됩니다. 제가 이 기사에서 이를 하는 방법의 예시를 나중에 보여 드리겠습니다!\n\n# RNN의 종류\n\n위의 예는 많은 입력으로부터 하나의 RNN 프로세스의 논리적인 과정을 설명하고 있습니다. 우리는 여러 입력(1,2,3)으로 시작하여 시퀀스에서 다음 숫자를 예측하기 위해 노력하고 있는데, 이는 단일 값입니다.\n\n그러나 다른 작업을 위한 여러 종류의 RNN이 있으며, 우리는 지금 그것들을 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 일대일\n\n이것은 단일 예측을 내놓는 입력 세트가 하나인 전통적인 신경망입니다. 이것은 일반적인 지도 학습 문제를 해결하는 데 도움이 됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_11.png)\n\n## 일대다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하나의 입력이 여러 출력으로 이어집니다. 이미지 캡션을 만들거나 음악을 생성하는 데 사용할 수 있습니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_12.png)\n\n## Many-To-One\n\n여러 입력이 하나의 최종 출력을 생성합니다; 이 아키텍처는 감성 분석에 사용됩니다. 영화 리뷰를 제공하면 영화가 좋은지 나쁜지에 따라 +1 또는 -1을 할당합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_13.png)\n\nThis one gets an input at every step and produces an output at each step. This architecture is used for machine translation and also for problems like speech tagging.\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_14.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 인코더-디코더\n\n마지막으로, 인코더-디코더 네트워크를 사용할 수 있습니다. 이는 많은 개별 데이터를 입력으로 받아 하나의 데이터를 출력하는 네트워크와, 그로부터 다시 많은 개별 데이터를 출력으로 하는 네트워크로 구성됩니다. 이는 주로 한 언어로 된 문장을 다른 언어로 번역하는 데 사용됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_15.png)\n\n# PyTorch 예시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위는 PyTorch에서 간단한 RNN을 구현하는 간단한 예제입니다. 위에서 해결한 문제를 시연합니다. 입력이 1,2,3이고 순서에 따라 다음 숫자를 예측하려고 합니다.\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# RNN 모델 정의\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(-1)\n        h_0 = torch.zeros(1, x.size(0), self.hidden_size)\n        rnn_out, _ = self.rnn(x, h_0)\n        out = self.fc(rnn_out[:, -1, :])\n        return out\n\n# 데이터셋\ntrain = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\ntarget = torch.tensor([5], dtype=torch.float32)\n\n# 모델 설정\ninput_size = 1\nhidden_size = 1\noutput_size = 1\nmodel = SimpleRNN(input_size, hidden_size, output_size)\n\n# 손실 및 옵티마이저\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    output = model(train.unsqueeze(0)).squeeze()  # 배치 차원 추가 및 목표 형태와 일치하도록 압축\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n# 다음 숫자 예측하는 함수\ndef predict(model, input_seq):\n    with torch.no_grad():\n        input_seq = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n        output = model(input_seq).squeeze().item()\n    return output\n\n# 예제 테스트 세트\ntest = [2, 3, 4]\npredicted = predict(model, test)\nprint(f'Input: {test}, Predicted Next Number: {predicted:.2f}')\n```\n\n1000번의 에폭 후 출력 결과는 5입니다! 이 경우에는 모델이 실제로 1000번의 역전파로 훈련되었기 때문에 성능이 우리가 위에서 손으로 계산한 예제보다 훨씬 좋습니다.\n\n소스 코드는 저의 GitHub에서 확인하실 수 있습니다: (GitHub URL)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 장점 대비 단점\n\n이 모든 새롭게 습득한 정보를 바탕으로 RNN의 주요 장단점을 살펴보겠습니다:\n\n## 장점\n\n- 이전 입력값의 형태를 기억할 수 있어서 순차적 데이터를 다루는 데 도움이 됩니다.\n- 정확한 가중치와 편향이 모든 시간 단계에서 공유되어, 더 적은 매개변수와 더 나은 일반화를 이끌어냅니다.\n- 재귀적 성격으로 인해 RNN은 가변 길이의 순차 데이터를 처리할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단점\n\n- RNN(순환 신경망)은 장기 기억 문제로 이어지는 사라지는 그래디언트 문제에서 상당히 고통받습니다.\n- 각 시간 단계는 이전 단계의 출력에 의존하기 때문에 RNN은 병렬 처리할 수 없어 계산 효율이 떨어집니다.\n\n# 요약\n\nRNN은 시퀀스 모델링에 매우 유용하며, 이전 실행의 정보와 메모리를 유지한 채 다음 예측으로 전파됩니다. 그들의 장점은 임의 길이의 입력을 처리할 수 있으며, 모델 크기가 이 입력 크기로 증가하지 않는다는 것입니다. 그러나 재귀적인 성격을 가지고 있기 때문에 병렬화할 수 없어 계산 효율이 낮으며, 사라지는 그래디언트 문제로 심각하게 고통받을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 또 다른 것!\n\n무료 뉴스레터 'Dishing the Data'를 갖고 있어요! 매주 더 나은 데이터 과학자가 되기 위한 조언과 분야에서의 경험을 나누고 있어요.\n\n# 저와 연결해보세요!\n\n- LinkedIn, X (트위터), 또는 인스타그램\n- 기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 제 유튜브 채널!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료 및 더 읽을거리\n\n- Stanford RNN Cheatsheet\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. September 2019. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649.","ogImage":{"url":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png"},"coverImage":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png\" alt=\"img\"\u003e\u003c/p\u003e\n\u003cp\u003e많은 문제와 현상은 순차적으로 발생합니다. 대표적인 예로는 음성, 날씨 패턴, 시계열 등이 있습니다. 이러한 시스템들의 다음 위치는 이전 상태에 따라 달라집니다.\u003c/p\u003e\n\u003cp\u003e안타깝게도, 전통적인 신경망은 이러한 유형의 데이터를 처리하거나 예측할 수 없습니다. 왜냐하면 입력값을 독립적으로 분석하기 때문에 데이터가 실제로 순차적이라는 개념을 이해하지 못하기 때문입니다.\u003c/p\u003e\n\u003cp\u003e그렇다면, 이러한 유형의 데이터를 어떻게 예측할 수 있을까요?\u003c/p\u003e\n\u003cp\u003e\"우리는 순환 신경망이라고 불리는 것으로 넘어갑니다!\u003c/p\u003e\n\u003cp\u003e표준 신경망에 익숙하지 않다면, 확인할 블로그 시리즈가 있어요! RNN으로 계속 진행하기 전에 이 일반적인 신경망이 어떻게 작동하는지 알아보는 것을 권장합니다.\u003c/p\u003e\n\u003ch1\u003e순환 신경망이란 무엇인가요?\u003c/h1\u003e\n\u003cp\u003e다음은 순환 신경망(RNNs)을 설명하는 다이어그램입니다:\"\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_1.png\" alt=\"RecurrentNeuralNetworksAnIntroductiontoSequenceModelling\"\u003e\u003c/p\u003e\n\u003cp\u003e왼쪽에는 순환 뉴런이 있고, 오른쪽에는 시간에 따라 펼쳐진 순환 뉴런이 있습니다. RNN은 바닐라 피드포워드 신경망과 비슷해 보이지만, 이전 반복 실행에서 입력을 받는 중요한 차이점이 있습니다.\u003c/p\u003e\n\u003cp\u003e그래서 그들을 \"순환\"이라고 부르는 것입니다. 각 단계의 출력이 시간 안에 전파되어 다음 단계의 값을 계산하는 데 도움이 됩니다. 시스템에는 어떤 내재적 \"기억\"이 있어서 모델이 과거의 패턴을 추적할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, Y_1을 예측할 때, X_1의 입력 및 이전 시간 단계 Y_0에서의 출력을 사용할 것입니다. Y_0가 Y_1에 영향을 미치기 때문에 Y_0가 Y_2에도간접적으로 영향을 줄 수 있다는 것을 알 수 있습니다. 이 알고리즘의 순환성을 명확하게 보여주는 사례입니다.\u003c/p\u003e\n\u003ch1\u003e숨겨진 상태\u003c/h1\u003e\n\u003cp\u003e문학 작품에서는 일반적으로 숨겨진 상태라는 개념을 볼 수 있습니다. 주로 순환 뉴런을 통해 전달되는 h로 표시됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e간단한 경우에는 숨겨진 상태가 셀의 출력인 경우도 있습니다. 즉, h=Y입니다. 그러나 우리가 나중에 살펴볼 것처럼, 장기 단기 메모리(LSTM) 및 게이트 순환 유닛(GRU)과 같은 보다 복잡한 셀의 경우에는 이것이 항상 참일 수 있는 것은 아닙니다.\u003c/p\u003e\n\u003cp\u003e따라서 각 뉴런으로 전달하는 것과 각 뉴런으로부터의 전달을 명시적으로 하는 것이 가장 좋습니다. 이것이 대부분의 문헌에서 위와 같이 표시되는 이유입니다.\u003c/p\u003e\n\u003ch1\u003e이론\u003c/h1\u003e\n\u003cp\u003e순환 뉴런의 각 숨겨진 상태는 다음과 같이 계산할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_3.png\" alt=\"Recurrent Neural Networks\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eh_t는 시간 t에서의 은닉 상태입니다.\u003c/li\u003e\n\u003cli\u003eh_'t−1'는 이전 시간 단계의 은닉 상태입니다.\u003c/li\u003e\n\u003cli\u003ex_t는 시간 t에서의 입력 데이터입니다.\u003c/li\u003e\n\u003cli\u003eW_h는 은닉 상태에 대한 가중치 행렬입니다.\u003c/li\u003e\n\u003cli\u003eW_x는 입력 데이터에 대한 가중치 행렬입니다.\u003c/li\u003e\n\u003cli\u003eb_h는 은닉 상태에 대한 편향 벡터입니다.\u003c/li\u003e\n\u003cli\u003eσ는 활성화 함수로, 일반적으로 tanh 또는 sigmoid 함수를 사용합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그리고 각 순환 뉴런의 출력을 예측하는 방법은 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_4.png\" alt=\"Recurrent Neurons\"\u003e\u003c/p\u003e\n\u003cp\u003e위와 같이:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ey_t는 시간 t에서의 출력입니다.\u003c/li\u003e\n\u003cli\u003eW_y는 출력과 관련된 가중치 행렬입니다.\u003c/li\u003e\n\u003cli\u003eb_y는 출력 편향 벡터입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e보시다시피 표기법과 변수 대부분은 일반 피드포워드 신경망과 유사합니다. 유일한 차이점은 숨겨진 상태의 전달로, 그것은 모델이 출력을 예측하는 데 사용할 다른 입력이나 특성으로 볼 수 있습니다.\u003c/p\u003e\n\u003cp\u003e각 숨겨진 층은 여러 반복 뉴런을 포함할 수 있으므로 각 후속 입력 뉴런에 숨겨진 상태의 벡터를 전달하게 됩니다. 이를 통해 네트워크는 데이터에서 더 복잡한 패턴을 포착하고 표현할 수 있습니다. 각 시간 단계에서 미니 신경망으로 생각할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e작업 예시\u003c/h1\u003e\n\u003cp\u003e우리는 RNN 내부에서 실제로 무슨 일이 일어나고 있는지 설명하기 위해 간단한 예제를 살펴볼 수 있습니다. 이 예제는 매우 단순한 시나리오일 것이지만, 알아야 할 주요 직관력을 설명해줄 것입니다. 실제로 현실에서는 어떤 문제도 이렇게 간단하지 않을 겁니다!\u003c/p\u003e\n\u003ch2\u003e설정\u003c/h2\u003e\n\u003cp\u003e1, 2, 3의 숫자 시퀀스가 있다고 가정해 보겠습니다. 이 시퀀스에서 다음 숫자인 4를 예측하기 위해 RNN을 훈련하려고 합니다.\u003c/p\u003e\n\u003cp\u003e당신의 RNN은 다음과 같은 구조를 가지고 있을 것입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e하나의 입력 뉴런\u003c/li\u003e\n\u003cli\u003e하나의 은닉 뉴런\u003c/li\u003e\n\u003cli\u003e하나의 출력 뉴런\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e가중치와 바이어스를 랜덤하게 초기화할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eW_x (입력에서 은닉으로의 가중치): 0.5\u003c/li\u003e\n\u003cli\u003eW_h (은닉에서 은닉으로의 가중치): 1.0\u003c/li\u003e\n\u003cli\u003eb_h (은닉 바이어스): 0\u003c/li\u003e\n\u003cli\u003e𝑏_𝑦 (출력 바이어스): 0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e위의 텍스트를 친근한 톤으로 한국어로 번역해 드리겠습니다.\u003c/p\u003e\n\u003cp\u003e다음 활성화 함수를 사용해주세요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e은닉층: tanh\u003c/li\u003e\n\u003cli\u003e출력층: 없음 (identity/linear)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e초기 은닉 상태 값:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eh_0 = 0\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTime Step 1 (Input: 1)\u003c/h2\u003e\n\u003cp\u003e다음은 첫 번째 숨겨진 상태입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_5.png\" alt=\"첫 번째 숨겨진 상태\"\u003e\u003c/p\u003e\n\u003cp\u003e그리고 출력은 다음과 같이 계산됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_6.png\" alt=\"Recurrent Neural Networks\"\u003e\u003c/p\u003e\n\u003cp\u003e이 예시에서 출력 활성화 함수는 identity이므로 출력 값은 hidden state 값과 동일합니다. 그러나 많은 문제에서 항상 그런 것은 아니라는 것을 기억하세요.\u003c/p\u003e\n\u003ch2\u003e시간 단계 2 (입력: 2)\u003c/h2\u003e\n\u003cp\u003e이제 최근에 계산된 h_1 값 사용하여 시간 단계 2에서 다음 입력 값을 위한 위 과정을 반복할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e아래는 Markdown 형식으로 테이블 태그를 바꿔본 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e한번 더, 우리는 시간 단계 2에서 출력 값을 계산합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e시간 단계 3 (입력: 3)\u003c/h2\u003e\n\u003cp\u003e마지막 입력 값과 세 번째 타임 스텝에서는 다음 이미지가 예측 모델을 보여줍니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_9.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_10.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e현재 모델은 다음 숫자를 0.984로 예측하고 있습니다. 실제 값인 4와는 분명히 멀리 떨어져 있습니다. 실제로는 더 많은 훈련 세트를 사용하여 시간을 거슬러 거슬러 역전파를 수행하여 매개 변수를 최적화할 것입니다. 이 내용은 다음 글에서 다룰 예정입니다!\u003c/p\u003e\n\u003cp\u003e다행히도 이 모든 계산과 최적화는 PyTorch와 TensorFlow와 같은 패키지를 통해 Python에서 수행됩니다. 제가 이 기사에서 이를 하는 방법의 예시를 나중에 보여 드리겠습니다!\u003c/p\u003e\n\u003ch1\u003eRNN의 종류\u003c/h1\u003e\n\u003cp\u003e위의 예는 많은 입력으로부터 하나의 RNN 프로세스의 논리적인 과정을 설명하고 있습니다. 우리는 여러 입력(1,2,3)으로 시작하여 시퀀스에서 다음 숫자를 예측하기 위해 노력하고 있는데, 이는 단일 값입니다.\u003c/p\u003e\n\u003cp\u003e그러나 다른 작업을 위한 여러 종류의 RNN이 있으며, 우리는 지금 그것들을 살펴볼 것입니다.\u003c/p\u003e\n\u003ch2\u003e일대일\u003c/h2\u003e\n\u003cp\u003e이것은 단일 예측을 내놓는 입력 세트가 하나인 전통적인 신경망입니다. 이것은 일반적인 지도 학습 문제를 해결하는 데 도움이 됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_11.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003e일대다\u003c/h2\u003e\n\u003cp\u003e하나의 입력이 여러 출력으로 이어집니다. 이미지 캡션을 만들거나 음악을 생성하는 데 사용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_12.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch2\u003eMany-To-One\u003c/h2\u003e\n\u003cp\u003e여러 입력이 하나의 최종 출력을 생성합니다; 이 아키텍처는 감성 분석에 사용됩니다. 영화 리뷰를 제공하면 영화가 좋은지 나쁜지에 따라 +1 또는 -1을 할당합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_13.png\" alt=\"Many-To-Many\"\u003e\u003c/p\u003e\n\u003cp\u003eThis one gets an input at every step and produces an output at each step. This architecture is used for machine translation and also for problems like speech tagging.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_14.png\" alt=\"Many-To-Many\"\u003e\u003c/p\u003e\n\u003ch2\u003e인코더-디코더\u003c/h2\u003e\n\u003cp\u003e마지막으로, 인코더-디코더 네트워크를 사용할 수 있습니다. 이는 많은 개별 데이터를 입력으로 받아 하나의 데이터를 출력하는 네트워크와, 그로부터 다시 많은 개별 데이터를 출력으로 하는 네트워크로 구성됩니다. 이는 주로 한 언어로 된 문장을 다른 언어로 번역하는 데 사용됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_15.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003ePyTorch 예시\u003c/h1\u003e\n\u003cp\u003e위는 PyTorch에서 간단한 RNN을 구현하는 간단한 예제입니다. 위에서 해결한 문제를 시연합니다. 입력이 1,2,3이고 순서에 따라 다음 숫자를 예측하려고 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003enn\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e nn\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003eoptim\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e optim\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eRNN\u003c/span\u003e 모델 정의\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSimpleRNN\u003c/span\u003e(nn.\u003cspan class=\"hljs-property\"\u003eModule\u003c/span\u003e):\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, input_size, hidden_size, output_size):\n        \u003cspan class=\"hljs-variable language_\"\u003esuper\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eSimpleRNN\u003c/span\u003e, self).\u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e()\n        self.\u003cspan class=\"hljs-property\"\u003ehidden_size\u003c/span\u003e = hidden_size\n        self.\u003cspan class=\"hljs-property\"\u003ernn\u003c/span\u003e = nn.\u003cspan class=\"hljs-title function_\"\u003eRNN\u003c/span\u003e(input_size, hidden_size, batch_first=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003efc\u003c/span\u003e = nn.\u003cspan class=\"hljs-title class_\"\u003eLinear\u003c/span\u003e(hidden_size, output_size)\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, x):\n        x = x.\u003cspan class=\"hljs-title function_\"\u003eunsqueeze\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        h_0 = torch.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, x.\u003cspan class=\"hljs-title function_\"\u003esize\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), self.\u003cspan class=\"hljs-property\"\u003ehidden_size\u003c/span\u003e)\n        rnn_out, _ = self.\u003cspan class=\"hljs-title function_\"\u003ernn\u003c/span\u003e(x, h_0)\n        out = self.\u003cspan class=\"hljs-title function_\"\u003efc\u003c/span\u003e(rnn_out[:, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, :])\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e out\n\n# 데이터셋\ntrain = torch.\u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e], dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat32\u003c/span\u003e)\ntarget = torch.\u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e], dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat32\u003c/span\u003e)\n\n# 모델 설정\ninput_size = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\nhidden_size = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\noutput_size = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\nmodel = \u003cspan class=\"hljs-title class_\"\u003eSimpleRNN\u003c/span\u003e(input_size, hidden_size, output_size)\n\n# 손실 및 옵티마이저\ncriterion = nn.\u003cspan class=\"hljs-title class_\"\u003eMSELoss\u003c/span\u003e()\noptimizer = optim.\u003cspan class=\"hljs-title class_\"\u003eAdam\u003c/span\u003e(model.\u003cspan class=\"hljs-title function_\"\u003eparameters\u003c/span\u003e(), lr=\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e):\n    optimizer.\u003cspan class=\"hljs-title function_\"\u003ezero_grad\u003c/span\u003e()\n    output = \u003cspan class=\"hljs-title function_\"\u003emodel\u003c/span\u003e(train.\u003cspan class=\"hljs-title function_\"\u003eunsqueeze\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)).\u003cspan class=\"hljs-title function_\"\u003esqueeze\u003c/span\u003e()  # 배치 차원 추가 및 목표 형태와 일치하도록 압축\n    loss = \u003cspan class=\"hljs-title function_\"\u003ecriterion\u003c/span\u003e(output, target)\n    loss.\u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e()\n    optimizer.\u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e()\n\n# 다음 숫자 예측하는 함수\ndef \u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(model, input_seq):\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e torch.\u003cspan class=\"hljs-title function_\"\u003eno_grad\u003c/span\u003e():\n        input_seq = torch.\u003cspan class=\"hljs-title function_\"\u003etensor\u003c/span\u003e(input_seq, dtype=torch.\u003cspan class=\"hljs-property\"\u003efloat32\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eunsqueeze\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        output = \u003cspan class=\"hljs-title function_\"\u003emodel\u003c/span\u003e(input_seq).\u003cspan class=\"hljs-title function_\"\u003esqueeze\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eitem\u003c/span\u003e()\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\n# 예제 테스트 세트\ntest = [\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e]\npredicted = \u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(model, test)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'Input: {test}, Predicted Next Number: {predicted:.2f}'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e1000번의 에폭 후 출력 결과는 5입니다! 이 경우에는 모델이 실제로 1000번의 역전파로 훈련되었기 때문에 성능이 우리가 위에서 손으로 계산한 예제보다 훨씬 좋습니다.\u003c/p\u003e\n\u003cp\u003e소스 코드는 저의 GitHub에서 확인하실 수 있습니다: (GitHub URL)\u003c/p\u003e\n\u003ch1\u003e장점 대비 단점\u003c/h1\u003e\n\u003cp\u003e이 모든 새롭게 습득한 정보를 바탕으로 RNN의 주요 장단점을 살펴보겠습니다:\u003c/p\u003e\n\u003ch2\u003e장점\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e이전 입력값의 형태를 기억할 수 있어서 순차적 데이터를 다루는 데 도움이 됩니다.\u003c/li\u003e\n\u003cli\u003e정확한 가중치와 편향이 모든 시간 단계에서 공유되어, 더 적은 매개변수와 더 나은 일반화를 이끌어냅니다.\u003c/li\u003e\n\u003cli\u003e재귀적 성격으로 인해 RNN은 가변 길이의 순차 데이터를 처리할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e단점\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRNN(순환 신경망)은 장기 기억 문제로 이어지는 사라지는 그래디언트 문제에서 상당히 고통받습니다.\u003c/li\u003e\n\u003cli\u003e각 시간 단계는 이전 단계의 출력에 의존하기 때문에 RNN은 병렬 처리할 수 없어 계산 효율이 떨어집니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e요약\u003c/h1\u003e\n\u003cp\u003eRNN은 시퀀스 모델링에 매우 유용하며, 이전 실행의 정보와 메모리를 유지한 채 다음 예측으로 전파됩니다. 그들의 장점은 임의 길이의 입력을 처리할 수 있으며, 모델 크기가 이 입력 크기로 증가하지 않는다는 것입니다. 그러나 재귀적인 성격을 가지고 있기 때문에 병렬화할 수 없어 계산 효율이 낮으며, 사라지는 그래디언트 문제로 심각하게 고통받을 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e또 다른 것!\u003c/h1\u003e\n\u003cp\u003e무료 뉴스레터 'Dishing the Data'를 갖고 있어요! 매주 더 나은 데이터 과학자가 되기 위한 조언과 분야에서의 경험을 나누고 있어요.\u003c/p\u003e\n\u003ch1\u003e저와 연결해보세요!\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eLinkedIn, X (트위터), 또는 인스타그램\u003c/li\u003e\n\u003cli\u003e기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 제 유튜브 채널!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e참고 자료 및 더 읽을거리\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eStanford RNN Cheatsheet\u003c/li\u003e\n\u003cli\u003eHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. September 2019. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling"},"buildId":"RZIEBQ2aNAp_DXFVTV6eL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>