<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>토큰화 - 완벽한 가이드 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-23-TokenizationACompleteGuide" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="토큰화 - 완벽한 가이드 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="토큰화 - 완벽한 가이드 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-23-TokenizationACompleteGuide_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-23-TokenizationACompleteGuide" data-gatsby-head="true"/><meta name="twitter:title" content="토큰화 - 완벽한 가이드 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-23-TokenizationACompleteGuide_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-23 18:17" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/R1x9p1CQYDDJESXyLXKOK/_buildManifest.js" defer=""></script><script src="/_next/static/R1x9p1CQYDDJESXyLXKOK/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">토큰화 - 완벽한 가이드</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="토큰화 - 완벽한 가이드" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 23, 2024</span><span class="posts_reading_time__f7YPP">36<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-23-TokenizationACompleteGuide&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><h2>바이트 페어 인코딩, 워드피스 등과 같은 것들과 함께 Python 코드!</h2>
<p>“LLMs from Scratch” 시리즈 중 제1부 — 대형 언어 모델을 이해하고 구축하는 완벽한 안내서입니다. 이 모델이 어떻게 작동하는지 더 자세히 알아보고 싶다면 다음을 읽어보는 것을 권장합니다:</p>
<ul>
<li>프롤로그: 대형 언어 모델의 간단한 역사</li>
<li>파트 1: 토크나이제이션 — 완벽한 안내서</li>
<li>파트 2: Python에서 워드투벡으로부터 스크래치로 단어 임베딩</li>
<li>파트 3: 코드로 설명하는 셀프 어텐션</li>
<li>파트 4: 코드로 이해하는 BERT의 완벽한 안내서</li>
</ul>
<p><img src="/assets/img/2024-05-23-TokenizationACompleteGuide_0.png" alt="이미지"/></p>
<div class="content-ad"></div>
<p>만약 이 콘텐츠가 도움이 되었다면, 아래 방법으로 저를 지원해주십시오:</p>
<ul>
<li>기사에 Clap(박수)을 보내세요</li>
<li>저를 Medium이나 LinkedIn에서 팔로우하여 향후 게시물에 대한 업데이트를 받으세요</li>
</ul>
<h2>서두</h2>
<p>대형 언어 모델 (LLM)은 2022년 11월 OpenAI의 ChatGPT가 출시된 이후 매우 인기를 얻었습니다. 그 이후로 이러한 언어 모델의 사용이 급증했으며, HuggingFace의 Transformer 라이브러리와 PyTorch와 같은 라이브러리의 도움을 받았습니다.</p>
<div class="content-ad"></div>
<p>하지만 모든 이들 준비하고 있는 완제품 도구들로 인해, 기본 수준에서 무슨 일이 일어나고 있는지 추상화하는 것이 쉽습니다. 그 결과로 많은 온라인 튜토리얼들이 당신이 자체 모델을 생성할 때 &#x27;무엇&#x27;을 알려주고 &#x27;왜&#x27;는 알려주지 않는 경우가 많습니다. 이 기사 시리즈는 이를 해결하고자 합니다. &#x27;처음부터 LLMs 만들기&#x27;는 대형 언어 모델을 구성하는 구성 요소를 분해하고, 내부 작동 방식을 설명합니다. 그의 목표는 다음과 같습니다:</p>
<ul>
<li>수학의 직관적 이해를 포함한, LLMs가 어떻게 작동하는지의 기본적인 이해 구축</li>
<li>각 구성 요소가 어떻게 작동하는지를 보여주며, Python에서 처음부터 구현 방법을 보여줌</li>
<li>불필요한 추상화를 줄이기 위해 가급적이면 최소한의 라이브러리 사용</li>
</ul>
<p>말이 다 되었으니, 시작해보겠습니다.</p>
<h1>토크나이저란 무엇인가?</h1>
<div class="content-ad"></div>
<p>자연어 처리 문제는 텍스트 데이터를 사용하는데, 기계가 즉시 이해하기 어렵습니다. 컴퓨터가 언어를 처리하려면 먼저 텍스트를 숫자 형식으로 변환해야 합니다. 이 프로세스는 토크나이저라는 모델에 의해 주로 두 단계로 수행됩니다.</p>
<p>단계 1: 입력 텍스트를 토큰으로 분할</p>
<p>토크나이저는 먼저 텍스트를 가져와 단어, 단어 부분 또는 개별 문자가 될 수 있는 작은 조각으로 나눕니다. 이러한 작은 텍스트 조각을 토큰이라고 합니다. 스탠포드 NLP 그룹은 토큰을 더 엄격하게 정의합니다.</p>
<p>단계 2: 각 토큰에 식별자 할당</p>
<div class="content-ad"></div>
<p>토크나이저가 텍스트를 토큰으로 분리한 후, 각 토큰에 토큰 ID라고 불리는 정수 번호를 할당할 수 있습니다. 예를 들어, &quot;cat&quot;이라는 단어가 15라는 값으로 할당될 수 있고, 따라서 입력 텍스트의 모든 cat 토큰은 숫자 15로 표시됩니다. 텍스트 토큰을 숫자 표현으로 교체하는 과정을 인코딩이라고 합니다. 비슷하게, 인코딩된 토큰을 다시 텍스트로 변환하는 과정을 디코딩이라고 합니다.</p>
<p>단일 숫자를 사용하여 토큰을 표현하는 것에는 단점이 있다는 것을 알 수 있습니다. 그래서 이러한 코드들은 단어 임베딩을 생성하기 위해 추가로 처리되며, 이것은 이 시리즈의 다음 기사의 주제입니다.</p>
<h1>토큰화 방법</h1>
<p>텍스트를 토큰으로 나누는 세 가지 주요 방법이 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>단어 기반</li>
<li>문자 기반</li>
<li>부분어 기반</li>
</ul>
<h2>단어 기반 토크나이저:</h2>
<p>단어 기반 토크나이제이션은 세 가지 토큰화 방법 중 가장 간단한 방법입니다. 여기서 토크나이저는 문장을 단어로 분할하는데 각 공백 문자를 기준으로 나눕니다(때로는 &#x27;화이트스페이스 기반 토큰화&#x27;라고도 함) 또는 유사한 규칙 세트(구두점 기반 토큰화, 트리뱅크 토큰화 등)에 따라 분할할 수도 있습니다 [12].</p>
<p>예를 들어, 다음과 같은 문장:</p>
<div class="content-ad"></div>
<p>고양이들은 멋지지만, 개들이 더 좋아요!</p>
<p>띄어쓰기 문자로 분할하면:</p>
<p>[<code>Cats</code>, <code>are</code>, <code>great,</code>, <code>but</code>, <code>dogs</code>, <code>are</code>, <code>better!</code>]</p>
<p>또는 구두점과 공백을 기준으로 분할하면:</p>
<div class="content-ad"></div>
<p>[<code>Cats</code>, <code>are</code>, <code>great</code>, <code>,</code>, <code>but</code>, <code>dogs</code>, <code>are</code>, <code>better</code>, <code>!</code>]</p>
<p>위 간단한 예제를 통해 분할을 결정하는 데 사용하는 규칙이 중요하다는 것을 분명히 이해할 수 있습니다. 공백 접근 방식은 잠재적으로 희귀한 토큰 <code>better!</code>를 제공하며, 두 번째 분할은 덜 희귀한 토큰 <code>better</code>와 <code>!</code>을 생성합니다. 문장부호를 완전히 제거하지 않도록 주의해야 합니다. 문장부호에는 매우 구체적인 의미가 있을 수 있기 때문입니다. 그 중 하나는 ‘작은따옴표(apostrophe)’입니다. 작은따옴표는 단수와 소유 형태를 구별할 수 있습니다. 예를 들어 “book&#x27;s”는 책의 속성을 가리키며 “the book&#x27;s spine is damaged”와 같이 사용되고, “books”는 여러 권의 책을 가리킵니다.</p>
<p>토큰을 생성한 후, 각 토큰에 번호를 할당할 수 있습니다. 토큰 생성기가 이미 본 토큰을 생성할 때 다음에 볼 토큰은 그 단어에 지정된 번호를 간단히 할당할 수 있습니다. 예를 들어 위 문장에서 <code>great</code>가 1이라는 값으로 할당된 경우, 이후의 <code>great</code> 단어는 모두 1의 값으로 할당됩니다.</p>
<p>단어 기반 토크나이저의 장단점:</p>
<div class="content-ad"></div>
<p>워드 기반 방법으로 생성된 토큰들은 각각 의미론적 및 문맥 정보를 포함하고 있어 많은 정보를 담고 있습니다. 그러나 이 방법의 가장 큰 단점 중 하나는 매우 유사한 단어가 완전히 다른 토큰으로 처리된다는 것입니다. 예를 들어, cat과 cats 간의 연결은 존재하지 않으며, 이들은 별개의 단어로 처리됩니다. 이는 많은 단어를 포함하는 대규모 응용 프로그램에서 문제가 될 수 있습니다. 모델 어휘의 가능한 토큰 수가 매우 커질 수 있기 때문입니다. 영어는 약 17만 단어가 있으며, 각 단어에 대한 복수형이나 과거형과 같은 다양한 문법 형태를 포함하면 폭발적인 어휘 문제가 발생할 수 있습니다. TransformerXL 토크나이저가 사용하는 공백 기반 분할은 어휘 크기가 25만 개를 초과하도록 이끌었습니다.</p>
<p>이 문제를 해결하는 한 가지 방법은 모델이 학습할 수 있는 토큰 수에 하드 리미트를 부여하는 것입니다(예: 1만). 이는 가장 빈도가 높은 1만개의 토큰을 벗어나는 모든 단어를 어휘 외로 처리하고, 숫자 값 대신 UNKNOWN 토큰 값을 할당하는 것입니다(UNK로 축약되기도 합니다). 이는 많은 알려지지 않은 단어가 있는 경우에 성능에 영향을 줄 수 있지만, 데이터에 대부분의 일반적인 단어가 포함된 경우에는 적합한 타협안이 될 수 있습니다.</p>
<p>장점 요약:</p>
<ul>
<li>간단한 방법</li>
<li>각 토큰에 저장된 높은 정보량</li>
<li>주로 일반적인 단어를 포함하는 데이터셋과 잘 작동하는 어휘 크기 제한 가능</li>
</ul>
<div class="content-ad"></div>
<p>요약:</p>
<ul>
<li>비슷한 단어에 대해 별도의 토큰이 생성됩니다 (예: cat과 cats)</li>
<li>매우 큰 어휘를 만들 수 있습니다.</li>
<li>어휘를 제한하면 드문 단어가 많은 데이터셋에서 성능이 크게 저하될 수 있습니다.</li>
</ul>
<h2>문자 기반 토크나이저:</h2>
<p>문자 기반 토크나이제이션은 글자, 숫자 및 구두점과 같은 특수 문자를 포함하여 텍스트를 각 문자 단위로 분할합니다. 이는 영어 언어를 단어 기반 접근법에서 필요한 17만 개 이상의 어휘 대신 약 256개의 토큰으로 표현할 수 있도록 어휘 크기를 크게 줄입니다 [5]. 중국어 및 일본어와 같은 동아시아 언어도 자신들의 문자 시스템에서 수천 개의 고유 문자를 포함하지만 어휘 크기가 크게 축소될 수 있습니다.</p>
<div class="content-ad"></div>
<p>문자 기반 토크나이저에서는 다음과 같은 문장을 아래와 같이 변환할 수 있습니다:</p>
<p>[<code>C</code>, <code>a</code>, <code>t</code>, <code>s</code>, <code> </code>, <code>a</code>, <code>r</code>, <code>e</code>, <code> </code>, <code>g</code>, <code>r</code>, <code>e</code>, <code>a</code>, <code>t</code>, <code>,</code>, <code> </code>, <code>b</code>, <code>u</code>, <code>t</code>, <code> </code>, <code>d</code>, <code>o</code>, <code>g</code>, <code>s</code>, <code> </code>, <code>a</code>, <code>r</code>, <code>e</code>, <code> </code>, <code>b</code>, <code>e</code>, <code>t</code>, <code>t</code>, <code>e</code>, <code>r</code>, <code>!</code>]</p>
<div class="content-ad"></div>
<p>캐릭터 기반 토크나이저의 장단점:</p>
<p>단어 기반 방법과 비교할 때, 캐릭터 기반 접근 방식은 훨씬 작은 어휘 크기를 가지며, 많은 수의 OOV(Out-Of-Vocabulary) 토큰을 생성하지 않는다. 심지어 맞춰 쓰인 단어들이 아닌 오타가 있는 단어들조차도 토큰화할 수 있다는 장점이 있습니다(다만 해당 단어의 올바른 형태와는 다르게 토큰화됩니다). 또한, 빈도 기반 어휘 제한 때문에 단어가 즉시 제거되는 것을 방지합니다.</p>
<p>하지만 이 접근 방식에는 몇 가지 단점도 있습니다. 먼저, 캐릭터 기반 방법으로 생성된 단일 토큰에 저장된 정보량은 매우 적습니다. 이는 단어 기반 방식의 토큰과 달리 의미론적이거나 문맥적인 의미가 캡처되지 않기 때문입니다(특히, 알파벳 기반 언어인 영어와 같은 언어에서). 마지막으로 이 방식은 입력 텍스트를 인코딩하기 위해 많은 수의 숫자가 필요하기 때문에, 언어 모델에 투입할 수 있는 토큰화된 입력의 크기에 제약이 생깁니다.</p>
<p>장점 요약:</p>
<div class="content-ad"></div>
<ul>
<li>어휘 크기가 작음</li>
<li>철자가 틀린 단어를 제거하지 않음</li>
</ul>
<p>단점 요약:</p>
<ul>
<li>각 토큰에 저장되는 정보량이 적으며, 알파벳 기반의 글쓰기 체계에서는 문맥적 또는 의미적 의미가 거의 없음</li>
<li>언어 모델에 입력되는 크기가 제한되며, 텍스트를 토큰화하는 데 필요한 숫자가 훨씬 더 많아짐 (단어 기반 접근 방식과 비교했을 때)</li>
</ul>
<h2>Subword-Based Tokenizers:</h2>
<div class="content-ad"></div>
<p>서브워드 기반 토큰화는 단어 기반 및 문자 기반 방법의 이점을 모두 활용하면서 그들의 단점을 최소화하려는 목표를 가지고 있어요. 서브워드 기반 방법은 단어 내에서 텍스트를 분할하여 의미 있는 토큰을 생성하려는 시도를 통해 중간 지점을 취하고 있어요, 심지어 그것들이 완전한 단어가 아니더라도요. 예를 들어, 토큰 ing와 ed는 문법적인 의미를 가지고 있지만 그 자체로 완전한 단어는 아니에요.</p>
<p>이 방법은 단어 기반 방법보다 작은 어휘 크기를 갖게 하지만, 문자 기반 방법보다 큰 어휘 크기를 갖게 해요. 또한 매 토큰 내에 저장된 정보 양도 두 가지 이전 방법으로 생성된 토큰 사이에 위치하게 되요. 서브워드 접근 방식은 다음 두 지침을 사용해요:</p>
<ul>
<li>자주 사용되는 단어를 서브워드로 분리하지 말고 전체 토큰으로 저장해야 함</li>
<li>드물게 사용되는 단어를 서브워드로 분리해야 함</li>
</ul>
<p>드물게 사용되는 단어만 분리함으로써 활용어나 복수형 등이 그 구성 요소로 분해되는 기회를 주면서 토큰 사이의 관계를 보존하게 돼요. 예를 들어 cat은 데이터셋에서 매우 흔한 단어지만 cats는 덜 흔할 수 있어요. 이 경우 cats는 cat과 s로 분리되어, cat은 이제 다른 모든 cat 토큰과 동일한 값을 갖게 되고, s는 다른 값을 갖게 됩니다. 이는 복수성의 의미를 인코딩할 수 있다는 것이에요. 또 다른 예시로는 단어 토큰화인데요, 이는 루트 단어 토큰과 접미사 ization으로 분할될 수 있어요. 이 방법은 구문 및 의미 유사성을 보존할 수 있습니다. 이러한 이유로, 서브워드 기반 토큰화기는 현재 많은 NLP 모델에서 널리 사용됩니다.</p>
<div class="content-ad"></div>
<h1>정규화 및 사전 토크나이제이션</h1>
<p>토크나이제이션 과정에서는 사전 처리 및 사후 처리 단계가 필요한데, 이 모든 것이 토크나이제이션 파이프라인을 이룹니다. 이것은 로우 텍스트를 토큰으로 변환하는 데 필요한 일련의 조치들을 설명합니다. 이 파이프라인의 단계는 다음과 같습니다:</p>
<ul>
<li>정규화</li>
<li>사전 토큰화</li>
<li>모델</li>
<li>후 처리</li>
</ul>
<p>여기서 토큰화 방법(서브워드 기반, 문자 기반 등)은 모델 단계에서 이루어집니다 [7]. 이 섹션에서는 서브워드 기반 토큰화 방식을 사용하는 토크나이저에 대해 각 단계를 다룰 것입니다.</p>
<div class="content-ad"></div>
<p>중요한 알림: 토큰화 파이프라인의 모든 단계는 Hugging Face의 토크나이저 및 트랜스포머 라이브러리와 같은 라이브러리에서 토크나이저를 사용할 때 자동으로 사용자 대신 처리됩니다. 전체 파이프라인은 Tokenizer라는 단일 객체에 의해 수행됩니다. 이 섹션은 대부분의 사용자가 NLP 작업을 수행할 때 직접 처리할 필요가 없는 코드 내부 작업에 대해 다룹니다. 나중에는 토크나이저 라이브러리의 기본 토크나이저 클래스를 사용자 정의하는 단계도 제시되어 필요한 경우 특정 작업용으로 토크나이저를 목적에 맞게 만들 수 있습니다.</p>
<h2>정규화 방법</h2>
<p>정규화는 텍스트를 토큰으로 분할하기 전에 정리하는 과정입니다. 이 과정에는 각 문자를 소문자로 변환하거나 문자에서 강세 기호를 제거하는 단계(예: é가 e가 됨), 불필요한 공백을 제거하는 것 등이 포함됩니다. 예를 들어, 문자열 ThÍs is áN ExaMPlé sÉnteNCE는 정규화 후에는 this is an example sentence가 됩니다. 서로 다른 정규화기는 서로 다른 단계를 수행하며, 사용 사례에 따라 유용할 수 있습니다. 예를 들어, 일부 상황에서는 대소문자나 강세 기호를 유지해야 할 수도 있습니다. 선택한 정규화기에 따라이 단계에서 다양한 효과를 얻을 수 있습니다.</p>
<p>Hugging Face의 tokenizers.normalizers 패키지에는 대규모 모델의 일부로서 다양한 토큰화기에서 사용되는 여러 기본 정규화기가 포함되어 있습니다. 아래는 NFC 유니코드, 소문자 및 BERT 정규화기입니다. 이들은 예제 문장에 다음과 같은 효과를 보여줍니다:</p>
<div class="content-ad"></div>
<ul>
<li>NFC: 대문자를 변환하지 않거나 악센트를 제거하지 않습니다.</li>
<li>Lower: 대문자를 변환하지만 악센트를 제거하지 않습니다.</li>
<li>BERT: 대문자를 변환하고 악센트를 제거합니다.</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> tokenizers.<span class="hljs-property">normalizers</span> <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">NFC</span>, <span class="hljs-title class_">Lowercase</span>, <span class="hljs-title class_">BertNormalizer</span>

# 정규화할 텍스트
text = <span class="hljs-string">&#x27;ThÍs is  áN ExaMPlé     sÉnteNCE&#x27;</span>

# 정규화 객체 인스턴스화
<span class="hljs-title class_">NFCNorm</span> = <span class="hljs-title function_">NFC</span>()
<span class="hljs-title class_">LowercaseNorm</span> = <span class="hljs-title class_">Lowercase</span>()
<span class="hljs-title class_">BertNorm</span> = <span class="hljs-title class_">BertNormalizer</span>()

# 텍스트 정규화
<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;NFC:   {NFCNorm.normalize_str(text)}&#x27;</span>)
<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;Lower: {LowercaseNorm.normalize_str(text)}&#x27;</span>)
<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;BERT:  {BertNorm.normalize_str(text)}&#x27;</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-attr">NFC</span>:   <span class="hljs-title class_">Th</span>Ís is  áN <span class="hljs-title class_">ExaMPl</span>é     sÉnteNCE
<span class="hljs-title class_">Lower</span>: thís is  án examplé     séntence
<span class="hljs-attr">BERT</span>:  <span class="hljs-variable language_">this</span> is  an example     sentence
</code></pre>
<p>위의 정규화기들은 Hugging Face transformers 라이브러리에서 가져올 수 있는 토크나이저 모델에서 사용됩니다. 아래 코드 셀은 Tokenizer.backend_tokenizer.normalizer를 통해 점 표기법(dot notation)을 사용하여 정규화기에 액세스하는 방법을 보여줍니다. 서로 다른 정규화 방법을 강조하기 위해 일부 비교를 보여줍니다. 이 예시들에서는 FNet 정규화기만 불필요한 공백을 제거합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">FNetTokenizerFast</span>, <span class="hljs-title class_">CamembertTokenizerFast</span>, \
                         <span class="hljs-title class_">BertTokenizerFast</span>

# <span class="hljs-title class_">Text</span> to normalize
text = <span class="hljs-string">&#x27;ThÍs is  áN ExaMPlé     sÉnteNCE&#x27;</span>

# <span class="hljs-title class_">Instantiate</span> tokenizers
<span class="hljs-title class_">FNetTokenizer</span> = <span class="hljs-title class_">FNetTokenizerFast</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">&#x27;google/fnet-base&#x27;</span>)
<span class="hljs-title class_">CamembertTokenizer</span> = <span class="hljs-title class_">CamembertTokenizerFast</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">&#x27;camembert-base&#x27;</span>)
<span class="hljs-title class_">BertTokenizer</span> = <span class="hljs-title class_">BertTokenizerFast</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)

# <span class="hljs-title class_">Normalize</span> the text
<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;FNet Output:      \
    {FNetTokenizer.backend_tokenizer.normalizer.normalize_str(text)}&#x27;</span>)

<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;CamemBERT Output: \
    {CamembertTokenizer.backend_tokenizer.normalizer.normalize_str(text)}&#x27;</span>)

<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;BERT Output:      \
    {BertTokenizer.backend_tokenizer.normalizer.normalize_str(text)}&#x27;</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-title class_">FNet</span> <span class="hljs-title class_">Output</span>:      <span class="hljs-title class_">Th</span>Ís is áN <span class="hljs-title class_">ExaMPl</span>é sÉnteNCE
<span class="hljs-title class_">CamemBERT</span> <span class="hljs-title class_">Output</span>: <span class="hljs-title class_">Th</span>Ís is  áN <span class="hljs-title class_">ExaMPl</span>é     sÉnteNCE
<span class="hljs-variable constant_">BERT</span> <span class="hljs-title class_">Output</span>:      <span class="hljs-variable language_">this</span> is  an example     sentence
</code></pre>
<h2>Pre-Tokenization Methods</h2>
<p>The pre-tokenization step is the first splitting of the raw text in the tokenization pipeline. The split is performed to give an upper bound to what the final tokens could be at the end of the pipeline. That is, a sentence can be split into words in the pre-tokenization step, then in the model step some of these words may be split further according to the tokenization method (e.g. subword-based). So the pre-tokenized text represents the largest possible tokens that could still remain after tokenization.</p>
<div class="content-ad"></div>
<p>정규화와 마찬가지로이 단계를 수행하는 여러 가지 방법이 있습니다. 예를 들어, 문장은 매 공백, 모든 공백 및 일부 구두점 또는 매 공백 및 모든 구두점을 기준으로 분할될 수 있습니다.</p>
<p>아래 셀은 기본 Whitespacesplit 프리 토크나이저와 Hugging Face 토크나이저의 pre_tokenizers 패키지에서 약간 더 복잡한 BertPreTokenizer 간의 비교를 보여줍니다. 공백 프리 토크나이저의 출력은 구두점을 그대로 두고 이웃하는 단어에 여전히 붙어 있는 것을 보여줍니다. 예를 들어, &quot;includes:&quot;는 이 경우에는 단일 단어로 처리됩니다. 반면 BERT 프리 토크나이저는 구두점을 개별 단어로 취급합니다 [8].</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> tokenizers.<span class="hljs-property">pre_tokenizers</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">WhitespaceSplit</span>, <span class="hljs-title class_">BertPreTokenizer</span>

# 텍스트 정규화
text = (<span class="hljs-string">&quot;this sentence&#x27;s content includes: characters, spaces, and &quot;</span> \
        <span class="hljs-string">&quot;punctuation.&quot;</span>)

# 프리 토큰화된 출력을 표시하는 도우미 함수 정의
def <span class="hljs-title function_">print_pretokenized_str</span>(pre_tokens):
    <span class="hljs-keyword">for</span> pre_token <span class="hljs-keyword">in</span> <span class="hljs-attr">pre_tokens</span>:
        <span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;&quot;{pre_token[0]}&quot;, &#x27;</span>, end=<span class="hljs-string">&#x27;&#x27;</span>)

# 프리 토크나이저 인스턴스화
wss = <span class="hljs-title class_">WhitespaceSplit</span>()
bpt = <span class="hljs-title class_">BertPreTokenizer</span>()

# 텍스트를 프리 토큰화
<span class="hljs-title function_">print</span>(<span class="hljs-string">&#x27;Whitespace Pre-Tokenizer:&#x27;</span>)
<span class="hljs-title function_">print_pretokenized_str</span>(wss.<span class="hljs-title function_">pre_tokenize_str</span>(text))

<span class="hljs-title function_">print</span>(<span class="hljs-string">&#x27;\n\nBERT Pre-Tokenizer:&#x27;</span>)
<span class="hljs-title function_">print_pretokenized_str</span>(bpt.<span class="hljs-title function_">pre_tokenize_str</span>(text))
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-title class_">Whitespace</span> <span class="hljs-title class_">Pre</span>-<span class="hljs-title class_">Tokenizer</span>:
<span class="hljs-string">&quot;this&quot;</span>, <span class="hljs-string">&quot;sentence&#x27;s&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>, <span class="hljs-string">&quot;includes:&quot;</span>, <span class="hljs-string">&quot;characters,&quot;</span>, <span class="hljs-string">&quot;spaces,&quot;</span>,
<span class="hljs-string">&quot;and&quot;</span>, <span class="hljs-string">&quot;punctuation.&quot;</span>,

<span class="hljs-variable constant_">BERT</span> <span class="hljs-title class_">Pre</span>-<span class="hljs-title class_">Tokenizer</span>:
<span class="hljs-string">&quot;this&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>, <span class="hljs-string">&quot;includes&quot;</span>, <span class="hljs-string">&quot;:&quot;</span>, <span class="hljs-string">&quot;characters&quot;</span>,
<span class="hljs-string">&quot;,&quot;</span>, <span class="hljs-string">&quot;spaces&quot;</span>, <span class="hljs-string">&quot;,&quot;</span>, <span class="hljs-string">&quot;and&quot;</span>, <span class="hljs-string">&quot;punctuation&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>,
</code></pre>
<div class="content-ad"></div>
<p>정규화 방법들과 마찬가지로 GPT-2와 ALBERT (A Lite BERT) 토크나이저와 같은 일반적인 토크나이저에서 사전 토큰화 방법을 직접 호출할 수 있습니다. 이들은 위에서 보여진 표준 BERT 사전 토큰화 방법과 약간 다른 방식을 사용합니다. 토큰을 분할할 때 공백 문자를 제거하지 않고 특수 문자로 대체합니다. 그 결과, 공백 문자를 처리할 때 무시할 수 있지만 필요할 경우 원래 문장을 검색할 수 있습니다. GPT-2 모델은 Ġ 문자를 사용하며, 이는 위에 점을 찍은 대문자 G가 특징입니다. ALBERT 모델은 밑줄 문자를 사용합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-comment"># 사전 토큰화할 텍스트</span>
text = (<span class="hljs-string">&quot;this sentence&#x27;s content includes: characters, spaces, and &quot;</span> \
        <span class="hljs-string">&quot;punctuation.&quot;</span>)

<span class="hljs-comment"># 사전 토큰화 객체 생성</span>
GPT2_PreTokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;gpt2&#x27;</span>).backend_tokenizer \
                    .pre_tokenizer

Albert_PreTokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;albert-base-v1&#x27;</span>) \
                      .backend_tokenizer.pre_tokenizer

<span class="hljs-comment"># 텍스트를 사전 토큰화</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;GPT-2 사전 토크나이저:&#x27;</span>)
print_pretokenized_str(GPT2_PreTokenizer.pre_tokenize_str(text))
<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n\nALBERT 사전 토크나이저:&#x27;</span>)
print_pretokenized_str(Albert_PreTokenizer.pre_tokenize_str(text))
</code></pre>
<pre><code class="hljs language-python">GPT-<span class="hljs-number">2</span> 사전 토크나이저:
<span class="hljs-string">&quot;this&quot;</span>, <span class="hljs-string">&quot;Ġsentence&quot;</span>, <span class="hljs-string">&quot;&#x27;s&quot;</span>, <span class="hljs-string">&quot;Ġcontent&quot;</span>, <span class="hljs-string">&quot;Ġincludes&quot;</span>, <span class="hljs-string">&quot;:&quot;</span>, <span class="hljs-string">&quot;Ġcharacters&quot;</span>, <span class="hljs-string">&quot;,&quot;</span>,
<span class="hljs-string">&quot;Ġspaces&quot;</span>, <span class="hljs-string">&quot;,&quot;</span>, <span class="hljs-string">&quot;Ġand&quot;</span>, <span class="hljs-string">&quot;Ġpunctuation&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>

ALBERT 사전 토크나이저:
<span class="hljs-string">&quot;▁this&quot;</span>, <span class="hljs-string">&quot;▁sentence&#x27;s&quot;</span>, <span class="hljs-string">&quot;▁content&quot;</span>, <span class="hljs-string">&quot;▁includes:&quot;</span>, <span class="hljs-string">&quot;▁characters,&quot;</span>, <span class="hljs-string">&quot;▁spaces,&quot;</span>,
<span class="hljs-string">&quot;▁and&quot;</span>, <span class="hljs-string">&quot;▁punctuation.&quot;</span>
</code></pre>
<p>위의 예제 문장에 대한 BERT 사전 토큰화 단계 결과를 수정 없이 출력한 내용이 아래에 나와 있습니다. 반환된 객체는 원본 입력 텍스트에서 문자열의 시작 및 끝 색인을 포함하는 파이썬 리스트입니다. 문자열의 시작 색인은 포함되며, 끝 색인은 배타적입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> tokenizers.<span class="hljs-property">pre_tokenizers</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">WhitespaceSplit</span>, <span class="hljs-title class_">BertPreTokenizer</span>

# <span class="hljs-title class_">Pre</span>-tokenizer 인스턴스 생성
text = (<span class="hljs-string">&quot;this sentence의 내용은: characters, spaces, 그리고 &quot;</span> \
        <span class="hljs-string">&quot;punctuation이 포함되어 있습니다.&quot;</span>)
bpt = <span class="hljs-title class_">BertPreTokenizer</span>()
bpt.<span class="hljs-title function_">pre_tokenize_str</span>(text)
</code></pre>
<pre><code class="hljs language-js">[
  (<span class="hljs-string">&quot;this&quot;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">4</span>)),
  (<span class="hljs-string">&quot;sentence&quot;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">13</span>)),
  (<span class="hljs-string">&quot;&#x27;&quot;</span>, (<span class="hljs-number">13</span>, <span class="hljs-number">14</span>)),
  (<span class="hljs-string">&quot;s&quot;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)),
  (<span class="hljs-string">&quot;content&quot;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">23</span>)),
  (<span class="hljs-string">&quot;includes&quot;</span>, (<span class="hljs-number">24</span>, <span class="hljs-number">32</span>)),
  (<span class="hljs-string">&quot;:&quot;</span>, (<span class="hljs-number">32</span>, <span class="hljs-number">33</span>)),
  (<span class="hljs-string">&quot;characters&quot;</span>, (<span class="hljs-number">34</span>, <span class="hljs-number">44</span>)),
  (<span class="hljs-string">&quot;,&quot;</span>, (<span class="hljs-number">44</span>, <span class="hljs-number">45</span>)),
  (<span class="hljs-string">&quot;spaces&quot;</span>, (<span class="hljs-number">46</span>, <span class="hljs-number">52</span>)),
  (<span class="hljs-string">&quot;,&quot;</span>, (<span class="hljs-number">52</span>, <span class="hljs-number">53</span>)),
  (<span class="hljs-string">&quot;and&quot;</span>, (<span class="hljs-number">54</span>, <span class="hljs-number">57</span>)),
  (<span class="hljs-string">&quot;punctuation&quot;</span>, (<span class="hljs-number">58</span>, <span class="hljs-number">69</span>)),
  (<span class="hljs-string">&quot;.&quot;</span>, (<span class="hljs-number">69</span>, <span class="hljs-number">70</span>)),
];
</code></pre>
<h1>서브워드 토큰화 방법</h1>
<p>토큰화 파이프라인의 모델 단계는 토큰화 방법이 사용되는 곳입니다. 이전에 설명한대로 여기서 선택할 수 있는 옵션은: 단어 기반, 문자 기반, 서브워드 기반입니다. 서브워드 기반 방법이 일반적으로 선호되는데, 이 방법들은 단어 기반 및 문자 기반 접근법의 한계를 극복하기 위해 설계되었습니다.</p>
<div class="content-ad"></div>
<p>트랜스포머 모델에는 하위 단어 기반 토큰화를 구현하는 데 일반적으로 사용되는 세 가지 토크나이저 방법이 있습니다. 이 방법들은 다음과 같습니다:</p>
<ul>
<li>바이트 페어 인코딩 (BPE)</li>
<li>워드피스</li>
<li>유니그램</li>
</ul>
<p>각각의 방법은 빈도가 낮은 단어를 더 작은 토큰으로 분리하기 위해 약간 다른 기술을 사용합니다. BPE 및 워드피스 알고리즘의 구현 방법도 여기에 소개되어 있어서 접근 방식 사이의 유사점과 차이점을 강조하는 데 도움이 될 것입니다.</p>
<h2>바이트 페어 인코딩</h2>
<div class="content-ad"></div>
<p>바이트 페어 인코딩 알고리즘은 GPT 및 GPT-2 모델 (OpenAI), BART (Lewis et al.) 및 기타 많은 트랜스포머 모델에서 발견되는 일반적으로 사용되는 토크나이저입니다 [9-10]. 이 알고리즘은 원래 텍스트 압축 알고리즘으로 설계되었지만, 언어 모델의 토큰화 작업에 매우 효과적으로 작동한다는 것이 밝혀졌습니다. BPE 알고리즘은 텍스트 문자열을 참조 말뭉치(토큰화 모델을 훈련하는 데 사용되는 텍스트)에서 빈번히 나타나는 부분 단어 단위로 분해합니다 [11]. BPE 모델은 다음과 같이 훈련됩니다:</p>
<h2>단계 1) 말뭉치 작성</h2>
<p>입력 텍스트는 정규화 및 사전 토큰화 모델에 제공되어 깨끗한 단어를 생성합니다. 단어는 BPE 모델에 제공되어 각 단어의 빈도를 결정하고, 이 빈도를 단어와 함께 목록인 말뭉치에 저장합니다.</p>
<h2>단계 2) 어휘 작성</h2>
<div class="content-ad"></div>
<p>말뭉치에서 단어들은 개별 문자로 분해되어 &quot;어휘(vocabulary)&quot;라는 비어있는 목록에 추가됩니다. 알고리즘은 어떤 문자 쌍을 함께 병합할 수 있는지를 결정할 때마다 이 어휘에 계속 추가합니다.</p>
<p>단계 3) 문자 쌍의 빈도 찾기</p>
<p>그런 다음, 말뭉치의 각 단어에 대해 문자 쌍의 빈도가 기록됩니다. 예를 들어, 단어 &quot;cats&quot;는 문자 쌍 &quot;ca&quot;, &quot;at&quot;, &quot;ts&quot;를 가집니다. 이와 같은 방식으로 모든 단어가 검사되어 전역 빈도 카운터에 기여합니다. 따라서 토큰 중에서 어떤 ca가 발견되는 경우, ca 쌍에 대한 빈도 카운터가 증가합니다.</p>
<p>단계 4) 병합 규칙 작성</p>
<div class="content-ad"></div>
<p>각 문자 쌍의 빈도가 알려진 경우, 가장 빈번한 문자 쌍이 어휘에 추가됩니다. 어휘는 이제 토큰 내의 모든 개별 문자와 가장 빈번한 문자 쌍으로 구성됩니다. 또한 모델이 사용할 수있는 병합 규칙이 제공됩니다. 예를 들어 모델이 ca가 가장 빈번한 문자 쌍이라는 것을 학습하면, 모델은 말뭉친 c와 a의 모든 인접 인스턴스를 ca로 병합해 ca를 제공합니다. 이제 이를 나머지 단계의 단일 문자 ca로 취급할 수 있습니다.</p>
<p>단계 5) 단계 3과 4 반복</p>
<p>그런 다음 단계 3과 4를 반복하여 더 많은 병합 규칙을 찾고 어휘에 더 많은 문자 쌍을 추가합니다. 이 프로세스는 교육 시작 시 지정된 대상 크기에 도달 할 때까지 계속됩니다.</p>
<p>BPE 알고리즘이 교육되었으므로 (즉, 모든 병합 규칙이 찾아졌다), 모델은 모든 텍스트를 토큰화하기 위해 모든 단어를 각 문자로 분할하고, 그런 다음 병합 규칙에 따라 병합하여 사용할 수 있습니다.</p>
<div class="content-ad"></div>
<p>위의 표는 마크다운 형식으로 변경하겠습니다.</p>
<p>아래는 BPE 알고리즘의 Python 구현입니다. 위에서 설명한 단계를 따르고 있습니다. 그 후에는 이 모델을 장난감 데이터세트에서 훈련하고 몇 가지 예제 단어에서 테스트합니다.</p>
<pre><code class="hljs language-py"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TargetVocabularySizeError</span>(<span class="hljs-title class_ inherited__">Exception</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, message</span>):
        <span class="hljs-built_in">super</span>().__init__(message)

<span class="hljs-keyword">class</span> <span class="hljs-title class_">BPE</span>:
    <span class="hljs-string">&#x27;&#x27;&#x27;Byte Pair Encoding tokenizer의 구현.&#x27;&#x27;&#x27;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">calculate_frequency</span>(<span class="hljs-params">self, words</span>):
        <span class="hljs-string">&#x27;&#x27;&#x27; 주어진 단어 목록에서 각 단어의 빈도를 계산합니다.

            문자열로 저장된 단어 목록을 받아서, 각 단어의 빈도를 나타내는 정수를 값을 가진 튜플의 목록을 반환합니다.

            매개변수:
                words (list): 어떠한 순서로든 단어들(문자열)의 목록입니다.

            반환값:
                corpus (list[tuple(str, int)]): 단어 목록의 각 단어를 나타내는 첫 번째 요소가 문자열이고,
                  두 번째 요소가 목록에서 단어의 빈도를 나타내는 정수인 튜플의 목록입니다.
        &#x27;&#x27;&#x27;</span>
        freq_dict = <span class="hljs-built_in">dict</span>()

        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> freq_dict:
                freq_dict[word] = <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:
                freq_dict[word] += <span class="hljs-number">1</span>

        corpus = [(word, freq_dict[word]) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> freq_dict.keys()]

        <span class="hljs-keyword">return</span> corpus

    <span class="hljs-comment"># 나머지 코드 생략</span>
</code></pre>
<p>BPE 알고리즘은 &#x27;고양이&#x27;에 관한 몇 가지 단어가 포함된 장난감 데이터세트에서 훈련됩니다. 토크나이저의 목표는 데이터세트의 단어의 가장 유용하고 의미 있는 하위 단위를 결정하여 토큰으로 사용하는 것입니다. 검사 결과, &#x27;cat&#x27;, &#x27;eat&#x27;, &#x27;ing&#x27; 등의 단위가 유용한 하위 단위가 될 것임이 분명합니다.</p>
<p>21개의 대상 어휘 크기로 토크나이저를 실행하면(이는 5회 병합만 필요합니다), 위에서 언급한 모든 원하는 하위 단위를 포착하는 데 충분합니다. 더 큰 데이터세트의 경우 대상 어휘도 훨씬 더 높아지겠지만, 이는 BPE 토크나이저가 얼마나 강력한지를 보여줍니다.```</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"># <span class="hljs-title class_">Training</span> set
words = [<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;cat&#x27;</span>,
         <span class="hljs-string">&#x27;cats&#x27;</span>, <span class="hljs-string">&#x27;cats&#x27;</span>,
         <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;eat&#x27;</span>,
         <span class="hljs-string">&#x27;eating&#x27;</span>, <span class="hljs-string">&#x27;eating&#x27;</span>, <span class="hljs-string">&#x27;eating&#x27;</span>,
         <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>,
         <span class="hljs-string">&#x27;jumping&#x27;</span>,
         <span class="hljs-string">&#x27;food&#x27;</span>, <span class="hljs-string">&#x27;food&#x27;</span>, <span class="hljs-string">&#x27;food&#x27;</span>, <span class="hljs-string">&#x27;food&#x27;</span>, <span class="hljs-string">&#x27;food&#x27;</span>, <span class="hljs-string">&#x27;food&#x27;</span>]

# <span class="hljs-title class_">Instantiate</span> the tokenizer
bpe = <span class="hljs-title function_">BPE</span>()
bpe.<span class="hljs-title function_">train</span>(words, <span class="hljs-number">21</span>)

# <span class="hljs-title class_">Print</span> the corpus at each stage <span class="hljs-keyword">of</span> the process, and the merge rule used
<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;INITIAL CORPUS:\n{bpe.corpus_history[0]}\n&#x27;</span>)
<span class="hljs-keyword">for</span> rule, corpus <span class="hljs-keyword">in</span> <span class="hljs-title function_">list</span>(<span class="hljs-title function_">zip</span>(bpe.<span class="hljs-property">merge_rules</span>, bpe.<span class="hljs-property">corpus_history</span>[<span class="hljs-number">1</span>:])):
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;NEW MERGE RULE: Combine &quot;{rule[0]}&quot; and &quot;{rule[1]}&quot;&#x27;</span>)
    <span class="hljs-title function_">print</span>(corpus, end=<span class="hljs-string">&#x27;\n\n&#x27;</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">INITIAL</span> <span class="hljs-attr">CORPUS</span>:
[([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>], <span class="hljs-number">10</span>),
([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;t&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">3</span>), ([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">1</span>), ([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], <span class="hljs-number">6</span>)]

<span class="hljs-variable constant_">NEW</span> <span class="hljs-variable constant_">MERGE</span> <span class="hljs-attr">RULE</span>: <span class="hljs-title class_">Combine</span> <span class="hljs-string">&quot;a&quot;</span> and <span class="hljs-string">&quot;t&quot;</span>
[([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>], <span class="hljs-number">10</span>),
([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">3</span>), ([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">1</span>), ([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], <span class="hljs-number">6</span>)]

<span class="hljs-variable constant_">NEW</span> <span class="hljs-variable constant_">MERGE</span> <span class="hljs-attr">RULE</span>: <span class="hljs-title class_">Combine</span> <span class="hljs-string">&quot;e&quot;</span> and <span class="hljs-string">&quot;at&quot;</span>
[([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>], <span class="hljs-number">10</span>),
([<span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">3</span>), ([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">1</span>), ([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], <span class="hljs-number">6</span>)]

<span class="hljs-variable constant_">NEW</span> <span class="hljs-variable constant_">MERGE</span> <span class="hljs-attr">RULE</span>: <span class="hljs-title class_">Combine</span> <span class="hljs-string">&quot;c&quot;</span> and <span class="hljs-string">&quot;at&quot;</span>
[([<span class="hljs-string">&#x27;cat&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>], <span class="hljs-number">10</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">3</span>),
([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">1</span>), ([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], <span class="hljs-number">6</span>)]

<span class="hljs-variable constant_">NEW</span> <span class="hljs-variable constant_">MERGE</span> <span class="hljs-attr">RULE</span>: <span class="hljs-title class_">Combine</span> <span class="hljs-string">&quot;i&quot;</span> and <span class="hljs-string">&quot;n&quot;</span>
[([<span class="hljs-string">&#x27;cat&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>], <span class="hljs-number">10</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">3</span>),
([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;g&#x27;</span>], <span class="hljs-number">1</span>),
([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], <span class="hljs-number">6</span>)]

<span class="hljs-variable constant_">NEW</span> <span class="hljs-variable constant_">MERGE</span> <span class="hljs-attr">RULE</span>: <span class="hljs-title class_">Combine</span> <span class="hljs-string">&quot;in&quot;</span> and <span class="hljs-string">&quot;g&quot;</span>
[([<span class="hljs-string">&#x27;cat&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>], <span class="hljs-number">10</span>), ([<span class="hljs-string">&#x27;eat&#x27;</span>, <span class="hljs-string">&#x27;ing&#x27;</span>], <span class="hljs-number">3</span>),
([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>, <span class="hljs-string">&#x27;ing&#x27;</span>], <span class="hljs-number">2</span>), ([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;ing&#x27;</span>], <span class="hljs-number">1</span>),
([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;d&#x27;</span>], <span class="hljs-number">6</span>)]
</code></pre>
<p>크게 작은 데이터셋으로 BPE 알고리즘을 학습했으므로 이제 예제 단어를 토큰화하는 데 사용할 수 있습니다. 아래 셀은 토크나이저가 이전에 본 단어들 및 이전에 보지 못한 단어들을 토큰화하는 데 사용되는 것을 보여줍니다. 토크나이저는 동사 접미사 &quot;ing&quot;을 학습했으므로 이를 토큰으로 분리할 수 있습니다. 이 때문에 훈련 데이터에는 &#x27;eat&#x27;이 포함되어 있어 &#x27;eat&#x27;이 중요한 토큰임을 학습했습니다. 그러나 모델은 &#x27;run&#x27;과 &#x27;ski&#x27;라는 단어를 본 적이 없기 때문에 이를 성공적으로 토큰화하지 못합니다. 이는 토크나이저를 훈련시킬 때 다양하고 광범위한 훈련 세트의 중요성을 강조합니다.</p>
<pre><code class="hljs language-js"><span class="hljs-title function_">print</span>(bpe.<span class="hljs-title function_">tokenize</span>(<span class="hljs-string">&quot;eating&quot;</span>));
<span class="hljs-title function_">print</span>(bpe.<span class="hljs-title function_">tokenize</span>(<span class="hljs-string">&quot;running&quot;</span>));
<span class="hljs-title function_">print</span>(bpe.<span class="hljs-title function_">tokenize</span>(<span class="hljs-string">&quot;skiing&quot;</span>));
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js">[<span class="hljs-string">&quot;먹&quot;</span>, <span class="hljs-string">&quot;어&quot;</span>, <span class="hljs-string">&quot;•&quot;</span>, <span class="hljs-string">&quot;ᆼ&quot;</span>][(<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;ᄂ&quot;</span>, <span class="hljs-string">&quot;ᄂ&quot;</span>, <span class="hljs-string">&quot;•&quot;</span>, <span class="hljs-string">&quot;ᆼ&quot;</span>)][(<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;스&quot;</span>, <span class="hljs-string">&quot;키&quot;</span>, <span class="hljs-string">&quot;•&quot;</span>, <span class="hljs-string">&quot;ᆼ&quot;</span>)];
</code></pre>
<p>BPE 토크나이저는 훈련 데이터에 나타난 문자만 인식할 수 있습니다. 예를 들어, 위의 훈련 데이터에는 고양이에 대해 이야기할 때 필요한 문자만 포함되어 있어서 z가 필요하지 않았습니다. 따라서 해당 토크나이저 버전은 z 문자를 어휘에 포함시키지 않으며, 실제 데이터를 토큰화할 때 해당 문자를 알 수 없는 토큰으로 변환합니다 (실제로, 오류 처리가 없어 모델이 알 수 없는 토큰을 생성하도록 지시하는 기능도 없으므로 모델이 충돌할 것이지만, 제품화된 모델에서는 이런 일이 발생할 수 있습니다).</p>
<p>GPT-2 및 RoBERTa에서 사용되는 BPE 토크나이저는 이 문제가 없으며 코드 내에 한 가지 속임수가 있습니다. Unicode 문자를 기반으로 훈련 데이터를 분석하는 대신, 문자의 바이트를 분석합니다. 이를 Byte-Level BPE라고 하며, 소규모 기본 어휘를 사용하여 모델이 볼 수 있는 모든 문자를 토큰화할 수 있게 합니다.</p>
<h2>WordPiece</h2>
<div class="content-ad"></div>
<p>WordPiece는 구글이 개발한 토큰화 방법으로, 그들의 중요한 BERT 모델 및 이로부터 파생된 모델들인 DistilBERT 및 MobileBERT에서 사용됩니다.</p>
<p>WordPiece 알고리즘의 전체 세부 내용은 공개되지 않았기 때문에 여기서 제시하는 방법론은 Hugging Face에 의해 제시된 해석을 기반으로 합니다. WordPiece 알고리즘은 BPE와 유사하지만 병합 규칙을 결정하는 데 다른 지표를 사용합니다. 가장 빈도가 높은 문자 쌍을 선택하는 대신 각 쌍에 대해 점수가 계산되고, 가장 높은 점수를 가진 쌍이 병합될 문자를 결정합니다. WordPiece는 다음과 같이 훈련됩니다.</p>
<p>단계 1) 말뭉치 구축</p>
<p>다시 한 번 입력 텍스트는 정규화 및 사전 토큰화 모델에 제공되어 깨끗한 단어를 생성합니다. 단어는 WordPiece 모델에 제공되어 각 단어의 빈도를 결정하고, 이 번호를 단어와 함께 &quot;말뭉치&quot;라고 불리는 리스트에 저장합니다.</p>
<div class="content-ad"></div>
<p>단계 2) 어휘 구성</p>
<p>BPE와 같이 코퍼스에서 단어를 개별 문자로 분해한 후, 단어들은 비어 있는 어휘 목록에 추가됩니다. 그러나 이번에는 단순히 각 개별 문자를 저장하는 대신, 두 개의 # 기호가 사용되어 문자가 단어의 시작에서 발견되었는지 또는 단어의 중간/끝에서 발견되었는지를 표시하는 마커로 사용됩니다. 예를 들어, 단어 cat은 BPE에서 [<code>c</code>, <code>a</code>, <code>t</code>]로 분할되지만 WordPiece에서는 [<code>c</code>, <code>##a</code>, <code>##t</code>]로 나타납니다. 이 시스템에서는 단어의 시작에서의 c와 단어의 중간 또는 끝에서의 ##c가 다르게 처리됩니다. 알고리즘은 매번 어떤 문자 쌍을 함께 병합할 수 있는지 결정할 때마다 이 어휘에 추가됩니다.</p>
<p>단계 3) 인접 문자 쌍의 쌍 점수 계산</p>
<p>BPE 모델과 달리, 이번에는 각 문자 쌍에 대해 점수가 계산됩니다. 먼저, 코퍼스에서 각 인접 문자 쌍을 식별하고, <code>c##a</code>, ##a##t 등이 계산됩니다. 그리고 빈도가 계산됩니다. 각 문자의 빈도도 결정됩니다. 이러한 값들을 알면, 다음 공식에 따라 쌍 점수를 계산할 수 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-23-TokenizationACompleteGuide_1.png" alt="Tokenization Guide"/></p>
<p>이 메트릭은 함께 자주 나타나지만 개별적으로나 다른 문자와 자주 나타나지 않는 문자에 더 높은 점수를 할당합니다. 이것이 WordPiece와 BPE 사이의 주된 차이점인데, BPE는 개별 문자의 전체 빈도를 고려하지 않습니다.</p>
<p>단계 4) 병합 규칙 생성</p>
<p>높은 점수는 자주 함께 나타나는 문자 쌍을 나타냅니다. 즉, c##a가 높은 쌍 점수를 가지면 c와 a가 말뭉치에서 함께 자주 나타나고 개별적으로는 그리 자주 나타나지 않는 것입니다. BPE와 마찬가지로, 병합 규칙은 가장 높은 점수를 가진 문자 쌍에 의해 결정됩니다. 이번에는 빈도가 점수를 결정하는 대신 쌍 점수로 결정됩니다.</p>
<div class="content-ad"></div>
<h2>단계 5) 단계 3과 4를 반복합니다.</h2>
<p>그런 다음 단계 3과 4를 반복하여 더 많은 병합 규칙을 찾고 어휘에 더 많은 문자 쌍을 추가합니다. 이 프로세스는 교육의 시작 부분에서 지정된 목표 크기에 도달할 때까지 계속됩니다.</p>
<p>아래는 이전에 작성한 BPE 모델을 상속하는 WordPiece의 구현입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordPiece</span>(<span class="hljs-variable constant_">BPE</span>):

    def <span class="hljs-title function_">add_hashes</span>(self, word):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 단어의 각 문자에 # 기호 추가

            문자열로 된 단어를 받아서 처음을 제외한 각 문자에 # 기호를 추가합니다.
            결과를 반환하며 각 요소가 처음 문자만 일반 문자이고 나머지는 # 기호가
            앞에 붙은 문자인 리스트로 반환합니다.

            인수:
                word (str): # 기호를 추가할 단어

            반환값:
                hashed_word (list): # 기호를 추가한 문자의 목록
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        hashed_word = [word[<span class="hljs-number">0</span>]]

        <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> word[<span class="hljs-number">1</span>:]:
            hashed_word.<span class="hljs-title function_">append</span>(f<span class="hljs-string">&#x27;##{char}&#x27;</span>)

        <span class="hljs-keyword">return</span> hashed_word


    def <span class="hljs-title function_">create_merge_rule</span>(self, corpus):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 병합 규칙을 만들어 self.merge_rules 목록에 추가합니다.

            인수:
                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자
                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소 및 단어
                    빈도를 나타내는 정수를 두 번째 요소로 하는 튜플의 목록

            반환값:
                없음
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        pair_frequencies = self.<span class="hljs-title function_">find_pair_frequencies</span>(corpus)
        char_frequencies = self.<span class="hljs-title function_">find_char_frequencies</span>(corpus)
        pair_scores = self.<span class="hljs-title function_">find_pair_scores</span>(pair_frequencies, char_frequencies)

        highest_scoring_pair = <span class="hljs-title function_">max</span>(pair_scores, key=pair_scores.<span class="hljs-property">get</span>)
        self.<span class="hljs-property">merge_rules</span>.<span class="hljs-title function_">append</span>(highest_scoring_pair.<span class="hljs-title function_">split</span>(<span class="hljs-string">&#x27;,&#x27;</span>))
        self.<span class="hljs-property">vocabulary</span>.<span class="hljs-title function_">append</span>(highest_scoring_pair)


    def <span class="hljs-title function_">create_vocabulary</span>(self, words):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 단어 목록에서 고유 문자 목록을 생성합니다.

            BPE 알고리즘과 달리 각 문자를 일반적으로 저장하는 대신 단어의 시작
            문자 (표시되지 않음)와 단어의 중간 또는 끝에 있는 문자(&#x27;</span>##<span class="hljs-string">&#x27;로 표시)를
            구분합니다. 예를 들어, 단어 &#x27;</span>cat<span class="hljs-string">&#x27;은 [&#x27;</span>c<span class="hljs-string">&#x27;, &#x27;</span>##a<span class="hljs-string">&#x27;, &#x27;</span>##t<span class="hljs-string">&#x27;]로 분할됩니다.

            인수:
                words (list): 입력 텍스트의 단어를 포함하는 문자열의 목록

            반환값:
                vocabulary (list): 입력 단어 목록의 모든 고유 문자 목록
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        vocabulary = <span class="hljs-title function_">set</span>()
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> <span class="hljs-attr">words</span>:
            vocabulary.<span class="hljs-title function_">add</span>(word[<span class="hljs-number">0</span>])
            <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> word[<span class="hljs-number">1</span>:]:
                vocabulary.<span class="hljs-title function_">add</span>(f<span class="hljs-string">&#x27;##{char}&#x27;</span>)

        # 나중에 추가할 수 있도록 목록으로 변환
        vocabulary = <span class="hljs-title function_">list</span>(vocabulary)
        <span class="hljs-keyword">return</span> vocabulary


    def <span class="hljs-title function_">find_char_frequencies</span>(self, corpus):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 코퍼스에서 각 문자의 빈도수를 찾습니다.

            코퍼스를 순환하고 문자의 빈도수를 계산합니다.
            &#x27;</span>c<span class="hljs-string">&#x27;와 &#x27;</span>##c<span class="hljs-string">&#x27;는 서로 다른 문자임에 유의하세요.
            &#x27;</span>c<span class="hljs-string">&#x27;는 단어의 시작 문자를 나타내고, &#x27;</span>##c<span class="hljs-string">&#x27;는 단어의 중간 또는 끝을
            나타냅니다. 각 문자 쌍을 키로, 해당 빈도를 값으로 하는 사전 반환합니다.

            인수:
                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자
                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소 및 단어
                    빈도를 나타내는 정수를 두 번째 요소로 하는 튜플의 목록

            반환값:
                char_frequencies (dict): 입력 코퍼스의 문자 및 해당 빈도수를
                    키와 값으로 하는 사전
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        char_frequencies = <span class="hljs-title function_">dict</span>()

        <span class="hljs-keyword">for</span> word, word_freq <span class="hljs-keyword">in</span> <span class="hljs-attr">corpus</span>:
            <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> <span class="hljs-attr">word</span>:
                <span class="hljs-keyword">if</span> char <span class="hljs-keyword">in</span> <span class="hljs-attr">char_frequencies</span>:
                    char_frequencies[char] += word_freq
                <span class="hljs-attr">else</span>:
                    char_frequencies[char] = word_freq

        <span class="hljs-keyword">return</span> char_frequencies


    def <span class="hljs-title function_">find_pair_scores</span>(self, pair_frequencies, char_frequencies):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 코퍼스에서 각 문자 쌍에 대한 쌍 점수를 찾습니다.

            pair_frequencies 사전을 순환하고 코퍼스에서 각 인접 문자 쌍의 쌍
            점수를 계산합니다. 점수를 사전에 저장하고 반환합니다.

            인수:
                pair_frequencies (dict): 코퍼스에서 인접 문자 쌍을 키로, 각
                    쌍 빈도수를 값으로 하는 사전

                char_frequencies (dict): 코퍼스에서 문자를 키로, 해당 빈도수를
                    값으로 하는 사전

            반환값:
                pair_scores (dict): 입력 코퍼스의 인접 문자 쌍을 키로, 해당
                    쌍 점수를 값으로 하는 사전
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        pair_scores = <span class="hljs-title function_">dict</span>()

        <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> pair_frequencies.<span class="hljs-title function_">keys</span>():
            char_1 = pair.<span class="hljs-title function_">split</span>(<span class="hljs-string">&#x27;,&#x27;</span>)[<span class="hljs-number">0</span>]
            char_2 = pair.<span class="hljs-title function_">split</span>(<span class="hljs-string">&#x27;,&#x27;</span>)[<span class="hljs-number">1</span>]
            denominator = (char_frequencies[char_1] * char_frequencies[char_2])
            score = (pair_frequencies[pair]) / denominator
            pair_scores[pair] = score

        <span class="hljs-keyword">return</span> pair_scores


    def <span class="hljs-title function_">get_merged_chars</span>(self, char_1, char_2):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 가장 높은 점수의 쌍을 병합하고 self.merge 메서드에 반환합니다.

            필요에 따라 # 기호를 제거하고 가장 높은 점수의 쌍을 병합한 후
            병합된 문자를 self.merge 메서드에 반환합니다.

            인수:
                char_1 (str): 가장 높은 점수의 쌍에서 첫 번째 문자
                char_2 (str): 가장 높은 점수의 쌍에서 두 번째 문자

            반환값:
                merged_chars (str): 병합된 문자
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        <span class="hljs-keyword">if</span> char_2.<span class="hljs-title function_">startswith</span>(<span class="hljs-string">&#x27;##&#x27;</span>):
            merged_chars = char_1 + char_2[<span class="hljs-number">2</span>:]
        <span class="hljs-attr">else</span>:
            merged_chars = char_1 + char_2

        <span class="hljs-keyword">return</span> merged_chars


    def <span class="hljs-title function_">initialize_corpus</span>(self, words):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 각 단어를 문자로 분할하고 단어 빈도수를 계산합니다.

            입력 단어 목록의 각 단어를 모든 문자로 분할합니다. 각 단어에 대해
            분할된 단어를 튜플의 첫 번째 요소로 리스트로 저장합니다.
            단어의 빈도수는 정수로 튜플의 두 번째 요소로 저장합니다.
            이 작업을 수행한 후 결과인 &#x27;</span>corpus<span class="hljs-string">&#x27; 목록 반환합니다.

            인수:
                없음

            반환값:
                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자
                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소와 단어
                    목록에서 해당 단어의 빈도수를 나타내는 정수를 표시하는
                    두 번째 요소로 하는 튜플의 목록
        &#x27;</span><span class="hljs-string">&#x27;&#x27;</span>
        corpus = self.<span class="hljs-title function_">calculate_frequency</span>(words)
        corpus = [(self.<span class="hljs-title function_">add_hashes</span>(word), freq) <span class="hljs-keyword">for</span> (word, freq) <span class="hljs-keyword">in</span> corpus]
        <span class="hljs-keyword">return</span> corpus

    def <span class="hljs-title function_">tokenize</span>(self, text):
        <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-string">&#x27; 텍스트를 토큰 목록으로 만듭니다.

            인수
</span></code></pre>
<div class="content-ad"></div>
<p>WordPiece 알고리즘은 BPE 알고리즘에 주어진 장난감 데이터세트와 동일한 데이터세트로 아래에서 훈련됩니다. 이번에 학습한 토큰은 매우 다른 것을 알 수 있습니다. WordPiece는 문자가 서로 더 자주 함께 나타나는 경우를 선호하며, 그래서 데이터세트에 함께만 존재하고 홀로 존재하지 않는 &#x27;m&#x27;과 &#x27;p&#x27;는 즉시 병합됩니다. 여기서 이 아이디어는 모델이 문자를 병합함으로써 무엇이 손실되는지 고려하도록 강요하는 것입니다. 즉, 이러한 문자들이 항상 함께 있는가요? 그렇다면, 전혀 하나의 단위로 명백하게 병합되어야 합니다. 또는, 코퍼스에서 문자가 매우 빈번한가요? 그렇다면, 문자는 그냥 일반적이며 데이터세트 안에서 풍부하게 나타나므로 다른 토큰 옆에 나타날 것입니다.</p>
<pre><code class="hljs language-js">wp = <span class="hljs-title class_">WordPiece</span>()
wp.<span class="hljs-title function_">train</span>(words, <span class="hljs-number">30</span>)

<span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;INITIAL CORPUS:\n{wp.corpus_history[0]}\n&#x27;</span>)
<span class="hljs-keyword">for</span> rule, corpus <span class="hljs-keyword">in</span> <span class="hljs-title function_">list</span>(<span class="hljs-title function_">zip</span>(wp.<span class="hljs-property">merge_rules</span>, wp.<span class="hljs-property">corpus_history</span>[<span class="hljs-number">1</span>:])):
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">&#x27;NEW MERGE RULE: Combine &quot;{rule[0]}&quot; and &quot;{rule[1]}&quot;&#x27;</span>)
    <span class="hljs-title function_">print</span>(corpus, end=<span class="hljs-string">&#x27;\n\n&#x27;</span>)
</code></pre>
<pre><code class="hljs language-js">초기 코퍼스:
[([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>, <span class="hljs-string">&#x27;##s&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>], <span class="hljs-number">10</span>), ([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>, <span class="hljs-string">&#x27;##i&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##g&#x27;</span>], <span class="hljs-number">3</span>),
([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;##u&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##i&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##g&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;##u&#x27;</span>, <span class="hljs-string">&#x27;##m&#x27;</span>, <span class="hljs-string">&#x27;##p&#x27;</span>, <span class="hljs-string">&#x27;##i&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##g&#x27;</span>], <span class="hljs-number">1</span>),
([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;##o&#x27;</span>, <span class="hljs-string">&#x27;##o&#x27;</span>, <span class="hljs-string">&#x27;##d&#x27;</span>], <span class="hljs-number">6</span>)]

<span class="hljs-variable constant_">NEW</span> <span class="hljs-variable constant_">MERGE</span> <span class="hljs-attr">RULE</span>: <span class="hljs-string">&quot;##m&quot;</span>과 <span class="hljs-string">&quot;##p&quot;</span> 병합
[([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>], <span class="hljs-number">5</span>), ([<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>, <span class="hljs-string">&#x27;##s&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>], <span class="hljs-number">10</span>), ([<span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;##a&#x27;</span>, <span class="hljs-string">&#x27;##t&#x27;</span>, <span class="hljs-string">&#x27;##i&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##g&#x27;</span>], <span class="hljs-number">3</span>),
([<span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;##u&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##i&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##g&#x27;</span>], <span class="hljs-number">2</span>),
([<span class="hljs-string">&#x27;j&#x27;</span>, <span class="hljs-string">&#x27;##u&#x27;</span>, <span class="hljs-string">&#x27;##mp&#x27;</span>, <span class="hljs-string">&#x27;##i&#x27;</span>, <span class="hljs-string">&#x27;##n&#x27;</span>, <span class="hljs-string">&#x27;##g&#x27;</span>], <span class="hljs-number">1</span>),
([<span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;##o&#x27;</span>, <span class="hljs-string">&#x27;##o&#x27;</span>, <span class="hljs-string">&#x27;##d&#x27;</span>], <span class="hljs-number">6</span>)]

(이하 생략)
</code></pre>
<p>이제 WordPiece 알고리즘이 훈련되었으므로(즉, 모든 병합 규칙이 발견되었으므로), 모델은 모든 텍스트를 토큰화하기 위해 각 단어를 모든 문자로 분리한 다음 문자열의 처음부분에 대해 알려진 토큰을 찾을 수 있는 최대 토큰을 찾아서, 나머지 부분은 찾을 수 있는 최대 토큰을 찾는 방식으로 사용할 수 있습니다. 이 과정은 더 이상 훈련 데이터로부터 알려진 토큰과 일치하지 않을 때까지 반복되며, 따라서 문자열의 남은 부분은 최종 토큰으로 취합니다.</p>
<div class="content-ad"></div>
<p>학습 데이터가 제한적이지만, 모델은 여전히 유용한 토큰을 학습했습니다. 그러나 많은 추가 학습 데이터가 필요함을 명백히 알 수 있습니다. 이 토크나이저를 유용하게 만들기 위해 더 많은 학습 데이터가 필요합니다. 예시 문자열에 대한 성능을 테스트할 수 있습니다. 예시로 &#x27;jumper&#x27; 단어로 시작해보겠습니다. 먼저 문자열은 [&#x27;jump&#x27;, &#x27;er&#x27;]로 분리됩니다. 왜냐하면 jump는 단어의 시작에서 발견할 수 있는 가장 큰 토큰이기 때문입니다. 다음으로 er 문자열은 각각의 문자 e와 r로 나뉩니다.</p>
<pre><code class="hljs language-js"><span class="hljs-title function_">print</span>(wp.<span class="hljs-title function_">tokenize</span>(<span class="hljs-string">&quot;jumper&quot;</span>));
</code></pre>
<pre><code class="hljs language-js">[<span class="hljs-string">&quot;jump&quot;</span>, <span class="hljs-string">&quot;e&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>];
</code></pre>
<h2>단일 토큰화</h2>
<div class="content-ad"></div>
<p>Unigram 토크나이저는 BPE와 WordPiece와 다른 방식으로 작동합니다. 큰 어휘로 시작하여 원하는 크기에 도달할 때까지 반복적으로 줄여나갑니다.</p>
<p>Unigram 모델은 각 단어 또는 문자의 확률을 고려하는 통계적 방법을 사용합니다. 예를 들어, &quot;Cats are great but dogs are better&quot;라는 문장은 [<code>Cats</code>, <code>are</code>, <code>great</code>, <code>but</code>, <code>dogs</code>, <code>are</code>, <code>better</code>] 또는 [<code>C</code>, <code>a</code>, <code>t</code>, <code>s</code>, <code>_a</code>, <code>r</code>, <code>e</code>, <code>_g</code>,<code>r</code>, <code>e</code>, <code>a</code>, <code>t</code>, <code>_b</code>, <code>u</code>, <code>t</code>, <code>_d</code>, <code>o</code>, <code>g</code>, <code>s</code> <code>_a</code>, <code>r</code>, <code>e</code>, <code>_b</code>, <code>e</code>, <code>t</code>, <code>t</code>, <code>e</code>, <code>r</code>]로 분할될 수 있습니다. 문장이 문자로 분할된 경우, 새로운 단어의 시작을 나타내기 위해 각 문자의 시작 부분에 밑줄이 추가됩니다.</p>
<p>이러한 목록의 각 요소는 토큰 t로 간주될 수 있으며, t1, t2, ..., tn의 일련의 토큰이 발생할 확률은 다음과 같습니다:</p>
<img src="/assets/img/2024-05-23-TokenizationACompleteGuide_2.png"/>
<div class="content-ad"></div>
<p>Unigram 모델은 다음 단계를 통해 훈련됩니다:</p>
<p>단계 1) 코퍼스 구성</p>
<p>언제나처럼 입력 텍스트는 정규화 및 사전 토크나이제이션 모델에 전달되어 깨끗한 단어가 생성됩니다. 그런 다음 단어들은 Unigram 모델에 전달되어 각 단어의 빈도를 결정하고, 이 숫자를 단어와 함께 코퍼스라는 목록에 저장합니다.</p>
<p>단계 2) 어휘 구성</p>
<div class="content-ad"></div>
<p>Unigram 모델의 어휘 크기는 매우 크게 시작되고, 원하는 크기에 도달할 때까지 반복적으로 감소합니다. 초기 어휘를 구성하려면 말뭉치에서 가능한 모든 부분 문자열을 찾습니다. 예를 들어, 말뭉치의 첫 번째 단어가 &#x27;cats&#x27;인 경우, 부분 문자열 [&#x27;c&#x27;, &#x27;a&#x27;, &#x27;t&#x27;, &#x27;s&#x27;, &#x27;ca&#x27;, &#x27;at&#x27;, &#x27;ts&#x27;, &#x27;cat&#x27;, &#x27;ats&#x27;]이 어휘에 추가됩니다.</p>
<p>3단계) 각 토큰의 확률 계산</p>
<p>토큰의 확률은 말뭉치에서 토큰의 발생 횟수를 찾아 총 토큰 발생 횟수로 나누어 근사적으로 계산됩니다.</p>
<p><img src="/assets/img/2024-05-23-TokenizationACompleteGuide_3.png" alt="이미지"/></p>
<div class="content-ad"></div>
<p>4단계) 단어의 모든 가능한 세분화 찾기</p>
<p>학습 말뭉치에서 단어가 cat인 경우를 고려해보겠습니다. 이는 다음과 같이 세분화될 수 있습니다:</p>
<p>[<code>c</code>, <code>a</code>, <code>t</code>]</p>
<p>[<code>ca</code>, <code>t</code>]</p>
<div class="content-ad"></div>
<p>[<code>c</code>, <code>at</code>]</p>
<p>[<code>cat</code>]</p>
<p>단계 5) 말뭉치에서 발생 가능한 각 세분화의 근사 확률 계산</p>
<p>위의 방정식들을 결합하면 각 토큰 시리즈에 대한 확률을 얻을 수 있습니다. 예를 들어, 이것은 다음과 같이 보일 수 있습니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-23-TokenizationACompleteGuide_4.png"/>
<p>가장 높은 확률 점수를 가진 세그먼트 [<code>c</code>, <code>at</code>]가 사용되어 단어를 토크나이즈했습니다. 따라서 단어 cat은 [<code>c</code>, <code>at</code>]으로 토큰화됩니다. 단어가 긴 경우 토큰화시 단어 내 여러 곳에서 분할이 발생할 수 있습니다. 예를 들어 [<code>token</code>, <code>iza</code>, tion] 또는 [<code>token</code>, <code>ization</code>] 같은 경우도 있을 수 있습니다.</p>
<p>6단계) 손실 계산</p>
<p>손실이란 모델의 점수를 나타내며, 중요한 토큰이 어휘에서 제거되면 손실이 크게 증가하지만 중요하지 않은 토큰이 제거되면 손실은 크게 증가하지 않습니다. 모델에서 각 토큰을 제거했을 때 손실이 얼마나 되는지 계산하여, 어휘 중에서 가장 쓸모없는 토큰을 찾을 수 있습니다. 훈련 세트 말뭉치에서 가장 유용한 토큰만 남도록 어휘 크기가 감소할 때까지 반복적으로 수행할 수 있습니다. 손실은 다음과 같이 주어집니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-23-TokenizationACompleteGuide_5.png" alt="Tokenization Guide"/></p>
<p>필요한 양만큼 문자가 제거되어 어휘를 원하는 크기로 줄일 때, 교육은 완료되고 모델을 사용하여 단어를 토큰화할 수 있습니다.</p>
<h2>BPE, WordPiece 및 Unigram 비교</h2>
<p>학습 세트 및 토큰화해야 할 데이터에 따라 어떤 토크나이저가 다른 것보다 더 잘 작동할 수 있습니다. 언어 모델에 대한 토크나이저를 선택할 때, 특정 사용 사례에 사용된 학습 세트를 실험하여 최상의 결과를 얻는 것이 가장 좋을 수 있습니다. 그러나 이 세 가지 토크나이저의 일반적인 경향에 대해 논의하는 것이 유용할 수 있습니다.</p>
<div class="content-ad"></div>
<p>세 가지 중에서 BPE가 현재 언어 모델 토크나이저로 가장 인기 있는 선택인 것으로 보입니다. 그러나 변화가 빠르게 일어나는 이 분야에서는 앞으로 변동이 있을 수 있습니다. 사실, SentencePiece와 같은 다른 서브워드 토크나이저들이 최근에 훨씬 더 인기를 얻고 있습니다.</p>
<p>WordPiece는 BPE와 Unigram에 비해 더 많은 단어 토큰을 생성하는 것으로 보입니다. 그러나 모델 선택과 관계 없이 어휘 크기가 커질수록 모든 토크나이저가 더 적은 토큰을 생성하는 것으로 보입니다.</p>
<p>최종적으로, 토크나이저의 선택은 모델과 함께 사용하려는 데이터셋에 따라 다릅니다. 안전한 선택은 BPE 또는 SentencePiece를 시도하고, 그 이후에 실험하는 것일 수 있습니다.</p>
<h2>후처리</h2>
<div class="content-ad"></div>
<p>토큰화 파이프라인의 마지막 단계는 후처리입니다. 여기서 필요한 경우 출력에 최종 수정을 가할 수 있습니다. BERT는 이 단계를 사용하여 두 가지 추가 토큰을 추가하는 데 유명합니다:</p>
<ul>
<li>[CLS] - 이 토큰은 <code>classification</code>를 나타내며 입력 텍스트의 시작을 표시하는 데 사용됩니다. 이는 BERT에서 필요한 것인데, 이 토큰의 이름에서 알 수 있듯이 이를 사용하여 분류 작업이 수행되었기 때문입니다. 분류 작업에 사용되지 않을 때도 모델에서 여전히 이 토큰을 예상합니다.</li>
<li>[SEP] - 이 토큰은 <code>separation</code>을 나타내며 입력에서 문장을 분리하는 데 사용됩니다. BERT가 수행하는 많은 작업에 유용하며, 동일한 프롬프트에서 동시에 여러 지시사항을 처리할 때도 사용됩니다.</li>
</ul>
<h1>Python 라이브러리의 토크나이저</h1>
<p>Hugging Face는 Python을 포함한 여러 프로그래밍 언어에서 사용할 수 있는 토크나이저 라이브러리를 제공합니다. 이 라이브러리에는 사용자가 사전 훈련된 모델을 사용할 수 있는 일반 Tokenizer 클래스가 포함되어 있으며, 전체 목록은 Hugging Face 웹사이트에서 확인할 수 있습니다. 게다가, 라이브러리에는 사용자가 자체 데이터로 훈련할 수 있는 네 가지 사전 제작되지만 미학습된 모델도 포함되어 있습니다. 이는 특정 유형의 문서에 튜닝된 특정 토크나이저를 작성하는 데 유용합니다. 아래 셀은 Python에서 사전 훈련된 및 미학습된 토크나이저를 사용하는 예시를 보여줍니다.</p>
<div class="content-ad"></div>
<p>프리트레인 토크나이저 사용하기</p>
<p>토크나이저 라이브러리를 사용하면 프리트레인 토크나이저를 쉽게 사용할 수 있습니다. Tokenizer 클래스를 가져와서 from_pretrained 메소드를 호출하고 사용할 토크나이저의 모델 이름을 전달하면 됩니다. 모델의 목록은 [16]에서 확인할 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> <span class="hljs-title class_">Tokenizer</span>

tokenizer = <span class="hljs-title class_">Tokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
</code></pre>
<p>토크나이저 학습하기</p>
<div class="content-ad"></div>
<p>원하는 토큰 만들기되지만 미학습 토크나이저를 사용하려면 tokenizers 라이브러리에서 원하는 모델을 가져와서 모델 클래스의 인스턴스를 만들면 됩니다. 위에서 설명한대로 라이브러리에는 네 가지 모델이 포함되어 있습니다:</p>
<ul>
<li>BertWordPieceTokenizer - 유명한 Bert 토크나이저인 WordPiece를 사용합니다.</li>
<li>CharBPETokenizer - 원래의 BPE(BPE)</li>
<li>ByteLevelBPETokenizer - BPE의 바이트 레벨 버전</li>
<li>SentencePieceBPETokenizer - SentencePiece에서 사용하는 BPE 구현과 호환되는 버전</li>
</ul>
<p>모델을 학습하려면 train 메서드를 사용하고 학습 데이터가 포함된 파일의 경로(또는 파일 경로 목록)를 전달하면 됩니다. 학습을 마치면 모델은 encode 메서드를 사용하여 일부 텍스트를 토큰화하는 데 사용할 수 있습니다. 마지막으로 학습된 토크나이저는 save 메서드를 사용하여 저장할 수 있으므로 학습을 다시 수행할 필요가 없습니다. 아래는 Hugging Face Tokenizers GitHub 페이지에서 제공되는 예제를 수정한 예시 코드입니다 [17].</p>
<pre><code class="hljs language-js"># 토크나이저 가져오기
<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> <span class="hljs-title class_">BertWordPieceTokenizer</span>, <span class="hljs-title class_">CharBPETokenizer</span>, \
                       <span class="hljs-title class_">ByteLevelBPETokenizer</span>, <span class="hljs-title class_">SentencePieceBPETokenizer</span>

# 모델 인스턴스화
tokenizer = <span class="hljs-title class_">CharBPETokenizer</span>()

# 모델 학습
tokenizer.<span class="hljs-title function_">train</span>([<span class="hljs-string">&#x27;./path/to/files/1.txt&#x27;</span>, <span class="hljs-string">&#x27;./path/to/files/2.txt&#x27;</span>])

# 텍스트 토큰화
encoded = tokenizer.<span class="hljs-title function_">encode</span>(<span class="hljs-string">&#x27;I can feel the magic, can you?&#x27;</span>)

# 모델 저장
tokenizer.<span class="hljs-title function_">save</span>(<span class="hljs-string">&#x27;./path/to/directory/my-bpe.tokenizer.json&#x27;</span>)
</code></pre>
<div class="content-ad"></div>
<p>토크나이저 라이브러리는 이전 섹션에서 보여주었던 것과 같이 처음부터 전체 모델을 구현할 필요 없이 매우 빠르게 사용자 정의 토크나이저를 만들 수 있는 구성 요소도 제공합니다. 아래 셀에는 Hugging Face GitHub 페이지 [17]에서 가져온 예시가 표시되어 있습니다. 해당 예시에서는 토크나이저의 사전 토크나이제이션 및 디코딩 단계를 사용자 정의하는 방법을 보여줍니다. 이 경우, 사전 토크나이제이션 단계에서 접두어 공백이 추가되었고, 디코더로는 ByteLevel 디코더가 선택되었습니다. Hugging Face 문서 [18]에는 사용자 정의 옵션의 전체 목록이 제공됩니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> <span class="hljs-title class_">Tokenizer</span>, models, pre_tokenizers, decoders, trainers, \
                       processors

# 토크나이저 초기화
tokenizer = <span class="hljs-title class_">Tokenizer</span>(models.<span class="hljs-title function_">BPE</span>())

# 사전 토크나이제이션 및 디코딩 사용자 정의
tokenizer.<span class="hljs-property">pre_tokenizer</span> = pre_tokenizers.<span class="hljs-title class_">ByteLevel</span>(add_prefix_space=<span class="hljs-title class_">True</span>)
tokenizer.<span class="hljs-property">decoder</span> = decoders.<span class="hljs-title class_">ByteLevel</span>()
tokenizer.<span class="hljs-property">post_processor</span> = processors.<span class="hljs-title class_">ByteLevel</span>(trim_offsets=<span class="hljs-title class_">True</span>)

# 그리고 학습
trainer = trainers.<span class="hljs-title class_">BpeTrainer</span>(
    vocab_size=<span class="hljs-number">20000</span>,
    min_frequency=<span class="hljs-number">2</span>,
    initial_alphabet=pre_tokenizers.<span class="hljs-property">ByteLevel</span>.<span class="hljs-title function_">alphabet</span>()
)
tokenizer.<span class="hljs-title function_">train</span>([
    <span class="hljs-string">&quot;./path/to/dataset/1.txt&quot;</span>,
    <span class="hljs-string">&quot;./path/to/dataset/2.txt&quot;</span>,
    <span class="hljs-string">&quot;./path/to/dataset/3.txt&quot;</span>
], trainer=trainer)

# 그리고 저장
tokenizer.<span class="hljs-title function_">save</span>(<span class="hljs-string">&quot;byte-level-bpe.tokenizer.json&quot;</span>, pretty=<span class="hljs-title class_">True</span>)
</code></pre>
<h1>결론</h1>
<p>토큰화 파이프라인은 언어 모델의 중요한 부분이며, 어떤 종류의 토크나이저를 사용할지 결정할 때 신중한 고려가 필요합니다. 요즘에는 Hugging Face와 같은 라이브러리의 개발자들이 우리를 대신하여 많은 이러한 결정을 내려주고 있습니다. 이를 통해 사용자는 빠르게 사용자 지정 데이터로 언어 모델을 학습하고 사용할 수 있습니다. 그러나 토큰화 방법에 대한 탄탄한 이해는 모델을 미세 조정하고 다양한 데이터셋에서 추가 성능을 얻는 데 귀중합니다.</p>
<div class="content-ad"></div>
<h1>참고 자료</h1>
<p>[1] 표지 이미지 — Stable Diffusion Web</p>
<p>[2] 토큰 정의 — Stanford NLP 그룹</p>
<p>[3] 단어 토크나이저 — Towards Data Science</p>
<div class="content-ad"></div>
<ul>
<li>
<p>[4] TransformerXL 논문 — ArXiv</p>
</li>
<li>
<p>[5] Tokenizers — Hugging Face</p>
</li>
<li>
<p>[6] 단어 기반, 서브워드, 문자 기반 토크나이저 — Towards Data Science</p>
</li>
<li>
<p>[7] 토큰화 파이프라인 — Hugging Face</p>
</li>
</ul>
<div class="content-ad"></div>
<p>[8] Pre-tokenizers — Hugging Face</p>
<p>[9] Language Models are Unsupervised Multitask Learners — OpenAI</p>
<p>[10] BART Model for Text Autocompletion in NLP — Geeks for Geeks</p>
<p>[11] Byte Pair Encoding — Hugging Face</p>
<div class="content-ad"></div>
<p>[12] WordPiece 토큰화 — Hugging Face</p>
<p>[13] 두 분 NLP — 토큰화 방법론의 분류 — Medium</p>
<p>[14] 서브워드 토크나이저 비교 — Vinija AI</p>
<p>[15] BERT가 단어 맥락 관계를 배우는 데 어떻게 Attention 메커니즘과 Transformer를 활용하는가 — Medium</p>
<div class="content-ad"></div>
<p>[16] 사전 훈련된 모델 목록 — Hugging Face</p>
<p>[17] Hugging Face Tokenizers 라이브러리 — GitHub</p>
<p>[18] 사전 토크나이제이션 문서 — Hugging Face</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"토큰화 - 완벽한 가이드","description":"","date":"2024-05-23 18:17","slug":"2024-05-23-TokenizationACompleteGuide","content":"\n## 바이트 페어 인코딩, 워드피스 등과 같은 것들과 함께 Python 코드!\n\n“LLMs from Scratch” 시리즈 중 제1부 — 대형 언어 모델을 이해하고 구축하는 완벽한 안내서입니다. 이 모델이 어떻게 작동하는지 더 자세히 알아보고 싶다면 다음을 읽어보는 것을 권장합니다:\n\n- 프롤로그: 대형 언어 모델의 간단한 역사\n- 파트 1: 토크나이제이션 — 완벽한 안내서\n- 파트 2: Python에서 워드투벡으로부터 스크래치로 단어 임베딩\n- 파트 3: 코드로 설명하는 셀프 어텐션\n- 파트 4: 코드로 이해하는 BERT의 완벽한 안내서\n\n![이미지](/assets/img/2024-05-23-TokenizationACompleteGuide_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 이 콘텐츠가 도움이 되었다면, 아래 방법으로 저를 지원해주십시오:\n\n- 기사에 Clap(박수)을 보내세요\n- 저를 Medium이나 LinkedIn에서 팔로우하여 향후 게시물에 대한 업데이트를 받으세요\n\n## 서두\n\n대형 언어 모델 (LLM)은 2022년 11월 OpenAI의 ChatGPT가 출시된 이후 매우 인기를 얻었습니다. 그 이후로 이러한 언어 모델의 사용이 급증했으며, HuggingFace의 Transformer 라이브러리와 PyTorch와 같은 라이브러리의 도움을 받았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 모든 이들 준비하고 있는 완제품 도구들로 인해, 기본 수준에서 무슨 일이 일어나고 있는지 추상화하는 것이 쉽습니다. 그 결과로 많은 온라인 튜토리얼들이 당신이 자체 모델을 생성할 때 '무엇'을 알려주고 '왜'는 알려주지 않는 경우가 많습니다. 이 기사 시리즈는 이를 해결하고자 합니다. '처음부터 LLMs 만들기'는 대형 언어 모델을 구성하는 구성 요소를 분해하고, 내부 작동 방식을 설명합니다. 그의 목표는 다음과 같습니다:\n\n- 수학의 직관적 이해를 포함한, LLMs가 어떻게 작동하는지의 기본적인 이해 구축\n- 각 구성 요소가 어떻게 작동하는지를 보여주며, Python에서 처음부터 구현 방법을 보여줌\n- 불필요한 추상화를 줄이기 위해 가급적이면 최소한의 라이브러리 사용\n\n말이 다 되었으니, 시작해보겠습니다.\n\n# 토크나이저란 무엇인가?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자연어 처리 문제는 텍스트 데이터를 사용하는데, 기계가 즉시 이해하기 어렵습니다. 컴퓨터가 언어를 처리하려면 먼저 텍스트를 숫자 형식으로 변환해야 합니다. 이 프로세스는 토크나이저라는 모델에 의해 주로 두 단계로 수행됩니다.\n\n단계 1: 입력 텍스트를 토큰으로 분할\n\n토크나이저는 먼저 텍스트를 가져와 단어, 단어 부분 또는 개별 문자가 될 수 있는 작은 조각으로 나눕니다. 이러한 작은 텍스트 조각을 토큰이라고 합니다. 스탠포드 NLP 그룹은 토큰을 더 엄격하게 정의합니다.\n\n단계 2: 각 토큰에 식별자 할당\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토크나이저가 텍스트를 토큰으로 분리한 후, 각 토큰에 토큰 ID라고 불리는 정수 번호를 할당할 수 있습니다. 예를 들어, \"cat\"이라는 단어가 15라는 값으로 할당될 수 있고, 따라서 입력 텍스트의 모든 cat 토큰은 숫자 15로 표시됩니다. 텍스트 토큰을 숫자 표현으로 교체하는 과정을 인코딩이라고 합니다. 비슷하게, 인코딩된 토큰을 다시 텍스트로 변환하는 과정을 디코딩이라고 합니다.\n\n단일 숫자를 사용하여 토큰을 표현하는 것에는 단점이 있다는 것을 알 수 있습니다. 그래서 이러한 코드들은 단어 임베딩을 생성하기 위해 추가로 처리되며, 이것은 이 시리즈의 다음 기사의 주제입니다.\n\n# 토큰화 방법\n\n텍스트를 토큰으로 나누는 세 가지 주요 방법이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 단어 기반\n- 문자 기반\n- 부분어 기반\n\n## 단어 기반 토크나이저:\n\n단어 기반 토크나이제이션은 세 가지 토큰화 방법 중 가장 간단한 방법입니다. 여기서 토크나이저는 문장을 단어로 분할하는데 각 공백 문자를 기준으로 나눕니다(때로는 '화이트스페이스 기반 토큰화'라고도 함) 또는 유사한 규칙 세트(구두점 기반 토큰화, 트리뱅크 토큰화 등)에 따라 분할할 수도 있습니다 [12].\n\n예를 들어, 다음과 같은 문장:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고양이들은 멋지지만, 개들이 더 좋아요!\n\n띄어쓰기 문자로 분할하면:\n\n[`Cats`, `are`, `great,`, `but`, `dogs`, `are`, `better!`]\n\n또는 구두점과 공백을 기준으로 분할하면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[`Cats`, `are`, `great`, `,`, `but`, `dogs`, `are`, `better`, `!`]\n\n위 간단한 예제를 통해 분할을 결정하는 데 사용하는 규칙이 중요하다는 것을 분명히 이해할 수 있습니다. 공백 접근 방식은 잠재적으로 희귀한 토큰 `better!`를 제공하며, 두 번째 분할은 덜 희귀한 토큰 `better`와 `!`을 생성합니다. 문장부호를 완전히 제거하지 않도록 주의해야 합니다. 문장부호에는 매우 구체적인 의미가 있을 수 있기 때문입니다. 그 중 하나는 ‘작은따옴표(apostrophe)’입니다. 작은따옴표는 단수와 소유 형태를 구별할 수 있습니다. 예를 들어 “book's”는 책의 속성을 가리키며 “the book's spine is damaged”와 같이 사용되고, “books”는 여러 권의 책을 가리킵니다.\n\n토큰을 생성한 후, 각 토큰에 번호를 할당할 수 있습니다. 토큰 생성기가 이미 본 토큰을 생성할 때 다음에 볼 토큰은 그 단어에 지정된 번호를 간단히 할당할 수 있습니다. 예를 들어 위 문장에서 `great`가 1이라는 값으로 할당된 경우, 이후의 `great` 단어는 모두 1의 값으로 할당됩니다.\n\n단어 기반 토크나이저의 장단점:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n워드 기반 방법으로 생성된 토큰들은 각각 의미론적 및 문맥 정보를 포함하고 있어 많은 정보를 담고 있습니다. 그러나 이 방법의 가장 큰 단점 중 하나는 매우 유사한 단어가 완전히 다른 토큰으로 처리된다는 것입니다. 예를 들어, cat과 cats 간의 연결은 존재하지 않으며, 이들은 별개의 단어로 처리됩니다. 이는 많은 단어를 포함하는 대규모 응용 프로그램에서 문제가 될 수 있습니다. 모델 어휘의 가능한 토큰 수가 매우 커질 수 있기 때문입니다. 영어는 약 17만 단어가 있으며, 각 단어에 대한 복수형이나 과거형과 같은 다양한 문법 형태를 포함하면 폭발적인 어휘 문제가 발생할 수 있습니다. TransformerXL 토크나이저가 사용하는 공백 기반 분할은 어휘 크기가 25만 개를 초과하도록 이끌었습니다.\n\n이 문제를 해결하는 한 가지 방법은 모델이 학습할 수 있는 토큰 수에 하드 리미트를 부여하는 것입니다(예: 1만). 이는 가장 빈도가 높은 1만개의 토큰을 벗어나는 모든 단어를 어휘 외로 처리하고, 숫자 값 대신 UNKNOWN 토큰 값을 할당하는 것입니다(UNK로 축약되기도 합니다). 이는 많은 알려지지 않은 단어가 있는 경우에 성능에 영향을 줄 수 있지만, 데이터에 대부분의 일반적인 단어가 포함된 경우에는 적합한 타협안이 될 수 있습니다.\n\n장점 요약:\n\n- 간단한 방법\n- 각 토큰에 저장된 높은 정보량\n- 주로 일반적인 단어를 포함하는 데이터셋과 잘 작동하는 어휘 크기 제한 가능\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약:\n\n- 비슷한 단어에 대해 별도의 토큰이 생성됩니다 (예: cat과 cats)\n- 매우 큰 어휘를 만들 수 있습니다.\n- 어휘를 제한하면 드문 단어가 많은 데이터셋에서 성능이 크게 저하될 수 있습니다.\n\n## 문자 기반 토크나이저:\n\n문자 기반 토크나이제이션은 글자, 숫자 및 구두점과 같은 특수 문자를 포함하여 텍스트를 각 문자 단위로 분할합니다. 이는 영어 언어를 단어 기반 접근법에서 필요한 17만 개 이상의 어휘 대신 약 256개의 토큰으로 표현할 수 있도록 어휘 크기를 크게 줄입니다 [5]. 중국어 및 일본어와 같은 동아시아 언어도 자신들의 문자 시스템에서 수천 개의 고유 문자를 포함하지만 어휘 크기가 크게 축소될 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문자 기반 토크나이저에서는 다음과 같은 문장을 아래와 같이 변환할 수 있습니다:\n\n[`C`, `a`, `t`, `s`, ` `, `a`, `r`, `e`, ` `, `g`, `r`, `e`, `a`, `t`, `,`, ` `, `b`, `u`, `t`, ` `, `d`, `o`, `g`, `s`, ` `, `a`, `r`, `e`, ` `, `b`, `e`, `t`, `t`, `e`, `r`, `!`]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n캐릭터 기반 토크나이저의 장단점:\n\n단어 기반 방법과 비교할 때, 캐릭터 기반 접근 방식은 훨씬 작은 어휘 크기를 가지며, 많은 수의 OOV(Out-Of-Vocabulary) 토큰을 생성하지 않는다. 심지어 맞춰 쓰인 단어들이 아닌 오타가 있는 단어들조차도 토큰화할 수 있다는 장점이 있습니다(다만 해당 단어의 올바른 형태와는 다르게 토큰화됩니다). 또한, 빈도 기반 어휘 제한 때문에 단어가 즉시 제거되는 것을 방지합니다.\n\n하지만 이 접근 방식에는 몇 가지 단점도 있습니다. 먼저, 캐릭터 기반 방법으로 생성된 단일 토큰에 저장된 정보량은 매우 적습니다. 이는 단어 기반 방식의 토큰과 달리 의미론적이거나 문맥적인 의미가 캡처되지 않기 때문입니다(특히, 알파벳 기반 언어인 영어와 같은 언어에서). 마지막으로 이 방식은 입력 텍스트를 인코딩하기 위해 많은 수의 숫자가 필요하기 때문에, 언어 모델에 투입할 수 있는 토큰화된 입력의 크기에 제약이 생깁니다.\n\n장점 요약:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 어휘 크기가 작음\n- 철자가 틀린 단어를 제거하지 않음\n\n단점 요약:\n\n- 각 토큰에 저장되는 정보량이 적으며, 알파벳 기반의 글쓰기 체계에서는 문맥적 또는 의미적 의미가 거의 없음\n- 언어 모델에 입력되는 크기가 제한되며, 텍스트를 토큰화하는 데 필요한 숫자가 훨씬 더 많아짐 (단어 기반 접근 방식과 비교했을 때)\n\n## Subword-Based Tokenizers:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서브워드 기반 토큰화는 단어 기반 및 문자 기반 방법의 이점을 모두 활용하면서 그들의 단점을 최소화하려는 목표를 가지고 있어요. 서브워드 기반 방법은 단어 내에서 텍스트를 분할하여 의미 있는 토큰을 생성하려는 시도를 통해 중간 지점을 취하고 있어요, 심지어 그것들이 완전한 단어가 아니더라도요. 예를 들어, 토큰 ing와 ed는 문법적인 의미를 가지고 있지만 그 자체로 완전한 단어는 아니에요.\n\n이 방법은 단어 기반 방법보다 작은 어휘 크기를 갖게 하지만, 문자 기반 방법보다 큰 어휘 크기를 갖게 해요. 또한 매 토큰 내에 저장된 정보 양도 두 가지 이전 방법으로 생성된 토큰 사이에 위치하게 되요. 서브워드 접근 방식은 다음 두 지침을 사용해요:\n\n- 자주 사용되는 단어를 서브워드로 분리하지 말고 전체 토큰으로 저장해야 함\n- 드물게 사용되는 단어를 서브워드로 분리해야 함\n\n드물게 사용되는 단어만 분리함으로써 활용어나 복수형 등이 그 구성 요소로 분해되는 기회를 주면서 토큰 사이의 관계를 보존하게 돼요. 예를 들어 cat은 데이터셋에서 매우 흔한 단어지만 cats는 덜 흔할 수 있어요. 이 경우 cats는 cat과 s로 분리되어, cat은 이제 다른 모든 cat 토큰과 동일한 값을 갖게 되고, s는 다른 값을 갖게 됩니다. 이는 복수성의 의미를 인코딩할 수 있다는 것이에요. 또 다른 예시로는 단어 토큰화인데요, 이는 루트 단어 토큰과 접미사 ization으로 분할될 수 있어요. 이 방법은 구문 및 의미 유사성을 보존할 수 있습니다. 이러한 이유로, 서브워드 기반 토큰화기는 현재 많은 NLP 모델에서 널리 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 정규화 및 사전 토크나이제이션\n\n토크나이제이션 과정에서는 사전 처리 및 사후 처리 단계가 필요한데, 이 모든 것이 토크나이제이션 파이프라인을 이룹니다. 이것은 로우 텍스트를 토큰으로 변환하는 데 필요한 일련의 조치들을 설명합니다. 이 파이프라인의 단계는 다음과 같습니다:\n\n- 정규화\n- 사전 토큰화\n- 모델\n- 후 처리\n\n여기서 토큰화 방법(서브워드 기반, 문자 기반 등)은 모델 단계에서 이루어집니다 [7]. 이 섹션에서는 서브워드 기반 토큰화 방식을 사용하는 토크나이저에 대해 각 단계를 다룰 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중요한 알림: 토큰화 파이프라인의 모든 단계는 Hugging Face의 토크나이저 및 트랜스포머 라이브러리와 같은 라이브러리에서 토크나이저를 사용할 때 자동으로 사용자 대신 처리됩니다. 전체 파이프라인은 Tokenizer라는 단일 객체에 의해 수행됩니다. 이 섹션은 대부분의 사용자가 NLP 작업을 수행할 때 직접 처리할 필요가 없는 코드 내부 작업에 대해 다룹니다. 나중에는 토크나이저 라이브러리의 기본 토크나이저 클래스를 사용자 정의하는 단계도 제시되어 필요한 경우 특정 작업용으로 토크나이저를 목적에 맞게 만들 수 있습니다.\n\n## 정규화 방법\n\n정규화는 텍스트를 토큰으로 분할하기 전에 정리하는 과정입니다. 이 과정에는 각 문자를 소문자로 변환하거나 문자에서 강세 기호를 제거하는 단계(예: é가 e가 됨), 불필요한 공백을 제거하는 것 등이 포함됩니다. 예를 들어, 문자열 ThÍs is áN ExaMPlé sÉnteNCE는 정규화 후에는 this is an example sentence가 됩니다. 서로 다른 정규화기는 서로 다른 단계를 수행하며, 사용 사례에 따라 유용할 수 있습니다. 예를 들어, 일부 상황에서는 대소문자나 강세 기호를 유지해야 할 수도 있습니다. 선택한 정규화기에 따라이 단계에서 다양한 효과를 얻을 수 있습니다.\n\nHugging Face의 tokenizers.normalizers 패키지에는 대규모 모델의 일부로서 다양한 토큰화기에서 사용되는 여러 기본 정규화기가 포함되어 있습니다. 아래는 NFC 유니코드, 소문자 및 BERT 정규화기입니다. 이들은 예제 문장에 다음과 같은 효과를 보여줍니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- NFC: 대문자를 변환하지 않거나 악센트를 제거하지 않습니다.\n- Lower: 대문자를 변환하지만 악센트를 제거하지 않습니다.\n- BERT: 대문자를 변환하고 악센트를 제거합니다.\n\n```js\nfrom tokenizers.normalizers import NFC, Lowercase, BertNormalizer\n\n# 정규화할 텍스트\ntext = 'ThÍs is  áN ExaMPlé     sÉnteNCE'\n\n# 정규화 객체 인스턴스화\nNFCNorm = NFC()\nLowercaseNorm = Lowercase()\nBertNorm = BertNormalizer()\n\n# 텍스트 정규화\nprint(f'NFC:   {NFCNorm.normalize_str(text)}')\nprint(f'Lower: {LowercaseNorm.normalize_str(text)}')\nprint(f'BERT:  {BertNorm.normalize_str(text)}')\n```\n\n```js\nNFC:   ThÍs is  áN ExaMPlé     sÉnteNCE\nLower: thís is  án examplé     séntence\nBERT:  this is  an example     sentence\n```\n\n위의 정규화기들은 Hugging Face transformers 라이브러리에서 가져올 수 있는 토크나이저 모델에서 사용됩니다. 아래 코드 셀은 Tokenizer.backend_tokenizer.normalizer를 통해 점 표기법(dot notation)을 사용하여 정규화기에 액세스하는 방법을 보여줍니다. 서로 다른 정규화 방법을 강조하기 위해 일부 비교를 보여줍니다. 이 예시들에서는 FNet 정규화기만 불필요한 공백을 제거합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom transformers import FNetTokenizerFast, CamembertTokenizerFast, \\\n                         BertTokenizerFast\n\n# Text to normalize\ntext = 'ThÍs is  áN ExaMPlé     sÉnteNCE'\n\n# Instantiate tokenizers\nFNetTokenizer = FNetTokenizerFast.from_pretrained('google/fnet-base')\nCamembertTokenizer = CamembertTokenizerFast.from_pretrained('camembert-base')\nBertTokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Normalize the text\nprint(f'FNet Output:      \\\n    {FNetTokenizer.backend_tokenizer.normalizer.normalize_str(text)}')\n\nprint(f'CamemBERT Output: \\\n    {CamembertTokenizer.backend_tokenizer.normalizer.normalize_str(text)}')\n\nprint(f'BERT Output:      \\\n    {BertTokenizer.backend_tokenizer.normalizer.normalize_str(text)}')\n```\n\n```js\nFNet Output:      ThÍs is áN ExaMPlé sÉnteNCE\nCamemBERT Output: ThÍs is  áN ExaMPlé     sÉnteNCE\nBERT Output:      this is  an example     sentence\n```\n\n## Pre-Tokenization Methods\n\nThe pre-tokenization step is the first splitting of the raw text in the tokenization pipeline. The split is performed to give an upper bound to what the final tokens could be at the end of the pipeline. That is, a sentence can be split into words in the pre-tokenization step, then in the model step some of these words may be split further according to the tokenization method (e.g. subword-based). So the pre-tokenized text represents the largest possible tokens that could still remain after tokenization.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정규화와 마찬가지로이 단계를 수행하는 여러 가지 방법이 있습니다. 예를 들어, 문장은 매 공백, 모든 공백 및 일부 구두점 또는 매 공백 및 모든 구두점을 기준으로 분할될 수 있습니다.\n\n아래 셀은 기본 Whitespacesplit 프리 토크나이저와 Hugging Face 토크나이저의 pre_tokenizers 패키지에서 약간 더 복잡한 BertPreTokenizer 간의 비교를 보여줍니다. 공백 프리 토크나이저의 출력은 구두점을 그대로 두고 이웃하는 단어에 여전히 붙어 있는 것을 보여줍니다. 예를 들어, \"includes:\"는 이 경우에는 단일 단어로 처리됩니다. 반면 BERT 프리 토크나이저는 구두점을 개별 단어로 취급합니다 [8].\n\n```js\nfrom tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n\n# 텍스트 정규화\ntext = (\"this sentence's content includes: characters, spaces, and \" \\\n        \"punctuation.\")\n\n# 프리 토큰화된 출력을 표시하는 도우미 함수 정의\ndef print_pretokenized_str(pre_tokens):\n    for pre_token in pre_tokens:\n        print(f'\"{pre_token[0]}\", ', end='')\n\n# 프리 토크나이저 인스턴스화\nwss = WhitespaceSplit()\nbpt = BertPreTokenizer()\n\n# 텍스트를 프리 토큰화\nprint('Whitespace Pre-Tokenizer:')\nprint_pretokenized_str(wss.pre_tokenize_str(text))\n\nprint('\\n\\nBERT Pre-Tokenizer:')\nprint_pretokenized_str(bpt.pre_tokenize_str(text))\n```\n\n```js\nWhitespace Pre-Tokenizer:\n\"this\", \"sentence's\", \"content\", \"includes:\", \"characters,\", \"spaces,\",\n\"and\", \"punctuation.\",\n\nBERT Pre-Tokenizer:\n\"this\", \"sentence\", \"'\", \"s\", \"content\", \"includes\", \":\", \"characters\",\n\",\", \"spaces\", \",\", \"and\", \"punctuation\", \".\",\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정규화 방법들과 마찬가지로 GPT-2와 ALBERT (A Lite BERT) 토크나이저와 같은 일반적인 토크나이저에서 사전 토큰화 방법을 직접 호출할 수 있습니다. 이들은 위에서 보여진 표준 BERT 사전 토큰화 방법과 약간 다른 방식을 사용합니다. 토큰을 분할할 때 공백 문자를 제거하지 않고 특수 문자로 대체합니다. 그 결과, 공백 문자를 처리할 때 무시할 수 있지만 필요할 경우 원래 문장을 검색할 수 있습니다. GPT-2 모델은 Ġ 문자를 사용하며, 이는 위에 점을 찍은 대문자 G가 특징입니다. ALBERT 모델은 밑줄 문자를 사용합니다.\n\n```python\nfrom transformers import AutoTokenizer\n\n# 사전 토큰화할 텍스트\ntext = (\"this sentence's content includes: characters, spaces, and \" \\\n        \"punctuation.\")\n\n# 사전 토큰화 객체 생성\nGPT2_PreTokenizer = AutoTokenizer.from_pretrained('gpt2').backend_tokenizer \\\n                    .pre_tokenizer\n\nAlbert_PreTokenizer = AutoTokenizer.from_pretrained('albert-base-v1') \\\n                      .backend_tokenizer.pre_tokenizer\n\n# 텍스트를 사전 토큰화\nprint('GPT-2 사전 토크나이저:')\nprint_pretokenized_str(GPT2_PreTokenizer.pre_tokenize_str(text))\nprint('\\n\\nALBERT 사전 토크나이저:')\nprint_pretokenized_str(Albert_PreTokenizer.pre_tokenize_str(text))\n```\n\n```python\nGPT-2 사전 토크나이저:\n\"this\", \"Ġsentence\", \"'s\", \"Ġcontent\", \"Ġincludes\", \":\", \"Ġcharacters\", \",\",\n\"Ġspaces\", \",\", \"Ġand\", \"Ġpunctuation\", \".\"\n\nALBERT 사전 토크나이저:\n\"▁this\", \"▁sentence's\", \"▁content\", \"▁includes:\", \"▁characters,\", \"▁spaces,\",\n\"▁and\", \"▁punctuation.\"\n```\n\n위의 예제 문장에 대한 BERT 사전 토큰화 단계 결과를 수정 없이 출력한 내용이 아래에 나와 있습니다. 반환된 객체는 원본 입력 텍스트에서 문자열의 시작 및 끝 색인을 포함하는 파이썬 리스트입니다. 문자열의 시작 색인은 포함되며, 끝 색인은 배타적입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n\n# Pre-tokenizer 인스턴스 생성\ntext = (\"this sentence의 내용은: characters, spaces, 그리고 \" \\\n        \"punctuation이 포함되어 있습니다.\")\nbpt = BertPreTokenizer()\nbpt.pre_tokenize_str(text)\n```\n\n```js\n[\n  (\"this\", (0, 4)),\n  (\"sentence\", (5, 13)),\n  (\"'\", (13, 14)),\n  (\"s\", (14, 15)),\n  (\"content\", (16, 23)),\n  (\"includes\", (24, 32)),\n  (\":\", (32, 33)),\n  (\"characters\", (34, 44)),\n  (\",\", (44, 45)),\n  (\"spaces\", (46, 52)),\n  (\",\", (52, 53)),\n  (\"and\", (54, 57)),\n  (\"punctuation\", (58, 69)),\n  (\".\", (69, 70)),\n];\n```\n\n# 서브워드 토큰화 방법\n\n토큰화 파이프라인의 모델 단계는 토큰화 방법이 사용되는 곳입니다. 이전에 설명한대로 여기서 선택할 수 있는 옵션은: 단어 기반, 문자 기반, 서브워드 기반입니다. 서브워드 기반 방법이 일반적으로 선호되는데, 이 방법들은 단어 기반 및 문자 기반 접근법의 한계를 극복하기 위해 설계되었습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n트랜스포머 모델에는 하위 단어 기반 토큰화를 구현하는 데 일반적으로 사용되는 세 가지 토크나이저 방법이 있습니다. 이 방법들은 다음과 같습니다:\n\n- 바이트 페어 인코딩 (BPE)\n- 워드피스\n- 유니그램\n\n각각의 방법은 빈도가 낮은 단어를 더 작은 토큰으로 분리하기 위해 약간 다른 기술을 사용합니다. BPE 및 워드피스 알고리즘의 구현 방법도 여기에 소개되어 있어서 접근 방식 사이의 유사점과 차이점을 강조하는 데 도움이 될 것입니다.\n\n## 바이트 페어 인코딩\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n바이트 페어 인코딩 알고리즘은 GPT 및 GPT-2 모델 (OpenAI), BART (Lewis et al.) 및 기타 많은 트랜스포머 모델에서 발견되는 일반적으로 사용되는 토크나이저입니다 [9-10]. 이 알고리즘은 원래 텍스트 압축 알고리즘으로 설계되었지만, 언어 모델의 토큰화 작업에 매우 효과적으로 작동한다는 것이 밝혀졌습니다. BPE 알고리즘은 텍스트 문자열을 참조 말뭉치(토큰화 모델을 훈련하는 데 사용되는 텍스트)에서 빈번히 나타나는 부분 단어 단위로 분해합니다 [11]. BPE 모델은 다음과 같이 훈련됩니다:\n\n## 단계 1) 말뭉치 작성\n\n입력 텍스트는 정규화 및 사전 토큰화 모델에 제공되어 깨끗한 단어를 생성합니다. 단어는 BPE 모델에 제공되어 각 단어의 빈도를 결정하고, 이 빈도를 단어와 함께 목록인 말뭉치에 저장합니다.\n\n## 단계 2) 어휘 작성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n말뭉치에서 단어들은 개별 문자로 분해되어 \"어휘(vocabulary)\"라는 비어있는 목록에 추가됩니다. 알고리즘은 어떤 문자 쌍을 함께 병합할 수 있는지를 결정할 때마다 이 어휘에 계속 추가합니다.\n\n단계 3) 문자 쌍의 빈도 찾기\n\n그런 다음, 말뭉치의 각 단어에 대해 문자 쌍의 빈도가 기록됩니다. 예를 들어, 단어 \"cats\"는 문자 쌍 \"ca\", \"at\", \"ts\"를 가집니다. 이와 같은 방식으로 모든 단어가 검사되어 전역 빈도 카운터에 기여합니다. 따라서 토큰 중에서 어떤 ca가 발견되는 경우, ca 쌍에 대한 빈도 카운터가 증가합니다.\n\n단계 4) 병합 규칙 작성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 문자 쌍의 빈도가 알려진 경우, 가장 빈번한 문자 쌍이 어휘에 추가됩니다. 어휘는 이제 토큰 내의 모든 개별 문자와 가장 빈번한 문자 쌍으로 구성됩니다. 또한 모델이 사용할 수있는 병합 규칙이 제공됩니다. 예를 들어 모델이 ca가 가장 빈번한 문자 쌍이라는 것을 학습하면, 모델은 말뭉친 c와 a의 모든 인접 인스턴스를 ca로 병합해 ca를 제공합니다. 이제 이를 나머지 단계의 단일 문자 ca로 취급할 수 있습니다.\n\n단계 5) 단계 3과 4 반복\n\n그런 다음 단계 3과 4를 반복하여 더 많은 병합 규칙을 찾고 어휘에 더 많은 문자 쌍을 추가합니다. 이 프로세스는 교육 시작 시 지정된 대상 크기에 도달 할 때까지 계속됩니다.\n\nBPE 알고리즘이 교육되었으므로 (즉, 모든 병합 규칙이 찾아졌다), 모델은 모든 텍스트를 토큰화하기 위해 모든 단어를 각 문자로 분할하고, 그런 다음 병합 규칙에 따라 병합하여 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 표는 마크다운 형식으로 변경하겠습니다.\n\n아래는 BPE 알고리즘의 Python 구현입니다. 위에서 설명한 단계를 따르고 있습니다. 그 후에는 이 모델을 장난감 데이터세트에서 훈련하고 몇 가지 예제 단어에서 테스트합니다.\n\n```py\nclass TargetVocabularySizeError(Exception):\n    def __init__(self, message):\n        super().__init__(message)\n\nclass BPE:\n    '''Byte Pair Encoding tokenizer의 구현.'''\n\n    def calculate_frequency(self, words):\n        ''' 주어진 단어 목록에서 각 단어의 빈도를 계산합니다.\n\n            문자열로 저장된 단어 목록을 받아서, 각 단어의 빈도를 나타내는 정수를 값을 가진 튜플의 목록을 반환합니다.\n\n            매개변수:\n                words (list): 어떠한 순서로든 단어들(문자열)의 목록입니다.\n\n            반환값:\n                corpus (list[tuple(str, int)]): 단어 목록의 각 단어를 나타내는 첫 번째 요소가 문자열이고,\n                  두 번째 요소가 목록에서 단어의 빈도를 나타내는 정수인 튜플의 목록입니다.\n        '''\n        freq_dict = dict()\n\n        for word in words:\n            if word not in freq_dict:\n                freq_dict[word] = 1\n            else:\n                freq_dict[word] += 1\n\n        corpus = [(word, freq_dict[word]) for word in freq_dict.keys()]\n\n        return corpus\n\n    # 나머지 코드 생략\n```\n\nBPE 알고리즘은 '고양이'에 관한 몇 가지 단어가 포함된 장난감 데이터세트에서 훈련됩니다. 토크나이저의 목표는 데이터세트의 단어의 가장 유용하고 의미 있는 하위 단위를 결정하여 토큰으로 사용하는 것입니다. 검사 결과, 'cat', 'eat', 'ing' 등의 단위가 유용한 하위 단위가 될 것임이 분명합니다.\n\n21개의 대상 어휘 크기로 토크나이저를 실행하면(이는 5회 병합만 필요합니다), 위에서 언급한 모든 원하는 하위 단위를 포착하는 데 충분합니다. 더 큰 데이터세트의 경우 대상 어휘도 훨씬 더 높아지겠지만, 이는 BPE 토크나이저가 얼마나 강력한지를 보여줍니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# Training set\nwords = ['cat', 'cat', 'cat', 'cat', 'cat',\n         'cats', 'cats',\n         'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat', 'eat',\n         'eating', 'eating', 'eating',\n         'running', 'running',\n         'jumping',\n         'food', 'food', 'food', 'food', 'food', 'food']\n\n# Instantiate the tokenizer\nbpe = BPE()\nbpe.train(words, 21)\n\n# Print the corpus at each stage of the process, and the merge rule used\nprint(f'INITIAL CORPUS:\\n{bpe.corpus_history[0]}\\n')\nfor rule, corpus in list(zip(bpe.merge_rules, bpe.corpus_history[1:])):\n    print(f'NEW MERGE RULE: Combine \"{rule[0]}\" and \"{rule[1]}\"')\n    print(corpus, end='\\n\\n')\n```\n\n```js\nINITIAL CORPUS:\n[(['c', 'a', 't'], 5), (['c', 'a', 't', 's'], 2), (['e', 'a', 't'], 10),\n(['e', 'a', 't', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2),\n(['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n\nNEW MERGE RULE: Combine \"a\" and \"t\"\n[(['c', 'at'], 5), (['c', 'at', 's'], 2), (['e', 'at'], 10),\n(['e', 'at', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2),\n(['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n\nNEW MERGE RULE: Combine \"e\" and \"at\"\n[(['c', 'at'], 5), (['c', 'at', 's'], 2), (['eat'], 10),\n(['eat', 'i', 'n', 'g'], 3), (['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2),\n(['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n\nNEW MERGE RULE: Combine \"c\" and \"at\"\n[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'i', 'n', 'g'], 3),\n(['r', 'u', 'n', 'n', 'i', 'n', 'g'], 2),\n(['j', 'u', 'm', 'p', 'i', 'n', 'g'], 1), (['f', 'o', 'o', 'd'], 6)]\n\nNEW MERGE RULE: Combine \"i\" and \"n\"\n[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'in', 'g'], 3),\n(['r', 'u', 'n', 'n', 'in', 'g'], 2), (['j', 'u', 'm', 'p', 'in', 'g'], 1),\n(['f', 'o', 'o', 'd'], 6)]\n\nNEW MERGE RULE: Combine \"in\" and \"g\"\n[(['cat'], 5), (['cat', 's'], 2), (['eat'], 10), (['eat', 'ing'], 3),\n(['r', 'u', 'n', 'n', 'ing'], 2), (['j', 'u', 'm', 'p', 'ing'], 1),\n(['f', 'o', 'o', 'd'], 6)]\n```\n\n크게 작은 데이터셋으로 BPE 알고리즘을 학습했으므로 이제 예제 단어를 토큰화하는 데 사용할 수 있습니다. 아래 셀은 토크나이저가 이전에 본 단어들 및 이전에 보지 못한 단어들을 토큰화하는 데 사용되는 것을 보여줍니다. 토크나이저는 동사 접미사 \"ing\"을 학습했으므로 이를 토큰으로 분리할 수 있습니다. 이 때문에 훈련 데이터에는 'eat'이 포함되어 있어 'eat'이 중요한 토큰임을 학습했습니다. 그러나 모델은 'run'과 'ski'라는 단어를 본 적이 없기 때문에 이를 성공적으로 토큰화하지 못합니다. 이는 토크나이저를 훈련시킬 때 다양하고 광범위한 훈련 세트의 중요성을 강조합니다.\n\n```js\nprint(bpe.tokenize(\"eating\"));\nprint(bpe.tokenize(\"running\"));\nprint(bpe.tokenize(\"skiing\"));\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[\"먹\", \"어\", \"•\", \"ᆼ\"][(\"\", \"ᄂ\", \"ᄂ\", \"•\", \"ᆼ\")][(\"\", \"스\", \"키\", \"•\", \"ᆼ\")];\n```\n\nBPE 토크나이저는 훈련 데이터에 나타난 문자만 인식할 수 있습니다. 예를 들어, 위의 훈련 데이터에는 고양이에 대해 이야기할 때 필요한 문자만 포함되어 있어서 z가 필요하지 않았습니다. 따라서 해당 토크나이저 버전은 z 문자를 어휘에 포함시키지 않으며, 실제 데이터를 토큰화할 때 해당 문자를 알 수 없는 토큰으로 변환합니다 (실제로, 오류 처리가 없어 모델이 알 수 없는 토큰을 생성하도록 지시하는 기능도 없으므로 모델이 충돌할 것이지만, 제품화된 모델에서는 이런 일이 발생할 수 있습니다).\n\nGPT-2 및 RoBERTa에서 사용되는 BPE 토크나이저는 이 문제가 없으며 코드 내에 한 가지 속임수가 있습니다. Unicode 문자를 기반으로 훈련 데이터를 분석하는 대신, 문자의 바이트를 분석합니다. 이를 Byte-Level BPE라고 하며, 소규모 기본 어휘를 사용하여 모델이 볼 수 있는 모든 문자를 토큰화할 수 있게 합니다.\n\n## WordPiece\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWordPiece는 구글이 개발한 토큰화 방법으로, 그들의 중요한 BERT 모델 및 이로부터 파생된 모델들인 DistilBERT 및 MobileBERT에서 사용됩니다.\n\nWordPiece 알고리즘의 전체 세부 내용은 공개되지 않았기 때문에 여기서 제시하는 방법론은 Hugging Face에 의해 제시된 해석을 기반으로 합니다. WordPiece 알고리즘은 BPE와 유사하지만 병합 규칙을 결정하는 데 다른 지표를 사용합니다. 가장 빈도가 높은 문자 쌍을 선택하는 대신 각 쌍에 대해 점수가 계산되고, 가장 높은 점수를 가진 쌍이 병합될 문자를 결정합니다. WordPiece는 다음과 같이 훈련됩니다.\n\n단계 1) 말뭉치 구축\n\n다시 한 번 입력 텍스트는 정규화 및 사전 토큰화 모델에 제공되어 깨끗한 단어를 생성합니다. 단어는 WordPiece 모델에 제공되어 각 단어의 빈도를 결정하고, 이 번호를 단어와 함께 \"말뭉치\"라고 불리는 리스트에 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 2) 어휘 구성\n\nBPE와 같이 코퍼스에서 단어를 개별 문자로 분해한 후, 단어들은 비어 있는 어휘 목록에 추가됩니다. 그러나 이번에는 단순히 각 개별 문자를 저장하는 대신, 두 개의 # 기호가 사용되어 문자가 단어의 시작에서 발견되었는지 또는 단어의 중간/끝에서 발견되었는지를 표시하는 마커로 사용됩니다. 예를 들어, 단어 cat은 BPE에서 [`c`, `a`, `t`]로 분할되지만 WordPiece에서는 [`c`, `##a`, `##t`]로 나타납니다. 이 시스템에서는 단어의 시작에서의 c와 단어의 중간 또는 끝에서의 ##c가 다르게 처리됩니다. 알고리즘은 매번 어떤 문자 쌍을 함께 병합할 수 있는지 결정할 때마다 이 어휘에 추가됩니다.\n\n단계 3) 인접 문자 쌍의 쌍 점수 계산\n\nBPE 모델과 달리, 이번에는 각 문자 쌍에 대해 점수가 계산됩니다. 먼저, 코퍼스에서 각 인접 문자 쌍을 식별하고, `c##a`, ##a##t 등이 계산됩니다. 그리고 빈도가 계산됩니다. 각 문자의 빈도도 결정됩니다. 이러한 값들을 알면, 다음 공식에 따라 쌍 점수를 계산할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n![Tokenization Guide](/assets/img/2024-05-23-TokenizationACompleteGuide_1.png)\n\n이 메트릭은 함께 자주 나타나지만 개별적으로나 다른 문자와 자주 나타나지 않는 문자에 더 높은 점수를 할당합니다. 이것이 WordPiece와 BPE 사이의 주된 차이점인데, BPE는 개별 문자의 전체 빈도를 고려하지 않습니다.\n\n단계 4) 병합 규칙 생성\n\n높은 점수는 자주 함께 나타나는 문자 쌍을 나타냅니다. 즉, c##a가 높은 쌍 점수를 가지면 c와 a가 말뭉치에서 함께 자주 나타나고 개별적으로는 그리 자주 나타나지 않는 것입니다. BPE와 마찬가지로, 병합 규칙은 가장 높은 점수를 가진 문자 쌍에 의해 결정됩니다. 이번에는 빈도가 점수를 결정하는 대신 쌍 점수로 결정됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단계 5) 단계 3과 4를 반복합니다.\n\n그런 다음 단계 3과 4를 반복하여 더 많은 병합 규칙을 찾고 어휘에 더 많은 문자 쌍을 추가합니다. 이 프로세스는 교육의 시작 부분에서 지정된 목표 크기에 도달할 때까지 계속됩니다.\n\n아래는 이전에 작성한 BPE 모델을 상속하는 WordPiece의 구현입니다.\n\n```js\nclass WordPiece(BPE):\n\n    def add_hashes(self, word):\n        ''' 단어의 각 문자에 # 기호 추가\n\n            문자열로 된 단어를 받아서 처음을 제외한 각 문자에 # 기호를 추가합니다.\n            결과를 반환하며 각 요소가 처음 문자만 일반 문자이고 나머지는 # 기호가\n            앞에 붙은 문자인 리스트로 반환합니다.\n\n            인수:\n                word (str): # 기호를 추가할 단어\n\n            반환값:\n                hashed_word (list): # 기호를 추가한 문자의 목록\n        '''\n        hashed_word = [word[0]]\n\n        for char in word[1:]:\n            hashed_word.append(f'##{char}')\n\n        return hashed_word\n\n\n    def create_merge_rule(self, corpus):\n        ''' 병합 규칙을 만들어 self.merge_rules 목록에 추가합니다.\n\n            인수:\n                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자\n                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소 및 단어\n                    빈도를 나타내는 정수를 두 번째 요소로 하는 튜플의 목록\n\n            반환값:\n                없음\n        '''\n        pair_frequencies = self.find_pair_frequencies(corpus)\n        char_frequencies = self.find_char_frequencies(corpus)\n        pair_scores = self.find_pair_scores(pair_frequencies, char_frequencies)\n\n        highest_scoring_pair = max(pair_scores, key=pair_scores.get)\n        self.merge_rules.append(highest_scoring_pair.split(','))\n        self.vocabulary.append(highest_scoring_pair)\n\n\n    def create_vocabulary(self, words):\n        ''' 단어 목록에서 고유 문자 목록을 생성합니다.\n\n            BPE 알고리즘과 달리 각 문자를 일반적으로 저장하는 대신 단어의 시작\n            문자 (표시되지 않음)와 단어의 중간 또는 끝에 있는 문자('##'로 표시)를\n            구분합니다. 예를 들어, 단어 'cat'은 ['c', '##a', '##t']로 분할됩니다.\n\n            인수:\n                words (list): 입력 텍스트의 단어를 포함하는 문자열의 목록\n\n            반환값:\n                vocabulary (list): 입력 단어 목록의 모든 고유 문자 목록\n        '''\n        vocabulary = set()\n        for word in words:\n            vocabulary.add(word[0])\n            for char in word[1:]:\n                vocabulary.add(f'##{char}')\n\n        # 나중에 추가할 수 있도록 목록으로 변환\n        vocabulary = list(vocabulary)\n        return vocabulary\n\n\n    def find_char_frequencies(self, corpus):\n        ''' 코퍼스에서 각 문자의 빈도수를 찾습니다.\n\n            코퍼스를 순환하고 문자의 빈도수를 계산합니다.\n            'c'와 '##c'는 서로 다른 문자임에 유의하세요.\n            'c'는 단어의 시작 문자를 나타내고, '##c'는 단어의 중간 또는 끝을\n            나타냅니다. 각 문자 쌍을 키로, 해당 빈도를 값으로 하는 사전 반환합니다.\n\n            인수:\n                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자\n                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소 및 단어\n                    빈도를 나타내는 정수를 두 번째 요소로 하는 튜플의 목록\n\n            반환값:\n                char_frequencies (dict): 입력 코퍼스의 문자 및 해당 빈도수를\n                    키와 값으로 하는 사전\n        '''\n        char_frequencies = dict()\n\n        for word, word_freq in corpus:\n            for char in word:\n                if char in char_frequencies:\n                    char_frequencies[char] += word_freq\n                else:\n                    char_frequencies[char] = word_freq\n\n        return char_frequencies\n\n\n    def find_pair_scores(self, pair_frequencies, char_frequencies):\n        ''' 코퍼스에서 각 문자 쌍에 대한 쌍 점수를 찾습니다.\n\n            pair_frequencies 사전을 순환하고 코퍼스에서 각 인접 문자 쌍의 쌍\n            점수를 계산합니다. 점수를 사전에 저장하고 반환합니다.\n\n            인수:\n                pair_frequencies (dict): 코퍼스에서 인접 문자 쌍을 키로, 각\n                    쌍 빈도수를 값으로 하는 사전\n\n                char_frequencies (dict): 코퍼스에서 문자를 키로, 해당 빈도수를\n                    값으로 하는 사전\n\n            반환값:\n                pair_scores (dict): 입력 코퍼스의 인접 문자 쌍을 키로, 해당\n                    쌍 점수를 값으로 하는 사전\n        '''\n        pair_scores = dict()\n\n        for pair in pair_frequencies.keys():\n            char_1 = pair.split(',')[0]\n            char_2 = pair.split(',')[1]\n            denominator = (char_frequencies[char_1] * char_frequencies[char_2])\n            score = (pair_frequencies[pair]) / denominator\n            pair_scores[pair] = score\n\n        return pair_scores\n\n\n    def get_merged_chars(self, char_1, char_2):\n        ''' 가장 높은 점수의 쌍을 병합하고 self.merge 메서드에 반환합니다.\n\n            필요에 따라 # 기호를 제거하고 가장 높은 점수의 쌍을 병합한 후\n            병합된 문자를 self.merge 메서드에 반환합니다.\n\n            인수:\n                char_1 (str): 가장 높은 점수의 쌍에서 첫 번째 문자\n                char_2 (str): 가장 높은 점수의 쌍에서 두 번째 문자\n\n            반환값:\n                merged_chars (str): 병합된 문자\n        '''\n        if char_2.startswith('##'):\n            merged_chars = char_1 + char_2[2:]\n        else:\n            merged_chars = char_1 + char_2\n\n        return merged_chars\n\n\n    def initialize_corpus(self, words):\n        ''' 각 단어를 문자로 분할하고 단어 빈도수를 계산합니다.\n\n            입력 단어 목록의 각 단어를 모든 문자로 분할합니다. 각 단어에 대해\n            분할된 단어를 튜플의 첫 번째 요소로 리스트로 저장합니다.\n            단어의 빈도수는 정수로 튜플의 두 번째 요소로 저장합니다.\n            이 작업을 수행한 후 결과인 'corpus' 목록 반환합니다.\n\n            인수:\n                없음\n\n            반환값:\n                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자\n                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소와 단어\n                    목록에서 해당 단어의 빈도수를 나타내는 정수를 표시하는\n                    두 번째 요소로 하는 튜플의 목록\n        '''\n        corpus = self.calculate_frequency(words)\n        corpus = [(self.add_hashes(word), freq) for (word, freq) in corpus]\n        return corpus\n\n    def tokenize(self, text):\n        ''' 텍스트를 토큰 목록으로 만듭니다.\n\n            인수\n```\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nWordPiece 알고리즘은 BPE 알고리즘에 주어진 장난감 데이터세트와 동일한 데이터세트로 아래에서 훈련됩니다. 이번에 학습한 토큰은 매우 다른 것을 알 수 있습니다. WordPiece는 문자가 서로 더 자주 함께 나타나는 경우를 선호하며, 그래서 데이터세트에 함께만 존재하고 홀로 존재하지 않는 'm'과 'p'는 즉시 병합됩니다. 여기서 이 아이디어는 모델이 문자를 병합함으로써 무엇이 손실되는지 고려하도록 강요하는 것입니다. 즉, 이러한 문자들이 항상 함께 있는가요? 그렇다면, 전혀 하나의 단위로 명백하게 병합되어야 합니다. 또는, 코퍼스에서 문자가 매우 빈번한가요? 그렇다면, 문자는 그냥 일반적이며 데이터세트 안에서 풍부하게 나타나므로 다른 토큰 옆에 나타날 것입니다.\n\n```js\nwp = WordPiece()\nwp.train(words, 30)\n\nprint(f'INITIAL CORPUS:\\n{wp.corpus_history[0]}\\n')\nfor rule, corpus in list(zip(wp.merge_rules, wp.corpus_history[1:])):\n    print(f'NEW MERGE RULE: Combine \"{rule[0]}\" and \"{rule[1]}\"')\n    print(corpus, end='\\n\\n')\n```\n\n```js\n초기 코퍼스:\n[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2),\n(['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3),\n(['r', '##u', '##n', '##n', '##i', '##n', '##g'], 2),\n(['j', '##u', '##m', '##p', '##i', '##n', '##g'], 1),\n(['f', '##o', '##o', '##d'], 6)]\n\nNEW MERGE RULE: \"##m\"과 \"##p\" 병합\n[(['c', '##a', '##t'], 5), (['c', '##a', '##t', '##s'], 2),\n(['e', '##a', '##t'], 10), (['e', '##a', '##t', '##i', '##n', '##g'], 3),\n(['r', '##u', '##n', '##n', '##i', '##n', '##g'], 2),\n(['j', '##u', '##mp', '##i', '##n', '##g'], 1),\n(['f', '##o', '##o', '##d'], 6)]\n\n(이하 생략)\n```\n\n이제 WordPiece 알고리즘이 훈련되었으므로(즉, 모든 병합 규칙이 발견되었으므로), 모델은 모든 텍스트를 토큰화하기 위해 각 단어를 모든 문자로 분리한 다음 문자열의 처음부분에 대해 알려진 토큰을 찾을 수 있는 최대 토큰을 찾아서, 나머지 부분은 찾을 수 있는 최대 토큰을 찾는 방식으로 사용할 수 있습니다. 이 과정은 더 이상 훈련 데이터로부터 알려진 토큰과 일치하지 않을 때까지 반복되며, 따라서 문자열의 남은 부분은 최종 토큰으로 취합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n학습 데이터가 제한적이지만, 모델은 여전히 유용한 토큰을 학습했습니다. 그러나 많은 추가 학습 데이터가 필요함을 명백히 알 수 있습니다. 이 토크나이저를 유용하게 만들기 위해 더 많은 학습 데이터가 필요합니다. 예시 문자열에 대한 성능을 테스트할 수 있습니다. 예시로 'jumper' 단어로 시작해보겠습니다. 먼저 문자열은 ['jump', 'er']로 분리됩니다. 왜냐하면 jump는 단어의 시작에서 발견할 수 있는 가장 큰 토큰이기 때문입니다. 다음으로 er 문자열은 각각의 문자 e와 r로 나뉩니다.\n\n```js\nprint(wp.tokenize(\"jumper\"));\n```\n\n```js\n[\"jump\", \"e\", \"r\"];\n```\n\n## 단일 토큰화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUnigram 토크나이저는 BPE와 WordPiece와 다른 방식으로 작동합니다. 큰 어휘로 시작하여 원하는 크기에 도달할 때까지 반복적으로 줄여나갑니다.\n\nUnigram 모델은 각 단어 또는 문자의 확률을 고려하는 통계적 방법을 사용합니다. 예를 들어, \"Cats are great but dogs are better\"라는 문장은 [`Cats`, `are`, `great`, `but`, `dogs`, `are`, `better`] 또는 [`C`, `a`, `t`, `s`, `_a`, `r`, `e`, `_g`,`r`, `e`, `a`, `t`, `_b`, `u`, `t`, `_d`, `o`, `g`, `s` `_a`, `r`, `e`, `_b`, `e`, `t`, `t`, `e`, `r`]로 분할될 수 있습니다. 문장이 문자로 분할된 경우, 새로운 단어의 시작을 나타내기 위해 각 문자의 시작 부분에 밑줄이 추가됩니다.\n\n이러한 목록의 각 요소는 토큰 t로 간주될 수 있으며, t1, t2, ..., tn의 일련의 토큰이 발생할 확률은 다음과 같습니다:\n\n\u003cimg src=\"/assets/img/2024-05-23-TokenizationACompleteGuide_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUnigram 모델은 다음 단계를 통해 훈련됩니다:\n\n단계 1) 코퍼스 구성\n\n언제나처럼 입력 텍스트는 정규화 및 사전 토크나이제이션 모델에 전달되어 깨끗한 단어가 생성됩니다. 그런 다음 단어들은 Unigram 모델에 전달되어 각 단어의 빈도를 결정하고, 이 숫자를 단어와 함께 코퍼스라는 목록에 저장합니다.\n\n단계 2) 어휘 구성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nUnigram 모델의 어휘 크기는 매우 크게 시작되고, 원하는 크기에 도달할 때까지 반복적으로 감소합니다. 초기 어휘를 구성하려면 말뭉치에서 가능한 모든 부분 문자열을 찾습니다. 예를 들어, 말뭉치의 첫 번째 단어가 'cats'인 경우, 부분 문자열 ['c', 'a', 't', 's', 'ca', 'at', 'ts', 'cat', 'ats']이 어휘에 추가됩니다.\n\n3단계) 각 토큰의 확률 계산\n\n토큰의 확률은 말뭉치에서 토큰의 발생 횟수를 찾아 총 토큰 발생 횟수로 나누어 근사적으로 계산됩니다.\n\n![이미지](/assets/img/2024-05-23-TokenizationACompleteGuide_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4단계) 단어의 모든 가능한 세분화 찾기\n\n학습 말뭉치에서 단어가 cat인 경우를 고려해보겠습니다. 이는 다음과 같이 세분화될 수 있습니다:\n\n[`c`, `a`, `t`]\n\n[`ca`, `t`]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[`c`, `at`]\n\n[`cat`]\n\n단계 5) 말뭉치에서 발생 가능한 각 세분화의 근사 확률 계산\n\n위의 방정식들을 결합하면 각 토큰 시리즈에 대한 확률을 얻을 수 있습니다. 예를 들어, 이것은 다음과 같이 보일 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-05-23-TokenizationACompleteGuide_4.png\" /\u003e\n\n가장 높은 확률 점수를 가진 세그먼트 [`c`, `at`]가 사용되어 단어를 토크나이즈했습니다. 따라서 단어 cat은 [`c`, `at`]으로 토큰화됩니다. 단어가 긴 경우 토큰화시 단어 내 여러 곳에서 분할이 발생할 수 있습니다. 예를 들어 [`token`, `iza`, tion] 또는 [`token`, `ization`] 같은 경우도 있을 수 있습니다.\n\n6단계) 손실 계산\n\n손실이란 모델의 점수를 나타내며, 중요한 토큰이 어휘에서 제거되면 손실이 크게 증가하지만 중요하지 않은 토큰이 제거되면 손실은 크게 증가하지 않습니다. 모델에서 각 토큰을 제거했을 때 손실이 얼마나 되는지 계산하여, 어휘 중에서 가장 쓸모없는 토큰을 찾을 수 있습니다. 훈련 세트 말뭉치에서 가장 유용한 토큰만 남도록 어휘 크기가 감소할 때까지 반복적으로 수행할 수 있습니다. 손실은 다음과 같이 주어집니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Tokenization Guide](/assets/img/2024-05-23-TokenizationACompleteGuide_5.png)\n\n필요한 양만큼 문자가 제거되어 어휘를 원하는 크기로 줄일 때, 교육은 완료되고 모델을 사용하여 단어를 토큰화할 수 있습니다.\n\n## BPE, WordPiece 및 Unigram 비교\n\n학습 세트 및 토큰화해야 할 데이터에 따라 어떤 토크나이저가 다른 것보다 더 잘 작동할 수 있습니다. 언어 모델에 대한 토크나이저를 선택할 때, 특정 사용 사례에 사용된 학습 세트를 실험하여 최상의 결과를 얻는 것이 가장 좋을 수 있습니다. 그러나 이 세 가지 토크나이저의 일반적인 경향에 대해 논의하는 것이 유용할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세 가지 중에서 BPE가 현재 언어 모델 토크나이저로 가장 인기 있는 선택인 것으로 보입니다. 그러나 변화가 빠르게 일어나는 이 분야에서는 앞으로 변동이 있을 수 있습니다. 사실, SentencePiece와 같은 다른 서브워드 토크나이저들이 최근에 훨씬 더 인기를 얻고 있습니다.\n\nWordPiece는 BPE와 Unigram에 비해 더 많은 단어 토큰을 생성하는 것으로 보입니다. 그러나 모델 선택과 관계 없이 어휘 크기가 커질수록 모든 토크나이저가 더 적은 토큰을 생성하는 것으로 보입니다.\n\n최종적으로, 토크나이저의 선택은 모델과 함께 사용하려는 데이터셋에 따라 다릅니다. 안전한 선택은 BPE 또는 SentencePiece를 시도하고, 그 이후에 실험하는 것일 수 있습니다.\n\n## 후처리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토큰화 파이프라인의 마지막 단계는 후처리입니다. 여기서 필요한 경우 출력에 최종 수정을 가할 수 있습니다. BERT는 이 단계를 사용하여 두 가지 추가 토큰을 추가하는 데 유명합니다:\n\n- [CLS] - 이 토큰은 `classification`를 나타내며 입력 텍스트의 시작을 표시하는 데 사용됩니다. 이는 BERT에서 필요한 것인데, 이 토큰의 이름에서 알 수 있듯이 이를 사용하여 분류 작업이 수행되었기 때문입니다. 분류 작업에 사용되지 않을 때도 모델에서 여전히 이 토큰을 예상합니다.\n- [SEP] - 이 토큰은 `separation`을 나타내며 입력에서 문장을 분리하는 데 사용됩니다. BERT가 수행하는 많은 작업에 유용하며, 동일한 프롬프트에서 동시에 여러 지시사항을 처리할 때도 사용됩니다.\n\n# Python 라이브러리의 토크나이저\n\nHugging Face는 Python을 포함한 여러 프로그래밍 언어에서 사용할 수 있는 토크나이저 라이브러리를 제공합니다. 이 라이브러리에는 사용자가 사전 훈련된 모델을 사용할 수 있는 일반 Tokenizer 클래스가 포함되어 있으며, 전체 목록은 Hugging Face 웹사이트에서 확인할 수 있습니다. 게다가, 라이브러리에는 사용자가 자체 데이터로 훈련할 수 있는 네 가지 사전 제작되지만 미학습된 모델도 포함되어 있습니다. 이는 특정 유형의 문서에 튜닝된 특정 토크나이저를 작성하는 데 유용합니다. 아래 셀은 Python에서 사전 훈련된 및 미학습된 토크나이저를 사용하는 예시를 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프리트레인 토크나이저 사용하기\n\n토크나이저 라이브러리를 사용하면 프리트레인 토크나이저를 쉽게 사용할 수 있습니다. Tokenizer 클래스를 가져와서 from_pretrained 메소드를 호출하고 사용할 토크나이저의 모델 이름을 전달하면 됩니다. 모델의 목록은 [16]에서 확인할 수 있습니다.\n\n```js\nfrom tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained('bert-base-cased')\n```\n\n토크나이저 학습하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원하는 토큰 만들기되지만 미학습 토크나이저를 사용하려면 tokenizers 라이브러리에서 원하는 모델을 가져와서 모델 클래스의 인스턴스를 만들면 됩니다. 위에서 설명한대로 라이브러리에는 네 가지 모델이 포함되어 있습니다:\n\n- BertWordPieceTokenizer - 유명한 Bert 토크나이저인 WordPiece를 사용합니다.\n- CharBPETokenizer - 원래의 BPE(BPE)\n- ByteLevelBPETokenizer - BPE의 바이트 레벨 버전\n- SentencePieceBPETokenizer - SentencePiece에서 사용하는 BPE 구현과 호환되는 버전\n\n모델을 학습하려면 train 메서드를 사용하고 학습 데이터가 포함된 파일의 경로(또는 파일 경로 목록)를 전달하면 됩니다. 학습을 마치면 모델은 encode 메서드를 사용하여 일부 텍스트를 토큰화하는 데 사용할 수 있습니다. 마지막으로 학습된 토크나이저는 save 메서드를 사용하여 저장할 수 있으므로 학습을 다시 수행할 필요가 없습니다. 아래는 Hugging Face Tokenizers GitHub 페이지에서 제공되는 예제를 수정한 예시 코드입니다 [17].\n\n```js\n# 토크나이저 가져오기\nfrom tokenizers import BertWordPieceTokenizer, CharBPETokenizer, \\\n                       ByteLevelBPETokenizer, SentencePieceBPETokenizer\n\n# 모델 인스턴스화\ntokenizer = CharBPETokenizer()\n\n# 모델 학습\ntokenizer.train(['./path/to/files/1.txt', './path/to/files/2.txt'])\n\n# 텍스트 토큰화\nencoded = tokenizer.encode('I can feel the magic, can you?')\n\n# 모델 저장\ntokenizer.save('./path/to/directory/my-bpe.tokenizer.json')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토크나이저 라이브러리는 이전 섹션에서 보여주었던 것과 같이 처음부터 전체 모델을 구현할 필요 없이 매우 빠르게 사용자 정의 토크나이저를 만들 수 있는 구성 요소도 제공합니다. 아래 셀에는 Hugging Face GitHub 페이지 [17]에서 가져온 예시가 표시되어 있습니다. 해당 예시에서는 토크나이저의 사전 토크나이제이션 및 디코딩 단계를 사용자 정의하는 방법을 보여줍니다. 이 경우, 사전 토크나이제이션 단계에서 접두어 공백이 추가되었고, 디코더로는 ByteLevel 디코더가 선택되었습니다. Hugging Face 문서 [18]에는 사용자 정의 옵션의 전체 목록이 제공됩니다.\n\n```js\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, \\\n                       processors\n\n# 토크나이저 초기화\ntokenizer = Tokenizer(models.BPE())\n\n# 사전 토크나이제이션 및 디코딩 사용자 정의\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\ntokenizer.decoder = decoders.ByteLevel()\ntokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n\n# 그리고 학습\ntrainer = trainers.BpeTrainer(\n    vocab_size=20000,\n    min_frequency=2,\n    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n)\ntokenizer.train([\n    \"./path/to/dataset/1.txt\",\n    \"./path/to/dataset/2.txt\",\n    \"./path/to/dataset/3.txt\"\n], trainer=trainer)\n\n# 그리고 저장\ntokenizer.save(\"byte-level-bpe.tokenizer.json\", pretty=True)\n```\n\n# 결론\n\n토큰화 파이프라인은 언어 모델의 중요한 부분이며, 어떤 종류의 토크나이저를 사용할지 결정할 때 신중한 고려가 필요합니다. 요즘에는 Hugging Face와 같은 라이브러리의 개발자들이 우리를 대신하여 많은 이러한 결정을 내려주고 있습니다. 이를 통해 사용자는 빠르게 사용자 지정 데이터로 언어 모델을 학습하고 사용할 수 있습니다. 그러나 토큰화 방법에 대한 탄탄한 이해는 모델을 미세 조정하고 다양한 데이터셋에서 추가 성능을 얻는 데 귀중합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n[1] 표지 이미지 — Stable Diffusion Web\n\n[2] 토큰 정의 — Stanford NLP 그룹\n\n[3] 단어 토크나이저 — Towards Data Science\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- [4] TransformerXL 논문 — ArXiv\n\n- [5] Tokenizers — Hugging Face\n\n- [6] 단어 기반, 서브워드, 문자 기반 토크나이저 — Towards Data Science\n\n- [7] 토큰화 파이프라인 — Hugging Face\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\\[8\\] Pre-tokenizers — Hugging Face\n\n\\[9\\] Language Models are Unsupervised Multitask Learners — OpenAI\n\n\\[10\\] BART Model for Text Autocompletion in NLP — Geeks for Geeks\n\n\\[11\\] Byte Pair Encoding — Hugging Face\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[12] WordPiece 토큰화 — Hugging Face\n\n[13] 두 분 NLP — 토큰화 방법론의 분류 — Medium\n\n[14] 서브워드 토크나이저 비교 — Vinija AI\n\n[15] BERT가 단어 맥락 관계를 배우는 데 어떻게 Attention 메커니즘과 Transformer를 활용하는가 — Medium\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[16] 사전 훈련된 모델 목록 — Hugging Face\n\n[17] Hugging Face Tokenizers 라이브러리 — GitHub\n\n[18] 사전 토크나이제이션 문서 — Hugging Face\n","ogImage":{"url":"/assets/img/2024-05-23-TokenizationACompleteGuide_0.png"},"coverImage":"/assets/img/2024-05-23-TokenizationACompleteGuide_0.png","tag":["Tech"],"readingTime":36},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h2: \"h2\",\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    img: \"img\",\n    h1: \"h1\",\n    code: \"code\",\n    pre: \"pre\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h2, {\n      children: \"바이트 페어 인코딩, 워드피스 등과 같은 것들과 함께 Python 코드!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"“LLMs from Scratch” 시리즈 중 제1부 — 대형 언어 모델을 이해하고 구축하는 완벽한 안내서입니다. 이 모델이 어떻게 작동하는지 더 자세히 알아보고 싶다면 다음을 읽어보는 것을 권장합니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"프롤로그: 대형 언어 모델의 간단한 역사\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"파트 1: 토크나이제이션 — 완벽한 안내서\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"파트 2: Python에서 워드투벡으로부터 스크래치로 단어 임베딩\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"파트 3: 코드로 설명하는 셀프 어텐션\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"파트 4: 코드로 이해하는 BERT의 완벽한 안내서\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-TokenizationACompleteGuide_0.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"만약 이 콘텐츠가 도움이 되었다면, 아래 방법으로 저를 지원해주십시오:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"기사에 Clap(박수)을 보내세요\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"저를 Medium이나 LinkedIn에서 팔로우하여 향후 게시물에 대한 업데이트를 받으세요\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"서두\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"대형 언어 모델 (LLM)은 2022년 11월 OpenAI의 ChatGPT가 출시된 이후 매우 인기를 얻었습니다. 그 이후로 이러한 언어 모델의 사용이 급증했으며, HuggingFace의 Transformer 라이브러리와 PyTorch와 같은 라이브러리의 도움을 받았습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 모든 이들 준비하고 있는 완제품 도구들로 인해, 기본 수준에서 무슨 일이 일어나고 있는지 추상화하는 것이 쉽습니다. 그 결과로 많은 온라인 튜토리얼들이 당신이 자체 모델을 생성할 때 '무엇'을 알려주고 '왜'는 알려주지 않는 경우가 많습니다. 이 기사 시리즈는 이를 해결하고자 합니다. '처음부터 LLMs 만들기'는 대형 언어 모델을 구성하는 구성 요소를 분해하고, 내부 작동 방식을 설명합니다. 그의 목표는 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"수학의 직관적 이해를 포함한, LLMs가 어떻게 작동하는지의 기본적인 이해 구축\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"각 구성 요소가 어떻게 작동하는지를 보여주며, Python에서 처음부터 구현 방법을 보여줌\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"불필요한 추상화를 줄이기 위해 가급적이면 최소한의 라이브러리 사용\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"말이 다 되었으니, 시작해보겠습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"토크나이저란 무엇인가?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"자연어 처리 문제는 텍스트 데이터를 사용하는데, 기계가 즉시 이해하기 어렵습니다. 컴퓨터가 언어를 처리하려면 먼저 텍스트를 숫자 형식으로 변환해야 합니다. 이 프로세스는 토크나이저라는 모델에 의해 주로 두 단계로 수행됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 1: 입력 텍스트를 토큰으로 분할\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토크나이저는 먼저 텍스트를 가져와 단어, 단어 부분 또는 개별 문자가 될 수 있는 작은 조각으로 나눕니다. 이러한 작은 텍스트 조각을 토큰이라고 합니다. 스탠포드 NLP 그룹은 토큰을 더 엄격하게 정의합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 2: 각 토큰에 식별자 할당\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토크나이저가 텍스트를 토큰으로 분리한 후, 각 토큰에 토큰 ID라고 불리는 정수 번호를 할당할 수 있습니다. 예를 들어, \\\"cat\\\"이라는 단어가 15라는 값으로 할당될 수 있고, 따라서 입력 텍스트의 모든 cat 토큰은 숫자 15로 표시됩니다. 텍스트 토큰을 숫자 표현으로 교체하는 과정을 인코딩이라고 합니다. 비슷하게, 인코딩된 토큰을 다시 텍스트로 변환하는 과정을 디코딩이라고 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단일 숫자를 사용하여 토큰을 표현하는 것에는 단점이 있다는 것을 알 수 있습니다. 그래서 이러한 코드들은 단어 임베딩을 생성하기 위해 추가로 처리되며, 이것은 이 시리즈의 다음 기사의 주제입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"토큰화 방법\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"텍스트를 토큰으로 나누는 세 가지 주요 방법이 있습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"단어 기반\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"문자 기반\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"부분어 기반\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단어 기반 토크나이저:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단어 기반 토크나이제이션은 세 가지 토큰화 방법 중 가장 간단한 방법입니다. 여기서 토크나이저는 문장을 단어로 분할하는데 각 공백 문자를 기준으로 나눕니다(때로는 '화이트스페이스 기반 토큰화'라고도 함) 또는 유사한 규칙 세트(구두점 기반 토큰화, 트리뱅크 토큰화 등)에 따라 분할할 수도 있습니다 [12].\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, 다음과 같은 문장:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"고양이들은 멋지지만, 개들이 더 좋아요!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"띄어쓰기 문자로 분할하면:\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"Cats\"\n      }), \", \", _jsx(_components.code, {\n        children: \"are\"\n      }), \", \", _jsx(_components.code, {\n        children: \"great,\"\n      }), \", \", _jsx(_components.code, {\n        children: \"but\"\n      }), \", \", _jsx(_components.code, {\n        children: \"dogs\"\n      }), \", \", _jsx(_components.code, {\n        children: \"are\"\n      }), \", \", _jsx(_components.code, {\n        children: \"better!\"\n      }), \"]\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또는 구두점과 공백을 기준으로 분할하면:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"Cats\"\n      }), \", \", _jsx(_components.code, {\n        children: \"are\"\n      }), \", \", _jsx(_components.code, {\n        children: \"great\"\n      }), \", \", _jsx(_components.code, {\n        children: \",\"\n      }), \", \", _jsx(_components.code, {\n        children: \"but\"\n      }), \", \", _jsx(_components.code, {\n        children: \"dogs\"\n      }), \", \", _jsx(_components.code, {\n        children: \"are\"\n      }), \", \", _jsx(_components.code, {\n        children: \"better\"\n      }), \", \", _jsx(_components.code, {\n        children: \"!\"\n      }), \"]\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"위 간단한 예제를 통해 분할을 결정하는 데 사용하는 규칙이 중요하다는 것을 분명히 이해할 수 있습니다. 공백 접근 방식은 잠재적으로 희귀한 토큰 \", _jsx(_components.code, {\n        children: \"better!\"\n      }), \"를 제공하며, 두 번째 분할은 덜 희귀한 토큰 \", _jsx(_components.code, {\n        children: \"better\"\n      }), \"와 \", _jsx(_components.code, {\n        children: \"!\"\n      }), \"을 생성합니다. 문장부호를 완전히 제거하지 않도록 주의해야 합니다. 문장부호에는 매우 구체적인 의미가 있을 수 있기 때문입니다. 그 중 하나는 ‘작은따옴표(apostrophe)’입니다. 작은따옴표는 단수와 소유 형태를 구별할 수 있습니다. 예를 들어 “book's”는 책의 속성을 가리키며 “the book's spine is damaged”와 같이 사용되고, “books”는 여러 권의 책을 가리킵니다.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"토큰을 생성한 후, 각 토큰에 번호를 할당할 수 있습니다. 토큰 생성기가 이미 본 토큰을 생성할 때 다음에 볼 토큰은 그 단어에 지정된 번호를 간단히 할당할 수 있습니다. 예를 들어 위 문장에서 \", _jsx(_components.code, {\n        children: \"great\"\n      }), \"가 1이라는 값으로 할당된 경우, 이후의 \", _jsx(_components.code, {\n        children: \"great\"\n      }), \" 단어는 모두 1의 값으로 할당됩니다.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단어 기반 토크나이저의 장단점:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"워드 기반 방법으로 생성된 토큰들은 각각 의미론적 및 문맥 정보를 포함하고 있어 많은 정보를 담고 있습니다. 그러나 이 방법의 가장 큰 단점 중 하나는 매우 유사한 단어가 완전히 다른 토큰으로 처리된다는 것입니다. 예를 들어, cat과 cats 간의 연결은 존재하지 않으며, 이들은 별개의 단어로 처리됩니다. 이는 많은 단어를 포함하는 대규모 응용 프로그램에서 문제가 될 수 있습니다. 모델 어휘의 가능한 토큰 수가 매우 커질 수 있기 때문입니다. 영어는 약 17만 단어가 있으며, 각 단어에 대한 복수형이나 과거형과 같은 다양한 문법 형태를 포함하면 폭발적인 어휘 문제가 발생할 수 있습니다. TransformerXL 토크나이저가 사용하는 공백 기반 분할은 어휘 크기가 25만 개를 초과하도록 이끌었습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 문제를 해결하는 한 가지 방법은 모델이 학습할 수 있는 토큰 수에 하드 리미트를 부여하는 것입니다(예: 1만). 이는 가장 빈도가 높은 1만개의 토큰을 벗어나는 모든 단어를 어휘 외로 처리하고, 숫자 값 대신 UNKNOWN 토큰 값을 할당하는 것입니다(UNK로 축약되기도 합니다). 이는 많은 알려지지 않은 단어가 있는 경우에 성능에 영향을 줄 수 있지만, 데이터에 대부분의 일반적인 단어가 포함된 경우에는 적합한 타협안이 될 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"장점 요약:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"간단한 방법\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"각 토큰에 저장된 높은 정보량\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"주로 일반적인 단어를 포함하는 데이터셋과 잘 작동하는 어휘 크기 제한 가능\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"요약:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"비슷한 단어에 대해 별도의 토큰이 생성됩니다 (예: cat과 cats)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"매우 큰 어휘를 만들 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"어휘를 제한하면 드문 단어가 많은 데이터셋에서 성능이 크게 저하될 수 있습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"문자 기반 토크나이저:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"문자 기반 토크나이제이션은 글자, 숫자 및 구두점과 같은 특수 문자를 포함하여 텍스트를 각 문자 단위로 분할합니다. 이는 영어 언어를 단어 기반 접근법에서 필요한 17만 개 이상의 어휘 대신 약 256개의 토큰으로 표현할 수 있도록 어휘 크기를 크게 줄입니다 [5]. 중국어 및 일본어와 같은 동아시아 언어도 자신들의 문자 시스템에서 수천 개의 고유 문자를 포함하지만 어휘 크기가 크게 축소될 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"문자 기반 토크나이저에서는 다음과 같은 문장을 아래와 같이 변환할 수 있습니다:\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"C\"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"s\"\n      }), \", \", _jsx(_components.code, {\n        children: \" \"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \" \"\n      }), \", \", _jsx(_components.code, {\n        children: \"g\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \",\"\n      }), \", \", _jsx(_components.code, {\n        children: \" \"\n      }), \", \", _jsx(_components.code, {\n        children: \"b\"\n      }), \", \", _jsx(_components.code, {\n        children: \"u\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \" \"\n      }), \", \", _jsx(_components.code, {\n        children: \"d\"\n      }), \", \", _jsx(_components.code, {\n        children: \"o\"\n      }), \", \", _jsx(_components.code, {\n        children: \"g\"\n      }), \", \", _jsx(_components.code, {\n        children: \"s\"\n      }), \", \", _jsx(_components.code, {\n        children: \" \"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \" \"\n      }), \", \", _jsx(_components.code, {\n        children: \"b\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"!\"\n      }), \"]\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"캐릭터 기반 토크나이저의 장단점:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단어 기반 방법과 비교할 때, 캐릭터 기반 접근 방식은 훨씬 작은 어휘 크기를 가지며, 많은 수의 OOV(Out-Of-Vocabulary) 토큰을 생성하지 않는다. 심지어 맞춰 쓰인 단어들이 아닌 오타가 있는 단어들조차도 토큰화할 수 있다는 장점이 있습니다(다만 해당 단어의 올바른 형태와는 다르게 토큰화됩니다). 또한, 빈도 기반 어휘 제한 때문에 단어가 즉시 제거되는 것을 방지합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 이 접근 방식에는 몇 가지 단점도 있습니다. 먼저, 캐릭터 기반 방법으로 생성된 단일 토큰에 저장된 정보량은 매우 적습니다. 이는 단어 기반 방식의 토큰과 달리 의미론적이거나 문맥적인 의미가 캡처되지 않기 때문입니다(특히, 알파벳 기반 언어인 영어와 같은 언어에서). 마지막으로 이 방식은 입력 텍스트를 인코딩하기 위해 많은 수의 숫자가 필요하기 때문에, 언어 모델에 투입할 수 있는 토큰화된 입력의 크기에 제약이 생깁니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"장점 요약:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"어휘 크기가 작음\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"철자가 틀린 단어를 제거하지 않음\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단점 요약:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"각 토큰에 저장되는 정보량이 적으며, 알파벳 기반의 글쓰기 체계에서는 문맥적 또는 의미적 의미가 거의 없음\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"언어 모델에 입력되는 크기가 제한되며, 텍스트를 토큰화하는 데 필요한 숫자가 훨씬 더 많아짐 (단어 기반 접근 방식과 비교했을 때)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Subword-Based Tokenizers:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"서브워드 기반 토큰화는 단어 기반 및 문자 기반 방법의 이점을 모두 활용하면서 그들의 단점을 최소화하려는 목표를 가지고 있어요. 서브워드 기반 방법은 단어 내에서 텍스트를 분할하여 의미 있는 토큰을 생성하려는 시도를 통해 중간 지점을 취하고 있어요, 심지어 그것들이 완전한 단어가 아니더라도요. 예를 들어, 토큰 ing와 ed는 문법적인 의미를 가지고 있지만 그 자체로 완전한 단어는 아니에요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 방법은 단어 기반 방법보다 작은 어휘 크기를 갖게 하지만, 문자 기반 방법보다 큰 어휘 크기를 갖게 해요. 또한 매 토큰 내에 저장된 정보 양도 두 가지 이전 방법으로 생성된 토큰 사이에 위치하게 되요. 서브워드 접근 방식은 다음 두 지침을 사용해요:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"자주 사용되는 단어를 서브워드로 분리하지 말고 전체 토큰으로 저장해야 함\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"드물게 사용되는 단어를 서브워드로 분리해야 함\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"드물게 사용되는 단어만 분리함으로써 활용어나 복수형 등이 그 구성 요소로 분해되는 기회를 주면서 토큰 사이의 관계를 보존하게 돼요. 예를 들어 cat은 데이터셋에서 매우 흔한 단어지만 cats는 덜 흔할 수 있어요. 이 경우 cats는 cat과 s로 분리되어, cat은 이제 다른 모든 cat 토큰과 동일한 값을 갖게 되고, s는 다른 값을 갖게 됩니다. 이는 복수성의 의미를 인코딩할 수 있다는 것이에요. 또 다른 예시로는 단어 토큰화인데요, 이는 루트 단어 토큰과 접미사 ization으로 분할될 수 있어요. 이 방법은 구문 및 의미 유사성을 보존할 수 있습니다. 이러한 이유로, 서브워드 기반 토큰화기는 현재 많은 NLP 모델에서 널리 사용됩니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"정규화 및 사전 토크나이제이션\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토크나이제이션 과정에서는 사전 처리 및 사후 처리 단계가 필요한데, 이 모든 것이 토크나이제이션 파이프라인을 이룹니다. 이것은 로우 텍스트를 토큰으로 변환하는 데 필요한 일련의 조치들을 설명합니다. 이 파이프라인의 단계는 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"정규화\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"사전 토큰화\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"모델\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"후 처리\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"여기서 토큰화 방법(서브워드 기반, 문자 기반 등)은 모델 단계에서 이루어집니다 [7]. 이 섹션에서는 서브워드 기반 토큰화 방식을 사용하는 토크나이저에 대해 각 단계를 다룰 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"중요한 알림: 토큰화 파이프라인의 모든 단계는 Hugging Face의 토크나이저 및 트랜스포머 라이브러리와 같은 라이브러리에서 토크나이저를 사용할 때 자동으로 사용자 대신 처리됩니다. 전체 파이프라인은 Tokenizer라는 단일 객체에 의해 수행됩니다. 이 섹션은 대부분의 사용자가 NLP 작업을 수행할 때 직접 처리할 필요가 없는 코드 내부 작업에 대해 다룹니다. 나중에는 토크나이저 라이브러리의 기본 토크나이저 클래스를 사용자 정의하는 단계도 제시되어 필요한 경우 특정 작업용으로 토크나이저를 목적에 맞게 만들 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"정규화 방법\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"정규화는 텍스트를 토큰으로 분할하기 전에 정리하는 과정입니다. 이 과정에는 각 문자를 소문자로 변환하거나 문자에서 강세 기호를 제거하는 단계(예: é가 e가 됨), 불필요한 공백을 제거하는 것 등이 포함됩니다. 예를 들어, 문자열 ThÍs is áN ExaMPlé sÉnteNCE는 정규화 후에는 this is an example sentence가 됩니다. 서로 다른 정규화기는 서로 다른 단계를 수행하며, 사용 사례에 따라 유용할 수 있습니다. 예를 들어, 일부 상황에서는 대소문자나 강세 기호를 유지해야 할 수도 있습니다. 선택한 정규화기에 따라이 단계에서 다양한 효과를 얻을 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Hugging Face의 tokenizers.normalizers 패키지에는 대규모 모델의 일부로서 다양한 토큰화기에서 사용되는 여러 기본 정규화기가 포함되어 있습니다. 아래는 NFC 유니코드, 소문자 및 BERT 정규화기입니다. 이들은 예제 문장에 다음과 같은 효과를 보여줍니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"NFC: 대문자를 변환하지 않거나 악센트를 제거하지 않습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Lower: 대문자를 변환하지만 악센트를 제거하지 않습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"BERT: 대문자를 변환하고 악센트를 제거합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"normalizers\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NFC\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Lowercase\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertNormalizer\"\n        }), \"\\n\\n# 정규화할 텍스트\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ThÍs is  áN ExaMPlé     sÉnteNCE'\"\n        }), \"\\n\\n# 정규화 객체 인스턴스화\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"NFCNorm\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"NFC\"\n        }), \"()\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"LowercaseNorm\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Lowercase\"\n        }), \"()\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertNorm\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertNormalizer\"\n        }), \"()\\n\\n# 텍스트 정규화\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'NFC:   {NFCNorm.normalize_str(text)}'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Lower: {LowercaseNorm.normalize_str(text)}'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'BERT:  {BertNorm.normalize_str(text)}'\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"NFC\"\n        }), \":   \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Th\"\n        }), \"Ís is  áN \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ExaMPl\"\n        }), \"é     sÉnteNCE\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Lower\"\n        }), \": thís is  án examplé     séntence\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"BERT\"\n        }), \":  \", _jsx(_components.span, {\n          className: \"hljs-variable language_\",\n          children: \"this\"\n        }), \" is  an example     sentence\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 정규화기들은 Hugging Face transformers 라이브러리에서 가져올 수 있는 토크나이저 모델에서 사용됩니다. 아래 코드 셀은 Tokenizer.backend_tokenizer.normalizer를 통해 점 표기법(dot notation)을 사용하여 정규화기에 액세스하는 방법을 보여줍니다. 서로 다른 정규화 방법을 강조하기 위해 일부 비교를 보여줍니다. 이 예시들에서는 FNet 정규화기만 불필요한 공백을 제거합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" transformers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"FNetTokenizerFast\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CamembertTokenizerFast\"\n        }), \", \\\\\\n                         \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertTokenizerFast\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Text\"\n        }), \" to normalize\\ntext = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ThÍs is  áN ExaMPlé     sÉnteNCE'\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Instantiate\"\n        }), \" tokenizers\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"FNetTokenizer\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"FNetTokenizerFast\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'google/fnet-base'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CamembertTokenizer\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CamembertTokenizerFast\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'camembert-base'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertTokenizer\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertTokenizerFast\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'bert-base-uncased'\"\n        }), \")\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Normalize\"\n        }), \" the text\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'FNet Output:      \\\\\\n    {FNetTokenizer.backend_tokenizer.normalizer.normalize_str(text)}'\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'CamemBERT Output: \\\\\\n    {CamembertTokenizer.backend_tokenizer.normalizer.normalize_str(text)}'\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'BERT Output:      \\\\\\n    {BertTokenizer.backend_tokenizer.normalizer.normalize_str(text)}'\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"FNet\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Output\"\n        }), \":      \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Th\"\n        }), \"Ís is áN \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ExaMPl\"\n        }), \"é sÉnteNCE\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CamemBERT\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Output\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Th\"\n        }), \"Ís is  áN \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ExaMPl\"\n        }), \"é     sÉnteNCE\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BERT\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Output\"\n        }), \":      \", _jsx(_components.span, {\n          className: \"hljs-variable language_\",\n          children: \"this\"\n        }), \" is  an example     sentence\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Pre-Tokenization Methods\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The pre-tokenization step is the first splitting of the raw text in the tokenization pipeline. The split is performed to give an upper bound to what the final tokens could be at the end of the pipeline. That is, a sentence can be split into words in the pre-tokenization step, then in the model step some of these words may be split further according to the tokenization method (e.g. subword-based). So the pre-tokenized text represents the largest possible tokens that could still remain after tokenization.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"정규화와 마찬가지로이 단계를 수행하는 여러 가지 방법이 있습니다. 예를 들어, 문장은 매 공백, 모든 공백 및 일부 구두점 또는 매 공백 및 모든 구두점을 기준으로 분할될 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래 셀은 기본 Whitespacesplit 프리 토크나이저와 Hugging Face 토크나이저의 pre_tokenizers 패키지에서 약간 더 복잡한 BertPreTokenizer 간의 비교를 보여줍니다. 공백 프리 토크나이저의 출력은 구두점을 그대로 두고 이웃하는 단어에 여전히 붙어 있는 것을 보여줍니다. 예를 들어, \\\"includes:\\\"는 이 경우에는 단일 단어로 처리됩니다. 반면 BERT 프리 토크나이저는 구두점을 개별 단어로 취급합니다 [8].\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"pre_tokenizers\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"WhitespaceSplit\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertPreTokenizer\"\n        }), \"\\n\\n# 텍스트 정규화\\ntext = (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this sentence's content includes: characters, spaces, and \\\"\"\n        }), \" \\\\\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"punctuation.\\\"\"\n        }), \")\\n\\n# 프리 토큰화된 출력을 표시하는 도우미 함수 정의\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print_pretokenized_str\"\n        }), \"(pre_tokens):\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" pre_token \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"pre_tokens\"\n        }), \":\\n        \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'\\\"{pre_token[0]}\\\", '\"\n        }), \", end=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \")\\n\\n# 프리 토크나이저 인스턴스화\\nwss = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"WhitespaceSplit\"\n        }), \"()\\nbpt = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertPreTokenizer\"\n        }), \"()\\n\\n# 텍스트를 프리 토큰화\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'Whitespace Pre-Tokenizer:'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print_pretokenized_str\"\n        }), \"(wss.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"pre_tokenize_str\"\n        }), \"(text))\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'\\\\n\\\\nBERT Pre-Tokenizer:'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print_pretokenized_str\"\n        }), \"(bpt.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"pre_tokenize_str\"\n        }), \"(text))\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Whitespace\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Pre\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Tokenizer\"\n        }), \":\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"sentence's\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"includes:\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"characters,\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"spaces,\\\"\"\n        }), \",\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"and\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"punctuation.\\\"\"\n        }), \",\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BERT\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Pre\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Tokenizer\"\n        }), \":\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"sentence\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"'\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"s\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"includes\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\":\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"characters\\\"\"\n        }), \",\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\",\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"spaces\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\",\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"and\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"punctuation\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\".\\\"\"\n        }), \",\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"정규화 방법들과 마찬가지로 GPT-2와 ALBERT (A Lite BERT) 토크나이저와 같은 일반적인 토크나이저에서 사전 토큰화 방법을 직접 호출할 수 있습니다. 이들은 위에서 보여진 표준 BERT 사전 토큰화 방법과 약간 다른 방식을 사용합니다. 토큰을 분할할 때 공백 문자를 제거하지 않고 특수 문자로 대체합니다. 그 결과, 공백 문자를 처리할 때 무시할 수 있지만 필요할 경우 원래 문장을 검색할 수 있습니다. GPT-2 모델은 Ġ 문자를 사용하며, 이는 위에 점을 찍은 대문자 G가 특징입니다. ALBERT 모델은 밑줄 문자를 사용합니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" transformers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" AutoTokenizer\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 사전 토큰화할 텍스트\"\n        }), \"\\ntext = (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this sentence's content includes: characters, spaces, and \\\"\"\n        }), \" \\\\\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"punctuation.\\\"\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 사전 토큰화 객체 생성\"\n        }), \"\\nGPT2_PreTokenizer = AutoTokenizer.from_pretrained(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'gpt2'\"\n        }), \").backend_tokenizer \\\\\\n                    .pre_tokenizer\\n\\nAlbert_PreTokenizer = AutoTokenizer.from_pretrained(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'albert-base-v1'\"\n        }), \") \\\\\\n                      .backend_tokenizer.pre_tokenizer\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 텍스트를 사전 토큰화\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'GPT-2 사전 토크나이저:'\"\n        }), \")\\nprint_pretokenized_str(GPT2_PreTokenizer.pre_tokenize_str(text))\\n\", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"print\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'\\\\n\\\\nALBERT 사전 토크나이저:'\"\n        }), \")\\nprint_pretokenized_str(Albert_PreTokenizer.pre_tokenize_str(text))\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [\"GPT-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \" 사전 토크나이저:\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġsentence\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"'s\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġcontent\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġincludes\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\":\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġcharacters\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\",\\\"\"\n        }), \",\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġspaces\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\",\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġand\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Ġpunctuation\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\".\\\"\"\n        }), \"\\n\\nALBERT 사전 토크나이저:\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁this\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁sentence's\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁content\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁includes:\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁characters,\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁spaces,\\\"\"\n        }), \",\\n\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁and\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"▁punctuation.\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 예제 문장에 대한 BERT 사전 토큰화 단계 결과를 수정 없이 출력한 내용이 아래에 나와 있습니다. 반환된 객체는 원본 입력 텍스트에서 문자열의 시작 및 끝 색인을 포함하는 파이썬 리스트입니다. 문자열의 시작 색인은 포함되며, 끝 색인은 배타적입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"pre_tokenizers\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"WhitespaceSplit\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertPreTokenizer\"\n        }), \"\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Pre\"\n        }), \"-tokenizer 인스턴스 생성\\ntext = (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this sentence의 내용은: characters, spaces, 그리고 \\\"\"\n        }), \" \\\\\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"punctuation이 포함되어 있습니다.\\\"\"\n        }), \")\\nbpt = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertPreTokenizer\"\n        }), \"()\\nbpt.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"pre_tokenize_str\"\n        }), \"(text)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"[\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"this\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"sentence\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"13\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"'\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"13\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"14\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"s\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"14\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"15\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"16\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"23\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"includes\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"24\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"32\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\":\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"32\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"33\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"characters\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"34\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"44\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\",\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"44\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"45\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"spaces\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"46\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"52\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\",\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"52\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"53\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"and\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"54\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"57\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"punctuation\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"58\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"69\"\n        }), \")),\\n  (\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\".\\\"\"\n        }), \", (\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"69\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"70\"\n        }), \")),\\n];\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"서브워드 토큰화 방법\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토큰화 파이프라인의 모델 단계는 토큰화 방법이 사용되는 곳입니다. 이전에 설명한대로 여기서 선택할 수 있는 옵션은: 단어 기반, 문자 기반, 서브워드 기반입니다. 서브워드 기반 방법이 일반적으로 선호되는데, 이 방법들은 단어 기반 및 문자 기반 접근법의 한계를 극복하기 위해 설계되었습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"트랜스포머 모델에는 하위 단어 기반 토큰화를 구현하는 데 일반적으로 사용되는 세 가지 토크나이저 방법이 있습니다. 이 방법들은 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"바이트 페어 인코딩 (BPE)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"워드피스\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"유니그램\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"각각의 방법은 빈도가 낮은 단어를 더 작은 토큰으로 분리하기 위해 약간 다른 기술을 사용합니다. BPE 및 워드피스 알고리즘의 구현 방법도 여기에 소개되어 있어서 접근 방식 사이의 유사점과 차이점을 강조하는 데 도움이 될 것입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"바이트 페어 인코딩\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"바이트 페어 인코딩 알고리즘은 GPT 및 GPT-2 모델 (OpenAI), BART (Lewis et al.) 및 기타 많은 트랜스포머 모델에서 발견되는 일반적으로 사용되는 토크나이저입니다 [9-10]. 이 알고리즘은 원래 텍스트 압축 알고리즘으로 설계되었지만, 언어 모델의 토큰화 작업에 매우 효과적으로 작동한다는 것이 밝혀졌습니다. BPE 알고리즘은 텍스트 문자열을 참조 말뭉치(토큰화 모델을 훈련하는 데 사용되는 텍스트)에서 빈번히 나타나는 부분 단어 단위로 분해합니다 [11]. BPE 모델은 다음과 같이 훈련됩니다:\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단계 1) 말뭉치 작성\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"입력 텍스트는 정규화 및 사전 토큰화 모델에 제공되어 깨끗한 단어를 생성합니다. 단어는 BPE 모델에 제공되어 각 단어의 빈도를 결정하고, 이 빈도를 단어와 함께 목록인 말뭉치에 저장합니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단계 2) 어휘 작성\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"말뭉치에서 단어들은 개별 문자로 분해되어 \\\"어휘(vocabulary)\\\"라는 비어있는 목록에 추가됩니다. 알고리즘은 어떤 문자 쌍을 함께 병합할 수 있는지를 결정할 때마다 이 어휘에 계속 추가합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 3) 문자 쌍의 빈도 찾기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그런 다음, 말뭉치의 각 단어에 대해 문자 쌍의 빈도가 기록됩니다. 예를 들어, 단어 \\\"cats\\\"는 문자 쌍 \\\"ca\\\", \\\"at\\\", \\\"ts\\\"를 가집니다. 이와 같은 방식으로 모든 단어가 검사되어 전역 빈도 카운터에 기여합니다. 따라서 토큰 중에서 어떤 ca가 발견되는 경우, ca 쌍에 대한 빈도 카운터가 증가합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 4) 병합 규칙 작성\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"각 문자 쌍의 빈도가 알려진 경우, 가장 빈번한 문자 쌍이 어휘에 추가됩니다. 어휘는 이제 토큰 내의 모든 개별 문자와 가장 빈번한 문자 쌍으로 구성됩니다. 또한 모델이 사용할 수있는 병합 규칙이 제공됩니다. 예를 들어 모델이 ca가 가장 빈번한 문자 쌍이라는 것을 학습하면, 모델은 말뭉친 c와 a의 모든 인접 인스턴스를 ca로 병합해 ca를 제공합니다. 이제 이를 나머지 단계의 단일 문자 ca로 취급할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 5) 단계 3과 4 반복\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그런 다음 단계 3과 4를 반복하여 더 많은 병합 규칙을 찾고 어휘에 더 많은 문자 쌍을 추가합니다. 이 프로세스는 교육 시작 시 지정된 대상 크기에 도달 할 때까지 계속됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"BPE 알고리즘이 교육되었으므로 (즉, 모든 병합 규칙이 찾아졌다), 모델은 모든 텍스트를 토큰화하기 위해 모든 단어를 각 문자로 분할하고, 그런 다음 병합 규칙에 따라 병합하여 사용할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 표는 마크다운 형식으로 변경하겠습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래는 BPE 알고리즘의 Python 구현입니다. 위에서 설명한 단계를 따르고 있습니다. 그 후에는 이 모델을 장난감 데이터세트에서 훈련하고 몇 가지 예제 단어에서 테스트합니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-py\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"class\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"TargetVocabularySizeError\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title class_ inherited__\",\n          children: \"Exception\"\n        }), \"):\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"def\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"__init__\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-params\",\n          children: \"self, message\"\n        }), \"):\\n        \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"super\"\n        }), \"().__init__(message)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"class\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BPE\"\n        }), \":\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'''Byte Pair Encoding tokenizer의 구현.'''\"\n        }), \"\\n\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"def\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"calculate_frequency\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-params\",\n          children: \"self, words\"\n        }), \"):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''' 주어진 단어 목록에서 각 단어의 빈도를 계산합니다.\\n\\n            문자열로 저장된 단어 목록을 받아서, 각 단어의 빈도를 나타내는 정수를 값을 가진 튜플의 목록을 반환합니다.\\n\\n            매개변수:\\n                words (list): 어떠한 순서로든 단어들(문자열)의 목록입니다.\\n\\n            반환값:\\n                corpus (list[tuple(str, int)]): 단어 목록의 각 단어를 나타내는 첫 번째 요소가 문자열이고,\\n                  두 번째 요소가 목록에서 단어의 빈도를 나타내는 정수인 튜플의 목록입니다.\\n        '''\"\n        }), \"\\n        freq_dict = \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"dict\"\n        }), \"()\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" word \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" words:\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" word \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"not\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" freq_dict:\\n                freq_dict[word] = \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"else\"\n        }), \":\\n                freq_dict[word] += \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"\\n\\n        corpus = [(word, freq_dict[word]) \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" word \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" freq_dict.keys()]\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" corpus\\n\\n    \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 나머지 코드 생략\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"BPE 알고리즘은 '고양이'에 관한 몇 가지 단어가 포함된 장난감 데이터세트에서 훈련됩니다. 토크나이저의 목표는 데이터세트의 단어의 가장 유용하고 의미 있는 하위 단위를 결정하여 토큰으로 사용하는 것입니다. 검사 결과, 'cat', 'eat', 'ing' 등의 단위가 유용한 하위 단위가 될 것임이 분명합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"21개의 대상 어휘 크기로 토크나이저를 실행하면(이는 5회 병합만 필요합니다), 위에서 언급한 모든 원하는 하위 단위를 포착하는 데 충분합니다. 더 큰 데이터세트의 경우 대상 어휘도 훨씬 더 높아지겠지만, 이는 BPE 토크나이저가 얼마나 강력한지를 보여줍니다.```\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Training\"\n        }), \" set\\nwords = [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \",\\n         \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cats'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cats'\"\n        }), \",\\n         \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \",\\n         \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eating'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eating'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eating'\"\n        }), \",\\n         \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'running'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'running'\"\n        }), \",\\n         \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'jumping'\"\n        }), \",\\n         \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'food'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'food'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'food'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'food'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'food'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'food'\"\n        }), \"]\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Instantiate\"\n        }), \" the tokenizer\\nbpe = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"BPE\"\n        }), \"()\\nbpe.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(words, \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"21\"\n        }), \")\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Print\"\n        }), \" the corpus at each stage \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"of\"\n        }), \" the process, and the merge rule used\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'INITIAL CORPUS:\\\\n{bpe.corpus_history[0]}\\\\n'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" rule, corpus \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"list\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"zip\"\n        }), \"(bpe.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"merge_rules\"\n        }), \", bpe.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"corpus_history\"\n        }), \"[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":])):\\n    \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'NEW MERGE RULE: Combine \\\"{rule[0]}\\\" and \\\"{rule[1]}\\\"'\"\n        }), \")\\n    \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(corpus, end=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'\\\\n\\\\n'\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"INITIAL\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"CORPUS\"\n        }), \":\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'t'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'t'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'t'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'t'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NEW\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MERGE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"RULE\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Combine\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"a\\\"\"\n        }), \" and \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"t\\\"\"\n        }), \"\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'at'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'at'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'at'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'at'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NEW\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MERGE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"RULE\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Combine\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"e\\\"\"\n        }), \" and \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"at\\\"\"\n        }), \"\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'at'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'at'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NEW\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MERGE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"RULE\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Combine\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"c\\\"\"\n        }), \" and \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"at\\\"\"\n        }), \"\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NEW\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MERGE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"RULE\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Combine\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"i\\\"\"\n        }), \" and \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"n\\\"\"\n        }), \"\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'in'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'in'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'in'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NEW\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MERGE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"RULE\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Combine\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"in\\\"\"\n        }), \" and \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"g\\\"\"\n        }), \"\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'cat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'eat'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ing'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ing'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'ing'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"크게 작은 데이터셋으로 BPE 알고리즘을 학습했으므로 이제 예제 단어를 토큰화하는 데 사용할 수 있습니다. 아래 셀은 토크나이저가 이전에 본 단어들 및 이전에 보지 못한 단어들을 토큰화하는 데 사용되는 것을 보여줍니다. 토크나이저는 동사 접미사 \\\"ing\\\"을 학습했으므로 이를 토큰으로 분리할 수 있습니다. 이 때문에 훈련 데이터에는 'eat'이 포함되어 있어 'eat'이 중요한 토큰임을 학습했습니다. 그러나 모델은 'run'과 'ski'라는 단어를 본 적이 없기 때문에 이를 성공적으로 토큰화하지 못합니다. 이는 토크나이저를 훈련시킬 때 다양하고 광범위한 훈련 세트의 중요성을 강조합니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(bpe.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"tokenize\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"eating\\\"\"\n        }), \"));\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(bpe.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"tokenize\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"running\\\"\"\n        }), \"));\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(bpe.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"tokenize\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"skiing\\\"\"\n        }), \"));\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"먹\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"어\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"•\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ᆼ\\\"\"\n        }), \"][(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ᄂ\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ᄂ\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"•\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ᆼ\\\"\"\n        }), \")][(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"스\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"키\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"•\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"ᆼ\\\"\"\n        }), \")];\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"BPE 토크나이저는 훈련 데이터에 나타난 문자만 인식할 수 있습니다. 예를 들어, 위의 훈련 데이터에는 고양이에 대해 이야기할 때 필요한 문자만 포함되어 있어서 z가 필요하지 않았습니다. 따라서 해당 토크나이저 버전은 z 문자를 어휘에 포함시키지 않으며, 실제 데이터를 토큰화할 때 해당 문자를 알 수 없는 토큰으로 변환합니다 (실제로, 오류 처리가 없어 모델이 알 수 없는 토큰을 생성하도록 지시하는 기능도 없으므로 모델이 충돌할 것이지만, 제품화된 모델에서는 이런 일이 발생할 수 있습니다).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"GPT-2 및 RoBERTa에서 사용되는 BPE 토크나이저는 이 문제가 없으며 코드 내에 한 가지 속임수가 있습니다. Unicode 문자를 기반으로 훈련 데이터를 분석하는 대신, 문자의 바이트를 분석합니다. 이를 Byte-Level BPE라고 하며, 소규모 기본 어휘를 사용하여 모델이 볼 수 있는 모든 문자를 토큰화할 수 있게 합니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"WordPiece\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"WordPiece는 구글이 개발한 토큰화 방법으로, 그들의 중요한 BERT 모델 및 이로부터 파생된 모델들인 DistilBERT 및 MobileBERT에서 사용됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"WordPiece 알고리즘의 전체 세부 내용은 공개되지 않았기 때문에 여기서 제시하는 방법론은 Hugging Face에 의해 제시된 해석을 기반으로 합니다. WordPiece 알고리즘은 BPE와 유사하지만 병합 규칙을 결정하는 데 다른 지표를 사용합니다. 가장 빈도가 높은 문자 쌍을 선택하는 대신 각 쌍에 대해 점수가 계산되고, 가장 높은 점수를 가진 쌍이 병합될 문자를 결정합니다. WordPiece는 다음과 같이 훈련됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 1) 말뭉치 구축\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다시 한 번 입력 텍스트는 정규화 및 사전 토큰화 모델에 제공되어 깨끗한 단어를 생성합니다. 단어는 WordPiece 모델에 제공되어 각 단어의 빈도를 결정하고, 이 번호를 단어와 함께 \\\"말뭉치\\\"라고 불리는 리스트에 저장합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 2) 어휘 구성\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"BPE와 같이 코퍼스에서 단어를 개별 문자로 분해한 후, 단어들은 비어 있는 어휘 목록에 추가됩니다. 그러나 이번에는 단순히 각 개별 문자를 저장하는 대신, 두 개의 # 기호가 사용되어 문자가 단어의 시작에서 발견되었는지 또는 단어의 중간/끝에서 발견되었는지를 표시하는 마커로 사용됩니다. 예를 들어, 단어 cat은 BPE에서 [\", _jsx(_components.code, {\n        children: \"c\"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \"]로 분할되지만 WordPiece에서는 [\", _jsx(_components.code, {\n        children: \"c\"\n      }), \", \", _jsx(_components.code, {\n        children: \"##a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"##t\"\n      }), \"]로 나타납니다. 이 시스템에서는 단어의 시작에서의 c와 단어의 중간 또는 끝에서의 ##c가 다르게 처리됩니다. 알고리즘은 매번 어떤 문자 쌍을 함께 병합할 수 있는지 결정할 때마다 이 어휘에 추가됩니다.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 3) 인접 문자 쌍의 쌍 점수 계산\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"BPE 모델과 달리, 이번에는 각 문자 쌍에 대해 점수가 계산됩니다. 먼저, 코퍼스에서 각 인접 문자 쌍을 식별하고, \", _jsx(_components.code, {\n        children: \"c##a\"\n      }), \", ##a##t 등이 계산됩니다. 그리고 빈도가 계산됩니다. 각 문자의 빈도도 결정됩니다. 이러한 값들을 알면, 다음 공식에 따라 쌍 점수를 계산할 수 있습니다:\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-TokenizationACompleteGuide_1.png\",\n        alt: \"Tokenization Guide\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 메트릭은 함께 자주 나타나지만 개별적으로나 다른 문자와 자주 나타나지 않는 문자에 더 높은 점수를 할당합니다. 이것이 WordPiece와 BPE 사이의 주된 차이점인데, BPE는 개별 문자의 전체 빈도를 고려하지 않습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 4) 병합 규칙 생성\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"높은 점수는 자주 함께 나타나는 문자 쌍을 나타냅니다. 즉, c##a가 높은 쌍 점수를 가지면 c와 a가 말뭉치에서 함께 자주 나타나고 개별적으로는 그리 자주 나타나지 않는 것입니다. BPE와 마찬가지로, 병합 규칙은 가장 높은 점수를 가진 문자 쌍에 의해 결정됩니다. 이번에는 빈도가 점수를 결정하는 대신 쌍 점수로 결정됩니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단계 5) 단계 3과 4를 반복합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그런 다음 단계 3과 4를 반복하여 더 많은 병합 규칙을 찾고 어휘에 더 많은 문자 쌍을 추가합니다. 이 프로세스는 교육의 시작 부분에서 지정된 목표 크기에 도달할 때까지 계속됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래는 이전에 작성한 BPE 모델을 상속하는 WordPiece의 구현입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"class\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"WordPiece\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BPE\"\n        }), \"):\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"add_hashes\"\n        }), \"(self, word):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 단어의 각 문자에 # 기호 추가\\n\\n            문자열로 된 단어를 받아서 처음을 제외한 각 문자에 # 기호를 추가합니다.\\n            결과를 반환하며 각 요소가 처음 문자만 일반 문자이고 나머지는 # 기호가\\n            앞에 붙은 문자인 리스트로 반환합니다.\\n\\n            인수:\\n                word (str): # 기호를 추가할 단어\\n\\n            반환값:\\n                hashed_word (list): # 기호를 추가한 문자의 목록\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        hashed_word = [word[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \"]]\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" char \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" word[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":]:\\n            hashed_word.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"append\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##{char}'\"\n        }), \")\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" hashed_word\\n\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"create_merge_rule\"\n        }), \"(self, corpus):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 병합 규칙을 만들어 self.merge_rules 목록에 추가합니다.\\n\\n            인수:\\n                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자\\n                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소 및 단어\\n                    빈도를 나타내는 정수를 두 번째 요소로 하는 튜플의 목록\\n\\n            반환값:\\n                없음\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        pair_frequencies = self.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"find_pair_frequencies\"\n        }), \"(corpus)\\n        char_frequencies = self.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"find_char_frequencies\"\n        }), \"(corpus)\\n        pair_scores = self.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"find_pair_scores\"\n        }), \"(pair_frequencies, char_frequencies)\\n\\n        highest_scoring_pair = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"max\"\n        }), \"(pair_scores, key=pair_scores.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"get\"\n        }), \")\\n        self.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"merge_rules\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"append\"\n        }), \"(highest_scoring_pair.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"','\"\n        }), \"))\\n        self.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"vocabulary\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"append\"\n        }), \"(highest_scoring_pair)\\n\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"create_vocabulary\"\n        }), \"(self, words):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 단어 목록에서 고유 문자 목록을 생성합니다.\\n\\n            BPE 알고리즘과 달리 각 문자를 일반적으로 저장하는 대신 단어의 시작\\n            문자 (표시되지 않음)와 단어의 중간 또는 끝에 있는 문자('\"\n        }), \"##\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'로 표시)를\\n            구분합니다. 예를 들어, 단어 '\"\n        }), \"cat\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'은 ['\"\n        }), \"c\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"', '\"\n        }), \"##a\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"', '\"\n        }), \"##t\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"']로 분할됩니다.\\n\\n            인수:\\n                words (list): 입력 텍스트의 단어를 포함하는 문자열의 목록\\n\\n            반환값:\\n                vocabulary (list): 입력 단어 목록의 모든 고유 문자 목록\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        vocabulary = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"set\"\n        }), \"()\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" word \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"words\"\n        }), \":\\n            vocabulary.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"add\"\n        }), \"(word[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \"])\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" char \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" word[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":]:\\n                vocabulary.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"add\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##{char}'\"\n        }), \")\\n\\n        # 나중에 추가할 수 있도록 목록으로 변환\\n        vocabulary = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"list\"\n        }), \"(vocabulary)\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" vocabulary\\n\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"find_char_frequencies\"\n        }), \"(self, corpus):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 코퍼스에서 각 문자의 빈도수를 찾습니다.\\n\\n            코퍼스를 순환하고 문자의 빈도수를 계산합니다.\\n            '\"\n        }), \"c\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'와 '\"\n        }), \"##c\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'는 서로 다른 문자임에 유의하세요.\\n            '\"\n        }), \"c\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'는 단어의 시작 문자를 나타내고, '\"\n        }), \"##c\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'는 단어의 중간 또는 끝을\\n            나타냅니다. 각 문자 쌍을 키로, 해당 빈도를 값으로 하는 사전 반환합니다.\\n\\n            인수:\\n                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자\\n                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소 및 단어\\n                    빈도를 나타내는 정수를 두 번째 요소로 하는 튜플의 목록\\n\\n            반환값:\\n                char_frequencies (dict): 입력 코퍼스의 문자 및 해당 빈도수를\\n                    키와 값으로 하는 사전\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        char_frequencies = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"dict\"\n        }), \"()\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" word, word_freq \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"corpus\"\n        }), \":\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" char \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"word\"\n        }), \":\\n                \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" char \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"char_frequencies\"\n        }), \":\\n                    char_frequencies[char] += word_freq\\n                \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"else\"\n        }), \":\\n                    char_frequencies[char] = word_freq\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" char_frequencies\\n\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"find_pair_scores\"\n        }), \"(self, pair_frequencies, char_frequencies):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 코퍼스에서 각 문자 쌍에 대한 쌍 점수를 찾습니다.\\n\\n            pair_frequencies 사전을 순환하고 코퍼스에서 각 인접 문자 쌍의 쌍\\n            점수를 계산합니다. 점수를 사전에 저장하고 반환합니다.\\n\\n            인수:\\n                pair_frequencies (dict): 코퍼스에서 인접 문자 쌍을 키로, 각\\n                    쌍 빈도수를 값으로 하는 사전\\n\\n                char_frequencies (dict): 코퍼스에서 문자를 키로, 해당 빈도수를\\n                    값으로 하는 사전\\n\\n            반환값:\\n                pair_scores (dict): 입력 코퍼스의 인접 문자 쌍을 키로, 해당\\n                    쌍 점수를 값으로 하는 사전\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        pair_scores = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"dict\"\n        }), \"()\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" pair \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" pair_frequencies.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"keys\"\n        }), \"():\\n            char_1 = pair.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"','\"\n        }), \")[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \"]\\n            char_2 = pair.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"split\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"','\"\n        }), \")[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"]\\n            denominator = (char_frequencies[char_1] * char_frequencies[char_2])\\n            score = (pair_frequencies[pair]) / denominator\\n            pair_scores[pair] = score\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" pair_scores\\n\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"get_merged_chars\"\n        }), \"(self, char_1, char_2):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 가장 높은 점수의 쌍을 병합하고 self.merge 메서드에 반환합니다.\\n\\n            필요에 따라 # 기호를 제거하고 가장 높은 점수의 쌍을 병합한 후\\n            병합된 문자를 self.merge 메서드에 반환합니다.\\n\\n            인수:\\n                char_1 (str): 가장 높은 점수의 쌍에서 첫 번째 문자\\n                char_2 (str): 가장 높은 점수의 쌍에서 두 번째 문자\\n\\n            반환값:\\n                merged_chars (str): 병합된 문자\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" char_2.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"startswith\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##'\"\n        }), \"):\\n            merged_chars = char_1 + char_2[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \":]\\n        \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"else\"\n        }), \":\\n            merged_chars = char_1 + char_2\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" merged_chars\\n\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"initialize_corpus\"\n        }), \"(self, words):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 각 단어를 문자로 분할하고 단어 빈도수를 계산합니다.\\n\\n            입력 단어 목록의 각 단어를 모든 문자로 분할합니다. 각 단어에 대해\\n            분할된 단어를 튜플의 첫 번째 요소로 리스트로 저장합니다.\\n            단어의 빈도수는 정수로 튜플의 두 번째 요소로 저장합니다.\\n            이 작업을 수행한 후 결과인 '\"\n        }), \"corpus\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 목록 반환합니다.\\n\\n            인수:\\n                없음\\n\\n            반환값:\\n                corpus (list[tuple(list, int)]): 단어 목록에서 단어의 개별 문자\\n                    (또는 나중 반복에서 단어의 하위단어)를 표현하는 요소와 단어\\n                    목록에서 해당 단어의 빈도수를 나타내는 정수를 표시하는\\n                    두 번째 요소로 하는 튜플의 목록\\n        '\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), \"\\n        corpus = self.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"calculate_frequency\"\n        }), \"(words)\\n        corpus = [(self.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"add_hashes\"\n        }), \"(word), freq) \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" (word, freq) \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" corpus]\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" corpus\\n\\n    def \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"tokenize\"\n        }), \"(self, text):\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"''\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"' 텍스트를 토큰 목록으로 만듭니다.\\n\\n            인수\\n\"\n        })]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"WordPiece 알고리즘은 BPE 알고리즘에 주어진 장난감 데이터세트와 동일한 데이터세트로 아래에서 훈련됩니다. 이번에 학습한 토큰은 매우 다른 것을 알 수 있습니다. WordPiece는 문자가 서로 더 자주 함께 나타나는 경우를 선호하며, 그래서 데이터세트에 함께만 존재하고 홀로 존재하지 않는 'm'과 'p'는 즉시 병합됩니다. 여기서 이 아이디어는 모델이 문자를 병합함으로써 무엇이 손실되는지 고려하도록 강요하는 것입니다. 즉, 이러한 문자들이 항상 함께 있는가요? 그렇다면, 전혀 하나의 단위로 명백하게 병합되어야 합니다. 또는, 코퍼스에서 문자가 매우 빈번한가요? 그렇다면, 문자는 그냥 일반적이며 데이터세트 안에서 풍부하게 나타나므로 다른 토큰 옆에 나타날 것입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"wp = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"WordPiece\"\n        }), \"()\\nwp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"(words, \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"30\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'INITIAL CORPUS:\\\\n{wp.corpus_history[0]}\\\\n'\"\n        }), \")\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" rule, corpus \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"list\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"zip\"\n        }), \"(wp.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"merge_rules\"\n        }), \", wp.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"corpus_history\"\n        }), \"[\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":])):\\n    \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'NEW MERGE RULE: Combine \\\"{rule[0]}\\\" and \\\"{rule[1]}\\\"'\"\n        }), \")\\n    \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(corpus, end=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'\\\\n\\\\n'\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"초기 코퍼스:\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##m'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##p'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"NEW\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MERGE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"RULE\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"##m\\\"\"\n        }), \"과 \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"##p\\\"\"\n        }), \" 병합\\n[([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'c'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##s'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \"), ([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'e'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##a'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##t'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'r'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'j'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##u'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##mp'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##i'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##n'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##g'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"),\\n([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'f'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##o'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'##d'\"\n        }), \"], \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"6\"\n        }), \")]\\n\\n(이하 생략)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이제 WordPiece 알고리즘이 훈련되었으므로(즉, 모든 병합 규칙이 발견되었으므로), 모델은 모든 텍스트를 토큰화하기 위해 각 단어를 모든 문자로 분리한 다음 문자열의 처음부분에 대해 알려진 토큰을 찾을 수 있는 최대 토큰을 찾아서, 나머지 부분은 찾을 수 있는 최대 토큰을 찾는 방식으로 사용할 수 있습니다. 이 과정은 더 이상 훈련 데이터로부터 알려진 토큰과 일치하지 않을 때까지 반복되며, 따라서 문자열의 남은 부분은 최종 토큰으로 취합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"학습 데이터가 제한적이지만, 모델은 여전히 유용한 토큰을 학습했습니다. 그러나 많은 추가 학습 데이터가 필요함을 명백히 알 수 있습니다. 이 토크나이저를 유용하게 만들기 위해 더 많은 학습 데이터가 필요합니다. 예시 문자열에 대한 성능을 테스트할 수 있습니다. 예시로 'jumper' 단어로 시작해보겠습니다. 먼저 문자열은 ['jump', 'er']로 분리됩니다. 왜냐하면 jump는 단어의 시작에서 발견할 수 있는 가장 큰 토큰이기 때문입니다. 다음으로 er 문자열은 각각의 문자 e와 r로 나뉩니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(wp.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"tokenize\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"jumper\\\"\"\n        }), \"));\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"jump\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"e\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"r\\\"\"\n        }), \"];\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단일 토큰화\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Unigram 토크나이저는 BPE와 WordPiece와 다른 방식으로 작동합니다. 큰 어휘로 시작하여 원하는 크기에 도달할 때까지 반복적으로 줄여나갑니다.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Unigram 모델은 각 단어 또는 문자의 확률을 고려하는 통계적 방법을 사용합니다. 예를 들어, \\\"Cats are great but dogs are better\\\"라는 문장은 [\", _jsx(_components.code, {\n        children: \"Cats\"\n      }), \", \", _jsx(_components.code, {\n        children: \"are\"\n      }), \", \", _jsx(_components.code, {\n        children: \"great\"\n      }), \", \", _jsx(_components.code, {\n        children: \"but\"\n      }), \", \", _jsx(_components.code, {\n        children: \"dogs\"\n      }), \", \", _jsx(_components.code, {\n        children: \"are\"\n      }), \", \", _jsx(_components.code, {\n        children: \"better\"\n      }), \"] 또는 [\", _jsx(_components.code, {\n        children: \"C\"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"s\"\n      }), \", \", _jsx(_components.code, {\n        children: \"_a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"_g\"\n      }), \",\", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"_b\"\n      }), \", \", _jsx(_components.code, {\n        children: \"u\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"_d\"\n      }), \", \", _jsx(_components.code, {\n        children: \"o\"\n      }), \", \", _jsx(_components.code, {\n        children: \"g\"\n      }), \", \", _jsx(_components.code, {\n        children: \"s\"\n      }), \" \", _jsx(_components.code, {\n        children: \"_a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"_b\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \", \", _jsx(_components.code, {\n        children: \"e\"\n      }), \", \", _jsx(_components.code, {\n        children: \"r\"\n      }), \"]로 분할될 수 있습니다. 문장이 문자로 분할된 경우, 새로운 단어의 시작을 나타내기 위해 각 문자의 시작 부분에 밑줄이 추가됩니다.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이러한 목록의 각 요소는 토큰 t로 간주될 수 있으며, t1, t2, ..., tn의 일련의 토큰이 발생할 확률은 다음과 같습니다:\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-23-TokenizationACompleteGuide_2.png\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Unigram 모델은 다음 단계를 통해 훈련됩니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 1) 코퍼스 구성\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"언제나처럼 입력 텍스트는 정규화 및 사전 토크나이제이션 모델에 전달되어 깨끗한 단어가 생성됩니다. 그런 다음 단어들은 Unigram 모델에 전달되어 각 단어의 빈도를 결정하고, 이 숫자를 단어와 함께 코퍼스라는 목록에 저장합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 2) 어휘 구성\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Unigram 모델의 어휘 크기는 매우 크게 시작되고, 원하는 크기에 도달할 때까지 반복적으로 감소합니다. 초기 어휘를 구성하려면 말뭉치에서 가능한 모든 부분 문자열을 찾습니다. 예를 들어, 말뭉치의 첫 번째 단어가 'cats'인 경우, 부분 문자열 ['c', 'a', 't', 's', 'ca', 'at', 'ts', 'cat', 'ats']이 어휘에 추가됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"3단계) 각 토큰의 확률 계산\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토큰의 확률은 말뭉치에서 토큰의 발생 횟수를 찾아 총 토큰 발생 횟수로 나누어 근사적으로 계산됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-TokenizationACompleteGuide_3.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"4단계) 단어의 모든 가능한 세분화 찾기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"학습 말뭉치에서 단어가 cat인 경우를 고려해보겠습니다. 이는 다음과 같이 세분화될 수 있습니다:\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"c\"\n      }), \", \", _jsx(_components.code, {\n        children: \"a\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \"]\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"ca\"\n      }), \", \", _jsx(_components.code, {\n        children: \"t\"\n      }), \"]\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"c\"\n      }), \", \", _jsx(_components.code, {\n        children: \"at\"\n      }), \"]\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.code, {\n        children: \"cat\"\n      }), \"]\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 5) 말뭉치에서 발생 가능한 각 세분화의 근사 확률 계산\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 방정식들을 결합하면 각 토큰 시리즈에 대한 확률을 얻을 수 있습니다. 예를 들어, 이것은 다음과 같이 보일 수 있습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-23-TokenizationACompleteGuide_4.png\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"가장 높은 확률 점수를 가진 세그먼트 [\", _jsx(_components.code, {\n        children: \"c\"\n      }), \", \", _jsx(_components.code, {\n        children: \"at\"\n      }), \"]가 사용되어 단어를 토크나이즈했습니다. 따라서 단어 cat은 [\", _jsx(_components.code, {\n        children: \"c\"\n      }), \", \", _jsx(_components.code, {\n        children: \"at\"\n      }), \"]으로 토큰화됩니다. 단어가 긴 경우 토큰화시 단어 내 여러 곳에서 분할이 발생할 수 있습니다. 예를 들어 [\", _jsx(_components.code, {\n        children: \"token\"\n      }), \", \", _jsx(_components.code, {\n        children: \"iza\"\n      }), \", tion] 또는 [\", _jsx(_components.code, {\n        children: \"token\"\n      }), \", \", _jsx(_components.code, {\n        children: \"ization\"\n      }), \"] 같은 경우도 있을 수 있습니다.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"6단계) 손실 계산\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"손실이란 모델의 점수를 나타내며, 중요한 토큰이 어휘에서 제거되면 손실이 크게 증가하지만 중요하지 않은 토큰이 제거되면 손실은 크게 증가하지 않습니다. 모델에서 각 토큰을 제거했을 때 손실이 얼마나 되는지 계산하여, 어휘 중에서 가장 쓸모없는 토큰을 찾을 수 있습니다. 훈련 세트 말뭉치에서 가장 유용한 토큰만 남도록 어휘 크기가 감소할 때까지 반복적으로 수행할 수 있습니다. 손실은 다음과 같이 주어집니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-23-TokenizationACompleteGuide_5.png\",\n        alt: \"Tokenization Guide\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"필요한 양만큼 문자가 제거되어 어휘를 원하는 크기로 줄일 때, 교육은 완료되고 모델을 사용하여 단어를 토큰화할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"BPE, WordPiece 및 Unigram 비교\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"학습 세트 및 토큰화해야 할 데이터에 따라 어떤 토크나이저가 다른 것보다 더 잘 작동할 수 있습니다. 언어 모델에 대한 토크나이저를 선택할 때, 특정 사용 사례에 사용된 학습 세트를 실험하여 최상의 결과를 얻는 것이 가장 좋을 수 있습니다. 그러나 이 세 가지 토크나이저의 일반적인 경향에 대해 논의하는 것이 유용할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"세 가지 중에서 BPE가 현재 언어 모델 토크나이저로 가장 인기 있는 선택인 것으로 보입니다. 그러나 변화가 빠르게 일어나는 이 분야에서는 앞으로 변동이 있을 수 있습니다. 사실, SentencePiece와 같은 다른 서브워드 토크나이저들이 최근에 훨씬 더 인기를 얻고 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"WordPiece는 BPE와 Unigram에 비해 더 많은 단어 토큰을 생성하는 것으로 보입니다. 그러나 모델 선택과 관계 없이 어휘 크기가 커질수록 모든 토크나이저가 더 적은 토큰을 생성하는 것으로 보입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"최종적으로, 토크나이저의 선택은 모델과 함께 사용하려는 데이터셋에 따라 다릅니다. 안전한 선택은 BPE 또는 SentencePiece를 시도하고, 그 이후에 실험하는 것일 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"후처리\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토큰화 파이프라인의 마지막 단계는 후처리입니다. 여기서 필요한 경우 출력에 최종 수정을 가할 수 있습니다. BERT는 이 단계를 사용하여 두 가지 추가 토큰을 추가하는 데 유명합니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"[CLS] - 이 토큰은 \", _jsx(_components.code, {\n          children: \"classification\"\n        }), \"를 나타내며 입력 텍스트의 시작을 표시하는 데 사용됩니다. 이는 BERT에서 필요한 것인데, 이 토큰의 이름에서 알 수 있듯이 이를 사용하여 분류 작업이 수행되었기 때문입니다. 분류 작업에 사용되지 않을 때도 모델에서 여전히 이 토큰을 예상합니다.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"[SEP] - 이 토큰은 \", _jsx(_components.code, {\n          children: \"separation\"\n        }), \"을 나타내며 입력에서 문장을 분리하는 데 사용됩니다. BERT가 수행하는 많은 작업에 유용하며, 동일한 프롬프트에서 동시에 여러 지시사항을 처리할 때도 사용됩니다.\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Python 라이브러리의 토크나이저\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Hugging Face는 Python을 포함한 여러 프로그래밍 언어에서 사용할 수 있는 토크나이저 라이브러리를 제공합니다. 이 라이브러리에는 사용자가 사전 훈련된 모델을 사용할 수 있는 일반 Tokenizer 클래스가 포함되어 있으며, 전체 목록은 Hugging Face 웹사이트에서 확인할 수 있습니다. 게다가, 라이브러리에는 사용자가 자체 데이터로 훈련할 수 있는 네 가지 사전 제작되지만 미학습된 모델도 포함되어 있습니다. 이는 특정 유형의 문서에 튜닝된 특정 토크나이저를 작성하는 데 유용합니다. 아래 셀은 Python에서 사전 훈련된 및 미학습된 토크나이저를 사용하는 예시를 보여줍니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"프리트레인 토크나이저 사용하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토크나이저 라이브러리를 사용하면 프리트레인 토크나이저를 쉽게 사용할 수 있습니다. Tokenizer 클래스를 가져와서 from_pretrained 메소드를 호출하고 사용할 토크나이저의 모델 이름을 전달하면 됩니다. 모델의 목록은 [16]에서 확인할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Tokenizer\"\n        }), \"\\n\\ntokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Tokenizer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'bert-base-cased'\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토크나이저 학습하기\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"원하는 토큰 만들기되지만 미학습 토크나이저를 사용하려면 tokenizers 라이브러리에서 원하는 모델을 가져와서 모델 클래스의 인스턴스를 만들면 됩니다. 위에서 설명한대로 라이브러리에는 네 가지 모델이 포함되어 있습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"BertWordPieceTokenizer - 유명한 Bert 토크나이저인 WordPiece를 사용합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"CharBPETokenizer - 원래의 BPE(BPE)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ByteLevelBPETokenizer - BPE의 바이트 레벨 버전\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"SentencePieceBPETokenizer - SentencePiece에서 사용하는 BPE 구현과 호환되는 버전\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델을 학습하려면 train 메서드를 사용하고 학습 데이터가 포함된 파일의 경로(또는 파일 경로 목록)를 전달하면 됩니다. 학습을 마치면 모델은 encode 메서드를 사용하여 일부 텍스트를 토큰화하는 데 사용할 수 있습니다. 마지막으로 학습된 토크나이저는 save 메서드를 사용하여 저장할 수 있으므로 학습을 다시 수행할 필요가 없습니다. 아래는 Hugging Face Tokenizers GitHub 페이지에서 제공되는 예제를 수정한 예시 코드입니다 [17].\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# 토크나이저 가져오기\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BertWordPieceTokenizer\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CharBPETokenizer\"\n        }), \", \\\\\\n                       \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevelBPETokenizer\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SentencePieceBPETokenizer\"\n        }), \"\\n\\n# 모델 인스턴스화\\ntokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CharBPETokenizer\"\n        }), \"()\\n\\n# 모델 학습\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"([\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'./path/to/files/1.txt'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'./path/to/files/2.txt'\"\n        }), \"])\\n\\n# 텍스트 토큰화\\nencoded = tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"encode\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'I can feel the magic, can you?'\"\n        }), \")\\n\\n# 모델 저장\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"save\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'./path/to/directory/my-bpe.tokenizer.json'\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토크나이저 라이브러리는 이전 섹션에서 보여주었던 것과 같이 처음부터 전체 모델을 구현할 필요 없이 매우 빠르게 사용자 정의 토크나이저를 만들 수 있는 구성 요소도 제공합니다. 아래 셀에는 Hugging Face GitHub 페이지 [17]에서 가져온 예시가 표시되어 있습니다. 해당 예시에서는 토크나이저의 사전 토크나이제이션 및 디코딩 단계를 사용자 정의하는 방법을 보여줍니다. 이 경우, 사전 토크나이제이션 단계에서 접두어 공백이 추가되었고, 디코더로는 ByteLevel 디코더가 선택되었습니다. Hugging Face 문서 [18]에는 사용자 정의 옵션의 전체 목록이 제공됩니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" tokenizers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Tokenizer\"\n        }), \", models, pre_tokenizers, decoders, trainers, \\\\\\n                       processors\\n\\n# 토크나이저 초기화\\ntokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Tokenizer\"\n        }), \"(models.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"BPE\"\n        }), \"())\\n\\n# 사전 토크나이제이션 및 디코딩 사용자 정의\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"pre_tokenizer\"\n        }), \" = pre_tokenizers.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevel\"\n        }), \"(add_prefix_space=\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \")\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"decoder\"\n        }), \" = decoders.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevel\"\n        }), \"()\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"post_processor\"\n        }), \" = processors.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"ByteLevel\"\n        }), \"(trim_offsets=\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \")\\n\\n# 그리고 학습\\ntrainer = trainers.\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BpeTrainer\"\n        }), \"(\\n    vocab_size=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"20000\"\n        }), \",\\n    min_frequency=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"2\"\n        }), \",\\n    initial_alphabet=pre_tokenizers.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"ByteLevel\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"alphabet\"\n        }), \"()\\n)\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"train\"\n        }), \"([\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"./path/to/dataset/1.txt\\\"\"\n        }), \",\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"./path/to/dataset/2.txt\\\"\"\n        }), \",\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"./path/to/dataset/3.txt\\\"\"\n        }), \"\\n], trainer=trainer)\\n\\n# 그리고 저장\\ntokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"save\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"byte-level-bpe.tokenizer.json\\\"\"\n        }), \", pretty=\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토큰화 파이프라인은 언어 모델의 중요한 부분이며, 어떤 종류의 토크나이저를 사용할지 결정할 때 신중한 고려가 필요합니다. 요즘에는 Hugging Face와 같은 라이브러리의 개발자들이 우리를 대신하여 많은 이러한 결정을 내려주고 있습니다. 이를 통해 사용자는 빠르게 사용자 지정 데이터로 언어 모델을 학습하고 사용할 수 있습니다. 그러나 토큰화 방법에 대한 탄탄한 이해는 모델을 미세 조정하고 다양한 데이터셋에서 추가 성능을 얻는 데 귀중합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"참고 자료\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[1] 표지 이미지 — Stable Diffusion Web\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[2] 토큰 정의 — Stanford NLP 그룹\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[3] 단어 토크나이저 — Towards Data Science\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[4] TransformerXL 논문 — ArXiv\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[5] Tokenizers — Hugging Face\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[6] 단어 기반, 서브워드, 문자 기반 토크나이저 — Towards Data Science\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[7] 토큰화 파이프라인 — Hugging Face\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[8] Pre-tokenizers — Hugging Face\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[9] Language Models are Unsupervised Multitask Learners — OpenAI\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[10] BART Model for Text Autocompletion in NLP — Geeks for Geeks\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[11] Byte Pair Encoding — Hugging Face\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[12] WordPiece 토큰화 — Hugging Face\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[13] 두 분 NLP — 토큰화 방법론의 분류 — Medium\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[14] 서브워드 토크나이저 비교 — Vinija AI\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[15] BERT가 단어 맥락 관계를 배우는 데 어떻게 Attention 메커니즘과 Transformer를 활용하는가 — Medium\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[16] 사전 훈련된 모델 목록 — Hugging Face\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[17] Hugging Face Tokenizers 라이브러리 — GitHub\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[18] 사전 토크나이제이션 문서 — Hugging Face\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-23-TokenizationACompleteGuide"},"buildId":"R1x9p1CQYDDJESXyLXKOK","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>