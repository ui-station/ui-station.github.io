<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings" data-gatsby-head="true"/><meta name="twitter:title" content="100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-27 16:21" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_buildManifest.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 27, 2024</span><span class="posts_reading_time__f7YPP">17<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>가장 큰 문제 중 하나는 RAG 어플리케이션의 연산 검색 시간입니다. 1조 개의 임베딩 벡터 기록이 있는 벡터 데이터베이스가 있다고 상상해보세요. 사용자 쿼리를 1조 개의 벡터와 일치시키려고 하면 올바른 정보를 검색하는 데 1분 이상이 걸릴 것입니다.</p>
<p>시간을 단축하기 위해서는 사용자 쿼리 임베딩 벡터와 벡터 데이터베이스에 저장된 백만, 십억 또는 심지어 1조 개의 다른 임베딩 벡터 사이의 코사인 유사도를 계산하는 효율적인 방법을 찾아야 합니다.</p>
<p>MIT 라이선스 하에 Chunkdot은 밀집(dense) 및 희소(sparse) 행렬에 대한 멀티 스레드 행렬 곱셈을 제공하기 위해 특별히 설계되었습니다. Numba를 사용하여 계산을 가속화하며 항목 행렬 표현(임베딩)을 세분화하여 대규모 항목에 대한 K개의 가장 유사한 항목을 계산하는 데 적합합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Chunkdot GitHub 저장소</p>
<p>HuggingFace에는 Qdrant의 이 데이터셋과 같이 백만 개 이상의 엔트리의 임베딩 벡터를 제공하는 다양한 데이터셋이 많이 있습니다. Chunkdot 성능을 테스트하는 데 사용할 수 있습니다. 그러나 세부 성능 측정을 위해 우리는 NumPy 라이브러리를 사용하여 여러 차원의 임의의 임베딩 벡터를 생성할 것입니다.</p>
<p>Chunkdot의 접근 방식과 코사인 유사도의 의사 코드를 비교할 것이며, 크기와 차원을 증가시킴으로써 성능이 어떻게 영향을 받는지 관찰할 것입니다. 이 작업에는 일관성을 보장하기 위해 Kaggle (GPU 없음) 노트북을 사용할 것입니다.</p>
<p>본 블로그의 모든 코드는 제 GitHub 저장소에서 확인할 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>목차</h1>
<ul>
<li>준비 단계 설정</li>
<li>의사 코드 알고리즘 코딩</li>
<li>Chunkdot 알고리즘 코딩</li>
<li>계산 시간 함수 코딩</li>
<li>10k 벡터 임베딩에 대한 테스트</li>
<li>100k 벡터 임베딩에 대한 테스트</li>
<li>1 백만 벡터 임베딩에 대한 테스트</li>
<li>확장성 영향 시각화</li>
<li>Chunkdot의 특징</li>
<li>다음에 할 일</li>
</ul>
<h1>준비 단계 설정</h1>
<p>Chunkdot을 설치하려면 다른 라이브러리와 마찬가지인 유사한 설치 과정이 필요합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>chunkdot 설치하기</h1>
<pre><code class="hljs language-bash">pip install chunkdot
</code></pre>
<p>무엇이든 실행하기 전에 먼저 Kaggle 환경에서 사용 가능한 메모리를 확인해야 합니다.</p>
<h1>사용 가능한 메모리 확인하기</h1>
<pre><code class="hljs language-bash">!free -h
</code></pre>
<img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_1.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Chunkdot 에게 사용 가능한 메모리를 확인하는 것은 매우 중요합니다. 벡터 데이터베이스 크기가 증가할수록 계산 메모리도 증가합니다. 사용 가능한 메모리를 초과하지 않도록 하려면 하드웨어의 남은 메모리를 모니터링하는 것이 중요합니다. 제 경우에는 버퍼/캐시를 제외하고 25GB의 여유 공간이 있습니다.</p>
<p>필요한 라이브러리를 가져오겠습니다.</p>
<pre><code class="hljs language-js"># 행렬을 생성하기 위한 라이브러리
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

# chunkdot에서 코사인 유사도 모듈을 가져옵니다.
<span class="hljs-keyword">from</span> chunkdot <span class="hljs-keyword">import</span> cosine_similarity_top_k

# 계산 시간을 측정하기 위한 라이브러리
<span class="hljs-keyword">import</span> timeit
</code></pre>
<h1>코딩 의사 코드 알고리즘</h1>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>먼저 데이터베이스나 로컬에 저장된 수백만 벡터들과 사용자 쿼리 벡터 간의 코사인 유사도를 계산하는 의사 코드 알고리즘을 만들어 볼게요.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">cosine_pseudocode</span>(query_v, doc_v, num_indices):
    <span class="hljs-string">""</span><span class="hljs-string">"
    Embedding 벡터와 쿼리 벡터 간의 가장 높은 코사인 유사도 값을 가진 인덱스를 반환합니다.

    매개변수:
        query_v (numpy.ndarray): 쿼리 벡터.
        doc_v (list of numpy.ndarray): Embedding 벡터의 목록.
        num_indices (int): 반환할 상위 인덱스 개수.

    반환값:
        list of int: 가장 높은 코사인 유사도 값을 가진 인덱스들.
    "</span><span class="hljs-string">""</span>
    cosine_similarities = []  # 코사인 유사도를 저장할 빈 리스트를 초기화합니다.

    query_norm = np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">norm</span>(query_v)  # 쿼리 벡터의 노름을 계산합니다.

    # 리스트에 있는 각 문서의 임베딩 벡터에 대해 반복합니다.
    <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> <span class="hljs-attr">doc_v</span>:
        dot_product = np.<span class="hljs-title function_">dot</span>(vec, query_v.<span class="hljs-property">T</span>)  # 임베딩 벡터와 쿼리 벡터 간의 내적을 계산합니다.
        embedding_norm = np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">norm</span>(vec)  # 임베딩 벡터의 노름을 계산합니다.
        cosine_similarity = dot_product / (embedding_norm * query_norm)  # 코사인 유사도 계산
        cosine_similarities.<span class="hljs-title function_">append</span>(cosine_similarity)  # 코사인 유사도를 리스트에 추가합니다.

    cosine_similarities = np.<span class="hljs-title function_">array</span>(cosine_similarities)  # 리스트를 넘파이 배열로 변환합니다.

    # 배열을 내림차순으로 정렬합니다.
    sorted_array = <span class="hljs-title function_">sorted</span>(<span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(cosine_similarities)), key=lambda <span class="hljs-attr">i</span>: cosine_similarities[i], reverse=<span class="hljs-title class_">True</span>)

    # 상위 num_indices 값을 가져옵니다.
    top_indices = sorted_array[:num_indices]

    # 가장 높은 코사인 유사도 값을 가진 인덱스를 반환합니다.
    <span class="hljs-keyword">return</span> top_indices
</code></pre>
<p>이 코사인 유사도 함수는 NumPy를 제외한 모든 라이브러리와 독립적으로 작동하며 세 가지 입력을 받습니다:</p>
<ul>
<li>query_v는 사용자 쿼리의 임베딩 벡터입니다.</li>
<li>doc_v는 어딘가에 저장된 문서들의 임베딩 벡터들입니다.</li>
<li>num_indices는 유사한 상위 k결과로부터 문서의 인덱스 번호입니다.</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>코딩 Chunkdot 알고리즘</h1>
<p>의사코드 알고리즘을 코딩했으니, 다음 단계는 Chunkdot 코사인 유사도 함수를 코딩하는 것입니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">cosine_chunkdot</span>(query_v, doc_v, num_indices, max_memory):
    <span class="hljs-string">""</span><span class="hljs-string">"
    청크닷 라이브러리를 사용하여 코사인 유사도를 계산합니다.

    매개변수:
        query_v (numpy.ndarray): 쿼리 벡터.
        doc_v (numpy.ndarray): 임베딩 벡터 목록.
        num_indices (int): 검색할 상위 인덱스 수.
        max_memory (float): 사용할 최대 메모리.

    반환값:
        numpy.ndarray: 상위 k 인덱스.
    "</span><span class="hljs-string">""</span>

    # 코사인 유사도 계산
    cosine_array = <span class="hljs-title function_">cosine_similarity_top_k</span>(embeddings=query_v, embeddings_right=doc_v,
                                         top_k=num_indices, max_memory=max_memory)  # 청크닷을 사용하여 코사인 유사도 계산

    # 상위 값의 인덱스 가져오기
    top_indices = cosine_array.<span class="hljs-title function_">nonzero</span>()[<span class="hljs-number">1</span>]

    # 상위 유사 결과 반환
    <span class="hljs-keyword">return</span> top_indices
</code></pre>
<p>이 Chunkdot 함수는 네 개의 입력을 받습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>query_v은 사용자 쿼리의 임베딩 벡터를 나타냅니다.</li>
<li>doc_v는 어딘가에 저장된 문서의 임베딩 벡터들을 나타냅니다.</li>
<li>num_indices는 유사한 상위 k개 결과에 대한 문서의 인덱스 번호를 나타냅니다.</li>
<li>max_memory는 계산에 사용할 수 있는 사용 가능한 메모리를 바이트 단위로 나타냅니다. 예를 들어, 1E9는 1GB를 의미하며, 10E9는 10GB를 의미합니다.</li>
</ul>
<p>두 함수를 샘플 데이터셋에서 테스트하여 출력을 관찰해보겠습니다.</p>
<pre><code class="hljs language-js">doc_embeddings = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(<span class="hljs-number">10</span>, <span class="hljs-number">100</span>) # <span class="hljs-number">10</span>개의 문서 임베딩 (<span class="hljs-number">100</span>차원)

user_query = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>) # <span class="hljs-number">1</span>개의 사용자 쿼리 (<span class="hljs-number">100</span>차원)

top_indices = <span class="hljs-number">1</span> # 검색할 상위 인덱스 수

max_memory = <span class="hljs-number">5E9</span> # 사용할 최대 메모리 (5GB)

# 의사코드를 사용하여 가장 높은 코사인 유사도 값을 갖는 인덱스를 검색
<span class="hljs-title function_">print</span>(<span class="hljs-string">"의사코드를 사용한 상위 인덱스:"</span>, <span class="hljs-title function_">cosine_pseudocode</span>(user_query, doc_embeddings, top_indices))

# chunkdot을 사용하여 가장 높은 코사인 유사도 값을 갖는 인덱스를 검색
<span class="hljs-title function_">print</span>(<span class="hljs-string">"chunkdot을 사용한 상위 인덱스:"</span>, <span class="hljs-title function_">cosine_chunkdot</span>(user_query, doc_embeddings, top_indices, max_memory))
</code></pre>
<pre><code class="hljs language-js">### 출력 ###
의사코드를 사용한 상위 인덱스: [<span class="hljs-number">4</span>]
chunkdot을 사용한 상위 인덱스: [<span class="hljs-number">4</span>]
### 출력 ###
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>문서 임베딩에 대한 유사한 항목의 인덱스를 최고의 코사인 유사도를 기반으로 한 항목만 반환한다는 것을 의미하는 top_indices 매개변수를 1로 설정했습니다. 메모리 사용량을 5E9로 설정했는데, 이는 5GB와 동일합니다. 두 함수 모두 동일한 인덱스 4를 반환하여 두 함수를 정확하게 코딩했음을 나타냅니다.</p>
<h1>코딩 계산 시간 함수</h1>
<p>또한 이러한 두 함수에 의해 소요된 계산 시간을 측정할 수 있는 타이밍 함수를 만들어야 합니다.</p>
<pre><code class="hljs language-js"># 소요 시간 계산
def <span class="hljs-title function_">calculate_execution_time</span>(query_v, doc_v, num_indices, max_memory, times):

    # 의사코드 함수 실행에 걸리는 시간 계산
    pseudocode_time = <span class="hljs-title function_">round</span>(timeit.<span class="hljs-title function_">timeit</span>(<span class="hljs-attr">lambda</span>: <span class="hljs-title function_">cosine_pseudocode</span>(query_v, doc_v, num_indices), number=times), <span class="hljs-number">5</span>)

    # chunkdot 함수 실행에 걸리는 시간 계산
    chunkdot_time = <span class="hljs-title function_">round</span>(timeit.<span class="hljs-title function_">timeit</span>(<span class="hljs-attr">lambda</span>: <span class="hljs-title function_">cosine_chunkdot</span>(query_v, doc_v, num_indices, max_memory), number=times), <span class="hljs-number">5</span>)

    # 소요 시간 출력
    <span class="hljs-title function_">print</span>(<span class="hljs-string">"의사코드 함수에 걸리는 시간:"</span>, pseudocode_time, <span class="hljs-string">"초"</span>)
    <span class="hljs-title function_">print</span>(<span class="hljs-string">"chunkdot 함수에 걸리는 시간:"</span>, chunkdot_time, <span class="hljs-string">"초"</span>)
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>우리는 이미 이 함수로 전달되는 매개변수를 검토했습니다. 여기서 새로운 매개변수는 times입니다. 이 매개변수는 코드를 몇 번 실행할지를 함수에 알려줍니다. 더 큰 규모에서 Chunkdot 성능을 테스트해 봅시다.</p>
<h1>10,000 벡터 임베딩 테스트</h1>
<p>우리는 합리적인 수의 문서 임베딩인 10000개로 시작할 것입니다. 이는 소규모 도메인별 RAG 애플리케이션과 비교 가능합니다. 각 임베딩 벡터의 차원을 1536으로 설정했습니다. 이는 OpenAI 임베딩 모델 텍스트 임베딩-3-small과 동등합니다.</p>
<p>각 접근 방식의 계산 시간을 계산하기 위해 각각 100번 실행하여 효율성을 테스트해 봅시다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python">doc_embeddings = np.random.randn(<span class="hljs-number">10000</span>, <span class="hljs-number">1536</span>)  <span class="hljs-comment"># 10K 문서 임베딩 (1536 차원)</span>

user_query = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-number">1536</span>)  <span class="hljs-comment"># 사용자 쿼리 (1536 차원)</span>

top_indices = <span class="hljs-number">1</span>  <span class="hljs-comment"># 검색할 상위 인덱스 수</span>

max_memory = <span class="hljs-number">5E9</span>  <span class="hljs-comment"># 최대 메모리를 5GB로 설정</span>

<span class="hljs-comment"># 함수 실행에 소요된 시간을 계산합니다.</span>
calculate_execution_time(user_query, doc_embeddings, top_indices, max_memory, <span class="hljs-number">100</span>)
</code></pre>
<p>10,000개의 문서 임베딩, 1536 차원을 가진 두 알고리즘을 100번 실행한 결과는 다음과 같습니다:</p>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_2.png" alt="그래프"></p>
<p>Chunkdot은 유사도 비교 코드와 비교해 더 많은 시간이 소요됩니다. 이는 Chunkdot이 먼저 청크를 생성하고 각 청크에서 계산을 수행한 후 병합하기 때문입니다. 따라서 이 소규모 예제에 대해서는 적합한 해결책이 아닐 수 있습니다. 그러나 나중에 더 큰 예제를 다룰 때 Chunkdot의 이점을 확인하게 될 것입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100k 벡터 임베딩 테스트</h1>
<p>10천 개의 가상 코드 방식으로는 우리가 이긴다고 하지만, 이제는 문서 임베딩 벡터를 중형 규모의 RAG 응용 프로그램과 유사한 100K 벡터까지 늘려보겠습니다.</p>
<p>각 방법에 대한 계산 시간을 계산해 봅시다. 이번에는 벡터의 수가 매우 많기 때문에 계산을 여러 번 수행할 필요가 없으므로 times 매개변수를 1로 설정하여 코드를 한 번 실행합니다.</p>
<pre><code class="hljs language-js">doc_embeddings = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(<span class="hljs-number">100000</span>, <span class="hljs-number">1536</span>) # 100K 문서 임베딩 (<span class="hljs-number">1536</span> 차원)

user_query = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">rand</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1536</span>) # 사용자 쿼리 (<span class="hljs-number">1536</span> 차원)

top_indices = <span class="hljs-number">1</span> # 반환할 상위 인덱스 수

max_memory = <span class="hljs-number">5E9</span> # 최대 메모리를 5GB로 설정

times = <span class="hljs-number">1</span> # 함수를 실행할 횟수

# 함수 실행 시간 계산
<span class="hljs-title function_">calculate_execution_time</span>(user_query, doc_embeddings, top_indices, max_memory, times)
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>100,000개의 문서 임베딩, 차원이 1536인 경우, 두 알고리즘을 한 번씩 실행하여 비교한 결과는 다음과 같습니다:</p>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_3.png" alt="Comparison Chart"></p>
<p>Chunkdot은 의사코드에 비해 거의 절반의 시간이 소요됩니다. 이제 Chunkdot의 유망한 영향을 확인하고 있습니다.</p>
<p>1백만 벡터 임베딩을 위한 테스트 중입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>백만 개의 임베딩을 다루는 작업 중에, 첫 번째로 확인해야 할 사항은 문서 임베딩 벡터가 얼마나 많은 메모리를 차지하는지입니다.</p>
<pre><code class="hljs language-js"># <span class="hljs-number">1</span> 백만 개의 문서 임베딩 (<span class="hljs-number">1536</span> 차원)
doc_embeddings = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(<span class="hljs-number">1000000</span>, <span class="hljs-number">1536</span>)

# 사용자 쿼리 (<span class="hljs-number">1536</span> 차원)
user_query = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1536</span>)

# doc_embeddings와 user_query 임베딩의 메모리 크기 확인
<span class="hljs-title function_">print</span>(doc_embeddings.<span class="hljs-property">nbytes</span> / (<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>),
      user_query.<span class="hljs-property">nbytes</span> / (<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>))
</code></pre>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_4.png" alt="이미지"></p>
<p>저희 문서 임베딩은 대략 12GB를 차지합니다. 남은 공간을 확인해 보세요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_5.png" alt="image"></p>
<p>17GB의 사용 가능한 메모리가 있습니다. 어떠한 메모리 오류도 피하기 위해 max_memory 매개변수의 안전한 값을 12GB로 설정할 것입니다. 결과를 확인해 봅시다.</p>
<pre><code class="hljs language-js"># <span class="hljs-number">100</span>만 개의 문서 임베딩 (<span class="hljs-number">1536</span> 차원)
doc_embeddings = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(<span class="hljs-number">1000000</span>, <span class="hljs-number">1536</span>)

# 사용자 쿼리 (<span class="hljs-number">1536</span> 차원)
user_query = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">rand</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1536</span>)

top_indices = <span class="hljs-number">1</span> # 가져올 상위 인덱스 수

max_memory = <span class="hljs-number">12E9</span> # 최대 메모리 설정 --- 12GB ---

times = <span class="hljs-number">1</span> # 함수를 실행할 횟수

# 함수 실행 시간 계산하기
<span class="hljs-title function_">calculate_execution_time</span>(user_query, doc_embeddings, top_indices, max_memory, times)
</code></pre>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_6.png" alt="image"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>ChunkDot은 실제로 계산을 효과적으로 줄입니다. 진지한 RAG 앱을 만들려면 적어도 백만 개의 쿼리부터 시작하는 것이 좋습니다. 높은 차원의 임베딩 모델로 작업을하는 경우 4000까지 증가합니다. 이 접근 방식은 훨씬 더 효율적이어질 것입니다.</p>
<h1>확장성 영향 시각화</h1>
<p>문서 임베딩 벡터 수를 증가시키는 영향을 시각화해 보겠습니다. 시작은 10,000부터 매우 큰 수까지입니다.</p>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_7.png" alt="이미지"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>저는 세 가지 방법을 그래프로 나타내었고, 문서 임베딩의 수를 늘릴 때 Chunkdot이 모든 방법 중에서 가장 우수하다는 것을 확인했습니다. 이제 임베딩 벡터의 차원이 계산 시간에 어떤 영향을 미치는지 살펴보겠습니다.</p>
<p><img src="/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_8.png" alt="Embedding Vectors Computation Time"></p>
<p>벡터의 차원을 늘리면서 10만 개의 문서를 사용했고, 문서 수를 늘릴 때 관찰한 현상과 동일한 결과를 얻었습니다.</p>
<h1>Chunkdot의 특징</h1>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Chunkdot에는 진행 표시 막대를 표시할 수 있는 기능이 있어서 얼마만큼의 계산이 남았는지 추적할 수 있습니다.</p>
<pre><code class="hljs language-js">doc_embeddings = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(<span class="hljs-number">100000</span>, <span class="hljs-number">1536</span>) # 100K <span class="hljs-variable language_">document</span> embeddings (<span class="hljs-number">1536</span> dim)

user_query = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">rand</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1536</span>) # user query (<span class="hljs-number">1536</span> dim)

top_indices = <span class="hljs-number">100</span> # 검색할 상위 인덱스 수

max_memory = <span class="hljs-number">5E9</span> # 최대 메모리를 5GB로 설정

# 진행 표시 막대와 함께
output_array = <span class="hljs-title function_">cosine_similarity_top_k</span>(user_query, doc_embeddings,
                        top_k=top_indices,
                        show_progress=<span class="hljs-title class_">True</span>)
</code></pre>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*3A2KQ9fDvAA-VfQNKjphJw.gif" alt="진행 표시 막대"></p>
<p>Chunkdot의 출력은 희소 행렬이며, 다음을 사용하여 배열로 변환할 수 있습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>출력 변환</h1>
<p>output_array.toarray()</p>
<p>Chunkdot은 문서 임베딩에 대해서만 사용할 수 있습니다. 이는 각 문서 임베딩 요소에 대해 가장 유사한 상위 k개 요소를 반환할 것입니다.</p>
<h1>총 5개의 문서 임베딩</h1>
<p>embeddings = np.random.randn(5, 256)</p>
<h1>각각의 상위 2개 가장 유사한 항목 인덱스 반환</h1>
<p>cosine_similarity_top_k(embeddings, top_k=2).toarray()</p>
<h3>출력</h3>
<p>array([[1.        , 0.        , 0.        , 0.        , 0.09924064],
[0.        , 1.        , 0.        , 0.09935381, 0.        ],
[0.02358785, 0.        , 1.        , 0.        , 0.        ],
[0.        , 0.09935381, 0.        , 1.        , 0.        ],
[0.09924064, 0.        , 0.        , 0.        , 1.]]</p>
<h3>출력</h3>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>비슷한 방법으로 top_k 매개변수에 음수 값을 제공하여 가장 다른 항목을 반환할 수도 있습니다.</p>
<pre><code class="hljs language-js"># 총 <span class="hljs-number">5</span>개의 문서 임베딩
embeddings = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(<span class="hljs-number">5</span>, <span class="hljs-number">256</span>)

# 각 항목의 가장 다른 상위 <span class="hljs-number">2</span>개 항목 인덱스 반환
# <span class="hljs-title class_">Top</span>_K = -<span class="hljs-number">2</span>
<span class="hljs-title function_">cosine_similarity_top_k</span>(embeddings, top_k=-<span class="hljs-number">2</span>).<span class="hljs-title function_">toarray</span>()
</code></pre>
<pre><code class="hljs language-js">### 출력 ###
<span class="hljs-title function_">array</span>([[ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , -<span class="hljs-number">0.04357524</span>,  <span class="hljs-number">0.</span>        , -<span class="hljs-number">0.05118288</span>],
       [ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.01619543</span>, -<span class="hljs-number">0.01836534</span>],
       [-<span class="hljs-number">0.04357524</span>,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , -<span class="hljs-number">0.02466613</span>,  <span class="hljs-number">0.</span>        ],
       [ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.01619543</span>, -<span class="hljs-number">0.02466613</span>,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ],
       [-<span class="hljs-number">0.05118288</span>, -<span class="hljs-number">0.01836534</span>,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ]])
### 출력 ###
</code></pre>
<p>당신의 상황이 아닐 수도 있지만, 1만 차원까지의 희소한 임베딩을 처리하는 경우 density 매개변수를 사용하여 계산을 더 효율적으로 줄일 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"># 희소 임베딩 생성을 위해
<span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> sparse

# 각각 <span class="hljs-number">10</span>,<span class="hljs-number">000</span> 차원을 가진 <span class="hljs-number">100</span>,<span class="hljs-number">000</span>개의 문서로 이루어진 희소 매트릭스 생성
# 밀도를 <span class="hljs-number">0.005</span>로 정의
임베딩 = sparse.<span class="hljs-title function_">rand</span>(<span class="hljs-number">100000</span>, <span class="hljs-number">10000</span>, density=<span class="hljs-number">0.005</span>)

# 시스템의 모든 메모리 사용
코사인 유사도 상위 <span class="hljs-title function_">k</span>(embeddings, top_k=<span class="hljs-number">50</span>)
</code></pre>
<h1>다음 단계</h1>
<p>Chunkdot 알고리즘이 어떻게 작동하는지 알고 싶다면, 저자의 멋진 블로그를 확인해보세요. Chunkdot의 가장 큰 장점 중 하나는 CPU 코어에서 작동한다는 것입니다. 앞으로, GPU 지원을 통합할 계획이 있으며, 이는 연산에 소요되는 시간을 크게 줄일 것입니다. 로컬 환경에 충분한 RAM이 없는 경우에는 Kaggle이나 GitHub Codespaces 같은 플랫폼을 활용할 수 있습니다. 이러한 클라우드 CPU 코어와 RAM은 GPU 비용에 비해 매우 저렴합니다. Chunkdot가 어떻게 작동하는지를 잘 설명한 공식 GitHub 저장소와 블로그도 꼭 확인해보세요.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"100배 빠르게 - 수십억 개의 임베딩을 위한 RAG 앱 확장하기","description":"","date":"2024-05-27 16:21","slug":"2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings","content":"\n가장 큰 문제 중 하나는 RAG 어플리케이션의 연산 검색 시간입니다. 1조 개의 임베딩 벡터 기록이 있는 벡터 데이터베이스가 있다고 상상해보세요. 사용자 쿼리를 1조 개의 벡터와 일치시키려고 하면 올바른 정보를 검색하는 데 1분 이상이 걸릴 것입니다.\n\n시간을 단축하기 위해서는 사용자 쿼리 임베딩 벡터와 벡터 데이터베이스에 저장된 백만, 십억 또는 심지어 1조 개의 다른 임베딩 벡터 사이의 코사인 유사도를 계산하는 효율적인 방법을 찾아야 합니다.\n\nMIT 라이선스 하에 Chunkdot은 밀집(dense) 및 희소(sparse) 행렬에 대한 멀티 스레드 행렬 곱셈을 제공하기 위해 특별히 설계되었습니다. Numba를 사용하여 계산을 가속화하며 항목 행렬 표현(임베딩)을 세분화하여 대규모 항목에 대한 K개의 가장 유사한 항목을 계산하는 데 적합합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nChunkdot GitHub 저장소\n\nHuggingFace에는 Qdrant의 이 데이터셋과 같이 백만 개 이상의 엔트리의 임베딩 벡터를 제공하는 다양한 데이터셋이 많이 있습니다. Chunkdot 성능을 테스트하는 데 사용할 수 있습니다. 그러나 세부 성능 측정을 위해 우리는 NumPy 라이브러리를 사용하여 여러 차원의 임의의 임베딩 벡터를 생성할 것입니다.\n\nChunkdot의 접근 방식과 코사인 유사도의 의사 코드를 비교할 것이며, 크기와 차원을 증가시킴으로써 성능이 어떻게 영향을 받는지 관찰할 것입니다. 이 작업에는 일관성을 보장하기 위해 Kaggle (GPU 없음) 노트북을 사용할 것입니다.\n\n본 블로그의 모든 코드는 제 GitHub 저장소에서 확인할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 목차\n\n- 준비 단계 설정\n- 의사 코드 알고리즘 코딩\n- Chunkdot 알고리즘 코딩\n- 계산 시간 함수 코딩\n- 10k 벡터 임베딩에 대한 테스트\n- 100k 벡터 임베딩에 대한 테스트\n- 1 백만 벡터 임베딩에 대한 테스트\n- 확장성 영향 시각화\n- Chunkdot의 특징\n- 다음에 할 일\n\n# 준비 단계 설정\n\nChunkdot을 설치하려면 다른 라이브러리와 마찬가지인 유사한 설치 과정이 필요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# chunkdot 설치하기\n\n```bash\npip install chunkdot\n```\n\n무엇이든 실행하기 전에 먼저 Kaggle 환경에서 사용 가능한 메모리를 확인해야 합니다.\n\n# 사용 가능한 메모리 확인하기\n\n```bash\n!free -h\n```\n\n\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_1.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nChunkdot 에게 사용 가능한 메모리를 확인하는 것은 매우 중요합니다. 벡터 데이터베이스 크기가 증가할수록 계산 메모리도 증가합니다. 사용 가능한 메모리를 초과하지 않도록 하려면 하드웨어의 남은 메모리를 모니터링하는 것이 중요합니다. 제 경우에는 버퍼/캐시를 제외하고 25GB의 여유 공간이 있습니다.\n\n필요한 라이브러리를 가져오겠습니다.\n\n```js\n# 행렬을 생성하기 위한 라이브러리\nimport numpy as np\n\n# chunkdot에서 코사인 유사도 모듈을 가져옵니다.\nfrom chunkdot import cosine_similarity_top_k\n\n# 계산 시간을 측정하기 위한 라이브러리\nimport timeit\n```\n\n# 코딩 의사 코드 알고리즘\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n먼저 데이터베이스나 로컬에 저장된 수백만 벡터들과 사용자 쿼리 벡터 간의 코사인 유사도를 계산하는 의사 코드 알고리즘을 만들어 볼게요.\n\n```js\ndef cosine_pseudocode(query_v, doc_v, num_indices):\n    \"\"\"\n    Embedding 벡터와 쿼리 벡터 간의 가장 높은 코사인 유사도 값을 가진 인덱스를 반환합니다.\n\n    매개변수:\n        query_v (numpy.ndarray): 쿼리 벡터.\n        doc_v (list of numpy.ndarray): Embedding 벡터의 목록.\n        num_indices (int): 반환할 상위 인덱스 개수.\n\n    반환값:\n        list of int: 가장 높은 코사인 유사도 값을 가진 인덱스들.\n    \"\"\"\n    cosine_similarities = []  # 코사인 유사도를 저장할 빈 리스트를 초기화합니다.\n\n    query_norm = np.linalg.norm(query_v)  # 쿼리 벡터의 노름을 계산합니다.\n\n    # 리스트에 있는 각 문서의 임베딩 벡터에 대해 반복합니다.\n    for vec in doc_v:\n        dot_product = np.dot(vec, query_v.T)  # 임베딩 벡터와 쿼리 벡터 간의 내적을 계산합니다.\n        embedding_norm = np.linalg.norm(vec)  # 임베딩 벡터의 노름을 계산합니다.\n        cosine_similarity = dot_product / (embedding_norm * query_norm)  # 코사인 유사도 계산\n        cosine_similarities.append(cosine_similarity)  # 코사인 유사도를 리스트에 추가합니다.\n\n    cosine_similarities = np.array(cosine_similarities)  # 리스트를 넘파이 배열로 변환합니다.\n\n    # 배열을 내림차순으로 정렬합니다.\n    sorted_array = sorted(range(len(cosine_similarities)), key=lambda i: cosine_similarities[i], reverse=True)\n\n    # 상위 num_indices 값을 가져옵니다.\n    top_indices = sorted_array[:num_indices]\n\n    # 가장 높은 코사인 유사도 값을 가진 인덱스를 반환합니다.\n    return top_indices\n```\n\n이 코사인 유사도 함수는 NumPy를 제외한 모든 라이브러리와 독립적으로 작동하며 세 가지 입력을 받습니다:\n\n- query_v는 사용자 쿼리의 임베딩 벡터입니다.\n- doc_v는 어딘가에 저장된 문서들의 임베딩 벡터들입니다.\n- num_indices는 유사한 상위 k결과로부터 문서의 인덱스 번호입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 코딩 Chunkdot 알고리즘\n\n의사코드 알고리즘을 코딩했으니, 다음 단계는 Chunkdot 코사인 유사도 함수를 코딩하는 것입니다.\n\n```js\ndef cosine_chunkdot(query_v, doc_v, num_indices, max_memory):\n    \"\"\"\n    청크닷 라이브러리를 사용하여 코사인 유사도를 계산합니다.\n\n    매개변수:\n        query_v (numpy.ndarray): 쿼리 벡터.\n        doc_v (numpy.ndarray): 임베딩 벡터 목록.\n        num_indices (int): 검색할 상위 인덱스 수.\n        max_memory (float): 사용할 최대 메모리.\n\n    반환값:\n        numpy.ndarray: 상위 k 인덱스.\n    \"\"\"\n\n    # 코사인 유사도 계산\n    cosine_array = cosine_similarity_top_k(embeddings=query_v, embeddings_right=doc_v,\n                                         top_k=num_indices, max_memory=max_memory)  # 청크닷을 사용하여 코사인 유사도 계산\n\n    # 상위 값의 인덱스 가져오기\n    top_indices = cosine_array.nonzero()[1]\n\n    # 상위 유사 결과 반환\n    return top_indices\n```\n\n이 Chunkdot 함수는 네 개의 입력을 받습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- query_v은 사용자 쿼리의 임베딩 벡터를 나타냅니다.\n- doc_v는 어딘가에 저장된 문서의 임베딩 벡터들을 나타냅니다.\n- num_indices는 유사한 상위 k개 결과에 대한 문서의 인덱스 번호를 나타냅니다.\n- max_memory는 계산에 사용할 수 있는 사용 가능한 메모리를 바이트 단위로 나타냅니다. 예를 들어, 1E9는 1GB를 의미하며, 10E9는 10GB를 의미합니다.\n\n두 함수를 샘플 데이터셋에서 테스트하여 출력을 관찰해보겠습니다.\n\n```js\ndoc_embeddings = np.random.randn(10, 100) # 10개의 문서 임베딩 (100차원)\n\nuser_query = np.random.rand(1, 100) # 1개의 사용자 쿼리 (100차원)\n\ntop_indices = 1 # 검색할 상위 인덱스 수\n\nmax_memory = 5E9 # 사용할 최대 메모리 (5GB)\n\n# 의사코드를 사용하여 가장 높은 코사인 유사도 값을 갖는 인덱스를 검색\nprint(\"의사코드를 사용한 상위 인덱스:\", cosine_pseudocode(user_query, doc_embeddings, top_indices))\n\n# chunkdot을 사용하여 가장 높은 코사인 유사도 값을 갖는 인덱스를 검색\nprint(\"chunkdot을 사용한 상위 인덱스:\", cosine_chunkdot(user_query, doc_embeddings, top_indices, max_memory))\n```\n\n```js\n### 출력 ###\n의사코드를 사용한 상위 인덱스: [4]\nchunkdot을 사용한 상위 인덱스: [4]\n### 출력 ###\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n문서 임베딩에 대한 유사한 항목의 인덱스를 최고의 코사인 유사도를 기반으로 한 항목만 반환한다는 것을 의미하는 top_indices 매개변수를 1로 설정했습니다. 메모리 사용량을 5E9로 설정했는데, 이는 5GB와 동일합니다. 두 함수 모두 동일한 인덱스 4를 반환하여 두 함수를 정확하게 코딩했음을 나타냅니다.\n\n# 코딩 계산 시간 함수\n\n또한 이러한 두 함수에 의해 소요된 계산 시간을 측정할 수 있는 타이밍 함수를 만들어야 합니다.\n\n```js\n# 소요 시간 계산\ndef calculate_execution_time(query_v, doc_v, num_indices, max_memory, times):\n\n    # 의사코드 함수 실행에 걸리는 시간 계산\n    pseudocode_time = round(timeit.timeit(lambda: cosine_pseudocode(query_v, doc_v, num_indices), number=times), 5)\n\n    # chunkdot 함수 실행에 걸리는 시간 계산\n    chunkdot_time = round(timeit.timeit(lambda: cosine_chunkdot(query_v, doc_v, num_indices, max_memory), number=times), 5)\n\n    # 소요 시간 출력\n    print(\"의사코드 함수에 걸리는 시간:\", pseudocode_time, \"초\")\n    print(\"chunkdot 함수에 걸리는 시간:\", chunkdot_time, \"초\")\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 이미 이 함수로 전달되는 매개변수를 검토했습니다. 여기서 새로운 매개변수는 times입니다. 이 매개변수는 코드를 몇 번 실행할지를 함수에 알려줍니다. 더 큰 규모에서 Chunkdot 성능을 테스트해 봅시다.\n\n# 10,000 벡터 임베딩 테스트\n\n우리는 합리적인 수의 문서 임베딩인 10000개로 시작할 것입니다. 이는 소규모 도메인별 RAG 애플리케이션과 비교 가능합니다. 각 임베딩 벡터의 차원을 1536으로 설정했습니다. 이는 OpenAI 임베딩 모델 텍스트 임베딩-3-small과 동등합니다.\n\n각 접근 방식의 계산 시간을 계산하기 위해 각각 100번 실행하여 효율성을 테스트해 봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\ndoc_embeddings = np.random.randn(10000, 1536)  # 10K 문서 임베딩 (1536 차원)\n\nuser_query = np.random.rand(1, 1536)  # 사용자 쿼리 (1536 차원)\n\ntop_indices = 1  # 검색할 상위 인덱스 수\n\nmax_memory = 5E9  # 최대 메모리를 5GB로 설정\n\n# 함수 실행에 소요된 시간을 계산합니다.\ncalculate_execution_time(user_query, doc_embeddings, top_indices, max_memory, 100)\n```\n\n10,000개의 문서 임베딩, 1536 차원을 가진 두 알고리즘을 100번 실행한 결과는 다음과 같습니다:\n\n![그래프](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_2.png)\n\nChunkdot은 유사도 비교 코드와 비교해 더 많은 시간이 소요됩니다. 이는 Chunkdot이 먼저 청크를 생성하고 각 청크에서 계산을 수행한 후 병합하기 때문입니다. 따라서 이 소규모 예제에 대해서는 적합한 해결책이 아닐 수 있습니다. 그러나 나중에 더 큰 예제를 다룰 때 Chunkdot의 이점을 확인하게 될 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 100k 벡터 임베딩 테스트\n\n10천 개의 가상 코드 방식으로는 우리가 이긴다고 하지만, 이제는 문서 임베딩 벡터를 중형 규모의 RAG 응용 프로그램과 유사한 100K 벡터까지 늘려보겠습니다.\n\n각 방법에 대한 계산 시간을 계산해 봅시다. 이번에는 벡터의 수가 매우 많기 때문에 계산을 여러 번 수행할 필요가 없으므로 times 매개변수를 1로 설정하여 코드를 한 번 실행합니다.\n\n```js\ndoc_embeddings = np.random.randn(100000, 1536) # 100K 문서 임베딩 (1536 차원)\n\nuser_query = np.random.rand(1,1536) # 사용자 쿼리 (1536 차원)\n\ntop_indices = 1 # 반환할 상위 인덱스 수\n\nmax_memory = 5E9 # 최대 메모리를 5GB로 설정\n\ntimes = 1 # 함수를 실행할 횟수\n\n# 함수 실행 시간 계산\ncalculate_execution_time(user_query, doc_embeddings, top_indices, max_memory, times)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n100,000개의 문서 임베딩, 차원이 1536인 경우, 두 알고리즘을 한 번씩 실행하여 비교한 결과는 다음과 같습니다:\n\n![Comparison Chart](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_3.png)\n\nChunkdot은 의사코드에 비해 거의 절반의 시간이 소요됩니다. 이제 Chunkdot의 유망한 영향을 확인하고 있습니다.\n\n1백만 벡터 임베딩을 위한 테스트 중입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n백만 개의 임베딩을 다루는 작업 중에, 첫 번째로 확인해야 할 사항은 문서 임베딩 벡터가 얼마나 많은 메모리를 차지하는지입니다.\n\n```js\n# 1 백만 개의 문서 임베딩 (1536 차원)\ndoc_embeddings = np.random.randn(1000000, 1536)\n\n# 사용자 쿼리 (1536 차원)\nuser_query = np.random.rand(1, 1536)\n\n# doc_embeddings와 user_query 임베딩의 메모리 크기 확인\nprint(doc_embeddings.nbytes / (1024 * 1024 * 1024),\n      user_query.nbytes / (1024 * 1024))\n```\n\n![이미지](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_4.png)\n\n저희 문서 임베딩은 대략 12GB를 차지합니다. 남은 공간을 확인해 보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_5.png)\n\n17GB의 사용 가능한 메모리가 있습니다. 어떠한 메모리 오류도 피하기 위해 max_memory 매개변수의 안전한 값을 12GB로 설정할 것입니다. 결과를 확인해 봅시다.\n\n```js\n# 100만 개의 문서 임베딩 (1536 차원)\ndoc_embeddings = np.random.randn(1000000, 1536)\n\n# 사용자 쿼리 (1536 차원)\nuser_query = np.random.rand(1, 1536)\n\ntop_indices = 1 # 가져올 상위 인덱스 수\n\nmax_memory = 12E9 # 최대 메모리 설정 --- 12GB ---\n\ntimes = 1 # 함수를 실행할 횟수\n\n# 함수 실행 시간 계산하기\ncalculate_execution_time(user_query, doc_embeddings, top_indices, max_memory, times)\n```\n\n![image](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_6.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nChunkDot은 실제로 계산을 효과적으로 줄입니다. 진지한 RAG 앱을 만들려면 적어도 백만 개의 쿼리부터 시작하는 것이 좋습니다. 높은 차원의 임베딩 모델로 작업을하는 경우 4000까지 증가합니다. 이 접근 방식은 훨씬 더 효율적이어질 것입니다.\n\n# 확장성 영향 시각화\n\n문서 임베딩 벡터 수를 증가시키는 영향을 시각화해 보겠습니다. 시작은 10,000부터 매우 큰 수까지입니다.\n\n![이미지](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_7.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저는 세 가지 방법을 그래프로 나타내었고, 문서 임베딩의 수를 늘릴 때 Chunkdot이 모든 방법 중에서 가장 우수하다는 것을 확인했습니다. 이제 임베딩 벡터의 차원이 계산 시간에 어떤 영향을 미치는지 살펴보겠습니다.\n\n![Embedding Vectors Computation Time](/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_8.png)\n\n벡터의 차원을 늘리면서 10만 개의 문서를 사용했고, 문서 수를 늘릴 때 관찰한 현상과 동일한 결과를 얻었습니다.\n\n# Chunkdot의 특징\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nChunkdot에는 진행 표시 막대를 표시할 수 있는 기능이 있어서 얼마만큼의 계산이 남았는지 추적할 수 있습니다.\n\n```js\ndoc_embeddings = np.random.randn(100000, 1536) # 100K document embeddings (1536 dim)\n\nuser_query = np.random.rand(1,1536) # user query (1536 dim)\n\ntop_indices = 100 # 검색할 상위 인덱스 수\n\nmax_memory = 5E9 # 최대 메모리를 5GB로 설정\n\n# 진행 표시 막대와 함께\noutput_array = cosine_similarity_top_k(user_query, doc_embeddings,\n                        top_k=top_indices,\n                        show_progress=True)\n```\n\n![진행 표시 막대](https://miro.medium.com/v2/resize:fit:1400/1*3A2KQ9fDvAA-VfQNKjphJw.gif)\n\nChunkdot의 출력은 희소 행렬이며, 다음을 사용하여 배열로 변환할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 출력 변환\n\noutput_array.toarray()\n\nChunkdot은 문서 임베딩에 대해서만 사용할 수 있습니다. 이는 각 문서 임베딩 요소에 대해 가장 유사한 상위 k개 요소를 반환할 것입니다.\n\n# 총 5개의 문서 임베딩\n\nembeddings = np.random.randn(5, 256)\n\n# 각각의 상위 2개 가장 유사한 항목 인덱스 반환\n\ncosine_similarity_top_k(embeddings, top_k=2).toarray()\n\n### 출력\n\narray([[1.        , 0.        , 0.        , 0.        , 0.09924064],\n       [0.        , 1.        , 0.        , 0.09935381, 0.        ],\n       [0.02358785, 0.        , 1.        , 0.        , 0.        ],\n       [0.        , 0.09935381, 0.        , 1.        , 0.        ],\n       [0.09924064, 0.        , 0.        , 0.        , 1.]]\n\n### 출력\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n비슷한 방법으로 top_k 매개변수에 음수 값을 제공하여 가장 다른 항목을 반환할 수도 있습니다.\n\n```js\n# 총 5개의 문서 임베딩\nembeddings = np.random.randn(5, 256)\n\n# 각 항목의 가장 다른 상위 2개 항목 인덱스 반환\n# Top_K = -2\ncosine_similarity_top_k(embeddings, top_k=-2).toarray()\n```\n\n```js\n### 출력 ###\narray([[ 0.        ,  0.        , -0.04357524,  0.        , -0.05118288],\n       [ 0.        ,  0.        ,  0.        ,  0.01619543, -0.01836534],\n       [-0.04357524,  0.        ,  0.        , -0.02466613,  0.        ],\n       [ 0.        ,  0.01619543, -0.02466613,  0.        ,  0.        ],\n       [-0.05118288, -0.01836534,  0.        ,  0.        ,  0.        ]])\n### 출력 ###\n```\n\n당신의 상황이 아닐 수도 있지만, 1만 차원까지의 희소한 임베딩을 처리하는 경우 density 매개변수를 사용하여 계산을 더 효율적으로 줄일 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n# 희소 임베딩 생성을 위해\nfrom scipy import sparse\n\n# 각각 10,000 차원을 가진 100,000개의 문서로 이루어진 희소 매트릭스 생성\n# 밀도를 0.005로 정의\n임베딩 = sparse.rand(100000, 10000, density=0.005)\n\n# 시스템의 모든 메모리 사용\n코사인 유사도 상위 k(embeddings, top_k=50)\n```\n\n# 다음 단계\n\nChunkdot 알고리즘이 어떻게 작동하는지 알고 싶다면, 저자의 멋진 블로그를 확인해보세요. Chunkdot의 가장 큰 장점 중 하나는 CPU 코어에서 작동한다는 것입니다. 앞으로, GPU 지원을 통합할 계획이 있으며, 이는 연산에 소요되는 시간을 크게 줄일 것입니다. 로컬 환경에 충분한 RAM이 없는 경우에는 Kaggle이나 GitHub Codespaces 같은 플랫폼을 활용할 수 있습니다. 이러한 클라우드 CPU 코어와 RAM은 GPU 비용에 비해 매우 저렴합니다. Chunkdot가 어떻게 작동하는지를 잘 설명한 공식 GitHub 저장소와 블로그도 꼭 확인해보세요.\n","ogImage":{"url":"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_0.png"},"coverImage":"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_0.png","tag":["Tech"],"readingTime":17},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e가장 큰 문제 중 하나는 RAG 어플리케이션의 연산 검색 시간입니다. 1조 개의 임베딩 벡터 기록이 있는 벡터 데이터베이스가 있다고 상상해보세요. 사용자 쿼리를 1조 개의 벡터와 일치시키려고 하면 올바른 정보를 검색하는 데 1분 이상이 걸릴 것입니다.\u003c/p\u003e\n\u003cp\u003e시간을 단축하기 위해서는 사용자 쿼리 임베딩 벡터와 벡터 데이터베이스에 저장된 백만, 십억 또는 심지어 1조 개의 다른 임베딩 벡터 사이의 코사인 유사도를 계산하는 효율적인 방법을 찾아야 합니다.\u003c/p\u003e\n\u003cp\u003eMIT 라이선스 하에 Chunkdot은 밀집(dense) 및 희소(sparse) 행렬에 대한 멀티 스레드 행렬 곱셈을 제공하기 위해 특별히 설계되었습니다. Numba를 사용하여 계산을 가속화하며 항목 행렬 표현(임베딩)을 세분화하여 대규모 항목에 대한 K개의 가장 유사한 항목을 계산하는 데 적합합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eChunkdot GitHub 저장소\u003c/p\u003e\n\u003cp\u003eHuggingFace에는 Qdrant의 이 데이터셋과 같이 백만 개 이상의 엔트리의 임베딩 벡터를 제공하는 다양한 데이터셋이 많이 있습니다. Chunkdot 성능을 테스트하는 데 사용할 수 있습니다. 그러나 세부 성능 측정을 위해 우리는 NumPy 라이브러리를 사용하여 여러 차원의 임의의 임베딩 벡터를 생성할 것입니다.\u003c/p\u003e\n\u003cp\u003eChunkdot의 접근 방식과 코사인 유사도의 의사 코드를 비교할 것이며, 크기와 차원을 증가시킴으로써 성능이 어떻게 영향을 받는지 관찰할 것입니다. 이 작업에는 일관성을 보장하기 위해 Kaggle (GPU 없음) 노트북을 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e본 블로그의 모든 코드는 제 GitHub 저장소에서 확인할 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e목차\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e준비 단계 설정\u003c/li\u003e\n\u003cli\u003e의사 코드 알고리즘 코딩\u003c/li\u003e\n\u003cli\u003eChunkdot 알고리즘 코딩\u003c/li\u003e\n\u003cli\u003e계산 시간 함수 코딩\u003c/li\u003e\n\u003cli\u003e10k 벡터 임베딩에 대한 테스트\u003c/li\u003e\n\u003cli\u003e100k 벡터 임베딩에 대한 테스트\u003c/li\u003e\n\u003cli\u003e1 백만 벡터 임베딩에 대한 테스트\u003c/li\u003e\n\u003cli\u003e확장성 영향 시각화\u003c/li\u003e\n\u003cli\u003eChunkdot의 특징\u003c/li\u003e\n\u003cli\u003e다음에 할 일\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e준비 단계 설정\u003c/h1\u003e\n\u003cp\u003eChunkdot을 설치하려면 다른 라이브러리와 마찬가지인 유사한 설치 과정이 필요합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003echunkdot 설치하기\u003c/h1\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003epip install chunkdot\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e무엇이든 실행하기 전에 먼저 Kaggle 환경에서 사용 가능한 메모리를 확인해야 합니다.\u003c/p\u003e\n\u003ch1\u003e사용 가능한 메모리 확인하기\u003c/h1\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e!free -h\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_1.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eChunkdot 에게 사용 가능한 메모리를 확인하는 것은 매우 중요합니다. 벡터 데이터베이스 크기가 증가할수록 계산 메모리도 증가합니다. 사용 가능한 메모리를 초과하지 않도록 하려면 하드웨어의 남은 메모리를 모니터링하는 것이 중요합니다. 제 경우에는 버퍼/캐시를 제외하고 25GB의 여유 공간이 있습니다.\u003c/p\u003e\n\u003cp\u003e필요한 라이브러리를 가져오겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 행렬을 생성하기 위한 라이브러리\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\n# chunkdot에서 코사인 유사도 모듈을 가져옵니다.\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e chunkdot \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cosine_similarity_top_k\n\n# 계산 시간을 측정하기 위한 라이브러리\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e timeit\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e코딩 의사 코드 알고리즘\u003c/h1\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e먼저 데이터베이스나 로컬에 저장된 수백만 벡터들과 사용자 쿼리 벡터 간의 코사인 유사도를 계산하는 의사 코드 알고리즘을 만들어 볼게요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003ecosine_pseudocode\u003c/span\u003e(query_v, doc_v, num_indices):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    Embedding 벡터와 쿼리 벡터 간의 가장 높은 코사인 유사도 값을 가진 인덱스를 반환합니다.\n\n    매개변수:\n        query_v (numpy.ndarray): 쿼리 벡터.\n        doc_v (list of numpy.ndarray): Embedding 벡터의 목록.\n        num_indices (int): 반환할 상위 인덱스 개수.\n\n    반환값:\n        list of int: 가장 높은 코사인 유사도 값을 가진 인덱스들.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    cosine_similarities = []  # 코사인 유사도를 저장할 빈 리스트를 초기화합니다.\n\n    query_norm = np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enorm\u003c/span\u003e(query_v)  # 쿼리 벡터의 노름을 계산합니다.\n\n    # 리스트에 있는 각 문서의 임베딩 벡터에 대해 반복합니다.\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e vec \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003edoc_v\u003c/span\u003e:\n        dot_product = np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(vec, query_v.\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e)  # 임베딩 벡터와 쿼리 벡터 간의 내적을 계산합니다.\n        embedding_norm = np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enorm\u003c/span\u003e(vec)  # 임베딩 벡터의 노름을 계산합니다.\n        cosine_similarity = dot_product / (embedding_norm * query_norm)  # 코사인 유사도 계산\n        cosine_similarities.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(cosine_similarity)  # 코사인 유사도를 리스트에 추가합니다.\n\n    cosine_similarities = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(cosine_similarities)  # 리스트를 넘파이 배열로 변환합니다.\n\n    # 배열을 내림차순으로 정렬합니다.\n    sorted_array = \u003cspan class=\"hljs-title function_\"\u003esorted\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(cosine_similarities)), key=lambda \u003cspan class=\"hljs-attr\"\u003ei\u003c/span\u003e: cosine_similarities[i], reverse=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n    # 상위 num_indices 값을 가져옵니다.\n    top_indices = sorted_array[:num_indices]\n\n    # 가장 높은 코사인 유사도 값을 가진 인덱스를 반환합니다.\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e top_indices\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 코사인 유사도 함수는 NumPy를 제외한 모든 라이브러리와 독립적으로 작동하며 세 가지 입력을 받습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003equery_v는 사용자 쿼리의 임베딩 벡터입니다.\u003c/li\u003e\n\u003cli\u003edoc_v는 어딘가에 저장된 문서들의 임베딩 벡터들입니다.\u003c/li\u003e\n\u003cli\u003enum_indices는 유사한 상위 k결과로부터 문서의 인덱스 번호입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e코딩 Chunkdot 알고리즘\u003c/h1\u003e\n\u003cp\u003e의사코드 알고리즘을 코딩했으니, 다음 단계는 Chunkdot 코사인 유사도 함수를 코딩하는 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003ecosine_chunkdot\u003c/span\u003e(query_v, doc_v, num_indices, max_memory):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    청크닷 라이브러리를 사용하여 코사인 유사도를 계산합니다.\n\n    매개변수:\n        query_v (numpy.ndarray): 쿼리 벡터.\n        doc_v (numpy.ndarray): 임베딩 벡터 목록.\n        num_indices (int): 검색할 상위 인덱스 수.\n        max_memory (float): 사용할 최대 메모리.\n\n    반환값:\n        numpy.ndarray: 상위 k 인덱스.\n    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n    # 코사인 유사도 계산\n    cosine_array = \u003cspan class=\"hljs-title function_\"\u003ecosine_similarity_top_k\u003c/span\u003e(embeddings=query_v, embeddings_right=doc_v,\n                                         top_k=num_indices, max_memory=max_memory)  # 청크닷을 사용하여 코사인 유사도 계산\n\n    # 상위 값의 인덱스 가져오기\n    top_indices = cosine_array.\u003cspan class=\"hljs-title function_\"\u003enonzero\u003c/span\u003e()[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\n    # 상위 유사 결과 반환\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e top_indices\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 Chunkdot 함수는 네 개의 입력을 받습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003equery_v은 사용자 쿼리의 임베딩 벡터를 나타냅니다.\u003c/li\u003e\n\u003cli\u003edoc_v는 어딘가에 저장된 문서의 임베딩 벡터들을 나타냅니다.\u003c/li\u003e\n\u003cli\u003enum_indices는 유사한 상위 k개 결과에 대한 문서의 인덱스 번호를 나타냅니다.\u003c/li\u003e\n\u003cli\u003emax_memory는 계산에 사용할 수 있는 사용 가능한 메모리를 바이트 단위로 나타냅니다. 예를 들어, 1E9는 1GB를 의미하며, 10E9는 10GB를 의미합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e두 함수를 샘플 데이터셋에서 테스트하여 출력을 관찰해보겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edoc_embeddings = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e) # \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e개의 문서 임베딩 (\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e차원)\n\nuser_query = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e) # \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e개의 사용자 쿼리 (\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e차원)\n\ntop_indices = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e # 검색할 상위 인덱스 수\n\nmax_memory = \u003cspan class=\"hljs-number\"\u003e5E9\u003c/span\u003e # 사용할 최대 메모리 (5GB)\n\n# 의사코드를 사용하여 가장 높은 코사인 유사도 값을 갖는 인덱스를 검색\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"의사코드를 사용한 상위 인덱스:\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003ecosine_pseudocode\u003c/span\u003e(user_query, doc_embeddings, top_indices))\n\n# chunkdot을 사용하여 가장 높은 코사인 유사도 값을 갖는 인덱스를 검색\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"chunkdot을 사용한 상위 인덱스:\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003ecosine_chunkdot\u003c/span\u003e(user_query, doc_embeddings, top_indices, max_memory))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e### 출력 ###\n의사코드를 사용한 상위 인덱스: [\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e]\nchunkdot을 사용한 상위 인덱스: [\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e]\n### 출력 ###\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e문서 임베딩에 대한 유사한 항목의 인덱스를 최고의 코사인 유사도를 기반으로 한 항목만 반환한다는 것을 의미하는 top_indices 매개변수를 1로 설정했습니다. 메모리 사용량을 5E9로 설정했는데, 이는 5GB와 동일합니다. 두 함수 모두 동일한 인덱스 4를 반환하여 두 함수를 정확하게 코딩했음을 나타냅니다.\u003c/p\u003e\n\u003ch1\u003e코딩 계산 시간 함수\u003c/h1\u003e\n\u003cp\u003e또한 이러한 두 함수에 의해 소요된 계산 시간을 측정할 수 있는 타이밍 함수를 만들어야 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 소요 시간 계산\ndef \u003cspan class=\"hljs-title function_\"\u003ecalculate_execution_time\u003c/span\u003e(query_v, doc_v, num_indices, max_memory, times):\n\n    # 의사코드 함수 실행에 걸리는 시간 계산\n    pseudocode_time = \u003cspan class=\"hljs-title function_\"\u003eround\u003c/span\u003e(timeit.\u003cspan class=\"hljs-title function_\"\u003etimeit\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003ecosine_pseudocode\u003c/span\u003e(query_v, doc_v, num_indices), number=times), \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n\n    # chunkdot 함수 실행에 걸리는 시간 계산\n    chunkdot_time = \u003cspan class=\"hljs-title function_\"\u003eround\u003c/span\u003e(timeit.\u003cspan class=\"hljs-title function_\"\u003etimeit\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003elambda\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003ecosine_chunkdot\u003c/span\u003e(query_v, doc_v, num_indices, max_memory), number=times), \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n\n    # 소요 시간 출력\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"의사코드 함수에 걸리는 시간:\"\u003c/span\u003e, pseudocode_time, \u003cspan class=\"hljs-string\"\u003e\"초\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"chunkdot 함수에 걸리는 시간:\"\u003c/span\u003e, chunkdot_time, \u003cspan class=\"hljs-string\"\u003e\"초\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e우리는 이미 이 함수로 전달되는 매개변수를 검토했습니다. 여기서 새로운 매개변수는 times입니다. 이 매개변수는 코드를 몇 번 실행할지를 함수에 알려줍니다. 더 큰 규모에서 Chunkdot 성능을 테스트해 봅시다.\u003c/p\u003e\n\u003ch1\u003e10,000 벡터 임베딩 테스트\u003c/h1\u003e\n\u003cp\u003e우리는 합리적인 수의 문서 임베딩인 10000개로 시작할 것입니다. 이는 소규모 도메인별 RAG 애플리케이션과 비교 가능합니다. 각 임베딩 벡터의 차원을 1536으로 설정했습니다. 이는 OpenAI 임베딩 모델 텍스트 임베딩-3-small과 동등합니다.\u003c/p\u003e\n\u003cp\u003e각 접근 방식의 계산 시간을 계산하기 위해 각각 100번 실행하여 효율성을 테스트해 봅시다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003edoc_embeddings = np.random.randn(\u003cspan class=\"hljs-number\"\u003e10000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e)  \u003cspan class=\"hljs-comment\"\u003e# 10K 문서 임베딩 (1536 차원)\u003c/span\u003e\n\nuser_query = np.random.rand(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e)  \u003cspan class=\"hljs-comment\"\u003e# 사용자 쿼리 (1536 차원)\u003c/span\u003e\n\ntop_indices = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e  \u003cspan class=\"hljs-comment\"\u003e# 검색할 상위 인덱스 수\u003c/span\u003e\n\nmax_memory = \u003cspan class=\"hljs-number\"\u003e5E9\u003c/span\u003e  \u003cspan class=\"hljs-comment\"\u003e# 최대 메모리를 5GB로 설정\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# 함수 실행에 소요된 시간을 계산합니다.\u003c/span\u003e\ncalculate_execution_time(user_query, doc_embeddings, top_indices, max_memory, \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e10,000개의 문서 임베딩, 1536 차원을 가진 두 알고리즘을 100번 실행한 결과는 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_2.png\" alt=\"그래프\"\u003e\u003c/p\u003e\n\u003cp\u003eChunkdot은 유사도 비교 코드와 비교해 더 많은 시간이 소요됩니다. 이는 Chunkdot이 먼저 청크를 생성하고 각 청크에서 계산을 수행한 후 병합하기 때문입니다. 따라서 이 소규모 예제에 대해서는 적합한 해결책이 아닐 수 있습니다. 그러나 나중에 더 큰 예제를 다룰 때 Chunkdot의 이점을 확인하게 될 것입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e100k 벡터 임베딩 테스트\u003c/h1\u003e\n\u003cp\u003e10천 개의 가상 코드 방식으로는 우리가 이긴다고 하지만, 이제는 문서 임베딩 벡터를 중형 규모의 RAG 응용 프로그램과 유사한 100K 벡터까지 늘려보겠습니다.\u003c/p\u003e\n\u003cp\u003e각 방법에 대한 계산 시간을 계산해 봅시다. 이번에는 벡터의 수가 매우 많기 때문에 계산을 여러 번 수행할 필요가 없으므로 times 매개변수를 1로 설정하여 코드를 한 번 실행합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edoc_embeddings = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e) # 100K 문서 임베딩 (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e 차원)\n\nuser_query = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e) # 사용자 쿼리 (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e 차원)\n\ntop_indices = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e # 반환할 상위 인덱스 수\n\nmax_memory = \u003cspan class=\"hljs-number\"\u003e5E9\u003c/span\u003e # 최대 메모리를 5GB로 설정\n\ntimes = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e # 함수를 실행할 횟수\n\n# 함수 실행 시간 계산\n\u003cspan class=\"hljs-title function_\"\u003ecalculate_execution_time\u003c/span\u003e(user_query, doc_embeddings, top_indices, max_memory, times)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e100,000개의 문서 임베딩, 차원이 1536인 경우, 두 알고리즘을 한 번씩 실행하여 비교한 결과는 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_3.png\" alt=\"Comparison Chart\"\u003e\u003c/p\u003e\n\u003cp\u003eChunkdot은 의사코드에 비해 거의 절반의 시간이 소요됩니다. 이제 Chunkdot의 유망한 영향을 확인하고 있습니다.\u003c/p\u003e\n\u003cp\u003e1백만 벡터 임베딩을 위한 테스트 중입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e백만 개의 임베딩을 다루는 작업 중에, 첫 번째로 확인해야 할 사항은 문서 임베딩 벡터가 얼마나 많은 메모리를 차지하는지입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e 백만 개의 문서 임베딩 (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e 차원)\ndoc_embeddings = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1000000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e)\n\n# 사용자 쿼리 (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e 차원)\nuser_query = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e)\n\n# doc_embeddings와 user_query 임베딩의 메모리 크기 확인\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(doc_embeddings.\u003cspan class=\"hljs-property\"\u003enbytes\u003c/span\u003e / (\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e * \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e * \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e),\n      user_query.\u003cspan class=\"hljs-property\"\u003enbytes\u003c/span\u003e / (\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e * \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e저희 문서 임베딩은 대략 12GB를 차지합니다. 남은 공간을 확인해 보세요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e17GB의 사용 가능한 메모리가 있습니다. 어떠한 메모리 오류도 피하기 위해 max_memory 매개변수의 안전한 값을 12GB로 설정할 것입니다. 결과를 확인해 봅시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e만 개의 문서 임베딩 (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e 차원)\ndoc_embeddings = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1000000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e)\n\n# 사용자 쿼리 (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e 차원)\nuser_query = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e)\n\ntop_indices = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e # 가져올 상위 인덱스 수\n\nmax_memory = \u003cspan class=\"hljs-number\"\u003e12E9\u003c/span\u003e # 최대 메모리 설정 --- 12GB ---\n\ntimes = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e # 함수를 실행할 횟수\n\n# 함수 실행 시간 계산하기\n\u003cspan class=\"hljs-title function_\"\u003ecalculate_execution_time\u003c/span\u003e(user_query, doc_embeddings, top_indices, max_memory, times)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eChunkDot은 실제로 계산을 효과적으로 줄입니다. 진지한 RAG 앱을 만들려면 적어도 백만 개의 쿼리부터 시작하는 것이 좋습니다. 높은 차원의 임베딩 모델로 작업을하는 경우 4000까지 증가합니다. 이 접근 방식은 훨씬 더 효율적이어질 것입니다.\u003c/p\u003e\n\u003ch1\u003e확장성 영향 시각화\u003c/h1\u003e\n\u003cp\u003e문서 임베딩 벡터 수를 증가시키는 영향을 시각화해 보겠습니다. 시작은 10,000부터 매우 큰 수까지입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e저는 세 가지 방법을 그래프로 나타내었고, 문서 임베딩의 수를 늘릴 때 Chunkdot이 모든 방법 중에서 가장 우수하다는 것을 확인했습니다. 이제 임베딩 벡터의 차원이 계산 시간에 어떤 영향을 미치는지 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings_8.png\" alt=\"Embedding Vectors Computation Time\"\u003e\u003c/p\u003e\n\u003cp\u003e벡터의 차원을 늘리면서 10만 개의 문서를 사용했고, 문서 수를 늘릴 때 관찰한 현상과 동일한 결과를 얻었습니다.\u003c/p\u003e\n\u003ch1\u003eChunkdot의 특징\u003c/h1\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eChunkdot에는 진행 표시 막대를 표시할 수 있는 기능이 있어서 얼마만큼의 계산이 남았는지 추적할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edoc_embeddings = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e) # 100K \u003cspan class=\"hljs-variable language_\"\u003edocument\u003c/span\u003e embeddings (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e dim)\n\nuser_query = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e) # user query (\u003cspan class=\"hljs-number\"\u003e1536\u003c/span\u003e dim)\n\ntop_indices = \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e # 검색할 상위 인덱스 수\n\nmax_memory = \u003cspan class=\"hljs-number\"\u003e5E9\u003c/span\u003e # 최대 메모리를 5GB로 설정\n\n# 진행 표시 막대와 함께\noutput_array = \u003cspan class=\"hljs-title function_\"\u003ecosine_similarity_top_k\u003c/span\u003e(user_query, doc_embeddings,\n                        top_k=top_indices,\n                        show_progress=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*3A2KQ9fDvAA-VfQNKjphJw.gif\" alt=\"진행 표시 막대\"\u003e\u003c/p\u003e\n\u003cp\u003eChunkdot의 출력은 희소 행렬이며, 다음을 사용하여 배열로 변환할 수 있습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e출력 변환\u003c/h1\u003e\n\u003cp\u003eoutput_array.toarray()\u003c/p\u003e\n\u003cp\u003eChunkdot은 문서 임베딩에 대해서만 사용할 수 있습니다. 이는 각 문서 임베딩 요소에 대해 가장 유사한 상위 k개 요소를 반환할 것입니다.\u003c/p\u003e\n\u003ch1\u003e총 5개의 문서 임베딩\u003c/h1\u003e\n\u003cp\u003eembeddings = np.random.randn(5, 256)\u003c/p\u003e\n\u003ch1\u003e각각의 상위 2개 가장 유사한 항목 인덱스 반환\u003c/h1\u003e\n\u003cp\u003ecosine_similarity_top_k(embeddings, top_k=2).toarray()\u003c/p\u003e\n\u003ch3\u003e출력\u003c/h3\u003e\n\u003cp\u003earray([[1.        , 0.        , 0.        , 0.        , 0.09924064],\n[0.        , 1.        , 0.        , 0.09935381, 0.        ],\n[0.02358785, 0.        , 1.        , 0.        , 0.        ],\n[0.        , 0.09935381, 0.        , 1.        , 0.        ],\n[0.09924064, 0.        , 0.        , 0.        , 1.]]\u003c/p\u003e\n\u003ch3\u003e출력\u003c/h3\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e비슷한 방법으로 top_k 매개변수에 음수 값을 제공하여 가장 다른 항목을 반환할 수도 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 총 \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e개의 문서 임베딩\nembeddings = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e)\n\n# 각 항목의 가장 다른 상위 \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e개 항목 인덱스 반환\n# \u003cspan class=\"hljs-title class_\"\u003eTop\u003c/span\u003e_K = -\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n\u003cspan class=\"hljs-title function_\"\u003ecosine_similarity_top_k\u003c/span\u003e(embeddings, top_k=-\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003etoarray\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e### 출력 ###\n\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([[ \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        , -\u003cspan class=\"hljs-number\"\u003e0.04357524\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        , -\u003cspan class=\"hljs-number\"\u003e0.05118288\u003c/span\u003e],\n       [ \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.01619543\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e0.01836534\u003c/span\u003e],\n       [-\u003cspan class=\"hljs-number\"\u003e0.04357524\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        , -\u003cspan class=\"hljs-number\"\u003e0.02466613\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ],\n       [ \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.01619543\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e0.02466613\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ],\n       [-\u003cspan class=\"hljs-number\"\u003e0.05118288\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e0.01836534\u003c/span\u003e,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ,  \u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e        ]])\n### 출력 ###\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e당신의 상황이 아닐 수도 있지만, 1만 차원까지의 희소한 임베딩을 처리하는 경우 density 매개변수를 사용하여 계산을 더 효율적으로 줄일 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 희소 임베딩 생성을 위해\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e scipy \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sparse\n\n# 각각 \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e000\u003c/span\u003e 차원을 가진 \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e000\u003c/span\u003e개의 문서로 이루어진 희소 매트릭스 생성\n# 밀도를 \u003cspan class=\"hljs-number\"\u003e0.005\u003c/span\u003e로 정의\n임베딩 = sparse.\u003cspan class=\"hljs-title function_\"\u003erand\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e100000\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10000\u003c/span\u003e, density=\u003cspan class=\"hljs-number\"\u003e0.005\u003c/span\u003e)\n\n# 시스템의 모든 메모리 사용\n코사인 유사도 상위 \u003cspan class=\"hljs-title function_\"\u003ek\u003c/span\u003e(embeddings, top_k=\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e다음 단계\u003c/h1\u003e\n\u003cp\u003eChunkdot 알고리즘이 어떻게 작동하는지 알고 싶다면, 저자의 멋진 블로그를 확인해보세요. Chunkdot의 가장 큰 장점 중 하나는 CPU 코어에서 작동한다는 것입니다. 앞으로, GPU 지원을 통합할 계획이 있으며, 이는 연산에 소요되는 시간을 크게 줄일 것입니다. 로컬 환경에 충분한 RAM이 없는 경우에는 Kaggle이나 GitHub Codespaces 같은 플랫폼을 활용할 수 있습니다. 이러한 클라우드 CPU 코어와 RAM은 GPU 비용에 비해 매우 저렴합니다. Chunkdot가 어떻게 작동하는지를 잘 설명한 공식 GitHub 저장소와 블로그도 꼭 확인해보세요.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-27-100xFasterScalingYourRAGAppforBillionsofEmbeddings"},"buildId":"YUMR4jSyk_WlOHHc7UfOk","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>