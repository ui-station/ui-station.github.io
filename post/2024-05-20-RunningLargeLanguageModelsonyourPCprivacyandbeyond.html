<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond" data-gatsby-head="true"/><meta name="twitter:title" content="자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-20 21:36" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_buildManifest.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 20, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png" alt="Running LLM on Your PC: Privacy and Beyond"></p>
<p>이 첫 번째 기사를 통해, 인터넷 연결 없이 PC에 로컬로 내 LLM을 설치하고 설정하고 실행하는 여정을 문서화할 일련의 기사를 시작합니다.</p>
<p>내 PC에서 이 프로젝트를 구축하고 개인적으로 사용하기로 결정한 이유는 이 기사에서 확인할 수 있습니다.</p>
<p>Chat-GPT 3.5와 비슷하거나 심지어 동일한 기능을 가지려고 합니다. 이것은 이루기 어렵게 들릴 수 있지만, 기술은 매일 발전하고 있으며, 적절한 하드웨어, 소프트웨어 및 LLM의 조합을 통해 가능할 것으로 생각합니다. 이 과정에서 취하는 단계, 최적화 및 조정, 실수와 성공, 사용하는 소스, 그리고 향후 하드웨어 업그레이드에 대해 공유할 것입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 여정이 끝날 때 모든 기사들은 사용자 안내서로 요약될 것이며, 저는 이를 무료로 발행하여 누구나 이 경험에서 혜택을 받고 직접 시도해볼 수 있도록 할 것입니다. 물론 이 과정에서 모든 의견, 제안 및 지침은 환영합니다.</p>
<p>이 여정이 시작되기 전에, 로컬 및 프라이빗 LLM을 위한 기반이 될 하드웨어와 소프트웨어인 나의 출발점을 공유할 것입니다.</p>
<p>나의 초기 컴퓨터 구성은 다음과 같습니다:</p>
<ul>
<li>CPU: AMD Ryzen 9 7950X3D, 16코어 @ 4.20 GHz</li>
<li>RAM: Corsair VENGEANCE DDR5, 64GB (2 X 32 GB)</li>
<li>HDD: 삼성 990 EVO NVMe™ M.2 SSD (1 TB 각 2개)</li>
<li>비디오: Nvidia GeForce RTX 4070, 12GB (평택 에디션)</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 설정의 소프트웨어 구성 요소는 다음과 같습니다:</p>
<ul>
<li>Windows 11 Pro, ver. 23H2 (주 OS)</li>
<li>Ubuntu 22.04 (Windows PowerShell 콘솔에 설치 및 실행됨)</li>
<li>Ollama (LLM을 위해)</li>
<li>Docker (컨테이너용)</li>
<li>Python 3.11 (데이터 세트 및 일부 라이브러리 및 구성 요소 용)</li>
<li>Visual Studio Code (필요할 때 코딩을 위해)</li>
</ul>
<p>Windows PowerShell 콘솔에 Ubuntu를 설치해야 합니다. Windows에서 "시작" 메뉴를 열고 "실행"을 입력하고 새 창에서 "PowerShell"을 입력한 후 Enter 키를 누르십시오. 콘솔에서 다음 명령을 입력하십시오:</p>
<pre><code class="hljs language-js">wsl --install
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>설치 프로세스를 시작하고, 완료되면 Linux용 사용자 이름과 비밀번호를 입력하라는 메시지가 표시됩니다. 성공적으로 설치되면 다음과 같은 화면을 볼 수 있습니다:</p>
<p><img src="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_1.png" alt="이미지"></p>
<p>설치가 끝나면 Ubuntu 배포판을 업데이트하고 업그레이드하는 것과 같은 좋은 실천 동작들을 수행해야 합니다. 다음 명령어를 사용하여 이를 실행할 수 있습니다(사용자 이름과 비밀번호가 요청될 것입니다):</p>
<pre><code class="hljs language-bash">sudo apt update
sudo apt upgrade -y
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이제 여러 추가 패키지가 설치됩니다. 그리고 이로써 귀하의 로컬 PC에 Ubuntu를 설치하는 작업이 완료되었습니다.</p>
<p>다음은 Ollama입니다. 이를 통해 LLMs를 설치하고 실행할 수 있습니다.</p>
<p>참고: Ollama를 설치하기 전에 이미 Linux 환경에 있는지 확인하세요. Ubuntu를 시작하려면 다음 명령을 입력하십시오:</p>
<pre><code class="hljs language-js">wsl -d <span class="hljs-title class_">Ubuntu</span>
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>설치는 콘솔에서 다시 실행되며 다음 명령어로 실행할 수 있습니다:</p>
<pre><code class="hljs language-js">curl -fsSL <span class="hljs-attr">https</span>:<span class="hljs-comment">//ollama.com/install.sh | sh</span>
</code></pre>
<p>성공적으로 설치되면 이 화면이 표시되어야 합니다. 여기서 중요한 점은 프로세스가 완료된 후 비디오 카드가 인식되어야 한다는 것입니다 (빨간색으로 표시됨).</p>
<img src="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_2.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>비디오 카드는 NVIDIA에서 제공하는 것이 좋습니다. 큰 언어 모델 작업에 있어서 결정적인 부분입니다. 머신에 설치된(또는 적합한) 카드가 없는 경우 모델은 CPU 전력을 사용하게 되어 작업이 훨씬 느리고 정확도가 떨어지게 됩니다.</p>
<p>Ollama를 설치한 후에는 하나 이상의 대형 언어 모델(LLM)을 설치해야 합니다. 저는 llama2:13b를 선택하여 Meta에서 제공하는 130억 파라미터 모델을 설치했습니다. 이 모델은 제 개인 GPT에서 사용하고 제 데이터로 훈련시킬 모델입니다. 모델 크기는 7.4GB이므로 제 12GB (RTX 4070 FE) 비디오 카드에 적합합니다.</p>
<p><img src="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_3.png" alt="Running Large Language Models on your PC: Privacy and Beyond"></p>
<p>설치는 콘솔과 리눅스 환경에서 다시 진행됩니다. 아래 명령어로 실행할 수 있습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>오레카 풀라마 2:13b</p>
<p>모델을 다운로드하고 하드 드라이브에 저장하는 프로세스가 시작됩니다. 프로세스를 시작하기 전에 충분한 여유 공간이 있는지 확인하세요. 공간이 부족한 경우, llama:7b와 같은 더 작은 모델을 다운로드할 수도 있습니다. 이는 동일한 명령으로 수행할 수 있습니다: "llama:13b"를 "llama:7b"로 교체하면 됩니다. 7b 모델은 4.7GB이며, llama:13b 대신 사용할 수 있습니다.</p>
<p>성공적인 설치 후 다음 화면을 볼 수 있어야 합니다:</p>
<p><img src="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_4.png" alt="image"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>모델이 이미 하드 드라이브에 있으면 상호 작용을 시작할 수 있습니다. 다음 명령어로 콘솔에서 수행할 수 있어요:</p>
<pre><code class="hljs language-js">ollama run <span class="hljs-attr">llama</span>:13b
</code></pre>
<p>Enter 키를 누르면 다음 메시지가 표시됩니다:</p>
<p>메시지를 전송하세요 (도움말을 보려면 /?)</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이제 모델이 준비되어 입력을 받을 준비가 되었습니다. 리눅스 환경에서 모델을 실험하고 탐색하여 기능을 느끼고 전파를 연습할 수 있습니다. 상호 작용은 다음과 같아야 합니다:</p>
<p><img src="/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_5.png" alt="image"></p>
<p>이제 1단계인 리눅스 및 LLM 설치를 마치며 마무리합니다. 이제 인터넷 연결 없이도 로컬 PC에서 LLM을 실행하고 사용할 수 있습니다.</p>
<p>다음 글에서는 컨테이너를 위해 Docker를 설치하고 모델을 위한 웹 인터페이스를 설치하고 모델을 훈련 및 최적화하는 데 도움이 되는 기타 Python 라이브러리와 도구를 검토할 것입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다음에 또 봐요!</p>
<p>Stan</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"자신의 PC에서 대규모 언어 모델 실행하기 개인 정보 보호 및 그 이상","description":"","date":"2024-05-20 21:36","slug":"2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond","content":"\n![Running LLM on Your PC: Privacy and Beyond](/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png)\n\n이 첫 번째 기사를 통해, 인터넷 연결 없이 PC에 로컬로 내 LLM을 설치하고 설정하고 실행하는 여정을 문서화할 일련의 기사를 시작합니다.\n\n내 PC에서 이 프로젝트를 구축하고 개인적으로 사용하기로 결정한 이유는 이 기사에서 확인할 수 있습니다.\n\nChat-GPT 3.5와 비슷하거나 심지어 동일한 기능을 가지려고 합니다. 이것은 이루기 어렵게 들릴 수 있지만, 기술은 매일 발전하고 있으며, 적절한 하드웨어, 소프트웨어 및 LLM의 조합을 통해 가능할 것으로 생각합니다. 이 과정에서 취하는 단계, 최적화 및 조정, 실수와 성공, 사용하는 소스, 그리고 향후 하드웨어 업그레이드에 대해 공유할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 여정이 끝날 때 모든 기사들은 사용자 안내서로 요약될 것이며, 저는 이를 무료로 발행하여 누구나 이 경험에서 혜택을 받고 직접 시도해볼 수 있도록 할 것입니다. 물론 이 과정에서 모든 의견, 제안 및 지침은 환영합니다.\n\n이 여정이 시작되기 전에, 로컬 및 프라이빗 LLM을 위한 기반이 될 하드웨어와 소프트웨어인 나의 출발점을 공유할 것입니다.\n\n나의 초기 컴퓨터 구성은 다음과 같습니다:\n\n- CPU: AMD Ryzen 9 7950X3D, 16코어 @ 4.20 GHz\n- RAM: Corsair VENGEANCE DDR5, 64GB (2 X 32 GB)\n- HDD: 삼성 990 EVO NVMe™ M.2 SSD (1 TB 각 2개)\n- 비디오: Nvidia GeForce RTX 4070, 12GB (평택 에디션)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 설정의 소프트웨어 구성 요소는 다음과 같습니다:\n\n- Windows 11 Pro, ver. 23H2 (주 OS)\n- Ubuntu 22.04 (Windows PowerShell 콘솔에 설치 및 실행됨)\n- Ollama (LLM을 위해)\n- Docker (컨테이너용)\n- Python 3.11 (데이터 세트 및 일부 라이브러리 및 구성 요소 용)\n- Visual Studio Code (필요할 때 코딩을 위해)\n\nWindows PowerShell 콘솔에 Ubuntu를 설치해야 합니다. Windows에서 \"시작\" 메뉴를 열고 \"실행\"을 입력하고 새 창에서 \"PowerShell\"을 입력한 후 Enter 키를 누르십시오. 콘솔에서 다음 명령을 입력하십시오:\n\n```js\nwsl --install\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n설치 프로세스를 시작하고, 완료되면 Linux용 사용자 이름과 비밀번호를 입력하라는 메시지가 표시됩니다. 성공적으로 설치되면 다음과 같은 화면을 볼 수 있습니다:\n\n![이미지](/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_1.png)\n\n설치가 끝나면 Ubuntu 배포판을 업데이트하고 업그레이드하는 것과 같은 좋은 실천 동작들을 수행해야 합니다. 다음 명령어를 사용하여 이를 실행할 수 있습니다(사용자 이름과 비밀번호가 요청될 것입니다):\n\n```bash\nsudo apt update\nsudo apt upgrade -y\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 여러 추가 패키지가 설치됩니다. 그리고 이로써 귀하의 로컬 PC에 Ubuntu를 설치하는 작업이 완료되었습니다.\n\n다음은 Ollama입니다. 이를 통해 LLMs를 설치하고 실행할 수 있습니다.\n\n참고: Ollama를 설치하기 전에 이미 Linux 환경에 있는지 확인하세요. Ubuntu를 시작하려면 다음 명령을 입력하십시오:\n\n```js\nwsl -d Ubuntu\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n설치는 콘솔에서 다시 실행되며 다음 명령어로 실행할 수 있습니다:\n\n```js\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n성공적으로 설치되면 이 화면이 표시되어야 합니다. 여기서 중요한 점은 프로세스가 완료된 후 비디오 카드가 인식되어야 한다는 것입니다 (빨간색으로 표시됨).\n\n\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_2.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n비디오 카드는 NVIDIA에서 제공하는 것이 좋습니다. 큰 언어 모델 작업에 있어서 결정적인 부분입니다. 머신에 설치된(또는 적합한) 카드가 없는 경우 모델은 CPU 전력을 사용하게 되어 작업이 훨씬 느리고 정확도가 떨어지게 됩니다.\n\nOllama를 설치한 후에는 하나 이상의 대형 언어 모델(LLM)을 설치해야 합니다. 저는 llama2:13b를 선택하여 Meta에서 제공하는 130억 파라미터 모델을 설치했습니다. 이 모델은 제 개인 GPT에서 사용하고 제 데이터로 훈련시킬 모델입니다. 모델 크기는 7.4GB이므로 제 12GB (RTX 4070 FE) 비디오 카드에 적합합니다.\n\n![Running Large Language Models on your PC: Privacy and Beyond](/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_3.png)\n\n설치는 콘솔과 리눅스 환경에서 다시 진행됩니다. 아래 명령어로 실행할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n오레카 풀라마 2:13b\n\n모델을 다운로드하고 하드 드라이브에 저장하는 프로세스가 시작됩니다. 프로세스를 시작하기 전에 충분한 여유 공간이 있는지 확인하세요. 공간이 부족한 경우, llama:7b와 같은 더 작은 모델을 다운로드할 수도 있습니다. 이는 동일한 명령으로 수행할 수 있습니다: \"llama:13b\"를 \"llama:7b\"로 교체하면 됩니다. 7b 모델은 4.7GB이며, llama:13b 대신 사용할 수 있습니다.\n\n성공적인 설치 후 다음 화면을 볼 수 있어야 합니다:\n\n![image](/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델이 이미 하드 드라이브에 있으면 상호 작용을 시작할 수 있습니다. 다음 명령어로 콘솔에서 수행할 수 있어요:\n\n```js\nollama run llama:13b\n```\n\nEnter 키를 누르면 다음 메시지가 표시됩니다:\n\n메시지를 전송하세요 (도움말을 보려면 /?)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 모델이 준비되어 입력을 받을 준비가 되었습니다. 리눅스 환경에서 모델을 실험하고 탐색하여 기능을 느끼고 전파를 연습할 수 있습니다. 상호 작용은 다음과 같아야 합니다:\n\n![image](/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_5.png)\n\n이제 1단계인 리눅스 및 LLM 설치를 마치며 마무리합니다. 이제 인터넷 연결 없이도 로컬 PC에서 LLM을 실행하고 사용할 수 있습니다.\n\n다음 글에서는 컨테이너를 위해 Docker를 설치하고 모델을 위한 웹 인터페이스를 설치하고 모델을 훈련 및 최적화하는 데 도움이 되는 기타 Python 라이브러리와 도구를 검토할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음에 또 봐요!\n\nStan\n","ogImage":{"url":"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png"},"coverImage":"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_0.png\" alt=\"Running LLM on Your PC: Privacy and Beyond\"\u003e\u003c/p\u003e\n\u003cp\u003e이 첫 번째 기사를 통해, 인터넷 연결 없이 PC에 로컬로 내 LLM을 설치하고 설정하고 실행하는 여정을 문서화할 일련의 기사를 시작합니다.\u003c/p\u003e\n\u003cp\u003e내 PC에서 이 프로젝트를 구축하고 개인적으로 사용하기로 결정한 이유는 이 기사에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eChat-GPT 3.5와 비슷하거나 심지어 동일한 기능을 가지려고 합니다. 이것은 이루기 어렵게 들릴 수 있지만, 기술은 매일 발전하고 있으며, 적절한 하드웨어, 소프트웨어 및 LLM의 조합을 통해 가능할 것으로 생각합니다. 이 과정에서 취하는 단계, 최적화 및 조정, 실수와 성공, 사용하는 소스, 그리고 향후 하드웨어 업그레이드에 대해 공유할 것입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 여정이 끝날 때 모든 기사들은 사용자 안내서로 요약될 것이며, 저는 이를 무료로 발행하여 누구나 이 경험에서 혜택을 받고 직접 시도해볼 수 있도록 할 것입니다. 물론 이 과정에서 모든 의견, 제안 및 지침은 환영합니다.\u003c/p\u003e\n\u003cp\u003e이 여정이 시작되기 전에, 로컬 및 프라이빗 LLM을 위한 기반이 될 하드웨어와 소프트웨어인 나의 출발점을 공유할 것입니다.\u003c/p\u003e\n\u003cp\u003e나의 초기 컴퓨터 구성은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU: AMD Ryzen 9 7950X3D, 16코어 @ 4.20 GHz\u003c/li\u003e\n\u003cli\u003eRAM: Corsair VENGEANCE DDR5, 64GB (2 X 32 GB)\u003c/li\u003e\n\u003cli\u003eHDD: 삼성 990 EVO NVMe™ M.2 SSD (1 TB 각 2개)\u003c/li\u003e\n\u003cli\u003e비디오: Nvidia GeForce RTX 4070, 12GB (평택 에디션)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 설정의 소프트웨어 구성 요소는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWindows 11 Pro, ver. 23H2 (주 OS)\u003c/li\u003e\n\u003cli\u003eUbuntu 22.04 (Windows PowerShell 콘솔에 설치 및 실행됨)\u003c/li\u003e\n\u003cli\u003eOllama (LLM을 위해)\u003c/li\u003e\n\u003cli\u003eDocker (컨테이너용)\u003c/li\u003e\n\u003cli\u003ePython 3.11 (데이터 세트 및 일부 라이브러리 및 구성 요소 용)\u003c/li\u003e\n\u003cli\u003eVisual Studio Code (필요할 때 코딩을 위해)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWindows PowerShell 콘솔에 Ubuntu를 설치해야 합니다. Windows에서 \"시작\" 메뉴를 열고 \"실행\"을 입력하고 새 창에서 \"PowerShell\"을 입력한 후 Enter 키를 누르십시오. 콘솔에서 다음 명령을 입력하십시오:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ewsl --install\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e설치 프로세스를 시작하고, 완료되면 Linux용 사용자 이름과 비밀번호를 입력하라는 메시지가 표시됩니다. 성공적으로 설치되면 다음과 같은 화면을 볼 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e설치가 끝나면 Ubuntu 배포판을 업데이트하고 업그레이드하는 것과 같은 좋은 실천 동작들을 수행해야 합니다. 다음 명령어를 사용하여 이를 실행할 수 있습니다(사용자 이름과 비밀번호가 요청될 것입니다):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003esudo apt update\nsudo apt upgrade -y\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이제 여러 추가 패키지가 설치됩니다. 그리고 이로써 귀하의 로컬 PC에 Ubuntu를 설치하는 작업이 완료되었습니다.\u003c/p\u003e\n\u003cp\u003e다음은 Ollama입니다. 이를 통해 LLMs를 설치하고 실행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e참고: Ollama를 설치하기 전에 이미 Linux 환경에 있는지 확인하세요. Ubuntu를 시작하려면 다음 명령을 입력하십시오:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ewsl -d \u003cspan class=\"hljs-title class_\"\u003eUbuntu\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e설치는 콘솔에서 다시 실행되며 다음 명령어로 실행할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecurl -fsSL \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//ollama.com/install.sh | sh\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e성공적으로 설치되면 이 화면이 표시되어야 합니다. 여기서 중요한 점은 프로세스가 완료된 후 비디오 카드가 인식되어야 한다는 것입니다 (빨간색으로 표시됨).\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_2.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e비디오 카드는 NVIDIA에서 제공하는 것이 좋습니다. 큰 언어 모델 작업에 있어서 결정적인 부분입니다. 머신에 설치된(또는 적합한) 카드가 없는 경우 모델은 CPU 전력을 사용하게 되어 작업이 훨씬 느리고 정확도가 떨어지게 됩니다.\u003c/p\u003e\n\u003cp\u003eOllama를 설치한 후에는 하나 이상의 대형 언어 모델(LLM)을 설치해야 합니다. 저는 llama2:13b를 선택하여 Meta에서 제공하는 130억 파라미터 모델을 설치했습니다. 이 모델은 제 개인 GPT에서 사용하고 제 데이터로 훈련시킬 모델입니다. 모델 크기는 7.4GB이므로 제 12GB (RTX 4070 FE) 비디오 카드에 적합합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_3.png\" alt=\"Running Large Language Models on your PC: Privacy and Beyond\"\u003e\u003c/p\u003e\n\u003cp\u003e설치는 콘솔과 리눅스 환경에서 다시 진행됩니다. 아래 명령어로 실행할 수 있습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e오레카 풀라마 2:13b\u003c/p\u003e\n\u003cp\u003e모델을 다운로드하고 하드 드라이브에 저장하는 프로세스가 시작됩니다. 프로세스를 시작하기 전에 충분한 여유 공간이 있는지 확인하세요. 공간이 부족한 경우, llama:7b와 같은 더 작은 모델을 다운로드할 수도 있습니다. 이는 동일한 명령으로 수행할 수 있습니다: \"llama:13b\"를 \"llama:7b\"로 교체하면 됩니다. 7b 모델은 4.7GB이며, llama:13b 대신 사용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e성공적인 설치 후 다음 화면을 볼 수 있어야 합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e모델이 이미 하드 드라이브에 있으면 상호 작용을 시작할 수 있습니다. 다음 명령어로 콘솔에서 수행할 수 있어요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eollama run \u003cspan class=\"hljs-attr\"\u003ellama\u003c/span\u003e:13b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEnter 키를 누르면 다음 메시지가 표시됩니다:\u003c/p\u003e\n\u003cp\u003e메시지를 전송하세요 (도움말을 보려면 /?)\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이제 모델이 준비되어 입력을 받을 준비가 되었습니다. 리눅스 환경에서 모델을 실험하고 탐색하여 기능을 느끼고 전파를 연습할 수 있습니다. 상호 작용은 다음과 같아야 합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이제 1단계인 리눅스 및 LLM 설치를 마치며 마무리합니다. 이제 인터넷 연결 없이도 로컬 PC에서 LLM을 실행하고 사용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e다음 글에서는 컨테이너를 위해 Docker를 설치하고 모델을 위한 웹 인터페이스를 설치하고 모델을 훈련 및 최적화하는 데 도움이 되는 기타 Python 라이브러리와 도구를 검토할 것입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다음에 또 봐요!\u003c/p\u003e\n\u003cp\u003eStan\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-20-RunningLargeLanguageModelsonyourPCprivacyandbeyond"},"buildId":"JlBEgQDLGRx6DYlBnT8eD","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>