<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>로봇 학습의 현황 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-TheStateofRobotLearning" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="로봇 학습의 현황 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="로봇 학습의 현황 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-TheStateofRobotLearning_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-TheStateofRobotLearning" data-gatsby-head="true"/><meta name="twitter:title" content="로봇 학습의 현황 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-TheStateofRobotLearning_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 19:17" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/wOkGEDZCvEs3S_XaNsdwr/_buildManifest.js" defer=""></script><script src="/_next/static/wOkGEDZCvEs3S_XaNsdwr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">로봇 학습의 현황</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="로봇 학습의 현황" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-TheStateofRobotLearning&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>부분적으로 관찰한, 반 확률적인, 자아 중심적인 관점.</h2>
<p><img src="/assets/img/2024-05-18-TheStateofRobotLearning_0.png" alt="image"></p>
<p>이 글은 내가 Nvidia GTC에서 한 발표에 대한 동반자로, 약간의 스파이스를 더했습니다. 이것은 구글 딥마인드의 의견이 아니며, 제 팀과 동료들의 다양한 관점을 반영하지 않을 수 있습니다.</p>
<p>나는 작성 시점으로 6개월 전에 애틀랜타에서 열린 최근 로봇 학습 회의의 분위기가 자신 있었던 것을 알 수 있었습니다. 내가 7년 전부터 참석한 모든 회의와는 다르게 뭔가 변화의 느낌이 났다. 많은 발표가 실제로 ... 꽤 잘 작동하는 로봇 시스템을 보여줬습니다! 비록 일부 학술적 정의에 따르면 그 한계 내에서 작동했다고 할지라도요. 이전에 커뮤니티가 빨간 블록을 파란 블록 위에 쌓는 것과 같은 간단한 작업에서 고심했던 점에서, 이제 우리는 복잡한 현실 세계 문제에 대해 실질적인 진전을 거둔 시스템들을 보고 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>자주 농담을 해요. 연구 분야에서의 경력은 실제로 작동하지 않는 것들에 대해 평생을 일하는 것을 의미한다고 말이죠. 그리고 세션 간의 복도 대화 중에 연구자들이 “이제 어떻게 해야 할까요?” 라고 자신에게 물어보는 소리가 들릴 수 있어요. 이는 사실 일이 실제로 잘 작동하고 전체 사업에 대해 “임무 완료” 라고 누구도 부르지 않을 정도로 잘 작동한 것이 아니라, 로봇공학의 진행 속도가 가속화되었음을 깨닫게 되어 연구 방향과 채택된 방법론을 재평가해야 한다는 것을 반영하고 있어요.</p>
<p>우리가 어떻게 그 경로에 이르렀는지 궁금하시다구요? 음, 모든 인공지능 관련 사항과 마찬가지로 GPT로의 이동과 현대 LLM 출현을 2021년경으로 거슬러 올라가 볼 수 있어요. 갑자기 전례없는 추론 능력이 모두의 손끝에 있다는 것처럼 보였고, AGI는 곧 다가온다는 것이었죠. 그때쯤 로봇공학계에도 다른 일이 발생했는데, 대중 언론에는 그렇게 많이 보도되지 않았어요: FOMO가 엄청나게 증가했던 거예요. 로봇공학 또는 더 스타일리시한 이름으로 “실체화된 AI”는 AGI로 가는 길이 되어야 했고, 상황 인지, 현실 세계 기반, 상식적 추론에 대한 진정한 해법이었어요. NLP 커뮤니티가 10년 동안 방치한 것으로도 볼 수 있는 그리 증오 받는 서브필드인 “언어 모델링”이 갑자기 주목을 받자 로봇공학계에 무겁게 작용했어요.</p>
<p>물론, 그들을 이기지 못하면 함께 하라는 말이죠. 그래서 우리도 그렇게 했어요. “로봇공학과 LLMs의 만남”이라는 연구 방향은 매우 얕게 나올 수도 있었어요: 아마 당신이 로봇과 대화를 나누는 데 언어 모델을 사용할 수도 있었겠죠. 또는 로봇이 클링곤 시를 낭독하게 할 수도 있었을 거예요. 그러나 실제로 일어난 일은 제 경력의 가장 큰 놀람이었어요: 연결점이 매우 깊게 생겨나서 오늘까지 우리는 그를 해결하기 시작한 것에 불과해요.</p>
<p>LLMs를 “언어”에 관한 것으로 생각하는 것은 흔한 실수예요. 언어는 LLMs가 주로 사용하는 표면형임은 확실히 맞지만(코드도 마찬가지), LLM의 슈퍼파워는 모두 상식적 추론에 관한 것이에요: LLMs는 “책은 책장에 두어야 하지 욕조에는 넣어두지 말아야 한다”나 “커피를 내리는 방법” 같은 간단한 진리를 알아요. 그것이 실제 세계에서 움직이려는 실체화된 에이전트들에게 중요한 문제인 것이 결국 크게 작용한다는 것은 놀랍지 않아요. 그래서 LLMs가 가장 먼저 영향을 줄 로봇학의 한 부분이 계획에 영향을 받을 것이라는 것은 당연한 일이었어요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>로봇이 어떻게 작동하는지 개념적으로 이해하는 도표를 그리는 것이 도움이 될 수 있습니다. 이 정도로 단순화하면 커뮤니티에서 친구를 많이 사귈 수 있을 것 같진 않지만, "모든 모델은 틀리지만 어떤 것은 유용하다"는 정신으로 해석하면 매우 유익한 모습이 나옵니다. 당신의 로봇이 세계의 상태를 인지하고, 그 상태를 계획자에게 보내어 목표와 함께 계획을 세우는 루프를 상상해보세요. 그 계획은 로봇 컨트롤러에 전달되고, 하드웨어를 작동시켜 실행을 담당합니다. 물론 세계는 계속 변하기 때문에 아마도 계획의 처음 단계만 실행되고, 상태 추정이 업데이트되고 로봇이 다음 단계 실행 계획을 수립하고, 이와 같은 일을 반복하게 될 것입니다.</p>
<p>이는 로봇 공학의 주요 분야에 매핑되는 임의적인 스케치로, 상태 추정, 작업 및 동작 계획, 제어와 관련하여 전통적으로 병행 발전해 왔으며, 시스템 수준의 문제가 크게 무시되고 문제가 종종 다른 분야로 던져지는 것으로 이어졌다고 주장하는 사람도 많습니다: 너무나 많은 TAMP 논문들이 완벽한 상태 추정을 당연시하고, 많은 제어 전략들이 실행할 수 없는 계획을 속삭이게 되며, 그 경계를 넘어 그래디언트가 흐를 수 있도록 하는 것에 대해 언급할 필요도 없군요!</p>
<p>그래서 당연히 첫 번째 파장은 계획자 쪽에서 발생했습니다. 아마도 주관적일 수 있지만, 나는 SayCan을 "아하" 순간으로 꼽을 것입니다. 커뮤니티가 인지한 것은 계획의 많은 부분을 "의미 공간"으로 옮길 수 있다면 기하학 공간이 아닌 곳에서 계획을 수행할 수 있으며, 이를 통해 LLMs를 사용하여 이 작업을 수행할 수 있고, 데이터를 수집하거나 로봇에 특화된 온톨로지를 작성하거나 상징적 추론 엔진을 구축할 필요 없이 그들의 상식 능력의 모든 이점을 누릴 수 있음을 깨달은 순간이었다고 생각합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-18-TheStateofRobotLearning_2.png">
<p>인지와 실행 모두 자연어를 사용하는 인터페이스를 갖게되면 모든 것에 대해 언어를 API로 사용하는 것이 매우 유혹적일 것입니다. 언어에는 많은 장점이 있습니다: 유연하며 해석 가능하며 선택한 추상화 수준으로 사물을 설명할 수 있습니다. 이것은 고정된 API에 대한 거대한 문제였습니다: 예를 들어 자율 주행 자동차에게 세계가 바운딩 박스의 모음처럼 보이는 것은 괜찮을 수 있지만, 사물에 직접 접촉하려고 하면 아마도 더 풍부한 지오메트리와 의미론적 정보가 필요할 것입니다. 혹시 계획자가 인식 모듈이 제공할 유용한 정보를 미리 알지 못할 수도 있습니다. 아마도 양방향 대화가 필요할지도 모릅니다...</p>
<p>이것이 그 여정의 다음 단계로 나아가는데요: 계획자와 인지 시스템이 모두 자연어를 사용하도록합시다. VLM들이 정말 뛰어나게 발전하고 있으니, 이를 활용하여 양방향 대화를 실제 대화로 만들어봅시다. 이것이 Socratic Models의 아이디어입니다. 여기서는 세상의 상태와 그에 대한 행동 방법에 대한 합의를 모델 사이의 대화를 통해 달성할 수 있습니다. Inner Monologue는 주기적인 상태 재추정 및 재계획을 대화의 일부로 만드는 개념을 더 발전시켰습니다.</p>
<img src="/assets/img/2024-05-18-TheStateofRobotLearning_3.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>로봇 지능의 중추로 LLM이 있다면 새로운 일을 많이 할 수 있습니다. 예를 들어 AutoRT에서는 LLM을 활용하여 수행할 새로운 작업을 꿈꿨는데, 이로 인해 즉시 제기된 질문은 다음과 같습니다: 로봇이 스스로 해야 할 일을 생각한다면, 어떻게 그것들이 안전하고 유익한지를 보장할 수 있을까요? 우리는 LLM을 안전한 개념(“날카로운 물체를 집지 마세요”)으로 유도하거나 더 일반적인 인간 중심 가치를 제시할 수 있습니다. “인간에게 상처를 입히지 마십시오…” 소리가 익숙하신가요? 몇 년 전에 실제 로봇에 아지모프의 로봇 법칙을 구현할 수 있는 상당히 타당한 경로가 있다고 말해주었다면, 믿지 않았을 것입니다. 향후 시간이 이것을 사용할지 여부를 알려줄 것입니다. 헌법 AI를 로봇의 안전 스택의 일부로 사용하는 것이 실용적인지는 앞으로 알게 될 것이지만, 실제 세계에서 이에 대해 이야기하고 평가할 수 있다는 사실은공신입니다.</p>
<p>작동 구성 요소는 어떻게 되나요? 전통적인 로봇 공학의 마지막 요새인 그것조차 LLM 처리를 받을 수 있을까요? LLM이 정말 잘하는 한 가지는 코드 생성입니다. 결국, 컨트롤러 소프트웨어는 정책을 기술한 코드일 뿐입니다. 여기서 코드로 정책이라는 개념이 등장하며, LLM에게 저수준 제어 API를 제시하고 실제 실행할 정책을 설명하도록 할 수 있다는 아이디어가 있습니다.</p>
<img src="/assets/img/2024-05-18-TheStateofRobotLearning_4.png">
<p>코드 LMs는 제로샷에서 아주 잘 작동하는데요, 단지 프롬프트 디자인의 암흑 예술에 능숙해야 합니다. 마이크로소프트 동료들의 초창기 ChatGPT for Robotics 실험에서 사용된 것처럼 대화 전략을 사용하여 프롬프트를 반복하고 향상시킬 수 있습니다. 하지만 더 좋은 점은 제어 행위의 상호 작용을 통해 코드 LM을 세밀하게 조정하고 개선하는 반복적인 과정을 거칠 때입니다. 이것은 우리가 고전적인 모델 예측 제어에 유사하게 언어 모델 예측 제어라고 이름 붙인 것입니다. 이를 통해 모델이 새로운 작업에 대해 더 나은 제로샷 수행뿐만 아니라 사용자 상호 작용에서 더 빨리 학습할 수 있게 됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>여러 LLM이 내부 채팅방에서 소통하는 이미지가 로봇의 중추가 되면, 문제를 세 가지 구성 요소로 인수 분해하는 것이 여전히 유용한지 의문스러울 수 있습니다. 신경망은 서로 고대역폭의 미분 가능한 표현을 통해 통신할 수 있기 때문에, 왜 그것들을 단어로 축소시켜야 할까요? 해석 가능성을 어느 정도 얻을 수는 있지만, 정보 손실은 상당합니다. 예를 들어, 계획자가 여전히 본질적으로 맹목적이라고 상상해 보세요. 이러한 구성 요소를 일부 병합할 수 있을까요? 단순히 끝점 열광 때문이 아니라, 이미 이러한 모델에 내재된 모듈성 덕분에 가능합니다. 서로 다른 transformer 구성 요소가 서로 상호 작용하도록 허용함으로써 신경망 내부에서 '관심사의 분리'를 재현할 수 있기 때문입니다.</p>
<p>이 방향으로 첫 실험이 PaLM-E를 사용하여 인식과 계획을 병합하려고 시도되었습니다.</p>
<p><img src="/assets/img/2024-05-18-TheStateofRobotLearning_5.png" alt="이미지"></p>
<p>인식 및 계획 모듈을 공동 훈련하면 명확한 향상이 관찰되었으며, 작업 및 실행체에 대한 전이 증거도 있었습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다음 실험은 계획자를 제외하고 지각과 행동을 합치는 것이었습니다. "픽셀에서 행동" 모델이 많이 나왔지만 우리에게 혁신적인 방법은 RT-1이었습니다. TRI의 액션 확산, 버클리의 휴머노이드 트랜스포머, 스탠포드의 ACT와 Octo와 같은 방식으로 최근 몇 달 동안 이 분야에서 많은 일이 벌어졌으며, 성능과 기능성의 폭발을 보는 것은 정말 멋진 일이었습니다.</p>
<p>지금쯤 어디로 향하고 있는지 보이시나요? 반쪽채치런 선택보다 모든 것을 하는 단일 "로봇 두뇌"를 훈련하는 것이 더 낫지 않을까요? 이에 대한 우리의 첫 번째 시도는 RT-2이었으며, 여전히 로봇공학 관련 데이터 소스(특히 지각 및 의미적 이해를 위한 인터넷 데이터)를 활용하면서 전체 문제에 대해 공동으로 추론하는 능력이 얼마나 많은 도움을 주는지를 보여주었습니다. Meta의 동료들이 VC-1로 그 방향으로 진행한 또 다른 주목할 만한 한 걸음이었습니다. 다중 모달 모델을 위한 오픈소스 생태계가 번성하고 있으며, 우리가 이러한 모델의 공간 추론 능력을 어디까지 밀어낼 수 있는지 탐구하고 있는 사람들이 많아질 것으로 기대합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>여기서 우리는 어디로 가야 할까요? 반지도 학습 혁명은 왔다가 지났어요. RL은 여전히 상처를 핥고 있습니다. 확산 모델이 잘 돌아가고 있으며 오프라인 RL이 부활하고 있습니다. 데이터 효율성이 크게 향상되었음에도 불구하고, "실제 세계에서 작동"을 상상할 수 있게 되었지만, sim-to-real 접근에 모든 것을 걸 필요가 없어진 것처럼 보이는 상황에서도 여전히 데이터에 구애받고 있으며, 데이터 수집의 효율성과 다양성을 향상시키는 것이 중요합니다.</p>
<p>오늘날 이 분야에서 가장 큰 긴장감이 있습니다. 한편으로는 교차 존재 모델이 로봇간 능력을 전이하는 데 뛰어나게 작동한다는 것을 보여주고 있습니다. 로봇, 작업 간의 다양성을 높이고 문제에 대해 다양성 중심적인 접근을 취해야 한다는 주장이 있습니다. 다른 한편으로는 "모두를 지배할 한 가지 형태"로봇 학습 방식을 원하는 사람이 더 많아지고, 테슬라, 피규어, 1X, 어질리티, Unitree, Sanctuary, Apptronik 등과 같이 억만장자로 유명한 로봇에 투자하는 돈이 늘어나고 있습니다. 후자 방법의 장점은 인간적 존재에서 배우고 인간 공간에 배치될 수 있는 범용 능력이며, 단점은 매우 복잡하고 비싼 하드웨어를 데이터 수집의 중요 경로에 놓게 되어 최종 제품의 경제성에 큰 장벽을 높일 수 있다는 것입니다.</p>
<p>그것은 위험한 내기입니다. 특히 교차 존재 가설이 사실로 드러나고 Aloha, UMI, Stretch(및 Dobb.E), Mobile Aloha 및 Aloha 2와 같은 실험을 본다면 싸구려 로봇 떼가 분야를 완전히 혼란스럽게 만들고 다양한 저렴한 모습에 민첩한 능력을 가져올 수 있습니다. 그러나 이 시점에서 그 베팅 어느 쪽에도 100% 돈을 거는 일은 하지 않겠습니다. 앞으로 수개월은 이 분야가 어디로 가야 하는지에 대한 포문이 될 것입니다. 다양성 대 다양성, 싸구려와 두각을 내며 고급 DOF와 완전한 기능, 더 싸고 철저하게, 단점이 있다.</p>
<p>오늘 우리가 있는 세계는 어느 정도 둘 다입니다. "범용" 어림들이 널리 배치되어 있지만, 그들을 가치 있게 만드는 경제학은 잔혹하며, 그것들이 실제로 유용하게 만들기 위해 전용 도구 및 시스템 통합이 필요합니다.</p>
<p>데이터 확장 맥락에서 확신하는 점은 생성 모델이 시뮬레이션의 미래라는 것이며, "더 나은 시뮬레이터"뿐만 아니라 3D 및 비디오 생성이 외형뿐만 아니라 물리학 및 공간 관계도 존중하게 만드는 방법을 쫓아야 한다는 것입니다. 날씨 예측부터 단백질 접힘까지 다른 물리학 시뮬레이터 분야마다 생성 모델로 인해 혼란스러워지고 있습니다. 로봇 및 환경의 디지털 복제본을 작성하는 데 몇 시간을 보내는 대신 로봇의 센서를 잡아 "재생"을 누르면 가능한 미래를 생성할 수 있다면 시뮬에서 실로ら 릴 체리 뉴스를 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>저는 데이터 확장이 대부분 HRI 문제로 전환되고 있다고 점점 더 확신하게 되고 있습니다. 오늘날 HRI 커뮤니티가 주로 걱정하는 정확한 문제는 아닐 수 있지만; 대부분의 HRI는 최종 사용자가 로봇과 상호 작용하는 부분에 관심을 기울입니다. 새로운 다양한 작업을 설계하고, 데이터를 견고하게 수집하고, 행동을 개선 및 최적화하기 위해 더 나은 HRI 접근 방식이 필요합니다: 모델 및 행동을 훈련시키기 위해 데이터 수집에 필요한 모든 신중한 설계를 최적화하는 것은 최종 사용자가 최종 제품과 상호 작용하기 전에도 이루어져야 합니다. 우리는 실제로 모든 로봇을 눈, 팔, 다리가 있는 챗봇으로 바꿈으로써 HRI 커뮤니티에 새로운 가능성을 제공했습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"로봇 학습의 현황","description":"","date":"2024-05-18 19:17","slug":"2024-05-18-TheStateofRobotLearning","content":"\n## 부분적으로 관찰한, 반 확률적인, 자아 중심적인 관점.\n\n![image](/assets/img/2024-05-18-TheStateofRobotLearning_0.png)\n\n이 글은 내가 Nvidia GTC에서 한 발표에 대한 동반자로, 약간의 스파이스를 더했습니다. 이것은 구글 딥마인드의 의견이 아니며, 제 팀과 동료들의 다양한 관점을 반영하지 않을 수 있습니다.\n\n나는 작성 시점으로 6개월 전에 애틀랜타에서 열린 최근 로봇 학습 회의의 분위기가 자신 있었던 것을 알 수 있었습니다. 내가 7년 전부터 참석한 모든 회의와는 다르게 뭔가 변화의 느낌이 났다. 많은 발표가 실제로 ... 꽤 잘 작동하는 로봇 시스템을 보여줬습니다! 비록 일부 학술적 정의에 따르면 그 한계 내에서 작동했다고 할지라도요. 이전에 커뮤니티가 빨간 블록을 파란 블록 위에 쌓는 것과 같은 간단한 작업에서 고심했던 점에서, 이제 우리는 복잡한 현실 세계 문제에 대해 실질적인 진전을 거둔 시스템들을 보고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n자주 농담을 해요. 연구 분야에서의 경력은 실제로 작동하지 않는 것들에 대해 평생을 일하는 것을 의미한다고 말이죠. 그리고 세션 간의 복도 대화 중에 연구자들이 “이제 어떻게 해야 할까요?” 라고 자신에게 물어보는 소리가 들릴 수 있어요. 이는 사실 일이 실제로 잘 작동하고 전체 사업에 대해 “임무 완료” 라고 누구도 부르지 않을 정도로 잘 작동한 것이 아니라, 로봇공학의 진행 속도가 가속화되었음을 깨닫게 되어 연구 방향과 채택된 방법론을 재평가해야 한다는 것을 반영하고 있어요.\n\n우리가 어떻게 그 경로에 이르렀는지 궁금하시다구요? 음, 모든 인공지능 관련 사항과 마찬가지로 GPT로의 이동과 현대 LLM 출현을 2021년경으로 거슬러 올라가 볼 수 있어요. 갑자기 전례없는 추론 능력이 모두의 손끝에 있다는 것처럼 보였고, AGI는 곧 다가온다는 것이었죠. 그때쯤 로봇공학계에도 다른 일이 발생했는데, 대중 언론에는 그렇게 많이 보도되지 않았어요: FOMO가 엄청나게 증가했던 거예요. 로봇공학 또는 더 스타일리시한 이름으로 “실체화된 AI”는 AGI로 가는 길이 되어야 했고, 상황 인지, 현실 세계 기반, 상식적 추론에 대한 진정한 해법이었어요. NLP 커뮤니티가 10년 동안 방치한 것으로도 볼 수 있는 그리 증오 받는 서브필드인 “언어 모델링”이 갑자기 주목을 받자 로봇공학계에 무겁게 작용했어요.\n\n물론, 그들을 이기지 못하면 함께 하라는 말이죠. 그래서 우리도 그렇게 했어요. “로봇공학과 LLMs의 만남”이라는 연구 방향은 매우 얕게 나올 수도 있었어요: 아마 당신이 로봇과 대화를 나누는 데 언어 모델을 사용할 수도 있었겠죠. 또는 로봇이 클링곤 시를 낭독하게 할 수도 있었을 거예요. 그러나 실제로 일어난 일은 제 경력의 가장 큰 놀람이었어요: 연결점이 매우 깊게 생겨나서 오늘까지 우리는 그를 해결하기 시작한 것에 불과해요.\n\nLLMs를 “언어”에 관한 것으로 생각하는 것은 흔한 실수예요. 언어는 LLMs가 주로 사용하는 표면형임은 확실히 맞지만(코드도 마찬가지), LLM의 슈퍼파워는 모두 상식적 추론에 관한 것이에요: LLMs는 “책은 책장에 두어야 하지 욕조에는 넣어두지 말아야 한다”나 “커피를 내리는 방법” 같은 간단한 진리를 알아요. 그것이 실제 세계에서 움직이려는 실체화된 에이전트들에게 중요한 문제인 것이 결국 크게 작용한다는 것은 놀랍지 않아요. 그래서 LLMs가 가장 먼저 영향을 줄 로봇학의 한 부분이 계획에 영향을 받을 것이라는 것은 당연한 일이었어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n로봇이 어떻게 작동하는지 개념적으로 이해하는 도표를 그리는 것이 도움이 될 수 있습니다. 이 정도로 단순화하면 커뮤니티에서 친구를 많이 사귈 수 있을 것 같진 않지만, \"모든 모델은 틀리지만 어떤 것은 유용하다\"는 정신으로 해석하면 매우 유익한 모습이 나옵니다. 당신의 로봇이 세계의 상태를 인지하고, 그 상태를 계획자에게 보내어 목표와 함께 계획을 세우는 루프를 상상해보세요. 그 계획은 로봇 컨트롤러에 전달되고, 하드웨어를 작동시켜 실행을 담당합니다. 물론 세계는 계속 변하기 때문에 아마도 계획의 처음 단계만 실행되고, 상태 추정이 업데이트되고 로봇이 다음 단계 실행 계획을 수립하고, 이와 같은 일을 반복하게 될 것입니다.\n\n이는 로봇 공학의 주요 분야에 매핑되는 임의적인 스케치로, 상태 추정, 작업 및 동작 계획, 제어와 관련하여 전통적으로 병행 발전해 왔으며, 시스템 수준의 문제가 크게 무시되고 문제가 종종 다른 분야로 던져지는 것으로 이어졌다고 주장하는 사람도 많습니다: 너무나 많은 TAMP 논문들이 완벽한 상태 추정을 당연시하고, 많은 제어 전략들이 실행할 수 없는 계획을 속삭이게 되며, 그 경계를 넘어 그래디언트가 흐를 수 있도록 하는 것에 대해 언급할 필요도 없군요!\n\n그래서 당연히 첫 번째 파장은 계획자 쪽에서 발생했습니다. 아마도 주관적일 수 있지만, 나는 SayCan을 \"아하\" 순간으로 꼽을 것입니다. 커뮤니티가 인지한 것은 계획의 많은 부분을 \"의미 공간\"으로 옮길 수 있다면 기하학 공간이 아닌 곳에서 계획을 수행할 수 있으며, 이를 통해 LLMs를 사용하여 이 작업을 수행할 수 있고, 데이터를 수집하거나 로봇에 특화된 온톨로지를 작성하거나 상징적 추론 엔진을 구축할 필요 없이 그들의 상식 능력의 모든 이점을 누릴 수 있음을 깨달은 순간이었다고 생각합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_2.png\" /\u003e\n\n인지와 실행 모두 자연어를 사용하는 인터페이스를 갖게되면 모든 것에 대해 언어를 API로 사용하는 것이 매우 유혹적일 것입니다. 언어에는 많은 장점이 있습니다: 유연하며 해석 가능하며 선택한 추상화 수준으로 사물을 설명할 수 있습니다. 이것은 고정된 API에 대한 거대한 문제였습니다: 예를 들어 자율 주행 자동차에게 세계가 바운딩 박스의 모음처럼 보이는 것은 괜찮을 수 있지만, 사물에 직접 접촉하려고 하면 아마도 더 풍부한 지오메트리와 의미론적 정보가 필요할 것입니다. 혹시 계획자가 인식 모듈이 제공할 유용한 정보를 미리 알지 못할 수도 있습니다. 아마도 양방향 대화가 필요할지도 모릅니다...\n\n이것이 그 여정의 다음 단계로 나아가는데요: 계획자와 인지 시스템이 모두 자연어를 사용하도록합시다. VLM들이 정말 뛰어나게 발전하고 있으니, 이를 활용하여 양방향 대화를 실제 대화로 만들어봅시다. 이것이 Socratic Models의 아이디어입니다. 여기서는 세상의 상태와 그에 대한 행동 방법에 대한 합의를 모델 사이의 대화를 통해 달성할 수 있습니다. Inner Monologue는 주기적인 상태 재추정 및 재계획을 대화의 일부로 만드는 개념을 더 발전시켰습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_3.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n로봇 지능의 중추로 LLM이 있다면 새로운 일을 많이 할 수 있습니다. 예를 들어 AutoRT에서는 LLM을 활용하여 수행할 새로운 작업을 꿈꿨는데, 이로 인해 즉시 제기된 질문은 다음과 같습니다: 로봇이 스스로 해야 할 일을 생각한다면, 어떻게 그것들이 안전하고 유익한지를 보장할 수 있을까요? 우리는 LLM을 안전한 개념(“날카로운 물체를 집지 마세요”)으로 유도하거나 더 일반적인 인간 중심 가치를 제시할 수 있습니다. “인간에게 상처를 입히지 마십시오…” 소리가 익숙하신가요? 몇 년 전에 실제 로봇에 아지모프의 로봇 법칙을 구현할 수 있는 상당히 타당한 경로가 있다고 말해주었다면, 믿지 않았을 것입니다. 향후 시간이 이것을 사용할지 여부를 알려줄 것입니다. 헌법 AI를 로봇의 안전 스택의 일부로 사용하는 것이 실용적인지는 앞으로 알게 될 것이지만, 실제 세계에서 이에 대해 이야기하고 평가할 수 있다는 사실은공신입니다.\n\n작동 구성 요소는 어떻게 되나요? 전통적인 로봇 공학의 마지막 요새인 그것조차 LLM 처리를 받을 수 있을까요? LLM이 정말 잘하는 한 가지는 코드 생성입니다. 결국, 컨트롤러 소프트웨어는 정책을 기술한 코드일 뿐입니다. 여기서 코드로 정책이라는 개념이 등장하며, LLM에게 저수준 제어 API를 제시하고 실제 실행할 정책을 설명하도록 할 수 있다는 아이디어가 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_4.png\" /\u003e\n\n코드 LMs는 제로샷에서 아주 잘 작동하는데요, 단지 프롬프트 디자인의 암흑 예술에 능숙해야 합니다. 마이크로소프트 동료들의 초창기 ChatGPT for Robotics 실험에서 사용된 것처럼 대화 전략을 사용하여 프롬프트를 반복하고 향상시킬 수 있습니다. 하지만 더 좋은 점은 제어 행위의 상호 작용을 통해 코드 LM을 세밀하게 조정하고 개선하는 반복적인 과정을 거칠 때입니다. 이것은 우리가 고전적인 모델 예측 제어에 유사하게 언어 모델 예측 제어라고 이름 붙인 것입니다. 이를 통해 모델이 새로운 작업에 대해 더 나은 제로샷 수행뿐만 아니라 사용자 상호 작용에서 더 빨리 학습할 수 있게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여러 LLM이 내부 채팅방에서 소통하는 이미지가 로봇의 중추가 되면, 문제를 세 가지 구성 요소로 인수 분해하는 것이 여전히 유용한지 의문스러울 수 있습니다. 신경망은 서로 고대역폭의 미분 가능한 표현을 통해 통신할 수 있기 때문에, 왜 그것들을 단어로 축소시켜야 할까요? 해석 가능성을 어느 정도 얻을 수는 있지만, 정보 손실은 상당합니다. 예를 들어, 계획자가 여전히 본질적으로 맹목적이라고 상상해 보세요. 이러한 구성 요소를 일부 병합할 수 있을까요? 단순히 끝점 열광 때문이 아니라, 이미 이러한 모델에 내재된 모듈성 덕분에 가능합니다. 서로 다른 transformer 구성 요소가 서로 상호 작용하도록 허용함으로써 신경망 내부에서 '관심사의 분리'를 재현할 수 있기 때문입니다.\n\n이 방향으로 첫 실험이 PaLM-E를 사용하여 인식과 계획을 병합하려고 시도되었습니다.\n\n![이미지](/assets/img/2024-05-18-TheStateofRobotLearning_5.png)\n\n인식 및 계획 모듈을 공동 훈련하면 명확한 향상이 관찰되었으며, 작업 및 실행체에 대한 전이 증거도 있었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음 실험은 계획자를 제외하고 지각과 행동을 합치는 것이었습니다. \"픽셀에서 행동\" 모델이 많이 나왔지만 우리에게 혁신적인 방법은 RT-1이었습니다. TRI의 액션 확산, 버클리의 휴머노이드 트랜스포머, 스탠포드의 ACT와 Octo와 같은 방식으로 최근 몇 달 동안 이 분야에서 많은 일이 벌어졌으며, 성능과 기능성의 폭발을 보는 것은 정말 멋진 일이었습니다.\n\n지금쯤 어디로 향하고 있는지 보이시나요? 반쪽채치런 선택보다 모든 것을 하는 단일 \"로봇 두뇌\"를 훈련하는 것이 더 낫지 않을까요? 이에 대한 우리의 첫 번째 시도는 RT-2이었으며, 여전히 로봇공학 관련 데이터 소스(특히 지각 및 의미적 이해를 위한 인터넷 데이터)를 활용하면서 전체 문제에 대해 공동으로 추론하는 능력이 얼마나 많은 도움을 주는지를 보여주었습니다. Meta의 동료들이 VC-1로 그 방향으로 진행한 또 다른 주목할 만한 한 걸음이었습니다. 다중 모달 모델을 위한 오픈소스 생태계가 번성하고 있으며, 우리가 이러한 모델의 공간 추론 능력을 어디까지 밀어낼 수 있는지 탐구하고 있는 사람들이 많아질 것으로 기대합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기서 우리는 어디로 가야 할까요? 반지도 학습 혁명은 왔다가 지났어요. RL은 여전히 상처를 핥고 있습니다. 확산 모델이 잘 돌아가고 있으며 오프라인 RL이 부활하고 있습니다. 데이터 효율성이 크게 향상되었음에도 불구하고, \"실제 세계에서 작동\"을 상상할 수 있게 되었지만, sim-to-real 접근에 모든 것을 걸 필요가 없어진 것처럼 보이는 상황에서도 여전히 데이터에 구애받고 있으며, 데이터 수집의 효율성과 다양성을 향상시키는 것이 중요합니다.\n\n오늘날 이 분야에서 가장 큰 긴장감이 있습니다. 한편으로는 교차 존재 모델이 로봇간 능력을 전이하는 데 뛰어나게 작동한다는 것을 보여주고 있습니다. 로봇, 작업 간의 다양성을 높이고 문제에 대해 다양성 중심적인 접근을 취해야 한다는 주장이 있습니다. 다른 한편으로는 \"모두를 지배할 한 가지 형태\"로봇 학습 방식을 원하는 사람이 더 많아지고, 테슬라, 피규어, 1X, 어질리티, Unitree, Sanctuary, Apptronik 등과 같이 억만장자로 유명한 로봇에 투자하는 돈이 늘어나고 있습니다. 후자 방법의 장점은 인간적 존재에서 배우고 인간 공간에 배치될 수 있는 범용 능력이며, 단점은 매우 복잡하고 비싼 하드웨어를 데이터 수집의 중요 경로에 놓게 되어 최종 제품의 경제성에 큰 장벽을 높일 수 있다는 것입니다.\n\n그것은 위험한 내기입니다. 특히 교차 존재 가설이 사실로 드러나고 Aloha, UMI, Stretch(및 Dobb.E), Mobile Aloha 및 Aloha 2와 같은 실험을 본다면 싸구려 로봇 떼가 분야를 완전히 혼란스럽게 만들고 다양한 저렴한 모습에 민첩한 능력을 가져올 수 있습니다. 그러나 이 시점에서 그 베팅 어느 쪽에도 100% 돈을 거는 일은 하지 않겠습니다. 앞으로 수개월은 이 분야가 어디로 가야 하는지에 대한 포문이 될 것입니다. 다양성 대 다양성, 싸구려와 두각을 내며 고급 DOF와 완전한 기능, 더 싸고 철저하게, 단점이 있다.\n\n오늘 우리가 있는 세계는 어느 정도 둘 다입니다. \"범용\" 어림들이 널리 배치되어 있지만, 그들을 가치 있게 만드는 경제학은 잔혹하며, 그것들이 실제로 유용하게 만들기 위해 전용 도구 및 시스템 통합이 필요합니다.\n\n데이터 확장 맥락에서 확신하는 점은 생성 모델이 시뮬레이션의 미래라는 것이며, \"더 나은 시뮬레이터\"뿐만 아니라 3D 및 비디오 생성이 외형뿐만 아니라 물리학 및 공간 관계도 존중하게 만드는 방법을 쫓아야 한다는 것입니다. 날씨 예측부터 단백질 접힘까지 다른 물리학 시뮬레이터 분야마다 생성 모델로 인해 혼란스러워지고 있습니다. 로봇 및 환경의 디지털 복제본을 작성하는 데 몇 시간을 보내는 대신 로봇의 센서를 잡아 \"재생\"을 누르면 가능한 미래를 생성할 수 있다면 시뮬에서 실로ら 릴 체리 뉴스를 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저는 데이터 확장이 대부분 HRI 문제로 전환되고 있다고 점점 더 확신하게 되고 있습니다. 오늘날 HRI 커뮤니티가 주로 걱정하는 정확한 문제는 아닐 수 있지만; 대부분의 HRI는 최종 사용자가 로봇과 상호 작용하는 부분에 관심을 기울입니다. 새로운 다양한 작업을 설계하고, 데이터를 견고하게 수집하고, 행동을 개선 및 최적화하기 위해 더 나은 HRI 접근 방식이 필요합니다: 모델 및 행동을 훈련시키기 위해 데이터 수집에 필요한 모든 신중한 설계를 최적화하는 것은 최종 사용자가 최종 제품과 상호 작용하기 전에도 이루어져야 합니다. 우리는 실제로 모든 로봇을 눈, 팔, 다리가 있는 챗봇으로 바꿈으로써 HRI 커뮤니티에 새로운 가능성을 제공했습니다.\n","ogImage":{"url":"/assets/img/2024-05-18-TheStateofRobotLearning_0.png"},"coverImage":"/assets/img/2024-05-18-TheStateofRobotLearning_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e부분적으로 관찰한, 반 확률적인, 자아 중심적인 관점.\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 글은 내가 Nvidia GTC에서 한 발표에 대한 동반자로, 약간의 스파이스를 더했습니다. 이것은 구글 딥마인드의 의견이 아니며, 제 팀과 동료들의 다양한 관점을 반영하지 않을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e나는 작성 시점으로 6개월 전에 애틀랜타에서 열린 최근 로봇 학습 회의의 분위기가 자신 있었던 것을 알 수 있었습니다. 내가 7년 전부터 참석한 모든 회의와는 다르게 뭔가 변화의 느낌이 났다. 많은 발표가 실제로 ... 꽤 잘 작동하는 로봇 시스템을 보여줬습니다! 비록 일부 학술적 정의에 따르면 그 한계 내에서 작동했다고 할지라도요. 이전에 커뮤니티가 빨간 블록을 파란 블록 위에 쌓는 것과 같은 간단한 작업에서 고심했던 점에서, 이제 우리는 복잡한 현실 세계 문제에 대해 실질적인 진전을 거둔 시스템들을 보고 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e자주 농담을 해요. 연구 분야에서의 경력은 실제로 작동하지 않는 것들에 대해 평생을 일하는 것을 의미한다고 말이죠. 그리고 세션 간의 복도 대화 중에 연구자들이 “이제 어떻게 해야 할까요?” 라고 자신에게 물어보는 소리가 들릴 수 있어요. 이는 사실 일이 실제로 잘 작동하고 전체 사업에 대해 “임무 완료” 라고 누구도 부르지 않을 정도로 잘 작동한 것이 아니라, 로봇공학의 진행 속도가 가속화되었음을 깨닫게 되어 연구 방향과 채택된 방법론을 재평가해야 한다는 것을 반영하고 있어요.\u003c/p\u003e\n\u003cp\u003e우리가 어떻게 그 경로에 이르렀는지 궁금하시다구요? 음, 모든 인공지능 관련 사항과 마찬가지로 GPT로의 이동과 현대 LLM 출현을 2021년경으로 거슬러 올라가 볼 수 있어요. 갑자기 전례없는 추론 능력이 모두의 손끝에 있다는 것처럼 보였고, AGI는 곧 다가온다는 것이었죠. 그때쯤 로봇공학계에도 다른 일이 발생했는데, 대중 언론에는 그렇게 많이 보도되지 않았어요: FOMO가 엄청나게 증가했던 거예요. 로봇공학 또는 더 스타일리시한 이름으로 “실체화된 AI”는 AGI로 가는 길이 되어야 했고, 상황 인지, 현실 세계 기반, 상식적 추론에 대한 진정한 해법이었어요. NLP 커뮤니티가 10년 동안 방치한 것으로도 볼 수 있는 그리 증오 받는 서브필드인 “언어 모델링”이 갑자기 주목을 받자 로봇공학계에 무겁게 작용했어요.\u003c/p\u003e\n\u003cp\u003e물론, 그들을 이기지 못하면 함께 하라는 말이죠. 그래서 우리도 그렇게 했어요. “로봇공학과 LLMs의 만남”이라는 연구 방향은 매우 얕게 나올 수도 있었어요: 아마 당신이 로봇과 대화를 나누는 데 언어 모델을 사용할 수도 있었겠죠. 또는 로봇이 클링곤 시를 낭독하게 할 수도 있었을 거예요. 그러나 실제로 일어난 일은 제 경력의 가장 큰 놀람이었어요: 연결점이 매우 깊게 생겨나서 오늘까지 우리는 그를 해결하기 시작한 것에 불과해요.\u003c/p\u003e\n\u003cp\u003eLLMs를 “언어”에 관한 것으로 생각하는 것은 흔한 실수예요. 언어는 LLMs가 주로 사용하는 표면형임은 확실히 맞지만(코드도 마찬가지), LLM의 슈퍼파워는 모두 상식적 추론에 관한 것이에요: LLMs는 “책은 책장에 두어야 하지 욕조에는 넣어두지 말아야 한다”나 “커피를 내리는 방법” 같은 간단한 진리를 알아요. 그것이 실제 세계에서 움직이려는 실체화된 에이전트들에게 중요한 문제인 것이 결국 크게 작용한다는 것은 놀랍지 않아요. 그래서 LLMs가 가장 먼저 영향을 줄 로봇학의 한 부분이 계획에 영향을 받을 것이라는 것은 당연한 일이었어요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e로봇이 어떻게 작동하는지 개념적으로 이해하는 도표를 그리는 것이 도움이 될 수 있습니다. 이 정도로 단순화하면 커뮤니티에서 친구를 많이 사귈 수 있을 것 같진 않지만, \"모든 모델은 틀리지만 어떤 것은 유용하다\"는 정신으로 해석하면 매우 유익한 모습이 나옵니다. 당신의 로봇이 세계의 상태를 인지하고, 그 상태를 계획자에게 보내어 목표와 함께 계획을 세우는 루프를 상상해보세요. 그 계획은 로봇 컨트롤러에 전달되고, 하드웨어를 작동시켜 실행을 담당합니다. 물론 세계는 계속 변하기 때문에 아마도 계획의 처음 단계만 실행되고, 상태 추정이 업데이트되고 로봇이 다음 단계 실행 계획을 수립하고, 이와 같은 일을 반복하게 될 것입니다.\u003c/p\u003e\n\u003cp\u003e이는 로봇 공학의 주요 분야에 매핑되는 임의적인 스케치로, 상태 추정, 작업 및 동작 계획, 제어와 관련하여 전통적으로 병행 발전해 왔으며, 시스템 수준의 문제가 크게 무시되고 문제가 종종 다른 분야로 던져지는 것으로 이어졌다고 주장하는 사람도 많습니다: 너무나 많은 TAMP 논문들이 완벽한 상태 추정을 당연시하고, 많은 제어 전략들이 실행할 수 없는 계획을 속삭이게 되며, 그 경계를 넘어 그래디언트가 흐를 수 있도록 하는 것에 대해 언급할 필요도 없군요!\u003c/p\u003e\n\u003cp\u003e그래서 당연히 첫 번째 파장은 계획자 쪽에서 발생했습니다. 아마도 주관적일 수 있지만, 나는 SayCan을 \"아하\" 순간으로 꼽을 것입니다. 커뮤니티가 인지한 것은 계획의 많은 부분을 \"의미 공간\"으로 옮길 수 있다면 기하학 공간이 아닌 곳에서 계획을 수행할 수 있으며, 이를 통해 LLMs를 사용하여 이 작업을 수행할 수 있고, 데이터를 수집하거나 로봇에 특화된 온톨로지를 작성하거나 상징적 추론 엔진을 구축할 필요 없이 그들의 상식 능력의 모든 이점을 누릴 수 있음을 깨달은 순간이었다고 생각합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_2.png\"\u003e\n\u003cp\u003e인지와 실행 모두 자연어를 사용하는 인터페이스를 갖게되면 모든 것에 대해 언어를 API로 사용하는 것이 매우 유혹적일 것입니다. 언어에는 많은 장점이 있습니다: 유연하며 해석 가능하며 선택한 추상화 수준으로 사물을 설명할 수 있습니다. 이것은 고정된 API에 대한 거대한 문제였습니다: 예를 들어 자율 주행 자동차에게 세계가 바운딩 박스의 모음처럼 보이는 것은 괜찮을 수 있지만, 사물에 직접 접촉하려고 하면 아마도 더 풍부한 지오메트리와 의미론적 정보가 필요할 것입니다. 혹시 계획자가 인식 모듈이 제공할 유용한 정보를 미리 알지 못할 수도 있습니다. 아마도 양방향 대화가 필요할지도 모릅니다...\u003c/p\u003e\n\u003cp\u003e이것이 그 여정의 다음 단계로 나아가는데요: 계획자와 인지 시스템이 모두 자연어를 사용하도록합시다. VLM들이 정말 뛰어나게 발전하고 있으니, 이를 활용하여 양방향 대화를 실제 대화로 만들어봅시다. 이것이 Socratic Models의 아이디어입니다. 여기서는 세상의 상태와 그에 대한 행동 방법에 대한 합의를 모델 사이의 대화를 통해 달성할 수 있습니다. Inner Monologue는 주기적인 상태 재추정 및 재계획을 대화의 일부로 만드는 개념을 더 발전시켰습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_3.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e로봇 지능의 중추로 LLM이 있다면 새로운 일을 많이 할 수 있습니다. 예를 들어 AutoRT에서는 LLM을 활용하여 수행할 새로운 작업을 꿈꿨는데, 이로 인해 즉시 제기된 질문은 다음과 같습니다: 로봇이 스스로 해야 할 일을 생각한다면, 어떻게 그것들이 안전하고 유익한지를 보장할 수 있을까요? 우리는 LLM을 안전한 개념(“날카로운 물체를 집지 마세요”)으로 유도하거나 더 일반적인 인간 중심 가치를 제시할 수 있습니다. “인간에게 상처를 입히지 마십시오…” 소리가 익숙하신가요? 몇 년 전에 실제 로봇에 아지모프의 로봇 법칙을 구현할 수 있는 상당히 타당한 경로가 있다고 말해주었다면, 믿지 않았을 것입니다. 향후 시간이 이것을 사용할지 여부를 알려줄 것입니다. 헌법 AI를 로봇의 안전 스택의 일부로 사용하는 것이 실용적인지는 앞으로 알게 될 것이지만, 실제 세계에서 이에 대해 이야기하고 평가할 수 있다는 사실은공신입니다.\u003c/p\u003e\n\u003cp\u003e작동 구성 요소는 어떻게 되나요? 전통적인 로봇 공학의 마지막 요새인 그것조차 LLM 처리를 받을 수 있을까요? LLM이 정말 잘하는 한 가지는 코드 생성입니다. 결국, 컨트롤러 소프트웨어는 정책을 기술한 코드일 뿐입니다. 여기서 코드로 정책이라는 개념이 등장하며, LLM에게 저수준 제어 API를 제시하고 실제 실행할 정책을 설명하도록 할 수 있다는 아이디어가 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_4.png\"\u003e\n\u003cp\u003e코드 LMs는 제로샷에서 아주 잘 작동하는데요, 단지 프롬프트 디자인의 암흑 예술에 능숙해야 합니다. 마이크로소프트 동료들의 초창기 ChatGPT for Robotics 실험에서 사용된 것처럼 대화 전략을 사용하여 프롬프트를 반복하고 향상시킬 수 있습니다. 하지만 더 좋은 점은 제어 행위의 상호 작용을 통해 코드 LM을 세밀하게 조정하고 개선하는 반복적인 과정을 거칠 때입니다. 이것은 우리가 고전적인 모델 예측 제어에 유사하게 언어 모델 예측 제어라고 이름 붙인 것입니다. 이를 통해 모델이 새로운 작업에 대해 더 나은 제로샷 수행뿐만 아니라 사용자 상호 작용에서 더 빨리 학습할 수 있게 됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e여러 LLM이 내부 채팅방에서 소통하는 이미지가 로봇의 중추가 되면, 문제를 세 가지 구성 요소로 인수 분해하는 것이 여전히 유용한지 의문스러울 수 있습니다. 신경망은 서로 고대역폭의 미분 가능한 표현을 통해 통신할 수 있기 때문에, 왜 그것들을 단어로 축소시켜야 할까요? 해석 가능성을 어느 정도 얻을 수는 있지만, 정보 손실은 상당합니다. 예를 들어, 계획자가 여전히 본질적으로 맹목적이라고 상상해 보세요. 이러한 구성 요소를 일부 병합할 수 있을까요? 단순히 끝점 열광 때문이 아니라, 이미 이러한 모델에 내재된 모듈성 덕분에 가능합니다. 서로 다른 transformer 구성 요소가 서로 상호 작용하도록 허용함으로써 신경망 내부에서 '관심사의 분리'를 재현할 수 있기 때문입니다.\u003c/p\u003e\n\u003cp\u003e이 방향으로 첫 실험이 PaLM-E를 사용하여 인식과 계획을 병합하려고 시도되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-TheStateofRobotLearning_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e인식 및 계획 모듈을 공동 훈련하면 명확한 향상이 관찰되었으며, 작업 및 실행체에 대한 전이 증거도 있었습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다음 실험은 계획자를 제외하고 지각과 행동을 합치는 것이었습니다. \"픽셀에서 행동\" 모델이 많이 나왔지만 우리에게 혁신적인 방법은 RT-1이었습니다. TRI의 액션 확산, 버클리의 휴머노이드 트랜스포머, 스탠포드의 ACT와 Octo와 같은 방식으로 최근 몇 달 동안 이 분야에서 많은 일이 벌어졌으며, 성능과 기능성의 폭발을 보는 것은 정말 멋진 일이었습니다.\u003c/p\u003e\n\u003cp\u003e지금쯤 어디로 향하고 있는지 보이시나요? 반쪽채치런 선택보다 모든 것을 하는 단일 \"로봇 두뇌\"를 훈련하는 것이 더 낫지 않을까요? 이에 대한 우리의 첫 번째 시도는 RT-2이었으며, 여전히 로봇공학 관련 데이터 소스(특히 지각 및 의미적 이해를 위한 인터넷 데이터)를 활용하면서 전체 문제에 대해 공동으로 추론하는 능력이 얼마나 많은 도움을 주는지를 보여주었습니다. Meta의 동료들이 VC-1로 그 방향으로 진행한 또 다른 주목할 만한 한 걸음이었습니다. 다중 모달 모델을 위한 오픈소스 생태계가 번성하고 있으며, 우리가 이러한 모델의 공간 추론 능력을 어디까지 밀어낼 수 있는지 탐구하고 있는 사람들이 많아질 것으로 기대합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e여기서 우리는 어디로 가야 할까요? 반지도 학습 혁명은 왔다가 지났어요. RL은 여전히 상처를 핥고 있습니다. 확산 모델이 잘 돌아가고 있으며 오프라인 RL이 부활하고 있습니다. 데이터 효율성이 크게 향상되었음에도 불구하고, \"실제 세계에서 작동\"을 상상할 수 있게 되었지만, sim-to-real 접근에 모든 것을 걸 필요가 없어진 것처럼 보이는 상황에서도 여전히 데이터에 구애받고 있으며, 데이터 수집의 효율성과 다양성을 향상시키는 것이 중요합니다.\u003c/p\u003e\n\u003cp\u003e오늘날 이 분야에서 가장 큰 긴장감이 있습니다. 한편으로는 교차 존재 모델이 로봇간 능력을 전이하는 데 뛰어나게 작동한다는 것을 보여주고 있습니다. 로봇, 작업 간의 다양성을 높이고 문제에 대해 다양성 중심적인 접근을 취해야 한다는 주장이 있습니다. 다른 한편으로는 \"모두를 지배할 한 가지 형태\"로봇 학습 방식을 원하는 사람이 더 많아지고, 테슬라, 피규어, 1X, 어질리티, Unitree, Sanctuary, Apptronik 등과 같이 억만장자로 유명한 로봇에 투자하는 돈이 늘어나고 있습니다. 후자 방법의 장점은 인간적 존재에서 배우고 인간 공간에 배치될 수 있는 범용 능력이며, 단점은 매우 복잡하고 비싼 하드웨어를 데이터 수집의 중요 경로에 놓게 되어 최종 제품의 경제성에 큰 장벽을 높일 수 있다는 것입니다.\u003c/p\u003e\n\u003cp\u003e그것은 위험한 내기입니다. 특히 교차 존재 가설이 사실로 드러나고 Aloha, UMI, Stretch(및 Dobb.E), Mobile Aloha 및 Aloha 2와 같은 실험을 본다면 싸구려 로봇 떼가 분야를 완전히 혼란스럽게 만들고 다양한 저렴한 모습에 민첩한 능력을 가져올 수 있습니다. 그러나 이 시점에서 그 베팅 어느 쪽에도 100% 돈을 거는 일은 하지 않겠습니다. 앞으로 수개월은 이 분야가 어디로 가야 하는지에 대한 포문이 될 것입니다. 다양성 대 다양성, 싸구려와 두각을 내며 고급 DOF와 완전한 기능, 더 싸고 철저하게, 단점이 있다.\u003c/p\u003e\n\u003cp\u003e오늘 우리가 있는 세계는 어느 정도 둘 다입니다. \"범용\" 어림들이 널리 배치되어 있지만, 그들을 가치 있게 만드는 경제학은 잔혹하며, 그것들이 실제로 유용하게 만들기 위해 전용 도구 및 시스템 통합이 필요합니다.\u003c/p\u003e\n\u003cp\u003e데이터 확장 맥락에서 확신하는 점은 생성 모델이 시뮬레이션의 미래라는 것이며, \"더 나은 시뮬레이터\"뿐만 아니라 3D 및 비디오 생성이 외형뿐만 아니라 물리학 및 공간 관계도 존중하게 만드는 방법을 쫓아야 한다는 것입니다. 날씨 예측부터 단백질 접힘까지 다른 물리학 시뮬레이터 분야마다 생성 모델로 인해 혼란스러워지고 있습니다. 로봇 및 환경의 디지털 복제본을 작성하는 데 몇 시간을 보내는 대신 로봇의 센서를 잡아 \"재생\"을 누르면 가능한 미래를 생성할 수 있다면 시뮬에서 실로ら 릴 체리 뉴스를 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e저는 데이터 확장이 대부분 HRI 문제로 전환되고 있다고 점점 더 확신하게 되고 있습니다. 오늘날 HRI 커뮤니티가 주로 걱정하는 정확한 문제는 아닐 수 있지만; 대부분의 HRI는 최종 사용자가 로봇과 상호 작용하는 부분에 관심을 기울입니다. 새로운 다양한 작업을 설계하고, 데이터를 견고하게 수집하고, 행동을 개선 및 최적화하기 위해 더 나은 HRI 접근 방식이 필요합니다: 모델 및 행동을 훈련시키기 위해 데이터 수집에 필요한 모든 신중한 설계를 최적화하는 것은 최종 사용자가 최종 제품과 상호 작용하기 전에도 이루어져야 합니다. 우리는 실제로 모든 로봇을 눈, 팔, 다리가 있는 챗봇으로 바꿈으로써 HRI 커뮤니티에 새로운 가능성을 제공했습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-TheStateofRobotLearning"},"buildId":"wOkGEDZCvEs3S_XaNsdwr","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>