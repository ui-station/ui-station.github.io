<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>윈도우에서의 오마이 , 오마이 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-OllamaOllamainWindows" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="윈도우에서의 오마이 , 오마이 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="윈도우에서의 오마이 , 오마이 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-OllamaOllamainWindows_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-OllamaOllamainWindows" data-gatsby-head="true"/><meta name="twitter:title" content="윈도우에서의 오마이 , 오마이 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-OllamaOllamainWindows_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 17:46" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_buildManifest.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">윈도우에서의 오마이 , 오마이</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="윈도우에서의 오마이 , 오마이" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-OllamaOllamainWindows&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>Ollama는 주로 대형 언어 모델 (LLM)과 작업하는 데 사용되는 프레임워크 및 라이브러리를 가리킵니다.</p>
<p>로컬에서 LLM 실행을 위한 프레임워크: Ollama는 Llama 2, Mistral, Gemma와 같은 대형 언어 모델을 쉽게 로컬 컴퓨터에서 실행할 수 있는 가벼우면서 확장 가능한 프레임워크입니다. 이는 LLM과 실험하고자 하는 개발자나 제어 환경에서 그들의 동작을 연구하고자 하는 연구자들에게 유용할 수 있습니다.</p>
<h2>Ollama란?</h2>
<p>Ollama는 로컬 사용을 위한 오픈 소스 모델을 획득하도록 도와줍니다. 최적의 소스에서 모델을 자동으로 가져오며, 컴퓨터에 전용 GPU가 있는 경우 수동 구성이 필요하지 않고 GPU 가속을 자동으로 사용합니다. 프롬프트를 수정함으로써 모델을 쉽게 사용자 정의할 수 있으며 이를 위해 Langchain은 필수가 아닙니다. 또한 Ollama는 Docker 이미지로 액세스할 수 있어 사용자 지정 모델을 Docker 컨테이너로 배포할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_0.png" alt="이미지"/></p>
<div class="content-ad"></div>
<h2>Framework</h2>
<p>Ollama은 여러 오픈 소스 LLM을 사용자의 컴퓨터에서 간편하게 설정하고 실행할 수 있는 가벼운 방법을 제공합니다. 이를 통해 복잡한 설정이나 외부 서버에 의존하지 않아도 되므로 다양한 목적에 적합합니다:</p>
<ul>
<li>개발: 클라우드에 배포할 필요 없이 개발자가 LLM 프로젝트를 빠르게 실험하고 반복할 수 있습니다.</li>
<li>연구: 연구자들은 Ollama를 사용하여 제어된 환경에서 LLM의 행동을 연구하고 심층적인 분석을 용이하게 할 수 있습니다.</li>
<li>개인 정보 보호: 로컬에서 LLM을 실행하면 데이터가 사용자의 기기를 벗어나지 않아므로 중요한 정보에 대한 보호가 보장됩니다.</li>
</ul>
<p>Ollama 프레임워크:</p>
<div class="content-ad"></div>
<ul>
<li>간단한 설정: Ollama는 복잡한 구성 파일이나 배포의 필요성을 제거합니다. 모델 파일은 모델 가중치, 구성 및 데이터와 같은 필요한 구성 요소를 정의하여 설정 프로세스를 간소화합니다.</li>
<li>사용자 정의: Ollama를 사용하면 LLM 경험을 사용자 정의할 수 있습니다. 배치 크기, 시퀀스 길이 및 빔 검색 설정과 같은 매개변수를 조정하여 모델을 사용자의 특정 요구에 맞게 세밀하게 조정할 수 있습니다.</li>
<li>Multi-GPU 지원: Ollama는 기계에서 여러 GPU를 활용하여 성능이 중요한 작업에 대해 더 빠른 추론 및 향상된 성능을 제공합니다.</li>
<li>확장 가능한 아키텍처: 프레임워크는 모듈식 및 확장 가능하도록 설계되었습니다. 사용자 지정 모듈을 쉽게 통합하거나 기능을 확장하기 위해 커뮤니티에서 개발된 플러그인을 탐색할 수 있습니다.</li>
</ul>
<h2>라이브러리:</h2>
<p>Ollama에는 다음과 같은 사전 빌드된 훈련된 언어 모델 라이브러리가 포함되어 있습니다:</p>
<ul>
<li>Llama 2: 텍스트 생성, 번역, 질문에 응답과 같은 다양한 작업을 수행할 수 있는 대규모 언어 모델입니다.</li>
<li>Mistral: 방대한 텍스트와 코드 데이터셋에서 훈련된 사실 기반 언어 모델입니다.</li>
<li>Gemma: 매력적인 대화를 위해 설계된 대화형 언어 모델입니다.</li>
<li>LLaVA: 채팅 및 설명 사용 사례에 대해 훈련된 견고한 모델입니다.</li>
</ul>
<div class="content-ad"></div>
<p>이 라이브러리를 사용하면 미리 훈련된 모델을 쉽게 응용 프로그램에 통합할 수 있어서 처음부터 훈련시킬 필요가 없어 시간과 리소스를 절약할 수 있어요.</p>
<h1>Ollama의 기능</h1>
<p>사용 편의성:</p>
<ul>
<li>간편한 설치: Ollama은 복잡한 구성을 없애는 미리 정의된 &quot;모델 파일&quot;을 활용하여 설치와 설정을 제공됩니다. 기술적 지식이 제한된 사용자도 쉽게 사용할 수 있어요.</li>
<li>사용자 친화적 API: Ollama은 직관적인 API를 통해 미리 훈련된 모델과 상호 작용하여 개발자가 Python 응용 프로그램에 쉽게 LLMs를 통합할 수 있어요.</li>
</ul>
<div class="content-ad"></div>
<p>확장성:</p>
<ul>
<li>사용자 정의 가능한 모델: Ollama는 다양한 매개변수의 조절을 허용하여 사용자가 특정 작업과 선호도에 맞게 모델을 세밀하게 조정할 수 있도록 합니다.</li>
<li>모듈화된 아키텍처: 이 프레임워크는 사용자 정의 모듈과 커뮤니티에서 개발한 플러그인을 지원하여 개별적인 요구에 맞춰 확장성과 사용자 정의를 촉진합니다.</li>
</ul>
<p>강력한 기능:</p>
<ul>
<li>사전 훈련된 모델: Ollama는 텍스트 생성, 번역, 질문 응답 및 코드 생성과 같은 다양한 작업을 수행할 수 있는 사전 훈련된 LLM 라이브러리를 제공합니다.</li>
<li>로컬 실행: LLM은 완전히 사용자의 장치에서 실행되어 클라우드 배포가 필요 없으며 데이터 개인 정보 보호를 보장합니다.</li>
<li>멀티-GPU 지원: Ollama는 여러 GPU를 활용하여 추론 속도를 높이고 자원 집약적인 작업에서 성능을 향상시킵니다.</li>
</ul>
<div class="content-ad"></div>
<p>오픈소스 및 협업:</p>
<ul>
<li>무료 제공: Ollama의 오픈소스 성격은 누구나 기여하고 커뮤니티 기반 개선을 받아들일 수 있습니다.</li>
<li>지속적인 발전: Ollama는 적극적으로 유지보수되며 지속적인 업데이트와 향상이 정기적으로 출시됩니다.</li>
</ul>
<p>추가 기능:</p>
<ul>
<li>가벼움: Ollama는 효율적으로 작동하여 하드웨어 자원이 제한된 컴퓨터에 적합합니다.</li>
<li>오프라인 기능: 사전 훈련된 모델은 인터넷 연결 없이도 사용할 수 있어 유연성과 접근성을 제공합니다.</li>
</ul>
<div class="content-ad"></div>
<h1>윈도우에서의 Ollama:</h1>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_1.png" alt="Ollama 이미지"/></p>
<p>Ollama가 미리보기로 Windows에 제공되어 이제 새로운 네이티브 Windows 경험에서 대형 언어 모델을 끌어모으고 실행하고 생성하는 것이 가능해졌습니다. Windows에서의 Ollama에는 내장 GPU 가속, 전체 모델 라이브러리 접근 및 OpenAI 호환을 포함한 Ollama API가 포함되어 있습니다.</p>
<h1>하드웨어 가속</h1>
<div class="content-ad"></div>
<p>Ollama은 NVIDIA GPU 및 AVX 및 AVX2와 같은 현대 CPU 명령어 세트를 사용하여 모델을 가속화합니다. 구성이나 가상화 설정이 필요하지 않습니다!</p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_2.png" alt="image"/></p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_3.png" alt="image"/></p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_4.png" alt="image"/></p>
<div class="content-ad"></div>
<h1>모델 라이브러리의 전체 액세스</h1>
<p>Windows에서 실행할 수 있는 전체 Ollama 모델 라이브러리를 사용할 수 있습니다. LLaVA 1.6과 같은 비전 모델을 실행할 때는, 이미지를 Ollama 실행창으로 끌어다 놓아 메시지에 추가할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_5.png" alt="이미지1"/></p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_6.png" alt="이미지2"/></p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_7.png" alt="이미지"/></p>
<h1>항상 켜져 있는 Ollama API</h1>
<p>Ollama의 API는 자동으로 백그라운드에서 실행되며 http://localhost:11434에서 제공됩니다. 도구 및 응용 프로그램은 추가 설정 없이 연결할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_8.png" alt="이미지"/></p>
<div class="content-ad"></div>
<p>예를 들어, PowerShell을 사용하여 Ollama의 API를 호출하는 방법은 다음과 같습니다:</p>
<pre><code class="hljs language-js">(<span class="hljs-title class_">Invoke</span>-<span class="hljs-title class_">WebRequest</span> -method <span class="hljs-variable constant_">POST</span> -<span class="hljs-title class_">Body</span> <span class="hljs-string">&#x27;{&quot;model&quot;:&quot;llama2&quot;, &quot;prompt&quot;:&quot;Why is the sky blue?&quot;, &quot;stream&quot;: false}&#x27;</span> -uri <span class="hljs-attr">http</span>:<span class="hljs-comment">//localhost:11434/api/generate ).Content | ConvertFrom-json</span>
</code></pre>
<p>Windows에서 Ollama는 다른 플랫폼과 동일한 OpenAI 호환성을 지원하며, Ollama를 통해 로컬 모델을 사용하여 OpenAI용으로 작성된 기존 툴을 사용할 수 있도록 합니다.</p>
<p>Windows 미리보기에서 Ollama를 시작하려면:</p>
<div class="content-ad"></div>
<ul>
<li>Windows 용 Ollama 다운로드</li>
<li>OllamaSetup.exe를 더블 클릭하여 설치 프로그램을 실행하십시오.</li>
<li>설치 후 좋아하는 터미널을 열고 ollama run llama2를 실행하여 모델을 실행하십시오.</li>
</ul>
<p>새 버전이 출시되면 Ollama가 업데이트를 알려줍니다.</p>
<p>ollama serve:</p>
<p>이 명령은 Ollama 서버를 시작하여 다운로드한 모델을 API를 통해 접근할 수 있게 합니다. 이를 통해 웹 브라우저, 모바일 앱 또는 사용자 지정 스크립트와 같은 다양한 응용 프로그램에서 모델과 상호 작용할 수 있습니다.</p>
<div class="content-ad"></div>
<p>한가지 비유를 들어 볼게요. Ollama는 당신의 책(LMMs)을 보관하는 도서관 역할을 합니다. ollama serve를 실행하면 도서관을 열어 다른 사람들이 해당 도서관 시스템(API)을 통해 책에 접근하여 읽을(상호 작용할) 수 있게 됩니다.</p>
<p><img src="/assets/img/2024-05-18-OllamaOllamainWindows_9.png" alt="이미지"/></p>
<p>ollama run phi:</p>
<p>이 명령어는 특히 &quot;phi&quot; 모델을 다운로드하고 로컬 머신에서 실행하는 데 사용됩니다. &quot;phi&quot;란 Ollama 도서관에 존재하는 사전 학습된 LMM으로, GPT-3와 유사한 기능을 갖추고 있습니다.</p>
<div class="content-ad"></div>
<p>아래에 있는 것이 유사성 확장입니다:
만약 ollama serve가 도서관을 열면, ollama run phi는(이하 phi)이라는 특정 책을 (Ollama)사서로부터 요청하고(다운로드된 모델 필요) 그것을 읽는(모델 실행) 것이 로컬 머신(당신의 컴퓨터) 내에서 일어나는 일 같습니다.</p>
<ul>
<li>ollama serve는 미리 다운로드된 모델이 요구됩니다. 특정 모델을 다운로드하려면 ollama pull <code>model_name</code>을 사용하세요.</li>
<li>ollama run phi는 “phi”모델을 특정하게 다운로드하고 실행합니다.</li>
<li>ollama serve는 API를 통해 다운로드된 모델에 접근할 수 있게 해주는데 반해, ollama run phi는 로컬에서 특정 모델을 실행하는 데 중점을 둡니다.</li>
</ul>
<p>일반 명령어:</p>
<ul>
<li>ollama list: 시스템에 다운로드된 모든 모델을 나열합니다.</li>
<li>ollama rm <code>model_name</code>: 시스템에서 다운로드된 모델을 제거합니다.</li>
<li>ollama cp <code>model_name1</code> <code>model_name2</code>: 다운로드된 모델을 새 이름으로 복사합니다.</li>
<li>ollama info <code>model_name</code>: 다운로드된 모델에 대한 정보를 표시합니다.</li>
<li>ollama help: 사용 가능한 모든 명령어에 대한 도움말 문서를 제공합니다.</li>
</ul>
<div class="content-ad"></div>
<p>모델 관리:</p>
<ul>
<li><code>ollama pull model_name</code>: Ollama 모델 허브에서 모델을 다운로드합니다.</li>
</ul>
<p>모델 실행:</p>
<ul>
<li><code>ollama run model_name</code>: 로컬에서 다운로드한 모델을 실행합니다.</li>
<li><code>ollama serve</code>: Ollama 서버를 시작하여 API를 통해 다운로드한 모델에 액세스할 수 있게 합니다.</li>
</ul>
<div class="content-ad"></div>
<p>추가 명령:</p>
<ul>
<li>ollama update: Ollama를 최신 버전으로 업데이트합니다.</li>
<li>ollama config: Ollama 구성 설정을 관리합니다.</li>
</ul>
<img src="/assets/img/2024-05-18-OllamaOllamainWindows_10.png"/>
<h1>Langchain을 통한 Ollama:</h1>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> langchain_community.llms <span class="hljs-keyword">import</span> Ollama

llm = Ollama(model=<span class="hljs-string">&quot;llama2&quot;</span>)

llm.invoke(<span class="hljs-string">&quot;Tell me a joke&quot;</span>)
</code></pre>
<pre><code class="hljs language-python"><span class="hljs-string">&quot;물론이죠! 여기 빠른 하나 있어요:\n\n과학자들이 원자를 믿지 않는 이유는 뭘까요?\n왜냐하면 그들이 모든 것으로 이루어져 있으니까요!\n\n당신의 얼굴에 미소를 띄우길 바랍니다!&quot;</span>
</code></pre>
<p>토큰을 스트리밍하려면 .stream(...) 메서드를 사용하세요:</p>
<pre><code class="hljs language-python">query = <span class="hljs-string">&quot;Tell me a joke&quot;</span>

<span class="hljs-keyword">for</span> chunks <span class="hljs-keyword">in</span> llm.stream(query):
    <span class="hljs-built_in">print</span>(chunks)
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js">확실해요, 여기 하나 있어요:

왜 과학자들이 원자를 믿지 않을까요?
왜냐하면 그들이 모든 것을 이루기 때문이에요!

저렇게 즐겁게 느껴지셨으면 좋겠어요! 더 듣고 싶나요?
</code></pre>
<h1>Multi-modal</h1>
<p>Ollama는 bakllava와 llava와 같은 멀티 모달 LLM을 지원합니다.</p>
<p>ollama pull bakllava</p>
<div class="content-ad"></div>
<p>Ollama를 업데이트하여 멀티 모달을 지원하는 최신 버전을 사용해보세요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">llms</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Ollama</span>

bakllava = <span class="hljs-title class_">Ollama</span>(model=<span class="hljs-string">&quot;bakllava&quot;</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> <span class="hljs-title class_">BytesIO</span>

<span class="hljs-keyword">from</span> <span class="hljs-title class_">IPython</span>.<span class="hljs-property">display</span> <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">HTML</span>, display
<span class="hljs-keyword">from</span> <span class="hljs-variable constant_">PIL</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Image</span>


def <span class="hljs-title function_">convert_to_base64</span>(pil_image):
    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;
    PIL 이미지를 Base64로 인코딩된 문자열로 변환

    :param pil_image: PIL 이미지
    :return: 리사이즈된 Base64 문자열
    &quot;</span><span class="hljs-string">&quot;&quot;</span>

    buffered = <span class="hljs-title class_">BytesIO</span>()
    pil_image.<span class="hljs-title function_">save</span>(buffered, format=<span class="hljs-string">&quot;JPEG&quot;</span>)  # 필요시 형식 변경 가능
    img_str = base64.<span class="hljs-title function_">b64encode</span>(buffered.<span class="hljs-title function_">getvalue</span>()).<span class="hljs-title function_">decode</span>(<span class="hljs-string">&quot;utf-8&quot;</span>)
    <span class="hljs-keyword">return</span> img_str


def <span class="hljs-title function_">plt_img_base64</span>(img_base64):
    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;
    Base64로 인코드된 문자열을 이미지로 표시

    :param img_base64:  Base64 문자열
    &quot;</span><span class="hljs-string">&quot;&quot;</span>
    # <span class="hljs-title class_">Base64</span> 문자열을 소스로 사용하는 <span class="hljs-variable constant_">HTML</span> img 태그 생성
    image_html = f<span class="hljs-string">&#x27;&lt;img src=&quot;data:image/jpeg;base64,{img_base64}&quot; /&gt;&#x27;</span>
    # <span class="hljs-variable constant_">HTML</span>을 렌더링하여 이미지 표시
    <span class="hljs-title function_">display</span>(<span class="hljs-title function_">HTML</span>(image_html))


file_path = <span class="hljs-string">&quot;../../../static/img/ollama_example_img.jpg&quot;</span>
pil_image = <span class="hljs-title class_">Image</span>.<span class="hljs-title function_">open</span>(file_path)
image_b64 = <span class="hljs-title function_">convert_to_base64</span>(pil_image)
<span class="hljs-title function_">plt_img_base64</span>(image_b64)
</code></pre>
<img src="/assets/img/2024-05-18-OllamaOllamainWindows_11.png"/>
<div class="content-ad"></div>
<pre><code class="hljs language-js">llm_with_image_context = bakllava.<span class="hljs-title function_">bind</span>(images=[image_b64])
llm_with_image_context.<span class="hljs-title function_">invoke</span>(<span class="hljs-string">&quot;달러 기반 총 유지율은 얼마입니까:&quot;</span>)
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-string">&#x27;90%&#x27;</span>
</code></pre></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"윈도우에서의 오마이 , 오마이","description":"","date":"2024-05-18 17:46","slug":"2024-05-18-OllamaOllamainWindows","content":"\n\nOllama는 주로 대형 언어 모델 (LLM)과 작업하는 데 사용되는 프레임워크 및 라이브러리를 가리킵니다.\n\n로컬에서 LLM 실행을 위한 프레임워크: Ollama는 Llama 2, Mistral, Gemma와 같은 대형 언어 모델을 쉽게 로컬 컴퓨터에서 실행할 수 있는 가벼우면서 확장 가능한 프레임워크입니다. 이는 LLM과 실험하고자 하는 개발자나 제어 환경에서 그들의 동작을 연구하고자 하는 연구자들에게 유용할 수 있습니다.\n\n## Ollama란?\n\nOllama는 로컬 사용을 위한 오픈 소스 모델을 획득하도록 도와줍니다. 최적의 소스에서 모델을 자동으로 가져오며, 컴퓨터에 전용 GPU가 있는 경우 수동 구성이 필요하지 않고 GPU 가속을 자동으로 사용합니다. 프롬프트를 수정함으로써 모델을 쉽게 사용자 정의할 수 있으며 이를 위해 Langchain은 필수가 아닙니다. 또한 Ollama는 Docker 이미지로 액세스할 수 있어 사용자 지정 모델을 Docker 컨테이너로 배포할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-OllamaOllamainWindows_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Framework\n\nOllama은 여러 오픈 소스 LLM을 사용자의 컴퓨터에서 간편하게 설정하고 실행할 수 있는 가벼운 방법을 제공합니다. 이를 통해 복잡한 설정이나 외부 서버에 의존하지 않아도 되므로 다양한 목적에 적합합니다:\n\n- 개발: 클라우드에 배포할 필요 없이 개발자가 LLM 프로젝트를 빠르게 실험하고 반복할 수 있습니다.\n- 연구: 연구자들은 Ollama를 사용하여 제어된 환경에서 LLM의 행동을 연구하고 심층적인 분석을 용이하게 할 수 있습니다.\n- 개인 정보 보호: 로컬에서 LLM을 실행하면 데이터가 사용자의 기기를 벗어나지 않아므로 중요한 정보에 대한 보호가 보장됩니다.\n\nOllama 프레임워크:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 간단한 설정: Ollama는 복잡한 구성 파일이나 배포의 필요성을 제거합니다. 모델 파일은 모델 가중치, 구성 및 데이터와 같은 필요한 구성 요소를 정의하여 설정 프로세스를 간소화합니다.\n- 사용자 정의: Ollama를 사용하면 LLM 경험을 사용자 정의할 수 있습니다. 배치 크기, 시퀀스 길이 및 빔 검색 설정과 같은 매개변수를 조정하여 모델을 사용자의 특정 요구에 맞게 세밀하게 조정할 수 있습니다.\n- Multi-GPU 지원: Ollama는 기계에서 여러 GPU를 활용하여 성능이 중요한 작업에 대해 더 빠른 추론 및 향상된 성능을 제공합니다.\n- 확장 가능한 아키텍처: 프레임워크는 모듈식 및 확장 가능하도록 설계되었습니다. 사용자 지정 모듈을 쉽게 통합하거나 기능을 확장하기 위해 커뮤니티에서 개발된 플러그인을 탐색할 수 있습니다.\n\n## 라이브러리:\n\nOllama에는 다음과 같은 사전 빌드된 훈련된 언어 모델 라이브러리가 포함되어 있습니다:\n\n- Llama 2: 텍스트 생성, 번역, 질문에 응답과 같은 다양한 작업을 수행할 수 있는 대규모 언어 모델입니다.\n- Mistral: 방대한 텍스트와 코드 데이터셋에서 훈련된 사실 기반 언어 모델입니다.\n- Gemma: 매력적인 대화를 위해 설계된 대화형 언어 모델입니다.\n- LLaVA: 채팅 및 설명 사용 사례에 대해 훈련된 견고한 모델입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 라이브러리를 사용하면 미리 훈련된 모델을 쉽게 응용 프로그램에 통합할 수 있어서 처음부터 훈련시킬 필요가 없어 시간과 리소스를 절약할 수 있어요.\n\n# Ollama의 기능\n\n사용 편의성:\n\n- 간편한 설치: Ollama은 복잡한 구성을 없애는 미리 정의된 \"모델 파일\"을 활용하여 설치와 설정을 제공됩니다. 기술적 지식이 제한된 사용자도 쉽게 사용할 수 있어요.\n- 사용자 친화적 API: Ollama은 직관적인 API를 통해 미리 훈련된 모델과 상호 작용하여 개발자가 Python 응용 프로그램에 쉽게 LLMs를 통합할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장성:\n\n- 사용자 정의 가능한 모델: Ollama는 다양한 매개변수의 조절을 허용하여 사용자가 특정 작업과 선호도에 맞게 모델을 세밀하게 조정할 수 있도록 합니다.\n- 모듈화된 아키텍처: 이 프레임워크는 사용자 정의 모듈과 커뮤니티에서 개발한 플러그인을 지원하여 개별적인 요구에 맞춰 확장성과 사용자 정의를 촉진합니다.\n\n강력한 기능:\n\n- 사전 훈련된 모델: Ollama는 텍스트 생성, 번역, 질문 응답 및 코드 생성과 같은 다양한 작업을 수행할 수 있는 사전 훈련된 LLM 라이브러리를 제공합니다.\n- 로컬 실행: LLM은 완전히 사용자의 장치에서 실행되어 클라우드 배포가 필요 없으며 데이터 개인 정보 보호를 보장합니다.\n- 멀티-GPU 지원: Ollama는 여러 GPU를 활용하여 추론 속도를 높이고 자원 집약적인 작업에서 성능을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오픈소스 및 협업:\n\n- 무료 제공: Ollama의 오픈소스 성격은 누구나 기여하고 커뮤니티 기반 개선을 받아들일 수 있습니다.\n- 지속적인 발전: Ollama는 적극적으로 유지보수되며 지속적인 업데이트와 향상이 정기적으로 출시됩니다.\n\n추가 기능:\n\n- 가벼움: Ollama는 효율적으로 작동하여 하드웨어 자원이 제한된 컴퓨터에 적합합니다.\n- 오프라인 기능: 사전 훈련된 모델은 인터넷 연결 없이도 사용할 수 있어 유연성과 접근성을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 윈도우에서의 Ollama:\n\n![Ollama 이미지](/assets/img/2024-05-18-OllamaOllamainWindows_1.png)\n\nOllama가 미리보기로 Windows에 제공되어 이제 새로운 네이티브 Windows 경험에서 대형 언어 모델을 끌어모으고 실행하고 생성하는 것이 가능해졌습니다. Windows에서의 Ollama에는 내장 GPU 가속, 전체 모델 라이브러리 접근 및 OpenAI 호환을 포함한 Ollama API가 포함되어 있습니다.\n\n# 하드웨어 가속\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOllama은 NVIDIA GPU 및 AVX 및 AVX2와 같은 현대 CPU 명령어 세트를 사용하여 모델을 가속화합니다. 구성이나 가상화 설정이 필요하지 않습니다!\n\n![image](/assets/img/2024-05-18-OllamaOllamainWindows_2.png)\n\n![image](/assets/img/2024-05-18-OllamaOllamainWindows_3.png)\n\n![image](/assets/img/2024-05-18-OllamaOllamainWindows_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모델 라이브러리의 전체 액세스\n\nWindows에서 실행할 수 있는 전체 Ollama 모델 라이브러리를 사용할 수 있습니다. LLaVA 1.6과 같은 비전 모델을 실행할 때는, 이미지를 Ollama 실행창으로 끌어다 놓아 메시지에 추가할 수 있습니다.\n\n![이미지1](/assets/img/2024-05-18-OllamaOllamainWindows_5.png)\n\n![이미지2](/assets/img/2024-05-18-OllamaOllamainWindows_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-18-OllamaOllamainWindows_7.png)\n\n# 항상 켜져 있는 Ollama API\n\nOllama의 API는 자동으로 백그라운드에서 실행되며 http://localhost:11434에서 제공됩니다. 도구 및 응용 프로그램은 추가 설정 없이 연결할 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-OllamaOllamainWindows_8.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, PowerShell을 사용하여 Ollama의 API를 호출하는 방법은 다음과 같습니다:\n\n```js\n(Invoke-WebRequest -method POST -Body '{\"model\":\"llama2\", \"prompt\":\"Why is the sky blue?\", \"stream\": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n```\n\nWindows에서 Ollama는 다른 플랫폼과 동일한 OpenAI 호환성을 지원하며, Ollama를 통해 로컬 모델을 사용하여 OpenAI용으로 작성된 기존 툴을 사용할 수 있도록 합니다.\n\nWindows 미리보기에서 Ollama를 시작하려면:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Windows 용 Ollama 다운로드\n- OllamaSetup.exe를 더블 클릭하여 설치 프로그램을 실행하십시오.\n- 설치 후 좋아하는 터미널을 열고 ollama run llama2를 실행하여 모델을 실행하십시오.\n\n새 버전이 출시되면 Ollama가 업데이트를 알려줍니다.\n\nollama serve:\n\n이 명령은 Ollama 서버를 시작하여 다운로드한 모델을 API를 통해 접근할 수 있게 합니다. 이를 통해 웹 브라우저, 모바일 앱 또는 사용자 지정 스크립트와 같은 다양한 응용 프로그램에서 모델과 상호 작용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한가지 비유를 들어 볼게요. Ollama는 당신의 책(LMMs)을 보관하는 도서관 역할을 합니다. ollama serve를 실행하면 도서관을 열어 다른 사람들이 해당 도서관 시스템(API)을 통해 책에 접근하여 읽을(상호 작용할) 수 있게 됩니다.\n\n![이미지](/assets/img/2024-05-18-OllamaOllamainWindows_9.png)\n\nollama run phi:\n\n이 명령어는 특히 \"phi\" 모델을 다운로드하고 로컬 머신에서 실행하는 데 사용됩니다. \"phi\"란 Ollama 도서관에 존재하는 사전 학습된 LMM으로, GPT-3와 유사한 기능을 갖추고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래에 있는 것이 유사성 확장입니다: \n만약 ollama serve가 도서관을 열면, ollama run phi는(이하 phi)이라는 특정 책을 (Ollama)사서로부터 요청하고(다운로드된 모델 필요) 그것을 읽는(모델 실행) 것이 로컬 머신(당신의 컴퓨터) 내에서 일어나는 일 같습니다.\n\n- ollama serve는 미리 다운로드된 모델이 요구됩니다. 특정 모델을 다운로드하려면 ollama pull `model_name`을 사용하세요.\n- ollama run phi는 “phi”모델을 특정하게 다운로드하고 실행합니다.\n- ollama serve는 API를 통해 다운로드된 모델에 접근할 수 있게 해주는데 반해, ollama run phi는 로컬에서 특정 모델을 실행하는 데 중점을 둡니다.\n\n일반 명령어:\n\n- ollama list: 시스템에 다운로드된 모든 모델을 나열합니다.\n- ollama rm `model_name`: 시스템에서 다운로드된 모델을 제거합니다.\n- ollama cp `model_name1` `model_name2`: 다운로드된 모델을 새 이름으로 복사합니다.\n- ollama info `model_name`: 다운로드된 모델에 대한 정보를 표시합니다.\n- ollama help: 사용 가능한 모든 명령어에 대한 도움말 문서를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 관리:\n\n- `ollama pull model_name`: Ollama 모델 허브에서 모델을 다운로드합니다.\n\n모델 실행:\n\n- `ollama run model_name`: 로컬에서 다운로드한 모델을 실행합니다.\n- `ollama serve`: Ollama 서버를 시작하여 API를 통해 다운로드한 모델에 액세스할 수 있게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n추가 명령:\n\n- ollama update: Ollama를 최신 버전으로 업데이트합니다.\n- ollama config: Ollama 구성 설정을 관리합니다.\n\n\n\u003cimg src=\"/assets/img/2024-05-18-OllamaOllamainWindows_10.png\" /\u003e\n\n\n# Langchain을 통한 Ollama:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom langchain_community.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\n\nllm.invoke(\"Tell me a joke\")\n```\n\n```python\n\"물론이죠! 여기 빠른 하나 있어요:\\n\\n과학자들이 원자를 믿지 않는 이유는 뭘까요?\\n왜냐하면 그들이 모든 것으로 이루어져 있으니까요!\\n\\n당신의 얼굴에 미소를 띄우길 바랍니다!\"\n```\n\n토큰을 스트리밍하려면 .stream(...) 메서드를 사용하세요:\n\n```python\nquery = \"Tell me a joke\"\n\nfor chunks in llm.stream(query):\n    print(chunks)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n확실해요, 여기 하나 있어요:\n\n왜 과학자들이 원자를 믿지 않을까요?\n왜냐하면 그들이 모든 것을 이루기 때문이에요!\n\n저렇게 즐겁게 느껴지셨으면 좋겠어요! 더 듣고 싶나요?\n```\n\n# Multi-modal\n\nOllama는 bakllava와 llava와 같은 멀티 모달 LLM을 지원합니다.\n\nollama pull bakllava\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nOllama를 업데이트하여 멀티 모달을 지원하는 최신 버전을 사용해보세요.\n\n```js\nfrom langchain_community.llms import Ollama\n\nbakllava = Ollama(model=\"bakllava\")\n```\n\n```js\nimport base64\nfrom io import BytesIO\n\nfrom IPython.display import HTML, display\nfrom PIL import Image\n\n\ndef convert_to_base64(pil_image):\n    \"\"\"\n    PIL 이미지를 Base64로 인코딩된 문자열로 변환\n\n    :param pil_image: PIL 이미지\n    :return: 리사이즈된 Base64 문자열\n    \"\"\"\n\n    buffered = BytesIO()\n    pil_image.save(buffered, format=\"JPEG\")  # 필요시 형식 변경 가능\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return img_str\n\n\ndef plt_img_base64(img_base64):\n    \"\"\"\n    Base64로 인코드된 문자열을 이미지로 표시\n\n    :param img_base64:  Base64 문자열\n    \"\"\"\n    # Base64 문자열을 소스로 사용하는 HTML img 태그 생성\n    image_html = f'\u003cimg src=\"data:image/jpeg;base64,{img_base64}\" /\u003e'\n    # HTML을 렌더링하여 이미지 표시\n    display(HTML(image_html))\n\n\nfile_path = \"../../../static/img/ollama_example_img.jpg\"\npil_image = Image.open(file_path)\nimage_b64 = convert_to_base64(pil_image)\nplt_img_base64(image_b64)\n```\n\n\u003cimg src=\"/assets/img/2024-05-18-OllamaOllamainWindows_11.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nllm_with_image_context = bakllava.bind(images=[image_b64])\nllm_with_image_context.invoke(\"달러 기반 총 유지율은 얼마입니까:\")\n```\n\n```js\n'90%'\n```","ogImage":{"url":"/assets/img/2024-05-18-OllamaOllamainWindows_0.png"},"coverImage":"/assets/img/2024-05-18-OllamaOllamainWindows_0.png","tag":["Tech"],"readingTime":9},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h2: \"h2\",\n    img: \"img\",\n    ul: \"ul\",\n    li: \"li\",\n    h1: \"h1\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Ollama는 주로 대형 언어 모델 (LLM)과 작업하는 데 사용되는 프레임워크 및 라이브러리를 가리킵니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"로컬에서 LLM 실행을 위한 프레임워크: Ollama는 Llama 2, Mistral, Gemma와 같은 대형 언어 모델을 쉽게 로컬 컴퓨터에서 실행할 수 있는 가벼우면서 확장 가능한 프레임워크입니다. 이는 LLM과 실험하고자 하는 개발자나 제어 환경에서 그들의 동작을 연구하고자 하는 연구자들에게 유용할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Ollama란?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama는 로컬 사용을 위한 오픈 소스 모델을 획득하도록 도와줍니다. 최적의 소스에서 모델을 자동으로 가져오며, 컴퓨터에 전용 GPU가 있는 경우 수동 구성이 필요하지 않고 GPU 가속을 자동으로 사용합니다. 프롬프트를 수정함으로써 모델을 쉽게 사용자 정의할 수 있으며 이를 위해 Langchain은 필수가 아닙니다. 또한 Ollama는 Docker 이미지로 액세스할 수 있어 사용자 지정 모델을 Docker 컨테이너로 배포할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_0.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Framework\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama은 여러 오픈 소스 LLM을 사용자의 컴퓨터에서 간편하게 설정하고 실행할 수 있는 가벼운 방법을 제공합니다. 이를 통해 복잡한 설정이나 외부 서버에 의존하지 않아도 되므로 다양한 목적에 적합합니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"개발: 클라우드에 배포할 필요 없이 개발자가 LLM 프로젝트를 빠르게 실험하고 반복할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"연구: 연구자들은 Ollama를 사용하여 제어된 환경에서 LLM의 행동을 연구하고 심층적인 분석을 용이하게 할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"개인 정보 보호: 로컬에서 LLM을 실행하면 데이터가 사용자의 기기를 벗어나지 않아므로 중요한 정보에 대한 보호가 보장됩니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama 프레임워크:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"간단한 설정: Ollama는 복잡한 구성 파일이나 배포의 필요성을 제거합니다. 모델 파일은 모델 가중치, 구성 및 데이터와 같은 필요한 구성 요소를 정의하여 설정 프로세스를 간소화합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"사용자 정의: Ollama를 사용하면 LLM 경험을 사용자 정의할 수 있습니다. 배치 크기, 시퀀스 길이 및 빔 검색 설정과 같은 매개변수를 조정하여 모델을 사용자의 특정 요구에 맞게 세밀하게 조정할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Multi-GPU 지원: Ollama는 기계에서 여러 GPU를 활용하여 성능이 중요한 작업에 대해 더 빠른 추론 및 향상된 성능을 제공합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"확장 가능한 아키텍처: 프레임워크는 모듈식 및 확장 가능하도록 설계되었습니다. 사용자 지정 모듈을 쉽게 통합하거나 기능을 확장하기 위해 커뮤니티에서 개발된 플러그인을 탐색할 수 있습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"라이브러리:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama에는 다음과 같은 사전 빌드된 훈련된 언어 모델 라이브러리가 포함되어 있습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Llama 2: 텍스트 생성, 번역, 질문에 응답과 같은 다양한 작업을 수행할 수 있는 대규모 언어 모델입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Mistral: 방대한 텍스트와 코드 데이터셋에서 훈련된 사실 기반 언어 모델입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Gemma: 매력적인 대화를 위해 설계된 대화형 언어 모델입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"LLaVA: 채팅 및 설명 사용 사례에 대해 훈련된 견고한 모델입니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 라이브러리를 사용하면 미리 훈련된 모델을 쉽게 응용 프로그램에 통합할 수 있어서 처음부터 훈련시킬 필요가 없어 시간과 리소스를 절약할 수 있어요.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Ollama의 기능\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"사용 편의성:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"간편한 설치: Ollama은 복잡한 구성을 없애는 미리 정의된 \\\"모델 파일\\\"을 활용하여 설치와 설정을 제공됩니다. 기술적 지식이 제한된 사용자도 쉽게 사용할 수 있어요.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"사용자 친화적 API: Ollama은 직관적인 API를 통해 미리 훈련된 모델과 상호 작용하여 개발자가 Python 응용 프로그램에 쉽게 LLMs를 통합할 수 있어요.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"확장성:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"사용자 정의 가능한 모델: Ollama는 다양한 매개변수의 조절을 허용하여 사용자가 특정 작업과 선호도에 맞게 모델을 세밀하게 조정할 수 있도록 합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"모듈화된 아키텍처: 이 프레임워크는 사용자 정의 모듈과 커뮤니티에서 개발한 플러그인을 지원하여 개별적인 요구에 맞춰 확장성과 사용자 정의를 촉진합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"강력한 기능:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"사전 훈련된 모델: Ollama는 텍스트 생성, 번역, 질문 응답 및 코드 생성과 같은 다양한 작업을 수행할 수 있는 사전 훈련된 LLM 라이브러리를 제공합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"로컬 실행: LLM은 완전히 사용자의 장치에서 실행되어 클라우드 배포가 필요 없으며 데이터 개인 정보 보호를 보장합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"멀티-GPU 지원: Ollama는 여러 GPU를 활용하여 추론 속도를 높이고 자원 집약적인 작업에서 성능을 향상시킵니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"오픈소스 및 협업:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"무료 제공: Ollama의 오픈소스 성격은 누구나 기여하고 커뮤니티 기반 개선을 받아들일 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"지속적인 발전: Ollama는 적극적으로 유지보수되며 지속적인 업데이트와 향상이 정기적으로 출시됩니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"추가 기능:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"가벼움: Ollama는 효율적으로 작동하여 하드웨어 자원이 제한된 컴퓨터에 적합합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"오프라인 기능: 사전 훈련된 모델은 인터넷 연결 없이도 사용할 수 있어 유연성과 접근성을 제공합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"윈도우에서의 Ollama:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_1.png\",\n        alt: \"Ollama 이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama가 미리보기로 Windows에 제공되어 이제 새로운 네이티브 Windows 경험에서 대형 언어 모델을 끌어모으고 실행하고 생성하는 것이 가능해졌습니다. Windows에서의 Ollama에는 내장 GPU 가속, 전체 모델 라이브러리 접근 및 OpenAI 호환을 포함한 Ollama API가 포함되어 있습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"하드웨어 가속\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama은 NVIDIA GPU 및 AVX 및 AVX2와 같은 현대 CPU 명령어 세트를 사용하여 모델을 가속화합니다. 구성이나 가상화 설정이 필요하지 않습니다!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_2.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_4.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"모델 라이브러리의 전체 액세스\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Windows에서 실행할 수 있는 전체 Ollama 모델 라이브러리를 사용할 수 있습니다. LLaVA 1.6과 같은 비전 모델을 실행할 때는, 이미지를 Ollama 실행창으로 끌어다 놓아 메시지에 추가할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_5.png\",\n        alt: \"이미지1\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_6.png\",\n        alt: \"이미지2\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_7.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"항상 켜져 있는 Ollama API\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama의 API는 자동으로 백그라운드에서 실행되며 http://localhost:11434에서 제공됩니다. 도구 및 응용 프로그램은 추가 설정 없이 연결할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_8.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, PowerShell을 사용하여 Ollama의 API를 호출하는 방법은 다음과 같습니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"(\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Invoke\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"WebRequest\"\n        }), \" -method \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"POST\"\n        }), \" -\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Body\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'{\\\"model\\\":\\\"llama2\\\", \\\"prompt\\\":\\\"Why is the sky blue?\\\", \\\"stream\\\": false}'\"\n        }), \" -uri \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"http\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//localhost:11434/api/generate ).Content | ConvertFrom-json\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Windows에서 Ollama는 다른 플랫폼과 동일한 OpenAI 호환성을 지원하며, Ollama를 통해 로컬 모델을 사용하여 OpenAI용으로 작성된 기존 툴을 사용할 수 있도록 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Windows 미리보기에서 Ollama를 시작하려면:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Windows 용 Ollama 다운로드\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"OllamaSetup.exe를 더블 클릭하여 설치 프로그램을 실행하십시오.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"설치 후 좋아하는 터미널을 열고 ollama run llama2를 실행하여 모델을 실행하십시오.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"새 버전이 출시되면 Ollama가 업데이트를 알려줍니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ollama serve:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 명령은 Ollama 서버를 시작하여 다운로드한 모델을 API를 통해 접근할 수 있게 합니다. 이를 통해 웹 브라우저, 모바일 앱 또는 사용자 지정 스크립트와 같은 다양한 응용 프로그램에서 모델과 상호 작용할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"한가지 비유를 들어 볼게요. Ollama는 당신의 책(LMMs)을 보관하는 도서관 역할을 합니다. ollama serve를 실행하면 도서관을 열어 다른 사람들이 해당 도서관 시스템(API)을 통해 책에 접근하여 읽을(상호 작용할) 수 있게 됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-OllamaOllamainWindows_9.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ollama run phi:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 명령어는 특히 \\\"phi\\\" 모델을 다운로드하고 로컬 머신에서 실행하는 데 사용됩니다. \\\"phi\\\"란 Ollama 도서관에 존재하는 사전 학습된 LMM으로, GPT-3와 유사한 기능을 갖추고 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래에 있는 것이 유사성 확장입니다:\\n만약 ollama serve가 도서관을 열면, ollama run phi는(이하 phi)이라는 특정 책을 (Ollama)사서로부터 요청하고(다운로드된 모델 필요) 그것을 읽는(모델 실행) 것이 로컬 머신(당신의 컴퓨터) 내에서 일어나는 일 같습니다.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"ollama serve는 미리 다운로드된 모델이 요구됩니다. 특정 모델을 다운로드하려면 ollama pull \", _jsx(_components.code, {\n          children: \"model_name\"\n        }), \"을 사용하세요.\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ollama run phi는 “phi”모델을 특정하게 다운로드하고 실행합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ollama serve는 API를 통해 다운로드된 모델에 접근할 수 있게 해주는데 반해, ollama run phi는 로컬에서 특정 모델을 실행하는 데 중점을 둡니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"일반 명령어:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ollama list: 시스템에 다운로드된 모든 모델을 나열합니다.\"\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"ollama rm \", _jsx(_components.code, {\n          children: \"model_name\"\n        }), \": 시스템에서 다운로드된 모델을 제거합니다.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"ollama cp \", _jsx(_components.code, {\n          children: \"model_name1\"\n        }), \" \", _jsx(_components.code, {\n          children: \"model_name2\"\n        }), \": 다운로드된 모델을 새 이름으로 복사합니다.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"ollama info \", _jsx(_components.code, {\n          children: \"model_name\"\n        }), \": 다운로드된 모델에 대한 정보를 표시합니다.\"]\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ollama help: 사용 가능한 모든 명령어에 대한 도움말 문서를 제공합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델 관리:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"ollama pull model_name\"\n        }), \": Ollama 모델 허브에서 모델을 다운로드합니다.\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델 실행:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"ollama run model_name\"\n        }), \": 로컬에서 다운로드한 모델을 실행합니다.\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.code, {\n          children: \"ollama serve\"\n        }), \": Ollama 서버를 시작하여 API를 통해 다운로드한 모델에 액세스할 수 있게 합니다.\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"추가 명령:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ollama update: Ollama를 최신 버전으로 업데이트합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ollama config: Ollama 구성 설정을 관리합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-OllamaOllamainWindows_10.png\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Langchain을 통한 Ollama:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" langchain_community.llms \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" Ollama\\n\\nllm = Ollama(model=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"llama2\\\"\"\n        }), \")\\n\\nllm.invoke(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Tell me a joke\\\"\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"물론이죠! 여기 빠른 하나 있어요:\\\\n\\\\n과학자들이 원자를 믿지 않는 이유는 뭘까요?\\\\n왜냐하면 그들이 모든 것으로 이루어져 있으니까요!\\\\n\\\\n당신의 얼굴에 미소를 띄우길 바랍니다!\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"토큰을 스트리밍하려면 .stream(...) 메서드를 사용하세요:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [\"query = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Tell me a joke\\\"\"\n        }), \"\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" chunks \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" llm.stream(query):\\n    \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"print\"\n        }), \"(chunks)\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"확실해요, 여기 하나 있어요:\\n\\n왜 과학자들이 원자를 믿지 않을까요?\\n왜냐하면 그들이 모든 것을 이루기 때문이에요!\\n\\n저렇게 즐겁게 느껴지셨으면 좋겠어요! 더 듣고 싶나요?\\n\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Multi-modal\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama는 bakllava와 llava와 같은 멀티 모달 LLM을 지원합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ollama pull bakllava\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Ollama를 업데이트하여 멀티 모달을 지원하는 최신 버전을 사용해보세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" langchain_community.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"llms\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Ollama\"\n        }), \"\\n\\nbakllava = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Ollama\"\n        }), \"(model=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"bakllava\\\"\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" base64\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" io \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BytesIO\"\n        }), \"\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"IPython\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"display\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"HTML\"\n        }), \", display\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"PIL\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Image\"\n        }), \"\\n\\n\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"convert_to_base64\"\n        }), \"(pil_image):\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n    PIL 이미지를 Base64로 인코딩된 문자열로 변환\\n\\n    :param pil_image: PIL 이미지\\n    :return: 리사이즈된 Base64 문자열\\n    \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"\\n\\n    buffered = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"BytesIO\"\n        }), \"()\\n    pil_image.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"save\"\n        }), \"(buffered, format=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"JPEG\\\"\"\n        }), \")  # 필요시 형식 변경 가능\\n    img_str = base64.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"b64encode\"\n        }), \"(buffered.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"getvalue\"\n        }), \"()).\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"decode\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"utf-8\\\"\"\n        }), \")\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" img_str\\n\\n\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"plt_img_base64\"\n        }), \"(img_base64):\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n    Base64로 인코드된 문자열을 이미지로 표시\\n\\n    :param img_base64:  Base64 문자열\\n    \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"\\n    # \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Base64\"\n        }), \" 문자열을 소스로 사용하는 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"HTML\"\n        }), \" img 태그 생성\\n    image_html = f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'\u003cimg src=\\\"data:image/jpeg;base64,{img_base64}\\\" /\u003e'\"\n        }), \"\\n    # \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"HTML\"\n        }), \"을 렌더링하여 이미지 표시\\n    \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"HTML\"\n        }), \"(image_html))\\n\\n\\nfile_path = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"../../../static/img/ollama_example_img.jpg\\\"\"\n        }), \"\\npil_image = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Image\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"open\"\n        }), \"(file_path)\\nimage_b64 = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"convert_to_base64\"\n        }), \"(pil_image)\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"plt_img_base64\"\n        }), \"(image_b64)\\n\"]\n      })\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-OllamaOllamainWindows_11.png\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"llm_with_image_context = bakllava.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"bind\"\n        }), \"(images=[image_b64])\\nllm_with_image_context.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"invoke\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"달러 기반 총 유지율은 얼마입니까:\\\"\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'90%'\"\n        }), \"\\n\"]\n      })\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-OllamaOllamainWindows"},"buildId":"ll1cGyplNwh83dpggeai1","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>