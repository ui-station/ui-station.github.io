<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2" data-gatsby-head="true"/><meta name="twitter:title" content="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 19:28" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/wOkGEDZCvEs3S_XaNsdwr/_buildManifest.js" defer=""></script><script src="/_next/static/wOkGEDZCvEs3S_XaNsdwr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">39<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>"“Courage to Learn ML”의 새로운 장을 찾아주신 여러분, 환영합니다. 이 시리즈는 복잡한 주제들을 쉽고 재미있게 다루고, 멘토와 학습자 간의 캐주얼 대화처럼 친밀한 분위기를 제공하기 위해 만들어졌습니다. “용기로 방어하다”의 쓰기 스타일에서 영감을 받아 기계 학습에 특히 집중하고 있어요.</p>
<p>이번 시간에는 사라지는 그래디언트와 폭발하는 그래디언트의 어려움을 극복하는 방법에 대해 계속해서 탐구할 거예요. 첫 번째 세그먼트에서 우리는 네트워크 내에서 효율적인 학습을 보장하기 위해 안정적인 그래디언트 유지가 왜 중요한지에 대해 이야기했어요. 불안정한 그래디언트가 우리 네트워크의 심화를 방해할 수 있고 결국 깊은 "학습"의 잠재력을 제한할 수 있다는 것을 밝혀냈죠. 이러한 개념을 살려내기 위해 DNN(맛있고 영양가 있는 얹힌 작은 얼음 공장)이라는 소형 아이스크림 공장을 운영하는 비유를 사용하고 수렴한 팩토리 생산 라인을 조율하는 것과 유사한 DNN 훈련을 위한 강력한 전략을 명료하게 보여줬어요.</p>
<p>이제, 두 번째 이야기에서는 각 제안된 솔루션에 대해 더 심층적으로 탐구하며, 아이스크림 공장을 활기차게 만든 것과 같은 명확성과 창의성으로 그들을 살펴볼 거에요. 여기 이번 부분에서 다룰 주제 목록입니다:</p>
<ul>
<li>활성화 함수</li>
<li>가중치 초기화</li>
<li>배치 정규화</li>
<li>실제 적용(개인 경험)"</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>활성화 함수</h1>
<p>활성화 함수는 우리 "공장" 설정의 핵심입니다. 이 함수들은 우리의 DNN 조립 라인 내에서 전방 및 후방 전파를 통해 정보를 전달하는 역할을 합니다. 적절한 활성화 함수를 선택하는 것은 우리의 DNN 조립 라인 및 이에 따라 우리의 DNN 훈련 과정이 원활하게 작동하는 데 중요합니다. 이 부분은 활성화 함수의 장닿과 단점을 간단히 설명하는 것이 아닙니다. 여기서는 다양한 활성화 함수의 생성 배경을 파악하고 종종 간과되는 중요한 질문에 대답하기 위해 Q&#x26;A 형식을 사용할 것입니다.</p>
<p>이러한 함수들을 우리 아이스크림 생산 비유의 블렌더로 생각해보세요. 이용 가능한 블렌더 목록을 제공하는 대신, 각각의 혁신과 특정 개선 사항 뒤에 있는 이유를 심층적으로 검토하고 이해하는 데 도움을 드리겠습니다.</p>
<h2>활성화 함수란 무엇이며, 어떻게 적절한 함수를 선택할 수 있을까요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png" alt="Activation functions"></p>
<p>활성화 함수는 신경망 모델에 선형 및 비선형 관계를 모두 포착할 수 있는 유연성과 강력함을 부여하는 주요 요소입니다. 로지스틱 회귀와 DNN의 주요 차이점은 이러한 활성화 함수들과 여러 층을 결합하는 데 있습니다. 이들은 NN이 다양한 함수를 근사할 수 있게 합니다. 그러나 이러한 능력은 도전과제와 함께 제공됩니다. 활성화 함수 선택에는 더 주의를 기울여야 합니다. 잘못된 선택은 모델이 특히 역전파 중에 효과적으로 학습하는 것을 막을 수 있습니다.</p>
<p>당신이 당사 DNN 아이스크림 공장의 매니저로 상상해보세요. 당신은 생산 라인을 위해 적절한 활성화 함수(아이스크림 믹서로 생각해보세요)를 섬세하게 선택하고 싶을 것입니다. 즉, 당신의 요구 사항에 가장 적합한 것을 찾는 데 신중을 기울이고 최적의 선택지를 찾아내야 합니다.</p>
<p>따라서 효과적인 활성화 함수를 선택하는 첫 번째 단계는 두 가지 핵심 질문에 대한 대답을 찾는 것입니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>활성화 함수의 선택이 소멸 그래디언트나 폭발 그래디언트와 같은 문제에 어떤 영향을 미치나요? 어떤 기준이 좋은 활성화 함수를 정의하나요?</h2>
<p>은닉층에서 활성화 함수를 선택할 때, 주로 소멸 그래디언트와 관련된 문제가 발생합니다. 이는 전통적인 시그모이드 활성화 함수(가장 전통적이고 기본적인 모델)로 거슬러 올라갈 수 있습니다. 시그모이드 함수는 입력값을 확률 범위(0부터 1)에 매핑할 수 있는 능력으로 널리 사용되었습니다. 이는 이진 분류 작업에서 특히 유용합니다. 이 능력 덕분에 연구자들은 예측을 분류하기 위한 확률 임계값을 조정하여 모델의 유연성과 성능을 향상할 수 있었습니다.</p>
<p>그러나 이를 은닉층에 적용하는 것은 주로 소멸 그래디언트 문제를 야기했습니다. 이는 주로 두 가지 주요 요인으로 설명할 수 있습니다:</p>
<ul>
<li>순방향 전파 과정에서 시그모이드 함수는 입력을 0과 1 사이의 매우 좁은 범위로 압축합니다. 한 네트워크가 은닉층에서 활성화 함수로 시그모이드만 사용하는 경우, 여러 층을 거칠수록 이 범위가 더욱 좁아지게 됩니다. 이 압축 효과로 인해 출력의 변동성이 감소하고 양수 값으로의 편향이 도입됩니다. 입력 부호에 관계없이 출력은 0과 1 사이에 유지되기 때문에.</li>
<li>역전파 과정에서 시그모이드 함수의 도함수(종모양 곡선)는 0과 0.25 사이의 값을 생성합니다. 이 작은 범위는 입력을 가로지르는 그래디언트가 여러 층을 통과함에 따라 급속하게 감소할 수 있도록 할 수 있습니다. 이것은 앞선 층 그래디언트가 연속된층의 도함수의 곱으로 이루어지기 때문인데, 이러한 낮은 도함수의 복합 곱은 점점 더 작은 그래디언트를 결과로 가져와서 초기 층에서의 효과적인 학습을 방해합니다.</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이러한 제약 사항을 극복하기 위해 이상적인 활성화 함수는 다음과 같은 특성을 보여야 합니다:</p>
<ul>
<li>비선형성. 네트워크가 복잡한 패턴을 포착할 수 있도록 함.</li>
<li>비포화. 함수와 그 도함수가 입력 범위를 과도하게 압축하지 않아서 gradient 소멸을 방지해야 함.</li>
<li>중심이 0인 출력. 함수는 양수 및 음수 출력 둘 다를 허용해야 하며, 각 노드 사이의 평균 출력이 특정 방향으로의 편향을 도입하지 않도록 해야 함.</li>
<li>계산 효율성. 함수와 그 도함수가 계산적으로 간단하여 효율적인 학습을 용이하게 해야 함.</li>
</ul>
<h2>이러한 기본 특성들을 고려할 때, 인기있는 활성화 함수들이 기본 모델인 Sigmoid를 어떻게 개선하고 뛰어나게 만드는지 알아봅시다.</h2>
<p>이 섹션은 거의 모든 현재 활성화 함수에 대한 일반적인 개요를 제공하려고 합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>테이블 태그를 마크다운 형식으로 변경하세요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>ReLU의 고려 사항 중 하나는 선형 세그먼트 간의 급격한 전환으로 인해 x=0에서 미분 불가능하다는 것입니다. 실제로 PyTorch와 같은 프레임워크는 subgradient 개념을 사용하여 이를 해결하며, 종종 x=0에서 도함수를 0.5 또는 [0, 1] 내의 다른 값으로 설정합니다. 이는 보통 정확한 제로 입력이 드물고 데이터의 변동성 때문에 문제가 되지 않습니다.</p>
<p>그래서, ReLU가 여러분에게 적합한 선택일까요? 많은 연구자들은 그렇다고 말합니다. 이는 그 간결함, 효율성 및 주요 DNN 프레임워크의 지원 덕분입니다. 게다가 <a href="https://arxiv.org/abs/2310.04564" rel="nofollow" target="_blank">https://arxiv.org/abs/2310.04564</a> 같은 최근 연구들이 ReLU의 계속된 중요성을 강조하며, ML 분야에서의 부활과 같은 시대를 맞이한다고 강조하고 있습니다.</p>
<p>Leaky ReLUs는 클래식적인 ReLU에 약간의 변화를 준겳이며, ReLU를 더 자세히 살펴보면 몇 가지 문제점이 드러납니다. 음수 입력에 대한 제로 출력으로 이어지는 것은 'dying ReLU' 문제로 이어지며, 뉴런들이 훈련 중 업데이트되지 않게 됩니다. 또한, ReLU는 양수 값을 선호하여 모델에 방향성 편향을 도입할 수 있습니다. 이러한 단점을 극복하면서 ReLU의 이점을 유지하기 위해, 여러 연구자들이 'leaky' ReLU와 같은 여러 변형을 개발했습니다.</p>
<p>Leaky ReLU는 ReLU의 음수 부분을 수정하여 작고 0이 아닌 기울기를 부여합니다. 이 조정은 음수 입력이 작은 음수 출력을 생성하도록하며, 효과적으로 그 외의 0 출력 영역을 '누출'시킵니다. 이 누출의 기울기는 하이퍼파라미터 알파(α)에 의해 제어되며, 전형적으로 뉴런을 활성 유지와 희소성 사이의 균형을 유지하기 위해 0에 가깝게 설정됩니다. 작은 음수 출력을 허용함으로써, Leaky ReLU는 활성 함수의 출력을 0 주변으로 중앙 집중시키고 뉴런이 비활성화되지 않게 하여 'dying ReLU' 문제에 대응합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그러나 하이퍼파라미터로 α를 도입하면 모델 튜닝에 대한 복잡성이 추가됩니다. 이를 관리하기 위해 원본 Leaky ReLU의 변형이 개발되었습니다:</p>
<ul>
<li>Randomized Leaky ReLU (RReLU): 이 버전은 훈련 중에 α를 지정된 범위 내에서 무작위로 지정하고 평가 중에는 고정합니다. 무작위성은 모델을 정규화하고 과적합을 방지하는 데 도움이 될 수 있습니다.</li>
<li>Parametric Leaky ReLU (PReLU): PReLU는 훈련 중에 α를 학습할 수 있도록 하며, 활성화 함수를 데이터셋의 특정 요구에 맞게 조정할 수 있습니다. 이는 α를 훈련 데이터에 맞게 조정하여 모델 성능을 향상시킬 수 있지만, 과적합의 위험도 증가시킵니다.</li>
</ul>
<p>Leaky ReLU를 개선한 Exponential Linear Unit (ELU). Leaky ReLU와 ELU는 음의 값을 허용하여 평균 유닛 활성화를 제로에 가깝게 밀어내고 활성화 함수의 활력을 유지하는 데 도움이 됩니다. Leaky ReLU의 문제점은 이 음의 값의 범위를 조절할 수 없다는 것입니다. 이론적으로 이 값들은 작게 유지하려는 의도에도 불구하고 음의 무한대로 확장될 수 있습니다. ELU는 이를 해결하기 위해 비선형 지수 곡선을 비정상적인 입력에 통합하여 음의 출력 범위를 최대 -𝛼(일반적으로 1로 설정되는 새로운 하이퍼파라미터)로 좁히고 제어합니다. 또한 ELU는 매끄러운 함수입니다. 그 지수 요소 덕분에 음과 양 값 사이에서 매끄러운 전환을 가능하게 하며, 입력 값에 대한 잘 정의된 기울기를 보장하여 기울기 기반 최적화에 유리합니다. 이 기능은 ReLU와 Leaky ReLU에서 보이는 미분 불가능 문제를 해결합니다.</p>
<p>Self-Normalizing 속성을 갖춘 향상된 ELU인 Scaled Exponential Linear Unit (SELU). SELU는 신경망 내에서 제로 평균 및 단위 분산을 유지하도록 설계된 ELU의 확장된 버전입니다. 양의 순입력의 기울기가 1을 초과하도록 고정 스케일 요인 λ(1보다 큰 값)을 통합함으로써 SELU는 하위 레이어의 기울기가 줄어드는 상황에서 기울기를 증폭하여 딥 뉴럴 네트워크에서 자주 발생하는 소멸하는 기울기 문제를 예방하는 데 특히 유용합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>SELU에 대해, 매개변수(α 및 λ)는 고정된 값이며 학습할 수 없으므로 조정해야 할 매개변수가 적어 튜닝 과정이 간소화됩니다. SELU 구현에서 이러한 특정 값들을 찾을 수 있습니다.</p>
<p>SELU는 실제로 활성화 함수 세계에서 정교한 "믹서"인데요, 특정 요구 사항이 딸려옵니다. 단방향 또는 순차 네트워크에서 가장 효과적이며 RNN, LSTM 또는 건너뛰기 연결을 갖는 아키텍처에서는 그 설계 때문에 그런만큼 성능이 좋지 않을 수 있습니다.</p>
<p>SELU의 자기 정규화 기능을 위해서는 입력 피처가 표준화되어야 합니다. 평균이 0이고 표준 편차가 1인 것이 중요합니다. 또한, 매 숨겨진 레이어의 가중치는 LeCun 정규 초기화를 사용하여 초기화되어야 합니다. 여기서 가중치는 평균이 0이고 분산이 1/fan_in인 정규 분포에서 샘플링됩니다. "fan_in"이란 용어가 익숙하지 않다면, 가중치 초기화에 대한 전용 세션에서 설명하겠습니다.</p>
<p>요약하면 SELU의 자기 정규화가 효과적으로 기능하려면 입력 피처가 정규화되고 네트워크 구조가 끊기지 않는 것을 보장해야 합니다. 이 일관성은 네트워크 전체에서 자기 정규화 효과가 유지되도록 도와주며 누출 없이 계속 유지되도록 합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>GELU (Gaussian Error Linear Unit)은 Dropout으로부터 규제 아이디어를 통합한 혁신적인 활성화 함수입니다. 기존 ReLU가 음수 입력에 대해 0을 출력하는 반면, leaky ReLU, ELU 및 SELU는 음수 출력을 허용합니다. 이를 통해 활성화의 평균을 0에 가깝게 이동시켜 편향을 줄이는데 도움을 줍니다. 이는 ReLU와 비슷한 방식으로 편향을 줄이지만 음수 입력을 완전히 0으로 만들지 않고 음의 값을 허용한다는 것을 의미합니다. 그러나 이러한 누출은 "죽어 가는 ReLU"의 일부 이점을 잃어버릴 수 있음을 의미합니다. 여기서는 일부 뉴런의 비활성으로 더 sparse하고 일반화된 모델을 얻을 수 있습니다.</p>
<p>죽어 가는 ReLU 및 Dropout의 희소성 이점을 고려할 때, GELU는 여기에 한 발 더 나아간 것입니다. GELU는 0 출력의 특성을 가진 죽어 가는 ReLU를 무작위적인 요소와 결합하여 뉴런이 재활성화될 수 있는 가능성을 열어줍니다. 이 접근은 유익한 희소성을 유지하는 것뿐만 아니라 뉴런 활동을 재도입하여 GELU를 견고한 해결책으로 만듭니다. 이 메커니즘을 완전히 이해하기 위해 GELU의 정의를 자세히 살펴보겠습니다:</p>
<p><img src="/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_1.png" alt="이미지"></p>
<p>GELU 활성화 함수에서 CDF인 Φ(x) 또는 표준 가우스 누적 분포 함수가 중요한 역할을 합니다. 이 함수는 표준 정규 분포를 따를 때 x보다 작거나 같은 값을 갖는 것으로 나타내는 확률을 나타냅니다. Φ(x)는 음수 입력에 대해 0부터 양수 입력에 대해 1로 매끄럽게 전환되어, 입력의 스케일링을 효과적으로 제어합니다. Dan Hendrycks 외(출처)의 논문에 따르면 뉴런 입력은 배치 정규화를 사용할 때 특히 정규 분포를 따르는 경향이 있어 정규 분포의 사용이 정당화됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>해당 함수의 디자인은 x 값이 줄어들수록 입력이 더 자주 "떨어지도록" 허용하여 변환을 확률적이면서 입력 값에 의존적으로 만듭니다. 이 메커니즘은 흔히 사용되는 직선 함수인 f(x) = x를 더 부드럽게 만들어 ReLU 함수와 유사한 형태를 유지하며, 조각별 선형 함수에서 발생하는 갑작스러운 변화를 피합니다. GELU의 가장 중요한 특징 중 하나는 뉴런을 완전히 비활성화할 수 있다는 것으로, 이를 통해 입력 값의 변화에 따라 다시 활성화될 수 있습니다. 이러한 확률적 성질은 입력 값에 의존하지만 완전히 무작위적이지 않아 뉴런이 다시 활성화될 기회를 제공합니다.</p>
<p>아래는 GELU가 ReLU보다 두드러지는 이점이라고 요약할 수 있습니다. GELU는 어떤 입력 값이 양수인지 음수인지에 관계없이 전체 입력 값 범위를 고려합니다. Φ(x) 값이 감소함에 따라 GELU 함수의 출력이 0에 가까워지는 확률이 증가하여 뉴런을 부드럽게 "떨어뜨리게" 됩니다. 이 방법은 전형적인 드롭아웃 방식보다 더 정교하며, 무작위적으로 하는 것이 아니라 데이터에 따라 뉴런의 비활성화를 결정하도록 되어 있습니다. 이 방식은 매우 매력적으로 느껴지며, 마치 고급 디저트에 부드러운 크림을 추가하여 조금 더 향상된 미각을 경험하는 것과 같다고 생각합니다.</p>
<p>GELU는 GPT-3, BERT 및 다른 Transformers와 같은 모델에서 효율적이며 언어 처리 작업에서 강력한 성능을 보여 인기 있는 활성화 함수가 되었습니다. 확률적 성질 때문에 계산 위주이지만, 표준 가우스 누적 분포인 Φ(x)의 곡선은 시그모이드와 tanh 함수와 유사합니다. 흥미로운 점은 GELU가 tanh를 사용하거나 x(1.702*x) 공식을 사용하여 근사할 수 있다는 것입니다. 이러한 단순화 가능성에도 불구하고, PyTorch의 GELU 구현은 그러한 근사가 종종 불필요할 정도로 빠르게 진행됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>더 깊이 들어가기 전에 먼저 정리해보자면,</p>
<h2>ReLU를 검토하고 이로부터 영감받은 다른 활성화 함수가 무엇인지를 살펴보면 어떤 활성화 함수가 좋을지 정확히 알아볼까요?</h2>
<p>Günter Klambauer 등의 논문에서 SELU가 소개된 적이 있습니다. 여기서는 효과적인 활성화 함수의 중요한 특성을 강조했는데요.</p>
<ul>
<li>범위: 네트워크 전체의 평균 활성화 수준을 조절하는 데 도움이 되기 위해 음수와 양수 값을 출력해야 합니다.</li>
<li>포화 영역: 도함수가 제로에 가까워지는 영역으로, 하위층의 너무 높은 분산을 안정화하는 데 도움을 줍니다.</li>
<li>증폭 슬로프: 하위 층에서 너무 낮은 분산을 높이기 위해 중요한 기울기가 있어야 합니다.</li>
<li>연속성: 연속적인 곡선은 분산의 변화를 안정화하고 증가시키는 효과를 균형있게 유지하는 고정점을 보장합니다.</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>또한 "이상적인" 활성화 함수에 대한 두 가지 추가 기준을 제안하고 싶습니다:</p>
<ul>
<li>비선형성: 이것은 명백하고 필수적입니다. 왜냐하면 선형 함수는 복잡한 패턴을 효과적으로 모델링할 수 없기 때문입니다.</li>
<li>동적 출력: 출력이 제로이고 입력 데이터에 따라 출력을 변경할 수 있는 능력은 동적 뉴런 활성화와 비활성화를 가능하게 합니다. 이렇게 하면 네트워크가 변화하는 데이터 조건에 효율적으로 적응할 수 있습니다.</li>
</ul>
<h2>활성화 함수가 음수를 출력하는 이유에 대해 더 직관적인 설명을 부탁드려도 될까요?</h2>
<p>활성화 함수를 입력 데이터를 변환하는 블렌더로 생각해 보세요. 일부 재료를 선호하는 블렌더처럼, 활성화 함수는 그들의 본질적인 특성에 따라 편향을 도입할 수 있습니다. 예를 들어, 시그모이드 및 ReLU 함수는 일반적으로 입력과 관계없이 비음수 출력만 나타냅니다. 이는 블렌더가 어떤 재료를 넣어도 항상 동일한 맛을 내는 것과 유사합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_3.png" alt="Image"></p>
<p>이 편향을 최소화하려면 부정적이고 긍정적인 값을 출력할 수 있는 활성화 함수를 가지는 것이 좋습니다. 기본적으로 우리는 중심이 제로인 출력을 목표로 합니다. 활성화 함수의 출력을 나타내는 놀이터를 상상해보세요. Sigmoid나 ReLU와 같은 함수로는, 이 놀이터는 부정적인 입력을 무시하거나 제로로 바꾸기 때문에 긍정적인 쪽으로 크게 기울어져 있습니다. Leaky ReLU는 음수 입력이 약간 음수 출력을 생성하도록 허용함으로써 이 놀이터를 균형잡게 시도하지만, 부정적 기울기의 선형 및 상수적 성격 때문에 조정이 미미합니다. 반면에 Exponential Linear Unit (ELU)은 지수 구성 요소로 음수 측면에 더 다이나믹한 밀어넣기를 제공하여, 더 균형 잡힌 상태에 가까워질 수 있도록 돕습니다. 이 균형은 긍정적 및 부정적 업데이트가 훈련에 기여하도록 보장함으로써 신경망에서 건강한 그레이디언트 플로우와 효율적인 학습을 유지하는 데 중요합니다, 단방향 업데이트의 제한을 피하기 위해.</p>
<h2>ReLU와 유사하게 양수 입력을 제로화하는 활성화 함수를 생성할 수 있을까요, min(0, x)를 사용하여 양수 입력을 제로화하는 함수를 선호하는 이유는 무엇인가요?</h2>
<p>확실히, ReLU의 양수 값을 제로화하고 음수 값을 그대로 통과시키는 버전을 설계할 수 있습니다. 이것은 기술적으로 실행 가능한데, 중요한 점은 여기서 값의 부호가 아니라 네트워크에 비선형성을 도입하는 것입니다. 이 활성화 함수들이 일반적으로 출력 레이어가 아닌 숨겨진 레이어에서 사용된다는 것을 기억하는 것이 중요합니다. 즉, 이 네트워크 내의 이러한 활성화 함수의 존재는 최종 출력의 부호에 영향을 미치지 않고 이 레이어의 특성에 의해 직접적으로 영향을 받지 않더라도 최종 출력이 여전히 양수와 음수 모두가 될 수 있다는 것을 의미합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>무슨 상황에서든, 네트워크의 가중치와 편향은 출력의 필요한 부호에 맞게 조정될 수 있습니다. 예를 들어, 전통적인 ReLU에서 출력이 1이고 다음 레이어의 가중치가 1이면 출력은 여전히 1로 유지됩니다. 마찬가지로, 제안된 ReLU 변형이 -1을 출력하고 가중치가 -1이면 결과는 여전히 1이 됩니다. 본질적으로, 우리는 출력의 부호보다는 크기에 더 신경을 씁니다.</p>
<p>따라서, ReLU가 부정적인 쪽에서 포화되는 것은 양수 쪽에서 포화되는 것과 근본적으로 다르지 않습니다. 그러나 우리가 영 중심 활성화 함수를 중요시하는 이유는 양수 또는 음수 값에 대한 내재적인 선호도를 방지하여 모델에서 불필요한 편향을 피하기 위한 것입니다. 이 균형은 네트워크 전체에 걸쳐 중립성과 효과적인 학습을 유지하는 데 도움이 됩니다.</p>
<h2>Leaky ReLU와 같은 함수들은 출력을 영 중심 주변에 유지하기 위해 음수값을 출력할 필요가 있습니다. 그렇다면 ELU, SELU, GELU는 왜 음수 입력에 포화되도록 특별히 설계되었을까요?</h2>
<p>이를 이해하기 위해, ReLU 뒤에 있는 생물학적 영감을 살펴볼 수 있습니다. ReLU는 생물학적 뉴런을 모방하는데, 이들은 한계값을 가지고 있습니다. 이 한계값을 초과하는 입력은 뉴런을 활성화시키고, 그 이하는 그렇지 않습니다. 활성 및 비활성 상태 간 전환 가능성은 신경 기능에서 중요합니다. ELU, SELU, GELU와 같은 변형을 고려할 때, 이들의 설계가 두 가지 다른 필요에 부합함을 알 수 있습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>긍정적 영역: 임계값을 초과하는 신호가 전달될 때 전달되는 원하는 신호를 전송하는 것을 의미합니다.</li>
<li>부정적 영역: 원치 않는 신호를 최소화하거나 걸러내며 대형 부정적 값의 영향을 완화하여 누수하는 게이트처럼 작동합니다.</li>
</ul>
<p>이러한 기능은 입력에 대한 게이트 역할을 하며, 뉴런의 출력에 영향을 미쳐야 하는 사항과 그렇지 말아야 하는 사항을 관리합니다. 예를 들어, SELU는 다음 두 가지 측면을 구분하여 활용합니다:</p>
<ul>
<li>긍정적 영역: 스케일링 인자 λ (1보다 큼)는 신호를 전달하지 않을 뿐만 아니라 약간 증폭시킵니다. 역전파 중 이 영역의 도함수는 일정하게 유지됩니다 (약 1.0507), 작지만 유용한 기울기를 증가하여 사그라들기 기울기를 희석시키기 위해 사용됩니다.</li>
<li>부정적 영역: 도함수는 0과 λα 사이의 값 사이를 이동합니다 (일반적인 값은 λ ≈ 1.0507 그리고 α ≈ 1.6733), 약 1.7583에 달하는 최대 도함수를 이끌어 냅니다. 여기서 함수는 거의 0에 가깝게 접근하며, 지나치게 큰 기울기를 줄여 폭발 문제를 해결하기 위해 돕습니다.</li>
</ul>
<p>이 설계는 이러한 활성화 함수들이 유용한 신호를 증가시키면서 잠재적으로 유해한 극단을 억제해 안정적인 학습 환경을 제공할 수 있도록 균형을 맞춘다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>활성화 함수가 게이트로 작용하는 개념은 새로운 아이디어가 아닙니다. 시그모이드 함수가 무엇을 기억하거나 업데이트하거나 잊어버릴지를 결정하는 LSTM과 같은 구조에서 강력한 전례가 있습니다. 이 게이팅 개념은 ReLU의 변형이 특정한 방식으로 설계된 이유를 이해하는 데 도움이 됩니다. 예를 들어 GELU는 표준 정규 분포의 누적 분포 함수(CDF)에서 유도된 스케일 계수를 사용하는 동적 게이트 역할을 합니다. 이 스케일링을 통해 입력의 작은 부분이 0에 가까울 때 통과되도록 하고, 더 큰 양수 값은 대부분 변경되지 않고 통과할 수 있게 합니다. 입력이 다음 레이어에 얼마나 많은 영향을 미치는지 제어함으로써, GELU는 정보 흐름의 효과적인 관리를 용이하게 해주며, 특히 transformer와 같은 구조에서 유용합니다.</p>
<p>언급된 ELU, SELU, 그리고 GELU 모두 음수 측면을 부드럽게 만듭니다. 음수 입력의 부드러운 포화는 큰 음수 값의 영향을 완화하는 것뿐만 아니라, 네트워크가 입력 데이터의 변동에 덜 민감해지도록 만듭니다. 이를 통해 더 안정적인 특징 표현이 이뤄지게 됩니다.</p>
<p>요약하면, 양수인지 음수인지에 상관없이 포화 영역이 구체적으로 중요하지 않습니다. 왜냐하면 이러한 활성화 함수들은 네트워크의 중간 레이어에서 작동하며, 여기서 가중치와 편향이 적절하게 조정될 수 있습니다. 하지만, 한쪽이 신호를 변경하지 않고 전달하거나 심지어 증폭할 수 있도록 허용하는 이러한 함수의 설계가 중요합니다. 이러한 배치는 신호를 조직화하고 효과적인 역전파를 용이하게 도와 전체 네트워크의 성능과 학습 안정성을 향상시킵니다.</p>
<h2>언제 각 활성화 함수를 선택해야 할까요? 왜 ReLU가 여전히 실무에서 가장 인기 있는 활성화 함수인가요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>올바른 활성화 함수를 선택하는 데는 계산 리소스, 네트워크 아키텍처의 특정 요구 사항 및 이전 모델로부터의 경험적 증거 등 여러 요소가 관련됩니다.</p>
<ul>
<li>계산 리소스: 충분한 계산 리소스가 있다면 교차 검증을 사용하여 다양한 활성화 함수를 실험해 보는 것이 유익할 수 있습니다. 이를 통해 모델과 데이터셋에 특화된 활성화 함수를 만들 수 있습니다. SELU를 사용할 때 배치 정규화가 필요 없는 경우가 대부분이며, 이는 다른 함수들과 달리 배치 정규화가 필요하지 않아 아키텍처를 간단하게 만들어 줍니다.</li>
<li>경험적 증거: 특정 응용 프로그램에는 특정 함수가 표준으로 사용될 수 있습니다. 예를 들어, 트랜스포머 모델을 훈련시키기 위해 GELU를 선호하는 경우가 많은데, 이는 해당 아키텍처에서 효과적이기 때문입니다. SELU는 자기 정규화 특성과 조절해야 할 하이퍼파라미터가 없다는 장점으로, 훈련 안정성이 핵심인 깊은 네트워크에 특히 유용합니다.</li>
<li>계산 효율성과 간결성: 계산 효율성과 간결성이 중요한 경우, ReLU 및 PReLU, ELU와 같은 변형들이 우수한 선택지입니다. 이들은 매개변수 조정의 필요성을 피하고 모델의 희소성 및 일반화를 지원하여 과적합을 줄이는 데 도움을 줍니다.</li>
</ul>
<p>더 정교한 함수가 등장했지만, ReLU는 여전히 간결하고 효율적이어서 매우 인기가 있습니다. 구현이 간단하고 이해하기 쉬우며 계산을 복잡하게 하지 않고 비선형성을 소개하는 명확한 방법을 제공합니다. 음수 부분을 제로 처리하는 함수의 능력으로 계산을 단순화하고 계산 속도를 향상시키므로, 특히 대규모 네트워크에서 매우 유리합니다.</p>
<p>ReLU의 설계는 음수 활성화를 제로처리하여 모델의 희소성을 기본적으로 증가시키며, 이는 일반화를 개선할 수 있습니다 — 훈련 중심의 과적합이 심각한 문제인 딥 뉴럴 네트워크에서 매우 중요한 요소입니다. 게다가 ReLU는 추가적인 하이퍼파라미터가 필요 없으며, PReLU나 ELU와 같은 함수와 달리 모델 훈련에 추가 복잡성을 도입하지 않습니다. 또한 ReLU가 널리 채택된 상태이므로, 많은 머신러닝 프레임워크와 라이브러리가 이를 위해 특화된 최적화를 제공하여, 많은 개발자에게 실용적인 선택이 됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>요약하자면, 새로운 활성화 함수는 특정 시나리오에 특정 이점을 제공하지만, ReLU의 간단함, 효율성, 효과적인 측면의 균형은 많은 응용 프로그램에서 선호하는 선택지가 되고 있습니다. 어떤 활성화 함수를 선택한다 하더라도, 그 특성을 철저히 이해하는 것이 중요하며 모델의 요구 사항과 일치하고 모델 훈련 중 문제 해결을 용이하게 하는 데 필수적입니다.</p>
<h1>가중치 초기화</h1>
<p>그래, 우리는 기욁할 기울기를 안정화시킬 완벽한 활성화 함수를 찾으려는 것을 그만두고, 가중치를 효율적으로 초기화하여 우리의 신경망을 올바르게 설정하는 다른 중요한 측면에 초점을 맞출 시간입니다.</p>
<p>가중치 초기화에 대한 가장 인기 있는 방법들에 대해 자세히 살펴보기 전에, 기본적인 질문을 하나 다루어 보겠습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>가중치 초기화는 왜 중요하며 불안정한 그래디언트를 완화하는 데 어떻게 도움이 될까요?</h2>
<p>적절한 가중치 초기화는 모델 전체를 따라 정확하게 그래디언트가 흐를 수 있도록 보장합니다. 이는 아이스크림 공장에서 반제품이 전달되는 방식과 유사합니다. 초기 기계 설정이 올바른 것만 중요한 것이 아니라 각 부서가 효율적으로 작동하는 것도 중요합니다.</p>
<p>가중치 초기화는 네트워크를 통해 전진 및 역방향으로 정보가 안정적으로 흐를 수 있도록 목표를 합니다. 너무 크거나 너무 작은 가중치는 문제를 일으킬 수 있습니다. 지나치게 큰 가중치는 전진 패스 중 출력을 지나치게 증가시켜 예측을 과대추정하게 할 수 있습니다. 반면 아주 작은 가중치는 출력을 지나치게 줄일 수 있습니다. 이러한 가중치의 크기는 역전파 중에 중요해집니다. 가중치가 너무 크면 그래디언트가 폭발할 수 있고, 너무 작으면 그래디언트가 사라질 수 있습니다. 이를 이해하여 우리는 출력 및 그래디언트를 무효화하는 영옵션 (zero)과 지나치게 높은 값과 같은 극단적인 초기화를 피합니다. 이 균형 잡힌 접근법은 네트워크의 효과성을 유지하고 불안정한 그래디언트와 관련된 문제를 방지하는 데 도움이 됩니다.</p>
<h2>가중치를 초기화하는 좋은 방법은 무엇인가요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>가장 중요한 것은 최적 가중치 초기화는 이미 학습된 가중치를 사용하는 것이 가장 좋습니다. 이미 일부 학습을 거친 가중치를 얻을 수 있다면 손실을 최소화하는 방향으로 진행 중인 이 가중치를 계속 사용하는 것이 이상적입니다.</p>
<p>그러나 처음부터 시작하는 경우 가중치를 초기화하는 방법을 신중하게 고려해야 합니다, 특히 불안정한 기울기를 방지하기 위해. 좋은 가중치 초기화에는 다음을 목표로 하는 것이 중요합니다:</p>
<ul>
<li>극단적인 값은 피해야 합니다. 이전에 논의했던 대로, 가중치는 너무 크거나 작지 않고 0도 아니어야 합니다. 적절히 조절된 가중치는 네트워크 훈련의 전진 및 역진행 중 안정성을 유지하는 데 도움이 됩니다.</li>
<li>대칭을 깨야 합니다. 가중치가 다양한 행동을 하도록 하는 것은 매우 중요합니다. 이렇게 하면 뉴런이 서로 거울에 비친 행동을 하지 않고 동일한 특성만 학습하게 되는 것을 방지합니다. 이러한 차별이 없으면 네트워크가 복잡한 패턴을 모델링하는 능력이 심각하게 제한될 수 있습니다. 각각의 다른 초기 가중치가 각 뉴런이 데이터의 다른 측면을 학습하기 시작하도록 도와줍니다. 이는 아이스크림 공장의 다양한 종류의 생산 라인을 가지고 다양한 맛을 생산할 수 있는 범위를 확대하는 것과 비슷합니다.</li>
<li>손실 표면에서 유리한 위치에 가중치를 배치해야 합니다. 초기 가중치는 모델이 글로벌 최솟값으로 향하는 여정을 더 쉽게 만들기 위해 손실 표면에서 양호한 시작 위치에 모델을 위치시켜야 합니다. 손실 랜드스케이프가 어떻게 보이는지 명확한 그림을 가지고 있지 않기 때문에 가중치 초기화에 약간의 무작위성을 도입하는 것이 유익할 수 있습니다.</li>
</ul>
<p>모든 가중치를 0으로 설정하는 것이 문제가 되는 이유입니다. 이는 모든 뉴런이 동일하게 행동하고 동일한 속도로 학습하기 때문에 대칭 문제를 발생시킵니다. 다양한 패턴을 효과적으로 포착하지 못하게되는 네트워크의 능력을 제한합니다. ReLU 및 그 변형과 함께 0 가중치는 출력이 0이 되어 학습이 멈추고 모든 뉴런이 비활성화되는 결과를 초래합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>모든 가중치를 작은 무작위 숫자로 초기화해야 하지 않을까요?</h2>
<p>가중치를 초기화할 때 작은 무작위 숫자를 사용하는 것은 도움이 될 수 있지만, 종종 충분한 제어가 없을 수 있습니다. 무작위로 할당된 가중치는 너무 작을 수 있어서 기욹기 소멸 문제로 이어질 수 있습니다. 이는 훈련 중 업데이트가 무의미하게 작아져 학습 과정이 정체될 수 있습니다. 또한, 완전히 무작위 초기화는 대칭을 깨는 것을 보장하지 않습니다. 예를 들어, 초기화된 값이 너무 유사하거나 모두 같은 부호를 가지는 경우, 뉴런들도 여전히 너무 유사하게 작동하여 데이터의 다양한 측면을 배우지 못할 수 있습니다.</p>
<p>실무에서는 초기화에 대해 더 구조화된 방법을 사용합니다. 유명한 방법에는 Glorot (또는 Xavier) 초기화, He (또는 Kaiming) 초기화, LeCun 초기화 등이 있습니다. 이러한 기술은 일반적으로 정규 분포나 균일 분포를 기반으로 하지만, 이전 및 다음 레이어의 크기를 고려하여 균형을 제공하는 것으로 조절됩니다. 이는 기울기 소실 또는 폭발의 위험이 없이 효과적인 학습을 촉진합니다.</p>
<h2>그렇다면, 가중치 초기화에 표준 정규 분포(N(0,1))를 사용하지 않는 이유는 무엇인가요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>일반적인 정규 분포(N(0,1))를 사용하면 무작위화 과정을 어느 정도 제어할 수 있지만, 분산을 효과적으로 제어할 수 없어 최적 가중치 초기화에는 부족합니다. 제로 평균은 가중치가 모두 동일한 부호를 공유하지 않도록 보장하여 대칭을 깨는 데 효과적입니다. 그러나 분산이 1인 것은 문제가 될 수 있습니다.</p>
<p>활성화 함수 입력 𝑍이 가중치에 의존하는 시나리오를 고려해 봅시다. 이전 레이어의 𝑁개 뉴런의 출력을 합산하여 계산된다고 가정해보면, 각각의 가중치는 표준 정규 분포에서 초기화됩니다. 여기서 𝑍도 평균이 0인 정규 분포를 따르지만, 분산은 𝑁이 됩니다. 예를 들어 𝑁=100인 경우, 𝑍의 분산은 100이 되어 너무 크기 때문에 활성화 함수로 입력이 불안정하게 전달되어 역전파 과정에서 그래디언트가 불안정해질 수 있습니다. 아이스크림 공장을 비유하면, 각 기계의 설정에서 오차 허용을 높게 설정하는 것은 품질 관리 부재로 인해 원하는 결과와 크게 벗어나는 최종 제품을 만드는 것과 같습니다.</p>
<p>그렇다면 왜 𝑍의 분산에 신경을 쓸까요? 분산은 𝑍 값의 퍼짐을 제어합니다. 분산이 너무 작으면 𝑍의 출력이 충분히 다양하지 않아 대칭을 깨는 데 효과적이지 못할 수 있습니다. 그러나 너무 큰 분산은 값이 너무 높거나 낮아질 수 있습니다. 시그모이드와 같은 활성화 함수의 경우, 극단적으로 높거나 낮은 입력값은 함수의 포화 극으로 출력을 밀어 넣어 그래디언트 소실 문제를 야기할 수 있습니다.</p>
<p>따라서, 분포에서 무작위로 가중치를 초기화할 때 평균과 분산 둘 다 중요합니다. 효과적으로 대칭을 깨기 위해 평균을 0으로 설정하고, 동시에 분산을 최소화하여 중간 제품(즉, 뉴런 출력)이 너무 크거나 작지 않도록 해야 합니다. 올바른 초기화는 네트워크를 통과하는 정보의 안정된 흐름을 보장하고, 전방 및 역방향으로 효율적인 학습 과정을 유지하며, 그래디언트에 불안전성을 도입하지 않습니다. 신중한 초기화 접근은 효과적이고 견고하게 학습하는 네트워크로 이어질 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>그래서, 신경망의 중간 층에서 출력 값을 제어하기 위해 연속된 층에도 입력으로 사용되는 가중치 초기화에 대해 신중히 선택한 평균과 분산을 사용합니다. 그렇다면, 가장 인기 있는 방법들이 어떻게 이 분산을 제어할 수 있는 걸까요?</h2>
<p>가중치를 초기화하는 가장 흔한 방법들을 살펴보기 전에, 𝑍Z의 분산은 가중치 초기화의 분산뿐만 아니라 𝑍Z를 계산하는 데 참여하는 뉴런의 수도 영향을 받는다는 점이 중요합니다. 만약 16개의 뉴런만 사용된다면, 𝑍Z의 분산은 16이 되고, 100개의 뉴런이 사용된다면 100이 됩니다. 이 변동은 가중치가 뽑히는 분포만이 아니라 계산에 기여하는 뉴런의 수, 즉 "팬-인"이라고도 불리는 요소에 의해 영향을 받습니다. "팬-인"은 뉴런으로 들어오는 입력 연결의 수를 의미하며, 비슷하게 "팬-아웃"은 뉴런이 가지는 출력 연결의 수를 나타냅니다.</p>
<p>예시를 통해 설명해드리겠습니다: 신경망의 중간 층에 200개의 뉴런이 있고, 이전 층의 100개 뉴런 및 다음 층의 300개 뉴런과 연결되어 있다고 가정해봅시다. 이 경우, 이 층의 팬-인은 100이고, 팬-아웃은 300입니다.</p>
<p>팬-인과 팬-아웃을 이용하면 가중치 초기화 중 분산을 제어할 수 있는 메커니즘을 제공합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>팬-인은 전방 전파 중 현재 레이어의 출력 𝑍의 분산을 조절하는 데 도움을 줍니다.</li>
<li>팬-아웃은 역전파 중 후속 레이어의 가중치가 얼마나 영향을 미치는지 조정합니다.</li>
</ul>
<p>현재 레이어로 전방 및 역방향에서 공급되는 뉴런의 수를 고려하여, 연구자들은 다양한 초기화 방법들을 아이디어 위에 구축해냈습니다. Lecun, Xavier/Glorot 초기화 및 He/Kaiming 초기화가 이러한 방법들 중 일부입니다. 이러한 방법들의 아이디어는 꽤 유사합니다. 가중치를 생성할 때 균일 분포 또는 정규 분포 중 하나를 사용하고, 분산을 조절하기 위해 팬-인 또는 팬-아웃을 사용합니다. 이 분포들의 평균은 모두 0으로 설정하여 출력 값의 평균을 0으로 만듭니다.</p>
<pre><code class="hljs language-js"># 초기화의 다양한 유형

| 초기화          | 활성화 함수             | σ² (정규)  |
| -------------- | ----------------------------- | --------------- |
| <span class="hljs-title class_">Xavier</span>/<span class="hljs-title class_">Glorot</span>  | <span class="hljs-title class_">None</span>, tanh, logistic, softmax | <span class="hljs-number">1</span> / 팬_평균 |
| <span class="hljs-title class_">He</span>/<span class="hljs-title class_">Kaiming</span>     | <span class="hljs-title class_">ReLU</span> 및 변형                  | <span class="hljs-number">2</span> / 팬-인    |
| <span class="hljs-title class_">LeCun</span>          | <span class="hljs-variable constant_">SELU</span>                          | <span class="hljs-number">1</span> / 팬-인    |
</code></pre>
<p>Lecun 초기화는 가중치 분포에 작은 분산을 사용하여 𝑍의 분산을 축소하는 것에 기반합니다. 𝑍의 분산이 팬-인과 각 가중치의 분산의 곱이라면, 𝑍가 분산이 1이 되도록 보장하려면 각 가중치의 분산은 1/팬-인이어야 합니다. 따라서 Lecun 초기화는 가중치를 𝑁(0,1/팬-인)에서 무작위로 선택합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>자비에/글로뤼 초기화는 이전 레이어의 가중치(fan-in)의 영향을 고려할 뿐만 아니라 역전파 중 이러한 가중치가 후속 레이어에 미치는 영향(fan-out)도 고려합니다. 순방향 및 역방향 전파 중 분산을 균형있게 유지하기 위해 분산에 대한 공식인 2/(fan_in + fan_out)을 사용하여 가중치를 그려놓을 수 있습니다. 이 때의 분산은 Normal 분포, N(0,2/(fan_in + fan_out)) 또는 Uniform 분포(- sqrt(6/ (fan_in + fan_out)), sqrt(6/ (fan_in + fan_out))) 중에서 선택할 수 있습니다.</p>
<p>희/카이밍 초기화는 ReLU 및 그 변형에 특히 맞추어져 있습니다. ReLU는 음수 입력을 제로로 처리하므로 뉴런 활성화의 절반은 0이 아닌 것으로 예상되며, 이는 분산을 줄이고 그라디언트 소멸을 유발할 수 있습니다. 이에 대비하여 희 초기화는 Lecun 방법에서 사용된 분산을 두 배로 늘리는데, 이를 통해 ReLU를 사용하는 레이어에 필요한 균형을 유지합니다. Leaky ReLU 및 ELU의 경우 약간의 조정이 필요하지만(예: ELU의 경우 2 대신 1.55 배 사용), 원칙은 그라디언트를 안정화하기 위해 분산을 조정하고자 한다는 것입니다. 반면 SELU의 경우 자체 정규화 속성을 활용하기 위해 모든 숨겨진 레이어에 Lecun 초기화를 사용해야 합니다.</p>
<p>이 토론은 PyTorch와 같은 프레임워크에서 가중치 초기화가 어떻게 구현되는지에 대한 흥미로운 측면을 엽니다. 이는 다음과 같은 질문으로 제시될 수 있습니다 —</p>
<h2>PyTorch에서 가중치 초기화는 어떻게 구현되고, 그것이 특별한 이유는 무엇인가요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>파이토치에서는 선형 레이어의 가중치 초기화에 대한 기본 접근 방식이 르쿤 초기화 방법을 기반으로 합니다. 반면 케라스에서는 기본 초기화 기술로 제비어/글로럿 초기화가 사용됩니다.</p>
<p>그러나 파이토치는 가중치 초기화에 대해 매우 유연한 접근 방식을 제공합니다. 사용자는 모델에서 사용된 다양한 활성화 함수의 특정 요구 사항과 일치하도록 프로세스를 세밀하게 조정할 수 있습니다. 이 세밀한 조정은 두 가지 주요 구성 요소를 고려하여 달성됩니다:</p>
<ul>
<li>모드: 이 구성 요소는 레이어의 입력 연결 수(fan-in) 또는 출력 연결 수(fan-out)에 따라 초기화된 가중치의 분산이 조정되는지를 결정합니다.</li>
<li>게인: 이는 모델에서 사용된 활성화 함수에 따라 초기화된 가중치의 스케일을 조정하는 스케일링 계수입니다. 파이토치는 가중치 초기화 프로세스를 최적화하기 위해 맞춤형 게인 값을 계산하는 torch.nn.init.calculate_gain 함수를 제공합니다.</li>
</ul>
<p>가중치 초기화 매개변수를 사용자 정의하는 이 유연성을 통해 모델에서 사용된 특정 활성화 함수와 비교 가능하고 호환되는 초기화 접근 방식을 설정할 수 있습니다. 흥미로운 점은 파이토치의 가중치 초기화 구현이 서로 다른 초기화 방법 간의 어떤 관계를 나타낼 수 있는데, 이를 통해 신경망의 전반적인 기능을 향상시키기 위한 초기화 프로세스를 활용할 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>예를 들어, SELU 활성화 함수에 대한 PyTorch 문서를 검토하면 가중치 초기화의 흥미로운 측면을 발견할 수 있습니다. 문서에는 SELU 활성화와 함께 kaiming<em>normal 또는 kaiming_normal</em>을 사용하여 초기화할 때, nonlinearity=<code>selu</code> 대신 nonlinearity=<code>linear</code>을 선택해야 자가 정규화를 달성할 수 있다고 언급되어 있습니다. 이 세부 사항은 흥미로운데, PyTorch의 기본 Lecun 초기화가 Kaiming 방법을 선형 비선형성에서 gain이 1로 설정했을 때 Lecun 초기화 방법을 효과적으로 복제한다는 점을 강조합니다. 이는 Lecun 초기화가 보다 일반적인 Kaiming 초기화 접근법의 특정 응용이라는 것을 보여줍니다. 마찬가지로, Xavier 초기화 방법은 입력 연결의 수(fan-in)와 출력 연결의 수(fan-out)를 모두 고려하는 Lecun 초기화의 다른 변형으로 볼 수 있습니다.</p>
<h2>가중치를 분포로부터 초기화할 때 평균과 분산을 신중하게 선택해야 하는 점에 동의합니다. 그러나 왜 초기 가중치를 정규 분포 대신 균일 분포에서 추출하려고 하는지에 대한 이유는 여전히 명확하지 않습니다. 무엇 때문에 한 가지를 다른 것보다 선호하게 되는지 설명해주실 수 있나요?</h2>
<p>가중치를 초기화할 때 분포로부터 추출할 때 평균과 분산을 신중하게 선택하는 중요성에 대한 귀하의 주장은 타당합니다. 신경망에서 가중치를 초기화할 때 중요한 고려 사항 중 하나는 정규 분포나 균일 분포 중에서 추출할지 결정하는 것입니다. 명확한 연구 결과를 지지하는 답변이 없지만, 이러한 선택을 하는 이유에는 몇 가지 타당한 이유가 있습니다:</p>
<p>균일 분포는 엔트로피가 가장 높은 분포로, 범위 내의 모든 값이 동등하게 가능성이 있습니다. 이 공정한 접근은 초기화에 어떤 값이 더 잘 작동할지에 대한 사전 지식이 부족할 때 유용할 수 있습니다. 각 잠정적인 가중치 값에 공정하게 대우하고 균일한 확률을 할당합니다. 이는 한정된 정보로 게임에서 모든 팀에 공평하게 건 게임과 비슷합니다 - 선호되는 결과의 가능성을 최대화합니다. 어떤 구체적인 값이 좋은 초기 가중치인지 알 수 없기 때문에 균일 분포를 사용하면 편향되지 않은 시작점을 보장합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>한편, 정규 분포는 일반적으로 가중치를 0에 가까운 작은 값으로 초기화하는 것이 더 자주 일어납니다. 초기 가중치가 작을수록 출력의 분산이 줄어들고 학습 중 안정적인 기울기를 유지하는 데 도움이 되기 때문에 작은 초기 가중치가 일반적으로 선호됩니다. 이는 가중치 초기화 방법에서 단위 분산 대신 작은 분산을 선호하는 이유와 유사합니다. 게다가, 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수는 작은 초기 가중치 값에서 더 나은 성능을 발휘하며 이러한 활성화 함수가 숨겨진 레이어가 아닌 최종 출력 레이어에서만 사용되더라도 그렇습니다.</p>
<p>근본적으로, 균일 분포는 사전 지식이 부족한 상황에서 공평한 시작점을 제공하여 모든 잠재적인 가중치 값들을 동등하게 가능성 있는 것으로 간주합니다. 반면 정규 분포는 0에 가까운 작은 초기 가중치를 선호하여 기울기 안정성을 돕고 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수와 잘 맞습니다. 이러한 분포 사이의 선택은 종종 다른 신경 아키텍처와 작업에 걸쳐 경험적인 결과에 따라 이루어집니다. 보편적으로 최적의 방법은 존재하지 않지만, 균일 및 정규 분포의 특성을 이해하면 더 많이 발견되고 문제에 특화된 초기화 결정을 할 수 있게 됩니다.</p>
<h2>우리는 편향 항에 대해서도 이러한 가중치 초기화 방법을 사용합니까? 편향 항을 어떻게 초기화합니까?</h2>
<p>좋은 질문입니다. 우리는 편향 항에 대해서는 가중치와 동일한 초기화 기술을 반드시 사용하지는 않습니다. 사실, 편향 값을 모두 간단히 0으로 초기화하는 것이 흔한 실천입니다. 그 이유는 가중치가 각 뉴런이 기본 데이터를 근사하는 함수의 모양을 결정하는 반면, 편향은 각 함수를 위아래로 이동시키는 오프셋 값을 제공하기 때문입니다. 그래서 편향은 가중치가 학습하는 전반적인 형태에 직접적으로 영향을 주지 않습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>초기화의 주요 목표는 대칭을 깨고 가중치 학습에 좋은 시작점을 제공하는 것이므로 편향이 어떻게 초기화되는지에 대해 크게 걱정할 필요가 없습니다. 그들을 모두 0으로 설정하는 것이 일반적으로 충분합니다. 이에 대해 더 자세한 논의는 CS231n 강의 노트에서 찾아볼 수 있습니다.</p>
<h1>배치 정규화</h1>
<p>선택한 활성화 함수와 적절하게 초기화된 가중치로 신경망을 훈련 시작할 수 있습니다 (우리의 미니 아이스크림 공장 생산 라인을 가동시키는 것과 같습니다). 그러나 품질 통제가 필요합니다. 초기에는 물론 훈련 반복 중에도요. 두 가지 주요 기술은 특성 정규화와 배치 정규화입니다.</p>
<p>이전 포스트에서 경사 하강법에 대해 논의한 것처럼, 이러한 기술은 빠른 수렴을 위해 손실 풍경을 재구성합니다. 특성 정규화는 초기 데이터 입력에 이를 적용하며, 배치 정규화는 에폭 사이에 숨겨진 레이어의 입력을 정규화합니다. 두 기술 모두 다른 단계에서 품질 보증 점검을 구현하는 것과 유사합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>배치 정규화는 훈련 중에 각 레이어의 입력을 평균이 0이고 분산이 1인 값으로 정규화하여 내부 공변량 이동을 줄이어 경사 소실/폭발 문제를 완화하는 데 도움을 줍니다. 내부 이동이 발생하는 이유에 대해 생각해 보죠. 각 레이어의 매개변수를 기울기에 기반하여 업데이트하는 과정은 신경망의 각 레이어가 공장의 서로 다른 부서라고 생각할 수 있습니다. 한 부서의 매개변수(또는 설정)를 업데이트할 때마다 다음 부서의 입력이 변경됩니다. 이로 인해 각 레이어마다 새로운 변화에 대한 조정이 필요하며 이를 심층 학습에서 내부 공변량 이동이라고 합니다. 그렇다면 이러한 이동이 자주 발생할 때 어떻게 될까요? 네트워크가 안정화하기 어려워지며 각 레이어의 입력이 계속 변화함에 따라 문제가 발생합니다. 이는 공장의 한 부분에서 지속적인 변화가 제품 품질에 일관성 없이 영향을 미치는 것과 유사합니다. 이는 작업자들을 혼란스럽게 하고 작업 흐름을 망치는 결과를 초래할 수 있습니다.</p>
<p>배치 정규화는 훈련 중 미니 배치 전체에서 각 레이어의 입력을 정규화하여 평균이 0이고 분산이 1인 값으로 설정하는 것을 목표로 합니다. 레이어가 예상할 수 있는 일관된, 통제된 입력 분포를 강요합니다. 공장 비유로 돌아가서, 다음 부서로 전달되기 전 각 부서의 출력에 엄격한 품질 기준을 설정하는 것과 유사합니다. 예를 들어, 베이킹 부서가 일관된 크기와 모양의 아이스크림콘을 생산해야 한다는 규칙을 설정하는 것입니다. 다음 장식 부서는 콘의 변화량을 고려할 필요가 없게 되며, 각 일반화된 콘에 동일한 양의 아이스크림을 추가할 수 있습니다.</p>
<p>정규화를 통해 내부 공변량 이동을 줄이는 것으로 배치 정규화는 훈련 과정 중에 기울기가 엉망이 되는 것을 방지합니다. 레이어들이 신속히 변하는 입력 분포에 계속해서 재조정할 필요가 없어져서 기울기가 더 안정적으로 유지됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>또한 정규화는 정규화자 역할을 하며 목적 함수 랜드스케이프를 부드럽게 만듭니다. 이를 통해 더 높은 학습 속도를 사용하여 수렴 속도를 높일 수 있습니다. 일반적으로 배치 정규화는 내부 분산 이동을 줄이고 그래디언트를 안정화시키며 목적함수를 정규화하고 훈련 가속화를 가능하게 합니다.</p>
<h2>배치 정규화를 어떻게 적용해야 하나요? 활성화 함수 이전 또는 이후에 적용해야 하나요? 훈련 및 테스트 중에 어떻게 처리해야 하나요?</h2>
<p>배치 정규화는 그래디언트를 안정화시키는 추가 레이어를 통해 DNN을 훈련하는 방식을 실제로 바꿨습니다. DL 영역에서 활성화 함수 이전 또는 이후에 적용해야 하는지에 대한 논쟁이 있습니다. 솔직히 말해서, 이는 모델에 따라 다르며 조금은 실험해 봐야 할 수도 있습니다. 그냥 방법을 일정하게 유지하도록 하고 변경하면 예상치 못한 문제가 발생할 수 있습니다.</p>
<p>훈련 중에 배치 정규화 레이어는 각 미니 배치를 통해 각 차원에 대한 평균과 표준편차를 계산합니다. 이러한 통계량은 출력을 정규화하는 데 사용되어 평균이 0이고 분산이 1임을 보장합니다. 이 프로세스는 입력 분포를 표준 정규 분포로 변환하는 것으로 생각할 수 있습니다. 전체 훈련 데이터 세트를 사용하여 특징 정규화를 하는 것과는 달리 배치 정규화는 각 미니 배치에 기초하여 조정되어 처리되는 데이터에 동적이며 반응성을 가지게 됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이제, 테스트는 다른 이야기입니다. 테스트 데이터에서 평균과 분산을 정규화에 사용하면 안 됩니다. 대신에 이러한 매개변수는 학습된 특징으로 간주되어 훈련 과정에서 유지되어야 합니다. 훈련 중 각 미니 배치는 고유의 평균과 분산을 가지지만, 일반적인 실천 방법은 이러한 값들의 이동 평균을 훈련 단계 동안 사용하는 것입니다. 이를 통해 안정된 추정값을 제공하여 테스트 중에 적용할 수 있게 됩니다. 다른 적은 일반적인 방법은 전체 훈련 데이터 세트를 사용하여 포괄적인 평균과 분산을 계산하는 추가 에포크를 실행하는 방법도 있습니다.</p>
<p>PyTorch로 DNN 프레임워크로 훈련할 때, 조정 가능한 하이퍼파라미터인 γ와 β를 사용할 수 있습니다. 이러한 파라미터를 조정하여 배치 정규화 과정을 세밀하게 조정할 수 있습니다. 일반적으로 기본 설정은 매우 효과적입니다. 그러나 훈련 중에 PyTorch는 분산을 계산하기 위해 편향 추정량을 사용하지만, 테스트 중에 이동 평균을 위해 불편 추정량을 사용합니다. 이러한 조정은 모델이 미처 못 본 조건에서 인구 표준 편차를 더 정확하게 근사하고 모델의 신뢰성을 향상하는 데 도움이 됩니다.</p>
<p>배치 정규화를 올바르게 적용하는 것은 네트워크에서 효율적인 학습에 중요합니다. 네트워크가 잘 학습하는 것뿐만 아니라 다양한 데이터 집합과 테스트 시나리오에서 성능을 유지할 수 있게 합니다. 생산 라인의 각 세그먼트를 정확하게 교정하여 운전을 원활하고 일관되게 유지하는 것으로 생각해보세요.</p>
<h2>왜 역전파 중에 그래디언트에 직접 배치 정규화를 적용하는 대신 순전파 중에 배치 정규화가 적용되나요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>일반적으로 순방향 패스 중 입력 또는 활성화에 배치 정규화를 적용하는 이유가 역전파 중에 기울기 자체에 직접 배치 정규화를 적용하는 것보다 더 일반적입니다.</p>
<p>먼저, 기울기에 배치 정규화를 직접 적용하는 이점을 보여주는 실증적 증거나 실무가 부족합니다. 내부 공변량 이동의 개념은 주로 순방향 패스 중에 발생하며, 계층 입력의 분포가 매개변수 업데이트로 인해 변경됩니다. 따라서, 후속 계층에서 처리되기 전에 이러한 입력을 안정화시키기 위해 이 단계에서 배치 정규화를 적용하는 것이 합리적입니다. 또한, 기울기에 배치 정규화를 직접 적용하는 것은 기울기의 크기와 방향이 나르는 중요한 정보를 왜곡할 수 있습니다. 이는 내재적 의미를 변경하는 방식으로 고객 피드백을 변조하는 것과 유사하며, 이는 미니 아이스크림 공장의 제조 프로세스에 대한 향후 조정을 잘못 이끌 수 있습니다.</p>
<p>그러나, 기울기를 경사 클리핑과 같은 마이너 조정을 하는 것은 일반적으로 허용되며 유익합니다. 이 기법은 기울기를 지나치게 크게 만들지 않고 안전한 범위 내에 유지하여 기울기를 제한하는 도구입니다. 이는 피드백에서 극단적 아웃라이어를 걸러내는 것과 유사하며, 이는 프로세스를 방해할 수 있는 급격한 반응을 방지하면서 전체 피드백의 무결성을 유지하는 데 도움이 됩니다. PyTorch에서는 기울기 노름을 모니터링하는 것이 일반적이며, 기울기가 폭발하기 시작하면 경사 클리핑과 같은 기법을 사용할 수 있습니다. PyTorch는 torch.nn.utils.clip<em>grad_norm</em> 및 torch.nn.utils.clip<em>grad_value</em>와 같은 함수를 제공하여 이를 관리할 수 있습니다.</p>
<h2>직접 정규화 대신 기울기를 클리핑하는 옵션을 언급했습니다. 왜 기울기를 클리핑하는 대신 바닥값을 설정하지 않는지 정확히 선택하는 이유가 무엇인가요?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>기울기 클리핑은 폭발하는 기울기 문제를 방지하는 데 도움이 되는 간단하면서도 효율적인 기술입니다. 종종 기울기의 최대값을 수동으로 제한합니다. 예를 들어 ReLU 활성화 함수는 상한값을 6으로 설정할 수 있으며, PyTorch에서는 ReLU6로 알려져 있습니다. 이 상한을 설정함으로써, 각 층에서 역전파 과정 중 기울기가 연쇄 법칙에 따라 곱해질 때 값이 지나치게 커지지 않도록 보장합니다. 이러한 클리핑은 기울기가 학습 과정을 방해할 정도로 급격하게 증가하는 것을 방지하여 그 값을 관리 가능한 한도 내에 유지합니다.</p>
<p>한편, 기울기를 억제하는 것은 너무 작아지지 않도록 하기 위해 하한값을 설정하는 것입니다. 그러나 이는 사그라들어 가는 기울기 문제의 근본적인 해결책이 되지는 않습니다. 일부 활성화 함수인 시그모이드나 tanh 같은 경우 입력이 0에서 멀어질수록 기울기 값을 매우 심각하게 축소시키기 때문에 기울기의 사그라들음 문제가 발생합니다. 이는 학습 속도가 극도로 느려지거나 정체되는 매우 작은 기울기 값을 야기합니다. 기울기를 억제해도 해결되지 않는 이유는 문제의 근본이 활성화 함수의 성질에 기인하기 때문입니다. 즉, 단순히 값이 너무 작은데만 있지 않고 활성화 함수가 기울기 값을 압축하는 것에 있습니다. 따라서 사그라드는 기울기 문제를 효과적으로 해결하기 위해서는 네트워크 아키텍처나 활성화 함수 선택을 조정하는 것이 더 유익합니다. 기울기가 사그라들지 않도록 하는 활성화 함수 사용(ReLU같은), ResNet 아키텍처에서 볼 수 있는 스킵 연결 추가, LSTM이나 GRU 같은 RNN에서 게이트 메커니즘을 사용하는 등의 기술을 통해 기울기는 역전파 중 네트워크 전반에 걸쳐 더 건강한 흐름을 보장하여 자연스럽게 사그라드는 것을 방지할 수 있습니다.</p>
<p>요약하면, 기울기 클리핑은 지나치게 큰 기울기를 효과적으로 관리하지만, 하한값을 설정하는 기울기 억제는 지나치게 작은 기울기 문제를 효과적으로 다루지 못합니다. 대신, 사그라드와 관련된 문제를 해결하려면 일반적으로 구조적인 조정이 필요합니다.</p>
<p>#실무에서의 경험(개인 경험)</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>요약할 때, 모든 논의된 방법이 사라지는 그래디언트 문제와 폭주하는 그래디언트 문제를 해결하는 데 유용하다는 것은 명백합니다. 이들은 모두 모델의 학습 과정을 향상시킬 수 있는 실용적인 접근 방법입니다. 이 글을 마무리하며 한 가지 질문으로 마무리하고 싶습니다 -</p>
<h2>현실은 무엇인가요? 실무에서는 어떤 일반적인 과정이 있나요?</h2>
<p>실무에서 좋은 소식은 가능한 모든 해결책을 실험할 필요가 없다는 것입니다. 활성화 함수를 선택할 때, ReLU가 종종 선택되는 것이며 매우 비용 효율적입니다. ReLU는 양의 입력의 크기를 변경하지 않고 전달합니다 (시그모이드나 tanh는 큰 값을 크기와 관계없이 항상 1로 압축합니다) 그리고 계산 및 미분 측면에서 간단합니다. 주요 프레임워크에서 잘 지원되며 dead ReLU 문제를 우려한다면 Leaky ReLU, ELU, SELU, 또는 GELU와 같은 대안을 고려할 수 있지만 일반적으로 시그모이드와 tanh를 피해야 하는 사라지는 그래디언트 문제를 피하기 위해 명확을 지켜야 합니다.</p>
<p>선호되는 활성화 함수인 ReLU로 인해 가중치 초기화가 지나치게 민감하게 작용하는 문제에 대해 덜 걱정해도 됩니다. 시그모이드, tanh 및 SELU와 같은 함수에서 주로 발생하는 문제일 뿐입니다. 대신, 선택한 활성화 함수에 권장되는 가중치 초기화 방법에 집중하는 것이 적당합니다 (예를 들어, ReLU에 대해 He/Kaiming 초기화를 사용하는 이유는 ReLU의 비선형성을 고려하기 때문입니다).</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>네트워크에는 항상 배치 정규화를 포함하세요. 활성화 함수 전 또는 후에 적용할지 결정(또는 실험)하고, 모델 전체에서 일관되게 그 선택을 유지하세요. 배치 정규화는 규제 효과와 높은 학습률 사용이 가능해지는 등 여러 가지 이점을 제공합니다. 이는 교육 및 수렴 속도를 높일 수 있습니다.</p>
<p>그래서 어떤 것을 실험해볼 가치가 있을까요? 옵티마이저는 탐구할 가치가 있습니다. 이전 글에서 그라디언트 디센트 및 그 인기 있는 변형 등 다양한 옵티마이저를 논의했습니다. Adam은 빠르지만 과적합을 유발하고 학습률을 너무 빨리 감소시킬 수 있습니다. SGD는 신뢰성이 있고 병렬 컴퓨팅 환경에서 특히 효과적일 수 있습니다. 느릴 수 있지만 모델로부터 최대 성능을 뽑아내려면 확실한 선택입니다. 때로는 RMSprop이 더 나은 대안일 수 있습니다. 저는 Adam으로 시작하여 속도를 이유로 한 후에 더 나은 최소값을 찾고 과적합을 방지하기 위해 후기 에포크에서 SGD로 전환하는 것이 좋은 전략으로 생각합니다.</p>
<p>만약 이 시리즈를 즐기고 계시다면, 상호작용(박수, 댓글 및 팔로우)이 지지뿐만 아니라 시리즈를 이어가는 원동력이자 저의 계속된 공유를 영감받는 기반이 됩니다.</p>
<p>이 시리즈의 다른 게시물:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>ML 학습의 용기: L1 및 L2 정규화 해독하기 (파트 1)</li>
<li>ML 학습의 용기: 우도, MLE 및 MAP 해독하기</li>
<li>ML 학습의 용기: F1, 재현율, 정밀도 및 ROC 곡선에 대한 심층 탐구</li>
<li>ML 학습의 용기: 가장 일반적인 손실 함수에 대한 상세 가이드</li>
<li>ML 학습의 용기: 경사 하강법과 인기 있는 옵티마이저에 대한 심층 탐구</li>
<li>ML 학습의 용기: 수학적 이론부터 코딩 실무까지 백프로파게이션 설명</li>
</ul>
<h2>참고 자료</h2>
<p>활성화 함수</p>
<ul>
<li><a href="https://ml-explained.com/blog/activation-functions-explained#gaussian-error-linear-unit-gelu" rel="nofollow" target="_blank">가우시안 에러 선형 유닛 (GeLU) 설명</a></li>
<li><a href="https://www.mldawn.com/relu-activation-function/" rel="nofollow" target="_blank">ReLU 활성화 함수</a></li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>가중치 초기화</p>
<ul>
<li><a href="https://datascience.stackexchange.com/questions/102036/where-does-the-normal-glorot-initialization-come-from" rel="nofollow" target="_blank">normal glorot initialization(일반 글로럿 초기화)의 원천</a></li>
<li><a href="https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/2" rel="nofollow" target="_blank">파이토치(PyTorch)에서의 기본 초기화에 대한 명확한 이해</a></li>
</ul>
<p>그래디언트 클리핑</p>
<ul>
<li><a href="https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch" rel="nofollow" target="_blank">파이토치(PyTorch)에서 그래디언트 클리핑 하는 방법</a></li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2","description":"","date":"2024-05-18 19:28","slug":"2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2","content":"\n\"“Courage to Learn ML”의 새로운 장을 찾아주신 여러분, 환영합니다. 이 시리즈는 복잡한 주제들을 쉽고 재미있게 다루고, 멘토와 학습자 간의 캐주얼 대화처럼 친밀한 분위기를 제공하기 위해 만들어졌습니다. “용기로 방어하다”의 쓰기 스타일에서 영감을 받아 기계 학습에 특히 집중하고 있어요.\n\n이번 시간에는 사라지는 그래디언트와 폭발하는 그래디언트의 어려움을 극복하는 방법에 대해 계속해서 탐구할 거예요. 첫 번째 세그먼트에서 우리는 네트워크 내에서 효율적인 학습을 보장하기 위해 안정적인 그래디언트 유지가 왜 중요한지에 대해 이야기했어요. 불안정한 그래디언트가 우리 네트워크의 심화를 방해할 수 있고 결국 깊은 \"학습\"의 잠재력을 제한할 수 있다는 것을 밝혀냈죠. 이러한 개념을 살려내기 위해 DNN(맛있고 영양가 있는 얹힌 작은 얼음 공장)이라는 소형 아이스크림 공장을 운영하는 비유를 사용하고 수렴한 팩토리 생산 라인을 조율하는 것과 유사한 DNN 훈련을 위한 강력한 전략을 명료하게 보여줬어요.\n\n이제, 두 번째 이야기에서는 각 제안된 솔루션에 대해 더 심층적으로 탐구하며, 아이스크림 공장을 활기차게 만든 것과 같은 명확성과 창의성으로 그들을 살펴볼 거에요. 여기 이번 부분에서 다룰 주제 목록입니다:\n\n- 활성화 함수\n- 가중치 초기화\n- 배치 정규화\n- 실제 적용(개인 경험)\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 활성화 함수\n\n활성화 함수는 우리 \"공장\" 설정의 핵심입니다. 이 함수들은 우리의 DNN 조립 라인 내에서 전방 및 후방 전파를 통해 정보를 전달하는 역할을 합니다. 적절한 활성화 함수를 선택하는 것은 우리의 DNN 조립 라인 및 이에 따라 우리의 DNN 훈련 과정이 원활하게 작동하는 데 중요합니다. 이 부분은 활성화 함수의 장닿과 단점을 간단히 설명하는 것이 아닙니다. 여기서는 다양한 활성화 함수의 생성 배경을 파악하고 종종 간과되는 중요한 질문에 대답하기 위해 Q\u0026A 형식을 사용할 것입니다.\n\n이러한 함수들을 우리 아이스크림 생산 비유의 블렌더로 생각해보세요. 이용 가능한 블렌더 목록을 제공하는 대신, 각각의 혁신과 특정 개선 사항 뒤에 있는 이유를 심층적으로 검토하고 이해하는 데 도움을 드리겠습니다.\n\n## 활성화 함수란 무엇이며, 어떻게 적절한 함수를 선택할 수 있을까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Activation functions](/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png)\n\n활성화 함수는 신경망 모델에 선형 및 비선형 관계를 모두 포착할 수 있는 유연성과 강력함을 부여하는 주요 요소입니다. 로지스틱 회귀와 DNN의 주요 차이점은 이러한 활성화 함수들과 여러 층을 결합하는 데 있습니다. 이들은 NN이 다양한 함수를 근사할 수 있게 합니다. 그러나 이러한 능력은 도전과제와 함께 제공됩니다. 활성화 함수 선택에는 더 주의를 기울여야 합니다. 잘못된 선택은 모델이 특히 역전파 중에 효과적으로 학습하는 것을 막을 수 있습니다.\n\n당신이 당사 DNN 아이스크림 공장의 매니저로 상상해보세요. 당신은 생산 라인을 위해 적절한 활성화 함수(아이스크림 믹서로 생각해보세요)를 섬세하게 선택하고 싶을 것입니다. 즉, 당신의 요구 사항에 가장 적합한 것을 찾는 데 신중을 기울이고 최적의 선택지를 찾아내야 합니다.\n\n따라서 효과적인 활성화 함수를 선택하는 첫 번째 단계는 두 가지 핵심 질문에 대한 대답을 찾는 것입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 활성화 함수의 선택이 소멸 그래디언트나 폭발 그래디언트와 같은 문제에 어떤 영향을 미치나요? 어떤 기준이 좋은 활성화 함수를 정의하나요?\n\n은닉층에서 활성화 함수를 선택할 때, 주로 소멸 그래디언트와 관련된 문제가 발생합니다. 이는 전통적인 시그모이드 활성화 함수(가장 전통적이고 기본적인 모델)로 거슬러 올라갈 수 있습니다. 시그모이드 함수는 입력값을 확률 범위(0부터 1)에 매핑할 수 있는 능력으로 널리 사용되었습니다. 이는 이진 분류 작업에서 특히 유용합니다. 이 능력 덕분에 연구자들은 예측을 분류하기 위한 확률 임계값을 조정하여 모델의 유연성과 성능을 향상할 수 있었습니다.\n\n그러나 이를 은닉층에 적용하는 것은 주로 소멸 그래디언트 문제를 야기했습니다. 이는 주로 두 가지 주요 요인으로 설명할 수 있습니다:\n\n- 순방향 전파 과정에서 시그모이드 함수는 입력을 0과 1 사이의 매우 좁은 범위로 압축합니다. 한 네트워크가 은닉층에서 활성화 함수로 시그모이드만 사용하는 경우, 여러 층을 거칠수록 이 범위가 더욱 좁아지게 됩니다. 이 압축 효과로 인해 출력의 변동성이 감소하고 양수 값으로의 편향이 도입됩니다. 입력 부호에 관계없이 출력은 0과 1 사이에 유지되기 때문에.\n- 역전파 과정에서 시그모이드 함수의 도함수(종모양 곡선)는 0과 0.25 사이의 값을 생성합니다. 이 작은 범위는 입력을 가로지르는 그래디언트가 여러 층을 통과함에 따라 급속하게 감소할 수 있도록 할 수 있습니다. 이것은 앞선 층 그래디언트가 연속된층의 도함수의 곱으로 이루어지기 때문인데, 이러한 낮은 도함수의 복합 곱은 점점 더 작은 그래디언트를 결과로 가져와서 초기 층에서의 효과적인 학습을 방해합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이러한 제약 사항을 극복하기 위해 이상적인 활성화 함수는 다음과 같은 특성을 보여야 합니다:\n\n- 비선형성. 네트워크가 복잡한 패턴을 포착할 수 있도록 함.\n- 비포화. 함수와 그 도함수가 입력 범위를 과도하게 압축하지 않아서 gradient 소멸을 방지해야 함.\n- 중심이 0인 출력. 함수는 양수 및 음수 출력 둘 다를 허용해야 하며, 각 노드 사이의 평균 출력이 특정 방향으로의 편향을 도입하지 않도록 해야 함.\n- 계산 효율성. 함수와 그 도함수가 계산적으로 간단하여 효율적인 학습을 용이하게 해야 함.\n\n## 이러한 기본 특성들을 고려할 때, 인기있는 활성화 함수들이 기본 모델인 Sigmoid를 어떻게 개선하고 뛰어나게 만드는지 알아봅시다.\n\n이 섹션은 거의 모든 현재 활성화 함수에 대한 일반적인 개요를 제공하려고 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n테이블 태그를 마크다운 형식으로 변경하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nReLU의 고려 사항 중 하나는 선형 세그먼트 간의 급격한 전환으로 인해 x=0에서 미분 불가능하다는 것입니다. 실제로 PyTorch와 같은 프레임워크는 subgradient 개념을 사용하여 이를 해결하며, 종종 x=0에서 도함수를 0.5 또는 [0, 1] 내의 다른 값으로 설정합니다. 이는 보통 정확한 제로 입력이 드물고 데이터의 변동성 때문에 문제가 되지 않습니다.\n\n그래서, ReLU가 여러분에게 적합한 선택일까요? 많은 연구자들은 그렇다고 말합니다. 이는 그 간결함, 효율성 및 주요 DNN 프레임워크의 지원 덕분입니다. 게다가 https://arxiv.org/abs/2310.04564 같은 최근 연구들이 ReLU의 계속된 중요성을 강조하며, ML 분야에서의 부활과 같은 시대를 맞이한다고 강조하고 있습니다.\n\nLeaky ReLUs는 클래식적인 ReLU에 약간의 변화를 준겳이며, ReLU를 더 자세히 살펴보면 몇 가지 문제점이 드러납니다. 음수 입력에 대한 제로 출력으로 이어지는 것은 'dying ReLU' 문제로 이어지며, 뉴런들이 훈련 중 업데이트되지 않게 됩니다. 또한, ReLU는 양수 값을 선호하여 모델에 방향성 편향을 도입할 수 있습니다. 이러한 단점을 극복하면서 ReLU의 이점을 유지하기 위해, 여러 연구자들이 'leaky' ReLU와 같은 여러 변형을 개발했습니다.\n\nLeaky ReLU는 ReLU의 음수 부분을 수정하여 작고 0이 아닌 기울기를 부여합니다. 이 조정은 음수 입력이 작은 음수 출력을 생성하도록하며, 효과적으로 그 외의 0 출력 영역을 '누출'시킵니다. 이 누출의 기울기는 하이퍼파라미터 알파(α)에 의해 제어되며, 전형적으로 뉴런을 활성 유지와 희소성 사이의 균형을 유지하기 위해 0에 가깝게 설정됩니다. 작은 음수 출력을 허용함으로써, Leaky ReLU는 활성 함수의 출력을 0 주변으로 중앙 집중시키고 뉴런이 비활성화되지 않게 하여 'dying ReLU' 문제에 대응합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 하이퍼파라미터로 α를 도입하면 모델 튜닝에 대한 복잡성이 추가됩니다. 이를 관리하기 위해 원본 Leaky ReLU의 변형이 개발되었습니다:\n\n- Randomized Leaky ReLU (RReLU): 이 버전은 훈련 중에 α를 지정된 범위 내에서 무작위로 지정하고 평가 중에는 고정합니다. 무작위성은 모델을 정규화하고 과적합을 방지하는 데 도움이 될 수 있습니다.\n- Parametric Leaky ReLU (PReLU): PReLU는 훈련 중에 α를 학습할 수 있도록 하며, 활성화 함수를 데이터셋의 특정 요구에 맞게 조정할 수 있습니다. 이는 α를 훈련 데이터에 맞게 조정하여 모델 성능을 향상시킬 수 있지만, 과적합의 위험도 증가시킵니다.\n\nLeaky ReLU를 개선한 Exponential Linear Unit (ELU). Leaky ReLU와 ELU는 음의 값을 허용하여 평균 유닛 활성화를 제로에 가깝게 밀어내고 활성화 함수의 활력을 유지하는 데 도움이 됩니다. Leaky ReLU의 문제점은 이 음의 값의 범위를 조절할 수 없다는 것입니다. 이론적으로 이 값들은 작게 유지하려는 의도에도 불구하고 음의 무한대로 확장될 수 있습니다. ELU는 이를 해결하기 위해 비선형 지수 곡선을 비정상적인 입력에 통합하여 음의 출력 범위를 최대 -𝛼(일반적으로 1로 설정되는 새로운 하이퍼파라미터)로 좁히고 제어합니다. 또한 ELU는 매끄러운 함수입니다. 그 지수 요소 덕분에 음과 양 값 사이에서 매끄러운 전환을 가능하게 하며, 입력 값에 대한 잘 정의된 기울기를 보장하여 기울기 기반 최적화에 유리합니다. 이 기능은 ReLU와 Leaky ReLU에서 보이는 미분 불가능 문제를 해결합니다.\n\nSelf-Normalizing 속성을 갖춘 향상된 ELU인 Scaled Exponential Linear Unit (SELU). SELU는 신경망 내에서 제로 평균 및 단위 분산을 유지하도록 설계된 ELU의 확장된 버전입니다. 양의 순입력의 기울기가 1을 초과하도록 고정 스케일 요인 λ(1보다 큰 값)을 통합함으로써 SELU는 하위 레이어의 기울기가 줄어드는 상황에서 기울기를 증폭하여 딥 뉴럴 네트워크에서 자주 발생하는 소멸하는 기울기 문제를 예방하는 데 특히 유용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSELU에 대해, 매개변수(α 및 λ)는 고정된 값이며 학습할 수 없으므로 조정해야 할 매개변수가 적어 튜닝 과정이 간소화됩니다. SELU 구현에서 이러한 특정 값들을 찾을 수 있습니다.\n\nSELU는 실제로 활성화 함수 세계에서 정교한 \"믹서\"인데요, 특정 요구 사항이 딸려옵니다. 단방향 또는 순차 네트워크에서 가장 효과적이며 RNN, LSTM 또는 건너뛰기 연결을 갖는 아키텍처에서는 그 설계 때문에 그런만큼 성능이 좋지 않을 수 있습니다.\n\nSELU의 자기 정규화 기능을 위해서는 입력 피처가 표준화되어야 합니다. 평균이 0이고 표준 편차가 1인 것이 중요합니다. 또한, 매 숨겨진 레이어의 가중치는 LeCun 정규 초기화를 사용하여 초기화되어야 합니다. 여기서 가중치는 평균이 0이고 분산이 1/fan_in인 정규 분포에서 샘플링됩니다. \"fan_in\"이란 용어가 익숙하지 않다면, 가중치 초기화에 대한 전용 세션에서 설명하겠습니다.\n\n요약하면 SELU의 자기 정규화가 효과적으로 기능하려면 입력 피처가 정규화되고 네트워크 구조가 끊기지 않는 것을 보장해야 합니다. 이 일관성은 네트워크 전체에서 자기 정규화 효과가 유지되도록 도와주며 누출 없이 계속 유지되도록 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nGELU (Gaussian Error Linear Unit)은 Dropout으로부터 규제 아이디어를 통합한 혁신적인 활성화 함수입니다. 기존 ReLU가 음수 입력에 대해 0을 출력하는 반면, leaky ReLU, ELU 및 SELU는 음수 출력을 허용합니다. 이를 통해 활성화의 평균을 0에 가깝게 이동시켜 편향을 줄이는데 도움을 줍니다. 이는 ReLU와 비슷한 방식으로 편향을 줄이지만 음수 입력을 완전히 0으로 만들지 않고 음의 값을 허용한다는 것을 의미합니다. 그러나 이러한 누출은 \"죽어 가는 ReLU\"의 일부 이점을 잃어버릴 수 있음을 의미합니다. 여기서는 일부 뉴런의 비활성으로 더 sparse하고 일반화된 모델을 얻을 수 있습니다.\n\n죽어 가는 ReLU 및 Dropout의 희소성 이점을 고려할 때, GELU는 여기에 한 발 더 나아간 것입니다. GELU는 0 출력의 특성을 가진 죽어 가는 ReLU를 무작위적인 요소와 결합하여 뉴런이 재활성화될 수 있는 가능성을 열어줍니다. 이 접근은 유익한 희소성을 유지하는 것뿐만 아니라 뉴런 활동을 재도입하여 GELU를 견고한 해결책으로 만듭니다. 이 메커니즘을 완전히 이해하기 위해 GELU의 정의를 자세히 살펴보겠습니다:\n\n![이미지](/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_1.png)\n\nGELU 활성화 함수에서 CDF인 Φ(x) 또는 표준 가우스 누적 분포 함수가 중요한 역할을 합니다. 이 함수는 표준 정규 분포를 따를 때 x보다 작거나 같은 값을 갖는 것으로 나타내는 확률을 나타냅니다. Φ(x)는 음수 입력에 대해 0부터 양수 입력에 대해 1로 매끄럽게 전환되어, 입력의 스케일링을 효과적으로 제어합니다. Dan Hendrycks 외(출처)의 논문에 따르면 뉴런 입력은 배치 정규화를 사용할 때 특히 정규 분포를 따르는 경향이 있어 정규 분포의 사용이 정당화됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n해당 함수의 디자인은 x 값이 줄어들수록 입력이 더 자주 \"떨어지도록\" 허용하여 변환을 확률적이면서 입력 값에 의존적으로 만듭니다. 이 메커니즘은 흔히 사용되는 직선 함수인 f(x) = x를 더 부드럽게 만들어 ReLU 함수와 유사한 형태를 유지하며, 조각별 선형 함수에서 발생하는 갑작스러운 변화를 피합니다. GELU의 가장 중요한 특징 중 하나는 뉴런을 완전히 비활성화할 수 있다는 것으로, 이를 통해 입력 값의 변화에 따라 다시 활성화될 수 있습니다. 이러한 확률적 성질은 입력 값에 의존하지만 완전히 무작위적이지 않아 뉴런이 다시 활성화될 기회를 제공합니다.\n\n아래는 GELU가 ReLU보다 두드러지는 이점이라고 요약할 수 있습니다. GELU는 어떤 입력 값이 양수인지 음수인지에 관계없이 전체 입력 값 범위를 고려합니다. Φ(x) 값이 감소함에 따라 GELU 함수의 출력이 0에 가까워지는 확률이 증가하여 뉴런을 부드럽게 \"떨어뜨리게\" 됩니다. 이 방법은 전형적인 드롭아웃 방식보다 더 정교하며, 무작위적으로 하는 것이 아니라 데이터에 따라 뉴런의 비활성화를 결정하도록 되어 있습니다. 이 방식은 매우 매력적으로 느껴지며, 마치 고급 디저트에 부드러운 크림을 추가하여 조금 더 향상된 미각을 경험하는 것과 같다고 생각합니다.\n\nGELU는 GPT-3, BERT 및 다른 Transformers와 같은 모델에서 효율적이며 언어 처리 작업에서 강력한 성능을 보여 인기 있는 활성화 함수가 되었습니다. 확률적 성질 때문에 계산 위주이지만, 표준 가우스 누적 분포인 Φ(x)의 곡선은 시그모이드와 tanh 함수와 유사합니다. 흥미로운 점은 GELU가 tanh를 사용하거나 x(1.702\\*x) 공식을 사용하여 근사할 수 있다는 것입니다. 이러한 단순화 가능성에도 불구하고, PyTorch의 GELU 구현은 그러한 근사가 종종 불필요할 정도로 빠르게 진행됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n더 깊이 들어가기 전에 먼저 정리해보자면,\n\n## ReLU를 검토하고 이로부터 영감받은 다른 활성화 함수가 무엇인지를 살펴보면 어떤 활성화 함수가 좋을지 정확히 알아볼까요?\n\nGünter Klambauer 등의 논문에서 SELU가 소개된 적이 있습니다. 여기서는 효과적인 활성화 함수의 중요한 특성을 강조했는데요.\n\n- 범위: 네트워크 전체의 평균 활성화 수준을 조절하는 데 도움이 되기 위해 음수와 양수 값을 출력해야 합니다.\n- 포화 영역: 도함수가 제로에 가까워지는 영역으로, 하위층의 너무 높은 분산을 안정화하는 데 도움을 줍니다.\n- 증폭 슬로프: 하위 층에서 너무 낮은 분산을 높이기 위해 중요한 기울기가 있어야 합니다.\n- 연속성: 연속적인 곡선은 분산의 변화를 안정화하고 증가시키는 효과를 균형있게 유지하는 고정점을 보장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 \"이상적인\" 활성화 함수에 대한 두 가지 추가 기준을 제안하고 싶습니다:\n\n- 비선형성: 이것은 명백하고 필수적입니다. 왜냐하면 선형 함수는 복잡한 패턴을 효과적으로 모델링할 수 없기 때문입니다.\n- 동적 출력: 출력이 제로이고 입력 데이터에 따라 출력을 변경할 수 있는 능력은 동적 뉴런 활성화와 비활성화를 가능하게 합니다. 이렇게 하면 네트워크가 변화하는 데이터 조건에 효율적으로 적응할 수 있습니다.\n\n## 활성화 함수가 음수를 출력하는 이유에 대해 더 직관적인 설명을 부탁드려도 될까요?\n\n활성화 함수를 입력 데이터를 변환하는 블렌더로 생각해 보세요. 일부 재료를 선호하는 블렌더처럼, 활성화 함수는 그들의 본질적인 특성에 따라 편향을 도입할 수 있습니다. 예를 들어, 시그모이드 및 ReLU 함수는 일반적으로 입력과 관계없이 비음수 출력만 나타냅니다. 이는 블렌더가 어떤 재료를 넣어도 항상 동일한 맛을 내는 것과 유사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_3.png)\n\n이 편향을 최소화하려면 부정적이고 긍정적인 값을 출력할 수 있는 활성화 함수를 가지는 것이 좋습니다. 기본적으로 우리는 중심이 제로인 출력을 목표로 합니다. 활성화 함수의 출력을 나타내는 놀이터를 상상해보세요. Sigmoid나 ReLU와 같은 함수로는, 이 놀이터는 부정적인 입력을 무시하거나 제로로 바꾸기 때문에 긍정적인 쪽으로 크게 기울어져 있습니다. Leaky ReLU는 음수 입력이 약간 음수 출력을 생성하도록 허용함으로써 이 놀이터를 균형잡게 시도하지만, 부정적 기울기의 선형 및 상수적 성격 때문에 조정이 미미합니다. 반면에 Exponential Linear Unit (ELU)은 지수 구성 요소로 음수 측면에 더 다이나믹한 밀어넣기를 제공하여, 더 균형 잡힌 상태에 가까워질 수 있도록 돕습니다. 이 균형은 긍정적 및 부정적 업데이트가 훈련에 기여하도록 보장함으로써 신경망에서 건강한 그레이디언트 플로우와 효율적인 학습을 유지하는 데 중요합니다, 단방향 업데이트의 제한을 피하기 위해.\n\n## ReLU와 유사하게 양수 입력을 제로화하는 활성화 함수를 생성할 수 있을까요, min(0, x)를 사용하여 양수 입력을 제로화하는 함수를 선호하는 이유는 무엇인가요?\n\n확실히, ReLU의 양수 값을 제로화하고 음수 값을 그대로 통과시키는 버전을 설계할 수 있습니다. 이것은 기술적으로 실행 가능한데, 중요한 점은 여기서 값의 부호가 아니라 네트워크에 비선형성을 도입하는 것입니다. 이 활성화 함수들이 일반적으로 출력 레이어가 아닌 숨겨진 레이어에서 사용된다는 것을 기억하는 것이 중요합니다. 즉, 이 네트워크 내의 이러한 활성화 함수의 존재는 최종 출력의 부호에 영향을 미치지 않고 이 레이어의 특성에 의해 직접적으로 영향을 받지 않더라도 최종 출력이 여전히 양수와 음수 모두가 될 수 있다는 것을 의미합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n무슨 상황에서든, 네트워크의 가중치와 편향은 출력의 필요한 부호에 맞게 조정될 수 있습니다. 예를 들어, 전통적인 ReLU에서 출력이 1이고 다음 레이어의 가중치가 1이면 출력은 여전히 1로 유지됩니다. 마찬가지로, 제안된 ReLU 변형이 -1을 출력하고 가중치가 -1이면 결과는 여전히 1이 됩니다. 본질적으로, 우리는 출력의 부호보다는 크기에 더 신경을 씁니다.\n\n따라서, ReLU가 부정적인 쪽에서 포화되는 것은 양수 쪽에서 포화되는 것과 근본적으로 다르지 않습니다. 그러나 우리가 영 중심 활성화 함수를 중요시하는 이유는 양수 또는 음수 값에 대한 내재적인 선호도를 방지하여 모델에서 불필요한 편향을 피하기 위한 것입니다. 이 균형은 네트워크 전체에 걸쳐 중립성과 효과적인 학습을 유지하는 데 도움이 됩니다.\n\n## Leaky ReLU와 같은 함수들은 출력을 영 중심 주변에 유지하기 위해 음수값을 출력할 필요가 있습니다. 그렇다면 ELU, SELU, GELU는 왜 음수 입력에 포화되도록 특별히 설계되었을까요?\n\n이를 이해하기 위해, ReLU 뒤에 있는 생물학적 영감을 살펴볼 수 있습니다. ReLU는 생물학적 뉴런을 모방하는데, 이들은 한계값을 가지고 있습니다. 이 한계값을 초과하는 입력은 뉴런을 활성화시키고, 그 이하는 그렇지 않습니다. 활성 및 비활성 상태 간 전환 가능성은 신경 기능에서 중요합니다. ELU, SELU, GELU와 같은 변형을 고려할 때, 이들의 설계가 두 가지 다른 필요에 부합함을 알 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 긍정적 영역: 임계값을 초과하는 신호가 전달될 때 전달되는 원하는 신호를 전송하는 것을 의미합니다.\n- 부정적 영역: 원치 않는 신호를 최소화하거나 걸러내며 대형 부정적 값의 영향을 완화하여 누수하는 게이트처럼 작동합니다.\n\n이러한 기능은 입력에 대한 게이트 역할을 하며, 뉴런의 출력에 영향을 미쳐야 하는 사항과 그렇지 말아야 하는 사항을 관리합니다. 예를 들어, SELU는 다음 두 가지 측면을 구분하여 활용합니다:\n\n- 긍정적 영역: 스케일링 인자 λ (1보다 큼)는 신호를 전달하지 않을 뿐만 아니라 약간 증폭시킵니다. 역전파 중 이 영역의 도함수는 일정하게 유지됩니다 (약 1.0507), 작지만 유용한 기울기를 증가하여 사그라들기 기울기를 희석시키기 위해 사용됩니다.\n- 부정적 영역: 도함수는 0과 λα 사이의 값 사이를 이동합니다 (일반적인 값은 λ ≈ 1.0507 그리고 α ≈ 1.6733), 약 1.7583에 달하는 최대 도함수를 이끌어 냅니다. 여기서 함수는 거의 0에 가깝게 접근하며, 지나치게 큰 기울기를 줄여 폭발 문제를 해결하기 위해 돕습니다.\n\n이 설계는 이러한 활성화 함수들이 유용한 신호를 증가시키면서 잠재적으로 유해한 극단을 억제해 안정적인 학습 환경을 제공할 수 있도록 균형을 맞춘다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n활성화 함수가 게이트로 작용하는 개념은 새로운 아이디어가 아닙니다. 시그모이드 함수가 무엇을 기억하거나 업데이트하거나 잊어버릴지를 결정하는 LSTM과 같은 구조에서 강력한 전례가 있습니다. 이 게이팅 개념은 ReLU의 변형이 특정한 방식으로 설계된 이유를 이해하는 데 도움이 됩니다. 예를 들어 GELU는 표준 정규 분포의 누적 분포 함수(CDF)에서 유도된 스케일 계수를 사용하는 동적 게이트 역할을 합니다. 이 스케일링을 통해 입력의 작은 부분이 0에 가까울 때 통과되도록 하고, 더 큰 양수 값은 대부분 변경되지 않고 통과할 수 있게 합니다. 입력이 다음 레이어에 얼마나 많은 영향을 미치는지 제어함으로써, GELU는 정보 흐름의 효과적인 관리를 용이하게 해주며, 특히 transformer와 같은 구조에서 유용합니다.\n\n언급된 ELU, SELU, 그리고 GELU 모두 음수 측면을 부드럽게 만듭니다. 음수 입력의 부드러운 포화는 큰 음수 값의 영향을 완화하는 것뿐만 아니라, 네트워크가 입력 데이터의 변동에 덜 민감해지도록 만듭니다. 이를 통해 더 안정적인 특징 표현이 이뤄지게 됩니다.\n\n요약하면, 양수인지 음수인지에 상관없이 포화 영역이 구체적으로 중요하지 않습니다. 왜냐하면 이러한 활성화 함수들은 네트워크의 중간 레이어에서 작동하며, 여기서 가중치와 편향이 적절하게 조정될 수 있습니다. 하지만, 한쪽이 신호를 변경하지 않고 전달하거나 심지어 증폭할 수 있도록 허용하는 이러한 함수의 설계가 중요합니다. 이러한 배치는 신호를 조직화하고 효과적인 역전파를 용이하게 도와 전체 네트워크의 성능과 학습 안정성을 향상시킵니다.\n\n## 언제 각 활성화 함수를 선택해야 할까요? 왜 ReLU가 여전히 실무에서 가장 인기 있는 활성화 함수인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n올바른 활성화 함수를 선택하는 데는 계산 리소스, 네트워크 아키텍처의 특정 요구 사항 및 이전 모델로부터의 경험적 증거 등 여러 요소가 관련됩니다.\n\n- 계산 리소스: 충분한 계산 리소스가 있다면 교차 검증을 사용하여 다양한 활성화 함수를 실험해 보는 것이 유익할 수 있습니다. 이를 통해 모델과 데이터셋에 특화된 활성화 함수를 만들 수 있습니다. SELU를 사용할 때 배치 정규화가 필요 없는 경우가 대부분이며, 이는 다른 함수들과 달리 배치 정규화가 필요하지 않아 아키텍처를 간단하게 만들어 줍니다.\n- 경험적 증거: 특정 응용 프로그램에는 특정 함수가 표준으로 사용될 수 있습니다. 예를 들어, 트랜스포머 모델을 훈련시키기 위해 GELU를 선호하는 경우가 많은데, 이는 해당 아키텍처에서 효과적이기 때문입니다. SELU는 자기 정규화 특성과 조절해야 할 하이퍼파라미터가 없다는 장점으로, 훈련 안정성이 핵심인 깊은 네트워크에 특히 유용합니다.\n- 계산 효율성과 간결성: 계산 효율성과 간결성이 중요한 경우, ReLU 및 PReLU, ELU와 같은 변형들이 우수한 선택지입니다. 이들은 매개변수 조정의 필요성을 피하고 모델의 희소성 및 일반화를 지원하여 과적합을 줄이는 데 도움을 줍니다.\n\n더 정교한 함수가 등장했지만, ReLU는 여전히 간결하고 효율적이어서 매우 인기가 있습니다. 구현이 간단하고 이해하기 쉬우며 계산을 복잡하게 하지 않고 비선형성을 소개하는 명확한 방법을 제공합니다. 음수 부분을 제로 처리하는 함수의 능력으로 계산을 단순화하고 계산 속도를 향상시키므로, 특히 대규모 네트워크에서 매우 유리합니다.\n\nReLU의 설계는 음수 활성화를 제로처리하여 모델의 희소성을 기본적으로 증가시키며, 이는 일반화를 개선할 수 있습니다 — 훈련 중심의 과적합이 심각한 문제인 딥 뉴럴 네트워크에서 매우 중요한 요소입니다. 게다가 ReLU는 추가적인 하이퍼파라미터가 필요 없으며, PReLU나 ELU와 같은 함수와 달리 모델 훈련에 추가 복잡성을 도입하지 않습니다. 또한 ReLU가 널리 채택된 상태이므로, 많은 머신러닝 프레임워크와 라이브러리가 이를 위해 특화된 최적화를 제공하여, 많은 개발자에게 실용적인 선택이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요약하자면, 새로운 활성화 함수는 특정 시나리오에 특정 이점을 제공하지만, ReLU의 간단함, 효율성, 효과적인 측면의 균형은 많은 응용 프로그램에서 선호하는 선택지가 되고 있습니다. 어떤 활성화 함수를 선택한다 하더라도, 그 특성을 철저히 이해하는 것이 중요하며 모델의 요구 사항과 일치하고 모델 훈련 중 문제 해결을 용이하게 하는 데 필수적입니다.\n\n# 가중치 초기화\n\n그래, 우리는 기욁할 기울기를 안정화시킬 완벽한 활성화 함수를 찾으려는 것을 그만두고, 가중치를 효율적으로 초기화하여 우리의 신경망을 올바르게 설정하는 다른 중요한 측면에 초점을 맞출 시간입니다.\n\n가중치 초기화에 대한 가장 인기 있는 방법들에 대해 자세히 살펴보기 전에, 기본적인 질문을 하나 다루어 보겠습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 가중치 초기화는 왜 중요하며 불안정한 그래디언트를 완화하는 데 어떻게 도움이 될까요?\n\n적절한 가중치 초기화는 모델 전체를 따라 정확하게 그래디언트가 흐를 수 있도록 보장합니다. 이는 아이스크림 공장에서 반제품이 전달되는 방식과 유사합니다. 초기 기계 설정이 올바른 것만 중요한 것이 아니라 각 부서가 효율적으로 작동하는 것도 중요합니다.\n\n가중치 초기화는 네트워크를 통해 전진 및 역방향으로 정보가 안정적으로 흐를 수 있도록 목표를 합니다. 너무 크거나 너무 작은 가중치는 문제를 일으킬 수 있습니다. 지나치게 큰 가중치는 전진 패스 중 출력을 지나치게 증가시켜 예측을 과대추정하게 할 수 있습니다. 반면 아주 작은 가중치는 출력을 지나치게 줄일 수 있습니다. 이러한 가중치의 크기는 역전파 중에 중요해집니다. 가중치가 너무 크면 그래디언트가 폭발할 수 있고, 너무 작으면 그래디언트가 사라질 수 있습니다. 이를 이해하여 우리는 출력 및 그래디언트를 무효화하는 영옵션 (zero)과 지나치게 높은 값과 같은 극단적인 초기화를 피합니다. 이 균형 잡힌 접근법은 네트워크의 효과성을 유지하고 불안정한 그래디언트와 관련된 문제를 방지하는 데 도움이 됩니다.\n\n## 가중치를 초기화하는 좋은 방법은 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가장 중요한 것은 최적 가중치 초기화는 이미 학습된 가중치를 사용하는 것이 가장 좋습니다. 이미 일부 학습을 거친 가중치를 얻을 수 있다면 손실을 최소화하는 방향으로 진행 중인 이 가중치를 계속 사용하는 것이 이상적입니다.\n\n그러나 처음부터 시작하는 경우 가중치를 초기화하는 방법을 신중하게 고려해야 합니다, 특히 불안정한 기울기를 방지하기 위해. 좋은 가중치 초기화에는 다음을 목표로 하는 것이 중요합니다:\n\n- 극단적인 값은 피해야 합니다. 이전에 논의했던 대로, 가중치는 너무 크거나 작지 않고 0도 아니어야 합니다. 적절히 조절된 가중치는 네트워크 훈련의 전진 및 역진행 중 안정성을 유지하는 데 도움이 됩니다.\n- 대칭을 깨야 합니다. 가중치가 다양한 행동을 하도록 하는 것은 매우 중요합니다. 이렇게 하면 뉴런이 서로 거울에 비친 행동을 하지 않고 동일한 특성만 학습하게 되는 것을 방지합니다. 이러한 차별이 없으면 네트워크가 복잡한 패턴을 모델링하는 능력이 심각하게 제한될 수 있습니다. 각각의 다른 초기 가중치가 각 뉴런이 데이터의 다른 측면을 학습하기 시작하도록 도와줍니다. 이는 아이스크림 공장의 다양한 종류의 생산 라인을 가지고 다양한 맛을 생산할 수 있는 범위를 확대하는 것과 비슷합니다.\n- 손실 표면에서 유리한 위치에 가중치를 배치해야 합니다. 초기 가중치는 모델이 글로벌 최솟값으로 향하는 여정을 더 쉽게 만들기 위해 손실 표면에서 양호한 시작 위치에 모델을 위치시켜야 합니다. 손실 랜드스케이프가 어떻게 보이는지 명확한 그림을 가지고 있지 않기 때문에 가중치 초기화에 약간의 무작위성을 도입하는 것이 유익할 수 있습니다.\n\n모든 가중치를 0으로 설정하는 것이 문제가 되는 이유입니다. 이는 모든 뉴런이 동일하게 행동하고 동일한 속도로 학습하기 때문에 대칭 문제를 발생시킵니다. 다양한 패턴을 효과적으로 포착하지 못하게되는 네트워크의 능력을 제한합니다. ReLU 및 그 변형과 함께 0 가중치는 출력이 0이 되어 학습이 멈추고 모든 뉴런이 비활성화되는 결과를 초래합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 모든 가중치를 작은 무작위 숫자로 초기화해야 하지 않을까요?\n\n가중치를 초기화할 때 작은 무작위 숫자를 사용하는 것은 도움이 될 수 있지만, 종종 충분한 제어가 없을 수 있습니다. 무작위로 할당된 가중치는 너무 작을 수 있어서 기욹기 소멸 문제로 이어질 수 있습니다. 이는 훈련 중 업데이트가 무의미하게 작아져 학습 과정이 정체될 수 있습니다. 또한, 완전히 무작위 초기화는 대칭을 깨는 것을 보장하지 않습니다. 예를 들어, 초기화된 값이 너무 유사하거나 모두 같은 부호를 가지는 경우, 뉴런들도 여전히 너무 유사하게 작동하여 데이터의 다양한 측면을 배우지 못할 수 있습니다.\n\n실무에서는 초기화에 대해 더 구조화된 방법을 사용합니다. 유명한 방법에는 Glorot (또는 Xavier) 초기화, He (또는 Kaiming) 초기화, LeCun 초기화 등이 있습니다. 이러한 기술은 일반적으로 정규 분포나 균일 분포를 기반으로 하지만, 이전 및 다음 레이어의 크기를 고려하여 균형을 제공하는 것으로 조절됩니다. 이는 기울기 소실 또는 폭발의 위험이 없이 효과적인 학습을 촉진합니다.\n\n## 그렇다면, 가중치 초기화에 표준 정규 분포(N(0,1))를 사용하지 않는 이유는 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n일반적인 정규 분포(N(0,1))를 사용하면 무작위화 과정을 어느 정도 제어할 수 있지만, 분산을 효과적으로 제어할 수 없어 최적 가중치 초기화에는 부족합니다. 제로 평균은 가중치가 모두 동일한 부호를 공유하지 않도록 보장하여 대칭을 깨는 데 효과적입니다. 그러나 분산이 1인 것은 문제가 될 수 있습니다.\n\n활성화 함수 입력 𝑍이 가중치에 의존하는 시나리오를 고려해 봅시다. 이전 레이어의 𝑁개 뉴런의 출력을 합산하여 계산된다고 가정해보면, 각각의 가중치는 표준 정규 분포에서 초기화됩니다. 여기서 𝑍도 평균이 0인 정규 분포를 따르지만, 분산은 𝑁이 됩니다. 예를 들어 𝑁=100인 경우, 𝑍의 분산은 100이 되어 너무 크기 때문에 활성화 함수로 입력이 불안정하게 전달되어 역전파 과정에서 그래디언트가 불안정해질 수 있습니다. 아이스크림 공장을 비유하면, 각 기계의 설정에서 오차 허용을 높게 설정하는 것은 품질 관리 부재로 인해 원하는 결과와 크게 벗어나는 최종 제품을 만드는 것과 같습니다.\n\n그렇다면 왜 𝑍의 분산에 신경을 쓸까요? 분산은 𝑍 값의 퍼짐을 제어합니다. 분산이 너무 작으면 𝑍의 출력이 충분히 다양하지 않아 대칭을 깨는 데 효과적이지 못할 수 있습니다. 그러나 너무 큰 분산은 값이 너무 높거나 낮아질 수 있습니다. 시그모이드와 같은 활성화 함수의 경우, 극단적으로 높거나 낮은 입력값은 함수의 포화 극으로 출력을 밀어 넣어 그래디언트 소실 문제를 야기할 수 있습니다.\n\n따라서, 분포에서 무작위로 가중치를 초기화할 때 평균과 분산 둘 다 중요합니다. 효과적으로 대칭을 깨기 위해 평균을 0으로 설정하고, 동시에 분산을 최소화하여 중간 제품(즉, 뉴런 출력)이 너무 크거나 작지 않도록 해야 합니다. 올바른 초기화는 네트워크를 통과하는 정보의 안정된 흐름을 보장하고, 전방 및 역방향으로 효율적인 학습 과정을 유지하며, 그래디언트에 불안전성을 도입하지 않습니다. 신중한 초기화 접근은 효과적이고 견고하게 학습하는 네트워크로 이어질 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 그래서, 신경망의 중간 층에서 출력 값을 제어하기 위해 연속된 층에도 입력으로 사용되는 가중치 초기화에 대해 신중히 선택한 평균과 분산을 사용합니다. 그렇다면, 가장 인기 있는 방법들이 어떻게 이 분산을 제어할 수 있는 걸까요?\n\n가중치를 초기화하는 가장 흔한 방법들을 살펴보기 전에, 𝑍Z의 분산은 가중치 초기화의 분산뿐만 아니라 𝑍Z를 계산하는 데 참여하는 뉴런의 수도 영향을 받는다는 점이 중요합니다. 만약 16개의 뉴런만 사용된다면, 𝑍Z의 분산은 16이 되고, 100개의 뉴런이 사용된다면 100이 됩니다. 이 변동은 가중치가 뽑히는 분포만이 아니라 계산에 기여하는 뉴런의 수, 즉 \"팬-인\"이라고도 불리는 요소에 의해 영향을 받습니다. \"팬-인\"은 뉴런으로 들어오는 입력 연결의 수를 의미하며, 비슷하게 \"팬-아웃\"은 뉴런이 가지는 출력 연결의 수를 나타냅니다.\n\n예시를 통해 설명해드리겠습니다: 신경망의 중간 층에 200개의 뉴런이 있고, 이전 층의 100개 뉴런 및 다음 층의 300개 뉴런과 연결되어 있다고 가정해봅시다. 이 경우, 이 층의 팬-인은 100이고, 팬-아웃은 300입니다.\n\n팬-인과 팬-아웃을 이용하면 가중치 초기화 중 분산을 제어할 수 있는 메커니즘을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 팬-인은 전방 전파 중 현재 레이어의 출력 𝑍의 분산을 조절하는 데 도움을 줍니다.\n- 팬-아웃은 역전파 중 후속 레이어의 가중치가 얼마나 영향을 미치는지 조정합니다.\n\n현재 레이어로 전방 및 역방향에서 공급되는 뉴런의 수를 고려하여, 연구자들은 다양한 초기화 방법들을 아이디어 위에 구축해냈습니다. Lecun, Xavier/Glorot 초기화 및 He/Kaiming 초기화가 이러한 방법들 중 일부입니다. 이러한 방법들의 아이디어는 꽤 유사합니다. 가중치를 생성할 때 균일 분포 또는 정규 분포 중 하나를 사용하고, 분산을 조절하기 위해 팬-인 또는 팬-아웃을 사용합니다. 이 분포들의 평균은 모두 0으로 설정하여 출력 값의 평균을 0으로 만듭니다.\n\n```js\n# 초기화의 다양한 유형\n\n| 초기화          | 활성화 함수             | σ² (정규)  |\n| -------------- | ----------------------------- | --------------- |\n| Xavier/Glorot  | None, tanh, logistic, softmax | 1 / 팬_평균 |\n| He/Kaiming     | ReLU 및 변형                  | 2 / 팬-인    |\n| LeCun          | SELU                          | 1 / 팬-인    |\n```\n\nLecun 초기화는 가중치 분포에 작은 분산을 사용하여 𝑍의 분산을 축소하는 것에 기반합니다. 𝑍의 분산이 팬-인과 각 가중치의 분산의 곱이라면, 𝑍가 분산이 1이 되도록 보장하려면 각 가중치의 분산은 1/팬-인이어야 합니다. 따라서 Lecun 초기화는 가중치를 𝑁(0,1/팬-인)에서 무작위로 선택합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n자비에/글로뤼 초기화는 이전 레이어의 가중치(fan-in)의 영향을 고려할 뿐만 아니라 역전파 중 이러한 가중치가 후속 레이어에 미치는 영향(fan-out)도 고려합니다. 순방향 및 역방향 전파 중 분산을 균형있게 유지하기 위해 분산에 대한 공식인 2/(fan_in + fan_out)을 사용하여 가중치를 그려놓을 수 있습니다. 이 때의 분산은 Normal 분포, N(0,2/(fan_in + fan_out)) 또는 Uniform 분포(- sqrt(6/ (fan_in + fan_out)), sqrt(6/ (fan_in + fan_out))) 중에서 선택할 수 있습니다.\n\n희/카이밍 초기화는 ReLU 및 그 변형에 특히 맞추어져 있습니다. ReLU는 음수 입력을 제로로 처리하므로 뉴런 활성화의 절반은 0이 아닌 것으로 예상되며, 이는 분산을 줄이고 그라디언트 소멸을 유발할 수 있습니다. 이에 대비하여 희 초기화는 Lecun 방법에서 사용된 분산을 두 배로 늘리는데, 이를 통해 ReLU를 사용하는 레이어에 필요한 균형을 유지합니다. Leaky ReLU 및 ELU의 경우 약간의 조정이 필요하지만(예: ELU의 경우 2 대신 1.55 배 사용), 원칙은 그라디언트를 안정화하기 위해 분산을 조정하고자 한다는 것입니다. 반면 SELU의 경우 자체 정규화 속성을 활용하기 위해 모든 숨겨진 레이어에 Lecun 초기화를 사용해야 합니다.\n\n이 토론은 PyTorch와 같은 프레임워크에서 가중치 초기화가 어떻게 구현되는지에 대한 흥미로운 측면을 엽니다. 이는 다음과 같은 질문으로 제시될 수 있습니다 —\n\n## PyTorch에서 가중치 초기화는 어떻게 구현되고, 그것이 특별한 이유는 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n파이토치에서는 선형 레이어의 가중치 초기화에 대한 기본 접근 방식이 르쿤 초기화 방법을 기반으로 합니다. 반면 케라스에서는 기본 초기화 기술로 제비어/글로럿 초기화가 사용됩니다.\n\n그러나 파이토치는 가중치 초기화에 대해 매우 유연한 접근 방식을 제공합니다. 사용자는 모델에서 사용된 다양한 활성화 함수의 특정 요구 사항과 일치하도록 프로세스를 세밀하게 조정할 수 있습니다. 이 세밀한 조정은 두 가지 주요 구성 요소를 고려하여 달성됩니다:\n\n- 모드: 이 구성 요소는 레이어의 입력 연결 수(fan-in) 또는 출력 연결 수(fan-out)에 따라 초기화된 가중치의 분산이 조정되는지를 결정합니다.\n- 게인: 이는 모델에서 사용된 활성화 함수에 따라 초기화된 가중치의 스케일을 조정하는 스케일링 계수입니다. 파이토치는 가중치 초기화 프로세스를 최적화하기 위해 맞춤형 게인 값을 계산하는 torch.nn.init.calculate_gain 함수를 제공합니다.\n\n가중치 초기화 매개변수를 사용자 정의하는 이 유연성을 통해 모델에서 사용된 특정 활성화 함수와 비교 가능하고 호환되는 초기화 접근 방식을 설정할 수 있습니다. 흥미로운 점은 파이토치의 가중치 초기화 구현이 서로 다른 초기화 방법 간의 어떤 관계를 나타낼 수 있는데, 이를 통해 신경망의 전반적인 기능을 향상시키기 위한 초기화 프로세스를 활용할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어, SELU 활성화 함수에 대한 PyTorch 문서를 검토하면 가중치 초기화의 흥미로운 측면을 발견할 수 있습니다. 문서에는 SELU 활성화와 함께 kaiming*normal 또는 kaiming_normal*을 사용하여 초기화할 때, nonlinearity=`selu` 대신 nonlinearity=`linear`을 선택해야 자가 정규화를 달성할 수 있다고 언급되어 있습니다. 이 세부 사항은 흥미로운데, PyTorch의 기본 Lecun 초기화가 Kaiming 방법을 선형 비선형성에서 gain이 1로 설정했을 때 Lecun 초기화 방법을 효과적으로 복제한다는 점을 강조합니다. 이는 Lecun 초기화가 보다 일반적인 Kaiming 초기화 접근법의 특정 응용이라는 것을 보여줍니다. 마찬가지로, Xavier 초기화 방법은 입력 연결의 수(fan-in)와 출력 연결의 수(fan-out)를 모두 고려하는 Lecun 초기화의 다른 변형으로 볼 수 있습니다.\n\n## 가중치를 분포로부터 초기화할 때 평균과 분산을 신중하게 선택해야 하는 점에 동의합니다. 그러나 왜 초기 가중치를 정규 분포 대신 균일 분포에서 추출하려고 하는지에 대한 이유는 여전히 명확하지 않습니다. 무엇 때문에 한 가지를 다른 것보다 선호하게 되는지 설명해주실 수 있나요?\n\n가중치를 초기화할 때 분포로부터 추출할 때 평균과 분산을 신중하게 선택하는 중요성에 대한 귀하의 주장은 타당합니다. 신경망에서 가중치를 초기화할 때 중요한 고려 사항 중 하나는 정규 분포나 균일 분포 중에서 추출할지 결정하는 것입니다. 명확한 연구 결과를 지지하는 답변이 없지만, 이러한 선택을 하는 이유에는 몇 가지 타당한 이유가 있습니다:\n\n균일 분포는 엔트로피가 가장 높은 분포로, 범위 내의 모든 값이 동등하게 가능성이 있습니다. 이 공정한 접근은 초기화에 어떤 값이 더 잘 작동할지에 대한 사전 지식이 부족할 때 유용할 수 있습니다. 각 잠정적인 가중치 값에 공정하게 대우하고 균일한 확률을 할당합니다. 이는 한정된 정보로 게임에서 모든 팀에 공평하게 건 게임과 비슷합니다 - 선호되는 결과의 가능성을 최대화합니다. 어떤 구체적인 값이 좋은 초기 가중치인지 알 수 없기 때문에 균일 분포를 사용하면 편향되지 않은 시작점을 보장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n한편, 정규 분포는 일반적으로 가중치를 0에 가까운 작은 값으로 초기화하는 것이 더 자주 일어납니다. 초기 가중치가 작을수록 출력의 분산이 줄어들고 학습 중 안정적인 기울기를 유지하는 데 도움이 되기 때문에 작은 초기 가중치가 일반적으로 선호됩니다. 이는 가중치 초기화 방법에서 단위 분산 대신 작은 분산을 선호하는 이유와 유사합니다. 게다가, 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수는 작은 초기 가중치 값에서 더 나은 성능을 발휘하며 이러한 활성화 함수가 숨겨진 레이어가 아닌 최종 출력 레이어에서만 사용되더라도 그렇습니다.\n\n근본적으로, 균일 분포는 사전 지식이 부족한 상황에서 공평한 시작점을 제공하여 모든 잠재적인 가중치 값들을 동등하게 가능성 있는 것으로 간주합니다. 반면 정규 분포는 0에 가까운 작은 초기 가중치를 선호하여 기울기 안정성을 돕고 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수와 잘 맞습니다. 이러한 분포 사이의 선택은 종종 다른 신경 아키텍처와 작업에 걸쳐 경험적인 결과에 따라 이루어집니다. 보편적으로 최적의 방법은 존재하지 않지만, 균일 및 정규 분포의 특성을 이해하면 더 많이 발견되고 문제에 특화된 초기화 결정을 할 수 있게 됩니다.\n\n## 우리는 편향 항에 대해서도 이러한 가중치 초기화 방법을 사용합니까? 편향 항을 어떻게 초기화합니까?\n\n좋은 질문입니다. 우리는 편향 항에 대해서는 가중치와 동일한 초기화 기술을 반드시 사용하지는 않습니다. 사실, 편향 값을 모두 간단히 0으로 초기화하는 것이 흔한 실천입니다. 그 이유는 가중치가 각 뉴런이 기본 데이터를 근사하는 함수의 모양을 결정하는 반면, 편향은 각 함수를 위아래로 이동시키는 오프셋 값을 제공하기 때문입니다. 그래서 편향은 가중치가 학습하는 전반적인 형태에 직접적으로 영향을 주지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n초기화의 주요 목표는 대칭을 깨고 가중치 학습에 좋은 시작점을 제공하는 것이므로 편향이 어떻게 초기화되는지에 대해 크게 걱정할 필요가 없습니다. 그들을 모두 0으로 설정하는 것이 일반적으로 충분합니다. 이에 대해 더 자세한 논의는 CS231n 강의 노트에서 찾아볼 수 있습니다.\n\n# 배치 정규화\n\n선택한 활성화 함수와 적절하게 초기화된 가중치로 신경망을 훈련 시작할 수 있습니다 (우리의 미니 아이스크림 공장 생산 라인을 가동시키는 것과 같습니다). 그러나 품질 통제가 필요합니다. 초기에는 물론 훈련 반복 중에도요. 두 가지 주요 기술은 특성 정규화와 배치 정규화입니다.\n\n이전 포스트에서 경사 하강법에 대해 논의한 것처럼, 이러한 기술은 빠른 수렴을 위해 손실 풍경을 재구성합니다. 특성 정규화는 초기 데이터 입력에 이를 적용하며, 배치 정규화는 에폭 사이에 숨겨진 레이어의 입력을 정규화합니다. 두 기술 모두 다른 단계에서 품질 보증 점검을 구현하는 것과 유사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n배치 정규화는 훈련 중에 각 레이어의 입력을 평균이 0이고 분산이 1인 값으로 정규화하여 내부 공변량 이동을 줄이어 경사 소실/폭발 문제를 완화하는 데 도움을 줍니다. 내부 이동이 발생하는 이유에 대해 생각해 보죠. 각 레이어의 매개변수를 기울기에 기반하여 업데이트하는 과정은 신경망의 각 레이어가 공장의 서로 다른 부서라고 생각할 수 있습니다. 한 부서의 매개변수(또는 설정)를 업데이트할 때마다 다음 부서의 입력이 변경됩니다. 이로 인해 각 레이어마다 새로운 변화에 대한 조정이 필요하며 이를 심층 학습에서 내부 공변량 이동이라고 합니다. 그렇다면 이러한 이동이 자주 발생할 때 어떻게 될까요? 네트워크가 안정화하기 어려워지며 각 레이어의 입력이 계속 변화함에 따라 문제가 발생합니다. 이는 공장의 한 부분에서 지속적인 변화가 제품 품질에 일관성 없이 영향을 미치는 것과 유사합니다. 이는 작업자들을 혼란스럽게 하고 작업 흐름을 망치는 결과를 초래할 수 있습니다.\n\n배치 정규화는 훈련 중 미니 배치 전체에서 각 레이어의 입력을 정규화하여 평균이 0이고 분산이 1인 값으로 설정하는 것을 목표로 합니다. 레이어가 예상할 수 있는 일관된, 통제된 입력 분포를 강요합니다. 공장 비유로 돌아가서, 다음 부서로 전달되기 전 각 부서의 출력에 엄격한 품질 기준을 설정하는 것과 유사합니다. 예를 들어, 베이킹 부서가 일관된 크기와 모양의 아이스크림콘을 생산해야 한다는 규칙을 설정하는 것입니다. 다음 장식 부서는 콘의 변화량을 고려할 필요가 없게 되며, 각 일반화된 콘에 동일한 양의 아이스크림을 추가할 수 있습니다.\n\n정규화를 통해 내부 공변량 이동을 줄이는 것으로 배치 정규화는 훈련 과정 중에 기울기가 엉망이 되는 것을 방지합니다. 레이어들이 신속히 변하는 입력 분포에 계속해서 재조정할 필요가 없어져서 기울기가 더 안정적으로 유지됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 정규화는 정규화자 역할을 하며 목적 함수 랜드스케이프를 부드럽게 만듭니다. 이를 통해 더 높은 학습 속도를 사용하여 수렴 속도를 높일 수 있습니다. 일반적으로 배치 정규화는 내부 분산 이동을 줄이고 그래디언트를 안정화시키며 목적함수를 정규화하고 훈련 가속화를 가능하게 합니다.\n\n## 배치 정규화를 어떻게 적용해야 하나요? 활성화 함수 이전 또는 이후에 적용해야 하나요? 훈련 및 테스트 중에 어떻게 처리해야 하나요?\n\n배치 정규화는 그래디언트를 안정화시키는 추가 레이어를 통해 DNN을 훈련하는 방식을 실제로 바꿨습니다. DL 영역에서 활성화 함수 이전 또는 이후에 적용해야 하는지에 대한 논쟁이 있습니다. 솔직히 말해서, 이는 모델에 따라 다르며 조금은 실험해 봐야 할 수도 있습니다. 그냥 방법을 일정하게 유지하도록 하고 변경하면 예상치 못한 문제가 발생할 수 있습니다.\n\n훈련 중에 배치 정규화 레이어는 각 미니 배치를 통해 각 차원에 대한 평균과 표준편차를 계산합니다. 이러한 통계량은 출력을 정규화하는 데 사용되어 평균이 0이고 분산이 1임을 보장합니다. 이 프로세스는 입력 분포를 표준 정규 분포로 변환하는 것으로 생각할 수 있습니다. 전체 훈련 데이터 세트를 사용하여 특징 정규화를 하는 것과는 달리 배치 정규화는 각 미니 배치에 기초하여 조정되어 처리되는 데이터에 동적이며 반응성을 가지게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제, 테스트는 다른 이야기입니다. 테스트 데이터에서 평균과 분산을 정규화에 사용하면 안 됩니다. 대신에 이러한 매개변수는 학습된 특징으로 간주되어 훈련 과정에서 유지되어야 합니다. 훈련 중 각 미니 배치는 고유의 평균과 분산을 가지지만, 일반적인 실천 방법은 이러한 값들의 이동 평균을 훈련 단계 동안 사용하는 것입니다. 이를 통해 안정된 추정값을 제공하여 테스트 중에 적용할 수 있게 됩니다. 다른 적은 일반적인 방법은 전체 훈련 데이터 세트를 사용하여 포괄적인 평균과 분산을 계산하는 추가 에포크를 실행하는 방법도 있습니다.\n\nPyTorch로 DNN 프레임워크로 훈련할 때, 조정 가능한 하이퍼파라미터인 γ와 β를 사용할 수 있습니다. 이러한 파라미터를 조정하여 배치 정규화 과정을 세밀하게 조정할 수 있습니다. 일반적으로 기본 설정은 매우 효과적입니다. 그러나 훈련 중에 PyTorch는 분산을 계산하기 위해 편향 추정량을 사용하지만, 테스트 중에 이동 평균을 위해 불편 추정량을 사용합니다. 이러한 조정은 모델이 미처 못 본 조건에서 인구 표준 편차를 더 정확하게 근사하고 모델의 신뢰성을 향상하는 데 도움이 됩니다.\n\n배치 정규화를 올바르게 적용하는 것은 네트워크에서 효율적인 학습에 중요합니다. 네트워크가 잘 학습하는 것뿐만 아니라 다양한 데이터 집합과 테스트 시나리오에서 성능을 유지할 수 있게 합니다. 생산 라인의 각 세그먼트를 정확하게 교정하여 운전을 원활하고 일관되게 유지하는 것으로 생각해보세요.\n\n## 왜 역전파 중에 그래디언트에 직접 배치 정규화를 적용하는 대신 순전파 중에 배치 정규화가 적용되나요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n일반적으로 순방향 패스 중 입력 또는 활성화에 배치 정규화를 적용하는 이유가 역전파 중에 기울기 자체에 직접 배치 정규화를 적용하는 것보다 더 일반적입니다.\n\n먼저, 기울기에 배치 정규화를 직접 적용하는 이점을 보여주는 실증적 증거나 실무가 부족합니다. 내부 공변량 이동의 개념은 주로 순방향 패스 중에 발생하며, 계층 입력의 분포가 매개변수 업데이트로 인해 변경됩니다. 따라서, 후속 계층에서 처리되기 전에 이러한 입력을 안정화시키기 위해 이 단계에서 배치 정규화를 적용하는 것이 합리적입니다. 또한, 기울기에 배치 정규화를 직접 적용하는 것은 기울기의 크기와 방향이 나르는 중요한 정보를 왜곡할 수 있습니다. 이는 내재적 의미를 변경하는 방식으로 고객 피드백을 변조하는 것과 유사하며, 이는 미니 아이스크림 공장의 제조 프로세스에 대한 향후 조정을 잘못 이끌 수 있습니다.\n\n그러나, 기울기를 경사 클리핑과 같은 마이너 조정을 하는 것은 일반적으로 허용되며 유익합니다. 이 기법은 기울기를 지나치게 크게 만들지 않고 안전한 범위 내에 유지하여 기울기를 제한하는 도구입니다. 이는 피드백에서 극단적 아웃라이어를 걸러내는 것과 유사하며, 이는 프로세스를 방해할 수 있는 급격한 반응을 방지하면서 전체 피드백의 무결성을 유지하는 데 도움이 됩니다. PyTorch에서는 기울기 노름을 모니터링하는 것이 일반적이며, 기울기가 폭발하기 시작하면 경사 클리핑과 같은 기법을 사용할 수 있습니다. PyTorch는 torch.nn.utils.clip*grad_norm* 및 torch.nn.utils.clip*grad_value*와 같은 함수를 제공하여 이를 관리할 수 있습니다.\n\n## 직접 정규화 대신 기울기를 클리핑하는 옵션을 언급했습니다. 왜 기울기를 클리핑하는 대신 바닥값을 설정하지 않는지 정확히 선택하는 이유가 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기울기 클리핑은 폭발하는 기울기 문제를 방지하는 데 도움이 되는 간단하면서도 효율적인 기술입니다. 종종 기울기의 최대값을 수동으로 제한합니다. 예를 들어 ReLU 활성화 함수는 상한값을 6으로 설정할 수 있으며, PyTorch에서는 ReLU6로 알려져 있습니다. 이 상한을 설정함으로써, 각 층에서 역전파 과정 중 기울기가 연쇄 법칙에 따라 곱해질 때 값이 지나치게 커지지 않도록 보장합니다. 이러한 클리핑은 기울기가 학습 과정을 방해할 정도로 급격하게 증가하는 것을 방지하여 그 값을 관리 가능한 한도 내에 유지합니다.\n\n한편, 기울기를 억제하는 것은 너무 작아지지 않도록 하기 위해 하한값을 설정하는 것입니다. 그러나 이는 사그라들어 가는 기울기 문제의 근본적인 해결책이 되지는 않습니다. 일부 활성화 함수인 시그모이드나 tanh 같은 경우 입력이 0에서 멀어질수록 기울기 값을 매우 심각하게 축소시키기 때문에 기울기의 사그라들음 문제가 발생합니다. 이는 학습 속도가 극도로 느려지거나 정체되는 매우 작은 기울기 값을 야기합니다. 기울기를 억제해도 해결되지 않는 이유는 문제의 근본이 활성화 함수의 성질에 기인하기 때문입니다. 즉, 단순히 값이 너무 작은데만 있지 않고 활성화 함수가 기울기 값을 압축하는 것에 있습니다. 따라서 사그라드는 기울기 문제를 효과적으로 해결하기 위해서는 네트워크 아키텍처나 활성화 함수 선택을 조정하는 것이 더 유익합니다. 기울기가 사그라들지 않도록 하는 활성화 함수 사용(ReLU같은), ResNet 아키텍처에서 볼 수 있는 스킵 연결 추가, LSTM이나 GRU 같은 RNN에서 게이트 메커니즘을 사용하는 등의 기술을 통해 기울기는 역전파 중 네트워크 전반에 걸쳐 더 건강한 흐름을 보장하여 자연스럽게 사그라드는 것을 방지할 수 있습니다.\n\n요약하면, 기울기 클리핑은 지나치게 큰 기울기를 효과적으로 관리하지만, 하한값을 설정하는 기울기 억제는 지나치게 작은 기울기 문제를 효과적으로 다루지 못합니다. 대신, 사그라드와 관련된 문제를 해결하려면 일반적으로 구조적인 조정이 필요합니다.\n\n#실무에서의 경험(개인 경험)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요약할 때, 모든 논의된 방법이 사라지는 그래디언트 문제와 폭주하는 그래디언트 문제를 해결하는 데 유용하다는 것은 명백합니다. 이들은 모두 모델의 학습 과정을 향상시킬 수 있는 실용적인 접근 방법입니다. 이 글을 마무리하며 한 가지 질문으로 마무리하고 싶습니다 -\n\n## 현실은 무엇인가요? 실무에서는 어떤 일반적인 과정이 있나요?\n\n실무에서 좋은 소식은 가능한 모든 해결책을 실험할 필요가 없다는 것입니다. 활성화 함수를 선택할 때, ReLU가 종종 선택되는 것이며 매우 비용 효율적입니다. ReLU는 양의 입력의 크기를 변경하지 않고 전달합니다 (시그모이드나 tanh는 큰 값을 크기와 관계없이 항상 1로 압축합니다) 그리고 계산 및 미분 측면에서 간단합니다. 주요 프레임워크에서 잘 지원되며 dead ReLU 문제를 우려한다면 Leaky ReLU, ELU, SELU, 또는 GELU와 같은 대안을 고려할 수 있지만 일반적으로 시그모이드와 tanh를 피해야 하는 사라지는 그래디언트 문제를 피하기 위해 명확을 지켜야 합니다.\n\n선호되는 활성화 함수인 ReLU로 인해 가중치 초기화가 지나치게 민감하게 작용하는 문제에 대해 덜 걱정해도 됩니다. 시그모이드, tanh 및 SELU와 같은 함수에서 주로 발생하는 문제일 뿐입니다. 대신, 선택한 활성화 함수에 권장되는 가중치 초기화 방법에 집중하는 것이 적당합니다 (예를 들어, ReLU에 대해 He/Kaiming 초기화를 사용하는 이유는 ReLU의 비선형성을 고려하기 때문입니다).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n네트워크에는 항상 배치 정규화를 포함하세요. 활성화 함수 전 또는 후에 적용할지 결정(또는 실험)하고, 모델 전체에서 일관되게 그 선택을 유지하세요. 배치 정규화는 규제 효과와 높은 학습률 사용이 가능해지는 등 여러 가지 이점을 제공합니다. 이는 교육 및 수렴 속도를 높일 수 있습니다.\n\n그래서 어떤 것을 실험해볼 가치가 있을까요? 옵티마이저는 탐구할 가치가 있습니다. 이전 글에서 그라디언트 디센트 및 그 인기 있는 변형 등 다양한 옵티마이저를 논의했습니다. Adam은 빠르지만 과적합을 유발하고 학습률을 너무 빨리 감소시킬 수 있습니다. SGD는 신뢰성이 있고 병렬 컴퓨팅 환경에서 특히 효과적일 수 있습니다. 느릴 수 있지만 모델로부터 최대 성능을 뽑아내려면 확실한 선택입니다. 때로는 RMSprop이 더 나은 대안일 수 있습니다. 저는 Adam으로 시작하여 속도를 이유로 한 후에 더 나은 최소값을 찾고 과적합을 방지하기 위해 후기 에포크에서 SGD로 전환하는 것이 좋은 전략으로 생각합니다.\n\n만약 이 시리즈를 즐기고 계시다면, 상호작용(박수, 댓글 및 팔로우)이 지지뿐만 아니라 시리즈를 이어가는 원동력이자 저의 계속된 공유를 영감받는 기반이 됩니다.\n\n이 시리즈의 다른 게시물:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- ML 학습의 용기: L1 및 L2 정규화 해독하기 (파트 1)\n- ML 학습의 용기: 우도, MLE 및 MAP 해독하기\n- ML 학습의 용기: F1, 재현율, 정밀도 및 ROC 곡선에 대한 심층 탐구\n- ML 학습의 용기: 가장 일반적인 손실 함수에 대한 상세 가이드\n- ML 학습의 용기: 경사 하강법과 인기 있는 옵티마이저에 대한 심층 탐구\n- ML 학습의 용기: 수학적 이론부터 코딩 실무까지 백프로파게이션 설명\n\n## 참고 자료\n\n활성화 함수\n\n- [가우시안 에러 선형 유닛 (GeLU) 설명](https://ml-explained.com/blog/activation-functions-explained#gaussian-error-linear-unit-gelu)\n- [ReLU 활성화 함수](https://www.mldawn.com/relu-activation-function/)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가중치 초기화\n\n- [normal glorot initialization(일반 글로럿 초기화)의 원천](https://datascience.stackexchange.com/questions/102036/where-does-the-normal-glorot-initialization-come-from)\n- [파이토치(PyTorch)에서의 기본 초기화에 대한 명확한 이해](https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/2)\n\n그래디언트 클리핑\n\n- [파이토치(PyTorch)에서 그래디언트 클리핑 하는 방법](https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch)\n","ogImage":{"url":"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png"},"coverImage":"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png","tag":["Tech"],"readingTime":39},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\"“Courage to Learn ML”의 새로운 장을 찾아주신 여러분, 환영합니다. 이 시리즈는 복잡한 주제들을 쉽고 재미있게 다루고, 멘토와 학습자 간의 캐주얼 대화처럼 친밀한 분위기를 제공하기 위해 만들어졌습니다. “용기로 방어하다”의 쓰기 스타일에서 영감을 받아 기계 학습에 특히 집중하고 있어요.\u003c/p\u003e\n\u003cp\u003e이번 시간에는 사라지는 그래디언트와 폭발하는 그래디언트의 어려움을 극복하는 방법에 대해 계속해서 탐구할 거예요. 첫 번째 세그먼트에서 우리는 네트워크 내에서 효율적인 학습을 보장하기 위해 안정적인 그래디언트 유지가 왜 중요한지에 대해 이야기했어요. 불안정한 그래디언트가 우리 네트워크의 심화를 방해할 수 있고 결국 깊은 \"학습\"의 잠재력을 제한할 수 있다는 것을 밝혀냈죠. 이러한 개념을 살려내기 위해 DNN(맛있고 영양가 있는 얹힌 작은 얼음 공장)이라는 소형 아이스크림 공장을 운영하는 비유를 사용하고 수렴한 팩토리 생산 라인을 조율하는 것과 유사한 DNN 훈련을 위한 강력한 전략을 명료하게 보여줬어요.\u003c/p\u003e\n\u003cp\u003e이제, 두 번째 이야기에서는 각 제안된 솔루션에 대해 더 심층적으로 탐구하며, 아이스크림 공장을 활기차게 만든 것과 같은 명확성과 창의성으로 그들을 살펴볼 거에요. 여기 이번 부분에서 다룰 주제 목록입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e활성화 함수\u003c/li\u003e\n\u003cli\u003e가중치 초기화\u003c/li\u003e\n\u003cli\u003e배치 정규화\u003c/li\u003e\n\u003cli\u003e실제 적용(개인 경험)\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e활성화 함수\u003c/h1\u003e\n\u003cp\u003e활성화 함수는 우리 \"공장\" 설정의 핵심입니다. 이 함수들은 우리의 DNN 조립 라인 내에서 전방 및 후방 전파를 통해 정보를 전달하는 역할을 합니다. 적절한 활성화 함수를 선택하는 것은 우리의 DNN 조립 라인 및 이에 따라 우리의 DNN 훈련 과정이 원활하게 작동하는 데 중요합니다. 이 부분은 활성화 함수의 장닿과 단점을 간단히 설명하는 것이 아닙니다. 여기서는 다양한 활성화 함수의 생성 배경을 파악하고 종종 간과되는 중요한 질문에 대답하기 위해 Q\u0026#x26;A 형식을 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e이러한 함수들을 우리 아이스크림 생산 비유의 블렌더로 생각해보세요. 이용 가능한 블렌더 목록을 제공하는 대신, 각각의 혁신과 특정 개선 사항 뒤에 있는 이유를 심층적으로 검토하고 이해하는 데 도움을 드리겠습니다.\u003c/p\u003e\n\u003ch2\u003e활성화 함수란 무엇이며, 어떻게 적절한 함수를 선택할 수 있을까요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png\" alt=\"Activation functions\"\u003e\u003c/p\u003e\n\u003cp\u003e활성화 함수는 신경망 모델에 선형 및 비선형 관계를 모두 포착할 수 있는 유연성과 강력함을 부여하는 주요 요소입니다. 로지스틱 회귀와 DNN의 주요 차이점은 이러한 활성화 함수들과 여러 층을 결합하는 데 있습니다. 이들은 NN이 다양한 함수를 근사할 수 있게 합니다. 그러나 이러한 능력은 도전과제와 함께 제공됩니다. 활성화 함수 선택에는 더 주의를 기울여야 합니다. 잘못된 선택은 모델이 특히 역전파 중에 효과적으로 학습하는 것을 막을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e당신이 당사 DNN 아이스크림 공장의 매니저로 상상해보세요. 당신은 생산 라인을 위해 적절한 활성화 함수(아이스크림 믹서로 생각해보세요)를 섬세하게 선택하고 싶을 것입니다. 즉, 당신의 요구 사항에 가장 적합한 것을 찾는 데 신중을 기울이고 최적의 선택지를 찾아내야 합니다.\u003c/p\u003e\n\u003cp\u003e따라서 효과적인 활성화 함수를 선택하는 첫 번째 단계는 두 가지 핵심 질문에 대한 대답을 찾는 것입니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e활성화 함수의 선택이 소멸 그래디언트나 폭발 그래디언트와 같은 문제에 어떤 영향을 미치나요? 어떤 기준이 좋은 활성화 함수를 정의하나요?\u003c/h2\u003e\n\u003cp\u003e은닉층에서 활성화 함수를 선택할 때, 주로 소멸 그래디언트와 관련된 문제가 발생합니다. 이는 전통적인 시그모이드 활성화 함수(가장 전통적이고 기본적인 모델)로 거슬러 올라갈 수 있습니다. 시그모이드 함수는 입력값을 확률 범위(0부터 1)에 매핑할 수 있는 능력으로 널리 사용되었습니다. 이는 이진 분류 작업에서 특히 유용합니다. 이 능력 덕분에 연구자들은 예측을 분류하기 위한 확률 임계값을 조정하여 모델의 유연성과 성능을 향상할 수 있었습니다.\u003c/p\u003e\n\u003cp\u003e그러나 이를 은닉층에 적용하는 것은 주로 소멸 그래디언트 문제를 야기했습니다. 이는 주로 두 가지 주요 요인으로 설명할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e순방향 전파 과정에서 시그모이드 함수는 입력을 0과 1 사이의 매우 좁은 범위로 압축합니다. 한 네트워크가 은닉층에서 활성화 함수로 시그모이드만 사용하는 경우, 여러 층을 거칠수록 이 범위가 더욱 좁아지게 됩니다. 이 압축 효과로 인해 출력의 변동성이 감소하고 양수 값으로의 편향이 도입됩니다. 입력 부호에 관계없이 출력은 0과 1 사이에 유지되기 때문에.\u003c/li\u003e\n\u003cli\u003e역전파 과정에서 시그모이드 함수의 도함수(종모양 곡선)는 0과 0.25 사이의 값을 생성합니다. 이 작은 범위는 입력을 가로지르는 그래디언트가 여러 층을 통과함에 따라 급속하게 감소할 수 있도록 할 수 있습니다. 이것은 앞선 층 그래디언트가 연속된층의 도함수의 곱으로 이루어지기 때문인데, 이러한 낮은 도함수의 복합 곱은 점점 더 작은 그래디언트를 결과로 가져와서 초기 층에서의 효과적인 학습을 방해합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이러한 제약 사항을 극복하기 위해 이상적인 활성화 함수는 다음과 같은 특성을 보여야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e비선형성. 네트워크가 복잡한 패턴을 포착할 수 있도록 함.\u003c/li\u003e\n\u003cli\u003e비포화. 함수와 그 도함수가 입력 범위를 과도하게 압축하지 않아서 gradient 소멸을 방지해야 함.\u003c/li\u003e\n\u003cli\u003e중심이 0인 출력. 함수는 양수 및 음수 출력 둘 다를 허용해야 하며, 각 노드 사이의 평균 출력이 특정 방향으로의 편향을 도입하지 않도록 해야 함.\u003c/li\u003e\n\u003cli\u003e계산 효율성. 함수와 그 도함수가 계산적으로 간단하여 효율적인 학습을 용이하게 해야 함.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e이러한 기본 특성들을 고려할 때, 인기있는 활성화 함수들이 기본 모델인 Sigmoid를 어떻게 개선하고 뛰어나게 만드는지 알아봅시다.\u003c/h2\u003e\n\u003cp\u003e이 섹션은 거의 모든 현재 활성화 함수에 대한 일반적인 개요를 제공하려고 합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e테이블 태그를 마크다운 형식으로 변경하세요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eReLU의 고려 사항 중 하나는 선형 세그먼트 간의 급격한 전환으로 인해 x=0에서 미분 불가능하다는 것입니다. 실제로 PyTorch와 같은 프레임워크는 subgradient 개념을 사용하여 이를 해결하며, 종종 x=0에서 도함수를 0.5 또는 [0, 1] 내의 다른 값으로 설정합니다. 이는 보통 정확한 제로 입력이 드물고 데이터의 변동성 때문에 문제가 되지 않습니다.\u003c/p\u003e\n\u003cp\u003e그래서, ReLU가 여러분에게 적합한 선택일까요? 많은 연구자들은 그렇다고 말합니다. 이는 그 간결함, 효율성 및 주요 DNN 프레임워크의 지원 덕분입니다. 게다가 \u003ca href=\"https://arxiv.org/abs/2310.04564\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://arxiv.org/abs/2310.04564\u003c/a\u003e 같은 최근 연구들이 ReLU의 계속된 중요성을 강조하며, ML 분야에서의 부활과 같은 시대를 맞이한다고 강조하고 있습니다.\u003c/p\u003e\n\u003cp\u003eLeaky ReLUs는 클래식적인 ReLU에 약간의 변화를 준겳이며, ReLU를 더 자세히 살펴보면 몇 가지 문제점이 드러납니다. 음수 입력에 대한 제로 출력으로 이어지는 것은 'dying ReLU' 문제로 이어지며, 뉴런들이 훈련 중 업데이트되지 않게 됩니다. 또한, ReLU는 양수 값을 선호하여 모델에 방향성 편향을 도입할 수 있습니다. 이러한 단점을 극복하면서 ReLU의 이점을 유지하기 위해, 여러 연구자들이 'leaky' ReLU와 같은 여러 변형을 개발했습니다.\u003c/p\u003e\n\u003cp\u003eLeaky ReLU는 ReLU의 음수 부분을 수정하여 작고 0이 아닌 기울기를 부여합니다. 이 조정은 음수 입력이 작은 음수 출력을 생성하도록하며, 효과적으로 그 외의 0 출력 영역을 '누출'시킵니다. 이 누출의 기울기는 하이퍼파라미터 알파(α)에 의해 제어되며, 전형적으로 뉴런을 활성 유지와 희소성 사이의 균형을 유지하기 위해 0에 가깝게 설정됩니다. 작은 음수 출력을 허용함으로써, Leaky ReLU는 활성 함수의 출력을 0 주변으로 중앙 집중시키고 뉴런이 비활성화되지 않게 하여 'dying ReLU' 문제에 대응합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그러나 하이퍼파라미터로 α를 도입하면 모델 튜닝에 대한 복잡성이 추가됩니다. 이를 관리하기 위해 원본 Leaky ReLU의 변형이 개발되었습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRandomized Leaky ReLU (RReLU): 이 버전은 훈련 중에 α를 지정된 범위 내에서 무작위로 지정하고 평가 중에는 고정합니다. 무작위성은 모델을 정규화하고 과적합을 방지하는 데 도움이 될 수 있습니다.\u003c/li\u003e\n\u003cli\u003eParametric Leaky ReLU (PReLU): PReLU는 훈련 중에 α를 학습할 수 있도록 하며, 활성화 함수를 데이터셋의 특정 요구에 맞게 조정할 수 있습니다. 이는 α를 훈련 데이터에 맞게 조정하여 모델 성능을 향상시킬 수 있지만, 과적합의 위험도 증가시킵니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLeaky ReLU를 개선한 Exponential Linear Unit (ELU). Leaky ReLU와 ELU는 음의 값을 허용하여 평균 유닛 활성화를 제로에 가깝게 밀어내고 활성화 함수의 활력을 유지하는 데 도움이 됩니다. Leaky ReLU의 문제점은 이 음의 값의 범위를 조절할 수 없다는 것입니다. 이론적으로 이 값들은 작게 유지하려는 의도에도 불구하고 음의 무한대로 확장될 수 있습니다. ELU는 이를 해결하기 위해 비선형 지수 곡선을 비정상적인 입력에 통합하여 음의 출력 범위를 최대 -𝛼(일반적으로 1로 설정되는 새로운 하이퍼파라미터)로 좁히고 제어합니다. 또한 ELU는 매끄러운 함수입니다. 그 지수 요소 덕분에 음과 양 값 사이에서 매끄러운 전환을 가능하게 하며, 입력 값에 대한 잘 정의된 기울기를 보장하여 기울기 기반 최적화에 유리합니다. 이 기능은 ReLU와 Leaky ReLU에서 보이는 미분 불가능 문제를 해결합니다.\u003c/p\u003e\n\u003cp\u003eSelf-Normalizing 속성을 갖춘 향상된 ELU인 Scaled Exponential Linear Unit (SELU). SELU는 신경망 내에서 제로 평균 및 단위 분산을 유지하도록 설계된 ELU의 확장된 버전입니다. 양의 순입력의 기울기가 1을 초과하도록 고정 스케일 요인 λ(1보다 큰 값)을 통합함으로써 SELU는 하위 레이어의 기울기가 줄어드는 상황에서 기울기를 증폭하여 딥 뉴럴 네트워크에서 자주 발생하는 소멸하는 기울기 문제를 예방하는 데 특히 유용합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eSELU에 대해, 매개변수(α 및 λ)는 고정된 값이며 학습할 수 없으므로 조정해야 할 매개변수가 적어 튜닝 과정이 간소화됩니다. SELU 구현에서 이러한 특정 값들을 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003eSELU는 실제로 활성화 함수 세계에서 정교한 \"믹서\"인데요, 특정 요구 사항이 딸려옵니다. 단방향 또는 순차 네트워크에서 가장 효과적이며 RNN, LSTM 또는 건너뛰기 연결을 갖는 아키텍처에서는 그 설계 때문에 그런만큼 성능이 좋지 않을 수 있습니다.\u003c/p\u003e\n\u003cp\u003eSELU의 자기 정규화 기능을 위해서는 입력 피처가 표준화되어야 합니다. 평균이 0이고 표준 편차가 1인 것이 중요합니다. 또한, 매 숨겨진 레이어의 가중치는 LeCun 정규 초기화를 사용하여 초기화되어야 합니다. 여기서 가중치는 평균이 0이고 분산이 1/fan_in인 정규 분포에서 샘플링됩니다. \"fan_in\"이란 용어가 익숙하지 않다면, 가중치 초기화에 대한 전용 세션에서 설명하겠습니다.\u003c/p\u003e\n\u003cp\u003e요약하면 SELU의 자기 정규화가 효과적으로 기능하려면 입력 피처가 정규화되고 네트워크 구조가 끊기지 않는 것을 보장해야 합니다. 이 일관성은 네트워크 전체에서 자기 정규화 효과가 유지되도록 도와주며 누출 없이 계속 유지되도록 합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eGELU (Gaussian Error Linear Unit)은 Dropout으로부터 규제 아이디어를 통합한 혁신적인 활성화 함수입니다. 기존 ReLU가 음수 입력에 대해 0을 출력하는 반면, leaky ReLU, ELU 및 SELU는 음수 출력을 허용합니다. 이를 통해 활성화의 평균을 0에 가깝게 이동시켜 편향을 줄이는데 도움을 줍니다. 이는 ReLU와 비슷한 방식으로 편향을 줄이지만 음수 입력을 완전히 0으로 만들지 않고 음의 값을 허용한다는 것을 의미합니다. 그러나 이러한 누출은 \"죽어 가는 ReLU\"의 일부 이점을 잃어버릴 수 있음을 의미합니다. 여기서는 일부 뉴런의 비활성으로 더 sparse하고 일반화된 모델을 얻을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e죽어 가는 ReLU 및 Dropout의 희소성 이점을 고려할 때, GELU는 여기에 한 발 더 나아간 것입니다. GELU는 0 출력의 특성을 가진 죽어 가는 ReLU를 무작위적인 요소와 결합하여 뉴런이 재활성화될 수 있는 가능성을 열어줍니다. 이 접근은 유익한 희소성을 유지하는 것뿐만 아니라 뉴런 활동을 재도입하여 GELU를 견고한 해결책으로 만듭니다. 이 메커니즘을 완전히 이해하기 위해 GELU의 정의를 자세히 살펴보겠습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eGELU 활성화 함수에서 CDF인 Φ(x) 또는 표준 가우스 누적 분포 함수가 중요한 역할을 합니다. 이 함수는 표준 정규 분포를 따를 때 x보다 작거나 같은 값을 갖는 것으로 나타내는 확률을 나타냅니다. Φ(x)는 음수 입력에 대해 0부터 양수 입력에 대해 1로 매끄럽게 전환되어, 입력의 스케일링을 효과적으로 제어합니다. Dan Hendrycks 외(출처)의 논문에 따르면 뉴런 입력은 배치 정규화를 사용할 때 특히 정규 분포를 따르는 경향이 있어 정규 분포의 사용이 정당화됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e해당 함수의 디자인은 x 값이 줄어들수록 입력이 더 자주 \"떨어지도록\" 허용하여 변환을 확률적이면서 입력 값에 의존적으로 만듭니다. 이 메커니즘은 흔히 사용되는 직선 함수인 f(x) = x를 더 부드럽게 만들어 ReLU 함수와 유사한 형태를 유지하며, 조각별 선형 함수에서 발생하는 갑작스러운 변화를 피합니다. GELU의 가장 중요한 특징 중 하나는 뉴런을 완전히 비활성화할 수 있다는 것으로, 이를 통해 입력 값의 변화에 따라 다시 활성화될 수 있습니다. 이러한 확률적 성질은 입력 값에 의존하지만 완전히 무작위적이지 않아 뉴런이 다시 활성화될 기회를 제공합니다.\u003c/p\u003e\n\u003cp\u003e아래는 GELU가 ReLU보다 두드러지는 이점이라고 요약할 수 있습니다. GELU는 어떤 입력 값이 양수인지 음수인지에 관계없이 전체 입력 값 범위를 고려합니다. Φ(x) 값이 감소함에 따라 GELU 함수의 출력이 0에 가까워지는 확률이 증가하여 뉴런을 부드럽게 \"떨어뜨리게\" 됩니다. 이 방법은 전형적인 드롭아웃 방식보다 더 정교하며, 무작위적으로 하는 것이 아니라 데이터에 따라 뉴런의 비활성화를 결정하도록 되어 있습니다. 이 방식은 매우 매력적으로 느껴지며, 마치 고급 디저트에 부드러운 크림을 추가하여 조금 더 향상된 미각을 경험하는 것과 같다고 생각합니다.\u003c/p\u003e\n\u003cp\u003eGELU는 GPT-3, BERT 및 다른 Transformers와 같은 모델에서 효율적이며 언어 처리 작업에서 강력한 성능을 보여 인기 있는 활성화 함수가 되었습니다. 확률적 성질 때문에 계산 위주이지만, 표준 가우스 누적 분포인 Φ(x)의 곡선은 시그모이드와 tanh 함수와 유사합니다. 흥미로운 점은 GELU가 tanh를 사용하거나 x(1.702*x) 공식을 사용하여 근사할 수 있다는 것입니다. 이러한 단순화 가능성에도 불구하고, PyTorch의 GELU 구현은 그러한 근사가 종종 불필요할 정도로 빠르게 진행됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e더 깊이 들어가기 전에 먼저 정리해보자면,\u003c/p\u003e\n\u003ch2\u003eReLU를 검토하고 이로부터 영감받은 다른 활성화 함수가 무엇인지를 살펴보면 어떤 활성화 함수가 좋을지 정확히 알아볼까요?\u003c/h2\u003e\n\u003cp\u003eGünter Klambauer 등의 논문에서 SELU가 소개된 적이 있습니다. 여기서는 효과적인 활성화 함수의 중요한 특성을 강조했는데요.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e범위: 네트워크 전체의 평균 활성화 수준을 조절하는 데 도움이 되기 위해 음수와 양수 값을 출력해야 합니다.\u003c/li\u003e\n\u003cli\u003e포화 영역: 도함수가 제로에 가까워지는 영역으로, 하위층의 너무 높은 분산을 안정화하는 데 도움을 줍니다.\u003c/li\u003e\n\u003cli\u003e증폭 슬로프: 하위 층에서 너무 낮은 분산을 높이기 위해 중요한 기울기가 있어야 합니다.\u003c/li\u003e\n\u003cli\u003e연속성: 연속적인 곡선은 분산의 변화를 안정화하고 증가시키는 효과를 균형있게 유지하는 고정점을 보장합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e또한 \"이상적인\" 활성화 함수에 대한 두 가지 추가 기준을 제안하고 싶습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e비선형성: 이것은 명백하고 필수적입니다. 왜냐하면 선형 함수는 복잡한 패턴을 효과적으로 모델링할 수 없기 때문입니다.\u003c/li\u003e\n\u003cli\u003e동적 출력: 출력이 제로이고 입력 데이터에 따라 출력을 변경할 수 있는 능력은 동적 뉴런 활성화와 비활성화를 가능하게 합니다. 이렇게 하면 네트워크가 변화하는 데이터 조건에 효율적으로 적응할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e활성화 함수가 음수를 출력하는 이유에 대해 더 직관적인 설명을 부탁드려도 될까요?\u003c/h2\u003e\n\u003cp\u003e활성화 함수를 입력 데이터를 변환하는 블렌더로 생각해 보세요. 일부 재료를 선호하는 블렌더처럼, 활성화 함수는 그들의 본질적인 특성에 따라 편향을 도입할 수 있습니다. 예를 들어, 시그모이드 및 ReLU 함수는 일반적으로 입력과 관계없이 비음수 출력만 나타냅니다. 이는 블렌더가 어떤 재료를 넣어도 항상 동일한 맛을 내는 것과 유사합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_3.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e이 편향을 최소화하려면 부정적이고 긍정적인 값을 출력할 수 있는 활성화 함수를 가지는 것이 좋습니다. 기본적으로 우리는 중심이 제로인 출력을 목표로 합니다. 활성화 함수의 출력을 나타내는 놀이터를 상상해보세요. Sigmoid나 ReLU와 같은 함수로는, 이 놀이터는 부정적인 입력을 무시하거나 제로로 바꾸기 때문에 긍정적인 쪽으로 크게 기울어져 있습니다. Leaky ReLU는 음수 입력이 약간 음수 출력을 생성하도록 허용함으로써 이 놀이터를 균형잡게 시도하지만, 부정적 기울기의 선형 및 상수적 성격 때문에 조정이 미미합니다. 반면에 Exponential Linear Unit (ELU)은 지수 구성 요소로 음수 측면에 더 다이나믹한 밀어넣기를 제공하여, 더 균형 잡힌 상태에 가까워질 수 있도록 돕습니다. 이 균형은 긍정적 및 부정적 업데이트가 훈련에 기여하도록 보장함으로써 신경망에서 건강한 그레이디언트 플로우와 효율적인 학습을 유지하는 데 중요합니다, 단방향 업데이트의 제한을 피하기 위해.\u003c/p\u003e\n\u003ch2\u003eReLU와 유사하게 양수 입력을 제로화하는 활성화 함수를 생성할 수 있을까요, min(0, x)를 사용하여 양수 입력을 제로화하는 함수를 선호하는 이유는 무엇인가요?\u003c/h2\u003e\n\u003cp\u003e확실히, ReLU의 양수 값을 제로화하고 음수 값을 그대로 통과시키는 버전을 설계할 수 있습니다. 이것은 기술적으로 실행 가능한데, 중요한 점은 여기서 값의 부호가 아니라 네트워크에 비선형성을 도입하는 것입니다. 이 활성화 함수들이 일반적으로 출력 레이어가 아닌 숨겨진 레이어에서 사용된다는 것을 기억하는 것이 중요합니다. 즉, 이 네트워크 내의 이러한 활성화 함수의 존재는 최종 출력의 부호에 영향을 미치지 않고 이 레이어의 특성에 의해 직접적으로 영향을 받지 않더라도 최종 출력이 여전히 양수와 음수 모두가 될 수 있다는 것을 의미합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e무슨 상황에서든, 네트워크의 가중치와 편향은 출력의 필요한 부호에 맞게 조정될 수 있습니다. 예를 들어, 전통적인 ReLU에서 출력이 1이고 다음 레이어의 가중치가 1이면 출력은 여전히 1로 유지됩니다. 마찬가지로, 제안된 ReLU 변형이 -1을 출력하고 가중치가 -1이면 결과는 여전히 1이 됩니다. 본질적으로, 우리는 출력의 부호보다는 크기에 더 신경을 씁니다.\u003c/p\u003e\n\u003cp\u003e따라서, ReLU가 부정적인 쪽에서 포화되는 것은 양수 쪽에서 포화되는 것과 근본적으로 다르지 않습니다. 그러나 우리가 영 중심 활성화 함수를 중요시하는 이유는 양수 또는 음수 값에 대한 내재적인 선호도를 방지하여 모델에서 불필요한 편향을 피하기 위한 것입니다. 이 균형은 네트워크 전체에 걸쳐 중립성과 효과적인 학습을 유지하는 데 도움이 됩니다.\u003c/p\u003e\n\u003ch2\u003eLeaky ReLU와 같은 함수들은 출력을 영 중심 주변에 유지하기 위해 음수값을 출력할 필요가 있습니다. 그렇다면 ELU, SELU, GELU는 왜 음수 입력에 포화되도록 특별히 설계되었을까요?\u003c/h2\u003e\n\u003cp\u003e이를 이해하기 위해, ReLU 뒤에 있는 생물학적 영감을 살펴볼 수 있습니다. ReLU는 생물학적 뉴런을 모방하는데, 이들은 한계값을 가지고 있습니다. 이 한계값을 초과하는 입력은 뉴런을 활성화시키고, 그 이하는 그렇지 않습니다. 활성 및 비활성 상태 간 전환 가능성은 신경 기능에서 중요합니다. ELU, SELU, GELU와 같은 변형을 고려할 때, 이들의 설계가 두 가지 다른 필요에 부합함을 알 수 있습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e긍정적 영역: 임계값을 초과하는 신호가 전달될 때 전달되는 원하는 신호를 전송하는 것을 의미합니다.\u003c/li\u003e\n\u003cli\u003e부정적 영역: 원치 않는 신호를 최소화하거나 걸러내며 대형 부정적 값의 영향을 완화하여 누수하는 게이트처럼 작동합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이러한 기능은 입력에 대한 게이트 역할을 하며, 뉴런의 출력에 영향을 미쳐야 하는 사항과 그렇지 말아야 하는 사항을 관리합니다. 예를 들어, SELU는 다음 두 가지 측면을 구분하여 활용합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e긍정적 영역: 스케일링 인자 λ (1보다 큼)는 신호를 전달하지 않을 뿐만 아니라 약간 증폭시킵니다. 역전파 중 이 영역의 도함수는 일정하게 유지됩니다 (약 1.0507), 작지만 유용한 기울기를 증가하여 사그라들기 기울기를 희석시키기 위해 사용됩니다.\u003c/li\u003e\n\u003cli\u003e부정적 영역: 도함수는 0과 λα 사이의 값 사이를 이동합니다 (일반적인 값은 λ ≈ 1.0507 그리고 α ≈ 1.6733), 약 1.7583에 달하는 최대 도함수를 이끌어 냅니다. 여기서 함수는 거의 0에 가깝게 접근하며, 지나치게 큰 기울기를 줄여 폭발 문제를 해결하기 위해 돕습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 설계는 이러한 활성화 함수들이 유용한 신호를 증가시키면서 잠재적으로 유해한 극단을 억제해 안정적인 학습 환경을 제공할 수 있도록 균형을 맞춘다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e활성화 함수가 게이트로 작용하는 개념은 새로운 아이디어가 아닙니다. 시그모이드 함수가 무엇을 기억하거나 업데이트하거나 잊어버릴지를 결정하는 LSTM과 같은 구조에서 강력한 전례가 있습니다. 이 게이팅 개념은 ReLU의 변형이 특정한 방식으로 설계된 이유를 이해하는 데 도움이 됩니다. 예를 들어 GELU는 표준 정규 분포의 누적 분포 함수(CDF)에서 유도된 스케일 계수를 사용하는 동적 게이트 역할을 합니다. 이 스케일링을 통해 입력의 작은 부분이 0에 가까울 때 통과되도록 하고, 더 큰 양수 값은 대부분 변경되지 않고 통과할 수 있게 합니다. 입력이 다음 레이어에 얼마나 많은 영향을 미치는지 제어함으로써, GELU는 정보 흐름의 효과적인 관리를 용이하게 해주며, 특히 transformer와 같은 구조에서 유용합니다.\u003c/p\u003e\n\u003cp\u003e언급된 ELU, SELU, 그리고 GELU 모두 음수 측면을 부드럽게 만듭니다. 음수 입력의 부드러운 포화는 큰 음수 값의 영향을 완화하는 것뿐만 아니라, 네트워크가 입력 데이터의 변동에 덜 민감해지도록 만듭니다. 이를 통해 더 안정적인 특징 표현이 이뤄지게 됩니다.\u003c/p\u003e\n\u003cp\u003e요약하면, 양수인지 음수인지에 상관없이 포화 영역이 구체적으로 중요하지 않습니다. 왜냐하면 이러한 활성화 함수들은 네트워크의 중간 레이어에서 작동하며, 여기서 가중치와 편향이 적절하게 조정될 수 있습니다. 하지만, 한쪽이 신호를 변경하지 않고 전달하거나 심지어 증폭할 수 있도록 허용하는 이러한 함수의 설계가 중요합니다. 이러한 배치는 신호를 조직화하고 효과적인 역전파를 용이하게 도와 전체 네트워크의 성능과 학습 안정성을 향상시킵니다.\u003c/p\u003e\n\u003ch2\u003e언제 각 활성화 함수를 선택해야 할까요? 왜 ReLU가 여전히 실무에서 가장 인기 있는 활성화 함수인가요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e올바른 활성화 함수를 선택하는 데는 계산 리소스, 네트워크 아키텍처의 특정 요구 사항 및 이전 모델로부터의 경험적 증거 등 여러 요소가 관련됩니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e계산 리소스: 충분한 계산 리소스가 있다면 교차 검증을 사용하여 다양한 활성화 함수를 실험해 보는 것이 유익할 수 있습니다. 이를 통해 모델과 데이터셋에 특화된 활성화 함수를 만들 수 있습니다. SELU를 사용할 때 배치 정규화가 필요 없는 경우가 대부분이며, 이는 다른 함수들과 달리 배치 정규화가 필요하지 않아 아키텍처를 간단하게 만들어 줍니다.\u003c/li\u003e\n\u003cli\u003e경험적 증거: 특정 응용 프로그램에는 특정 함수가 표준으로 사용될 수 있습니다. 예를 들어, 트랜스포머 모델을 훈련시키기 위해 GELU를 선호하는 경우가 많은데, 이는 해당 아키텍처에서 효과적이기 때문입니다. SELU는 자기 정규화 특성과 조절해야 할 하이퍼파라미터가 없다는 장점으로, 훈련 안정성이 핵심인 깊은 네트워크에 특히 유용합니다.\u003c/li\u003e\n\u003cli\u003e계산 효율성과 간결성: 계산 효율성과 간결성이 중요한 경우, ReLU 및 PReLU, ELU와 같은 변형들이 우수한 선택지입니다. 이들은 매개변수 조정의 필요성을 피하고 모델의 희소성 및 일반화를 지원하여 과적합을 줄이는 데 도움을 줍니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e더 정교한 함수가 등장했지만, ReLU는 여전히 간결하고 효율적이어서 매우 인기가 있습니다. 구현이 간단하고 이해하기 쉬우며 계산을 복잡하게 하지 않고 비선형성을 소개하는 명확한 방법을 제공합니다. 음수 부분을 제로 처리하는 함수의 능력으로 계산을 단순화하고 계산 속도를 향상시키므로, 특히 대규모 네트워크에서 매우 유리합니다.\u003c/p\u003e\n\u003cp\u003eReLU의 설계는 음수 활성화를 제로처리하여 모델의 희소성을 기본적으로 증가시키며, 이는 일반화를 개선할 수 있습니다 — 훈련 중심의 과적합이 심각한 문제인 딥 뉴럴 네트워크에서 매우 중요한 요소입니다. 게다가 ReLU는 추가적인 하이퍼파라미터가 필요 없으며, PReLU나 ELU와 같은 함수와 달리 모델 훈련에 추가 복잡성을 도입하지 않습니다. 또한 ReLU가 널리 채택된 상태이므로, 많은 머신러닝 프레임워크와 라이브러리가 이를 위해 특화된 최적화를 제공하여, 많은 개발자에게 실용적인 선택이 됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e요약하자면, 새로운 활성화 함수는 특정 시나리오에 특정 이점을 제공하지만, ReLU의 간단함, 효율성, 효과적인 측면의 균형은 많은 응용 프로그램에서 선호하는 선택지가 되고 있습니다. 어떤 활성화 함수를 선택한다 하더라도, 그 특성을 철저히 이해하는 것이 중요하며 모델의 요구 사항과 일치하고 모델 훈련 중 문제 해결을 용이하게 하는 데 필수적입니다.\u003c/p\u003e\n\u003ch1\u003e가중치 초기화\u003c/h1\u003e\n\u003cp\u003e그래, 우리는 기욁할 기울기를 안정화시킬 완벽한 활성화 함수를 찾으려는 것을 그만두고, 가중치를 효율적으로 초기화하여 우리의 신경망을 올바르게 설정하는 다른 중요한 측면에 초점을 맞출 시간입니다.\u003c/p\u003e\n\u003cp\u003e가중치 초기화에 대한 가장 인기 있는 방법들에 대해 자세히 살펴보기 전에, 기본적인 질문을 하나 다루어 보겠습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e가중치 초기화는 왜 중요하며 불안정한 그래디언트를 완화하는 데 어떻게 도움이 될까요?\u003c/h2\u003e\n\u003cp\u003e적절한 가중치 초기화는 모델 전체를 따라 정확하게 그래디언트가 흐를 수 있도록 보장합니다. 이는 아이스크림 공장에서 반제품이 전달되는 방식과 유사합니다. 초기 기계 설정이 올바른 것만 중요한 것이 아니라 각 부서가 효율적으로 작동하는 것도 중요합니다.\u003c/p\u003e\n\u003cp\u003e가중치 초기화는 네트워크를 통해 전진 및 역방향으로 정보가 안정적으로 흐를 수 있도록 목표를 합니다. 너무 크거나 너무 작은 가중치는 문제를 일으킬 수 있습니다. 지나치게 큰 가중치는 전진 패스 중 출력을 지나치게 증가시켜 예측을 과대추정하게 할 수 있습니다. 반면 아주 작은 가중치는 출력을 지나치게 줄일 수 있습니다. 이러한 가중치의 크기는 역전파 중에 중요해집니다. 가중치가 너무 크면 그래디언트가 폭발할 수 있고, 너무 작으면 그래디언트가 사라질 수 있습니다. 이를 이해하여 우리는 출력 및 그래디언트를 무효화하는 영옵션 (zero)과 지나치게 높은 값과 같은 극단적인 초기화를 피합니다. 이 균형 잡힌 접근법은 네트워크의 효과성을 유지하고 불안정한 그래디언트와 관련된 문제를 방지하는 데 도움이 됩니다.\u003c/p\u003e\n\u003ch2\u003e가중치를 초기화하는 좋은 방법은 무엇인가요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e가장 중요한 것은 최적 가중치 초기화는 이미 학습된 가중치를 사용하는 것이 가장 좋습니다. 이미 일부 학습을 거친 가중치를 얻을 수 있다면 손실을 최소화하는 방향으로 진행 중인 이 가중치를 계속 사용하는 것이 이상적입니다.\u003c/p\u003e\n\u003cp\u003e그러나 처음부터 시작하는 경우 가중치를 초기화하는 방법을 신중하게 고려해야 합니다, 특히 불안정한 기울기를 방지하기 위해. 좋은 가중치 초기화에는 다음을 목표로 하는 것이 중요합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e극단적인 값은 피해야 합니다. 이전에 논의했던 대로, 가중치는 너무 크거나 작지 않고 0도 아니어야 합니다. 적절히 조절된 가중치는 네트워크 훈련의 전진 및 역진행 중 안정성을 유지하는 데 도움이 됩니다.\u003c/li\u003e\n\u003cli\u003e대칭을 깨야 합니다. 가중치가 다양한 행동을 하도록 하는 것은 매우 중요합니다. 이렇게 하면 뉴런이 서로 거울에 비친 행동을 하지 않고 동일한 특성만 학습하게 되는 것을 방지합니다. 이러한 차별이 없으면 네트워크가 복잡한 패턴을 모델링하는 능력이 심각하게 제한될 수 있습니다. 각각의 다른 초기 가중치가 각 뉴런이 데이터의 다른 측면을 학습하기 시작하도록 도와줍니다. 이는 아이스크림 공장의 다양한 종류의 생산 라인을 가지고 다양한 맛을 생산할 수 있는 범위를 확대하는 것과 비슷합니다.\u003c/li\u003e\n\u003cli\u003e손실 표면에서 유리한 위치에 가중치를 배치해야 합니다. 초기 가중치는 모델이 글로벌 최솟값으로 향하는 여정을 더 쉽게 만들기 위해 손실 표면에서 양호한 시작 위치에 모델을 위치시켜야 합니다. 손실 랜드스케이프가 어떻게 보이는지 명확한 그림을 가지고 있지 않기 때문에 가중치 초기화에 약간의 무작위성을 도입하는 것이 유익할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e모든 가중치를 0으로 설정하는 것이 문제가 되는 이유입니다. 이는 모든 뉴런이 동일하게 행동하고 동일한 속도로 학습하기 때문에 대칭 문제를 발생시킵니다. 다양한 패턴을 효과적으로 포착하지 못하게되는 네트워크의 능력을 제한합니다. ReLU 및 그 변형과 함께 0 가중치는 출력이 0이 되어 학습이 멈추고 모든 뉴런이 비활성화되는 결과를 초래합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e모든 가중치를 작은 무작위 숫자로 초기화해야 하지 않을까요?\u003c/h2\u003e\n\u003cp\u003e가중치를 초기화할 때 작은 무작위 숫자를 사용하는 것은 도움이 될 수 있지만, 종종 충분한 제어가 없을 수 있습니다. 무작위로 할당된 가중치는 너무 작을 수 있어서 기욹기 소멸 문제로 이어질 수 있습니다. 이는 훈련 중 업데이트가 무의미하게 작아져 학습 과정이 정체될 수 있습니다. 또한, 완전히 무작위 초기화는 대칭을 깨는 것을 보장하지 않습니다. 예를 들어, 초기화된 값이 너무 유사하거나 모두 같은 부호를 가지는 경우, 뉴런들도 여전히 너무 유사하게 작동하여 데이터의 다양한 측면을 배우지 못할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e실무에서는 초기화에 대해 더 구조화된 방법을 사용합니다. 유명한 방법에는 Glorot (또는 Xavier) 초기화, He (또는 Kaiming) 초기화, LeCun 초기화 등이 있습니다. 이러한 기술은 일반적으로 정규 분포나 균일 분포를 기반으로 하지만, 이전 및 다음 레이어의 크기를 고려하여 균형을 제공하는 것으로 조절됩니다. 이는 기울기 소실 또는 폭발의 위험이 없이 효과적인 학습을 촉진합니다.\u003c/p\u003e\n\u003ch2\u003e그렇다면, 가중치 초기화에 표준 정규 분포(N(0,1))를 사용하지 않는 이유는 무엇인가요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e일반적인 정규 분포(N(0,1))를 사용하면 무작위화 과정을 어느 정도 제어할 수 있지만, 분산을 효과적으로 제어할 수 없어 최적 가중치 초기화에는 부족합니다. 제로 평균은 가중치가 모두 동일한 부호를 공유하지 않도록 보장하여 대칭을 깨는 데 효과적입니다. 그러나 분산이 1인 것은 문제가 될 수 있습니다.\u003c/p\u003e\n\u003cp\u003e활성화 함수 입력 𝑍이 가중치에 의존하는 시나리오를 고려해 봅시다. 이전 레이어의 𝑁개 뉴런의 출력을 합산하여 계산된다고 가정해보면, 각각의 가중치는 표준 정규 분포에서 초기화됩니다. 여기서 𝑍도 평균이 0인 정규 분포를 따르지만, 분산은 𝑁이 됩니다. 예를 들어 𝑁=100인 경우, 𝑍의 분산은 100이 되어 너무 크기 때문에 활성화 함수로 입력이 불안정하게 전달되어 역전파 과정에서 그래디언트가 불안정해질 수 있습니다. 아이스크림 공장을 비유하면, 각 기계의 설정에서 오차 허용을 높게 설정하는 것은 품질 관리 부재로 인해 원하는 결과와 크게 벗어나는 최종 제품을 만드는 것과 같습니다.\u003c/p\u003e\n\u003cp\u003e그렇다면 왜 𝑍의 분산에 신경을 쓸까요? 분산은 𝑍 값의 퍼짐을 제어합니다. 분산이 너무 작으면 𝑍의 출력이 충분히 다양하지 않아 대칭을 깨는 데 효과적이지 못할 수 있습니다. 그러나 너무 큰 분산은 값이 너무 높거나 낮아질 수 있습니다. 시그모이드와 같은 활성화 함수의 경우, 극단적으로 높거나 낮은 입력값은 함수의 포화 극으로 출력을 밀어 넣어 그래디언트 소실 문제를 야기할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e따라서, 분포에서 무작위로 가중치를 초기화할 때 평균과 분산 둘 다 중요합니다. 효과적으로 대칭을 깨기 위해 평균을 0으로 설정하고, 동시에 분산을 최소화하여 중간 제품(즉, 뉴런 출력)이 너무 크거나 작지 않도록 해야 합니다. 올바른 초기화는 네트워크를 통과하는 정보의 안정된 흐름을 보장하고, 전방 및 역방향으로 효율적인 학습 과정을 유지하며, 그래디언트에 불안전성을 도입하지 않습니다. 신중한 초기화 접근은 효과적이고 견고하게 학습하는 네트워크로 이어질 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e그래서, 신경망의 중간 층에서 출력 값을 제어하기 위해 연속된 층에도 입력으로 사용되는 가중치 초기화에 대해 신중히 선택한 평균과 분산을 사용합니다. 그렇다면, 가장 인기 있는 방법들이 어떻게 이 분산을 제어할 수 있는 걸까요?\u003c/h2\u003e\n\u003cp\u003e가중치를 초기화하는 가장 흔한 방법들을 살펴보기 전에, 𝑍Z의 분산은 가중치 초기화의 분산뿐만 아니라 𝑍Z를 계산하는 데 참여하는 뉴런의 수도 영향을 받는다는 점이 중요합니다. 만약 16개의 뉴런만 사용된다면, 𝑍Z의 분산은 16이 되고, 100개의 뉴런이 사용된다면 100이 됩니다. 이 변동은 가중치가 뽑히는 분포만이 아니라 계산에 기여하는 뉴런의 수, 즉 \"팬-인\"이라고도 불리는 요소에 의해 영향을 받습니다. \"팬-인\"은 뉴런으로 들어오는 입력 연결의 수를 의미하며, 비슷하게 \"팬-아웃\"은 뉴런이 가지는 출력 연결의 수를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e예시를 통해 설명해드리겠습니다: 신경망의 중간 층에 200개의 뉴런이 있고, 이전 층의 100개 뉴런 및 다음 층의 300개 뉴런과 연결되어 있다고 가정해봅시다. 이 경우, 이 층의 팬-인은 100이고, 팬-아웃은 300입니다.\u003c/p\u003e\n\u003cp\u003e팬-인과 팬-아웃을 이용하면 가중치 초기화 중 분산을 제어할 수 있는 메커니즘을 제공합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e팬-인은 전방 전파 중 현재 레이어의 출력 𝑍의 분산을 조절하는 데 도움을 줍니다.\u003c/li\u003e\n\u003cli\u003e팬-아웃은 역전파 중 후속 레이어의 가중치가 얼마나 영향을 미치는지 조정합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e현재 레이어로 전방 및 역방향에서 공급되는 뉴런의 수를 고려하여, 연구자들은 다양한 초기화 방법들을 아이디어 위에 구축해냈습니다. Lecun, Xavier/Glorot 초기화 및 He/Kaiming 초기화가 이러한 방법들 중 일부입니다. 이러한 방법들의 아이디어는 꽤 유사합니다. 가중치를 생성할 때 균일 분포 또는 정규 분포 중 하나를 사용하고, 분산을 조절하기 위해 팬-인 또는 팬-아웃을 사용합니다. 이 분포들의 평균은 모두 0으로 설정하여 출력 값의 평균을 0으로 만듭니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 초기화의 다양한 유형\n\n| 초기화          | 활성화 함수             | σ² (정규)  |\n| -------------- | ----------------------------- | --------------- |\n| \u003cspan class=\"hljs-title class_\"\u003eXavier\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003eGlorot\u003c/span\u003e  | \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e, tanh, logistic, softmax | \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / 팬_평균 |\n| \u003cspan class=\"hljs-title class_\"\u003eHe\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003eKaiming\u003c/span\u003e     | \u003cspan class=\"hljs-title class_\"\u003eReLU\u003c/span\u003e 및 변형                  | \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e / 팬-인    |\n| \u003cspan class=\"hljs-title class_\"\u003eLeCun\u003c/span\u003e          | \u003cspan class=\"hljs-variable constant_\"\u003eSELU\u003c/span\u003e                          | \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / 팬-인    |\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLecun 초기화는 가중치 분포에 작은 분산을 사용하여 𝑍의 분산을 축소하는 것에 기반합니다. 𝑍의 분산이 팬-인과 각 가중치의 분산의 곱이라면, 𝑍가 분산이 1이 되도록 보장하려면 각 가중치의 분산은 1/팬-인이어야 합니다. 따라서 Lecun 초기화는 가중치를 𝑁(0,1/팬-인)에서 무작위로 선택합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e자비에/글로뤼 초기화는 이전 레이어의 가중치(fan-in)의 영향을 고려할 뿐만 아니라 역전파 중 이러한 가중치가 후속 레이어에 미치는 영향(fan-out)도 고려합니다. 순방향 및 역방향 전파 중 분산을 균형있게 유지하기 위해 분산에 대한 공식인 2/(fan_in + fan_out)을 사용하여 가중치를 그려놓을 수 있습니다. 이 때의 분산은 Normal 분포, N(0,2/(fan_in + fan_out)) 또는 Uniform 분포(- sqrt(6/ (fan_in + fan_out)), sqrt(6/ (fan_in + fan_out))) 중에서 선택할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e희/카이밍 초기화는 ReLU 및 그 변형에 특히 맞추어져 있습니다. ReLU는 음수 입력을 제로로 처리하므로 뉴런 활성화의 절반은 0이 아닌 것으로 예상되며, 이는 분산을 줄이고 그라디언트 소멸을 유발할 수 있습니다. 이에 대비하여 희 초기화는 Lecun 방법에서 사용된 분산을 두 배로 늘리는데, 이를 통해 ReLU를 사용하는 레이어에 필요한 균형을 유지합니다. Leaky ReLU 및 ELU의 경우 약간의 조정이 필요하지만(예: ELU의 경우 2 대신 1.55 배 사용), 원칙은 그라디언트를 안정화하기 위해 분산을 조정하고자 한다는 것입니다. 반면 SELU의 경우 자체 정규화 속성을 활용하기 위해 모든 숨겨진 레이어에 Lecun 초기화를 사용해야 합니다.\u003c/p\u003e\n\u003cp\u003e이 토론은 PyTorch와 같은 프레임워크에서 가중치 초기화가 어떻게 구현되는지에 대한 흥미로운 측면을 엽니다. 이는 다음과 같은 질문으로 제시될 수 있습니다 —\u003c/p\u003e\n\u003ch2\u003ePyTorch에서 가중치 초기화는 어떻게 구현되고, 그것이 특별한 이유는 무엇인가요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e파이토치에서는 선형 레이어의 가중치 초기화에 대한 기본 접근 방식이 르쿤 초기화 방법을 기반으로 합니다. 반면 케라스에서는 기본 초기화 기술로 제비어/글로럿 초기화가 사용됩니다.\u003c/p\u003e\n\u003cp\u003e그러나 파이토치는 가중치 초기화에 대해 매우 유연한 접근 방식을 제공합니다. 사용자는 모델에서 사용된 다양한 활성화 함수의 특정 요구 사항과 일치하도록 프로세스를 세밀하게 조정할 수 있습니다. 이 세밀한 조정은 두 가지 주요 구성 요소를 고려하여 달성됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e모드: 이 구성 요소는 레이어의 입력 연결 수(fan-in) 또는 출력 연결 수(fan-out)에 따라 초기화된 가중치의 분산이 조정되는지를 결정합니다.\u003c/li\u003e\n\u003cli\u003e게인: 이는 모델에서 사용된 활성화 함수에 따라 초기화된 가중치의 스케일을 조정하는 스케일링 계수입니다. 파이토치는 가중치 초기화 프로세스를 최적화하기 위해 맞춤형 게인 값을 계산하는 torch.nn.init.calculate_gain 함수를 제공합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e가중치 초기화 매개변수를 사용자 정의하는 이 유연성을 통해 모델에서 사용된 특정 활성화 함수와 비교 가능하고 호환되는 초기화 접근 방식을 설정할 수 있습니다. 흥미로운 점은 파이토치의 가중치 초기화 구현이 서로 다른 초기화 방법 간의 어떤 관계를 나타낼 수 있는데, 이를 통해 신경망의 전반적인 기능을 향상시키기 위한 초기화 프로세스를 활용할 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e예를 들어, SELU 활성화 함수에 대한 PyTorch 문서를 검토하면 가중치 초기화의 흥미로운 측면을 발견할 수 있습니다. 문서에는 SELU 활성화와 함께 kaiming\u003cem\u003enormal 또는 kaiming_normal\u003c/em\u003e을 사용하여 초기화할 때, nonlinearity=\u003ccode\u003eselu\u003c/code\u003e 대신 nonlinearity=\u003ccode\u003elinear\u003c/code\u003e을 선택해야 자가 정규화를 달성할 수 있다고 언급되어 있습니다. 이 세부 사항은 흥미로운데, PyTorch의 기본 Lecun 초기화가 Kaiming 방법을 선형 비선형성에서 gain이 1로 설정했을 때 Lecun 초기화 방법을 효과적으로 복제한다는 점을 강조합니다. 이는 Lecun 초기화가 보다 일반적인 Kaiming 초기화 접근법의 특정 응용이라는 것을 보여줍니다. 마찬가지로, Xavier 초기화 방법은 입력 연결의 수(fan-in)와 출력 연결의 수(fan-out)를 모두 고려하는 Lecun 초기화의 다른 변형으로 볼 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e가중치를 분포로부터 초기화할 때 평균과 분산을 신중하게 선택해야 하는 점에 동의합니다. 그러나 왜 초기 가중치를 정규 분포 대신 균일 분포에서 추출하려고 하는지에 대한 이유는 여전히 명확하지 않습니다. 무엇 때문에 한 가지를 다른 것보다 선호하게 되는지 설명해주실 수 있나요?\u003c/h2\u003e\n\u003cp\u003e가중치를 초기화할 때 분포로부터 추출할 때 평균과 분산을 신중하게 선택하는 중요성에 대한 귀하의 주장은 타당합니다. 신경망에서 가중치를 초기화할 때 중요한 고려 사항 중 하나는 정규 분포나 균일 분포 중에서 추출할지 결정하는 것입니다. 명확한 연구 결과를 지지하는 답변이 없지만, 이러한 선택을 하는 이유에는 몇 가지 타당한 이유가 있습니다:\u003c/p\u003e\n\u003cp\u003e균일 분포는 엔트로피가 가장 높은 분포로, 범위 내의 모든 값이 동등하게 가능성이 있습니다. 이 공정한 접근은 초기화에 어떤 값이 더 잘 작동할지에 대한 사전 지식이 부족할 때 유용할 수 있습니다. 각 잠정적인 가중치 값에 공정하게 대우하고 균일한 확률을 할당합니다. 이는 한정된 정보로 게임에서 모든 팀에 공평하게 건 게임과 비슷합니다 - 선호되는 결과의 가능성을 최대화합니다. 어떤 구체적인 값이 좋은 초기 가중치인지 알 수 없기 때문에 균일 분포를 사용하면 편향되지 않은 시작점을 보장합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e한편, 정규 분포는 일반적으로 가중치를 0에 가까운 작은 값으로 초기화하는 것이 더 자주 일어납니다. 초기 가중치가 작을수록 출력의 분산이 줄어들고 학습 중 안정적인 기울기를 유지하는 데 도움이 되기 때문에 작은 초기 가중치가 일반적으로 선호됩니다. 이는 가중치 초기화 방법에서 단위 분산 대신 작은 분산을 선호하는 이유와 유사합니다. 게다가, 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수는 작은 초기 가중치 값에서 더 나은 성능을 발휘하며 이러한 활성화 함수가 숨겨진 레이어가 아닌 최종 출력 레이어에서만 사용되더라도 그렇습니다.\u003c/p\u003e\n\u003cp\u003e근본적으로, 균일 분포는 사전 지식이 부족한 상황에서 공평한 시작점을 제공하여 모든 잠재적인 가중치 값들을 동등하게 가능성 있는 것으로 간주합니다. 반면 정규 분포는 0에 가까운 작은 초기 가중치를 선호하여 기울기 안정성을 돕고 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수와 잘 맞습니다. 이러한 분포 사이의 선택은 종종 다른 신경 아키텍처와 작업에 걸쳐 경험적인 결과에 따라 이루어집니다. 보편적으로 최적의 방법은 존재하지 않지만, 균일 및 정규 분포의 특성을 이해하면 더 많이 발견되고 문제에 특화된 초기화 결정을 할 수 있게 됩니다.\u003c/p\u003e\n\u003ch2\u003e우리는 편향 항에 대해서도 이러한 가중치 초기화 방법을 사용합니까? 편향 항을 어떻게 초기화합니까?\u003c/h2\u003e\n\u003cp\u003e좋은 질문입니다. 우리는 편향 항에 대해서는 가중치와 동일한 초기화 기술을 반드시 사용하지는 않습니다. 사실, 편향 값을 모두 간단히 0으로 초기화하는 것이 흔한 실천입니다. 그 이유는 가중치가 각 뉴런이 기본 데이터를 근사하는 함수의 모양을 결정하는 반면, 편향은 각 함수를 위아래로 이동시키는 오프셋 값을 제공하기 때문입니다. 그래서 편향은 가중치가 학습하는 전반적인 형태에 직접적으로 영향을 주지 않습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e초기화의 주요 목표는 대칭을 깨고 가중치 학습에 좋은 시작점을 제공하는 것이므로 편향이 어떻게 초기화되는지에 대해 크게 걱정할 필요가 없습니다. 그들을 모두 0으로 설정하는 것이 일반적으로 충분합니다. 이에 대해 더 자세한 논의는 CS231n 강의 노트에서 찾아볼 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e배치 정규화\u003c/h1\u003e\n\u003cp\u003e선택한 활성화 함수와 적절하게 초기화된 가중치로 신경망을 훈련 시작할 수 있습니다 (우리의 미니 아이스크림 공장 생산 라인을 가동시키는 것과 같습니다). 그러나 품질 통제가 필요합니다. 초기에는 물론 훈련 반복 중에도요. 두 가지 주요 기술은 특성 정규화와 배치 정규화입니다.\u003c/p\u003e\n\u003cp\u003e이전 포스트에서 경사 하강법에 대해 논의한 것처럼, 이러한 기술은 빠른 수렴을 위해 손실 풍경을 재구성합니다. 특성 정규화는 초기 데이터 입력에 이를 적용하며, 배치 정규화는 에폭 사이에 숨겨진 레이어의 입력을 정규화합니다. 두 기술 모두 다른 단계에서 품질 보증 점검을 구현하는 것과 유사합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e배치 정규화는 훈련 중에 각 레이어의 입력을 평균이 0이고 분산이 1인 값으로 정규화하여 내부 공변량 이동을 줄이어 경사 소실/폭발 문제를 완화하는 데 도움을 줍니다. 내부 이동이 발생하는 이유에 대해 생각해 보죠. 각 레이어의 매개변수를 기울기에 기반하여 업데이트하는 과정은 신경망의 각 레이어가 공장의 서로 다른 부서라고 생각할 수 있습니다. 한 부서의 매개변수(또는 설정)를 업데이트할 때마다 다음 부서의 입력이 변경됩니다. 이로 인해 각 레이어마다 새로운 변화에 대한 조정이 필요하며 이를 심층 학습에서 내부 공변량 이동이라고 합니다. 그렇다면 이러한 이동이 자주 발생할 때 어떻게 될까요? 네트워크가 안정화하기 어려워지며 각 레이어의 입력이 계속 변화함에 따라 문제가 발생합니다. 이는 공장의 한 부분에서 지속적인 변화가 제품 품질에 일관성 없이 영향을 미치는 것과 유사합니다. 이는 작업자들을 혼란스럽게 하고 작업 흐름을 망치는 결과를 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e배치 정규화는 훈련 중 미니 배치 전체에서 각 레이어의 입력을 정규화하여 평균이 0이고 분산이 1인 값으로 설정하는 것을 목표로 합니다. 레이어가 예상할 수 있는 일관된, 통제된 입력 분포를 강요합니다. 공장 비유로 돌아가서, 다음 부서로 전달되기 전 각 부서의 출력에 엄격한 품질 기준을 설정하는 것과 유사합니다. 예를 들어, 베이킹 부서가 일관된 크기와 모양의 아이스크림콘을 생산해야 한다는 규칙을 설정하는 것입니다. 다음 장식 부서는 콘의 변화량을 고려할 필요가 없게 되며, 각 일반화된 콘에 동일한 양의 아이스크림을 추가할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e정규화를 통해 내부 공변량 이동을 줄이는 것으로 배치 정규화는 훈련 과정 중에 기울기가 엉망이 되는 것을 방지합니다. 레이어들이 신속히 변하는 입력 분포에 계속해서 재조정할 필요가 없어져서 기울기가 더 안정적으로 유지됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e또한 정규화는 정규화자 역할을 하며 목적 함수 랜드스케이프를 부드럽게 만듭니다. 이를 통해 더 높은 학습 속도를 사용하여 수렴 속도를 높일 수 있습니다. 일반적으로 배치 정규화는 내부 분산 이동을 줄이고 그래디언트를 안정화시키며 목적함수를 정규화하고 훈련 가속화를 가능하게 합니다.\u003c/p\u003e\n\u003ch2\u003e배치 정규화를 어떻게 적용해야 하나요? 활성화 함수 이전 또는 이후에 적용해야 하나요? 훈련 및 테스트 중에 어떻게 처리해야 하나요?\u003c/h2\u003e\n\u003cp\u003e배치 정규화는 그래디언트를 안정화시키는 추가 레이어를 통해 DNN을 훈련하는 방식을 실제로 바꿨습니다. DL 영역에서 활성화 함수 이전 또는 이후에 적용해야 하는지에 대한 논쟁이 있습니다. 솔직히 말해서, 이는 모델에 따라 다르며 조금은 실험해 봐야 할 수도 있습니다. 그냥 방법을 일정하게 유지하도록 하고 변경하면 예상치 못한 문제가 발생할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e훈련 중에 배치 정규화 레이어는 각 미니 배치를 통해 각 차원에 대한 평균과 표준편차를 계산합니다. 이러한 통계량은 출력을 정규화하는 데 사용되어 평균이 0이고 분산이 1임을 보장합니다. 이 프로세스는 입력 분포를 표준 정규 분포로 변환하는 것으로 생각할 수 있습니다. 전체 훈련 데이터 세트를 사용하여 특징 정규화를 하는 것과는 달리 배치 정규화는 각 미니 배치에 기초하여 조정되어 처리되는 데이터에 동적이며 반응성을 가지게 됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이제, 테스트는 다른 이야기입니다. 테스트 데이터에서 평균과 분산을 정규화에 사용하면 안 됩니다. 대신에 이러한 매개변수는 학습된 특징으로 간주되어 훈련 과정에서 유지되어야 합니다. 훈련 중 각 미니 배치는 고유의 평균과 분산을 가지지만, 일반적인 실천 방법은 이러한 값들의 이동 평균을 훈련 단계 동안 사용하는 것입니다. 이를 통해 안정된 추정값을 제공하여 테스트 중에 적용할 수 있게 됩니다. 다른 적은 일반적인 방법은 전체 훈련 데이터 세트를 사용하여 포괄적인 평균과 분산을 계산하는 추가 에포크를 실행하는 방법도 있습니다.\u003c/p\u003e\n\u003cp\u003ePyTorch로 DNN 프레임워크로 훈련할 때, 조정 가능한 하이퍼파라미터인 γ와 β를 사용할 수 있습니다. 이러한 파라미터를 조정하여 배치 정규화 과정을 세밀하게 조정할 수 있습니다. 일반적으로 기본 설정은 매우 효과적입니다. 그러나 훈련 중에 PyTorch는 분산을 계산하기 위해 편향 추정량을 사용하지만, 테스트 중에 이동 평균을 위해 불편 추정량을 사용합니다. 이러한 조정은 모델이 미처 못 본 조건에서 인구 표준 편차를 더 정확하게 근사하고 모델의 신뢰성을 향상하는 데 도움이 됩니다.\u003c/p\u003e\n\u003cp\u003e배치 정규화를 올바르게 적용하는 것은 네트워크에서 효율적인 학습에 중요합니다. 네트워크가 잘 학습하는 것뿐만 아니라 다양한 데이터 집합과 테스트 시나리오에서 성능을 유지할 수 있게 합니다. 생산 라인의 각 세그먼트를 정확하게 교정하여 운전을 원활하고 일관되게 유지하는 것으로 생각해보세요.\u003c/p\u003e\n\u003ch2\u003e왜 역전파 중에 그래디언트에 직접 배치 정규화를 적용하는 대신 순전파 중에 배치 정규화가 적용되나요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e일반적으로 순방향 패스 중 입력 또는 활성화에 배치 정규화를 적용하는 이유가 역전파 중에 기울기 자체에 직접 배치 정규화를 적용하는 것보다 더 일반적입니다.\u003c/p\u003e\n\u003cp\u003e먼저, 기울기에 배치 정규화를 직접 적용하는 이점을 보여주는 실증적 증거나 실무가 부족합니다. 내부 공변량 이동의 개념은 주로 순방향 패스 중에 발생하며, 계층 입력의 분포가 매개변수 업데이트로 인해 변경됩니다. 따라서, 후속 계층에서 처리되기 전에 이러한 입력을 안정화시키기 위해 이 단계에서 배치 정규화를 적용하는 것이 합리적입니다. 또한, 기울기에 배치 정규화를 직접 적용하는 것은 기울기의 크기와 방향이 나르는 중요한 정보를 왜곡할 수 있습니다. 이는 내재적 의미를 변경하는 방식으로 고객 피드백을 변조하는 것과 유사하며, 이는 미니 아이스크림 공장의 제조 프로세스에 대한 향후 조정을 잘못 이끌 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나, 기울기를 경사 클리핑과 같은 마이너 조정을 하는 것은 일반적으로 허용되며 유익합니다. 이 기법은 기울기를 지나치게 크게 만들지 않고 안전한 범위 내에 유지하여 기울기를 제한하는 도구입니다. 이는 피드백에서 극단적 아웃라이어를 걸러내는 것과 유사하며, 이는 프로세스를 방해할 수 있는 급격한 반응을 방지하면서 전체 피드백의 무결성을 유지하는 데 도움이 됩니다. PyTorch에서는 기울기 노름을 모니터링하는 것이 일반적이며, 기울기가 폭발하기 시작하면 경사 클리핑과 같은 기법을 사용할 수 있습니다. PyTorch는 torch.nn.utils.clip\u003cem\u003egrad_norm\u003c/em\u003e 및 torch.nn.utils.clip\u003cem\u003egrad_value\u003c/em\u003e와 같은 함수를 제공하여 이를 관리할 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e직접 정규화 대신 기울기를 클리핑하는 옵션을 언급했습니다. 왜 기울기를 클리핑하는 대신 바닥값을 설정하지 않는지 정확히 선택하는 이유가 무엇인가요?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e기울기 클리핑은 폭발하는 기울기 문제를 방지하는 데 도움이 되는 간단하면서도 효율적인 기술입니다. 종종 기울기의 최대값을 수동으로 제한합니다. 예를 들어 ReLU 활성화 함수는 상한값을 6으로 설정할 수 있으며, PyTorch에서는 ReLU6로 알려져 있습니다. 이 상한을 설정함으로써, 각 층에서 역전파 과정 중 기울기가 연쇄 법칙에 따라 곱해질 때 값이 지나치게 커지지 않도록 보장합니다. 이러한 클리핑은 기울기가 학습 과정을 방해할 정도로 급격하게 증가하는 것을 방지하여 그 값을 관리 가능한 한도 내에 유지합니다.\u003c/p\u003e\n\u003cp\u003e한편, 기울기를 억제하는 것은 너무 작아지지 않도록 하기 위해 하한값을 설정하는 것입니다. 그러나 이는 사그라들어 가는 기울기 문제의 근본적인 해결책이 되지는 않습니다. 일부 활성화 함수인 시그모이드나 tanh 같은 경우 입력이 0에서 멀어질수록 기울기 값을 매우 심각하게 축소시키기 때문에 기울기의 사그라들음 문제가 발생합니다. 이는 학습 속도가 극도로 느려지거나 정체되는 매우 작은 기울기 값을 야기합니다. 기울기를 억제해도 해결되지 않는 이유는 문제의 근본이 활성화 함수의 성질에 기인하기 때문입니다. 즉, 단순히 값이 너무 작은데만 있지 않고 활성화 함수가 기울기 값을 압축하는 것에 있습니다. 따라서 사그라드는 기울기 문제를 효과적으로 해결하기 위해서는 네트워크 아키텍처나 활성화 함수 선택을 조정하는 것이 더 유익합니다. 기울기가 사그라들지 않도록 하는 활성화 함수 사용(ReLU같은), ResNet 아키텍처에서 볼 수 있는 스킵 연결 추가, LSTM이나 GRU 같은 RNN에서 게이트 메커니즘을 사용하는 등의 기술을 통해 기울기는 역전파 중 네트워크 전반에 걸쳐 더 건강한 흐름을 보장하여 자연스럽게 사그라드는 것을 방지할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e요약하면, 기울기 클리핑은 지나치게 큰 기울기를 효과적으로 관리하지만, 하한값을 설정하는 기울기 억제는 지나치게 작은 기울기 문제를 효과적으로 다루지 못합니다. 대신, 사그라드와 관련된 문제를 해결하려면 일반적으로 구조적인 조정이 필요합니다.\u003c/p\u003e\n\u003cp\u003e#실무에서의 경험(개인 경험)\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e요약할 때, 모든 논의된 방법이 사라지는 그래디언트 문제와 폭주하는 그래디언트 문제를 해결하는 데 유용하다는 것은 명백합니다. 이들은 모두 모델의 학습 과정을 향상시킬 수 있는 실용적인 접근 방법입니다. 이 글을 마무리하며 한 가지 질문으로 마무리하고 싶습니다 -\u003c/p\u003e\n\u003ch2\u003e현실은 무엇인가요? 실무에서는 어떤 일반적인 과정이 있나요?\u003c/h2\u003e\n\u003cp\u003e실무에서 좋은 소식은 가능한 모든 해결책을 실험할 필요가 없다는 것입니다. 활성화 함수를 선택할 때, ReLU가 종종 선택되는 것이며 매우 비용 효율적입니다. ReLU는 양의 입력의 크기를 변경하지 않고 전달합니다 (시그모이드나 tanh는 큰 값을 크기와 관계없이 항상 1로 압축합니다) 그리고 계산 및 미분 측면에서 간단합니다. 주요 프레임워크에서 잘 지원되며 dead ReLU 문제를 우려한다면 Leaky ReLU, ELU, SELU, 또는 GELU와 같은 대안을 고려할 수 있지만 일반적으로 시그모이드와 tanh를 피해야 하는 사라지는 그래디언트 문제를 피하기 위해 명확을 지켜야 합니다.\u003c/p\u003e\n\u003cp\u003e선호되는 활성화 함수인 ReLU로 인해 가중치 초기화가 지나치게 민감하게 작용하는 문제에 대해 덜 걱정해도 됩니다. 시그모이드, tanh 및 SELU와 같은 함수에서 주로 발생하는 문제일 뿐입니다. 대신, 선택한 활성화 함수에 권장되는 가중치 초기화 방법에 집중하는 것이 적당합니다 (예를 들어, ReLU에 대해 He/Kaiming 초기화를 사용하는 이유는 ReLU의 비선형성을 고려하기 때문입니다).\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e네트워크에는 항상 배치 정규화를 포함하세요. 활성화 함수 전 또는 후에 적용할지 결정(또는 실험)하고, 모델 전체에서 일관되게 그 선택을 유지하세요. 배치 정규화는 규제 효과와 높은 학습률 사용이 가능해지는 등 여러 가지 이점을 제공합니다. 이는 교육 및 수렴 속도를 높일 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그래서 어떤 것을 실험해볼 가치가 있을까요? 옵티마이저는 탐구할 가치가 있습니다. 이전 글에서 그라디언트 디센트 및 그 인기 있는 변형 등 다양한 옵티마이저를 논의했습니다. Adam은 빠르지만 과적합을 유발하고 학습률을 너무 빨리 감소시킬 수 있습니다. SGD는 신뢰성이 있고 병렬 컴퓨팅 환경에서 특히 효과적일 수 있습니다. 느릴 수 있지만 모델로부터 최대 성능을 뽑아내려면 확실한 선택입니다. 때로는 RMSprop이 더 나은 대안일 수 있습니다. 저는 Adam으로 시작하여 속도를 이유로 한 후에 더 나은 최소값을 찾고 과적합을 방지하기 위해 후기 에포크에서 SGD로 전환하는 것이 좋은 전략으로 생각합니다.\u003c/p\u003e\n\u003cp\u003e만약 이 시리즈를 즐기고 계시다면, 상호작용(박수, 댓글 및 팔로우)이 지지뿐만 아니라 시리즈를 이어가는 원동력이자 저의 계속된 공유를 영감받는 기반이 됩니다.\u003c/p\u003e\n\u003cp\u003e이 시리즈의 다른 게시물:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003eML 학습의 용기: L1 및 L2 정규화 해독하기 (파트 1)\u003c/li\u003e\n\u003cli\u003eML 학습의 용기: 우도, MLE 및 MAP 해독하기\u003c/li\u003e\n\u003cli\u003eML 학습의 용기: F1, 재현율, 정밀도 및 ROC 곡선에 대한 심층 탐구\u003c/li\u003e\n\u003cli\u003eML 학습의 용기: 가장 일반적인 손실 함수에 대한 상세 가이드\u003c/li\u003e\n\u003cli\u003eML 학습의 용기: 경사 하강법과 인기 있는 옵티마이저에 대한 심층 탐구\u003c/li\u003e\n\u003cli\u003eML 학습의 용기: 수학적 이론부터 코딩 실무까지 백프로파게이션 설명\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e참고 자료\u003c/h2\u003e\n\u003cp\u003e활성화 함수\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ml-explained.com/blog/activation-functions-explained#gaussian-error-linear-unit-gelu\" rel=\"nofollow\" target=\"_blank\"\u003e가우시안 에러 선형 유닛 (GeLU) 설명\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.mldawn.com/relu-activation-function/\" rel=\"nofollow\" target=\"_blank\"\u003eReLU 활성화 함수\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e가중치 초기화\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://datascience.stackexchange.com/questions/102036/where-does-the-normal-glorot-initialization-come-from\" rel=\"nofollow\" target=\"_blank\"\u003enormal glorot initialization(일반 글로럿 초기화)의 원천\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/2\" rel=\"nofollow\" target=\"_blank\"\u003e파이토치(PyTorch)에서의 기본 초기화에 대한 명확한 이해\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그래디언트 클리핑\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\" rel=\"nofollow\" target=\"_blank\"\u003e파이토치(PyTorch)에서 그래디언트 클리핑 하는 방법\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2"},"buildId":"wOkGEDZCvEs3S_XaNsdwr","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>