<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin" data-gatsby-head="true"/><meta name="twitter:title" content="프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 20:03" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_buildManifest.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">15<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><h2>LLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICA</h2>
<p>→ LLM Twin 무료 코스의 첫 번째 강의</p>
<p>당신의 LLM Twin은 무엇인가요? LLM Twin은 당신의 스타일, 성격 및 목소리를 포함하여 당신처럼 쓰는 AI 캐릭터입니다.</p>
<p><img src="/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png" alt="이미지"/></p>
<div class="content-ad"></div>
<h2>이 강의가 다른 이유는 무엇인가요?</h2>
<p>무료 강의 &quot;LLM Twin: Building Your Production-Ready AI Replica&quot;를 완료하면 LLMs, 벡터 DB 및 LLMOps의 좋은 실천법에 의해 구동되는 스스로의 프로덕션 준비 AI 복제본을 설계, 훈련 및 배포하는 방법을 배울 수 있습니다.</p>
<h2>이 강의를 통해 어떤 것을 배우게 되나요?</h2>
<p>데이터 수집부터 배포까지 실제 LLM 시스템을 설계하고 구축하는 방법을 배우실 수 있습니다.</p>
<div class="content-ad"></div>
<p>MLOps의 최상의 실천법을 활용하는 방법을 배울 것입니다. 실험 추적기, 모델 레지스트리, 즉시 모니터링, 그리고 버전 관리 등이 있습니다.</p>
<p>최종 목표는 무엇일까요? 자신만의 LLM 쌍을 구축하고 배포하는 것입니다.</p>
<p>LLM 쌍의 아키텍처는 4개의 파이썬 마이크로서비스로 나뉩니다:</p>
<ul>
<li>데이터 수집 파이프라인: 다양한 소셜 미디어 플랫폼에서 디지털 데이터를 수집합니다. ETL 파이프라인을 통해 데이터를 정리, 정규화하고 NoSQL DB에 로드합니다. CDC 패턴을 사용하여 데이터베이스 변경 사항을 큐로 전송합니다. (AWS에 배포됨)</li>
<li>피처 파이프라인: 바이트왁스 스트리밍 파이프라인을 통해 큐에서 메시지를 소비합니다. 각 메시지는 실시간으로 정리되고 청크화되며 Superlinked를 사용하여 삽입되고 Qdrant 벡터 DB에 로드됩니다. (AWS에 배포됨)</li>
<li>트레이닝 파이프라인: 디지털 데이터를 기반으로 사용자 정의 데이터세트를 생성합니다. QLoRA를 사용하여 LLM을 세밀하게 조정합니다. Comet ML의 실험 추적기를 사용하여 실험을 모니터링합니다. 최상의 모델을 Comet의 모델 레지스트리에 저장 및 평가합니다. (Qwak에 배포됨)</li>
<li>인퍼런스 파이프라인: Comet의 모델 레지스트리에서 세밀하게 조정된 LLM을 로드하고 양자화합니다. 이를 REST API로 배포합니다. RAG를 사용하여 프롬프트를 향상시키고, LLM 쌍을 사용하여 콘텐츠를 생성합니다. Comet의 프롬프트 모니터링 대시보드를 사용하여 LLM을 모니터링합니다. (Qwak에 배포됨)</li>
</ul>
<div class="content-ad"></div>
<p>4개의 마이크로서비스를 통해 3가지 서버리스 도구를 통합하는 방법을 배울 수 있습니다:</p>
<ul>
<li>ML 플랫폼으로서의 Comet ML;</li>
<li>벡터 DB로서의 Qdrant;</li>
<li>ML 인프라로서의 Qwak;</li>
</ul>
<h2>누구를 위한 것인가요?</h2>
<p>대상: MLE, DE, DS 또는 SWE로서, LLMOps의 좋은 원칙을 사용하여 제품 준비 상태의 LLM 시스템을 설계하고 싶은 분들을 대상으로 합니다.</p>
<div class="content-ad"></div>
<p>수준: 중급</p>
<p>필수 조건: Python, ML 및 클라우드에 대한 기본 지식</p>
<h2>어떻게 학습하시겠습니까?</h2>
<p>이 강의에는 11개의 실습 수업 및 GitHub에서 액세스할 수 있는 오픈 소스 코드가 포함되어 있습니다.</p>
<div class="content-ad"></div>
<p>모두 자신의 속도로 모든 것을 읽을 수 있어요.</p>
<p>→ 이 과정을 최대한 효율적으로 활용하려면 강의를 따라가며 저장소를 복제하고 실행하는 것을 권장해요.</p>
<h2>비용은?</h2>
<p>기사와 코드는 완전히 무료에요. 언제나 무료로 제공될 거에요.</p>
<div class="content-ad"></div>
<p>그러나 코드를 실행하면서 읽으려면, 추가 비용이 발생할 수 있는 몇 가지 클라우드 도구를 사용한다는 것을 알아두어야 합니다.</p>
<p>클라우드 컴퓨팅 플랫폼(AWS, Qwak)은 pay-as-you-go 요금제를 제공합니다. Qwak은 무료 컴퓨팅 시간을 제공합니다. 따라서 우리는 비용을 최소화하기 위해 최선을 다하였습니다.</p>
<p>다른 서버리스 도구(Qdrant, Comet)의 경우, 무료로 사용할 수 있는 프리미엄 버전을 사용할 것입니다.</p>
<h2>선생님들을 만나보세요!</h2>
<div class="content-ad"></div>
<p>Decoding ML 우산 아래에서 개설 된 이 과정을 개발 한 사람들은 다음과 같습니다:</p>
<ul>
<li>Paul Iusztin | 시니어 ML &amp; MLOps 엔지니어</li>
<li>Alex Vesa | 시니어 AI 엔지니어</li>
<li>Alex Razvant | 시니어 ML &amp; MLOps 엔지니어</li>
</ul>
<h1>수업</h1>
<p>이 과정은 총 11개의 수업으로 구성되어 있습니다. 매체 기사 하나당 하나의 수업으로 나뉩니다.</p>
<div class="content-ad"></div>
<ul>
<li>LLM 시스템의 제품용 엔드 투 엔드 프레임워크 구축을 통한 LLM Twin</li>
<li>생성 모델 AI 시대의 데이터 파이프라인의 중요성</li>
<li>변경 데이터 캡처: 이벤트 주도 아키텍처 가능</li>
<li>실시간으로 LLM 및 RAG의 파이썬 스트리밍 파이프라인의 뛰어난 솔루션</li>
<li>구현해야 할 4가지 고급 RAG 알고리즘</li>
<li>특징 저장소의 역할 LLM 세부 조정에</li>
<li>LLM 세부 조정 [모듈 3] ...작업 중</li>
<li>LLM 평가 [모듈 4] ...작업 중</li>
<li>양자화 [모듈 5] ...작업 중</li>
<li>디지털 트윈 추론 파이프라인 구축 [모듈 6] ...작업 중</li>
<li>REST API로 디지털 트윈 배포 [모듈 6] ...작업 중</li>
</ul>
<p>첫 번째 레슨에서는 당신이 수강 기간 동안 구축할 프로젝트인 제품용 LLM Twin/AI 복제품을 소개할 것입니다.</p>
<div class="content-ad"></div>
<p>이후에는 3-파이프라인 디자인이 무엇인지와 이것이 표준 ML 시스템에 어떻게 적용되는지 설명할 것입니다.</p>
<p>마지막으로, LLM 프로젝트 시스템 디자인에 대해 자세히 살펴볼 것입니다.</p>
<p>소셜 미디어 데이터 수집 파이프라인 디자인에 대한 모든 아키텍처 결정과 LLM 마이크로서비스에 3-파이프라인 아키텍처를 적용하는 방법을 설명할 것입니다.</p>
<p>다음 수업에서는 각 구성 요소의 코드를 검토하고 AWS 및 Qwak에 구현하고 배포하는 방법을 배울 것입니다.</p>
<div class="content-ad"></div>
<h2>목차</h2>
<ul>
<li>무엇을 구축할 계획인가요? LLM twin 개념</li>
<li>3-파이프라인 아키텍처</li>
<li>LLM twin 시스템 디자인</li>
</ul>
<h1>1. 무엇을 구축할 계획인가요? LLM twin 개념</h1>
<p>이 과정의 목표는 당신만의 AI 레플리카를 구축하는 것입니다. 우리는 그것을 할 수 있도록 LLM을 사용할 것이며, 따라서 이 과정의 이름이 LLM Twin: 생산 준비가 완료된 AI 레플리카 구축입니다.</p>
<div class="content-ad"></div>
<p>LLM 쌍이 무엇인지 알고 싶으신가요?</p>
<p>간단히 말씀드리면, LLM 쌍은 여러분과 비슷한 방식으로 글을 쓰는 인공지능 캐릭터가 될 거에요.</p>
<p>여러분 그 자신이 되는 게 아니라, 여러분의 글 스타일과 성격을 활용하는 쓰기 기계예요.</p>
<p>구체적으로 말하면, 여러분이 자신의 목소리로 소셜미디어 글이나 기술 기사(이렇게 작성된 것처럼)를 쓰는 AI 판본을 만드실 수 있을 거에요.</p>
<div class="content-ad"></div>
<p>ChatGPT을 직접 사용하지 않는 이유가 무엇인가요? 궁금하시다면…</p>
<p>LLM을 사용하여 기사나 글을 생성할 때 결과물이 다음과 같은 경향이 있습니다:</p>
<ul>
<li>매우 일반적이고 미흡하게 나옵니다.</li>
<li>허상으로 인한 잘못된 정보가 포함될 수 있습니다.</li>
<li>원하는 결과를 얻기 위해 번거로운 프롬프팅이 필요할 수 있습니다.</li>
</ul>
<p>하지만 이런 문제를 해결하기 위해 우리가 할 일은 ↓↓↓</p>
<div class="content-ad"></div>
<p>먼저, LinkedIn, Medium, Substack 및 GitHub에서 수집한 디지털 데이터로 LLM을 세밀하게 조정할 것입니다.</p>
<p>이를 통해 LLM은 당신의 쓰기 스타일과 온라인 개성과 일치하게 될 것입니다. LLM을 통해 당신 온라인 버전처럼 대화하는 법을 배울 것입니다.</p>
<p>2024년 Meta가 Messenger 앱에서 발표한 AI 캐릭터의 우주를 보신 적이 있나요? 만약 아직이라면, 여기 [2]에서 더 자세히 알아볼 수 있습니다.</p>
<p>어느 정도 그것이 우리가 구축하려는 것입니다.</p>
<div class="content-ad"></div>
<p>하지만 우리의 사용 사례에서는 당신의 목소리를 반영하고 표현하는 소셜 미디어 게시물이나 글을 쓰는 LLM 쌍에 초점을 맞추겠습니다.</p>
<p>예를 들어, 우리는 당신의 LLM 쌍에게 LLM에 관한 LinkedIn 게시물을 작성하도록 요청할 수 있습니다. LLM에 관한 어떤 일반적이고 표현되지 않은 게시물(예: ChatGPT가 무엇을 할 것인지) 대신에 당신의 목소리와 스타일을 사용할 것입니다.</p>
<p>두 번째로, 우리는 환각을 피하기 위해 외부 정보에 액세스하기 위해 LLM에게 벡터 DB에 액세스할 수 있게 할 것입니다. 따라서 LLM이 구체적인 데이터에 기반하여만 쓸 수 있도록 할 것입니다.</p>
<p>최종적으로, 정보를 얻기 위해 벡터 DB에 액세스하는 것 외에도 생성 프로세스의 기본 블록 역할을 하는 외부 링크를 제공할 수 있습니다.</p>
<div class="content-ad"></div>
<p>예를 들어, 위의 예시를 다음과 같이 수정해 볼 수 있어요: &quot;이 링크의 기사를 기반으로 LLMs에 관한 1000단어 LinkedIn 게시물을 작성해주세요: [URL].&quot;</p>
<p>기대되시나요? 시작해봅시다!🔥</p>
<h1>2. 3단계 파이프라인 구조</h1>
<p>우리 모두는 머신러닝 시스템이 얼마나 엉망이 될 수 있는지 알고 있어요. 이 때 3단계 파이프라인 구조가 필요해요.</p>
<div class="content-ad"></div>
<p>3-파이프라인 디자인은 ML 시스템에 구조와 모듈성을 제공하면서 MLOps 프로세스를 개선합니다.</p>
<h2>문제점</h2>
<p>MLOps 도구의 발전에도 불구하고, 프로토타입에서 프로덕션으로의 전환은 여전히 어려움을 겪고 있습니다.</p>
<p>2022년에는 모델 중 54%만이 프로덕션 환경으로 이동한다고 합니다. 우웅.</p>
<div class="content-ad"></div>
<p>그래서 무슨 일이 생길까요?</p>
<p>입에 먼저 나오는 것은 아마도:</p>
<ul>
<li>모델이 충분히 성숙하지 않다</li>
<li>보안 위험(예: 데이터 개인 정보 보호)</li>
<li>충분한 데이터가 없다</li>
</ul>
<p>어느 정도는 이 사실이 맞습니다.</p>
<div class="content-ad"></div>
<p>그러나 현실은 많은 시나리오에서...</p>
<p>...ML 시스템의 아키텍처는 연구를 염두에 두고 구축되거나 ML 시스템이 오프라인에서 온라인으로 리팩터링하기 매우 어려운 거대한 단일체가 됩니다.</p>
<p>그러므로 좋은 소프트웨어 엔지니어링 프로세스와 명확히 정의된 아키텍처가 적합한 도구와 높은 정확도의 모델 사용만큼 중요합니다.</p>
<h2>솔루션</h2>
<div class="content-ad"></div>
<p>→ 3-파이프라인 아키텍처</p>
<p>3-파이프라인 디자인이 무엇인지 알아봅시다.</p>
<p>개발 과정을 단순화하고, 당신의 단일 ML 파이프라인을 3가지 구성 요소로 나누는 데 도움이 되는 정신적인 지도입니다:</p>
<ol>
<li>피처 파이프라인</li>
<li>트레이닝 파이프라인</li>
<li>인퍼런스 파이프라인</li>
</ol>
<p>...또한 피처/트레이닝/인퍼런스 (FTI) 아키텍처로 알려져 있습니다.</p>
<div class="content-ad"></div>
<p>#1. 피처 파이프라인은 데이터를 피처와 레이블로 변환하여, 해당 내용을 피처 스토어에 저장하고 버전을 관리합니다. 피처 스토어는 피처들의 중앙 저장소로 기능하며, 피처들은 피처 스토어를 통해서만 액세스하고 공유할 수 있습니다.</p>
<p>#2. 트레이닝 파이프라인은 피처 스토어에서 특정 버전의 피처와 레이블을 가져와서 훈련된 모델 가중치를 출력하며, 이러한 가중치는 모델 레지스트리에 저장되고 버전을 관리합니다. 모델은 모델 레지스트리를 통해서만 액세스하고 공유할 수 있습니다.</p>
<p>#3. 추론 파이프라인은 피처 스토어에서 특정 버전의 피처를 사용하고 모델 레지스트리에서 특정 버전의 모델을 다운로드합니다. 최종 목표는 클라이언트에 예측을 출력하는 것입니다.</p>
<p>이것이 3개의 파이프라인 디자인이 아름다운 이유입니다:</p>
<div class="content-ad"></div>
<ul>
<li>직관적입니다.</li>
<li>모든 머신 러닝 시스템은 이 3가지 구성 요소로 축소될 수 있어 더 높은 수준에서 구조를 가져옵니다.</li>
<li>3가지 구성 요소 간에 투명한 인터페이스를 정의하여 여러 팀이 협업하기 쉬워집니다.</li>
<li>머신 러닝 시스템은 처음부터 모듈화를 염두에 두고 구축되었습니다.</li>
<li>필요에 따라 3가지 구성 요소를 여러 팀 사이로 쉽게 분할할 수 있습니다.</li>
<li>각 구성 요소는 작업에 가장 적합한 기술 스택을 사용할 수 있습니다.</li>
<li>각 구성 요소는 독립적으로 배포, 확장 및 모니터링할 수 있습니다.</li>
<li>피처 파이프라인은 배치, 스트리밍 또는 둘 다로 쉽게 구현할 수 있습니다.</li>
</ul>
<p>하지만 가장 중요한 이점은...</p>
<p>...이 패턴을 따라가면 여러분의 머신 러닝 모델이 노트북에서 제작 환경으로 옮겨질 것을 100% 확신할 수 있습니다.</p>
<p>↳ 3-파이프라인 디자인에 대해 더 자세히 알고 싶으시다면, FTI 아키텍처의 창시자 중 한 명인 Jim Dowling이 작성한 훌륭한 [3] 글을 추천합니다.</p>
<div class="content-ad"></div>
<h1>3. LLM Twin System design</h1>
<p>LLM 시스템에 3-파이프라인 아키텍처를 어떻게 적용하는 지 알아봅시다.</p>
<p>LLM twin의 아키텍처는 다음과 같이 4개의 Python 마이크로서비스로 구성됩니다:</p>
<ul>
<li>데이터 수집 파이프라인</li>
<li>특징 추출 파이프라인</li>
<li>훈련 파이프라인</li>
<li>추론 파이프라인</li>
</ul>
<div class="content-ad"></div>
<p>보시다시피, 데이터 수집 파이프라인은 3-파이프라인 디자인을 따르지 않습니다. 이게 사실이에요.</p>
<p>그것은 ML 시스템 이전에 위치한 데이터 파이프라인을 나타냅니다.</p>
<p>데이터 엔지니어링 팀이 주로 구현하며, 이 파이프라인은 대시보드 또는 ML 모델을 구축하는 데 필요한 데이터를 수집, 정리, 정규화하고 저장하는 것이 목표입니다.</p>
<p>하지만 작은 팀의 구성원이라고 하면, 데이터 수집부터 모델 배포까지 모든 것을 직접 구축해야 할 수도 있겠죠.</p>
<div class="content-ad"></div>
<p>그렇기 때문에 데이터 파이프라인이 FTI 아키텍처와 어떻게 잘 맞고 상호 작용하는지를 보여 드리겠습니다. 이제 각 구성 요소를 자세히 살펴봐서 개별적으로 어떻게 작동하고 서로 상호 작용하는지 이해해 보겠습니다. ↓↓↓</p>
<h2>3.1. 데이터 수집 파이프라인</h2>
<p>그 범위는 주어진 사용자의 데이터를 크롤링하는 것입니다:</p>
<div class="content-ad"></div>
<ul>
<li>Medium (기사)</li>
<li>Substack (기사)</li>
<li>LinkedIn (게시물)</li>
<li>GitHub (코드)</li>
</ul>
<p>각 플랫폼마다 고유하므로, 우리는 각 웹사이트를 위해 다른 Extract Transform Load (ETL) 파이프라인을 구현했습니다.</p>
<p>🔗 ETL  파이프라인에 대한 1분 소요 읽기 [4]</p>
<p>그러나 각 플랫폼에 대한 기본 단계는 동일합니다.</p>
<div class="content-ad"></div>
<p>따라서 각 ETL 파이프라인에 대해 다음과 같은 기본 단계를 추상화할 수 있습니다:</p>
<ul>
<li>자격 증명을 사용하여 로그인</li>
<li>Selenium을 사용하여 프로필을 크롤링</li>
<li>HTML을 구문 분석하기 위해 Beautiful Soup 사용</li>
<li>추출된 HTML을 정리하고 표준화</li>
<li>정규화된 (그럼에도 불구하고 원시) 데이터를 Mongo DB에 저장</li>
</ul>
<p>중요 사항: 개인 정보 보호 문제로 인해 대다수 플랫폼에서 다른 사람의 데이터에 접근할 수 없기 때문에 우리는 단지 우리 자신의 데이터만을 수집합니다. 그러나 이는 우리에게 완벽한 선택입니다. LLM 트윈을 구축하기 위해서는 우리 자신의 디지털 데이터만 필요합니다.</p>
<p>왜 Mongo DB를 사용할까요?</p>
<div class="content-ad"></div>
<p>우리는 텍스트와 같이 구조화되지 않은 데이터를 빠르게 저장할 수 있는 NoSQL 데이터베이스를 원했습니다.</p>
<p>데이터 파이프라인은 피쳐 파이프라인과 어떻게 통신할건가요?</p>
<p>우리는 모든 Mongo DB의 변경 사항을 피쳐 파이프라인에 알리기 위해 Change Data Capture (CDC) 패턴을 사용할 것입니다.</p>
<p>🔗 CDC 패턴에 대한 1분 간의 읽기 [5]</p>
<div class="content-ad"></div>
<p>CDC를 간략히 설명하자면, 감시자는 Mongo DB에 발생하는 모든 CRUD 작업을 24/7 감지합니다.</p>
<p>감시자는 수정된 내용을 알려주는 이벤트를 발생시킵니다. 이 이벤트를 RabbitMQ 큐에 추가할 거에요.</p>
<p>기능 파이프라인은 계속해서 큐를 듣고, 메시지를 처리하여 Qdrant vector DB에 추가할 거에요.</p>
<p>예를 들어, 우리가 Mongo DB에 새 문서를 작성할 때, 감시자는 새 이벤트를 생성합니다. 이벤트가 RabbitMQ 큐에 추가되고, 최종적으로 기능 파이프라인이 소비하고 처리합니다.</p>
<div class="content-ad"></div>
<p>이를 통해 Mongo DB와 Vector DB가 항상 동기화되도록 보장할 수 있습니다.</p>
<p>CDC 기술을 사용하면, 일괄 ETL 파이프라인(데이터 파이프라인)에서 스트리밍 파이프라인(특징 파이프라인)으로 전환합니다.</p>
<p>CDC 패턴을 사용하면, Mongo DB와 vector DB 간의 차이를 계산하기 위한 복잡한 일괄 파이프라인을 구현하는 것을 피할 수 있습니다. 이 접근 방식은 대규모 데이터를 처리할 때 빠르게 느려질 수 있습니다.</p>
<p>데이터 파이프라인은 어디에 배포될 것인가요?</p>
<div class="content-ad"></div>
<p>데이터 수집 파이프라인과 RabbitMQ 서비스는 AWS에 배포될 예정입니다. 또한 MongoDB의 프리미엄 서버리스 버전을 사용할 것입니다.</p>
<h2>3.2. 기능 파이프라인</h2>
<p>기능 파이프라인은 Bytewax를 사용하여 구현되었습니다 (Python 인터페이스를 갖춘 Rust 스트리밍 엔진). 따라서, 우리의 특정 사용 사례에서는 이를 스트리밍 입력 파이프라인으로도 참조할 것입니다.</p>
<p>이것은 데이터 수집 파이프라인과 완전히 다른 서비스입니다.</p>
<div class="content-ad"></div>
<p>데이터 파이프라인과 어떻게 통신하나요?</p>
<p>이전 설명대로, 기능 파이프라인은 RabbitMQ 큐를 통해 데이터 파이프라인과 통신합니다.</p>
<p>현재, 스트리밍 파이프라인은 데이터가 어떻게 생성되었는지나 어디에서 왔는지에 관심이 없습니다.</p>
<p>그저 특정 큐를 듣고, 그곳에서 메시지를 소비하고 처리해야 한다는 것만 알고 있습니다.</p>
<div class="content-ad"></div>
<p>이렇게 하면 두 구성 요소를 완전히 분리할 수 있습니다. 미래에는 여러 소스에서 메시지를 큐에 쉽게 추가할 수 있으며, 스트리밍 파이프라인이 이를 어떻게 처리해야 하는지 알게 될 것입니다. 유일한 규칙은 큐에 있는 메시지가 항상 동일한 구조/인터페이스를 준수해야 한다는 것입니다.</p>
<p>기능 파이프라인의 범위는 무엇인가요?</p>
<p>이것은 RAG 시스템의 인계 구성 요소를 나타냅니다.</p>
<p>큐를 통해 전달된 원시 데이터를 가져 와서 다음과 같은 작업을 수행할 것입니다:</p>
<div class="content-ad"></div>
<ul>
<li>데이터 정리하기;</li>
<li>청크로 나누기;</li>
<li>Superlinked의 임베딩 모델을 사용해 임베딩하기;</li>
<li>Qdrant 벡터 DB에 로드하기.</li>
</ul>
<p>각 유형의 데이터(게시물, 기사, 코드)는 각자의 클래스 세트를 통해 독립적으로 처리됩니다.</p>
<p>모두 텍스트 기반이지만, 각 데이터 유형마다 독특한 특징이 있기 때문에 데이터를 정리, 청크화, 임베딩하는 데 각기 다른 전략을 사용해야 합니다.</p>
<p>어떤 종류의 데이터가 저장될 것인가요?</p>
<div class="content-ad"></div>
<p>학습 파이프라인은 피쳐 스토어에만 액세스할 수 있습니다. 우리의 경우, Qdrant 벡터 DB로 표현됩니다.</p>
<p>벡터 DB는 NoSQL DB로 사용할 수도 있다는 것을 기억하세요.</p>
<p>이 두 가지를 염두에 두고, 우리는 Qdrant에 데이터의 2개 스냅샷을 저장할 것입니다:</p>
<ol>
<li>정제된 데이터(인덱스로 벡터를 사용하지 않고 NoSQL 방식으로 저장).</li>
</ol>
<div class="content-ad"></div>
<ol start="2">
<li>정리된, 청크 처리된 및 내장된 데이터 (Qdrant의 벡터 인덱스를 활용)</li>
</ol>
<p>학습 파이프라인은 표준 및 보강 프롬프트에서 LLM을 세밀 조정하고자 하므로 두 형식의 데이터에 액세스해야 합니다.</p>
<p>정리된 데이터로 프롬프트 및 답변을 생성할 것입니다.</p>
<p>청크 처리된 데이터로는 프롬프트를 보강할 것입니다 (일명 RAG).</p>
<div class="content-ad"></div>
<p>스트리밍 파이프라인을 배치 파이프라인 대신 구현해야 하는 이유는 무엇인가요?</p>
<p>그 이유는 주로 2가지가 있습니다.</p>
<p>첫 번째 이유는 CDC 패턴과 결합해서 서로 다른 두 개의 데이터베이스를 동기화하는 가장 효율적인 방법이기 때문입니다. 그렇지 않으면 대규모 데이터를 처리할 때 확장 가능하지 않은 배치 폴링 또는 푸싱 기술을 구현해야 할 수 있습니다.</p>
<p>CDC + 스트리밍 파이프라인을 사용하면 소스 데이터베이스의 변경 사항만 처리하고 여분의 작업이 발생하지 않습니다.</p>
<div class="content-ad"></div>
<p>두 번째 이유는 그렇게 함으로써 소스와 벡터 데이터베이스가 항상 동기화된 상태가 유지됩니다. 따라서 RAG를 수행할 때 최신 데이터에 항상 액세스할 수 있습니다.</p>
<p>왜 Bytewax를 사용해야 하는가?</p>
<p>Bytewax는 Python 인터페이스를 노출하는 Rust로 구축된 스트리밍 엔진입니다. Bytewax를 사용하는 이유는 Rust의 놀라운 속도와 신뢰성을 파이썬의 사용 편의성과 생태계와 결합시킨 점에 있습니다. Python 개발자에게는 매우 가볍고 강력하며 사용하기 쉽습니다.</p>
<p>기능 파이프라인은 어디에 배포될 것인가요?</p>
<div class="content-ad"></div>
<p>기능 파이프라인이 AWS에 배포될 예정입니다. 또한 우리는 Qdrant의 무료 서버리스 버전을 사용할 것입니다.</p>
<h2>3.3. 훈련 파이프라인</h2>
<p>훈련 기능에 접근할 수 있는 방법은 무엇인가요?</p>
<p>3.2절에서 강조한 대로, 모든 훈련 데이터는 기능 저장소에서 접근할 수 있습니다. 저희 경우에는 기능 저장소인 Qdrant 벡터 DB에 다음과 같은 데이터가 포함됩니다:</p>
<div class="content-ad"></div>
<ul>
<li>우리는 프롬프트 및 답변을 생성할 정돈된 디지털 데이터를 사용할 것입니다.</li>
<li>RAG를 위해 청크와 임베디드된 데이터를 사용하여 정돈된 데이터를 보완할 것입니다.</li>
</ul>
<p>우리는 주요 데이터 유형(게시물, 기사, 코드)마다 다른 벡터 DB 검색 클라이언트를 구현할 것입니다.</p>
<p>각 유형의 고유한 특성 때문에 벡터 DB를 쿼리하기 전에 각 유형을 다르게 전처리해야 합니다.</p>
<p>또한, 우리는 각 클라이언트에 대해 벡터 DB에서 어떤 것을 쿼리하고 싶은지에 기반한 사용자 정의 동작을 추가할 것입니다. 그러나 이에 대해 자세히는 해당 레슨에서 설명하겠습니다.</p>
<div class="content-ad"></div>
<p>훈련 파이프라인은 무엇을 할까요?</p>
<p>훈련 파이프라인에는 벡터 DB로부터 검색된 데이터를 전처리하여 프롬프트로 변환하는 데이터-투-프롬프트 레이어가 포함되어 있습니다.</p>
<p>또한 HuggingFace 데이터셋을 입력으로 사용하고 QLoRA를 사용하여 주어진 LLM(예: Mistral)을 세밀하게 튜닝하는 LLM 세밀 조정 모듈이 포함될 것입니다. HuggingFace를 사용함으로써 다양한 LLM 사이를 쉽게 전환할 수 있기 때문에 특정 LLM에 너무 많은 집중이 필요하지 않습니다.</p>
<p>모든 실험은 Comet ML의 실험 추적기에 로그가 남겨질 것입니다.</p>
<div class="content-ad"></div>
<p>저희는 미세 조정된 LLM의 결과를 평가하기 위해 더 큰 LLM(예: GPT4)을 사용할 것입니다. 이러한 결과는 Comet의 실험 추적기에 기록될 것입니다.</p>
<p>생산용 후보 LLM은 어디에 저장될까요?</p>
<p>우리는 여러 실험을 비교한 뒤 최적의 결과를 선택하여 모델 레지스트리를 위한 LLM 생산용 후보를 발표할 것입니다.</p>
<p>이후, Comet의 프롬프트 모니터링 대시보드를 사용하여 LLM 생산용 후보를 수동으로 검토할 것입니다. 최종 수동 검사가 통과되면, 우리는 모델 레지스트리의 LLM을 수락된 상태로 표시할 것입니다.</p>
<div class="content-ad"></div>
<p>CI/CD 파이프라인이 실행되어 새로운 LLM 버전이 추론 파이프라인에 배포될 것입니다.</p>
<p>훈련 파이프라인은 어디에 배포될까요?</p>
<p>훈련 파이프라인은 Qwak에 배포될 예정입니다.</p>
<p>Qwak은 ML 모델을 훈련하고 배포하는 서버리스 솔루션입니다. 작업 확장을 쉽게 할 수 있으며 건설에 집중할 수 있습니다.</p>
<div class="content-ad"></div>
<p>위 내용은 다음을 위해 Comet ML의 프리미엄 버전을 사용할 것입니다:</p>
<ul>
<li>실험 추적기;</li>
<li>모델 레지스트리;</li>
<li>실시간 감시.</li>
</ul>
<h2>3.4. 추론 파이프라인</h2>
<p>추론 파이프라인은 LLM 시스템의 최종 구성 요소입니다. 클라이언트가 상호 작용할 구성 요소입니다.</p>
<div class="content-ad"></div>
<p>이것은 REST API 아래에 랩핑될 것입니다. 클라이언트들은 HTTP 요청을 통해 이를 호출할 수 있으며, 이는 ChatGPT나 비슷한 도구들과 유사한 경험입니다.</p>
<p>기능에 어떻게 접근할까요?</p>
<p>기능 저장소에 접근하기 위해, 훈련 파이프라인에서와 같이 Qdrant 벡터 DB 검색 클라이언트들을 사용할 것입니다.</p>
<p>이 경우 RAG를 수행하기 위해 청크 데이터에 접근하는데 기능 저장소가 필요할 것입니다.</p>
<div class="content-ad"></div>
<p>얼마나 미세 조정된 LLM에 액세스할 수 있나요?</p>
<p>미세 조정된 LLM은 항상 해당 태그(예: accepted)와 버전(예: v1.0.2, latest 등)을 기반으로 모델 레지스트리에서 다운로드됩니다.</p>
<p>미세 조정된 LLM은 어떻게 로드되나요?</p>
<p>우리는 여기서 추론 세계에 있습니다.</p>
<div class="content-ad"></div>
<p>LLM의 속도와 메모리 사용량을 최대한 최적화하려고 합니다. 그래서 모델 레지스트리에서 LLM을 다운로드한 후 양자화할 것입니다.</p>
<p>추론 파이프라인의 구성 요소는 무엇인가요?</p>
<p>첫 번째는 RAG를 수행하기 위해 벡터 데이터베이스에 액세스하는 검색 클라이언트입니다. 이는 교육 파이프라인에서 사용된 모듈과 동일합니다.</p>
<p>Qdrant에서 검색된 도큐먼트를 프롬프트로 매핑할 쿼리가 있으면 해당 쿼리 후보를 매핑해줄 계층이 있습니다.</p>
<div class="content-ad"></div>
<p>LLM이 답변을 생성한 후, 우리는 Comet의 프롬프트 모니터링 대시보드에 기록하고 클라이언트에게 반환할 것입니다.</p>
<p>예를 들어, 클라이언트는 추론 파이프라인에 다음을 요청할 수 있습니다:</p>
<p>&quot;LLM에 관한 1000단어의 LinkedIn 게시물 작성&quot;, 그리고 추론 파이프라인은 생성된 게시물을 반환하기 위해 위에서 설명한 모든 단계를 거칠 것입니다.</p>
<p>추론 파이프라인은 어디에 배포될 것인가요?</p>
<div class="content-ad"></div>
<p>추론 파이프라인은 Qwak로 배포됩니다.</p>
<p>기본 설정으로, Qwak은 자동 확장 솔루션과 생산 환경 자원을 모니터링하는 멋진 대시보드도 제공합니다.</p>
<p>훈련 파이프라인에 대해서는, 우리는 실시간 모니터링 대시보드를 제공하는 Comet의 서버리스 프리미엄 버전을 사용할 것입니다.</p>
<h1>결론</h1>
<div class="content-ad"></div>
<p>LLM Twin의 무료 코스 1번째 기사입니다.</p>
<p>이 강의에서는 이 코스 동안 구축할 내용을 소개했습니다.</p>
<p>간단히 ML 시스템을 설계하는 방법에 대해 논의한 후</p>
<p>최종적으로 이 코스의 시스템 디자인을 살펴보고 각 마이크로서비스의 아키텍처 및 서로 상호작용 방법을 소개했습니다.</p>
<div class="content-ad"></div>
<ul>
<li>데이터 수집 파이프라인</li>
<li>피쳐 파이프라인</li>
<li>훈련 파이프라인</li>
<li>추론 파이프라인</li>
</ul>
<p>제2 장에서는 데이터 수집 파이프라인을 더 자세히 살펴보고, 다양한 소셜 미디어 플랫폼에 크롤러를 구현하는 방법을 배우고, 수집한 데이터를 정리하여 Mongo DB에 저장하고, 마지막으로 AWS에 배포하는 방법을 안내해 드릴 거에요.</p>
<p>이 기사를 즐겁게 보셨나요? 그렇다면...</p>
<p>↓↓↓</p>
<div class="content-ad"></div>
<p>5천 명 이상의 엔지니어들과 함께하여, 프로덕션급 머신 러닝에 대한 검증된 컨텐츠를 살펴보세요. 매주 업데이트되는 콘텐츠를 놓치지 마세요:</p>
<h1>참고문헌</h1>
<p>[1] 당신의 LLM 트윈 코스 — GitHub 저장소 (2024년), Decoding ML GitHub 조직</p>
<p>[2] Meta에서 새로운 AI 경험 소개(2023년), Meta</p>
<div class="content-ad"></div>
<ul>
<li>
<p>[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks</p>
</li>
<li>
<p>[4] Extract Transform Load (ETL), Databricks Glossary</p>
</li>
<li>
<p>[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), Superlinked</p>
</li>
</ul></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"프로덕션에 적합한 LLM 시스템을 위한 엔드 투 엔드 프레임워크 LLM 트윈 구축하기","description":"","date":"2024-05-18 20:03","slug":"2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin","content":"\n\n## LLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICA\n\n→ LLM Twin 무료 코스의 첫 번째 강의\n\n당신의 LLM Twin은 무엇인가요? LLM Twin은 당신의 스타일, 성격 및 목소리를 포함하여 당신처럼 쓰는 AI 캐릭터입니다.\n\n![이미지](/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 이 강의가 다른 이유는 무엇인가요?\n\n무료 강의 \"LLM Twin: Building Your Production-Ready AI Replica\"를 완료하면 LLMs, 벡터 DB 및 LLMOps의 좋은 실천법에 의해 구동되는 스스로의 프로덕션 준비 AI 복제본을 설계, 훈련 및 배포하는 방법을 배울 수 있습니다.\n\n## 이 강의를 통해 어떤 것을 배우게 되나요?\n\n데이터 수집부터 배포까지 실제 LLM 시스템을 설계하고 구축하는 방법을 배우실 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMLOps의 최상의 실천법을 활용하는 방법을 배울 것입니다. 실험 추적기, 모델 레지스트리, 즉시 모니터링, 그리고 버전 관리 등이 있습니다.\n\n최종 목표는 무엇일까요? 자신만의 LLM 쌍을 구축하고 배포하는 것입니다.\n\nLLM 쌍의 아키텍처는 4개의 파이썬 마이크로서비스로 나뉩니다:\n\n- 데이터 수집 파이프라인: 다양한 소셜 미디어 플랫폼에서 디지털 데이터를 수집합니다. ETL 파이프라인을 통해 데이터를 정리, 정규화하고 NoSQL DB에 로드합니다. CDC 패턴을 사용하여 데이터베이스 변경 사항을 큐로 전송합니다. (AWS에 배포됨)\n- 피처 파이프라인: 바이트왁스 스트리밍 파이프라인을 통해 큐에서 메시지를 소비합니다. 각 메시지는 실시간으로 정리되고 청크화되며 Superlinked를 사용하여 삽입되고 Qdrant 벡터 DB에 로드됩니다. (AWS에 배포됨)\n- 트레이닝 파이프라인: 디지털 데이터를 기반으로 사용자 정의 데이터세트를 생성합니다. QLoRA를 사용하여 LLM을 세밀하게 조정합니다. Comet ML의 실험 추적기를 사용하여 실험을 모니터링합니다. 최상의 모델을 Comet의 모델 레지스트리에 저장 및 평가합니다. (Qwak에 배포됨)\n- 인퍼런스 파이프라인: Comet의 모델 레지스트리에서 세밀하게 조정된 LLM을 로드하고 양자화합니다. 이를 REST API로 배포합니다. RAG를 사용하여 프롬프트를 향상시키고, LLM 쌍을 사용하여 콘텐츠를 생성합니다. Comet의 프롬프트 모니터링 대시보드를 사용하여 LLM을 모니터링합니다. (Qwak에 배포됨)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4개의 마이크로서비스를 통해 3가지 서버리스 도구를 통합하는 방법을 배울 수 있습니다:\n\n- ML 플랫폼으로서의 Comet ML;\n- 벡터 DB로서의 Qdrant;\n- ML 인프라로서의 Qwak;\n\n## 누구를 위한 것인가요?\n\n대상: MLE, DE, DS 또는 SWE로서, LLMOps의 좋은 원칙을 사용하여 제품 준비 상태의 LLM 시스템을 설계하고 싶은 분들을 대상으로 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수준: 중급\n\n필수 조건: Python, ML 및 클라우드에 대한 기본 지식\n\n## 어떻게 학습하시겠습니까?\n\n이 강의에는 11개의 실습 수업 및 GitHub에서 액세스할 수 있는 오픈 소스 코드가 포함되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모두 자신의 속도로 모든 것을 읽을 수 있어요.\n\n→ 이 과정을 최대한 효율적으로 활용하려면 강의를 따라가며 저장소를 복제하고 실행하는 것을 권장해요.\n\n## 비용은?\n\n기사와 코드는 완전히 무료에요. 언제나 무료로 제공될 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 코드를 실행하면서 읽으려면, 추가 비용이 발생할 수 있는 몇 가지 클라우드 도구를 사용한다는 것을 알아두어야 합니다.\n\n클라우드 컴퓨팅 플랫폼(AWS, Qwak)은 pay-as-you-go 요금제를 제공합니다. Qwak은 무료 컴퓨팅 시간을 제공합니다. 따라서 우리는 비용을 최소화하기 위해 최선을 다하였습니다.\n\n다른 서버리스 도구(Qdrant, Comet)의 경우, 무료로 사용할 수 있는 프리미엄 버전을 사용할 것입니다.\n\n## 선생님들을 만나보세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDecoding ML 우산 아래에서 개설 된 이 과정을 개발 한 사람들은 다음과 같습니다:\n\n- Paul Iusztin | 시니어 ML \u0026 MLOps 엔지니어\n- Alex Vesa | 시니어 AI 엔지니어\n- Alex Razvant | 시니어 ML \u0026 MLOps 엔지니어\n\n# 수업\n\n이 과정은 총 11개의 수업으로 구성되어 있습니다. 매체 기사 하나당 하나의 수업으로 나뉩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- LLM 시스템의 제품용 엔드 투 엔드 프레임워크 구축을 통한 LLM Twin\n- 생성 모델 AI 시대의 데이터 파이프라인의 중요성\n- 변경 데이터 캡처: 이벤트 주도 아키텍처 가능\n- 실시간으로 LLM 및 RAG의 파이썬 스트리밍 파이프라인의 뛰어난 솔루션\n- 구현해야 할 4가지 고급 RAG 알고리즘\n- 특징 저장소의 역할 LLM 세부 조정에\n- LLM 세부 조정 [모듈 3] ...작업 중\n- LLM 평가 [모듈 4] ...작업 중\n- 양자화 [모듈 5] ...작업 중\n- 디지털 트윈 추론 파이프라인 구축 [모듈 6] ...작업 중\n- REST API로 디지털 트윈 배포 [모듈 6] ...작업 중\n\n첫 번째 레슨에서는 당신이 수강 기간 동안 구축할 프로젝트인 제품용 LLM Twin/AI 복제품을 소개할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이후에는 3-파이프라인 디자인이 무엇인지와 이것이 표준 ML 시스템에 어떻게 적용되는지 설명할 것입니다.\n\n마지막으로, LLM 프로젝트 시스템 디자인에 대해 자세히 살펴볼 것입니다.\n\n소셜 미디어 데이터 수집 파이프라인 디자인에 대한 모든 아키텍처 결정과 LLM 마이크로서비스에 3-파이프라인 아키텍처를 적용하는 방법을 설명할 것입니다.\n\n다음 수업에서는 각 구성 요소의 코드를 검토하고 AWS 및 Qwak에 구현하고 배포하는 방법을 배울 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 목차\n\n- 무엇을 구축할 계획인가요? LLM twin 개념\n- 3-파이프라인 아키텍처\n- LLM twin 시스템 디자인\n\n# 1. 무엇을 구축할 계획인가요? LLM twin 개념\n\n이 과정의 목표는 당신만의 AI 레플리카를 구축하는 것입니다. 우리는 그것을 할 수 있도록 LLM을 사용할 것이며, 따라서 이 과정의 이름이 LLM Twin: 생산 준비가 완료된 AI 레플리카 구축입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 쌍이 무엇인지 알고 싶으신가요?\n\n간단히 말씀드리면, LLM 쌍은 여러분과 비슷한 방식으로 글을 쓰는 인공지능 캐릭터가 될 거에요.\n\n여러분 그 자신이 되는 게 아니라, 여러분의 글 스타일과 성격을 활용하는 쓰기 기계예요.\n\n구체적으로 말하면, 여러분이 자신의 목소리로 소셜미디어 글이나 기술 기사(이렇게 작성된 것처럼)를 쓰는 AI 판본을 만드실 수 있을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nChatGPT을 직접 사용하지 않는 이유가 무엇인가요? 궁금하시다면…\n\nLLM을 사용하여 기사나 글을 생성할 때 결과물이 다음과 같은 경향이 있습니다:\n\n- 매우 일반적이고 미흡하게 나옵니다.\n- 허상으로 인한 잘못된 정보가 포함될 수 있습니다.\n- 원하는 결과를 얻기 위해 번거로운 프롬프팅이 필요할 수 있습니다.\n\n하지만 이런 문제를 해결하기 위해 우리가 할 일은 ↓↓↓\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, LinkedIn, Medium, Substack 및 GitHub에서 수집한 디지털 데이터로 LLM을 세밀하게 조정할 것입니다.\n\n이를 통해 LLM은 당신의 쓰기 스타일과 온라인 개성과 일치하게 될 것입니다. LLM을 통해 당신 온라인 버전처럼 대화하는 법을 배울 것입니다.\n\n2024년 Meta가 Messenger 앱에서 발표한 AI 캐릭터의 우주를 보신 적이 있나요? 만약 아직이라면, 여기 [2]에서 더 자세히 알아볼 수 있습니다.\n\n어느 정도 그것이 우리가 구축하려는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 우리의 사용 사례에서는 당신의 목소리를 반영하고 표현하는 소셜 미디어 게시물이나 글을 쓰는 LLM 쌍에 초점을 맞추겠습니다.\n\n예를 들어, 우리는 당신의 LLM 쌍에게 LLM에 관한 LinkedIn 게시물을 작성하도록 요청할 수 있습니다. LLM에 관한 어떤 일반적이고 표현되지 않은 게시물(예: ChatGPT가 무엇을 할 것인지) 대신에 당신의 목소리와 스타일을 사용할 것입니다.\n\n두 번째로, 우리는 환각을 피하기 위해 외부 정보에 액세스하기 위해 LLM에게 벡터 DB에 액세스할 수 있게 할 것입니다. 따라서 LLM이 구체적인 데이터에 기반하여만 쓸 수 있도록 할 것입니다.\n\n최종적으로, 정보를 얻기 위해 벡터 DB에 액세스하는 것 외에도 생성 프로세스의 기본 블록 역할을 하는 외부 링크를 제공할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 위의 예시를 다음과 같이 수정해 볼 수 있어요: \"이 링크의 기사를 기반으로 LLMs에 관한 1000단어 LinkedIn 게시물을 작성해주세요: [URL].\"\n\n기대되시나요? 시작해봅시다!🔥\n\n# 2. 3단계 파이프라인 구조\n\n우리 모두는 머신러닝 시스템이 얼마나 엉망이 될 수 있는지 알고 있어요. 이 때 3단계 파이프라인 구조가 필요해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3-파이프라인 디자인은 ML 시스템에 구조와 모듈성을 제공하면서 MLOps 프로세스를 개선합니다.\n\n## 문제점\n\nMLOps 도구의 발전에도 불구하고, 프로토타입에서 프로덕션으로의 전환은 여전히 어려움을 겪고 있습니다.\n\n2022년에는 모델 중 54%만이 프로덕션 환경으로 이동한다고 합니다. 우웅.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그래서 무슨 일이 생길까요?\n\n입에 먼저 나오는 것은 아마도:\n\n- 모델이 충분히 성숙하지 않다\n- 보안 위험(예: 데이터 개인 정보 보호)\n- 충분한 데이터가 없다\n\n어느 정도는 이 사실이 맞습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 현실은 많은 시나리오에서...\n\n...ML 시스템의 아키텍처는 연구를 염두에 두고 구축되거나 ML 시스템이 오프라인에서 온라인으로 리팩터링하기 매우 어려운 거대한 단일체가 됩니다.\n\n그러므로 좋은 소프트웨어 엔지니어링 프로세스와 명확히 정의된 아키텍처가 적합한 도구와 높은 정확도의 모델 사용만큼 중요합니다.\n\n## 솔루션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n→ 3-파이프라인 아키텍처\n\n3-파이프라인 디자인이 무엇인지 알아봅시다.\n\n개발 과정을 단순화하고, 당신의 단일 ML 파이프라인을 3가지 구성 요소로 나누는 데 도움이 되는 정신적인 지도입니다:\n1. 피처 파이프라인\n2. 트레이닝 파이프라인\n3. 인퍼런스 파이프라인\n\n...또한 피처/트레이닝/인퍼런스 (FTI) 아키텍처로 알려져 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n#1. 피처 파이프라인은 데이터를 피처와 레이블로 변환하여, 해당 내용을 피처 스토어에 저장하고 버전을 관리합니다. 피처 스토어는 피처들의 중앙 저장소로 기능하며, 피처들은 피처 스토어를 통해서만 액세스하고 공유할 수 있습니다.\n\n#2. 트레이닝 파이프라인은 피처 스토어에서 특정 버전의 피처와 레이블을 가져와서 훈련된 모델 가중치를 출력하며, 이러한 가중치는 모델 레지스트리에 저장되고 버전을 관리합니다. 모델은 모델 레지스트리를 통해서만 액세스하고 공유할 수 있습니다.\n\n#3. 추론 파이프라인은 피처 스토어에서 특정 버전의 피처를 사용하고 모델 레지스트리에서 특정 버전의 모델을 다운로드합니다. 최종 목표는 클라이언트에 예측을 출력하는 것입니다.\n\n이것이 3개의 파이프라인 디자인이 아름다운 이유입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 직관적입니다.\n- 모든 머신 러닝 시스템은 이 3가지 구성 요소로 축소될 수 있어 더 높은 수준에서 구조를 가져옵니다.\n- 3가지 구성 요소 간에 투명한 인터페이스를 정의하여 여러 팀이 협업하기 쉬워집니다.\n- 머신 러닝 시스템은 처음부터 모듈화를 염두에 두고 구축되었습니다.\n- 필요에 따라 3가지 구성 요소를 여러 팀 사이로 쉽게 분할할 수 있습니다.\n- 각 구성 요소는 작업에 가장 적합한 기술 스택을 사용할 수 있습니다.\n- 각 구성 요소는 독립적으로 배포, 확장 및 모니터링할 수 있습니다.\n- 피처 파이프라인은 배치, 스트리밍 또는 둘 다로 쉽게 구현할 수 있습니다.\n\n하지만 가장 중요한 이점은...\n\n...이 패턴을 따라가면 여러분의 머신 러닝 모델이 노트북에서 제작 환경으로 옮겨질 것을 100% 확신할 수 있습니다.\n\n↳ 3-파이프라인 디자인에 대해 더 자세히 알고 싶으시다면, FTI 아키텍처의 창시자 중 한 명인 Jim Dowling이 작성한 훌륭한 [3] 글을 추천합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3. LLM Twin System design\n\nLLM 시스템에 3-파이프라인 아키텍처를 어떻게 적용하는 지 알아봅시다.\n\nLLM twin의 아키텍처는 다음과 같이 4개의 Python 마이크로서비스로 구성됩니다:\n\n- 데이터 수집 파이프라인\n- 특징 추출 파이프라인\n- 훈련 파이프라인\n- 추론 파이프라인\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보시다시피, 데이터 수집 파이프라인은 3-파이프라인 디자인을 따르지 않습니다. 이게 사실이에요.\n\n그것은 ML 시스템 이전에 위치한 데이터 파이프라인을 나타냅니다.\n\n데이터 엔지니어링 팀이 주로 구현하며, 이 파이프라인은 대시보드 또는 ML 모델을 구축하는 데 필요한 데이터를 수집, 정리, 정규화하고 저장하는 것이 목표입니다.\n\n하지만 작은 팀의 구성원이라고 하면, 데이터 수집부터 모델 배포까지 모든 것을 직접 구축해야 할 수도 있겠죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그렇기 때문에 데이터 파이프라인이 FTI 아키텍처와 어떻게 잘 맞고 상호 작용하는지를 보여 드리겠습니다. 이제 각 구성 요소를 자세히 살펴봐서 개별적으로 어떻게 작동하고 서로 상호 작용하는지 이해해 보겠습니다. ↓↓↓\n\n## 3.1. 데이터 수집 파이프라인\n\n그 범위는 주어진 사용자의 데이터를 크롤링하는 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Medium (기사)\n- Substack (기사)\n- LinkedIn (게시물)\n- GitHub (코드)\n\n각 플랫폼마다 고유하므로, 우리는 각 웹사이트를 위해 다른 Extract Transform Load (ETL) 파이프라인을 구현했습니다.\n\n🔗 ETL  파이프라인에 대한 1분 소요 읽기 [4]\n\n그러나 각 플랫폼에 대한 기본 단계는 동일합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 각 ETL 파이프라인에 대해 다음과 같은 기본 단계를 추상화할 수 있습니다:\n\n- 자격 증명을 사용하여 로그인\n- Selenium을 사용하여 프로필을 크롤링\n- HTML을 구문 분석하기 위해 Beautiful Soup 사용\n- 추출된 HTML을 정리하고 표준화\n- 정규화된 (그럼에도 불구하고 원시) 데이터를 Mongo DB에 저장\n\n중요 사항: 개인 정보 보호 문제로 인해 대다수 플랫폼에서 다른 사람의 데이터에 접근할 수 없기 때문에 우리는 단지 우리 자신의 데이터만을 수집합니다. 그러나 이는 우리에게 완벽한 선택입니다. LLM 트윈을 구축하기 위해서는 우리 자신의 디지털 데이터만 필요합니다.\n\n왜 Mongo DB를 사용할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 텍스트와 같이 구조화되지 않은 데이터를 빠르게 저장할 수 있는 NoSQL 데이터베이스를 원했습니다.\n\n데이터 파이프라인은 피쳐 파이프라인과 어떻게 통신할건가요?\n\n우리는 모든 Mongo DB의 변경 사항을 피쳐 파이프라인에 알리기 위해 Change Data Capture (CDC) 패턴을 사용할 것입니다.\n\n🔗 CDC 패턴에 대한 1분 간의 읽기 [5]\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCDC를 간략히 설명하자면, 감시자는 Mongo DB에 발생하는 모든 CRUD 작업을 24/7 감지합니다.\n\n감시자는 수정된 내용을 알려주는 이벤트를 발생시킵니다. 이 이벤트를 RabbitMQ 큐에 추가할 거에요.\n\n기능 파이프라인은 계속해서 큐를 듣고, 메시지를 처리하여 Qdrant vector DB에 추가할 거에요.\n\n예를 들어, 우리가 Mongo DB에 새 문서를 작성할 때, 감시자는 새 이벤트를 생성합니다. 이벤트가 RabbitMQ 큐에 추가되고, 최종적으로 기능 파이프라인이 소비하고 처리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이를 통해 Mongo DB와 Vector DB가 항상 동기화되도록 보장할 수 있습니다.\n\nCDC 기술을 사용하면, 일괄 ETL 파이프라인(데이터 파이프라인)에서 스트리밍 파이프라인(특징 파이프라인)으로 전환합니다.\n\nCDC 패턴을 사용하면, Mongo DB와 vector DB 간의 차이를 계산하기 위한 복잡한 일괄 파이프라인을 구현하는 것을 피할 수 있습니다. 이 접근 방식은 대규모 데이터를 처리할 때 빠르게 느려질 수 있습니다.\n\n데이터 파이프라인은 어디에 배포될 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 수집 파이프라인과 RabbitMQ 서비스는 AWS에 배포될 예정입니다. 또한 MongoDB의 프리미엄 서버리스 버전을 사용할 것입니다.\n\n## 3.2. 기능 파이프라인\n\n기능 파이프라인은 Bytewax를 사용하여 구현되었습니다 (Python 인터페이스를 갖춘 Rust 스트리밍 엔진). 따라서, 우리의 특정 사용 사례에서는 이를 스트리밍 입력 파이프라인으로도 참조할 것입니다.\n\n이것은 데이터 수집 파이프라인과 완전히 다른 서비스입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 파이프라인과 어떻게 통신하나요?\n\n이전 설명대로, 기능 파이프라인은 RabbitMQ 큐를 통해 데이터 파이프라인과 통신합니다.\n\n현재, 스트리밍 파이프라인은 데이터가 어떻게 생성되었는지나 어디에서 왔는지에 관심이 없습니다.\n\n그저 특정 큐를 듣고, 그곳에서 메시지를 소비하고 처리해야 한다는 것만 알고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이렇게 하면 두 구성 요소를 완전히 분리할 수 있습니다. 미래에는 여러 소스에서 메시지를 큐에 쉽게 추가할 수 있으며, 스트리밍 파이프라인이 이를 어떻게 처리해야 하는지 알게 될 것입니다. 유일한 규칙은 큐에 있는 메시지가 항상 동일한 구조/인터페이스를 준수해야 한다는 것입니다.\n\n기능 파이프라인의 범위는 무엇인가요?\n\n이것은 RAG 시스템의 인계 구성 요소를 나타냅니다.\n\n큐를 통해 전달된 원시 데이터를 가져 와서 다음과 같은 작업을 수행할 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 정리하기;\n- 청크로 나누기;\n- Superlinked의 임베딩 모델을 사용해 임베딩하기;\n- Qdrant 벡터 DB에 로드하기.\n\n각 유형의 데이터(게시물, 기사, 코드)는 각자의 클래스 세트를 통해 독립적으로 처리됩니다.\n\n모두 텍스트 기반이지만, 각 데이터 유형마다 독특한 특징이 있기 때문에 데이터를 정리, 청크화, 임베딩하는 데 각기 다른 전략을 사용해야 합니다.\n\n어떤 종류의 데이터가 저장될 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n학습 파이프라인은 피쳐 스토어에만 액세스할 수 있습니다. 우리의 경우, Qdrant 벡터 DB로 표현됩니다.\n\n벡터 DB는 NoSQL DB로 사용할 수도 있다는 것을 기억하세요.\n\n이 두 가지를 염두에 두고, 우리는 Qdrant에 데이터의 2개 스냅샷을 저장할 것입니다:\n\n1. 정제된 데이터(인덱스로 벡터를 사용하지 않고 NoSQL 방식으로 저장).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. 정리된, 청크 처리된 및 내장된 데이터 (Qdrant의 벡터 인덱스를 활용)\n\n학습 파이프라인은 표준 및 보강 프롬프트에서 LLM을 세밀 조정하고자 하므로 두 형식의 데이터에 액세스해야 합니다.\n\n정리된 데이터로 프롬프트 및 답변을 생성할 것입니다.\n\n청크 처리된 데이터로는 프롬프트를 보강할 것입니다 (일명 RAG).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스트리밍 파이프라인을 배치 파이프라인 대신 구현해야 하는 이유는 무엇인가요?\n\n그 이유는 주로 2가지가 있습니다.\n\n첫 번째 이유는 CDC 패턴과 결합해서 서로 다른 두 개의 데이터베이스를 동기화하는 가장 효율적인 방법이기 때문입니다. 그렇지 않으면 대규모 데이터를 처리할 때 확장 가능하지 않은 배치 폴링 또는 푸싱 기술을 구현해야 할 수 있습니다.\n\nCDC + 스트리밍 파이프라인을 사용하면 소스 데이터베이스의 변경 사항만 처리하고 여분의 작업이 발생하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 번째 이유는 그렇게 함으로써 소스와 벡터 데이터베이스가 항상 동기화된 상태가 유지됩니다. 따라서 RAG를 수행할 때 최신 데이터에 항상 액세스할 수 있습니다.\n\n왜 Bytewax를 사용해야 하는가?\n\nBytewax는 Python 인터페이스를 노출하는 Rust로 구축된 스트리밍 엔진입니다. Bytewax를 사용하는 이유는 Rust의 놀라운 속도와 신뢰성을 파이썬의 사용 편의성과 생태계와 결합시킨 점에 있습니다. Python 개발자에게는 매우 가볍고 강력하며 사용하기 쉽습니다.\n\n기능 파이프라인은 어디에 배포될 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기능 파이프라인이 AWS에 배포될 예정입니다. 또한 우리는 Qdrant의 무료 서버리스 버전을 사용할 것입니다.\n\n## 3.3. 훈련 파이프라인\n\n훈련 기능에 접근할 수 있는 방법은 무엇인가요?\n\n3.2절에서 강조한 대로, 모든 훈련 데이터는 기능 저장소에서 접근할 수 있습니다. 저희 경우에는 기능 저장소인 Qdrant 벡터 DB에 다음과 같은 데이터가 포함됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 우리는 프롬프트 및 답변을 생성할 정돈된 디지털 데이터를 사용할 것입니다.\n- RAG를 위해 청크와 임베디드된 데이터를 사용하여 정돈된 데이터를 보완할 것입니다.\n\n우리는 주요 데이터 유형(게시물, 기사, 코드)마다 다른 벡터 DB 검색 클라이언트를 구현할 것입니다.\n\n각 유형의 고유한 특성 때문에 벡터 DB를 쿼리하기 전에 각 유형을 다르게 전처리해야 합니다.\n\n또한, 우리는 각 클라이언트에 대해 벡터 DB에서 어떤 것을 쿼리하고 싶은지에 기반한 사용자 정의 동작을 추가할 것입니다. 그러나 이에 대해 자세히는 해당 레슨에서 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 파이프라인은 무엇을 할까요?\n\n훈련 파이프라인에는 벡터 DB로부터 검색된 데이터를 전처리하여 프롬프트로 변환하는 데이터-투-프롬프트 레이어가 포함되어 있습니다.\n\n또한 HuggingFace 데이터셋을 입력으로 사용하고 QLoRA를 사용하여 주어진 LLM(예: Mistral)을 세밀하게 튜닝하는 LLM 세밀 조정 모듈이 포함될 것입니다. HuggingFace를 사용함으로써 다양한 LLM 사이를 쉽게 전환할 수 있기 때문에 특정 LLM에 너무 많은 집중이 필요하지 않습니다.\n\n모든 실험은 Comet ML의 실험 추적기에 로그가 남겨질 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희는 미세 조정된 LLM의 결과를 평가하기 위해 더 큰 LLM(예: GPT4)을 사용할 것입니다. 이러한 결과는 Comet의 실험 추적기에 기록될 것입니다.\n\n생산용 후보 LLM은 어디에 저장될까요?\n\n우리는 여러 실험을 비교한 뒤 최적의 결과를 선택하여 모델 레지스트리를 위한 LLM 생산용 후보를 발표할 것입니다.\n\n이후, Comet의 프롬프트 모니터링 대시보드를 사용하여 LLM 생산용 후보를 수동으로 검토할 것입니다. 최종 수동 검사가 통과되면, 우리는 모델 레지스트리의 LLM을 수락된 상태로 표시할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCI/CD 파이프라인이 실행되어 새로운 LLM 버전이 추론 파이프라인에 배포될 것입니다.\n\n훈련 파이프라인은 어디에 배포될까요?\n\n훈련 파이프라인은 Qwak에 배포될 예정입니다.\n\nQwak은 ML 모델을 훈련하고 배포하는 서버리스 솔루션입니다. 작업 확장을 쉽게 할 수 있으며 건설에 집중할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 내용은 다음을 위해 Comet ML의 프리미엄 버전을 사용할 것입니다:\n\n- 실험 추적기;\n- 모델 레지스트리;\n- 실시간 감시.\n\n## 3.4. 추론 파이프라인\n\n추론 파이프라인은 LLM 시스템의 최종 구성 요소입니다. 클라이언트가 상호 작용할 구성 요소입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 REST API 아래에 랩핑될 것입니다. 클라이언트들은 HTTP 요청을 통해 이를 호출할 수 있으며, 이는 ChatGPT나 비슷한 도구들과 유사한 경험입니다.\n\n기능에 어떻게 접근할까요?\n\n기능 저장소에 접근하기 위해, 훈련 파이프라인에서와 같이 Qdrant 벡터 DB 검색 클라이언트들을 사용할 것입니다.\n\n이 경우 RAG를 수행하기 위해 청크 데이터에 접근하는데 기능 저장소가 필요할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n얼마나 미세 조정된 LLM에 액세스할 수 있나요?\n\n미세 조정된 LLM은 항상 해당 태그(예: accepted)와 버전(예: v1.0.2, latest 등)을 기반으로 모델 레지스트리에서 다운로드됩니다.\n\n미세 조정된 LLM은 어떻게 로드되나요?\n\n우리는 여기서 추론 세계에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM의 속도와 메모리 사용량을 최대한 최적화하려고 합니다. 그래서 모델 레지스트리에서 LLM을 다운로드한 후 양자화할 것입니다.\n\n추론 파이프라인의 구성 요소는 무엇인가요?\n\n첫 번째는 RAG를 수행하기 위해 벡터 데이터베이스에 액세스하는 검색 클라이언트입니다. 이는 교육 파이프라인에서 사용된 모듈과 동일합니다.\n\nQdrant에서 검색된 도큐먼트를 프롬프트로 매핑할 쿼리가 있으면 해당 쿼리 후보를 매핑해줄 계층이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM이 답변을 생성한 후, 우리는 Comet의 프롬프트 모니터링 대시보드에 기록하고 클라이언트에게 반환할 것입니다.\n\n예를 들어, 클라이언트는 추론 파이프라인에 다음을 요청할 수 있습니다:\n\n\"LLM에 관한 1000단어의 LinkedIn 게시물 작성\", 그리고 추론 파이프라인은 생성된 게시물을 반환하기 위해 위에서 설명한 모든 단계를 거칠 것입니다.\n\n추론 파이프라인은 어디에 배포될 것인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n추론 파이프라인은 Qwak로 배포됩니다.\n\n기본 설정으로, Qwak은 자동 확장 솔루션과 생산 환경 자원을 모니터링하는 멋진 대시보드도 제공합니다.\n\n훈련 파이프라인에 대해서는, 우리는 실시간 모니터링 대시보드를 제공하는 Comet의 서버리스 프리미엄 버전을 사용할 것입니다.\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM Twin의 무료 코스 1번째 기사입니다.\n\n이 강의에서는 이 코스 동안 구축할 내용을 소개했습니다.\n\n간단히 ML 시스템을 설계하는 방법에 대해 논의한 후\n\n최종적으로 이 코스의 시스템 디자인을 살펴보고 각 마이크로서비스의 아키텍처 및 서로 상호작용 방법을 소개했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 수집 파이프라인\n- 피쳐 파이프라인\n- 훈련 파이프라인\n- 추론 파이프라인\n\n제2 장에서는 데이터 수집 파이프라인을 더 자세히 살펴보고, 다양한 소셜 미디어 플랫폼에 크롤러를 구현하는 방법을 배우고, 수집한 데이터를 정리하여 Mongo DB에 저장하고, 마지막으로 AWS에 배포하는 방법을 안내해 드릴 거에요.\n\n이 기사를 즐겁게 보셨나요? 그렇다면...\n\n↓↓↓\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n5천 명 이상의 엔지니어들과 함께하여, 프로덕션급 머신 러닝에 대한 검증된 컨텐츠를 살펴보세요. 매주 업데이트되는 콘텐츠를 놓치지 마세요:\n\n# 참고문헌\n\n[1] 당신의 LLM 트윈 코스 — GitHub 저장소 (2024년), Decoding ML GitHub 조직\n\n[2] Meta에서 새로운 AI 경험 소개(2023년), Meta\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- [3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks\n\n- [4] Extract Transform Load (ETL), Databricks Glossary\n\n- [5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), Superlinked","ogImage":{"url":"/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png"},"coverImage":"/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png","tag":["Tech"],"readingTime":15},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h2: \"h2\",\n    p: \"p\",\n    img: \"img\",\n    ul: \"ul\",\n    li: \"li\",\n    h1: \"h1\",\n    ol: \"ol\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h2, {\n      children: \"LLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICA\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"→ LLM Twin 무료 코스의 첫 번째 강의\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"당신의 LLM Twin은 무엇인가요? LLM Twin은 당신의 스타일, 성격 및 목소리를 포함하여 당신처럼 쓰는 AI 캐릭터입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin_0.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"이 강의가 다른 이유는 무엇인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"무료 강의 \\\"LLM Twin: Building Your Production-Ready AI Replica\\\"를 완료하면 LLMs, 벡터 DB 및 LLMOps의 좋은 실천법에 의해 구동되는 스스로의 프로덕션 준비 AI 복제본을 설계, 훈련 및 배포하는 방법을 배울 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"이 강의를 통해 어떤 것을 배우게 되나요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 수집부터 배포까지 실제 LLM 시스템을 설계하고 구축하는 방법을 배우실 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MLOps의 최상의 실천법을 활용하는 방법을 배울 것입니다. 실험 추적기, 모델 레지스트리, 즉시 모니터링, 그리고 버전 관리 등이 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"최종 목표는 무엇일까요? 자신만의 LLM 쌍을 구축하고 배포하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM 쌍의 아키텍처는 4개의 파이썬 마이크로서비스로 나뉩니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"데이터 수집 파이프라인: 다양한 소셜 미디어 플랫폼에서 디지털 데이터를 수집합니다. ETL 파이프라인을 통해 데이터를 정리, 정규화하고 NoSQL DB에 로드합니다. CDC 패턴을 사용하여 데이터베이스 변경 사항을 큐로 전송합니다. (AWS에 배포됨)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"피처 파이프라인: 바이트왁스 스트리밍 파이프라인을 통해 큐에서 메시지를 소비합니다. 각 메시지는 실시간으로 정리되고 청크화되며 Superlinked를 사용하여 삽입되고 Qdrant 벡터 DB에 로드됩니다. (AWS에 배포됨)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"트레이닝 파이프라인: 디지털 데이터를 기반으로 사용자 정의 데이터세트를 생성합니다. QLoRA를 사용하여 LLM을 세밀하게 조정합니다. Comet ML의 실험 추적기를 사용하여 실험을 모니터링합니다. 최상의 모델을 Comet의 모델 레지스트리에 저장 및 평가합니다. (Qwak에 배포됨)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"인퍼런스 파이프라인: Comet의 모델 레지스트리에서 세밀하게 조정된 LLM을 로드하고 양자화합니다. 이를 REST API로 배포합니다. RAG를 사용하여 프롬프트를 향상시키고, LLM 쌍을 사용하여 콘텐츠를 생성합니다. Comet의 프롬프트 모니터링 대시보드를 사용하여 LLM을 모니터링합니다. (Qwak에 배포됨)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"4개의 마이크로서비스를 통해 3가지 서버리스 도구를 통합하는 방법을 배울 수 있습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"ML 플랫폼으로서의 Comet ML;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"벡터 DB로서의 Qdrant;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"ML 인프라로서의 Qwak;\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"누구를 위한 것인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"대상: MLE, DE, DS 또는 SWE로서, LLMOps의 좋은 원칙을 사용하여 제품 준비 상태의 LLM 시스템을 설계하고 싶은 분들을 대상으로 합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"수준: 중급\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"필수 조건: Python, ML 및 클라우드에 대한 기본 지식\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"어떻게 학습하시겠습니까?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 강의에는 11개의 실습 수업 및 GitHub에서 액세스할 수 있는 오픈 소스 코드가 포함되어 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모두 자신의 속도로 모든 것을 읽을 수 있어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"→ 이 과정을 최대한 효율적으로 활용하려면 강의를 따라가며 저장소를 복제하고 실행하는 것을 권장해요.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"비용은?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기사와 코드는 완전히 무료에요. 언제나 무료로 제공될 거에요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 코드를 실행하면서 읽으려면, 추가 비용이 발생할 수 있는 몇 가지 클라우드 도구를 사용한다는 것을 알아두어야 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"클라우드 컴퓨팅 플랫폼(AWS, Qwak)은 pay-as-you-go 요금제를 제공합니다. Qwak은 무료 컴퓨팅 시간을 제공합니다. 따라서 우리는 비용을 최소화하기 위해 최선을 다하였습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다른 서버리스 도구(Qdrant, Comet)의 경우, 무료로 사용할 수 있는 프리미엄 버전을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"선생님들을 만나보세요!\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Decoding ML 우산 아래에서 개설 된 이 과정을 개발 한 사람들은 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Paul Iusztin | 시니어 ML \u0026 MLOps 엔지니어\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Alex Vesa | 시니어 AI 엔지니어\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Alex Razvant | 시니어 ML \u0026 MLOps 엔지니어\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"수업\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 과정은 총 11개의 수업으로 구성되어 있습니다. 매체 기사 하나당 하나의 수업으로 나뉩니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"LLM 시스템의 제품용 엔드 투 엔드 프레임워크 구축을 통한 LLM Twin\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"생성 모델 AI 시대의 데이터 파이프라인의 중요성\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"변경 데이터 캡처: 이벤트 주도 아키텍처 가능\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"실시간으로 LLM 및 RAG의 파이썬 스트리밍 파이프라인의 뛰어난 솔루션\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"구현해야 할 4가지 고급 RAG 알고리즘\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"특징 저장소의 역할 LLM 세부 조정에\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"LLM 세부 조정 [모듈 3] ...작업 중\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"LLM 평가 [모듈 4] ...작업 중\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"양자화 [모듈 5] ...작업 중\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"디지털 트윈 추론 파이프라인 구축 [모듈 6] ...작업 중\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"REST API로 디지털 트윈 배포 [모듈 6] ...작업 중\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"첫 번째 레슨에서는 당신이 수강 기간 동안 구축할 프로젝트인 제품용 LLM Twin/AI 복제품을 소개할 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이후에는 3-파이프라인 디자인이 무엇인지와 이것이 표준 ML 시스템에 어떻게 적용되는지 설명할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로, LLM 프로젝트 시스템 디자인에 대해 자세히 살펴볼 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"소셜 미디어 데이터 수집 파이프라인 디자인에 대한 모든 아키텍처 결정과 LLM 마이크로서비스에 3-파이프라인 아키텍처를 적용하는 방법을 설명할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음 수업에서는 각 구성 요소의 코드를 검토하고 AWS 및 Qwak에 구현하고 배포하는 방법을 배울 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"목차\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"무엇을 구축할 계획인가요? LLM twin 개념\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"3-파이프라인 아키텍처\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"LLM twin 시스템 디자인\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"1. 무엇을 구축할 계획인가요? LLM twin 개념\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 과정의 목표는 당신만의 AI 레플리카를 구축하는 것입니다. 우리는 그것을 할 수 있도록 LLM을 사용할 것이며, 따라서 이 과정의 이름이 LLM Twin: 생산 준비가 완료된 AI 레플리카 구축입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM 쌍이 무엇인지 알고 싶으신가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"간단히 말씀드리면, LLM 쌍은 여러분과 비슷한 방식으로 글을 쓰는 인공지능 캐릭터가 될 거에요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"여러분 그 자신이 되는 게 아니라, 여러분의 글 스타일과 성격을 활용하는 쓰기 기계예요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"구체적으로 말하면, 여러분이 자신의 목소리로 소셜미디어 글이나 기술 기사(이렇게 작성된 것처럼)를 쓰는 AI 판본을 만드실 수 있을 거에요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"ChatGPT을 직접 사용하지 않는 이유가 무엇인가요? 궁금하시다면…\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM을 사용하여 기사나 글을 생성할 때 결과물이 다음과 같은 경향이 있습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"매우 일반적이고 미흡하게 나옵니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"허상으로 인한 잘못된 정보가 포함될 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"원하는 결과를 얻기 위해 번거로운 프롬프팅이 필요할 수 있습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 이런 문제를 해결하기 위해 우리가 할 일은 ↓↓↓\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"먼저, LinkedIn, Medium, Substack 및 GitHub에서 수집한 디지털 데이터로 LLM을 세밀하게 조정할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이를 통해 LLM은 당신의 쓰기 스타일과 온라인 개성과 일치하게 될 것입니다. LLM을 통해 당신 온라인 버전처럼 대화하는 법을 배울 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"2024년 Meta가 Messenger 앱에서 발표한 AI 캐릭터의 우주를 보신 적이 있나요? 만약 아직이라면, 여기 [2]에서 더 자세히 알아볼 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"어느 정도 그것이 우리가 구축하려는 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 우리의 사용 사례에서는 당신의 목소리를 반영하고 표현하는 소셜 미디어 게시물이나 글을 쓰는 LLM 쌍에 초점을 맞추겠습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, 우리는 당신의 LLM 쌍에게 LLM에 관한 LinkedIn 게시물을 작성하도록 요청할 수 있습니다. LLM에 관한 어떤 일반적이고 표현되지 않은 게시물(예: ChatGPT가 무엇을 할 것인지) 대신에 당신의 목소리와 스타일을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"두 번째로, 우리는 환각을 피하기 위해 외부 정보에 액세스하기 위해 LLM에게 벡터 DB에 액세스할 수 있게 할 것입니다. 따라서 LLM이 구체적인 데이터에 기반하여만 쓸 수 있도록 할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"최종적으로, 정보를 얻기 위해 벡터 DB에 액세스하는 것 외에도 생성 프로세스의 기본 블록 역할을 하는 외부 링크를 제공할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, 위의 예시를 다음과 같이 수정해 볼 수 있어요: \\\"이 링크의 기사를 기반으로 LLMs에 관한 1000단어 LinkedIn 게시물을 작성해주세요: [URL].\\\"\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기대되시나요? 시작해봅시다!🔥\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"2. 3단계 파이프라인 구조\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리 모두는 머신러닝 시스템이 얼마나 엉망이 될 수 있는지 알고 있어요. 이 때 3단계 파이프라인 구조가 필요해요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"3-파이프라인 디자인은 ML 시스템에 구조와 모듈성을 제공하면서 MLOps 프로세스를 개선합니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"문제점\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MLOps 도구의 발전에도 불구하고, 프로토타입에서 프로덕션으로의 전환은 여전히 어려움을 겪고 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"2022년에는 모델 중 54%만이 프로덕션 환경으로 이동한다고 합니다. 우웅.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그래서 무슨 일이 생길까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"입에 먼저 나오는 것은 아마도:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"모델이 충분히 성숙하지 않다\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"보안 위험(예: 데이터 개인 정보 보호)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"충분한 데이터가 없다\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"어느 정도는 이 사실이 맞습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 현실은 많은 시나리오에서...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"...ML 시스템의 아키텍처는 연구를 염두에 두고 구축되거나 ML 시스템이 오프라인에서 온라인으로 리팩터링하기 매우 어려운 거대한 단일체가 됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러므로 좋은 소프트웨어 엔지니어링 프로세스와 명확히 정의된 아키텍처가 적합한 도구와 높은 정확도의 모델 사용만큼 중요합니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"솔루션\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"→ 3-파이프라인 아키텍처\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"3-파이프라인 디자인이 무엇인지 알아봅시다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"개발 과정을 단순화하고, 당신의 단일 ML 파이프라인을 3가지 구성 요소로 나누는 데 도움이 되는 정신적인 지도입니다:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"피처 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"트레이닝 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"인퍼런스 파이프라인\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"...또한 피처/트레이닝/인퍼런스 (FTI) 아키텍처로 알려져 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"#1. 피처 파이프라인은 데이터를 피처와 레이블로 변환하여, 해당 내용을 피처 스토어에 저장하고 버전을 관리합니다. 피처 스토어는 피처들의 중앙 저장소로 기능하며, 피처들은 피처 스토어를 통해서만 액세스하고 공유할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"#2. 트레이닝 파이프라인은 피처 스토어에서 특정 버전의 피처와 레이블을 가져와서 훈련된 모델 가중치를 출력하며, 이러한 가중치는 모델 레지스트리에 저장되고 버전을 관리합니다. 모델은 모델 레지스트리를 통해서만 액세스하고 공유할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"#3. 추론 파이프라인은 피처 스토어에서 특정 버전의 피처를 사용하고 모델 레지스트리에서 특정 버전의 모델을 다운로드합니다. 최종 목표는 클라이언트에 예측을 출력하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이것이 3개의 파이프라인 디자인이 아름다운 이유입니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"직관적입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"모든 머신 러닝 시스템은 이 3가지 구성 요소로 축소될 수 있어 더 높은 수준에서 구조를 가져옵니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"3가지 구성 요소 간에 투명한 인터페이스를 정의하여 여러 팀이 협업하기 쉬워집니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"머신 러닝 시스템은 처음부터 모듈화를 염두에 두고 구축되었습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"필요에 따라 3가지 구성 요소를 여러 팀 사이로 쉽게 분할할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"각 구성 요소는 작업에 가장 적합한 기술 스택을 사용할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"각 구성 요소는 독립적으로 배포, 확장 및 모니터링할 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"피처 파이프라인은 배치, 스트리밍 또는 둘 다로 쉽게 구현할 수 있습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 가장 중요한 이점은...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"...이 패턴을 따라가면 여러분의 머신 러닝 모델이 노트북에서 제작 환경으로 옮겨질 것을 100% 확신할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"↳ 3-파이프라인 디자인에 대해 더 자세히 알고 싶으시다면, FTI 아키텍처의 창시자 중 한 명인 Jim Dowling이 작성한 훌륭한 [3] 글을 추천합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"3. LLM Twin System design\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM 시스템에 3-파이프라인 아키텍처를 어떻게 적용하는 지 알아봅시다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM twin의 아키텍처는 다음과 같이 4개의 Python 마이크로서비스로 구성됩니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"데이터 수집 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"특징 추출 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"훈련 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"추론 파이프라인\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"보시다시피, 데이터 수집 파이프라인은 3-파이프라인 디자인을 따르지 않습니다. 이게 사실이에요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그것은 ML 시스템 이전에 위치한 데이터 파이프라인을 나타냅니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 엔지니어링 팀이 주로 구현하며, 이 파이프라인은 대시보드 또는 ML 모델을 구축하는 데 필요한 데이터를 수집, 정리, 정규화하고 저장하는 것이 목표입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 작은 팀의 구성원이라고 하면, 데이터 수집부터 모델 배포까지 모든 것을 직접 구축해야 할 수도 있겠죠.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그렇기 때문에 데이터 파이프라인이 FTI 아키텍처와 어떻게 잘 맞고 상호 작용하는지를 보여 드리겠습니다. 이제 각 구성 요소를 자세히 살펴봐서 개별적으로 어떻게 작동하고 서로 상호 작용하는지 이해해 보겠습니다. ↓↓↓\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3.1. 데이터 수집 파이프라인\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그 범위는 주어진 사용자의 데이터를 크롤링하는 것입니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Medium (기사)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Substack (기사)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"LinkedIn (게시물)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"GitHub (코드)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"각 플랫폼마다 고유하므로, 우리는 각 웹사이트를 위해 다른 Extract Transform Load (ETL) 파이프라인을 구현했습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"🔗 ETL  파이프라인에 대한 1분 소요 읽기 [4]\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 각 플랫폼에 대한 기본 단계는 동일합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"따라서 각 ETL 파이프라인에 대해 다음과 같은 기본 단계를 추상화할 수 있습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"자격 증명을 사용하여 로그인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Selenium을 사용하여 프로필을 크롤링\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"HTML을 구문 분석하기 위해 Beautiful Soup 사용\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"추출된 HTML을 정리하고 표준화\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"정규화된 (그럼에도 불구하고 원시) 데이터를 Mongo DB에 저장\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"중요 사항: 개인 정보 보호 문제로 인해 대다수 플랫폼에서 다른 사람의 데이터에 접근할 수 없기 때문에 우리는 단지 우리 자신의 데이터만을 수집합니다. 그러나 이는 우리에게 완벽한 선택입니다. LLM 트윈을 구축하기 위해서는 우리 자신의 디지털 데이터만 필요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"왜 Mongo DB를 사용할까요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 텍스트와 같이 구조화되지 않은 데이터를 빠르게 저장할 수 있는 NoSQL 데이터베이스를 원했습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 파이프라인은 피쳐 파이프라인과 어떻게 통신할건가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 모든 Mongo DB의 변경 사항을 피쳐 파이프라인에 알리기 위해 Change Data Capture (CDC) 패턴을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"🔗 CDC 패턴에 대한 1분 간의 읽기 [5]\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CDC를 간략히 설명하자면, 감시자는 Mongo DB에 발생하는 모든 CRUD 작업을 24/7 감지합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"감시자는 수정된 내용을 알려주는 이벤트를 발생시킵니다. 이 이벤트를 RabbitMQ 큐에 추가할 거에요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능 파이프라인은 계속해서 큐를 듣고, 메시지를 처리하여 Qdrant vector DB에 추가할 거에요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, 우리가 Mongo DB에 새 문서를 작성할 때, 감시자는 새 이벤트를 생성합니다. 이벤트가 RabbitMQ 큐에 추가되고, 최종적으로 기능 파이프라인이 소비하고 처리합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이를 통해 Mongo DB와 Vector DB가 항상 동기화되도록 보장할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CDC 기술을 사용하면, 일괄 ETL 파이프라인(데이터 파이프라인)에서 스트리밍 파이프라인(특징 파이프라인)으로 전환합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CDC 패턴을 사용하면, Mongo DB와 vector DB 간의 차이를 계산하기 위한 복잡한 일괄 파이프라인을 구현하는 것을 피할 수 있습니다. 이 접근 방식은 대규모 데이터를 처리할 때 빠르게 느려질 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 파이프라인은 어디에 배포될 것인가요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 수집 파이프라인과 RabbitMQ 서비스는 AWS에 배포될 예정입니다. 또한 MongoDB의 프리미엄 서버리스 버전을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3.2. 기능 파이프라인\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능 파이프라인은 Bytewax를 사용하여 구현되었습니다 (Python 인터페이스를 갖춘 Rust 스트리밍 엔진). 따라서, 우리의 특정 사용 사례에서는 이를 스트리밍 입력 파이프라인으로도 참조할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이것은 데이터 수집 파이프라인과 완전히 다른 서비스입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 파이프라인과 어떻게 통신하나요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이전 설명대로, 기능 파이프라인은 RabbitMQ 큐를 통해 데이터 파이프라인과 통신합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"현재, 스트리밍 파이프라인은 데이터가 어떻게 생성되었는지나 어디에서 왔는지에 관심이 없습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그저 특정 큐를 듣고, 그곳에서 메시지를 소비하고 처리해야 한다는 것만 알고 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이렇게 하면 두 구성 요소를 완전히 분리할 수 있습니다. 미래에는 여러 소스에서 메시지를 큐에 쉽게 추가할 수 있으며, 스트리밍 파이프라인이 이를 어떻게 처리해야 하는지 알게 될 것입니다. 유일한 규칙은 큐에 있는 메시지가 항상 동일한 구조/인터페이스를 준수해야 한다는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능 파이프라인의 범위는 무엇인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이것은 RAG 시스템의 인계 구성 요소를 나타냅니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"큐를 통해 전달된 원시 데이터를 가져 와서 다음과 같은 작업을 수행할 것입니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"데이터 정리하기;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"청크로 나누기;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Superlinked의 임베딩 모델을 사용해 임베딩하기;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Qdrant 벡터 DB에 로드하기.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"각 유형의 데이터(게시물, 기사, 코드)는 각자의 클래스 세트를 통해 독립적으로 처리됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모두 텍스트 기반이지만, 각 데이터 유형마다 독특한 특징이 있기 때문에 데이터를 정리, 청크화, 임베딩하는 데 각기 다른 전략을 사용해야 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"어떤 종류의 데이터가 저장될 것인가요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"학습 파이프라인은 피쳐 스토어에만 액세스할 수 있습니다. 우리의 경우, Qdrant 벡터 DB로 표현됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"벡터 DB는 NoSQL DB로 사용할 수도 있다는 것을 기억하세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 두 가지를 염두에 두고, 우리는 Qdrant에 데이터의 2개 스냅샷을 저장할 것입니다:\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"정제된 데이터(인덱스로 벡터를 사용하지 않고 NoSQL 방식으로 저장).\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"정리된, 청크 처리된 및 내장된 데이터 (Qdrant의 벡터 인덱스를 활용)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"학습 파이프라인은 표준 및 보강 프롬프트에서 LLM을 세밀 조정하고자 하므로 두 형식의 데이터에 액세스해야 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"정리된 데이터로 프롬프트 및 답변을 생성할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"청크 처리된 데이터로는 프롬프트를 보강할 것입니다 (일명 RAG).\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"스트리밍 파이프라인을 배치 파이프라인 대신 구현해야 하는 이유는 무엇인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그 이유는 주로 2가지가 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"첫 번째 이유는 CDC 패턴과 결합해서 서로 다른 두 개의 데이터베이스를 동기화하는 가장 효율적인 방법이기 때문입니다. 그렇지 않으면 대규모 데이터를 처리할 때 확장 가능하지 않은 배치 폴링 또는 푸싱 기술을 구현해야 할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CDC + 스트리밍 파이프라인을 사용하면 소스 데이터베이스의 변경 사항만 처리하고 여분의 작업이 발생하지 않습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"두 번째 이유는 그렇게 함으로써 소스와 벡터 데이터베이스가 항상 동기화된 상태가 유지됩니다. 따라서 RAG를 수행할 때 최신 데이터에 항상 액세스할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"왜 Bytewax를 사용해야 하는가?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Bytewax는 Python 인터페이스를 노출하는 Rust로 구축된 스트리밍 엔진입니다. Bytewax를 사용하는 이유는 Rust의 놀라운 속도와 신뢰성을 파이썬의 사용 편의성과 생태계와 결합시킨 점에 있습니다. Python 개발자에게는 매우 가볍고 강력하며 사용하기 쉽습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능 파이프라인은 어디에 배포될 것인가요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능 파이프라인이 AWS에 배포될 예정입니다. 또한 우리는 Qdrant의 무료 서버리스 버전을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3.3. 훈련 파이프라인\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 기능에 접근할 수 있는 방법은 무엇인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"3.2절에서 강조한 대로, 모든 훈련 데이터는 기능 저장소에서 접근할 수 있습니다. 저희 경우에는 기능 저장소인 Qdrant 벡터 DB에 다음과 같은 데이터가 포함됩니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"우리는 프롬프트 및 답변을 생성할 정돈된 디지털 데이터를 사용할 것입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"RAG를 위해 청크와 임베디드된 데이터를 사용하여 정돈된 데이터를 보완할 것입니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 주요 데이터 유형(게시물, 기사, 코드)마다 다른 벡터 DB 검색 클라이언트를 구현할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"각 유형의 고유한 특성 때문에 벡터 DB를 쿼리하기 전에 각 유형을 다르게 전처리해야 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한, 우리는 각 클라이언트에 대해 벡터 DB에서 어떤 것을 쿼리하고 싶은지에 기반한 사용자 정의 동작을 추가할 것입니다. 그러나 이에 대해 자세히는 해당 레슨에서 설명하겠습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 파이프라인은 무엇을 할까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 파이프라인에는 벡터 DB로부터 검색된 데이터를 전처리하여 프롬프트로 변환하는 데이터-투-프롬프트 레이어가 포함되어 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한 HuggingFace 데이터셋을 입력으로 사용하고 QLoRA를 사용하여 주어진 LLM(예: Mistral)을 세밀하게 튜닝하는 LLM 세밀 조정 모듈이 포함될 것입니다. HuggingFace를 사용함으로써 다양한 LLM 사이를 쉽게 전환할 수 있기 때문에 특정 LLM에 너무 많은 집중이 필요하지 않습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모든 실험은 Comet ML의 실험 추적기에 로그가 남겨질 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"저희는 미세 조정된 LLM의 결과를 평가하기 위해 더 큰 LLM(예: GPT4)을 사용할 것입니다. 이러한 결과는 Comet의 실험 추적기에 기록될 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"생산용 후보 LLM은 어디에 저장될까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 여러 실험을 비교한 뒤 최적의 결과를 선택하여 모델 레지스트리를 위한 LLM 생산용 후보를 발표할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이후, Comet의 프롬프트 모니터링 대시보드를 사용하여 LLM 생산용 후보를 수동으로 검토할 것입니다. 최종 수동 검사가 통과되면, 우리는 모델 레지스트리의 LLM을 수락된 상태로 표시할 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CI/CD 파이프라인이 실행되어 새로운 LLM 버전이 추론 파이프라인에 배포될 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 파이프라인은 어디에 배포될까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 파이프라인은 Qwak에 배포될 예정입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Qwak은 ML 모델을 훈련하고 배포하는 서버리스 솔루션입니다. 작업 확장을 쉽게 할 수 있으며 건설에 집중할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위 내용은 다음을 위해 Comet ML의 프리미엄 버전을 사용할 것입니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"실험 추적기;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"모델 레지스트리;\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"실시간 감시.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"3.4. 추론 파이프라인\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"추론 파이프라인은 LLM 시스템의 최종 구성 요소입니다. 클라이언트가 상호 작용할 구성 요소입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이것은 REST API 아래에 랩핑될 것입니다. 클라이언트들은 HTTP 요청을 통해 이를 호출할 수 있으며, 이는 ChatGPT나 비슷한 도구들과 유사한 경험입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능에 어떻게 접근할까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기능 저장소에 접근하기 위해, 훈련 파이프라인에서와 같이 Qdrant 벡터 DB 검색 클라이언트들을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 경우 RAG를 수행하기 위해 청크 데이터에 접근하는데 기능 저장소가 필요할 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"얼마나 미세 조정된 LLM에 액세스할 수 있나요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"미세 조정된 LLM은 항상 해당 태그(예: accepted)와 버전(예: v1.0.2, latest 등)을 기반으로 모델 레지스트리에서 다운로드됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"미세 조정된 LLM은 어떻게 로드되나요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 여기서 추론 세계에 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM의 속도와 메모리 사용량을 최대한 최적화하려고 합니다. 그래서 모델 레지스트리에서 LLM을 다운로드한 후 양자화할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"추론 파이프라인의 구성 요소는 무엇인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"첫 번째는 RAG를 수행하기 위해 벡터 데이터베이스에 액세스하는 검색 클라이언트입니다. 이는 교육 파이프라인에서 사용된 모듈과 동일합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Qdrant에서 검색된 도큐먼트를 프롬프트로 매핑할 쿼리가 있으면 해당 쿼리 후보를 매핑해줄 계층이 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM이 답변을 생성한 후, 우리는 Comet의 프롬프트 모니터링 대시보드에 기록하고 클라이언트에게 반환할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, 클라이언트는 추론 파이프라인에 다음을 요청할 수 있습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"\\\"LLM에 관한 1000단어의 LinkedIn 게시물 작성\\\", 그리고 추론 파이프라인은 생성된 게시물을 반환하기 위해 위에서 설명한 모든 단계를 거칠 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"추론 파이프라인은 어디에 배포될 것인가요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"추론 파이프라인은 Qwak로 배포됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기본 설정으로, Qwak은 자동 확장 솔루션과 생산 환경 자원을 모니터링하는 멋진 대시보드도 제공합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 파이프라인에 대해서는, 우리는 실시간 모니터링 대시보드를 제공하는 Comet의 서버리스 프리미엄 버전을 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"LLM Twin의 무료 코스 1번째 기사입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 강의에서는 이 코스 동안 구축할 내용을 소개했습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"간단히 ML 시스템을 설계하는 방법에 대해 논의한 후\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"최종적으로 이 코스의 시스템 디자인을 살펴보고 각 마이크로서비스의 아키텍처 및 서로 상호작용 방법을 소개했습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"데이터 수집 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"피쳐 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"훈련 파이프라인\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"추론 파이프라인\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"제2 장에서는 데이터 수집 파이프라인을 더 자세히 살펴보고, 다양한 소셜 미디어 플랫폼에 크롤러를 구현하는 방법을 배우고, 수집한 데이터를 정리하여 Mongo DB에 저장하고, 마지막으로 AWS에 배포하는 방법을 안내해 드릴 거에요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 기사를 즐겁게 보셨나요? 그렇다면...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"↓↓↓\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"5천 명 이상의 엔지니어들과 함께하여, 프로덕션급 머신 러닝에 대한 검증된 컨텐츠를 살펴보세요. 매주 업데이트되는 콘텐츠를 놓치지 마세요:\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"참고문헌\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[1] 당신의 LLM 트윈 코스 — GitHub 저장소 (2024년), Decoding ML GitHub 조직\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"[2] Meta에서 새로운 AI 경험 소개(2023년), Meta\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[4] Extract Transform Load (ETL), Databricks Glossary\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), Superlinked\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-AnEnd-to-EndFrameworkforProduction-ReadyLLMSystemsbyBuildingYourLLMTwin"},"buildId":"PgdIX9e0tvkvkdAmDT6qR","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>