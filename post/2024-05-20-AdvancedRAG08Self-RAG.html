<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>고급 RAG 08 Self-RAG | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-20-AdvancedRAG08Self-RAG" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="고급 RAG 08 Self-RAG | ui-station" data-gatsby-head="true"/><meta property="og:title" content="고급 RAG 08 Self-RAG | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-20-AdvancedRAG08Self-RAG" data-gatsby-head="true"/><meta name="twitter:title" content="고급 RAG 08 Self-RAG | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-20 21:10" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_buildManifest.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">고급 RAG 08 Self-RAG</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="고급 RAG 08 Self-RAG" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 20, 2024</span><span class="posts_reading_time__f7YPP">13<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-20-AdvancedRAG08Self-RAG&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>이 기사는 흔한 시나리오로 시작됩니다: 공개 시험을 보는 경우입니다. 일반적으로 두 가지 전략을 사용합니다:</p>
<ul>
<li>방법 1: 익숙한 주제에 대해서는 빠르게 답변하고, 익숙하지 않은 주제에 대해서는 참고서를 열어서 확인하고, 관련 부분을 빠르게 찾아내어 정리하고 요약한 다음, 시험지에 답변합니다.</li>
<li>방법 2: 모든 주제에 대해 책을 참고합니다. 적절한 부분을 찾아내고, 정리하고 요약한 다음, 시험지에 답변합니다.</li>
</ul>
<p>분명히 방법 1이 선호되는 방법입니다. 방법 2는 시간이 소비될 수 있고, 관련성 없는 정보나 잘못된 정보가 들어올 수 있어 혼란과 실수를 야기할 수 있습니다. 심지어 처음에 이해한 부분에서도 발생할 수 있습니다.</p>
<p>하지만, 방법 2는 고전적인 RAG 프로세스를 보여주며, 방법 1은 자체 RAG 프로세스를 대표합니다. 이에 대해 이 기사에서 더 자세히 다룰 것입니다.</p>
<div class="content-ad"></div>
<h1>개요</h1>
<p>그림 1은 RAG 및 Self-RAG의 주요 프로세스를 비교한 것을 보여줍니다:</p>
<p><img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png" alt="그림"/></p>
<p>Self-RAG는 세 단계로 구성되어 있습니다:</p>
<div class="content-ad"></div>
<ul>
<li>필요한 경우 검색: 모델이 검색을 요구하는 경우, 예를 들어 &quot;미국 주가 이름을 어떻게 얻었습니까?&quot; (그림 1의 오른쪽 상단)와 같은 쿼리가 있을 때, 모델의 출력에는 [검색] 토큰이 포함됩니다. 이는 쿼리와 관련된 내용을 검색해야 함을 나타냅니다. 반면에 &quot;최고의 여름 휴가에 대해 에세이를 쓰세요&quot; (그림 1의 오른쪽 아래)와 같이 물어볼 때, 모델은 검색 없이 직접 답변을 생성하도록 선택합니다.</li>
<li>병렬 생성: 모델은 프롬프트와 검색된 콘텐츠를 모두 사용하여 출력을 생성합니다. 이 과정에서 세 가지 유형의 반영 토큰이 검색된 콘텐츠의 관련성을 나타냅니다.</li>
<li>평가 및 선택: 단계 2에서 생성된 콘텐츠가 평가되고, 최상의 세그먼트가 출력으로 선택됩니다.</li>
</ul>
<p>상기 모델은 특별히 훈련된 모델이라는 것을 유의하십시오. 이 모델의 훈련 과정은 이 기사의 후반부에서 논의될 것입니다.</p>
<h1>반영 토큰</h1>
<p>Self-RAG 프레임워크의 RAG와 비교했을 때, Self-RAG 프레임워크의 차이는 생성 중 더 정확한 제어를 위해 반영 토큰을 사용한다는 것입니다. 그림 2에서 보여집니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_1.png"/>
<p>본질적으로, self-RAG는 네 가지 명확한 판단을 내립니다:</p>
<ul>
<li>[Retrieve]: 리소스 R로부터 정보를 검색할지를 결정하는 의사결정 과정.</li>
<li>[IsREL]: 주어진 데이터 d가 문제 x를 해결하는 데 필요한 정보를 포함하고 있는지를 결정하는 관련성 확인.</li>
<li>[IsSUP]: 제공된 응답 y의 내용이 데이터 d로부터 지원되는지를 확인하는 검증 과정.</li>
<li>[IsUSE]: 문제 x에 대한 응답 y의 유용성을 평가하는 평가 과정. 결과는 1에서 5까지의 점수로, 5는 가장 높은 유용성을 나타냅니다.</li>
</ul>
<p>RAG에서 검색은 상태에 관계없이 항상 처음에 수행되는 고정된 과정입니다. 반면 self-RAG는 반사 토큰을 도입하여 LLM을 더 적응적이고 지능적으로 만듭니다. LLM이 텍스트를 생성하다가 불확실성이 발생하는 부분에 도달하면 반사 토큰에서 일시 정지하여 신속하고 정확한 검색을 수행한 후 새로 습득한 정보를 사용하여 생성을 재개합니다.</p>
<div class="content-ad"></div>
<h1>코드 설명</h1>
<p>self-RAG 프로세스를 직관적으로 이해하기 위해 먼저 코드를 살펴보고 모델의 훈련 과정을 설명하겠습니다.</p>
<p>self-RAG는 오픈 소스이며, Langchain과 LlamaIndex에는 각각의 구현이 있습니다. 우리는 설명을 위해 LlamaIndex의 구현을 참조할 것입니다.</p>
<h2>환경 설정</h2>
<div class="content-ad"></div>
<p>먼저, 환경을 설정하세요.</p>
<pre><code class="hljs language-js">(base) <span class="hljs-title class_">Florian</span>@instance-<span class="hljs-number">1</span>:~$ conda create -n llamaindex python=<span class="hljs-number">3.11</span>

(base) <span class="hljs-title class_">Florian</span>@instance-<span class="hljs-number">1</span>:~$ conda activate llamaindex


(llamaindex) <span class="hljs-title class_">Florian</span>@instance-<span class="hljs-number">1</span>:~$ pip install llama-index

(llamaindex) <span class="hljs-title class_">Florian</span>@instance-<span class="hljs-number">1</span>:~$ pip install huggingface-hub

(llamaindex) <span class="hljs-title class_">Florian</span>@instance-<span class="hljs-number">1</span>:~$ huggingface-cli login
</code></pre>
<p>설치 후, LlamaIndex의 대응 버전은 다음과 같습니다:</p>
<pre><code class="hljs language-js">llama-index                             <span class="hljs-number">0.10</span><span class="hljs-number">.20</span>

llama-index-core                        <span class="hljs-number">0.10</span><span class="hljs-number">.20</span>.<span class="hljs-property">post2</span>
</code></pre>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> os
os.<span class="hljs-property">environ</span>[<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>] = <span class="hljs-string">&quot;여러분의 오픈AI API 키&quot;</span>

<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Document</span>, <span class="hljs-title class_">VectorStoreIndex</span>
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span>.<span class="hljs-property">retrievers</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">VectorIndexRetriever</span>
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span>.<span class="hljs-property">readers</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SimpleDirectoryReader</span>
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> <span class="hljs-title class_">Path</span>


# 옵션: <span class="hljs-title class_">SelfRAGPack</span> 다운로드
# 첫 실행 시 <span class="hljs-title class_">SelfRAGPack</span>을 다운로드해야 합니다. 
# 다음 실행부터는 이 부분을 주석 처리할 수 있습니다.
<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">core</span>.<span class="hljs-property">llama_pack</span> <span class="hljs-keyword">import</span> download_llama_pack
<span class="hljs-title function_">download_llama_pack</span>(
    <span class="hljs-string">&quot;SelfRAGPack&quot;</span>,
    <span class="hljs-string">&quot;./self_rag_pack&quot;</span>)

<span class="hljs-keyword">from</span> llama_index.<span class="hljs-property">packs</span>.<span class="hljs-property">self_rag</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SelfRAGQueryEngine</span>

# 이전에 다운로드하고 저장한 <span class="hljs-title class_">Llama2</span> 모델이 있는 디렉토리.
download_dir = <span class="hljs-string">&quot;여러분의 다운로드 모델 디렉토리&quot;</span>

# 테스트 문서 생성
documents = [
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;남극 얼음 위를 &#x27;웨들&#x27;이라고 불리는 물개 떼가 지나다녔다. 그들의 턱시도 같은 깃털은 눈 위에서 돋보였다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;펭귄 중 가장 키가 큰 황제펭귄은 다른 어떤 새보다도 더 깊이 다이빙을 할 수 있어서 500m 이상의 심해까지 다이빙을 합니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;펭귄들의 흑백색깔은 위험 방어라는 화장법의 한 종류인 카운터셰이딩입니다. 위에서 보면 펭귄의 검은 등은 바다 심지와 어우러지고, 아래에서는 펭귄의 흰 배는 밝은 표면과 어우러집니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;수직 자세이지만, 펭귄은 날지 못하는 조류입니다. 그들의 날개는 지느러미로 진화했기 때문에 수중에서 전문 수영가입니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;가장 빠른 펭귄 종류인 젠투 펭귄은 시속 36킬로미터까지 수영할 수 있으며, 수중을 순찰하는 동안 지느러미와 윤곽을 이용해 물을 가르는 식으로 전진합니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;펭귄은 집단생활을 하는 조류입니다. 많은 종들이 번식을 위해 수만 마리까지 이를 결성합니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;펭귄은 놀랍게도 귀가 우수하며 지저분한 떼 속에서 배우량과 새끼를 식별하는 데 명확한 호출을 의존합니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;가장 작은 펭귄 종인 리틀 블루 펭귄은 약 40cm 높이로, 남부 호주와 뉴질랜드 해안가에서 발견됩니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;번식 기간 중, 수컷 황제펭귄은 한없이 지속되는 남극 겨울을 버텨내며 몇 달간 급식없이 알을 부화시키는 반면, 암컷은 바다에서 사냥을 합니다.&quot;</span>
    ),
    <span class="hljs-title class_">Document</span>(
        text=<span class="hljs-string">&quot;펭귄은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 그리고 크릴로 이루어져 있으며 이를 수중 다이빙을 통해 잡습니다.&quot;</span>
    ),
]

index = <span class="hljs-title class_">VectorStoreIndex</span>.<span class="hljs-title function_">from_documents</span>(documents)

# 간단한 리트리버 설정
retriever = <span class="hljs-title class_">VectorIndexRetriever</span>(
    index=index,
    similarity_top_k=<span class="hljs-number">10</span>,
)


model_path = <span class="hljs-title class_">Path</span>(download_dir) / <span class="hljs-string">&quot;selfrag_llama2_7b.q4_k_m.gguf&quot;</span>
query_engine = <span class="hljs-title class_">SelfRAGQueryEngine</span>(<span class="hljs-title function_">str</span>(model_path), retriever, verbose=<span class="hljs-title class_">True</span>)

# 리트리벌 예시
response = query_engine.<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;어떤 장르인가요?&quot;</span>)

# 리트리벌 예시
response = query_engine.<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;가장 작은 펭귄의 키는 얼마인가요?&quot;</span>)
</code></pre>
<p>위의 테스트 코드는 다음 결과를 생성했습니다(대부분의 llama_cpp 디버깅 정보가 제거되었습니다):</p>
<pre><code class="hljs language-js">...
...
<span class="hljs-title class_">Model</span> <span class="hljs-attr">metadata</span>: {<span class="hljs-string">&#x27;tokenizer.ggml.add_eos_token&#x27;</span>: <span class="hljs-string">&#x27;false&#x27;</span>, <span class="hljs-string">&#x27;tokenizer.ggml.eos_token_id&#x27;</span>: <span class="hljs-string">&#x27;2&#x27;</span>, <span class="hljs-string">&#x27;general.architecture&#x27;</span>: <span class="hljs-string">&#x27;llama&#x27;</span>, <span class="hljs-string">&#x27;llama.rope.freq_base&#x27;</span>: <span class="hljs-string">&#x27;10000.000000&#x27;</span>, <span class="hljs-string">&#x27;llama.context_length&#x27;</span>: <span class="hljs-string">&#x27;4096&#x27;</span>, <span class="hljs-string">&#x27;general.name&#x27;</span>: <span class="hljs-string">&#x27;LLaMA v2&#x27;</span>, <span class="hljs-string">&#x27;tokenizer.ggml.add_bos_token&#x27;</span>: <span class="hljs-string">&#x27;true&#x27;</span>, <span class="hljs-string">&#x27;llama.embedding_length&#x27;</span>: <span class="hljs-string">&#x27;4096&#x27;</span>, <span class="hljs-string">&#x27;llama.feed_forward_length&#x27;</span>: <span class="hljs-string">&#x27;11008&#x27;</span>, <span class="hljs-string">&#x27;llama.attention.layer_norm_rms_epsilon&#x27;</span>: <span class="hljs-string">&#x27;0.000010&#x27;</span>, <span class="hljs-string">&#x27;llama.rope.dimension_count&#x27;</span>: <span class="hljs-string">&#x27;128&#x27;</span>, <span class="hljs-string">&#x27;tokenizer.ggml.bos_token_id&#x27;</span>: <span class="hljs-string">&#x27;1&#x27;</span>, <span class="hljs-string">&#x27;llama.attention.head_count&#x27;</span>: <span class="hljs-string">&#x27;32&#x27;</span>, <span class="hljs-string">&#x27;llama.block_count&#x27;</span>: <span class="hljs-string">&#x27;32&#x27;</span>, <span class="hljs-string">&#x27;llama.attention.head_count_kv&#x27;</span>: <span class="hljs-string">&#x27;32&#x27;</span>, <span class="hljs-string">&#x27;general.quantization_version&#x27;</span>: <span class="hljs-string">&#x27;2&#x27;</span>, <span class="hljs-string">&#x27;tokenizer.ggml.model&#x27;</span>: <span class="hljs-string">&#x27;llama&#x27;</span>, <span class="hljs-string">&#x27;general.file_type&#x27;</span>: <span class="hljs-string">&#x27;15&#x27;</span>}
<span class="hljs-title class_">Using</span> fallback chat <span class="hljs-attr">format</span>: <span class="hljs-title class_">None</span>

<span class="hljs-attr">llama_print_timings</span>:        load time =    <span class="hljs-number">4887.53</span> ms
<span class="hljs-attr">llama_print_timings</span>:      sample time =      <span class="hljs-number">11.29</span> ms /    <span class="hljs-number">22</span> runs   (    <span class="hljs-number">0.51</span> ms per token,  <span class="hljs-number">1947.76</span> tokens per second)
<span class="hljs-attr">llama_print_timings</span>: prompt <span class="hljs-built_in">eval</span> time =    <span class="hljs-number">4887.46</span> ms /    <span class="hljs-number">24</span> tokens (  <span class="hljs-number">203.64</span> ms per token,     <span class="hljs-number">4.91</span> tokens per second)
<span class="hljs-attr">llama_print_timings</span>:        <span class="hljs-built_in">eval</span> time =    <span class="hljs-number">5883.27</span> ms /    <span class="hljs-number">21</span> runs   (  <span class="hljs-number">280.16</span> ms per token,     <span class="hljs-number">3.57</span> tokens per second)
<span class="hljs-attr">llama_print_timings</span>:       total time =   <span class="hljs-number">10901.84</span> ms /    <span class="hljs-number">45</span> tokens
최종 답변: <span class="hljs-string">&#x27;오만과 편견&#x27;</span>은 제인 오스틴의 로맨스 소설입니다.
...
...
<span class="hljs-attr">llama_print_timings</span>:        load time =    <span class="hljs-number">4887.53</span> ms
<span class="hljs-attr">llama_print_timings</span>:      sample time =      <span class="hljs-number">11.74</span> ms /    <span class="hljs-number">20</span> runs   (    <span class="hljs-number">0.59</span> ms per token,  <span class="hljs-number">1703.29</span> tokens per second)
<span class="hljs-attr">llama_print_timings</span>: prompt <span class="hljs-built_in">eval</span> time =    <span class="hljs-number">7473.66</span> ms /    <span class="hljs-number">37</span> tokens (  <span class="hljs-number">201.99</span> ms per token,     <span class="hljs-number">4.95</span> tokens per second)
<span class="hljs-attr">llama_print_timings</span>:        <span class="hljs-built_in">eval</span> time =    <span class="hljs-number">5414.34</span> ms /    <span class="hljs-number">19</span> runs   (  <span class="hljs-number">284.96</span> ms per token,     <span class="hljs-number">3.51</span> tokens per second)
<span class="hljs-attr">llama_print_timings</span>:       total time =   <span class="hljs-number">13076.88</span> ms /    <span class="hljs-number">56</span> tokens
입력: ### 지시사항:
가장 작은 펭귄은 얼마나 키가 큰가요?

### 응답:
[검색]&lt;문단&gt;펭귄들은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 크릴로 구성되어 있으며 이를 다이빙으로 잡습니다.<span class="hljs-string">&quot;&lt;/문단&gt;
예측: [관련]가장 작은 펭귄 종류의 키는 종에 따라 달라질 수 있습니다.[지원되지 않음 / 모순][유틸리티:5]
점수: 1.4213598342974367
10/10 단락 완료

평가 종료
최상의 답변 선정: [관련]가

&lt;div class=&quot;</span>content-ad<span class="hljs-string">&quot;&gt;&lt;/div&gt;

테스트 코드를 이해하는 핵심은 SelfRAGQueryEngine 클래스의 구현에 있습니다. 이제 이 클래스를 자세히 살펴보겠습니다.

## 클래스 SelfRAGQueryEngine

먼저 생성자입니다. 주로 llama_cpp를 사용하여 Llama2-7B 모델을 로드하기 위해 사용됩니다.

```python
class SelfRAGQueryEngine(CustomQueryEngine):
    &quot;</span><span class="hljs-string">&quot;&quot;</span>간단한 <span class="hljs-title class_">Self</span> <span class="hljs-variable constant_">RAG</span> 쿼리 엔진.<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;

    llm: Any = Field(default=None, description=&quot;</span>llm<span class="hljs-string">&quot;)
    retriever: BaseRetriever = Field(default=None, description=&quot;</span>retriever<span class="hljs-string">&quot;)
    generate_kwargs: Dict = Field(default=None, description=&quot;</span>llm generation <span class="hljs-variable language_">arguments</span><span class="hljs-string">&quot;)
    verbose: bool = Field(default=True, description=&quot;</span><span class="hljs-title class_">Verbose</span>.<span class="hljs-string">&quot;)

    def __init__(
        self,
        model_path: str,
        retriever: BaseRetriever,
        verbose: bool = False,
        model_kwargs: Dict = None,
        generate_kwargs: Dict = None,
        **kwargs: Any,
    ) -&gt; None:
        &quot;</span><span class="hljs-string">&quot;&quot;</span>매개변수 초기화.<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;
        super().__init__(verbose=verbose, **kwargs)
        model_kwargs = model_kwargs or _MODEL_KWARGS
        self.generate_kwargs = generate_kwargs or _GENERATE_KWARGS
        try:
            from llama_cpp import Llama
        except ImportError:
            raise ImportError(_IMPORT_ERROR_MSG)
        self.llm = Llama(model_path=model_path, verbose=verbose, **model_kwargs)
        self.retriever = retriever
</span></code></pre>
<div class="content-ad"></div>
<p>그 다음으로 쿼리 기능에 대해 설명하겠습니다. 주요 프로세스는 아래 그림 3에 표시되어 있습니다:</p>
<p><img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_2.png" alt="Image"/></p>
<p>이해를 돕기 위해 주요 부분에는 주석이 달려 있습니다.</p>
<pre><code class="hljs language-python">    <span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_query</span>(<span class="hljs-params">self, query_str: <span class="hljs-built_in">str</span></span>) -&gt; Response:
        <span class="hljs-string">&quot;&quot;&quot;커스텀 쿼리 실행.&quot;&quot;&quot;</span>
        <span class="hljs-comment"># Llama2 모델을 사용하여 응답을 가져옵니다.</span>
        response = self.llm(prompt=_format_prompt(query_str), **_GENERATE_KWARGS)
        answer = response[<span class="hljs-string">&quot;choices&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>]
        source_nodes = []

        <span class="hljs-comment"># 검색이 필요한지 여부를 결정합니다.</span>
        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;[Retrieval]&quot;</span> <span class="hljs-keyword">in</span> answer:
            <span class="hljs-keyword">if</span> self.verbose:
                print_text(<span class="hljs-string">&quot;검색이 필요합니다\n&quot;</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)
            <span class="hljs-comment"># 그림 1의 단계 1, 필요한대로 검색합니다.</span>
            documents = self.retriever.retrieve(query_str)
            <span class="hljs-keyword">if</span> self.verbose:
                print_text(<span class="hljs-string">f&quot;받은 문서: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(documents)}</span>\n&quot;</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)
            paragraphs = [
                _format_prompt(query_str, document.node.text) <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> documents
            ]

            <span class="hljs-keyword">if</span> self.verbose:
                print_text(<span class="hljs-string">&quot;평가 시작\n&quot;</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)

            <span class="hljs-comment"># 그림 1의 단계 2 및 3, 병렬로 생성하고 평가합니다 </span>
            <span class="hljs-comment"># (코드에서 병렬화를 구현하지는 않음)</span>
            critic_output = self._run_critic(paragraphs)

            paragraphs_final_score = critic_output.paragraphs_final_score
            llm_response_per_paragraph = critic_output.llm_response_per_paragraph
            source_nodes = critic_output.source_nodes

            <span class="hljs-keyword">if</span> self.verbose:
                print_text(<span class="hljs-string">&quot;평가 종료\n&quot;</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)

            <span class="hljs-comment"># 가장 높은 점수를 받은 답변을 선택하고 반환합니다.</span>
            best_paragraph_id = <span class="hljs-built_in">max</span>(
                paragraphs_final_score, key=paragraphs_final_score.get
            )
            answer = llm_response_per_paragraph[best_paragraph_id]
            <span class="hljs-keyword">if</span> self.verbose:
                print_text(<span class="hljs-string">f&quot;최적 답변 선택: <span class="hljs-subst">{answer}</span>\n&quot;</span>, color=<span class="hljs-string">&quot;blue&quot;</span>)

        answer = _postprocess_answer(answer)
        <span class="hljs-keyword">if</span> self.verbose:
            print_text(<span class="hljs-string">f&quot;최종 답변: <span class="hljs-subst">{answer}</span>\n&quot;</span>, color=<span class="hljs-string">&quot;green&quot;</span>)
        <span class="hljs-keyword">return</span> Response(response=<span class="hljs-built_in">str</span>(answer), source_nodes=source_nodes)
</code></pre>
<div class="content-ad"></div>
<p>위의 코드에서 우리는 그림 1의 모든 세 단계가 표현된 것을 확인할 수 있습니다. 그러나 LlamaIndex의 코드는 병렬 처리를 구현하지 않았습니다. 더 자세한 정보는 관심 있는 독자들이 self._run_critic 함수를 살펴볼 수 있습니다. 해당 함수는 다양한 반사 토큰에 해당하는 점수를 처리합니다.</p>
<h1>Llama2-7B 모델 훈련 방법</h1>
<p>이전에 여러 번 Llama2-7B 모델을 사용해왔으니, 이제 어떻게 얻을 지 알아봅시다.</p>
<h2>훈련 목표</h2>
<div class="content-ad"></div>
<p>훈련 과정에서는 평가 모델 C와 생성 모델 M 두 가지 모델이 필요합니다. 평가 모델 C는 모델 M이 필요로 하는 감독 데이터를 생성합니다.</p>
<p>그러나 추론 과정에서는 모델 M만 사용되며 모델 C는 필요하지 않습니다.</p>
<div class="content-ad"></div>
<h2>비평가 모델 C</h2>
<p>비평가 모델은 반사 토큰을 생성하는 데 훈련됩니다. 이 모델을 사용하는 목적은 작업 출력 오프라인에 반사 토큰을 삽입하여 훈련 말뭉치를 업데이트하는 것입니다.</p>
<p>각 세그먼트의 반사 토큰을 수동으로 주석 달기는 비용이 많이 듭니다. Self-RAG는 GPT-4를 활용하여 각 반사 토큰에 대해 고유한 지침을 할당하여 서로 다른 정의, 입력 및 출력을 가지고 있기 때문에 효율적으로 데이터 주석 작업을 완료합니다. 예를 들어, [검색] 토큰의 지시는 GPT-4가 외부 문서를 통합하는 것이 결과를 향상시킬지를 평가하도록 요청합니다.</p>
<p>훈련 데이터 D_critic를 얻으면 표준 조건부 언어 모델을 기반으로 훈련 목표를 구성할 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_3.png" alt="image"/></p>
<p>비평가 모델 C는 어떤 언어 모델로도 초기화할 수 있습니다. 예를 들어 생성자와 동일한 모델로 초기화할 수 있습니다. 예를 들면 Llama2-7B와 같은 모델을 사용할 수 있습니다.</p>
<h2>생성자 모델 M</h2>
<p>Figure 4는 훈련 데이터를 수집하는 구체적인 과정을 보여줍니다. 입력-출력 쌍 (x, y)가 주어지면 self-RAG는 검색 및 비평가 모델을 사용하여 원래의 출력 y를 확장하고 지도 데이터를 생성합니다. y의 각 세그먼트 yt에 대해:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_4.png"/>
<p>Figure 4의 모든 조건 판단은 비평가 모델 C를 통해 실행됩니다. 획득한 훈련 데이터는 Figure 5에 나타나 있습니다:</p>
<img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_5.png"/>
<p>훈련 데이터 D_gen을 획득한 후, 다음 토큰 예측 표준 목적 함수를 다음과 같이 구성할 수 있습니다:</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_6.png" alt="image"/></p>
<p>M 생성기는 결과뿐만 아니라 반영 토큰도 예측해야 합니다.</p>
<h1>self-RAG에 대한 나의 인사이트와 생각</h1>
<p>일반적으로 self-RAG는 RAG 프로세스를 강화하는 새로운 관점을 제공합니다. 그러나 더 복잡한 훈련 과정이 필요하며 생성 단계 중에 여러 레이블 생성과 판단이 필요하기 때문에 추론 비용이 증가하기 때문에 실시간 성능이 필요한 프로젝트에는 중요한 영향을 줄 수 있습니다.</p>
<div class="content-ad"></div>
<p>또한, 이 프레임워크 내에서 최적화할 여지가 많이 있습니다. 더 많은 토론과 혁신을 일으키기 위해 몇 가지 포인트를 공유하겠습니다:</p>
<ul>
<li>반영 토큰을 최적화하는 방법. Self-RAG는 네 가지 반영 토큰을 설계했습니다. [검색] 토큰 외에도 세 가지([IsREL], [IsSUP], [IsUSE])는 특정 유사성이 있습니다. 더 적은 반영 토큰을 사용하거나 다른 의미를 나타내는 반영 토큰을 고려하는 것이 타당한 방향일 수 있습니다.</li>
<li>비평가 모델이 LLM을 사용하는 이유는 무엇인가요? 제 생각에는 [IsUSE]와 같은 토큰이 공통 지식에 많이 의존하기 때문일 수 있습니다. 질의에 대한 답변의 유용성을 판단하는 것은 더 작은 모델이 수행할 수도 있습니다. 그러나 이러한 모델은 일반적인 지식을 부족하게 습득하며 종래의 특정 교육 자료만을 학습합니다. 따라서 비평가 모델로 LLM을 사용하는 것이 합리적일 수 있습니다.</li>
<li>비평가 모델 크기 선택. Self-RAG는 7B 및 13B 모델로 테스트되어 우수한 결과를 얻었습니다. 그러나 만약 더 작은 LLM인 3B로 전환하면 어떤 차이를 관찰할 수 있을까요? 마찬가지로, 더 큰 LLM인 33B로 전환했을 때 얼마나 개선을 기대할 수 있을까요?</li>
<li>인간 피드백을 통한 강화학습(RLHF)을 사용하지 않는 이유는 무엇인가요? 논문에서는 작업 예제를 통해 대상 언어 모델을 학습하는 것을 제안합니다. 이 예제는 비평가 모델에서 오프라인으로 반영 토큰이 추가된 것입니다. 이로 인해 RLHF 대비 훨씬 낮은 교육 비용이 발생합니다. 또한, self-RAG의 반영 토큰은 추론 중 생성을 제어할 수 있게 만들어주며 RLHF는 훈련 중 인간의 선호도 조정에 초점을 두고 있습니다. 그러나 논문에는 RLHF와 관련된 비교 실험 내용이 포함되어 있지 않습니다.</li>
</ul>
<h1>결론</h1>
<p>본문은 직관적인 예시로 시작하여 Self-RAG의 기본적인 과정을 소개하고 코드 설명을 보완하는 내용을 담고 있습니다. 또한 제 생각과 통찰을 공유하였습니다.</p>
<div class="content-ad"></div>
<p>RAG 기술에 관심이 있다면, 내 다른 기사들도 살펴보세요.</p>
<p>또한, 최신 AI 관련 콘텐츠는 내 뉴스레터에서 찾을 수 있어요.</p>
<p>마지막으로, 어떠한 오류나 누락이 있거나 궁금한 사항이 있으시면 댓글 섹션에서 자유롭게 토론해 주세요.</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"고급 RAG 08 Self-RAG","description":"","date":"2024-05-20 21:10","slug":"2024-05-20-AdvancedRAG08Self-RAG","content":"\n\n이 기사는 흔한 시나리오로 시작됩니다: 공개 시험을 보는 경우입니다. 일반적으로 두 가지 전략을 사용합니다:\n\n- 방법 1: 익숙한 주제에 대해서는 빠르게 답변하고, 익숙하지 않은 주제에 대해서는 참고서를 열어서 확인하고, 관련 부분을 빠르게 찾아내어 정리하고 요약한 다음, 시험지에 답변합니다.\n- 방법 2: 모든 주제에 대해 책을 참고합니다. 적절한 부분을 찾아내고, 정리하고 요약한 다음, 시험지에 답변합니다.\n\n분명히 방법 1이 선호되는 방법입니다. 방법 2는 시간이 소비될 수 있고, 관련성 없는 정보나 잘못된 정보가 들어올 수 있어 혼란과 실수를 야기할 수 있습니다. 심지어 처음에 이해한 부분에서도 발생할 수 있습니다.\n\n하지만, 방법 2는 고전적인 RAG 프로세스를 보여주며, 방법 1은 자체 RAG 프로세스를 대표합니다. 이에 대해 이 기사에서 더 자세히 다룰 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 개요\n\n그림 1은 RAG 및 Self-RAG의 주요 프로세스를 비교한 것을 보여줍니다:\n\n![그림](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png)\n\nSelf-RAG는 세 단계로 구성되어 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 필요한 경우 검색: 모델이 검색을 요구하는 경우, 예를 들어 \"미국 주가 이름을 어떻게 얻었습니까?\" (그림 1의 오른쪽 상단)와 같은 쿼리가 있을 때, 모델의 출력에는 [검색] 토큰이 포함됩니다. 이는 쿼리와 관련된 내용을 검색해야 함을 나타냅니다. 반면에 \"최고의 여름 휴가에 대해 에세이를 쓰세요\" (그림 1의 오른쪽 아래)와 같이 물어볼 때, 모델은 검색 없이 직접 답변을 생성하도록 선택합니다.\n- 병렬 생성: 모델은 프롬프트와 검색된 콘텐츠를 모두 사용하여 출력을 생성합니다. 이 과정에서 세 가지 유형의 반영 토큰이 검색된 콘텐츠의 관련성을 나타냅니다.\n- 평가 및 선택: 단계 2에서 생성된 콘텐츠가 평가되고, 최상의 세그먼트가 출력으로 선택됩니다.\n\n상기 모델은 특별히 훈련된 모델이라는 것을 유의하십시오. 이 모델의 훈련 과정은 이 기사의 후반부에서 논의될 것입니다.\n\n# 반영 토큰\n\nSelf-RAG 프레임워크의 RAG와 비교했을 때, Self-RAG 프레임워크의 차이는 생성 중 더 정확한 제어를 위해 반영 토큰을 사용한다는 것입니다. 그림 2에서 보여집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_1.png\" /\u003e\n\n본질적으로, self-RAG는 네 가지 명확한 판단을 내립니다:\n\n- [Retrieve]: 리소스 R로부터 정보를 검색할지를 결정하는 의사결정 과정.\n- [IsREL]: 주어진 데이터 d가 문제 x를 해결하는 데 필요한 정보를 포함하고 있는지를 결정하는 관련성 확인.\n- [IsSUP]: 제공된 응답 y의 내용이 데이터 d로부터 지원되는지를 확인하는 검증 과정.\n- [IsUSE]: 문제 x에 대한 응답 y의 유용성을 평가하는 평가 과정. 결과는 1에서 5까지의 점수로, 5는 가장 높은 유용성을 나타냅니다.\n\nRAG에서 검색은 상태에 관계없이 항상 처음에 수행되는 고정된 과정입니다. 반면 self-RAG는 반사 토큰을 도입하여 LLM을 더 적응적이고 지능적으로 만듭니다. LLM이 텍스트를 생성하다가 불확실성이 발생하는 부분에 도달하면 반사 토큰에서 일시 정지하여 신속하고 정확한 검색을 수행한 후 새로 습득한 정보를 사용하여 생성을 재개합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 코드 설명\n\nself-RAG 프로세스를 직관적으로 이해하기 위해 먼저 코드를 살펴보고 모델의 훈련 과정을 설명하겠습니다.\n\nself-RAG는 오픈 소스이며, Langchain과 LlamaIndex에는 각각의 구현이 있습니다. 우리는 설명을 위해 LlamaIndex의 구현을 참조할 것입니다.\n\n## 환경 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 환경을 설정하세요.\n\n```js\n(base) Florian@instance-1:~$ conda create -n llamaindex python=3.11\n\n(base) Florian@instance-1:~$ conda activate llamaindex\n\n\n(llamaindex) Florian@instance-1:~$ pip install llama-index\n\n(llamaindex) Florian@instance-1:~$ pip install huggingface-hub\n\n(llamaindex) Florian@instance-1:~$ huggingface-cli login\n```\n\n설치 후, LlamaIndex의 대응 버전은 다음과 같습니다:\n\n```js\nllama-index                             0.10.20\n\nllama-index-core                        0.10.20.post2\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"여러분의 오픈AI API 키\"\n\nfrom llama_index.core import Document, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom pathlib import Path\n\n\n# 옵션: SelfRAGPack 다운로드\n# 첫 실행 시 SelfRAGPack을 다운로드해야 합니다. \n# 다음 실행부터는 이 부분을 주석 처리할 수 있습니다.\nfrom llama_index.core.llama_pack import download_llama_pack\ndownload_llama_pack(\n    \"SelfRAGPack\",\n    \"./self_rag_pack\")\n\nfrom llama_index.packs.self_rag import SelfRAGQueryEngine\n\n# 이전에 다운로드하고 저장한 Llama2 모델이 있는 디렉토리.\ndownload_dir = \"여러분의 다운로드 모델 디렉토리\"\n\n# 테스트 문서 생성\ndocuments = [\n    Document(\n        text=\"남극 얼음 위를 '웨들'이라고 불리는 물개 떼가 지나다녔다. 그들의 턱시도 같은 깃털은 눈 위에서 돋보였다.\"\n    ),\n    Document(\n        text=\"펭귄 중 가장 키가 큰 황제펭귄은 다른 어떤 새보다도 더 깊이 다이빙을 할 수 있어서 500m 이상의 심해까지 다이빙을 합니다.\"\n    ),\n    Document(\n        text=\"펭귄들의 흑백색깔은 위험 방어라는 화장법의 한 종류인 카운터셰이딩입니다. 위에서 보면 펭귄의 검은 등은 바다 심지와 어우러지고, 아래에서는 펭귄의 흰 배는 밝은 표면과 어우러집니다.\"\n    ),\n    Document(\n        text=\"수직 자세이지만, 펭귄은 날지 못하는 조류입니다. 그들의 날개는 지느러미로 진화했기 때문에 수중에서 전문 수영가입니다.\"\n    ),\n    Document(\n        text=\"가장 빠른 펭귄 종류인 젠투 펭귄은 시속 36킬로미터까지 수영할 수 있으며, 수중을 순찰하는 동안 지느러미와 윤곽을 이용해 물을 가르는 식으로 전진합니다.\"\n    ),\n    Document(\n        text=\"펭귄은 집단생활을 하는 조류입니다. 많은 종들이 번식을 위해 수만 마리까지 이를 결성합니다.\"\n    ),\n    Document(\n        text=\"펭귄은 놀랍게도 귀가 우수하며 지저분한 떼 속에서 배우량과 새끼를 식별하는 데 명확한 호출을 의존합니다.\"\n    ),\n    Document(\n        text=\"가장 작은 펭귄 종인 리틀 블루 펭귄은 약 40cm 높이로, 남부 호주와 뉴질랜드 해안가에서 발견됩니다.\"\n    ),\n    Document(\n        text=\"번식 기간 중, 수컷 황제펭귄은 한없이 지속되는 남극 겨울을 버텨내며 몇 달간 급식없이 알을 부화시키는 반면, 암컷은 바다에서 사냥을 합니다.\"\n    ),\n    Document(\n        text=\"펭귄은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 그리고 크릴로 이루어져 있으며 이를 수중 다이빙을 통해 잡습니다.\"\n    ),\n]\n\nindex = VectorStoreIndex.from_documents(documents)\n\n# 간단한 리트리버 설정\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=10,\n)\n\n\nmodel_path = Path(download_dir) / \"selfrag_llama2_7b.q4_k_m.gguf\"\nquery_engine = SelfRAGQueryEngine(str(model_path), retriever, verbose=True)\n\n# 리트리벌 예시\nresponse = query_engine.query(\"어떤 장르인가요?\")\n\n# 리트리벌 예시\nresponse = query_engine.query(\"가장 작은 펭귄의 키는 얼마인가요?\")\n```\n\n위의 테스트 코드는 다음 결과를 생성했습니다(대부분의 llama_cpp 디버깅 정보가 제거되었습니다):\n\n```js\n...\n...\nModel metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: None\n\nllama_print_timings:        load time =    4887.53 ms\nllama_print_timings:      sample time =      11.29 ms /    22 runs   (    0.51 ms per token,  1947.76 tokens per second)\nllama_print_timings: prompt eval time =    4887.46 ms /    24 tokens (  203.64 ms per token,     4.91 tokens per second)\nllama_print_timings:        eval time =    5883.27 ms /    21 runs   (  280.16 ms per token,     3.57 tokens per second)\nllama_print_timings:       total time =   10901.84 ms /    45 tokens\n최종 답변: '오만과 편견'은 제인 오스틴의 로맨스 소설입니다.\n...\n...\nllama_print_timings:        load time =    4887.53 ms\nllama_print_timings:      sample time =      11.74 ms /    20 runs   (    0.59 ms per token,  1703.29 tokens per second)\nllama_print_timings: prompt eval time =    7473.66 ms /    37 tokens (  201.99 ms per token,     4.95 tokens per second)\nllama_print_timings:        eval time =    5414.34 ms /    19 runs   (  284.96 ms per token,     3.51 tokens per second)\nllama_print_timings:       total time =   13076.88 ms /    56 tokens\n입력: ### 지시사항:\n가장 작은 펭귄은 얼마나 키가 큰가요?\n\n### 응답:\n[검색]\u003c문단\u003e펭귄들은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 크릴로 구성되어 있으며 이를 다이빙으로 잡습니다.\"\u003c/문단\u003e\n예측: [관련]가장 작은 펭귄 종류의 키는 종에 따라 달라질 수 있습니다.[지원되지 않음 / 모순][유틸리티:5]\n점수: 1.4213598342974367\n10/10 단락 완료\n\n평가 종료\n최상의 답변 선정: [관련]가\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테스트 코드를 이해하는 핵심은 SelfRAGQueryEngine 클래스의 구현에 있습니다. 이제 이 클래스를 자세히 살펴보겠습니다.\n\n## 클래스 SelfRAGQueryEngine\n\n먼저 생성자입니다. 주로 llama_cpp를 사용하여 Llama2-7B 모델을 로드하기 위해 사용됩니다.\n\n```python\nclass SelfRAGQueryEngine(CustomQueryEngine):\n    \"\"\"간단한 Self RAG 쿼리 엔진.\"\"\"\n\n    llm: Any = Field(default=None, description=\"llm\")\n    retriever: BaseRetriever = Field(default=None, description=\"retriever\")\n    generate_kwargs: Dict = Field(default=None, description=\"llm generation arguments\")\n    verbose: bool = Field(default=True, description=\"Verbose.\")\n\n    def __init__(\n        self,\n        model_path: str,\n        retriever: BaseRetriever,\n        verbose: bool = False,\n        model_kwargs: Dict = None,\n        generate_kwargs: Dict = None,\n        **kwargs: Any,\n    ) -\u003e None:\n        \"\"\"매개변수 초기화.\"\"\"\n        super().__init__(verbose=verbose, **kwargs)\n        model_kwargs = model_kwargs or _MODEL_KWARGS\n        self.generate_kwargs = generate_kwargs or _GENERATE_KWARGS\n        try:\n            from llama_cpp import Llama\n        except ImportError:\n            raise ImportError(_IMPORT_ERROR_MSG)\n        self.llm = Llama(model_path=model_path, verbose=verbose, **model_kwargs)\n        self.retriever = retriever\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 다음으로 쿼리 기능에 대해 설명하겠습니다. 주요 프로세스는 아래 그림 3에 표시되어 있습니다:\n\n![Image](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_2.png)\n\n이해를 돕기 위해 주요 부분에는 주석이 달려 있습니다.\n\n```python\n    def custom_query(self, query_str: str) -\u003e Response:\n        \"\"\"커스텀 쿼리 실행.\"\"\"\n        # Llama2 모델을 사용하여 응답을 가져옵니다.\n        response = self.llm(prompt=_format_prompt(query_str), **_GENERATE_KWARGS)\n        answer = response[\"choices\"][0][\"text\"]\n        source_nodes = []\n\n        # 검색이 필요한지 여부를 결정합니다.\n        if \"[Retrieval]\" in answer:\n            if self.verbose:\n                print_text(\"검색이 필요합니다\\n\", color=\"blue\")\n            # 그림 1의 단계 1, 필요한대로 검색합니다.\n            documents = self.retriever.retrieve(query_str)\n            if self.verbose:\n                print_text(f\"받은 문서: {len(documents)}\\n\", color=\"blue\")\n            paragraphs = [\n                _format_prompt(query_str, document.node.text) for document in documents\n            ]\n\n            if self.verbose:\n                print_text(\"평가 시작\\n\", color=\"blue\")\n\n            # 그림 1의 단계 2 및 3, 병렬로 생성하고 평가합니다 \n            # (코드에서 병렬화를 구현하지는 않음)\n            critic_output = self._run_critic(paragraphs)\n\n            paragraphs_final_score = critic_output.paragraphs_final_score\n            llm_response_per_paragraph = critic_output.llm_response_per_paragraph\n            source_nodes = critic_output.source_nodes\n\n            if self.verbose:\n                print_text(\"평가 종료\\n\", color=\"blue\")\n\n            # 가장 높은 점수를 받은 답변을 선택하고 반환합니다.\n            best_paragraph_id = max(\n                paragraphs_final_score, key=paragraphs_final_score.get\n            )\n            answer = llm_response_per_paragraph[best_paragraph_id]\n            if self.verbose:\n                print_text(f\"최적 답변 선택: {answer}\\n\", color=\"blue\")\n\n        answer = _postprocess_answer(answer)\n        if self.verbose:\n            print_text(f\"최종 답변: {answer}\\n\", color=\"green\")\n        return Response(response=str(answer), source_nodes=source_nodes)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 코드에서 우리는 그림 1의 모든 세 단계가 표현된 것을 확인할 수 있습니다. 그러나 LlamaIndex의 코드는 병렬 처리를 구현하지 않았습니다. 더 자세한 정보는 관심 있는 독자들이 self._run_critic 함수를 살펴볼 수 있습니다. 해당 함수는 다양한 반사 토큰에 해당하는 점수를 처리합니다.\n\n# Llama2-7B 모델 훈련 방법\n\n이전에 여러 번 Llama2-7B 모델을 사용해왔으니, 이제 어떻게 얻을 지 알아봅시다.\n\n## 훈련 목표\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n훈련 과정에서는 평가 모델 C와 생성 모델 M 두 가지 모델이 필요합니다. 평가 모델 C는 모델 M이 필요로 하는 감독 데이터를 생성합니다.\n\n그러나 추론 과정에서는 모델 M만 사용되며 모델 C는 필요하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 비평가 모델 C\n\n비평가 모델은 반사 토큰을 생성하는 데 훈련됩니다. 이 모델을 사용하는 목적은 작업 출력 오프라인에 반사 토큰을 삽입하여 훈련 말뭉치를 업데이트하는 것입니다.\n\n각 세그먼트의 반사 토큰을 수동으로 주석 달기는 비용이 많이 듭니다. Self-RAG는 GPT-4를 활용하여 각 반사 토큰에 대해 고유한 지침을 할당하여 서로 다른 정의, 입력 및 출력을 가지고 있기 때문에 효율적으로 데이터 주석 작업을 완료합니다. 예를 들어, [검색] 토큰의 지시는 GPT-4가 외부 문서를 통합하는 것이 결과를 향상시킬지를 평가하도록 요청합니다.\n\n훈련 데이터 D_critic를 얻으면 표준 조건부 언어 모델을 기반으로 훈련 목표를 구성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_3.png) \n\n비평가 모델 C는 어떤 언어 모델로도 초기화할 수 있습니다. 예를 들어 생성자와 동일한 모델로 초기화할 수 있습니다. 예를 들면 Llama2-7B와 같은 모델을 사용할 수 있습니다.\n\n## 생성자 모델 M\n\nFigure 4는 훈련 데이터를 수집하는 구체적인 과정을 보여줍니다. 입력-출력 쌍 (x, y)가 주어지면 self-RAG는 검색 및 비평가 모델을 사용하여 원래의 출력 y를 확장하고 지도 데이터를 생성합니다. y의 각 세그먼트 yt에 대해:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_4.png\" /\u003e\n\nFigure 4의 모든 조건 판단은 비평가 모델 C를 통해 실행됩니다. 획득한 훈련 데이터는 Figure 5에 나타나 있습니다:\n\n\u003cimg src=\"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_5.png\" /\u003e\n\n훈련 데이터 D_gen을 획득한 후, 다음 토큰 예측 표준 목적 함수를 다음과 같이 구성할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_6.png)\n\nM 생성기는 결과뿐만 아니라 반영 토큰도 예측해야 합니다.\n\n# self-RAG에 대한 나의 인사이트와 생각\n\n일반적으로 self-RAG는 RAG 프로세스를 강화하는 새로운 관점을 제공합니다. 그러나 더 복잡한 훈련 과정이 필요하며 생성 단계 중에 여러 레이블 생성과 판단이 필요하기 때문에 추론 비용이 증가하기 때문에 실시간 성능이 필요한 프로젝트에는 중요한 영향을 줄 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, 이 프레임워크 내에서 최적화할 여지가 많이 있습니다. 더 많은 토론과 혁신을 일으키기 위해 몇 가지 포인트를 공유하겠습니다:\n\n- 반영 토큰을 최적화하는 방법. Self-RAG는 네 가지 반영 토큰을 설계했습니다. [검색] 토큰 외에도 세 가지([IsREL], [IsSUP], [IsUSE])는 특정 유사성이 있습니다. 더 적은 반영 토큰을 사용하거나 다른 의미를 나타내는 반영 토큰을 고려하는 것이 타당한 방향일 수 있습니다.\n- 비평가 모델이 LLM을 사용하는 이유는 무엇인가요? 제 생각에는 [IsUSE]와 같은 토큰이 공통 지식에 많이 의존하기 때문일 수 있습니다. 질의에 대한 답변의 유용성을 판단하는 것은 더 작은 모델이 수행할 수도 있습니다. 그러나 이러한 모델은 일반적인 지식을 부족하게 습득하며 종래의 특정 교육 자료만을 학습합니다. 따라서 비평가 모델로 LLM을 사용하는 것이 합리적일 수 있습니다.\n- 비평가 모델 크기 선택. Self-RAG는 7B 및 13B 모델로 테스트되어 우수한 결과를 얻었습니다. 그러나 만약 더 작은 LLM인 3B로 전환하면 어떤 차이를 관찰할 수 있을까요? 마찬가지로, 더 큰 LLM인 33B로 전환했을 때 얼마나 개선을 기대할 수 있을까요?\n- 인간 피드백을 통한 강화학습(RLHF)을 사용하지 않는 이유는 무엇인가요? 논문에서는 작업 예제를 통해 대상 언어 모델을 학습하는 것을 제안합니다. 이 예제는 비평가 모델에서 오프라인으로 반영 토큰이 추가된 것입니다. 이로 인해 RLHF 대비 훨씬 낮은 교육 비용이 발생합니다. 또한, self-RAG의 반영 토큰은 추론 중 생성을 제어할 수 있게 만들어주며 RLHF는 훈련 중 인간의 선호도 조정에 초점을 두고 있습니다. 그러나 논문에는 RLHF와 관련된 비교 실험 내용이 포함되어 있지 않습니다.\n\n# 결론\n\n본문은 직관적인 예시로 시작하여 Self-RAG의 기본적인 과정을 소개하고 코드 설명을 보완하는 내용을 담고 있습니다. 또한 제 생각과 통찰을 공유하였습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRAG 기술에 관심이 있다면, 내 다른 기사들도 살펴보세요.\n\n또한, 최신 AI 관련 콘텐츠는 내 뉴스레터에서 찾을 수 있어요.\n\n마지막으로, 어떠한 오류나 누락이 있거나 궁금한 사항이 있으시면 댓글 섹션에서 자유롭게 토론해 주세요.","ogImage":{"url":"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png"},"coverImage":"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png","tag":["Tech"],"readingTime":13},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    ul: \"ul\",\n    li: \"li\",\n    h1: \"h1\",\n    img: \"img\",\n    h2: \"h2\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"이 기사는 흔한 시나리오로 시작됩니다: 공개 시험을 보는 경우입니다. 일반적으로 두 가지 전략을 사용합니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"방법 1: 익숙한 주제에 대해서는 빠르게 답변하고, 익숙하지 않은 주제에 대해서는 참고서를 열어서 확인하고, 관련 부분을 빠르게 찾아내어 정리하고 요약한 다음, 시험지에 답변합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"방법 2: 모든 주제에 대해 책을 참고합니다. 적절한 부분을 찾아내고, 정리하고 요약한 다음, 시험지에 답변합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"분명히 방법 1이 선호되는 방법입니다. 방법 2는 시간이 소비될 수 있고, 관련성 없는 정보나 잘못된 정보가 들어올 수 있어 혼란과 실수를 야기할 수 있습니다. 심지어 처음에 이해한 부분에서도 발생할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만, 방법 2는 고전적인 RAG 프로세스를 보여주며, 방법 1은 자체 RAG 프로세스를 대표합니다. 이에 대해 이 기사에서 더 자세히 다룰 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"개요\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그림 1은 RAG 및 Self-RAG의 주요 프로세스를 비교한 것을 보여줍니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png\",\n        alt: \"그림\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Self-RAG는 세 단계로 구성되어 있습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"필요한 경우 검색: 모델이 검색을 요구하는 경우, 예를 들어 \\\"미국 주가 이름을 어떻게 얻었습니까?\\\" (그림 1의 오른쪽 상단)와 같은 쿼리가 있을 때, 모델의 출력에는 [검색] 토큰이 포함됩니다. 이는 쿼리와 관련된 내용을 검색해야 함을 나타냅니다. 반면에 \\\"최고의 여름 휴가에 대해 에세이를 쓰세요\\\" (그림 1의 오른쪽 아래)와 같이 물어볼 때, 모델은 검색 없이 직접 답변을 생성하도록 선택합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"병렬 생성: 모델은 프롬프트와 검색된 콘텐츠를 모두 사용하여 출력을 생성합니다. 이 과정에서 세 가지 유형의 반영 토큰이 검색된 콘텐츠의 관련성을 나타냅니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"평가 및 선택: 단계 2에서 생성된 콘텐츠가 평가되고, 최상의 세그먼트가 출력으로 선택됩니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"상기 모델은 특별히 훈련된 모델이라는 것을 유의하십시오. 이 모델의 훈련 과정은 이 기사의 후반부에서 논의될 것입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"반영 토큰\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Self-RAG 프레임워크의 RAG와 비교했을 때, Self-RAG 프레임워크의 차이는 생성 중 더 정확한 제어를 위해 반영 토큰을 사용한다는 것입니다. 그림 2에서 보여집니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_1.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"본질적으로, self-RAG는 네 가지 명확한 판단을 내립니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"[Retrieve]: 리소스 R로부터 정보를 검색할지를 결정하는 의사결정 과정.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"[IsREL]: 주어진 데이터 d가 문제 x를 해결하는 데 필요한 정보를 포함하고 있는지를 결정하는 관련성 확인.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"[IsSUP]: 제공된 응답 y의 내용이 데이터 d로부터 지원되는지를 확인하는 검증 과정.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"[IsUSE]: 문제 x에 대한 응답 y의 유용성을 평가하는 평가 과정. 결과는 1에서 5까지의 점수로, 5는 가장 높은 유용성을 나타냅니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"RAG에서 검색은 상태에 관계없이 항상 처음에 수행되는 고정된 과정입니다. 반면 self-RAG는 반사 토큰을 도입하여 LLM을 더 적응적이고 지능적으로 만듭니다. LLM이 텍스트를 생성하다가 불확실성이 발생하는 부분에 도달하면 반사 토큰에서 일시 정지하여 신속하고 정확한 검색을 수행한 후 새로 습득한 정보를 사용하여 생성을 재개합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"코드 설명\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"self-RAG 프로세스를 직관적으로 이해하기 위해 먼저 코드를 살펴보고 모델의 훈련 과정을 설명하겠습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"self-RAG는 오픈 소스이며, Langchain과 LlamaIndex에는 각각의 구현이 있습니다. 우리는 설명을 위해 LlamaIndex의 구현을 참조할 것입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"환경 설정\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"먼저, 환경을 설정하세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"(base) \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Florian\"\n        }), \"@instance-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":~$ conda create -n llamaindex python=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3.11\"\n        }), \"\\n\\n(base) \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Florian\"\n        }), \"@instance-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":~$ conda activate llamaindex\\n\\n\\n(llamaindex) \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Florian\"\n        }), \"@instance-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":~$ pip install llama-index\\n\\n(llamaindex) \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Florian\"\n        }), \"@instance-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":~$ pip install huggingface-hub\\n\\n(llamaindex) \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Florian\"\n        }), \"@instance-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \":~$ huggingface-cli login\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"설치 후, LlamaIndex의 대응 버전은 다음과 같습니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"llama-index                             \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.10\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".20\"\n        }), \"\\n\\nllama-index-core                        \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.10\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".20\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"post2\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" os\\nos.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"environ\"\n        }), \"[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"OPENAI_API_KEY\\\"\"\n        }), \"] = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"여러분의 오픈AI API 키\\\"\"\n        }), \"\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" llama_index.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"core\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"VectorStoreIndex\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" llama_index.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"core\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"retrievers\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"VectorIndexRetriever\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" llama_index.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"core\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"readers\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SimpleDirectoryReader\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pathlib \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Path\"\n        }), \"\\n\\n\\n# 옵션: \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SelfRAGPack\"\n        }), \" 다운로드\\n# 첫 실행 시 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SelfRAGPack\"\n        }), \"을 다운로드해야 합니다. \\n# 다음 실행부터는 이 부분을 주석 처리할 수 있습니다.\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" llama_index.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"core\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"llama_pack\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" download_llama_pack\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"download_llama_pack\"\n        }), \"(\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"SelfRAGPack\\\"\"\n        }), \",\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"./self_rag_pack\\\"\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" llama_index.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"packs\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"self_rag\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SelfRAGQueryEngine\"\n        }), \"\\n\\n# 이전에 다운로드하고 저장한 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Llama2\"\n        }), \" 모델이 있는 디렉토리.\\ndownload_dir = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"여러분의 다운로드 모델 디렉토리\\\"\"\n        }), \"\\n\\n# 테스트 문서 생성\\ndocuments = [\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"남극 얼음 위를 '웨들'이라고 불리는 물개 떼가 지나다녔다. 그들의 턱시도 같은 깃털은 눈 위에서 돋보였다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"펭귄 중 가장 키가 큰 황제펭귄은 다른 어떤 새보다도 더 깊이 다이빙을 할 수 있어서 500m 이상의 심해까지 다이빙을 합니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"펭귄들의 흑백색깔은 위험 방어라는 화장법의 한 종류인 카운터셰이딩입니다. 위에서 보면 펭귄의 검은 등은 바다 심지와 어우러지고, 아래에서는 펭귄의 흰 배는 밝은 표면과 어우러집니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"수직 자세이지만, 펭귄은 날지 못하는 조류입니다. 그들의 날개는 지느러미로 진화했기 때문에 수중에서 전문 수영가입니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"가장 빠른 펭귄 종류인 젠투 펭귄은 시속 36킬로미터까지 수영할 수 있으며, 수중을 순찰하는 동안 지느러미와 윤곽을 이용해 물을 가르는 식으로 전진합니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"펭귄은 집단생활을 하는 조류입니다. 많은 종들이 번식을 위해 수만 마리까지 이를 결성합니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"펭귄은 놀랍게도 귀가 우수하며 지저분한 떼 속에서 배우량과 새끼를 식별하는 데 명확한 호출을 의존합니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"가장 작은 펭귄 종인 리틀 블루 펭귄은 약 40cm 높이로, 남부 호주와 뉴질랜드 해안가에서 발견됩니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"번식 기간 중, 수컷 황제펭귄은 한없이 지속되는 남극 겨울을 버텨내며 몇 달간 급식없이 알을 부화시키는 반면, 암컷은 바다에서 사냥을 합니다.\\\"\"\n        }), \"\\n    ),\\n    \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Document\"\n        }), \"(\\n        text=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"펭귄은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 그리고 크릴로 이루어져 있으며 이를 수중 다이빙을 통해 잡습니다.\\\"\"\n        }), \"\\n    ),\\n]\\n\\nindex = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"VectorStoreIndex\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_documents\"\n        }), \"(documents)\\n\\n# 간단한 리트리버 설정\\nretriever = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"VectorIndexRetriever\"\n        }), \"(\\n    index=index,\\n    similarity_top_k=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10\"\n        }), \",\\n)\\n\\n\\nmodel_path = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Path\"\n        }), \"(download_dir) / \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"selfrag_llama2_7b.q4_k_m.gguf\\\"\"\n        }), \"\\nquery_engine = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"SelfRAGQueryEngine\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"str\"\n        }), \"(model_path), retriever, verbose=\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \")\\n\\n# 리트리벌 예시\\nresponse = query_engine.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"query\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"어떤 장르인가요?\\\"\"\n        }), \")\\n\\n# 리트리벌 예시\\nresponse = query_engine.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"query\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"가장 작은 펭귄의 키는 얼마인가요?\\\"\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 테스트 코드는 다음 결과를 생성했습니다(대부분의 llama_cpp 디버깅 정보가 제거되었습니다):\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"...\\n...\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Model\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"metadata\"\n        }), \": {\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'tokenizer.ggml.add_eos_token'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'false'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'tokenizer.ggml.eos_token_id'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'2'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'general.architecture'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.rope.freq_base'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'10000.000000'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.context_length'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'4096'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'general.name'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'LLaMA v2'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'tokenizer.ggml.add_bos_token'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'true'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.embedding_length'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'4096'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.feed_forward_length'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'11008'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.attention.layer_norm_rms_epsilon'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'0.000010'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.rope.dimension_count'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'128'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'tokenizer.ggml.bos_token_id'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'1'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.attention.head_count'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'32'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.block_count'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'32'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama.attention.head_count_kv'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'32'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'general.quantization_version'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'2'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'tokenizer.ggml.model'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'llama'\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'general.file_type'\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'15'\"\n        }), \"}\\n\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Using\"\n        }), \" fallback chat \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"format\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"None\"\n        }), \"\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":        load time =    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4887.53\"\n        }), \" ms\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":      sample time =      \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"11.29\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"22\"\n        }), \" runs   (    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.51\"\n        }), \" ms per token,  \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1947.76\"\n        }), \" tokens per second)\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \": prompt \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"eval\"\n        }), \" time =    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4887.46\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"24\"\n        }), \" tokens (  \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"203.64\"\n        }), \" ms per token,     \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4.91\"\n        }), \" tokens per second)\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":        \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"eval\"\n        }), \" time =    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5883.27\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"21\"\n        }), \" runs   (  \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"280.16\"\n        }), \" ms per token,     \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3.57\"\n        }), \" tokens per second)\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":       total time =   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"10901.84\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"45\"\n        }), \" tokens\\n최종 답변: \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'오만과 편견'\"\n        }), \"은 제인 오스틴의 로맨스 소설입니다.\\n...\\n...\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":        load time =    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4887.53\"\n        }), \" ms\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":      sample time =      \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"11.74\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"20\"\n        }), \" runs   (    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.59\"\n        }), \" ms per token,  \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1703.29\"\n        }), \" tokens per second)\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \": prompt \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"eval\"\n        }), \" time =    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"7473.66\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"37\"\n        }), \" tokens (  \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"201.99\"\n        }), \" ms per token,     \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4.95\"\n        }), \" tokens per second)\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":        \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"eval\"\n        }), \" time =    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"5414.34\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"19\"\n        }), \" runs   (  \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"284.96\"\n        }), \" ms per token,     \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3.51\"\n        }), \" tokens per second)\\n\", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"llama_print_timings\"\n        }), \":       total time =   \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"13076.88\"\n        }), \" ms /    \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"56\"\n        }), \" tokens\\n입력: ### 지시사항:\\n가장 작은 펭귄은 얼마나 키가 큰가요?\\n\\n### 응답:\\n[검색]\u003c문단\u003e펭귄들은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 크릴로 구성되어 있으며 이를 다이빙으로 잡습니다.\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\u003c/문단\u003e\\n예측: [관련]가장 작은 펭귄 종류의 키는 종에 따라 달라질 수 있습니다.[지원되지 않음 / 모순][유틸리티:5]\\n점수: 1.4213598342974367\\n10/10 단락 완료\\n\\n평가 종료\\n최상의 답변 선정: [관련]가\\n\\n\u003cdiv class=\\\"\"\n        }), \"content-ad\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\u003e\u003c/div\u003e\\n\\n테스트 코드를 이해하는 핵심은 SelfRAGQueryEngine 클래스의 구현에 있습니다. 이제 이 클래스를 자세히 살펴보겠습니다.\\n\\n## 클래스 SelfRAGQueryEngine\\n\\n먼저 생성자입니다. 주로 llama_cpp를 사용하여 Llama2-7B 모델을 로드하기 위해 사용됩니다.\\n\\n```python\\nclass SelfRAGQueryEngine(CustomQueryEngine):\\n    \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"간단한 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Self\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"RAG\"\n        }), \" 쿼리 엔진.\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n\\n    llm: Any = Field(default=None, description=\\\"\"\n        }), \"llm\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\")\\n    retriever: BaseRetriever = Field(default=None, description=\\\"\"\n        }), \"retriever\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\")\\n    generate_kwargs: Dict = Field(default=None, description=\\\"\"\n        }), \"llm generation \", _jsx(_components.span, {\n          className: \"hljs-variable language_\",\n          children: \"arguments\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\")\\n    verbose: bool = Field(default=True, description=\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Verbose\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\")\\n\\n    def __init__(\\n        self,\\n        model_path: str,\\n        retriever: BaseRetriever,\\n        verbose: bool = False,\\n        model_kwargs: Dict = None,\\n        generate_kwargs: Dict = None,\\n        **kwargs: Any,\\n    ) -\u003e None:\\n        \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"매개변수 초기화.\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n        super().__init__(verbose=verbose, **kwargs)\\n        model_kwargs = model_kwargs or _MODEL_KWARGS\\n        self.generate_kwargs = generate_kwargs or _GENERATE_KWARGS\\n        try:\\n            from llama_cpp import Llama\\n        except ImportError:\\n            raise ImportError(_IMPORT_ERROR_MSG)\\n        self.llm = Llama(model_path=model_path, verbose=verbose, **model_kwargs)\\n        self.retriever = retriever\\n\"\n        })]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그 다음으로 쿼리 기능에 대해 설명하겠습니다. 주요 프로세스는 아래 그림 3에 표시되어 있습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_2.png\",\n        alt: \"Image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이해를 돕기 위해 주요 부분에는 주석이 달려 있습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [\"    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"def\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"custom_query\"\n        }), \"(\", _jsxs(_components.span, {\n          className: \"hljs-params\",\n          children: [\"self, query_str: \", _jsx(_components.span, {\n            className: \"hljs-built_in\",\n            children: \"str\"\n          })]\n        }), \") -\u003e Response:\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\\\"커스텀 쿼리 실행.\\\"\\\"\\\"\"\n        }), \"\\n        \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# Llama2 모델을 사용하여 응답을 가져옵니다.\"\n        }), \"\\n        response = self.llm(prompt=_format_prompt(query_str), **_GENERATE_KWARGS)\\n        answer = response[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"choices\\\"\"\n        }), \"][\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0\"\n        }), \"][\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"text\\\"\"\n        }), \"]\\n        source_nodes = []\\n\\n        \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 검색이 필요한지 여부를 결정합니다.\"\n        }), \"\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"[Retrieval]\\\"\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" answer:\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" self.verbose:\\n                print_text(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"검색이 필요합니다\\\\n\\\"\"\n        }), \", color=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"blue\\\"\"\n        }), \")\\n            \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 그림 1의 단계 1, 필요한대로 검색합니다.\"\n        }), \"\\n            documents = self.retriever.retrieve(query_str)\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" self.verbose:\\n                print_text(\", _jsxs(_components.span, {\n          className: \"hljs-string\",\n          children: [\"f\\\"받은 문서: \", _jsxs(_components.span, {\n            className: \"hljs-subst\",\n            children: [\"{\", _jsx(_components.span, {\n              className: \"hljs-built_in\",\n              children: \"len\"\n            }), \"(documents)}\"]\n          }), \"\\\\n\\\"\"]\n        }), \", color=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"blue\\\"\"\n        }), \")\\n            paragraphs = [\\n                _format_prompt(query_str, document.node.text) \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" document \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" documents\\n            ]\\n\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" self.verbose:\\n                print_text(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"평가 시작\\\\n\\\"\"\n        }), \", color=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"blue\\\"\"\n        }), \")\\n\\n            \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 그림 1의 단계 2 및 3, 병렬로 생성하고 평가합니다 \"\n        }), \"\\n            \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# (코드에서 병렬화를 구현하지는 않음)\"\n        }), \"\\n            critic_output = self._run_critic(paragraphs)\\n\\n            paragraphs_final_score = critic_output.paragraphs_final_score\\n            llm_response_per_paragraph = critic_output.llm_response_per_paragraph\\n            source_nodes = critic_output.source_nodes\\n\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" self.verbose:\\n                print_text(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"평가 종료\\\\n\\\"\"\n        }), \", color=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"blue\\\"\"\n        }), \")\\n\\n            \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# 가장 높은 점수를 받은 답변을 선택하고 반환합니다.\"\n        }), \"\\n            best_paragraph_id = \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"max\"\n        }), \"(\\n                paragraphs_final_score, key=paragraphs_final_score.get\\n            )\\n            answer = llm_response_per_paragraph[best_paragraph_id]\\n            \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" self.verbose:\\n                print_text(\", _jsxs(_components.span, {\n          className: \"hljs-string\",\n          children: [\"f\\\"최적 답변 선택: \", _jsx(_components.span, {\n            className: \"hljs-subst\",\n            children: \"{answer}\"\n          }), \"\\\\n\\\"\"]\n        }), \", color=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"blue\\\"\"\n        }), \")\\n\\n        answer = _postprocess_answer(answer)\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" self.verbose:\\n            print_text(\", _jsxs(_components.span, {\n          className: \"hljs-string\",\n          children: [\"f\\\"최종 답변: \", _jsx(_components.span, {\n            className: \"hljs-subst\",\n            children: \"{answer}\"\n          }), \"\\\\n\\\"\"]\n        }), \", color=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"green\\\"\"\n        }), \")\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" Response(response=\", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"str\"\n        }), \"(answer), source_nodes=source_nodes)\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 코드에서 우리는 그림 1의 모든 세 단계가 표현된 것을 확인할 수 있습니다. 그러나 LlamaIndex의 코드는 병렬 처리를 구현하지 않았습니다. 더 자세한 정보는 관심 있는 독자들이 self._run_critic 함수를 살펴볼 수 있습니다. 해당 함수는 다양한 반사 토큰에 해당하는 점수를 처리합니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Llama2-7B 모델 훈련 방법\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이전에 여러 번 Llama2-7B 모델을 사용해왔으니, 이제 어떻게 얻을 지 알아봅시다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"훈련 목표\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 과정에서는 평가 모델 C와 생성 모델 M 두 가지 모델이 필요합니다. 평가 모델 C는 모델 M이 필요로 하는 감독 데이터를 생성합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 추론 과정에서는 모델 M만 사용되며 모델 C는 필요하지 않습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"비평가 모델 C\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비평가 모델은 반사 토큰을 생성하는 데 훈련됩니다. 이 모델을 사용하는 목적은 작업 출력 오프라인에 반사 토큰을 삽입하여 훈련 말뭉치를 업데이트하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"각 세그먼트의 반사 토큰을 수동으로 주석 달기는 비용이 많이 듭니다. Self-RAG는 GPT-4를 활용하여 각 반사 토큰에 대해 고유한 지침을 할당하여 서로 다른 정의, 입력 및 출력을 가지고 있기 때문에 효율적으로 데이터 주석 작업을 완료합니다. 예를 들어, [검색] 토큰의 지시는 GPT-4가 외부 문서를 통합하는 것이 결과를 향상시킬지를 평가하도록 요청합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 데이터 D_critic를 얻으면 표준 조건부 언어 모델을 기반으로 훈련 목표를 구성할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비평가 모델 C는 어떤 언어 모델로도 초기화할 수 있습니다. 예를 들어 생성자와 동일한 모델로 초기화할 수 있습니다. 예를 들면 Llama2-7B와 같은 모델을 사용할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"생성자 모델 M\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Figure 4는 훈련 데이터를 수집하는 구체적인 과정을 보여줍니다. 입력-출력 쌍 (x, y)가 주어지면 self-RAG는 검색 및 비평가 모델을 사용하여 원래의 출력 y를 확장하고 지도 데이터를 생성합니다. y의 각 세그먼트 yt에 대해:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_4.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Figure 4의 모든 조건 판단은 비평가 모델 C를 통해 실행됩니다. 획득한 훈련 데이터는 Figure 5에 나타나 있습니다:\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_5.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 데이터 D_gen을 획득한 후, 다음 토큰 예측 표준 목적 함수를 다음과 같이 구성할 수 있습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_6.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"M 생성기는 결과뿐만 아니라 반영 토큰도 예측해야 합니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"self-RAG에 대한 나의 인사이트와 생각\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"일반적으로 self-RAG는 RAG 프로세스를 강화하는 새로운 관점을 제공합니다. 그러나 더 복잡한 훈련 과정이 필요하며 생성 단계 중에 여러 레이블 생성과 판단이 필요하기 때문에 추론 비용이 증가하기 때문에 실시간 성능이 필요한 프로젝트에는 중요한 영향을 줄 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한, 이 프레임워크 내에서 최적화할 여지가 많이 있습니다. 더 많은 토론과 혁신을 일으키기 위해 몇 가지 포인트를 공유하겠습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"반영 토큰을 최적화하는 방법. Self-RAG는 네 가지 반영 토큰을 설계했습니다. [검색] 토큰 외에도 세 가지([IsREL], [IsSUP], [IsUSE])는 특정 유사성이 있습니다. 더 적은 반영 토큰을 사용하거나 다른 의미를 나타내는 반영 토큰을 고려하는 것이 타당한 방향일 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"비평가 모델이 LLM을 사용하는 이유는 무엇인가요? 제 생각에는 [IsUSE]와 같은 토큰이 공통 지식에 많이 의존하기 때문일 수 있습니다. 질의에 대한 답변의 유용성을 판단하는 것은 더 작은 모델이 수행할 수도 있습니다. 그러나 이러한 모델은 일반적인 지식을 부족하게 습득하며 종래의 특정 교육 자료만을 학습합니다. 따라서 비평가 모델로 LLM을 사용하는 것이 합리적일 수 있습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"비평가 모델 크기 선택. Self-RAG는 7B 및 13B 모델로 테스트되어 우수한 결과를 얻었습니다. 그러나 만약 더 작은 LLM인 3B로 전환하면 어떤 차이를 관찰할 수 있을까요? 마찬가지로, 더 큰 LLM인 33B로 전환했을 때 얼마나 개선을 기대할 수 있을까요?\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"인간 피드백을 통한 강화학습(RLHF)을 사용하지 않는 이유는 무엇인가요? 논문에서는 작업 예제를 통해 대상 언어 모델을 학습하는 것을 제안합니다. 이 예제는 비평가 모델에서 오프라인으로 반영 토큰이 추가된 것입니다. 이로 인해 RLHF 대비 훨씬 낮은 교육 비용이 발생합니다. 또한, self-RAG의 반영 토큰은 추론 중 생성을 제어할 수 있게 만들어주며 RLHF는 훈련 중 인간의 선호도 조정에 초점을 두고 있습니다. 그러나 논문에는 RLHF와 관련된 비교 실험 내용이 포함되어 있지 않습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"본문은 직관적인 예시로 시작하여 Self-RAG의 기본적인 과정을 소개하고 코드 설명을 보완하는 내용을 담고 있습니다. 또한 제 생각과 통찰을 공유하였습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"RAG 기술에 관심이 있다면, 내 다른 기사들도 살펴보세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또한, 최신 AI 관련 콘텐츠는 내 뉴스레터에서 찾을 수 있어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로, 어떠한 오류나 누락이 있거나 궁금한 사항이 있으시면 댓글 섹션에서 자유롭게 토론해 주세요.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-20-AdvancedRAG08Self-RAG"},"buildId":"ll1cGyplNwh83dpggeai1","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>