<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents" data-gatsby-head="true"/><meta name="twitter:title" content="트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-23 22:06" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/bb_yO9GbCvdfc_n71SfUf/_buildManifest.js" defer=""></script><script src="/_next/static/bb_yO9GbCvdfc_n71SfUf/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 23, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>소개</h1>
<p>Twitter는 약 400억 건의 이벤트를 실시간으로 처리하고 매일 페타바이트의 데이터를 생성합니다. Twitter는 분산 데이터베이스, Kafka, Twitter 이벤트 버스 등과 같은 다양한 이벤트 소스에서 데이터를 소비합니다. 여기에서 블로그의 구현을 찾을 수도 있습니다.</p>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png" alt="이미지"></p>
<p>이 블로그에서는 다음을 이해하려고 노력할 것입니다:</p>
<p></p>
<ul>
<li>트위터가 이벤트를 처리하는 방식 및 이 방식에 대한 문제점은 무엇이 있었나요?</li>
<li>어떤 비즈니스와 고객 영향으로 인해 트위터가 새 아키텍처로 전환하게 되었나요?</li>
<li>새 아키텍처</li>
<li>이전 및 새로운 아키텍처의 성능 비교</li>
</ul>
<p>트위터는 다음과 같은 내부 도구 세트를 보유하고 있습니다.</p>
<ul>
<li>Scalding은 트위터가 배치 처리에 사용하는 도구입니다.</li>
<li>Heron은 트위터의 스트리밍 엔진입니다.</li>
<li>TimeSeriesAggregator(TSAR)는 배치 및 실시간 처리에 사용됩니다.</li>
</ul>
<p>이벤트 시스템이 어떻게 발전했는지 자세히 파헤치기 전에, 네 개의 내부 도구에 대해 간략히 알아보겠습니다.</p>
<p></p>
<ul>
<li>Scalding</li>
</ul>
<p>스콜딩(Scalding)은 하둡 맵리듀스 작업을 쉽게 지정할 수 있도록 도와주는 스칼라 라이브러리입니다. 스콜딩은 하둡의 하위 세부 사항을 추상화하는 자바 라이브러리인 카스케이딩(Cascading) 위에 구축되어 있습니다. 스콜딩은 Pig와 비교할 수 있지만, 스칼라와 강하게 통합되어 있어 스칼라의 장점을 하둡 맵리듀스 작업에 가져다줍니다.</p>
<ol start="2">
<li>Heron</li>
</ol>
<p>아파치 헤론(Heron)은 트위터의 자체 스트리밍 엔진으로, 페타바이트에 이르는 대량의 데이터를 처리하고, 개발자 생산성을 향상시키고, 디버깅을 단순화해야 하는 시스템의 필요성으로 개발되었습니다.</p>
<p></p>
<p>헤론에서 스트리밍 애플리케이션을 위한 구조를 토폴로지라고 합니다. 토폴로지는 데이터-컴퓨팅 요소를 나타내는 노드와 해당 요소 간에 흐르는 데이터 스트림을 나타내는 엣지로 이루어진 방향성 비순환 그래프입니다.</p>
<p>노드에는 2 종류가 있습니다:</p>
<ul>
<li>스파우트: 데이터 원본에 연결되어 데이터를 스트림에 삽입합니다.</li>
<li>볼트: 수신된 데이터를 처리하고 데이터를 방출합니다.</li>
</ul>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_1.png" alt="그림"></p>
<p></p>
<p>더 많은 정보를 보시려면 여기를 참고해주세요: <a href="https://blog.x.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron" rel="nofollow" target="_blank">https://blog.x.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron</a></p>
<ol start="3">
<li>TimeSeriesAggregator</li>
</ol>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_2.png" alt="Image"></p>
<p>Twitter의 데이터 엔지니어링 팀은 매일 일괄 및 실시간으로 수조 건의 이벤트를 처리하는 과제에 직면했습니다. TSAR는 프레임워크 기반의 강력하고 확장 가능한 실시간 이벤트 시간대 시계열 집계 도구로, tweet과의 상호 작용을 집계하며 장치, 참여 유형 등 다양한 차원을 따라 분할하여 주로 참여 모니터링을 위해 구축되었습니다.</p>
<p></p>
<p>트위터에서 어떻게 일이 처리되었는지 대략적으로 살펴보겠습니다. 트위터의 모든 기능은 전 세계에 10만 개 이상의 인스턴스로 퍼져 있는 마이크로서비스에 의해 지원됩니다. 이들은 이벤트를 생성하고, 이를 이벤트 집계 레이어로 보내는 역할을 합니다. 이 이벤트 집계 레이어는 메타에서 제공하는 오픈 소스 프로젝트를 기반으로 구축되었으며, 이벤트를 그룹화하고 집계 작업을 실행하며 데이터를 HDFS에 저장하는 역할을 합니다. 그런 다음 이러한 이벤트를 처리하고 형식을 변환하여 데이터를 다시 압축하여 잘 생성된 데이터 세트를 만들어냅니다.</p>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_3.png" alt="이미지"></p>
<h1>이전 아키텍처</h1>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_4.png" alt="이미지"></p>
<p></p>
<p>트위터의 구형 아키텍처는 람다 아키텍처를 기반으로 하고 있었어요. 이는 배치 레이어, 스피드 레이어 및 서빙 레이어로 구성되어 있어요. 배치 컴포넌트는 클라이언트가 생성한 로그이며, 이벤트 처리 후 하둡 분산 파일 시스템(HDFS)에 저장돼요. 트위터는 여러 스케일링 파이프라인을 구축하여 가공되지 않은 로그를 전처리하고 오프라인 소스로 Summingbird 플랫폼에 넣었어요. 실시간 컴포넌트 소스는 스피드 레이어의 일부인 Kafka 토픽들이에요.</p>
<p>데이터가 처리되면 배치 데이터는 맨해튼 분산 시스템에 저장되며, 실시간 데이터는 트위터의 자체 분산 캐시인 Nighthawk에 저장돼요. TSAR 시스템(예: TSAR 쿼리 서비스)은 캐시와 데이터베이스 둘 다 쿼리하는 서빙 레이어의 일부가 돼요.</p>
<p>트위터는 세 개의 다른 데이터 센터에 실시간 파이프라인과 쿼리 서비스를 가졌어요. 배치 컴퓨팅 비용을 줄이기 위해, 트위터는 한 데이터 센터에서 배치 파이프라인을 실행하고 데이터를 다른 두 데이터 센터로 복제해요.</p>
<p>실시간 데이터를 캐시에 저장했을까요? 데이터베이스에 저장하는 것보다 더 좋은 이유가 떠오르시나요?</p>
<p></p>
<h1>오래된 아키텍처의 도전 과제</h1>
<p>이 아키텍처가 실시간 이벤트의 경우 어떤 도전 과제를 가질 수 있는지 이해해 봅시다.</p>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_5.png" alt="이미지"></p>
<p>예를 통해 이해해 봅시다:</p>
<p></p>
<p>대규모 이벤트가 발생하면 FIFA 월드컵과 같은 큰 이벤트가 발생할 수 있습니다. 트윗 소스는 트윗 토폴로지로 많은 이벤트를 보낼 것입니다. 그러나 파싱 트윗 볼트가 이벤트를 적시에 처리하지 못해 토폴로지 내에서 백프레셔가 발생합니다. 시스템이 장기간 백프레셔 상태에 놓이면 헤론 볼트가 스파우트 라그를 축적할 수 있어 시스템의 지연 시간이 늘어날 수 있습니다. 트위터는 이러한 경우에 토폴로지 랙이 줄어드는 데 매우 오랜 시간이 걸린다고 관찰했습니다.</p>
<p>팀이 사용한 운영 솔루션은 헤론 컨테이너를 다시 시작하여 데이터 스트림 처리를 다시 시작하는 것이었습니다. 이로 인해 이행 중에 이벤트 손실이 발생할 수 있으며, 이는 캐시의 집계된 카운트에 부정확성을 초래할 수 있습니다.</p>
<p>이제 배치 이벤트 예제를 이해해 봅시다. 트위터는 PB 규모의 데이터를 처리하는 몇 가지 무거운 계산 파이프라인을 운영하고 맨해튼 데이터베이스에서 데이터를 시간별로 동기화합니다. 이제 시스템 동기화 작업이 1시간 이상 소요되고 다음 작업이 시작되기로 예정된 경우를 상상해 보십시오. 이는 시스템에 백프레셔가 증가하여 데이터 손실을 초래할 수 있는 상황으로 이어질 수 있습니다.</p>
<p>TSAR 쿼리 서비스는 맨해튼과 캐시 서비스를 통합하여 클라이언트에 데이터를 제공합니다. 실시간 데이터 손실 가능성으로 인해 TSAR 서비스는 고객들에게 부정확한 메트릭을 제공할 수 있습니다.</p>
<p></p>
<p>고객과 비즈니스에 미치는 영향을 이해하는 것을 시도해 봅시다:</p>
<ul>
<li>트위터 광고 서비스는 트위터의 주요 수익 모델 중 하나이며, 그 성능이 영향을 받으면 그들의 비즈니스 모델에 직접적인 영향을 끼칩니다.</li>
<li>트위터는 인상과 관련 지표에 대한 정보를 검색할 수 있는 다양한 데이터 제품 서비스를 제공하며, 이러한 서비스는 부정확한 데이터로 인해 영향을 받을 수 있습니다.</li>
<li>이 경우에 또다른 문제는 이벤트 생성부터 사용 가능할 때까지 배치 처리 작업으로 인해 몇 시간이 걸릴 수 있다는 점입니다. 이는 클라이언트가 수행해야 할 데이터 분석이나 기타 작업이 최신 데이터를 사용할 수 없게됨을 의미합니다. 몇 시간의 시차가 발생할 수 있습니다.</li>
</ul>
<p>이제 이는 사용자가 이벤트를 생성하고 사용자가 생성하는 이벤트에 기반한 사용자의 타임라인을 업데이트하려거나 트위터 시스템과 상호 작용하는 방식에 따라 사용자에 대한 행동 분석을 수행하려면 클라이언트가 배치 작업이 완료될 때까지 기다려야함을 의미합니다.</p>
<h1>새로운 아키텍처</h1>
<p></p>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_6.png" alt="2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_6"></p>
<p>Twitter의 새로운 아키텍처는 Twitter 데이터 센터 서비스와 Google Cloud 플랫폼 양쪽에 구축되었습니다. Twitter는 카파 주제를 퍼브 서브 주제로 변환하는 이벤트 처리 파이프 라인을 구축했으며, 이는 Google Cloud로 전송되었습니다. Google Cloud에서 실시간 집계를 수행하고 데이터를 BigTable에 싱크하는 스트리밍 데이터 플로우 작업이 수행되었습니다.</p>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_7.png" alt="2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_7"></p>
<p>서빙 레이어에는 Twitter가 Twitter 데이터 센터에 프런트 엔드와 Bigtable 및 BigQuery에 백엔드가 있는 LDC 쿼리 서비스를 사용합니다. 전체 시스템은 초당 수백만 건의 이벤트를 스트리밍할 수 있으며 지연 시간이 ~10밀리초까지 낮을 수 있습니다. 높은 트래픽 시 확장이 쉽습니다.</p>
<p></p>
<p>이 새로운 아키텍처는 일괄 파이프라인을 구축하는 비용을 절약해 줄 뿐만 아니라, 실시간 파이프라인에서는 트위터가 더 높은 집계 정확도와 안정적인 낮은 대기 시간을 달성할 수 있습니다. 또한, 그들은 여러 데이터 센터에서 다른 실시간 이벤트 집계를 유지할 필요가 없습니다.</p>
<h1>성능 비교</h1>
<p><img src="/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_8.png" alt="이미지"></p>
<p>새 아키텍처는 구 방식인 Heron 토폴로지와 비교하여 낮은 대기 시간을 제공하며 더 높은 처리량을 제공합니다. 또한, 새 아키텍처는 지연된 이벤트 계산을 처리하고 실시간 집계 중에 이벤트 손실이 없습니다. 또한, 새 아키텍처에는 일괄 구성 요소가 없어 구 방식에 있던 그것보다 설계를 단순화하고 컴퓨팅 비용을 줄입니다.</p>
<p></p>
<h1>결론</h1>
<p>TSAR로 구축된 이전 아키텍처를 트위터 데이터 센터와 구글 클라우드 플랫폼의 하이브리드 아키텍처로 이전함으로써 트위터는 수십억 이벤트를 실시간으로 처리하고 엔지니어들에게 낮은 대기 시간, 높은 정확성, 안정성, 아키텍처의 단순성 및 운영 비용 감소를 달성할 수 있었습니다.</p>
<p>Linkedin: <a href="https://www.linkedin.com/in/mayank-sharma-2002bb10b/" rel="nofollow" target="_blank">Mayank Sharma의 Linkedin 프로필</a></p>
<p>저와 모의 시스템 디자인 인터뷰 일정을 잡으려면 : <a href="https://www.meetapro.com/provider/listing/160769" rel="nofollow" target="_blank">Meetapro에서 예약하기</a></p>
<p></p>
<p>나의 웹사이트: imayanks.com</p>
<p>참고 자료:</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"트위터가 4000억 이벤트를 처리하는 방법을 개선한 비결","description":"","date":"2024-06-23 22:06","slug":"2024-06-23-HowTwitterimprovedtheprocessingof400billionevents","content":"\n# 소개\n\nTwitter는 약 400억 건의 이벤트를 실시간으로 처리하고 매일 페타바이트의 데이터를 생성합니다. Twitter는 분산 데이터베이스, Kafka, Twitter 이벤트 버스 등과 같은 다양한 이벤트 소스에서 데이터를 소비합니다. 여기에서 블로그의 구현을 찾을 수도 있습니다.\n\n![이미지](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png)\n\n이 블로그에서는 다음을 이해하려고 노력할 것입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 트위터가 이벤트를 처리하는 방식 및 이 방식에 대한 문제점은 무엇이 있었나요?\n- 어떤 비즈니스와 고객 영향으로 인해 트위터가 새 아키텍처로 전환하게 되었나요?\n- 새 아키텍처\n- 이전 및 새로운 아키텍처의 성능 비교\n\n트위터는 다음과 같은 내부 도구 세트를 보유하고 있습니다.\n\n- Scalding은 트위터가 배치 처리에 사용하는 도구입니다.\n- Heron은 트위터의 스트리밍 엔진입니다.\n- TimeSeriesAggregator(TSAR)는 배치 및 실시간 처리에 사용됩니다.\n\n이벤트 시스템이 어떻게 발전했는지 자세히 파헤치기 전에, 네 개의 내부 도구에 대해 간략히 알아보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- Scalding\n\n스콜딩(Scalding)은 하둡 맵리듀스 작업을 쉽게 지정할 수 있도록 도와주는 스칼라 라이브러리입니다. 스콜딩은 하둡의 하위 세부 사항을 추상화하는 자바 라이브러리인 카스케이딩(Cascading) 위에 구축되어 있습니다. 스콜딩은 Pig와 비교할 수 있지만, 스칼라와 강하게 통합되어 있어 스칼라의 장점을 하둡 맵리듀스 작업에 가져다줍니다.\n\n2. Heron\n\n아파치 헤론(Heron)은 트위터의 자체 스트리밍 엔진으로, 페타바이트에 이르는 대량의 데이터를 처리하고, 개발자 생산성을 향상시키고, 디버깅을 단순화해야 하는 시스템의 필요성으로 개발되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n헤론에서 스트리밍 애플리케이션을 위한 구조를 토폴로지라고 합니다. 토폴로지는 데이터-컴퓨팅 요소를 나타내는 노드와 해당 요소 간에 흐르는 데이터 스트림을 나타내는 엣지로 이루어진 방향성 비순환 그래프입니다.\n\n노드에는 2 종류가 있습니다:\n\n- 스파우트: 데이터 원본에 연결되어 데이터를 스트림에 삽입합니다.\n- 볼트: 수신된 데이터를 처리하고 데이터를 방출합니다.\n\n![그림](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n더 많은 정보를 보시려면 여기를 참고해주세요: [https://blog.x.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron](https://blog.x.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron)\n\n3. TimeSeriesAggregator\n\n![Image](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_2.png)\n\nTwitter의 데이터 엔지니어링 팀은 매일 일괄 및 실시간으로 수조 건의 이벤트를 처리하는 과제에 직면했습니다. TSAR는 프레임워크 기반의 강력하고 확장 가능한 실시간 이벤트 시간대 시계열 집계 도구로, tweet과의 상호 작용을 집계하며 장치, 참여 유형 등 다양한 차원을 따라 분할하여 주로 참여 모니터링을 위해 구축되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n트위터에서 어떻게 일이 처리되었는지 대략적으로 살펴보겠습니다. 트위터의 모든 기능은 전 세계에 10만 개 이상의 인스턴스로 퍼져 있는 마이크로서비스에 의해 지원됩니다. 이들은 이벤트를 생성하고, 이를 이벤트 집계 레이어로 보내는 역할을 합니다. 이 이벤트 집계 레이어는 메타에서 제공하는 오픈 소스 프로젝트를 기반으로 구축되었으며, 이벤트를 그룹화하고 집계 작업을 실행하며 데이터를 HDFS에 저장하는 역할을 합니다. 그런 다음 이러한 이벤트를 처리하고 형식을 변환하여 데이터를 다시 압축하여 잘 생성된 데이터 세트를 만들어냅니다.\n\n![이미지](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_3.png)\n\n# 이전 아키텍처\n\n![이미지](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n트위터의 구형 아키텍처는 람다 아키텍처를 기반으로 하고 있었어요. 이는 배치 레이어, 스피드 레이어 및 서빙 레이어로 구성되어 있어요. 배치 컴포넌트는 클라이언트가 생성한 로그이며, 이벤트 처리 후 하둡 분산 파일 시스템(HDFS)에 저장돼요. 트위터는 여러 스케일링 파이프라인을 구축하여 가공되지 않은 로그를 전처리하고 오프라인 소스로 Summingbird 플랫폼에 넣었어요. 실시간 컴포넌트 소스는 스피드 레이어의 일부인 Kafka 토픽들이에요.\n\n데이터가 처리되면 배치 데이터는 맨해튼 분산 시스템에 저장되며, 실시간 데이터는 트위터의 자체 분산 캐시인 Nighthawk에 저장돼요. TSAR 시스템(예: TSAR 쿼리 서비스)은 캐시와 데이터베이스 둘 다 쿼리하는 서빙 레이어의 일부가 돼요.\n\n트위터는 세 개의 다른 데이터 센터에 실시간 파이프라인과 쿼리 서비스를 가졌어요. 배치 컴퓨팅 비용을 줄이기 위해, 트위터는 한 데이터 센터에서 배치 파이프라인을 실행하고 데이터를 다른 두 데이터 센터로 복제해요.\n\n실시간 데이터를 캐시에 저장했을까요? 데이터베이스에 저장하는 것보다 더 좋은 이유가 떠오르시나요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 오래된 아키텍처의 도전 과제\n\n이 아키텍처가 실시간 이벤트의 경우 어떤 도전 과제를 가질 수 있는지 이해해 봅시다.\n\n![이미지](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_5.png)\n\n예를 통해 이해해 봅시다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n대규모 이벤트가 발생하면 FIFA 월드컵과 같은 큰 이벤트가 발생할 수 있습니다. 트윗 소스는 트윗 토폴로지로 많은 이벤트를 보낼 것입니다. 그러나 파싱 트윗 볼트가 이벤트를 적시에 처리하지 못해 토폴로지 내에서 백프레셔가 발생합니다. 시스템이 장기간 백프레셔 상태에 놓이면 헤론 볼트가 스파우트 라그를 축적할 수 있어 시스템의 지연 시간이 늘어날 수 있습니다. 트위터는 이러한 경우에 토폴로지 랙이 줄어드는 데 매우 오랜 시간이 걸린다고 관찰했습니다.\n\n팀이 사용한 운영 솔루션은 헤론 컨테이너를 다시 시작하여 데이터 스트림 처리를 다시 시작하는 것이었습니다. 이로 인해 이행 중에 이벤트 손실이 발생할 수 있으며, 이는 캐시의 집계된 카운트에 부정확성을 초래할 수 있습니다.\n\n이제 배치 이벤트 예제를 이해해 봅시다. 트위터는 PB 규모의 데이터를 처리하는 몇 가지 무거운 계산 파이프라인을 운영하고 맨해튼 데이터베이스에서 데이터를 시간별로 동기화합니다. 이제 시스템 동기화 작업이 1시간 이상 소요되고 다음 작업이 시작되기로 예정된 경우를 상상해 보십시오. 이는 시스템에 백프레셔가 증가하여 데이터 손실을 초래할 수 있는 상황으로 이어질 수 있습니다.\n\nTSAR 쿼리 서비스는 맨해튼과 캐시 서비스를 통합하여 클라이언트에 데이터를 제공합니다. 실시간 데이터 손실 가능성으로 인해 TSAR 서비스는 고객들에게 부정확한 메트릭을 제공할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n고객과 비즈니스에 미치는 영향을 이해하는 것을 시도해 봅시다:\n\n- 트위터 광고 서비스는 트위터의 주요 수익 모델 중 하나이며, 그 성능이 영향을 받으면 그들의 비즈니스 모델에 직접적인 영향을 끼칩니다.\n- 트위터는 인상과 관련 지표에 대한 정보를 검색할 수 있는 다양한 데이터 제품 서비스를 제공하며, 이러한 서비스는 부정확한 데이터로 인해 영향을 받을 수 있습니다.\n- 이 경우에 또다른 문제는 이벤트 생성부터 사용 가능할 때까지 배치 처리 작업으로 인해 몇 시간이 걸릴 수 있다는 점입니다. 이는 클라이언트가 수행해야 할 데이터 분석이나 기타 작업이 최신 데이터를 사용할 수 없게됨을 의미합니다. 몇 시간의 시차가 발생할 수 있습니다.\n\n이제 이는 사용자가 이벤트를 생성하고 사용자가 생성하는 이벤트에 기반한 사용자의 타임라인을 업데이트하려거나 트위터 시스템과 상호 작용하는 방식에 따라 사용자에 대한 행동 분석을 수행하려면 클라이언트가 배치 작업이 완료될 때까지 기다려야함을 의미합니다.\n\n# 새로운 아키텍처\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_6](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_6.png)\n\nTwitter의 새로운 아키텍처는 Twitter 데이터 센터 서비스와 Google Cloud 플랫폼 양쪽에 구축되었습니다. Twitter는 카파 주제를 퍼브 서브 주제로 변환하는 이벤트 처리 파이프 라인을 구축했으며, 이는 Google Cloud로 전송되었습니다. Google Cloud에서 실시간 집계를 수행하고 데이터를 BigTable에 싱크하는 스트리밍 데이터 플로우 작업이 수행되었습니다.\n\n![2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_7](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_7.png)\n\n서빙 레이어에는 Twitter가 Twitter 데이터 센터에 프런트 엔드와 Bigtable 및 BigQuery에 백엔드가 있는 LDC 쿼리 서비스를 사용합니다. 전체 시스템은 초당 수백만 건의 이벤트를 스트리밍할 수 있으며 지연 시간이 ~10밀리초까지 낮을 수 있습니다. 높은 트래픽 시 확장이 쉽습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 새로운 아키텍처는 일괄 파이프라인을 구축하는 비용을 절약해 줄 뿐만 아니라, 실시간 파이프라인에서는 트위터가 더 높은 집계 정확도와 안정적인 낮은 대기 시간을 달성할 수 있습니다. 또한, 그들은 여러 데이터 센터에서 다른 실시간 이벤트 집계를 유지할 필요가 없습니다.\n\n# 성능 비교\n\n![이미지](/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_8.png)\n\n새 아키텍처는 구 방식인 Heron 토폴로지와 비교하여 낮은 대기 시간을 제공하며 더 높은 처리량을 제공합니다. 또한, 새 아키텍처는 지연된 이벤트 계산을 처리하고 실시간 집계 중에 이벤트 손실이 없습니다. 또한, 새 아키텍처에는 일괄 구성 요소가 없어 구 방식에 있던 그것보다 설계를 단순화하고 컴퓨팅 비용을 줄입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\nTSAR로 구축된 이전 아키텍처를 트위터 데이터 센터와 구글 클라우드 플랫폼의 하이브리드 아키텍처로 이전함으로써 트위터는 수십억 이벤트를 실시간으로 처리하고 엔지니어들에게 낮은 대기 시간, 높은 정확성, 안정성, 아키텍처의 단순성 및 운영 비용 감소를 달성할 수 있었습니다.\n\nLinkedin: [Mayank Sharma의 Linkedin 프로필](https://www.linkedin.com/in/mayank-sharma-2002bb10b/)\n\n저와 모의 시스템 디자인 인터뷰 일정을 잡으려면 : [Meetapro에서 예약하기](https://www.meetapro.com/provider/listing/160769)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n나의 웹사이트: imayanks.com\n\n참고 자료:\n","ogImage":{"url":"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png"},"coverImage":"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003eTwitter는 약 400억 건의 이벤트를 실시간으로 처리하고 매일 페타바이트의 데이터를 생성합니다. Twitter는 분산 데이터베이스, Kafka, Twitter 이벤트 버스 등과 같은 다양한 이벤트 소스에서 데이터를 소비합니다. 여기에서 블로그의 구현을 찾을 수도 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이 블로그에서는 다음을 이해하려고 노력할 것입니다:\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e트위터가 이벤트를 처리하는 방식 및 이 방식에 대한 문제점은 무엇이 있었나요?\u003c/li\u003e\n\u003cli\u003e어떤 비즈니스와 고객 영향으로 인해 트위터가 새 아키텍처로 전환하게 되었나요?\u003c/li\u003e\n\u003cli\u003e새 아키텍처\u003c/li\u003e\n\u003cli\u003e이전 및 새로운 아키텍처의 성능 비교\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e트위터는 다음과 같은 내부 도구 세트를 보유하고 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eScalding은 트위터가 배치 처리에 사용하는 도구입니다.\u003c/li\u003e\n\u003cli\u003eHeron은 트위터의 스트리밍 엔진입니다.\u003c/li\u003e\n\u003cli\u003eTimeSeriesAggregator(TSAR)는 배치 및 실시간 처리에 사용됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이벤트 시스템이 어떻게 발전했는지 자세히 파헤치기 전에, 네 개의 내부 도구에 대해 간략히 알아보겠습니다.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eScalding\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e스콜딩(Scalding)은 하둡 맵리듀스 작업을 쉽게 지정할 수 있도록 도와주는 스칼라 라이브러리입니다. 스콜딩은 하둡의 하위 세부 사항을 추상화하는 자바 라이브러리인 카스케이딩(Cascading) 위에 구축되어 있습니다. 스콜딩은 Pig와 비교할 수 있지만, 스칼라와 강하게 통합되어 있어 스칼라의 장점을 하둡 맵리듀스 작업에 가져다줍니다.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eHeron\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e아파치 헤론(Heron)은 트위터의 자체 스트리밍 엔진으로, 페타바이트에 이르는 대량의 데이터를 처리하고, 개발자 생산성을 향상시키고, 디버깅을 단순화해야 하는 시스템의 필요성으로 개발되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e헤론에서 스트리밍 애플리케이션을 위한 구조를 토폴로지라고 합니다. 토폴로지는 데이터-컴퓨팅 요소를 나타내는 노드와 해당 요소 간에 흐르는 데이터 스트림을 나타내는 엣지로 이루어진 방향성 비순환 그래프입니다.\u003c/p\u003e\n\u003cp\u003e노드에는 2 종류가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e스파우트: 데이터 원본에 연결되어 데이터를 스트림에 삽입합니다.\u003c/li\u003e\n\u003cli\u003e볼트: 수신된 데이터를 처리하고 데이터를 방출합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_1.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e더 많은 정보를 보시려면 여기를 참고해주세요: \u003ca href=\"https://blog.x.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://blog.x.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eTimeSeriesAggregator\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_2.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eTwitter의 데이터 엔지니어링 팀은 매일 일괄 및 실시간으로 수조 건의 이벤트를 처리하는 과제에 직면했습니다. TSAR는 프레임워크 기반의 강력하고 확장 가능한 실시간 이벤트 시간대 시계열 집계 도구로, tweet과의 상호 작용을 집계하며 장치, 참여 유형 등 다양한 차원을 따라 분할하여 주로 참여 모니터링을 위해 구축되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e트위터에서 어떻게 일이 처리되었는지 대략적으로 살펴보겠습니다. 트위터의 모든 기능은 전 세계에 10만 개 이상의 인스턴스로 퍼져 있는 마이크로서비스에 의해 지원됩니다. 이들은 이벤트를 생성하고, 이를 이벤트 집계 레이어로 보내는 역할을 합니다. 이 이벤트 집계 레이어는 메타에서 제공하는 오픈 소스 프로젝트를 기반으로 구축되었으며, 이벤트를 그룹화하고 집계 작업을 실행하며 데이터를 HDFS에 저장하는 역할을 합니다. 그런 다음 이러한 이벤트를 처리하고 형식을 변환하여 데이터를 다시 압축하여 잘 생성된 데이터 세트를 만들어냅니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e이전 아키텍처\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e트위터의 구형 아키텍처는 람다 아키텍처를 기반으로 하고 있었어요. 이는 배치 레이어, 스피드 레이어 및 서빙 레이어로 구성되어 있어요. 배치 컴포넌트는 클라이언트가 생성한 로그이며, 이벤트 처리 후 하둡 분산 파일 시스템(HDFS)에 저장돼요. 트위터는 여러 스케일링 파이프라인을 구축하여 가공되지 않은 로그를 전처리하고 오프라인 소스로 Summingbird 플랫폼에 넣었어요. 실시간 컴포넌트 소스는 스피드 레이어의 일부인 Kafka 토픽들이에요.\u003c/p\u003e\n\u003cp\u003e데이터가 처리되면 배치 데이터는 맨해튼 분산 시스템에 저장되며, 실시간 데이터는 트위터의 자체 분산 캐시인 Nighthawk에 저장돼요. TSAR 시스템(예: TSAR 쿼리 서비스)은 캐시와 데이터베이스 둘 다 쿼리하는 서빙 레이어의 일부가 돼요.\u003c/p\u003e\n\u003cp\u003e트위터는 세 개의 다른 데이터 센터에 실시간 파이프라인과 쿼리 서비스를 가졌어요. 배치 컴퓨팅 비용을 줄이기 위해, 트위터는 한 데이터 센터에서 배치 파이프라인을 실행하고 데이터를 다른 두 데이터 센터로 복제해요.\u003c/p\u003e\n\u003cp\u003e실시간 데이터를 캐시에 저장했을까요? 데이터베이스에 저장하는 것보다 더 좋은 이유가 떠오르시나요?\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003ch1\u003e오래된 아키텍처의 도전 과제\u003c/h1\u003e\n\u003cp\u003e이 아키텍처가 실시간 이벤트의 경우 어떤 도전 과제를 가질 수 있는지 이해해 봅시다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e예를 통해 이해해 봅시다:\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e대규모 이벤트가 발생하면 FIFA 월드컵과 같은 큰 이벤트가 발생할 수 있습니다. 트윗 소스는 트윗 토폴로지로 많은 이벤트를 보낼 것입니다. 그러나 파싱 트윗 볼트가 이벤트를 적시에 처리하지 못해 토폴로지 내에서 백프레셔가 발생합니다. 시스템이 장기간 백프레셔 상태에 놓이면 헤론 볼트가 스파우트 라그를 축적할 수 있어 시스템의 지연 시간이 늘어날 수 있습니다. 트위터는 이러한 경우에 토폴로지 랙이 줄어드는 데 매우 오랜 시간이 걸린다고 관찰했습니다.\u003c/p\u003e\n\u003cp\u003e팀이 사용한 운영 솔루션은 헤론 컨테이너를 다시 시작하여 데이터 스트림 처리를 다시 시작하는 것이었습니다. 이로 인해 이행 중에 이벤트 손실이 발생할 수 있으며, 이는 캐시의 집계된 카운트에 부정확성을 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이제 배치 이벤트 예제를 이해해 봅시다. 트위터는 PB 규모의 데이터를 처리하는 몇 가지 무거운 계산 파이프라인을 운영하고 맨해튼 데이터베이스에서 데이터를 시간별로 동기화합니다. 이제 시스템 동기화 작업이 1시간 이상 소요되고 다음 작업이 시작되기로 예정된 경우를 상상해 보십시오. 이는 시스템에 백프레셔가 증가하여 데이터 손실을 초래할 수 있는 상황으로 이어질 수 있습니다.\u003c/p\u003e\n\u003cp\u003eTSAR 쿼리 서비스는 맨해튼과 캐시 서비스를 통합하여 클라이언트에 데이터를 제공합니다. 실시간 데이터 손실 가능성으로 인해 TSAR 서비스는 고객들에게 부정확한 메트릭을 제공할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e고객과 비즈니스에 미치는 영향을 이해하는 것을 시도해 봅시다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e트위터 광고 서비스는 트위터의 주요 수익 모델 중 하나이며, 그 성능이 영향을 받으면 그들의 비즈니스 모델에 직접적인 영향을 끼칩니다.\u003c/li\u003e\n\u003cli\u003e트위터는 인상과 관련 지표에 대한 정보를 검색할 수 있는 다양한 데이터 제품 서비스를 제공하며, 이러한 서비스는 부정확한 데이터로 인해 영향을 받을 수 있습니다.\u003c/li\u003e\n\u003cli\u003e이 경우에 또다른 문제는 이벤트 생성부터 사용 가능할 때까지 배치 처리 작업으로 인해 몇 시간이 걸릴 수 있다는 점입니다. 이는 클라이언트가 수행해야 할 데이터 분석이나 기타 작업이 최신 데이터를 사용할 수 없게됨을 의미합니다. 몇 시간의 시차가 발생할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이제 이는 사용자가 이벤트를 생성하고 사용자가 생성하는 이벤트에 기반한 사용자의 타임라인을 업데이트하려거나 트위터 시스템과 상호 작용하는 방식에 따라 사용자에 대한 행동 분석을 수행하려면 클라이언트가 배치 작업이 완료될 때까지 기다려야함을 의미합니다.\u003c/p\u003e\n\u003ch1\u003e새로운 아키텍처\u003c/h1\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_6.png\" alt=\"2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_6\"\u003e\u003c/p\u003e\n\u003cp\u003eTwitter의 새로운 아키텍처는 Twitter 데이터 센터 서비스와 Google Cloud 플랫폼 양쪽에 구축되었습니다. Twitter는 카파 주제를 퍼브 서브 주제로 변환하는 이벤트 처리 파이프 라인을 구축했으며, 이는 Google Cloud로 전송되었습니다. Google Cloud에서 실시간 집계를 수행하고 데이터를 BigTable에 싱크하는 스트리밍 데이터 플로우 작업이 수행되었습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_7.png\" alt=\"2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_7\"\u003e\u003c/p\u003e\n\u003cp\u003e서빙 레이어에는 Twitter가 Twitter 데이터 센터에 프런트 엔드와 Bigtable 및 BigQuery에 백엔드가 있는 LDC 쿼리 서비스를 사용합니다. 전체 시스템은 초당 수백만 건의 이벤트를 스트리밍할 수 있으며 지연 시간이 ~10밀리초까지 낮을 수 있습니다. 높은 트래픽 시 확장이 쉽습니다.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e이 새로운 아키텍처는 일괄 파이프라인을 구축하는 비용을 절약해 줄 뿐만 아니라, 실시간 파이프라인에서는 트위터가 더 높은 집계 정확도와 안정적인 낮은 대기 시간을 달성할 수 있습니다. 또한, 그들은 여러 데이터 센터에서 다른 실시간 이벤트 집계를 유지할 필요가 없습니다.\u003c/p\u003e\n\u003ch1\u003e성능 비교\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-23-HowTwitterimprovedtheprocessingof400billionevents_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e새 아키텍처는 구 방식인 Heron 토폴로지와 비교하여 낮은 대기 시간을 제공하며 더 높은 처리량을 제공합니다. 또한, 새 아키텍처는 지연된 이벤트 계산을 처리하고 실시간 집계 중에 이벤트 손실이 없습니다. 또한, 새 아키텍처에는 일괄 구성 요소가 없어 구 방식에 있던 그것보다 설계를 단순화하고 컴퓨팅 비용을 줄입니다.\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003eTSAR로 구축된 이전 아키텍처를 트위터 데이터 센터와 구글 클라우드 플랫폼의 하이브리드 아키텍처로 이전함으로써 트위터는 수십억 이벤트를 실시간으로 처리하고 엔지니어들에게 낮은 대기 시간, 높은 정확성, 안정성, 아키텍처의 단순성 및 운영 비용 감소를 달성할 수 있었습니다.\u003c/p\u003e\n\u003cp\u003eLinkedin: \u003ca href=\"https://www.linkedin.com/in/mayank-sharma-2002bb10b/\" rel=\"nofollow\" target=\"_blank\"\u003eMayank Sharma의 Linkedin 프로필\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e저와 모의 시스템 디자인 인터뷰 일정을 잡으려면 : \u003ca href=\"https://www.meetapro.com/provider/listing/160769\" rel=\"nofollow\" target=\"_blank\"\u003eMeetapro에서 예약하기\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e나의 웹사이트: imayanks.com\u003c/p\u003e\n\u003cp\u003e참고 자료:\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-23-HowTwitterimprovedtheprocessingof400billionevents"},"buildId":"bb_yO9GbCvdfc_n71SfUf","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>