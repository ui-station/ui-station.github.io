<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>알고 있는 것은 기억하는 것과도 같아요 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-23-ToKnowIsAlsotoRemember" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="알고 있는 것은 기억하는 것과도 같아요 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="알고 있는 것은 기억하는 것과도 같아요 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-23-ToKnowIsAlsotoRemember" data-gatsby-head="true"/><meta name="twitter:title" content="알고 있는 것은 기억하는 것과도 같아요 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-23 17:22" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_buildManifest.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">알고 있는 것은 기억하는 것과도 같아요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="알고 있는 것은 기억하는 것과도 같아요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 23, 2024</span><span class="posts_reading_time__f7YPP">13<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-23-ToKnowIsAlsotoRemember&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png" alt="이미지"></p>
<p>한 남자와 한 여자가 임상 연구 센터의 조용한 방 안에서 대화를 나누고 있습니다. 여자는 질문을 하고는 남자가 대답할 때까지 기다리면서 몇 가지 노트를 적습니다. 그냥 보통 대화처럼 보일 수도 있지만, 실제로는 전혀 보통이 아닙니다. 여자의 노트북 안에는 매 페이지마다 써 있는 날짜와 상관없이 남자의 대답이 항상 동일합니다. 대화가 80년대에 발생했더라도, 대답은 10년 이상 전에 일어난 사건을 참조하고 있습니다. Jenni Ogden은 나중에 네오심리학에 영향을 미치면서 그의 진짜 이름인 Henry Molaison으로 더 잘 알려지게 된 환자 H.M.과 대화를 나눈 최초의 연구자 중 한 명이었습니다. 며칠 후, 연구진은 Henry가 27세 때 받았던 뇌 절제술로 인해 새로운 기억을 생성하는 능력을 상실했다고 결론 내렸습니다. Henry의 사례는 뇌 기능과 기억 사이의 연결을 이해하고 단기와 장기 기억이라는 개념을 만들어내는 데 도움이 되었습니다. 이 개념은 기계 학습 분야에서 혁신적인 연구를 위한 토대를 마련했으며, 과학자들과 개발자들이 뇌의 신비한 내부 구조에서 더 나은 예측 모델을 만드는 데 노력하고 있습니다.</p>
<h1>소개</h1>
<p>인공 신경망(ANN)은 우리 뇌에서 작동하는 실제 신경망에서 영감을 받았습니다. 실제로 ANNs는 실제 신경세포가 어떻게 상호 연결되고 위에서 설명한 상황을 설명하는 추상화일 뿐입니다. 개미군 최적화, 차분 진화, 입자 미래 등의 프로세스와 유사하게, ANNs는 실제 과정의 본질을 포착하여 현재 대부분의 AI 솔루션 뒤에 있는 알고리즘을 설계하는 데 사용됩니다. ANNs가 정말로 학습하는지, 그들이 하는 일을 지능이라고 해야 하는지에 대한 논의는 넓고 계속됩니다. 그러나 그들의 다용도성과 성능은 부정할 수 없습니다. 새로운 ANN 구성은 매일 개발되고 있으며 다양한 문제에 성공적으로 적용되고 있습니다. 이러한 변형의 대부분은 여전히 실제 신경망의 행동에서 영감을 받고 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>RNN(RNNs)은 일련의 메모리 구성요소를 통합하여 처리하는데, 수년 전에 자연어 처리(NLP)에서 중요한 접근 방식을 나타냅니다. RNNs는 Long-Short Term Memory (LSTM) Networks로 나아가는 길을 열며, NLP 응용 프로그램에서 신경망의 성능을 높였습니다. 이후 LSTM 네트워크는 트랜스포머 모델과 GPT(Generative Pre-trained Transformer)에 의해 대체되었는데, 이것이 ChatGPT의 기초가 되었습니다. 이 기사에서는 LSTM 네트워크가 무엇이며, 그들을 특별하게 만드는 이유에 대해 살펴봅니다.</p>
<h1>RNN의 의미</h1>
<p>LSTM 네트워크가 어떻게 작동하는지 이해하기 위해서는 그 목적에 대해 생각해 보는 것이 중요합니다. RNN과 LSTM 네트워크는 비슷한 목표를 따릅니다. 이들은 순차적으로 저장된 데이터를 모델링하고 예측하는 데 사용됩니다. 이는 이 유형의 네트워크가 데이터 시퀀스를 읽고 다음 값이 무엇인지 예측하려고 한다는 것을 의미합니다. 특정 도시의 지난 30일간의 평균 온도를 기록한 로그가 있다고 가정해 봅시다. 그리고 31일차의 온도를 추정하고 싶다면 어떻게 할까요? 한 가지 방법은 온도를 다른 변수에 상관시켜서, 31일에 이러한 변수의 값에 따라 새로운 온도를 추정하는 것입니다. RNN은 몇 일, 예를 들어 30일 이내의 일부 날짜를 고려하여 이전 온도 값을 기반으로 31일의 온도를 예측합니다. 한 마디로, RNN은 시퀀스를 기억하고 다음 값 또는 값 그룹을 제시하려고 노력합니다. 이전에 작성한 기사 중에 RNN이 메모리 전문가와 어떻게 비교되면서 RNN이 어떻게 단계적으로 작동하는지 설명했습니다.</p>
<p>이전 날짜의 온도를 기반으로 새로운 온도를 예측하는 아이디어는 다른 응용 분야로 확장할 수 있습니다. 소개에서 언급된 것처럼, RNN은 NLP 중 첫 접근 방식 중 하나였습니다. 아이디어는 RNN을 텍스트로 학습한 다음, RNN을 사용하여 입력 후 다음에 나오는 단어 또는 단어 그룹을 예측하는 것입니다. 이러한 아이디어는 자동 번역뿐만 아니라 음성 및 필기 인식과 같은 과제에도 적용할 수 있습니다. RNN이 이러한 과제들을 다룰 때 직면하는 문제 중 하나는 죽거나 폭발하는 기울기(vanishing/exploding gradients)로 인한 어려움입니다. 이 문제의 해법은 LSTM 네트워크의 적용입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Figure 1은 완전히 연결된 인공 신경망(ANNs)과 순환 신경망(RNNs) 간의 주요 구조적 차이를 보여줍니다. 이 간단한 예에서는 (X1,Y1) 및 (X2,Y2)의 값이 Y3의 새로운 값 계산에 사용됩니다. 실제로는 ANN과 RNN이 많은 입력-출력 쌍으로 훈련됩니다. 훈련 과정이 완료되면 네트워크는 새로운 값을 예측하는 데 사용됩니다. 이 프로세스에 익숙하지 않다면, 이 기사의 끝에 유용한 참고 자료를 추가했습니다. 여기에 이 프로세스를 설명한 나의 시도도 곁들였습니다. ANNs와 RNNs 사이의 훈련 및 예측의 일반적인 아이디어는 비슷하지만, 구조적으로 큰 차이가 있습니다. RNN에서는 Y1과 Y2의 값이 X1과 X2 대신 네트워크를 훈련하는 데 사용됨을 주목하십시오. 또한 첫 번째 단위와 두 번째 단위를 연결하는 가중치가 있으며, 이는 이전 단위에서 온 활성화를 나타냅니다. 이 가중치는 RNN의 "기억" 구성 요소를 나타냅니다. 더 큰 가중치는 RNN이 이전 값에 더 많은 중요성을 부여함을 의미하고, 더 작은 가중치는 RNN이 과거 값을 잘 기억하지 못한다는 것을 의미합니다. 이것은 신경망 구조의 가중치이므로, 그 값은 프로세스 중에 학습되며, RNN은 재현하려는 순서가 더 많은지 덜 많은지를 결정할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_1.png" alt="Image"></p>
<p>RNN의 훈련 및 적용 중 중요한 측면은 네트워크가 읽고 훈련 및 예측에 사용하는 값 시퀀스의 길이입니다. 시퀀스 길이가 15라고 가정해 봅시다. 이는 RNN이 15개의 입력 값을 읽은 후 16번째 값을 찾아 훈련한다는 것을 의미합니다(항상 그렇지는 않습니다. 동적 RNN도 있기 때문입니다). 시퀀스 길이로 돌아가보면, 더 긴 시퀀스 길이는 RNN이 최종 출력에 여러 읽기를 통합할 수 있어 유익합니다. 그러나 더 긴 시퀀스 길이는 사라지는/폭주하는 그래디언트를 유발합니다. LSTM 네트워크는 이 문제를 어떻게 극복할까요?</p>
<h1>LSTM 네트워크</h1>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>LSTM 네트워크는 어떤 정보를 “기억”하고 어떤 정보를 “잊을지” 결정하도록 훈련됩니다. RNN에서는 이전 유닛의 활성화에만 적용되는 메모리 구성 요소에 대한 가중치가 있습니다. 그러나 LSTM에서는 메모리 구성 요소의 개념이 장기 메모리 구성 요소(셀 상태)와 단기 메모리 구성 요소(은닉 상태)로 대체됩니다. 각 구성 요소는 서로 다른 게이트에 분산된 일련의 편향 및 가중치와 관련이 있습니다. 이는 각 입력이 네트워크에서 얼마나 많은 정보를 유지하고 얼마나 버릴지 결정하는 게이트(또는 단계)를 통과한다는 것을 의미합니다. 이 결정은 훈련 과정 중에 학습된 가중치와 편향 값에 기반합니다.</p>
<p>그림 2는 단일 입력을 읽는 매우 간단한 LSTM 네트워크 스케치를 보여줍니다. 실제 LSTM 네트워크 유닛을 살펴보기 전에이 단순화된 다이어그램을 먼저 분석하겠습니다. RNN을 나타내는 이전 그림과 얼마나 다른지에 주목하세요. 기억할 점 중 첫 번째는 입력 값 외에도 LSTM 네트워크에는 단기 및 장기 메모리 구성 요소가 있다는 것입니다. 이러한 구성 요소는 공식적으로 셀 상태(C)와 숨겨진 상태(h)로 알려져 있습니다. 이 표기법은 나중에 사용되겠지만, 우리는 현재 이전 이름을 사용할 것입니다. 입력 및 메모리 구성 요소는 세 가지 서로 다른 게이트를 통과합니다: 삭제, 입력 및 출력.</p>
<ul>
<li>삭제 게이트는 장기 기억의 얼마나 보관해야 하는지를 결정합니다. 이 게이트는 현재 입력뿐만 아니라 단기 메모리 구성 요소를 고려하여 0과 1 사이의 값을 계산하여 장기 메모리 구성 요소를 곱합니다. 0의 삭제 게이트는 네트워크가 이전 정보를 보존하지 않음을 의미합니다. 그 답은 새로운 입력에만 기초합니다.</li>
<li>입력 게이트는 새 정보가 장기 메모리 구성 요소에 보존되어야 하는 양을 결정합니다. 이 게이트의 출력은 다음 입력에 보존되는 장기 기억 구성 요소에 추가됩니다. 이 구성 요소가 삭제 및 입력 게이트에만 연결된다는 점에 유의하십시오. 이는 각 반복에서 장기 기억이 무엇을 버리고 어떤 정보를 추가해야 하는지에 따라 업데이트된다는 것을 의미합니다.</li>
<li>출력 게이트는 입력 및 단기 메모리 구성 요소를 고려하고 이전 단기 메모리 구성 요소로 저장될 새로운 장기 메모리 구성 요소의 양을 계산합니다. 이는 LSTM 네트워크에서 나온 최종 값이 장기 및 단기 메모리 구성 요소뿐만 아니라 출력 게이트도 고려하여 계산된다는 것을 의미합니다.</li>
</ul>
<img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_2.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이제 LSTM 네트워크 셀의 구조를 파악했으니 여기서 발생하는 계산에 더 가깝게 살펴보겠습니다. Figure 3은 LSTM 네트워크 셀을 더 자세히 보여줍니다. 입력, 단기 기억 구성 요소 및 장기 기억 구성 요소는 각각 x, h 및 C로 표시됩니다. 이들 각각의 글자는 분석 중인 시간 기간에 해당하는 아래 첨자를 가지고 있습니다. t-1의 아래 첨자는 값이 이전 반복에 속한다는 것을 의미합니다. 예를 들어, forget gate는 현재 입력(xt)과 이전 반복의 단기 기억 구성 요소(ht-1)를 고려합니다. 게이트들은 또한 일련의 가중치와 편향을 고려합니다. forget 및 output 게이트에는 각각 3개의 매개변수가 있습니다:</p>
<ul>
<li>편향(bxf, bxo)</li>
<li>입력을 곱하는 가중치(wxf, wxo)</li>
<li>단기 기억 구성 요소를 곱하는 가중치(whf, who)</li>
</ul>
<p>가중치와 편향은 활성화 함수로 들어가기 전에 입력 값에 곱해지고 더해집니다. 이 예에서 forget 및 output 게이트 모두 시그모이드 활성화 함수를 가지며 그 최종 활성화(af, ao)는 Figure 3 우측에 표시됩니다. 이러한 게이트와 달리, input 게이트는 두 개의 활성화 함수, 시그모이드 및 tanh가 있으며 해당 가중치와 편향을 가집니다. 이는 단일 LSTM 네트워크 단위에 대해 훈련해야 할 매개변수의 총 수가 12임을 의미합니다.</p>
<p>LSTM 네트워크 셀에 대해 이해해야 할 마지막 중요한 측면은 새로운 C 및 h가 어떻게 계산되는지입니다. 이전 장기 기억 구성 요소는 먼저 forget 게이트의 출력과 곱해진 후 입력 게이트의 결과에 추가됩니다. 이는 forget 게이트가 C의 얼마나 다음 반복에 전달하는지를 결정하고, input 게이트가 C의 새 값에 얼마나 추가할지를 결정합니다. 단기 기억 구성 요소 계산을 위해 output 게이트의 결과는 새로운 C의 tanh와 곱해집니다. 이는 output 게이트가 장기 기억 구성 요소를 다음 반복에 얼마나 전달할지 결정합니다. C의 값이 1 이상일 수 있으므로, 값이 -1과 1 사이로 제한되도록 tanh 연산이 적용됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_3.png" alt="이미지"></p>
<p>입력이 모든 게이트를 통과하면, 새로운 C와 h가 다음 반복으로 전달되어 새 입력과 상호 작용합니다(그림 4). 이 과정은 시퀀스의 모든 값에 대해 반복되며, 해당 시퀀스의 최종 h에 도달할 때까지 반복됩니다.</p>
<p><img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_4.png" alt="이미지"></p>
<h1>앞으로 나아가기</h1>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>LSTM 네트워크는 다른 인공 신경망(ANN)에서 사용되는 방법론과 유사한 방법으로 훈련될 수 있습니다. 문제에 따라 각 순전파 사이클 이후 업데이트되는 손실 함수를 정의합니다. 그런 다음 이 손실 함수를 사용하여 역전파 과정을 통해 가중치와 편향을 업데이트합니다. RNN 및 LSTM 네트워크의 경우, 역전파는 일반적으로 모든 반복 유닛을 통해 가중치와 편향을 누적하는 과정이기 때문에 시간을 거슬러 역전파(backpropagation through time, BPTT)라고합니다. 이는 LSTM 네트워크의 단일 유닛을 읽는 LSTM 네트워크 셀에서 순전파 과정이 어떻게 진행되는지 설명하는 간단한 구현과 순전파, 역전파 과정에 대해 상세히 설명하는 주피터 노트북입니다.</p>
<p>그림 5는 LSTM 네트워크 셀에서 단일 유닌을 읽는 순전파 과정의 예시를 보여줍니다. h의 최종 값이 네트워크 내 모든 게이트를 통해 전달되는 정보를 함께 전달하는 반면, 최종 C는 출력 게이트와 상호작용하지 않는 것에 주목하세요. 이 게이트들 각각이 보존할 정보와 잊을 정보를 규제자로 작용합니다. 입력과 상호작용하는 최적의 방법을 학습하는 완전히 연결된 ANN에서 가중치와 편향이 학습되는 것과 유사하게, LSTM 네트워크에서 매개변수는 보존하거나 버릴 최적의 정보 양을 학습하기 위해 훈련됩니다.</p>
<p><img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_5.png" alt="image"></p>
<h1>더 많은 유닛?</h1>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>지금까지의 숫자와 예제는 단일 유단 LSTM 네트워크를 보여주었지만, 다른 유형의 네트워크와 마찬가지로 LSTM 네트워크는 여러 개의 유닌을 가질 수 있습니다. 단위 수에 따라 매개 변수 수는 어떻게 변하나요? 두 개의 유닌을 갖는 LSTM 네트워크의 경우, 학습할 매개 변수가 12개가 아닌 32개가 있습니다. 추가된 20개의 매개 변수는 어디에 있을까요? 그러면, 이제 조금 복잡해 질 것입니다. 이를 여러 부분으로 나눠서 살펴보겠습니다.</p>
<p>첫 번째 유닛에는 12개의 매개 변수가 있습니다. 이전에 설명한 것과 같은 매개 변수들입니다: 입력을 곱하는 4개의 가중치, 숨겨진 상태(h)를 곱하는 4개의 가중치, 그리고 각 게이트에 대한 4개의 바이어스입니다. 두 번째 유닌도 12개의 관련된 매개 변수를 가지고 있습니다. 이는 지금까지 총 24개의 매개 변수를 가지게 되었다는 것을 의미합니다.</p>
<p>LSTM 네트워크는 특정 유형의 RNN이므로 각 유닌 사이에 연결이 있을 것입니다. 이는 유닌 1에서 처리된 정보가 유닌 2로 전달된다는 것을 의미합니다. 각각의 연결은 고유의 가중치를 갖습니다. 두 개의 유닌을 갖는 LSTM 네트워크에서, 이전에 언급된 24개의 매개 변수 외에, 유닌 2의 각 게이트와 유닌 1의 각 게이트 사이의 연결 및 유닌 1의 숨겨진 상태와 유닌 2의 게이트 사이의 연결에 해당하는 8개의 매개 변수가 추가로 필요합니다. 그림 7은 유닌 2의 잊기 게이트에서 활성화를 계산하는 방법을 보여줍니다. 이 게이트가 이전 게이트와 이전 숨겨진 상태에 연결되어 있다는 점에 주목하세요. 그림에는 새로운 가중치가 5개만 표시되어 있지만, 실제로는 첫 번째 유닌의 숨겨진 상태가 두 번째 유닌의 각 게이트에 연결되기 때문에 8개의 가중치가 있습니다. n개의 유닌에 대한 매개 변수 수는 12n+4n(n-1) 또는 간소화된 표현으로 8(n+n²/2)입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_7.png">
<h1>Going backwards</h1>
<p>인공 신경망(ANNs)에서 흔히 볼 수 있는 바와 같이, 역전파 프로세스는 일반적으로 이해하고 구현하기 가장 어려운 부분입니다. 단일 유닛 LSTM 네트워크에서는 각 역전파가 4개의 편향과 8개의 가중치를 업데이트해야 하며, h(t-1) 및 C(t-1)도 업데이트해야 합니다. h(t-1)가 출력 게이트에 의존하고 현재 C는 다시 입력 및 망각 게이트에 따라 달라짐을 주목해야 합니다. 손실에 대한 편도함수를 계산할 때 이 사항을 고려하는 것이 중요합니다. 이전에 언급한 바와 같이, 이 Jupyter 노트북에는 역전파 프로세스를 포함한 간단한 LSTM 네트워크를 구축하는 데 필요한 모든 방정식이 포함되어 있습니다. 그림 8은 각 편도함수를 계산하는 데 도움이 되는 매개변수 간 의존성을 보여줍니다.</p>
<img src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_8.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>응용 프로그램</h1>
<p>다음 섹션에는 LSTM 네트워크의 세 가지 응용 프로그램 예시가 포함되어 있습니다. 예시는 간단한 순서로 제공됩니다. 각 예시에 대한 파이썬 코드를 이 Jupyter 노트북에서 찾을 수 있습니다.</p>
<h2>연속 함수 모델링을 위한 바닐라 LSTM 네트워크</h2>
<p>이것은 처음부터 LSTM 네트워크를 구현하는 매우 간단한 예시입니다. 여기서 배울 중요한 교훈은 LSTM 네트워크에 데이터를 공급하기 전에 데이터를 올바르게 준비하는 중요성입니다. LSTM 네트워크로 모델링하고 싶은 연속 함수가 있다면, 먼저 입력-타겟 데이터의 쌍을 생성해야 합니다 (Figure 1 참조). 이 데이터는 시퀀스 길이에 따라 달라집니다. 예를 들어, 시퀀스 길이가 15인 경우, 각 입력 항목은 15개의 값이 포함되며 16번째 값은 해당 입력의 타겟이 됩니다. 네트워크에 입력하기 전에 데이터를 정규화하는 것도 중요합니다. 이 예시에서는 sin(x) 함수와 함께 웰에서의 석유 생산 행동을 모델링하기 위해 간단한 LSTM 네트워크가 사용됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>워드 예측기를 구축하기 위한 바닐라 LSTM 네트워크</h2>
<p>이 예시에서는 이전의 바닐라 LSTM 네트워크가 워드 예측 문제에 적용되었습니다. 짧은 텍스트로 훈련된 후, 모델은 다음에 나올 단어를 예측합니다. 실제로 워드 처리 및 워드 예측 문제는 이 예시처럼 다가가지 않습니다. 그러나 LSTM 네트워크의 가능한 응용에 대한 간단하고 명확한 설명입니다.</p>
<h2>Keras의 LSTM 네트워크를 사용하여 워드 예측기 구축</h2>
<p>이 예시는 Keras의 LSTM 네트워크를 사용하여 긴 텍스트로 훈련된 후 다음 단어가 무엇인지 예측하는 더 현실적인 예제입니다. 이 예시에서는 NLP 문제에서 일반적인 추가인 임베딩 레이어를 사용합니다. 임베딩 레이어는 단어의 정수로 인코딩된 표현(인덱스)을 밀집된 벡터로 변환하여 단어 사이의 관계를 더 잘 모델링하는 데 도움이 되는 고정 크기의 벡터로 변환합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>결론</h1>
<p>인공 신경망 및 특히 순환 신경망을 사용하여 자연어 처리 문제에 접근하거나 연속 데이터를 모델링하는 것은 최근에 개발된 것이 아닙니다. 이 응용 프로그램은 오랫동안 존재해 왔으며 새로운 기능으로 계속 발전하고 있습니다. LSTM 네트워크가 어떻게 작동하는지 이해하고 그것을 특별한 종류의 RNN으로 만드는 요소를 파악하는 것은 결과와 예상대로 작동하지 않을 수 있는 이유에 대한 통찰력을 제공할 수 있습니다. 본문은 LSTM 네트워크에 대한 포괄적인 설명을 포함하고, 그 응용 예시 세 가지를 제시합니다. 대부분의 현재 NLP 도구 및 솔루션은 다른 네트워크 구조에 의존하지만 LSTM 네트워크 내부 작업에 대한 탄탄한 개념은 머신러닝 분야에서 항상 유익할 것입니다. 이것을 장기 기억 셀에 저장하는 것을 기억하세요! 😉</p>
<h1>참고문헌</h1>
<ul>
<li>Ng, Andrew. Machine Learning Specialization.</li>
<li>Keras 'Embedding' 레이어는 어떻게 작동합니까? CrossValidated 게시물. 2017</li>
<li>Cowan, Nelson (2009). 장기, 단기 및 직업기억 사이의 차이점은 무엇인가? — PMC. Prog Brain Res. 2008;169:323–38. doi: 10.1016/S0079–6123(07)00020–9. PMID: 18394484; PMCID: PMC2657600.</li>
<li>Erz, Hendrik (2023). ChatGPT를 생산적으로 사용하는 방법 | Hendrik Erz. hendrik-erz.de, 2023년 2월 14일</li>
<li>Adams, Tim (2013). Henry Molaison: 우리가 결코 잊지 않을 기억상실자 | 기억 | The Guardian</li>
<li>Dittrich, Luke (2016). 기억할 수 없던 두뇌 — 뉴욕 타임즈</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"알고 있는 것은 기억하는 것과도 같아요","description":"","date":"2024-05-23 17:22","slug":"2024-05-23-ToKnowIsAlsotoRemember","content":"\n![이미지](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png)\n\n한 남자와 한 여자가 임상 연구 센터의 조용한 방 안에서 대화를 나누고 있습니다. 여자는 질문을 하고는 남자가 대답할 때까지 기다리면서 몇 가지 노트를 적습니다. 그냥 보통 대화처럼 보일 수도 있지만, 실제로는 전혀 보통이 아닙니다. 여자의 노트북 안에는 매 페이지마다 써 있는 날짜와 상관없이 남자의 대답이 항상 동일합니다. 대화가 80년대에 발생했더라도, 대답은 10년 이상 전에 일어난 사건을 참조하고 있습니다. Jenni Ogden은 나중에 네오심리학에 영향을 미치면서 그의 진짜 이름인 Henry Molaison으로 더 잘 알려지게 된 환자 H.M.과 대화를 나눈 최초의 연구자 중 한 명이었습니다. 며칠 후, 연구진은 Henry가 27세 때 받았던 뇌 절제술로 인해 새로운 기억을 생성하는 능력을 상실했다고 결론 내렸습니다. Henry의 사례는 뇌 기능과 기억 사이의 연결을 이해하고 단기와 장기 기억이라는 개념을 만들어내는 데 도움이 되었습니다. 이 개념은 기계 학습 분야에서 혁신적인 연구를 위한 토대를 마련했으며, 과학자들과 개발자들이 뇌의 신비한 내부 구조에서 더 나은 예측 모델을 만드는 데 노력하고 있습니다.\n\n# 소개\n\n인공 신경망(ANN)은 우리 뇌에서 작동하는 실제 신경망에서 영감을 받았습니다. 실제로 ANNs는 실제 신경세포가 어떻게 상호 연결되고 위에서 설명한 상황을 설명하는 추상화일 뿐입니다. 개미군 최적화, 차분 진화, 입자 미래 등의 프로세스와 유사하게, ANNs는 실제 과정의 본질을 포착하여 현재 대부분의 AI 솔루션 뒤에 있는 알고리즘을 설계하는 데 사용됩니다. ANNs가 정말로 학습하는지, 그들이 하는 일을 지능이라고 해야 하는지에 대한 논의는 넓고 계속됩니다. 그러나 그들의 다용도성과 성능은 부정할 수 없습니다. 새로운 ANN 구성은 매일 개발되고 있으며 다양한 문제에 성공적으로 적용되고 있습니다. 이러한 변형의 대부분은 여전히 실제 신경망의 행동에서 영감을 받고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nRNN(RNNs)은 일련의 메모리 구성요소를 통합하여 처리하는데, 수년 전에 자연어 처리(NLP)에서 중요한 접근 방식을 나타냅니다. RNNs는 Long-Short Term Memory (LSTM) Networks로 나아가는 길을 열며, NLP 응용 프로그램에서 신경망의 성능을 높였습니다. 이후 LSTM 네트워크는 트랜스포머 모델과 GPT(Generative Pre-trained Transformer)에 의해 대체되었는데, 이것이 ChatGPT의 기초가 되었습니다. 이 기사에서는 LSTM 네트워크가 무엇이며, 그들을 특별하게 만드는 이유에 대해 살펴봅니다.\n\n# RNN의 의미\n\nLSTM 네트워크가 어떻게 작동하는지 이해하기 위해서는 그 목적에 대해 생각해 보는 것이 중요합니다. RNN과 LSTM 네트워크는 비슷한 목표를 따릅니다. 이들은 순차적으로 저장된 데이터를 모델링하고 예측하는 데 사용됩니다. 이는 이 유형의 네트워크가 데이터 시퀀스를 읽고 다음 값이 무엇인지 예측하려고 한다는 것을 의미합니다. 특정 도시의 지난 30일간의 평균 온도를 기록한 로그가 있다고 가정해 봅시다. 그리고 31일차의 온도를 추정하고 싶다면 어떻게 할까요? 한 가지 방법은 온도를 다른 변수에 상관시켜서, 31일에 이러한 변수의 값에 따라 새로운 온도를 추정하는 것입니다. RNN은 몇 일, 예를 들어 30일 이내의 일부 날짜를 고려하여 이전 온도 값을 기반으로 31일의 온도를 예측합니다. 한 마디로, RNN은 시퀀스를 기억하고 다음 값 또는 값 그룹을 제시하려고 노력합니다. 이전에 작성한 기사 중에 RNN이 메모리 전문가와 어떻게 비교되면서 RNN이 어떻게 단계적으로 작동하는지 설명했습니다.\n\n이전 날짜의 온도를 기반으로 새로운 온도를 예측하는 아이디어는 다른 응용 분야로 확장할 수 있습니다. 소개에서 언급된 것처럼, RNN은 NLP 중 첫 접근 방식 중 하나였습니다. 아이디어는 RNN을 텍스트로 학습한 다음, RNN을 사용하여 입력 후 다음에 나오는 단어 또는 단어 그룹을 예측하는 것입니다. 이러한 아이디어는 자동 번역뿐만 아니라 음성 및 필기 인식과 같은 과제에도 적용할 수 있습니다. RNN이 이러한 과제들을 다룰 때 직면하는 문제 중 하나는 죽거나 폭발하는 기울기(vanishing/exploding gradients)로 인한 어려움입니다. 이 문제의 해법은 LSTM 네트워크의 적용입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nFigure 1은 완전히 연결된 인공 신경망(ANNs)과 순환 신경망(RNNs) 간의 주요 구조적 차이를 보여줍니다. 이 간단한 예에서는 (X1,Y1) 및 (X2,Y2)의 값이 Y3의 새로운 값 계산에 사용됩니다. 실제로는 ANN과 RNN이 많은 입력-출력 쌍으로 훈련됩니다. 훈련 과정이 완료되면 네트워크는 새로운 값을 예측하는 데 사용됩니다. 이 프로세스에 익숙하지 않다면, 이 기사의 끝에 유용한 참고 자료를 추가했습니다. 여기에 이 프로세스를 설명한 나의 시도도 곁들였습니다. ANNs와 RNNs 사이의 훈련 및 예측의 일반적인 아이디어는 비슷하지만, 구조적으로 큰 차이가 있습니다. RNN에서는 Y1과 Y2의 값이 X1과 X2 대신 네트워크를 훈련하는 데 사용됨을 주목하십시오. 또한 첫 번째 단위와 두 번째 단위를 연결하는 가중치가 있으며, 이는 이전 단위에서 온 활성화를 나타냅니다. 이 가중치는 RNN의 \"기억\" 구성 요소를 나타냅니다. 더 큰 가중치는 RNN이 이전 값에 더 많은 중요성을 부여함을 의미하고, 더 작은 가중치는 RNN이 과거 값을 잘 기억하지 못한다는 것을 의미합니다. 이것은 신경망 구조의 가중치이므로, 그 값은 프로세스 중에 학습되며, RNN은 재현하려는 순서가 더 많은지 덜 많은지를 결정할 수 있습니다.\n\n![Image](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_1.png)\n\nRNN의 훈련 및 적용 중 중요한 측면은 네트워크가 읽고 훈련 및 예측에 사용하는 값 시퀀스의 길이입니다. 시퀀스 길이가 15라고 가정해 봅시다. 이는 RNN이 15개의 입력 값을 읽은 후 16번째 값을 찾아 훈련한다는 것을 의미합니다(항상 그렇지는 않습니다. 동적 RNN도 있기 때문입니다). 시퀀스 길이로 돌아가보면, 더 긴 시퀀스 길이는 RNN이 최종 출력에 여러 읽기를 통합할 수 있어 유익합니다. 그러나 더 긴 시퀀스 길이는 사라지는/폭주하는 그래디언트를 유발합니다. LSTM 네트워크는 이 문제를 어떻게 극복할까요?\n\n# LSTM 네트워크\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLSTM 네트워크는 어떤 정보를 “기억”하고 어떤 정보를 “잊을지” 결정하도록 훈련됩니다. RNN에서는 이전 유닛의 활성화에만 적용되는 메모리 구성 요소에 대한 가중치가 있습니다. 그러나 LSTM에서는 메모리 구성 요소의 개념이 장기 메모리 구성 요소(셀 상태)와 단기 메모리 구성 요소(은닉 상태)로 대체됩니다. 각 구성 요소는 서로 다른 게이트에 분산된 일련의 편향 및 가중치와 관련이 있습니다. 이는 각 입력이 네트워크에서 얼마나 많은 정보를 유지하고 얼마나 버릴지 결정하는 게이트(또는 단계)를 통과한다는 것을 의미합니다. 이 결정은 훈련 과정 중에 학습된 가중치와 편향 값에 기반합니다.\n\n그림 2는 단일 입력을 읽는 매우 간단한 LSTM 네트워크 스케치를 보여줍니다. 실제 LSTM 네트워크 유닛을 살펴보기 전에이 단순화된 다이어그램을 먼저 분석하겠습니다. RNN을 나타내는 이전 그림과 얼마나 다른지에 주목하세요. 기억할 점 중 첫 번째는 입력 값 외에도 LSTM 네트워크에는 단기 및 장기 메모리 구성 요소가 있다는 것입니다. 이러한 구성 요소는 공식적으로 셀 상태(C)와 숨겨진 상태(h)로 알려져 있습니다. 이 표기법은 나중에 사용되겠지만, 우리는 현재 이전 이름을 사용할 것입니다. 입력 및 메모리 구성 요소는 세 가지 서로 다른 게이트를 통과합니다: 삭제, 입력 및 출력.\n\n- 삭제 게이트는 장기 기억의 얼마나 보관해야 하는지를 결정합니다. 이 게이트는 현재 입력뿐만 아니라 단기 메모리 구성 요소를 고려하여 0과 1 사이의 값을 계산하여 장기 메모리 구성 요소를 곱합니다. 0의 삭제 게이트는 네트워크가 이전 정보를 보존하지 않음을 의미합니다. 그 답은 새로운 입력에만 기초합니다.\n- 입력 게이트는 새 정보가 장기 메모리 구성 요소에 보존되어야 하는 양을 결정합니다. 이 게이트의 출력은 다음 입력에 보존되는 장기 기억 구성 요소에 추가됩니다. 이 구성 요소가 삭제 및 입력 게이트에만 연결된다는 점에 유의하십시오. 이는 각 반복에서 장기 기억이 무엇을 버리고 어떤 정보를 추가해야 하는지에 따라 업데이트된다는 것을 의미합니다.\n- 출력 게이트는 입력 및 단기 메모리 구성 요소를 고려하고 이전 단기 메모리 구성 요소로 저장될 새로운 장기 메모리 구성 요소의 양을 계산합니다. 이는 LSTM 네트워크에서 나온 최종 값이 장기 및 단기 메모리 구성 요소뿐만 아니라 출력 게이트도 고려하여 계산된다는 것을 의미합니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_2.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 LSTM 네트워크 셀의 구조를 파악했으니 여기서 발생하는 계산에 더 가깝게 살펴보겠습니다. Figure 3은 LSTM 네트워크 셀을 더 자세히 보여줍니다. 입력, 단기 기억 구성 요소 및 장기 기억 구성 요소는 각각 x, h 및 C로 표시됩니다. 이들 각각의 글자는 분석 중인 시간 기간에 해당하는 아래 첨자를 가지고 있습니다. t-1의 아래 첨자는 값이 이전 반복에 속한다는 것을 의미합니다. 예를 들어, forget gate는 현재 입력(xt)과 이전 반복의 단기 기억 구성 요소(ht-1)를 고려합니다. 게이트들은 또한 일련의 가중치와 편향을 고려합니다. forget 및 output 게이트에는 각각 3개의 매개변수가 있습니다:\n\n- 편향(bxf, bxo)\n- 입력을 곱하는 가중치(wxf, wxo)\n- 단기 기억 구성 요소를 곱하는 가중치(whf, who)\n\n가중치와 편향은 활성화 함수로 들어가기 전에 입력 값에 곱해지고 더해집니다. 이 예에서 forget 및 output 게이트 모두 시그모이드 활성화 함수를 가지며 그 최종 활성화(af, ao)는 Figure 3 우측에 표시됩니다. 이러한 게이트와 달리, input 게이트는 두 개의 활성화 함수, 시그모이드 및 tanh가 있으며 해당 가중치와 편향을 가집니다. 이는 단일 LSTM 네트워크 단위에 대해 훈련해야 할 매개변수의 총 수가 12임을 의미합니다.\n\nLSTM 네트워크 셀에 대해 이해해야 할 마지막 중요한 측면은 새로운 C 및 h가 어떻게 계산되는지입니다. 이전 장기 기억 구성 요소는 먼저 forget 게이트의 출력과 곱해진 후 입력 게이트의 결과에 추가됩니다. 이는 forget 게이트가 C의 얼마나 다음 반복에 전달하는지를 결정하고, input 게이트가 C의 새 값에 얼마나 추가할지를 결정합니다. 단기 기억 구성 요소 계산을 위해 output 게이트의 결과는 새로운 C의 tanh와 곱해집니다. 이는 output 게이트가 장기 기억 구성 요소를 다음 반복에 얼마나 전달할지 결정합니다. C의 값이 1 이상일 수 있으므로, 값이 -1과 1 사이로 제한되도록 tanh 연산이 적용됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_3.png)\n\n입력이 모든 게이트를 통과하면, 새로운 C와 h가 다음 반복으로 전달되어 새 입력과 상호 작용합니다(그림 4). 이 과정은 시퀀스의 모든 값에 대해 반복되며, 해당 시퀀스의 최종 h에 도달할 때까지 반복됩니다.\n\n![이미지](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_4.png)\n\n# 앞으로 나아가기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLSTM 네트워크는 다른 인공 신경망(ANN)에서 사용되는 방법론과 유사한 방법으로 훈련될 수 있습니다. 문제에 따라 각 순전파 사이클 이후 업데이트되는 손실 함수를 정의합니다. 그런 다음 이 손실 함수를 사용하여 역전파 과정을 통해 가중치와 편향을 업데이트합니다. RNN 및 LSTM 네트워크의 경우, 역전파는 일반적으로 모든 반복 유닛을 통해 가중치와 편향을 누적하는 과정이기 때문에 시간을 거슬러 역전파(backpropagation through time, BPTT)라고합니다. 이는 LSTM 네트워크의 단일 유닛을 읽는 LSTM 네트워크 셀에서 순전파 과정이 어떻게 진행되는지 설명하는 간단한 구현과 순전파, 역전파 과정에 대해 상세히 설명하는 주피터 노트북입니다.\n\n그림 5는 LSTM 네트워크 셀에서 단일 유닌을 읽는 순전파 과정의 예시를 보여줍니다. h의 최종 값이 네트워크 내 모든 게이트를 통해 전달되는 정보를 함께 전달하는 반면, 최종 C는 출력 게이트와 상호작용하지 않는 것에 주목하세요. 이 게이트들 각각이 보존할 정보와 잊을 정보를 규제자로 작용합니다. 입력과 상호작용하는 최적의 방법을 학습하는 완전히 연결된 ANN에서 가중치와 편향이 학습되는 것과 유사하게, LSTM 네트워크에서 매개변수는 보존하거나 버릴 최적의 정보 양을 학습하기 위해 훈련됩니다.\n\n![image](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_5.png)\n\n# 더 많은 유닛?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금까지의 숫자와 예제는 단일 유단 LSTM 네트워크를 보여주었지만, 다른 유형의 네트워크와 마찬가지로 LSTM 네트워크는 여러 개의 유닌을 가질 수 있습니다. 단위 수에 따라 매개 변수 수는 어떻게 변하나요? 두 개의 유닌을 갖는 LSTM 네트워크의 경우, 학습할 매개 변수가 12개가 아닌 32개가 있습니다. 추가된 20개의 매개 변수는 어디에 있을까요? 그러면, 이제 조금 복잡해 질 것입니다. 이를 여러 부분으로 나눠서 살펴보겠습니다.\n\n첫 번째 유닛에는 12개의 매개 변수가 있습니다. 이전에 설명한 것과 같은 매개 변수들입니다: 입력을 곱하는 4개의 가중치, 숨겨진 상태(h)를 곱하는 4개의 가중치, 그리고 각 게이트에 대한 4개의 바이어스입니다. 두 번째 유닌도 12개의 관련된 매개 변수를 가지고 있습니다. 이는 지금까지 총 24개의 매개 변수를 가지게 되었다는 것을 의미합니다.\n\nLSTM 네트워크는 특정 유형의 RNN이므로 각 유닌 사이에 연결이 있을 것입니다. 이는 유닌 1에서 처리된 정보가 유닌 2로 전달된다는 것을 의미합니다. 각각의 연결은 고유의 가중치를 갖습니다. 두 개의 유닌을 갖는 LSTM 네트워크에서, 이전에 언급된 24개의 매개 변수 외에, 유닌 2의 각 게이트와 유닌 1의 각 게이트 사이의 연결 및 유닌 1의 숨겨진 상태와 유닌 2의 게이트 사이의 연결에 해당하는 8개의 매개 변수가 추가로 필요합니다. 그림 7은 유닌 2의 잊기 게이트에서 활성화를 계산하는 방법을 보여줍니다. 이 게이트가 이전 게이트와 이전 숨겨진 상태에 연결되어 있다는 점에 주목하세요. 그림에는 새로운 가중치가 5개만 표시되어 있지만, 실제로는 첫 번째 유닌의 숨겨진 상태가 두 번째 유닌의 각 게이트에 연결되기 때문에 8개의 가중치가 있습니다. n개의 유닌에 대한 매개 변수 수는 12n+4n(n-1) 또는 간소화된 표현으로 8(n+n²/2)입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_7.png\" /\u003e\n\n# Going backwards\n\n인공 신경망(ANNs)에서 흔히 볼 수 있는 바와 같이, 역전파 프로세스는 일반적으로 이해하고 구현하기 가장 어려운 부분입니다. 단일 유닛 LSTM 네트워크에서는 각 역전파가 4개의 편향과 8개의 가중치를 업데이트해야 하며, h(t-1) 및 C(t-1)도 업데이트해야 합니다. h(t-1)가 출력 게이트에 의존하고 현재 C는 다시 입력 및 망각 게이트에 따라 달라짐을 주목해야 합니다. 손실에 대한 편도함수를 계산할 때 이 사항을 고려하는 것이 중요합니다. 이전에 언급한 바와 같이, 이 Jupyter 노트북에는 역전파 프로세스를 포함한 간단한 LSTM 네트워크를 구축하는 데 필요한 모든 방정식이 포함되어 있습니다. 그림 8은 각 편도함수를 계산하는 데 도움이 되는 매개변수 간 의존성을 보여줍니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_8.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 응용 프로그램\n\n다음 섹션에는 LSTM 네트워크의 세 가지 응용 프로그램 예시가 포함되어 있습니다. 예시는 간단한 순서로 제공됩니다. 각 예시에 대한 파이썬 코드를 이 Jupyter 노트북에서 찾을 수 있습니다.\n\n## 연속 함수 모델링을 위한 바닐라 LSTM 네트워크\n\n이것은 처음부터 LSTM 네트워크를 구현하는 매우 간단한 예시입니다. 여기서 배울 중요한 교훈은 LSTM 네트워크에 데이터를 공급하기 전에 데이터를 올바르게 준비하는 중요성입니다. LSTM 네트워크로 모델링하고 싶은 연속 함수가 있다면, 먼저 입력-타겟 데이터의 쌍을 생성해야 합니다 (Figure 1 참조). 이 데이터는 시퀀스 길이에 따라 달라집니다. 예를 들어, 시퀀스 길이가 15인 경우, 각 입력 항목은 15개의 값이 포함되며 16번째 값은 해당 입력의 타겟이 됩니다. 네트워크에 입력하기 전에 데이터를 정규화하는 것도 중요합니다. 이 예시에서는 sin(x) 함수와 함께 웰에서의 석유 생산 행동을 모델링하기 위해 간단한 LSTM 네트워크가 사용됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 워드 예측기를 구축하기 위한 바닐라 LSTM 네트워크\n\n이 예시에서는 이전의 바닐라 LSTM 네트워크가 워드 예측 문제에 적용되었습니다. 짧은 텍스트로 훈련된 후, 모델은 다음에 나올 단어를 예측합니다. 실제로 워드 처리 및 워드 예측 문제는 이 예시처럼 다가가지 않습니다. 그러나 LSTM 네트워크의 가능한 응용에 대한 간단하고 명확한 설명입니다.\n\n## Keras의 LSTM 네트워크를 사용하여 워드 예측기 구축\n\n이 예시는 Keras의 LSTM 네트워크를 사용하여 긴 텍스트로 훈련된 후 다음 단어가 무엇인지 예측하는 더 현실적인 예제입니다. 이 예시에서는 NLP 문제에서 일반적인 추가인 임베딩 레이어를 사용합니다. 임베딩 레이어는 단어의 정수로 인코딩된 표현(인덱스)을 밀집된 벡터로 변환하여 단어 사이의 관계를 더 잘 모델링하는 데 도움이 되는 고정 크기의 벡터로 변환합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n인공 신경망 및 특히 순환 신경망을 사용하여 자연어 처리 문제에 접근하거나 연속 데이터를 모델링하는 것은 최근에 개발된 것이 아닙니다. 이 응용 프로그램은 오랫동안 존재해 왔으며 새로운 기능으로 계속 발전하고 있습니다. LSTM 네트워크가 어떻게 작동하는지 이해하고 그것을 특별한 종류의 RNN으로 만드는 요소를 파악하는 것은 결과와 예상대로 작동하지 않을 수 있는 이유에 대한 통찰력을 제공할 수 있습니다. 본문은 LSTM 네트워크에 대한 포괄적인 설명을 포함하고, 그 응용 예시 세 가지를 제시합니다. 대부분의 현재 NLP 도구 및 솔루션은 다른 네트워크 구조에 의존하지만 LSTM 네트워크 내부 작업에 대한 탄탄한 개념은 머신러닝 분야에서 항상 유익할 것입니다. 이것을 장기 기억 셀에 저장하는 것을 기억하세요! 😉\n\n# 참고문헌\n\n- Ng, Andrew. Machine Learning Specialization.\n- Keras 'Embedding' 레이어는 어떻게 작동합니까? CrossValidated 게시물. 2017\n- Cowan, Nelson (2009). 장기, 단기 및 직업기억 사이의 차이점은 무엇인가? — PMC. Prog Brain Res. 2008;169:323–38. doi: 10.1016/S0079–6123(07)00020–9. PMID: 18394484; PMCID: PMC2657600.\n- Erz, Hendrik (2023). ChatGPT를 생산적으로 사용하는 방법 | Hendrik Erz. hendrik-erz.de, 2023년 2월 14일\n- Adams, Tim (2013). Henry Molaison: 우리가 결코 잊지 않을 기억상실자 | 기억 | The Guardian\n- Dittrich, Luke (2016). 기억할 수 없던 두뇌 — 뉴욕 타임즈\n","ogImage":{"url":"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png"},"coverImage":"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png","tag":["Tech"],"readingTime":13},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e한 남자와 한 여자가 임상 연구 센터의 조용한 방 안에서 대화를 나누고 있습니다. 여자는 질문을 하고는 남자가 대답할 때까지 기다리면서 몇 가지 노트를 적습니다. 그냥 보통 대화처럼 보일 수도 있지만, 실제로는 전혀 보통이 아닙니다. 여자의 노트북 안에는 매 페이지마다 써 있는 날짜와 상관없이 남자의 대답이 항상 동일합니다. 대화가 80년대에 발생했더라도, 대답은 10년 이상 전에 일어난 사건을 참조하고 있습니다. Jenni Ogden은 나중에 네오심리학에 영향을 미치면서 그의 진짜 이름인 Henry Molaison으로 더 잘 알려지게 된 환자 H.M.과 대화를 나눈 최초의 연구자 중 한 명이었습니다. 며칠 후, 연구진은 Henry가 27세 때 받았던 뇌 절제술로 인해 새로운 기억을 생성하는 능력을 상실했다고 결론 내렸습니다. Henry의 사례는 뇌 기능과 기억 사이의 연결을 이해하고 단기와 장기 기억이라는 개념을 만들어내는 데 도움이 되었습니다. 이 개념은 기계 학습 분야에서 혁신적인 연구를 위한 토대를 마련했으며, 과학자들과 개발자들이 뇌의 신비한 내부 구조에서 더 나은 예측 모델을 만드는 데 노력하고 있습니다.\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e인공 신경망(ANN)은 우리 뇌에서 작동하는 실제 신경망에서 영감을 받았습니다. 실제로 ANNs는 실제 신경세포가 어떻게 상호 연결되고 위에서 설명한 상황을 설명하는 추상화일 뿐입니다. 개미군 최적화, 차분 진화, 입자 미래 등의 프로세스와 유사하게, ANNs는 실제 과정의 본질을 포착하여 현재 대부분의 AI 솔루션 뒤에 있는 알고리즘을 설계하는 데 사용됩니다. ANNs가 정말로 학습하는지, 그들이 하는 일을 지능이라고 해야 하는지에 대한 논의는 넓고 계속됩니다. 그러나 그들의 다용도성과 성능은 부정할 수 없습니다. 새로운 ANN 구성은 매일 개발되고 있으며 다양한 문제에 성공적으로 적용되고 있습니다. 이러한 변형의 대부분은 여전히 실제 신경망의 행동에서 영감을 받고 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eRNN(RNNs)은 일련의 메모리 구성요소를 통합하여 처리하는데, 수년 전에 자연어 처리(NLP)에서 중요한 접근 방식을 나타냅니다. RNNs는 Long-Short Term Memory (LSTM) Networks로 나아가는 길을 열며, NLP 응용 프로그램에서 신경망의 성능을 높였습니다. 이후 LSTM 네트워크는 트랜스포머 모델과 GPT(Generative Pre-trained Transformer)에 의해 대체되었는데, 이것이 ChatGPT의 기초가 되었습니다. 이 기사에서는 LSTM 네트워크가 무엇이며, 그들을 특별하게 만드는 이유에 대해 살펴봅니다.\u003c/p\u003e\n\u003ch1\u003eRNN의 의미\u003c/h1\u003e\n\u003cp\u003eLSTM 네트워크가 어떻게 작동하는지 이해하기 위해서는 그 목적에 대해 생각해 보는 것이 중요합니다. RNN과 LSTM 네트워크는 비슷한 목표를 따릅니다. 이들은 순차적으로 저장된 데이터를 모델링하고 예측하는 데 사용됩니다. 이는 이 유형의 네트워크가 데이터 시퀀스를 읽고 다음 값이 무엇인지 예측하려고 한다는 것을 의미합니다. 특정 도시의 지난 30일간의 평균 온도를 기록한 로그가 있다고 가정해 봅시다. 그리고 31일차의 온도를 추정하고 싶다면 어떻게 할까요? 한 가지 방법은 온도를 다른 변수에 상관시켜서, 31일에 이러한 변수의 값에 따라 새로운 온도를 추정하는 것입니다. RNN은 몇 일, 예를 들어 30일 이내의 일부 날짜를 고려하여 이전 온도 값을 기반으로 31일의 온도를 예측합니다. 한 마디로, RNN은 시퀀스를 기억하고 다음 값 또는 값 그룹을 제시하려고 노력합니다. 이전에 작성한 기사 중에 RNN이 메모리 전문가와 어떻게 비교되면서 RNN이 어떻게 단계적으로 작동하는지 설명했습니다.\u003c/p\u003e\n\u003cp\u003e이전 날짜의 온도를 기반으로 새로운 온도를 예측하는 아이디어는 다른 응용 분야로 확장할 수 있습니다. 소개에서 언급된 것처럼, RNN은 NLP 중 첫 접근 방식 중 하나였습니다. 아이디어는 RNN을 텍스트로 학습한 다음, RNN을 사용하여 입력 후 다음에 나오는 단어 또는 단어 그룹을 예측하는 것입니다. 이러한 아이디어는 자동 번역뿐만 아니라 음성 및 필기 인식과 같은 과제에도 적용할 수 있습니다. RNN이 이러한 과제들을 다룰 때 직면하는 문제 중 하나는 죽거나 폭발하는 기울기(vanishing/exploding gradients)로 인한 어려움입니다. 이 문제의 해법은 LSTM 네트워크의 적용입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eFigure 1은 완전히 연결된 인공 신경망(ANNs)과 순환 신경망(RNNs) 간의 주요 구조적 차이를 보여줍니다. 이 간단한 예에서는 (X1,Y1) 및 (X2,Y2)의 값이 Y3의 새로운 값 계산에 사용됩니다. 실제로는 ANN과 RNN이 많은 입력-출력 쌍으로 훈련됩니다. 훈련 과정이 완료되면 네트워크는 새로운 값을 예측하는 데 사용됩니다. 이 프로세스에 익숙하지 않다면, 이 기사의 끝에 유용한 참고 자료를 추가했습니다. 여기에 이 프로세스를 설명한 나의 시도도 곁들였습니다. ANNs와 RNNs 사이의 훈련 및 예측의 일반적인 아이디어는 비슷하지만, 구조적으로 큰 차이가 있습니다. RNN에서는 Y1과 Y2의 값이 X1과 X2 대신 네트워크를 훈련하는 데 사용됨을 주목하십시오. 또한 첫 번째 단위와 두 번째 단위를 연결하는 가중치가 있으며, 이는 이전 단위에서 온 활성화를 나타냅니다. 이 가중치는 RNN의 \"기억\" 구성 요소를 나타냅니다. 더 큰 가중치는 RNN이 이전 값에 더 많은 중요성을 부여함을 의미하고, 더 작은 가중치는 RNN이 과거 값을 잘 기억하지 못한다는 것을 의미합니다. 이것은 신경망 구조의 가중치이므로, 그 값은 프로세스 중에 학습되며, RNN은 재현하려는 순서가 더 많은지 덜 많은지를 결정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_1.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eRNN의 훈련 및 적용 중 중요한 측면은 네트워크가 읽고 훈련 및 예측에 사용하는 값 시퀀스의 길이입니다. 시퀀스 길이가 15라고 가정해 봅시다. 이는 RNN이 15개의 입력 값을 읽은 후 16번째 값을 찾아 훈련한다는 것을 의미합니다(항상 그렇지는 않습니다. 동적 RNN도 있기 때문입니다). 시퀀스 길이로 돌아가보면, 더 긴 시퀀스 길이는 RNN이 최종 출력에 여러 읽기를 통합할 수 있어 유익합니다. 그러나 더 긴 시퀀스 길이는 사라지는/폭주하는 그래디언트를 유발합니다. LSTM 네트워크는 이 문제를 어떻게 극복할까요?\u003c/p\u003e\n\u003ch1\u003eLSTM 네트워크\u003c/h1\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eLSTM 네트워크는 어떤 정보를 “기억”하고 어떤 정보를 “잊을지” 결정하도록 훈련됩니다. RNN에서는 이전 유닛의 활성화에만 적용되는 메모리 구성 요소에 대한 가중치가 있습니다. 그러나 LSTM에서는 메모리 구성 요소의 개념이 장기 메모리 구성 요소(셀 상태)와 단기 메모리 구성 요소(은닉 상태)로 대체됩니다. 각 구성 요소는 서로 다른 게이트에 분산된 일련의 편향 및 가중치와 관련이 있습니다. 이는 각 입력이 네트워크에서 얼마나 많은 정보를 유지하고 얼마나 버릴지 결정하는 게이트(또는 단계)를 통과한다는 것을 의미합니다. 이 결정은 훈련 과정 중에 학습된 가중치와 편향 값에 기반합니다.\u003c/p\u003e\n\u003cp\u003e그림 2는 단일 입력을 읽는 매우 간단한 LSTM 네트워크 스케치를 보여줍니다. 실제 LSTM 네트워크 유닛을 살펴보기 전에이 단순화된 다이어그램을 먼저 분석하겠습니다. RNN을 나타내는 이전 그림과 얼마나 다른지에 주목하세요. 기억할 점 중 첫 번째는 입력 값 외에도 LSTM 네트워크에는 단기 및 장기 메모리 구성 요소가 있다는 것입니다. 이러한 구성 요소는 공식적으로 셀 상태(C)와 숨겨진 상태(h)로 알려져 있습니다. 이 표기법은 나중에 사용되겠지만, 우리는 현재 이전 이름을 사용할 것입니다. 입력 및 메모리 구성 요소는 세 가지 서로 다른 게이트를 통과합니다: 삭제, 입력 및 출력.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e삭제 게이트는 장기 기억의 얼마나 보관해야 하는지를 결정합니다. 이 게이트는 현재 입력뿐만 아니라 단기 메모리 구성 요소를 고려하여 0과 1 사이의 값을 계산하여 장기 메모리 구성 요소를 곱합니다. 0의 삭제 게이트는 네트워크가 이전 정보를 보존하지 않음을 의미합니다. 그 답은 새로운 입력에만 기초합니다.\u003c/li\u003e\n\u003cli\u003e입력 게이트는 새 정보가 장기 메모리 구성 요소에 보존되어야 하는 양을 결정합니다. 이 게이트의 출력은 다음 입력에 보존되는 장기 기억 구성 요소에 추가됩니다. 이 구성 요소가 삭제 및 입력 게이트에만 연결된다는 점에 유의하십시오. 이는 각 반복에서 장기 기억이 무엇을 버리고 어떤 정보를 추가해야 하는지에 따라 업데이트된다는 것을 의미합니다.\u003c/li\u003e\n\u003cli\u003e출력 게이트는 입력 및 단기 메모리 구성 요소를 고려하고 이전 단기 메모리 구성 요소로 저장될 새로운 장기 메모리 구성 요소의 양을 계산합니다. 이는 LSTM 네트워크에서 나온 최종 값이 장기 및 단기 메모리 구성 요소뿐만 아니라 출력 게이트도 고려하여 계산된다는 것을 의미합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_2.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이제 LSTM 네트워크 셀의 구조를 파악했으니 여기서 발생하는 계산에 더 가깝게 살펴보겠습니다. Figure 3은 LSTM 네트워크 셀을 더 자세히 보여줍니다. 입력, 단기 기억 구성 요소 및 장기 기억 구성 요소는 각각 x, h 및 C로 표시됩니다. 이들 각각의 글자는 분석 중인 시간 기간에 해당하는 아래 첨자를 가지고 있습니다. t-1의 아래 첨자는 값이 이전 반복에 속한다는 것을 의미합니다. 예를 들어, forget gate는 현재 입력(xt)과 이전 반복의 단기 기억 구성 요소(ht-1)를 고려합니다. 게이트들은 또한 일련의 가중치와 편향을 고려합니다. forget 및 output 게이트에는 각각 3개의 매개변수가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e편향(bxf, bxo)\u003c/li\u003e\n\u003cli\u003e입력을 곱하는 가중치(wxf, wxo)\u003c/li\u003e\n\u003cli\u003e단기 기억 구성 요소를 곱하는 가중치(whf, who)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e가중치와 편향은 활성화 함수로 들어가기 전에 입력 값에 곱해지고 더해집니다. 이 예에서 forget 및 output 게이트 모두 시그모이드 활성화 함수를 가지며 그 최종 활성화(af, ao)는 Figure 3 우측에 표시됩니다. 이러한 게이트와 달리, input 게이트는 두 개의 활성화 함수, 시그모이드 및 tanh가 있으며 해당 가중치와 편향을 가집니다. 이는 단일 LSTM 네트워크 단위에 대해 훈련해야 할 매개변수의 총 수가 12임을 의미합니다.\u003c/p\u003e\n\u003cp\u003eLSTM 네트워크 셀에 대해 이해해야 할 마지막 중요한 측면은 새로운 C 및 h가 어떻게 계산되는지입니다. 이전 장기 기억 구성 요소는 먼저 forget 게이트의 출력과 곱해진 후 입력 게이트의 결과에 추가됩니다. 이는 forget 게이트가 C의 얼마나 다음 반복에 전달하는지를 결정하고, input 게이트가 C의 새 값에 얼마나 추가할지를 결정합니다. 단기 기억 구성 요소 계산을 위해 output 게이트의 결과는 새로운 C의 tanh와 곱해집니다. 이는 output 게이트가 장기 기억 구성 요소를 다음 반복에 얼마나 전달할지 결정합니다. C의 값이 1 이상일 수 있으므로, 값이 -1과 1 사이로 제한되도록 tanh 연산이 적용됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e입력이 모든 게이트를 통과하면, 새로운 C와 h가 다음 반복으로 전달되어 새 입력과 상호 작용합니다(그림 4). 이 과정은 시퀀스의 모든 값에 대해 반복되며, 해당 시퀀스의 최종 h에 도달할 때까지 반복됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e앞으로 나아가기\u003c/h1\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eLSTM 네트워크는 다른 인공 신경망(ANN)에서 사용되는 방법론과 유사한 방법으로 훈련될 수 있습니다. 문제에 따라 각 순전파 사이클 이후 업데이트되는 손실 함수를 정의합니다. 그런 다음 이 손실 함수를 사용하여 역전파 과정을 통해 가중치와 편향을 업데이트합니다. RNN 및 LSTM 네트워크의 경우, 역전파는 일반적으로 모든 반복 유닛을 통해 가중치와 편향을 누적하는 과정이기 때문에 시간을 거슬러 역전파(backpropagation through time, BPTT)라고합니다. 이는 LSTM 네트워크의 단일 유닛을 읽는 LSTM 네트워크 셀에서 순전파 과정이 어떻게 진행되는지 설명하는 간단한 구현과 순전파, 역전파 과정에 대해 상세히 설명하는 주피터 노트북입니다.\u003c/p\u003e\n\u003cp\u003e그림 5는 LSTM 네트워크 셀에서 단일 유닌을 읽는 순전파 과정의 예시를 보여줍니다. h의 최종 값이 네트워크 내 모든 게이트를 통해 전달되는 정보를 함께 전달하는 반면, 최종 C는 출력 게이트와 상호작용하지 않는 것에 주목하세요. 이 게이트들 각각이 보존할 정보와 잊을 정보를 규제자로 작용합니다. 입력과 상호작용하는 최적의 방법을 학습하는 완전히 연결된 ANN에서 가중치와 편향이 학습되는 것과 유사하게, LSTM 네트워크에서 매개변수는 보존하거나 버릴 최적의 정보 양을 학습하기 위해 훈련됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e더 많은 유닛?\u003c/h1\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e지금까지의 숫자와 예제는 단일 유단 LSTM 네트워크를 보여주었지만, 다른 유형의 네트워크와 마찬가지로 LSTM 네트워크는 여러 개의 유닌을 가질 수 있습니다. 단위 수에 따라 매개 변수 수는 어떻게 변하나요? 두 개의 유닌을 갖는 LSTM 네트워크의 경우, 학습할 매개 변수가 12개가 아닌 32개가 있습니다. 추가된 20개의 매개 변수는 어디에 있을까요? 그러면, 이제 조금 복잡해 질 것입니다. 이를 여러 부분으로 나눠서 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003e첫 번째 유닛에는 12개의 매개 변수가 있습니다. 이전에 설명한 것과 같은 매개 변수들입니다: 입력을 곱하는 4개의 가중치, 숨겨진 상태(h)를 곱하는 4개의 가중치, 그리고 각 게이트에 대한 4개의 바이어스입니다. 두 번째 유닌도 12개의 관련된 매개 변수를 가지고 있습니다. 이는 지금까지 총 24개의 매개 변수를 가지게 되었다는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003eLSTM 네트워크는 특정 유형의 RNN이므로 각 유닌 사이에 연결이 있을 것입니다. 이는 유닌 1에서 처리된 정보가 유닌 2로 전달된다는 것을 의미합니다. 각각의 연결은 고유의 가중치를 갖습니다. 두 개의 유닌을 갖는 LSTM 네트워크에서, 이전에 언급된 24개의 매개 변수 외에, 유닌 2의 각 게이트와 유닌 1의 각 게이트 사이의 연결 및 유닌 1의 숨겨진 상태와 유닌 2의 게이트 사이의 연결에 해당하는 8개의 매개 변수가 추가로 필요합니다. 그림 7은 유닌 2의 잊기 게이트에서 활성화를 계산하는 방법을 보여줍니다. 이 게이트가 이전 게이트와 이전 숨겨진 상태에 연결되어 있다는 점에 주목하세요. 그림에는 새로운 가중치가 5개만 표시되어 있지만, 실제로는 첫 번째 유닌의 숨겨진 상태가 두 번째 유닌의 각 게이트에 연결되기 때문에 8개의 가중치가 있습니다. n개의 유닌에 대한 매개 변수 수는 12n+4n(n-1) 또는 간소화된 표현으로 8(n+n²/2)입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_7.png\"\u003e\n\u003ch1\u003eGoing backwards\u003c/h1\u003e\n\u003cp\u003e인공 신경망(ANNs)에서 흔히 볼 수 있는 바와 같이, 역전파 프로세스는 일반적으로 이해하고 구현하기 가장 어려운 부분입니다. 단일 유닛 LSTM 네트워크에서는 각 역전파가 4개의 편향과 8개의 가중치를 업데이트해야 하며, h(t-1) 및 C(t-1)도 업데이트해야 합니다. h(t-1)가 출력 게이트에 의존하고 현재 C는 다시 입력 및 망각 게이트에 따라 달라짐을 주목해야 합니다. 손실에 대한 편도함수를 계산할 때 이 사항을 고려하는 것이 중요합니다. 이전에 언급한 바와 같이, 이 Jupyter 노트북에는 역전파 프로세스를 포함한 간단한 LSTM 네트워크를 구축하는 데 필요한 모든 방정식이 포함되어 있습니다. 그림 8은 각 편도함수를 계산하는 데 도움이 되는 매개변수 간 의존성을 보여줍니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_8.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e응용 프로그램\u003c/h1\u003e\n\u003cp\u003e다음 섹션에는 LSTM 네트워크의 세 가지 응용 프로그램 예시가 포함되어 있습니다. 예시는 간단한 순서로 제공됩니다. 각 예시에 대한 파이썬 코드를 이 Jupyter 노트북에서 찾을 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e연속 함수 모델링을 위한 바닐라 LSTM 네트워크\u003c/h2\u003e\n\u003cp\u003e이것은 처음부터 LSTM 네트워크를 구현하는 매우 간단한 예시입니다. 여기서 배울 중요한 교훈은 LSTM 네트워크에 데이터를 공급하기 전에 데이터를 올바르게 준비하는 중요성입니다. LSTM 네트워크로 모델링하고 싶은 연속 함수가 있다면, 먼저 입력-타겟 데이터의 쌍을 생성해야 합니다 (Figure 1 참조). 이 데이터는 시퀀스 길이에 따라 달라집니다. 예를 들어, 시퀀스 길이가 15인 경우, 각 입력 항목은 15개의 값이 포함되며 16번째 값은 해당 입력의 타겟이 됩니다. 네트워크에 입력하기 전에 데이터를 정규화하는 것도 중요합니다. 이 예시에서는 sin(x) 함수와 함께 웰에서의 석유 생산 행동을 모델링하기 위해 간단한 LSTM 네트워크가 사용됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e워드 예측기를 구축하기 위한 바닐라 LSTM 네트워크\u003c/h2\u003e\n\u003cp\u003e이 예시에서는 이전의 바닐라 LSTM 네트워크가 워드 예측 문제에 적용되었습니다. 짧은 텍스트로 훈련된 후, 모델은 다음에 나올 단어를 예측합니다. 실제로 워드 처리 및 워드 예측 문제는 이 예시처럼 다가가지 않습니다. 그러나 LSTM 네트워크의 가능한 응용에 대한 간단하고 명확한 설명입니다.\u003c/p\u003e\n\u003ch2\u003eKeras의 LSTM 네트워크를 사용하여 워드 예측기 구축\u003c/h2\u003e\n\u003cp\u003e이 예시는 Keras의 LSTM 네트워크를 사용하여 긴 텍스트로 훈련된 후 다음 단어가 무엇인지 예측하는 더 현실적인 예제입니다. 이 예시에서는 NLP 문제에서 일반적인 추가인 임베딩 레이어를 사용합니다. 임베딩 레이어는 단어의 정수로 인코딩된 표현(인덱스)을 밀집된 벡터로 변환하여 단어 사이의 관계를 더 잘 모델링하는 데 도움이 되는 고정 크기의 벡터로 변환합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e인공 신경망 및 특히 순환 신경망을 사용하여 자연어 처리 문제에 접근하거나 연속 데이터를 모델링하는 것은 최근에 개발된 것이 아닙니다. 이 응용 프로그램은 오랫동안 존재해 왔으며 새로운 기능으로 계속 발전하고 있습니다. LSTM 네트워크가 어떻게 작동하는지 이해하고 그것을 특별한 종류의 RNN으로 만드는 요소를 파악하는 것은 결과와 예상대로 작동하지 않을 수 있는 이유에 대한 통찰력을 제공할 수 있습니다. 본문은 LSTM 네트워크에 대한 포괄적인 설명을 포함하고, 그 응용 예시 세 가지를 제시합니다. 대부분의 현재 NLP 도구 및 솔루션은 다른 네트워크 구조에 의존하지만 LSTM 네트워크 내부 작업에 대한 탄탄한 개념은 머신러닝 분야에서 항상 유익할 것입니다. 이것을 장기 기억 셀에 저장하는 것을 기억하세요! 😉\u003c/p\u003e\n\u003ch1\u003e참고문헌\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eNg, Andrew. Machine Learning Specialization.\u003c/li\u003e\n\u003cli\u003eKeras 'Embedding' 레이어는 어떻게 작동합니까? CrossValidated 게시물. 2017\u003c/li\u003e\n\u003cli\u003eCowan, Nelson (2009). 장기, 단기 및 직업기억 사이의 차이점은 무엇인가? — PMC. Prog Brain Res. 2008;169:323–38. doi: 10.1016/S0079–6123(07)00020–9. PMID: 18394484; PMCID: PMC2657600.\u003c/li\u003e\n\u003cli\u003eErz, Hendrik (2023). ChatGPT를 생산적으로 사용하는 방법 | Hendrik Erz. hendrik-erz.de, 2023년 2월 14일\u003c/li\u003e\n\u003cli\u003eAdams, Tim (2013). Henry Molaison: 우리가 결코 잊지 않을 기억상실자 | 기억 | The Guardian\u003c/li\u003e\n\u003cli\u003eDittrich, Luke (2016). 기억할 수 없던 두뇌 — 뉴욕 타임즈\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-23-ToKnowIsAlsotoRemember"},"buildId":"JlBEgQDLGRx6DYlBnT8eD","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>