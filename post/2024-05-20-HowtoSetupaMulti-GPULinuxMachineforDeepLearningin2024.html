<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024" data-gatsby-head="true"/><meta name="twitter:title" content="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-20 17:53" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/o1YmnmSuZvAX2O4TI9r41/_buildManifest.js" defer=""></script><script src="/_next/static/o1YmnmSuZvAX2O4TI9r41/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 20, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>다중 GPU로 딥 러닝</h2>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png" alt="image"></p>
<p>딥 러닝 모델(특히 LLMs)이 점점 커지면, GPU 메모리(VRAM)가 더 많이 필요해집니다. 로컬에서 이들을 개발하고 사용하기 위해 다중 GPU 머신을 구축하거나 획득하는 것은 첫 번째 도전일 뿐입니다. 대부분의 라이브러리와 애플리케이션은 기본적으로 단일 GPU만 사용합니다. 따라서 머신에는 다중 GPU 설정을 활용할 수 있는 적절한 드라이버와 라이브러리가 필요합니다.</p>
<p>본 기사는 중요한 라이브러리를 갖춘 Nvidia 다중 GPU(Linux) 머신을 설정하는 방법에 대한 가이드를 제공합니다. 이를 통해 실험에 소요되는 시간을 아낄 수 있고 개발을 시작하는 데 도움이 될 것입니다.</p>
<p>마지막으로, 딥러닝을 위한 멀티 GPU 설정을 활용할 수 있는 인기 있는 오픈 소스 라이브러리로의 링크가 제공됩니다.</p>
<h2>대상</h2>
<p>딥러닝을 시작하기 위해 CUDA Toolkit, PyTorch 및 Miniconda를 설치하여 exllamaV2 및 torchtune과 같은 프레임워크를 사용할 것입니다.</p>
<p>©️ 이 이야기에서 언급된 모든 라이브러리 및 정보는 오픈 소스이거나 공개적으로 사용 가능합니다.</p>
<h2>시작하기</h2>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_1.png" alt="그림"></p>
<p>터미널에서 nvidia-smi 명령어를 사용하여 기계에 설치된 GPU 수를 확인하십시오. 설치된 GPU의 목록이 출력되어야 합니다. 만약 명령어가 작동하지 않거나 불일치가 있는 경우 먼저 해당 Linux 버전용 Nvidia 드라이버를 설치하십시오. nvidia-smi 명령어가 기계에 설치된 모든 GPU 목록을 출력하도록 확인하십시오.</p>
<p>이 페이지를 따라 설치하지 않은 경우 Nvidia 드라이버를 설치하십시오:</p>
<p>우분투 22.04에 NVIDIA 드라이버를 설치하는 방법 - Linux 자습서 - Linux 구성 학습 - (출처: linuxconfig.org)</p>
<h2>단계 1 CUDA-Toolkit 설치하기</h2>
<p>💡 usr/local/cuda-xx 경로에 기존 CUDA 폴더가 있는지 확인하세요. 해당 폴더가 있다면 CUDA의 버전이 이미 설치된 것입니다. 원하는 CUDA 툴킷이 이미 설치되어 있는 경우 (터미널에서 nvcc 명령으로 확인하실 수 있습니다.) 다음 단계인 단계 2로 이동해주세요.</p>
<p>PyTorch 라이브러리용 필요한 CUDA 버전을 확인하세요: 로컬에서 시작하기 | PyTorch (저희는 CUDA 12.1을 설치하고 있습니다)</p>
<p>CUDA Toolkit 12.1 Downloads | NVIDIA Developer 에 방문하여 CUDA 12.1을 설치하기 위한 Linux 명령어를 얻을 수 있어요 (OS 버전과 해당하는 “deb (local)” 설치 프로그램 유형을 선택).</p>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_2.png" alt="이미지"></p>
<p>선택한 옵션에 따라 기본 설치 프로그램의 터미널 명령어가 나타날 거에요. CUDA 툴킷을 설치하려면 해당 명령어를 Linux 터미널에서 복사하여 붙여넣기하여 실행하세요. 예를 들어, x86_64 Ubuntu 22 용으로 다음 명령어를 실행하려면 다운로드 폴더에서 터미널을 열고 다음 명령어를 실행하세요:</p>
<pre><code class="hljs language-js">wget <span class="hljs-attr">https</span>:<span class="hljs-comment">//developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</span>
sudo mv cuda-ubuntu2204.<span class="hljs-property">pin</span> /etc/apt/preferences.<span class="hljs-property">d</span>/cuda-repository-pin-<span class="hljs-number">600</span>
wget <span class="hljs-attr">https</span>:<span class="hljs-comment">//developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb</span>
sudo dpkg -i cuda-repo-ubuntu2204-<span class="hljs-number">12</span>-<span class="hljs-number">1</span>-local_12<span class="hljs-number">.1</span><span class="hljs-number">.0</span>-<span class="hljs-number">530.30</span><span class="hljs-number">.02</span>-1_amd64.<span class="hljs-property">deb</span>
sudo cp /<span class="hljs-keyword">var</span>/cuda-repo-ubuntu2204-<span class="hljs-number">12</span>-<span class="hljs-number">1</span>-local/cuda-*-keyring.<span class="hljs-property">gpg</span> /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda
</code></pre>
<p>⚠️ CUDA 툴킷을 설치하는 동안, 설치 프로그램이 커널 업데이트를 요구할 수 있습니다. 터미널에서 커널 업데이트를 요청하는 팝업이 나타나면 esc 버튼을 눌러 취소하세요. 이 단계에서는 커널을 업데이트하지 마세요! — 이렇게 하면 Nvidia 드라이버가 손상될 수 있습니다 ☠️.</p>
<p>설치 후 리눅스 머신을 다시 시작하세요. nvcc 명령이 여전히 작동하지 않을 것입니다. CUDA 설치를 PATH에 추가해야 합니다. 나노 편집기를 사용하여 .bashrc 파일을 엽니다.</p>
<pre><code class="hljs language-js">nano /home/$USER/.<span class="hljs-property">bashrc</span>
</code></pre>
<p>.bashrc 파일 맨 아래로 스크롤하여 다음 두 줄을 추가하세요:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-variable constant_">PATH</span>=<span class="hljs-string">"/usr/local/cuda-12.1/bin:$PATH"</span>
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">LD_LIBRARY_PATH</span>=<span class="hljs-string">"/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH"</span>
</code></pre>
<p>💡 참고로 설치된 CUDA 버전에 맞게 cuda-12.1을 필요에 따라 cuda-xx로 변경할 수 있습니다. 여기서 'xx'는 CUDA 버전을 나타냅니다.</p>
<p>변경 사항을 저장하고 nano 편집기를 닫으려면:</p>
<pre><code class="hljs language-js">변경 사항 저장 - 키보드에서 다음을 누르세요:

ctrl + o             --> 저장
엔터 또는 리턴 키     --> 변경 사항 수락
ctrl + x             --> 편집기 닫기
</code></pre>
<p>터미널을 닫고 다시 열어주세요. 그러면 nvcc --version 명령을 실행하면 설치된 CUDA 버전이 터미널에 표시될 겁니다.</p>
<h2>단계 2 - Miniconda 설치</h2>
<p>PyTorch를 설치하기 전에 Miniconda를 설치하고, 그 안에 PyTorch를 설치하는 것이 좋습니다. 또한 각 프로젝트마다 새로운 Conda 환경을 만드는 것이 편리합니다.</p>
<p>Downloads 폴더에서 터미널을 열고 다음 명령을 실행하세요.</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">mkdir</span> -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
<span class="hljs-built_in">rm</span> -rf ~/miniconda3/miniconda.sh
</code></pre>
<h1>initiate conda</h1>
<pre><code class="hljs language-bash">~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
</code></pre>
<p>터미널을 닫고 다시 열어주세요. 이제 conda 명령어를 사용할 수 있어요.</p>
<h2>단계-3 PyTorch 설치하기</h2>
<p>(선택 사항) — 프로젝트용 새로운 conda 환경을 생성하세요. <code>environment-name</code>을 원하는 이름으로 바꿀 수 있어요. 일반적으로 제 프로젝트 이름으로 지정해요. 💡 프로젝트 작업 전후에 conda activate <code>environment-name</code> 및 conda deactivate <code>environment-name</code> 명령어를 사용할 수 있어요.</p>
<pre><code class="hljs language-js">conda create -n &#x3C;environment-name> python=<span class="hljs-number">3.11</span>

# 환경 활성화
conda activate &#x3C;environment-name>
</code></pre>
<p>CUDA 버전에 맞게 PyTorch 라이브러리를 설치하세요. 아래 명령어는 우리가 cuda-12.1에 설치한 것입니다:</p>
<pre><code class="hljs language-js">pip3 install torch torchvision torchaudio
</code></pre>
<p>위 명령어는 PyTorch 설치 가이드 — 로컬에서 시작 | PyTorch 에서 확인할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_3.png" alt="PyTorch Installation"></p>
<p>PyTorch을 설치한 후 터미널에서 PyTorch에서 볼 수 있는 GPU의 수를 확인하세요.</p>
<pre><code class="hljs language-python">python

>> <span class="hljs-keyword">import</span> torch
>> <span class="hljs-built_in">print</span>(torch.cuda.device_count())
<span class="hljs-number">8</span>
</code></pre>
<p>이 명령은 시스템에 설치된 GPU의 수를 출력해야 합니다 (내 경우엔 8개), 그리고 nvidia-smi 명령에 나열된 GPU 수와 일치해야 합니다.</p>
<p>바로 시작해도 괜찮아요! 여러 개의 GPU를 활용하는 딥 러닝 프로젝트에 대해 작업할 준비가 되었어요 🥳.</p>
<h1>다음 단계는? Multi-GPU 설정을 활용한 딥 러닝 프로젝트로 시작해보세요 (LLMs)</h1>
<ol>
<li>
<p>🤗 시작하려면 Hugging Face에서 인기 있는 모델을 복제해보세요:</p>
</li>
<li>
<p>💬 추론에 대한 설치 (LLM 모델 사용)를 위해 exllamav2를 별도의 환경에 복제하고 설치하세요. 이렇게 하면 모든 GPU가 사용되어 더 빠른 추론이 가능합니다: (자세한 튜토리얼은 제 미디엄 페이지를 확인하세요)</p>
</li>
</ol>
<ol start="3">
<li>👨‍🏫 파인 튜닝이나 훈련을 위해 torchtune을 복제하고 설치할 수 있습니다. 지침을 따라 모델을 전체 미세 조정하거나 Lora 미세 조정할 수 있습니다. GPU를 최대한 활용하세요: (자세한 튜토리얼은 내 미디엄 페이지를 확인해주세요)</li>
</ol>
<h1>결론</h1>
<p>이 안내서는 다중 GPU 딥 러닝에 필요한 기계 설정을 안내해줍니다. 이제 torchtune과 같은 다중 GPU를 활용하는 모든 프로젝트에 대해 작업을 시작할 수 있습니다. 빠른 개발을 위한 torchtune과 exllamaV2의 자세한 튜토리얼을 기대해주세요.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기","description":"","date":"2024-05-20 17:53","slug":"2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024","content":"\n\n## 다중 GPU로 딥 러닝\n\n![image](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png)\n\n딥 러닝 모델(특히 LLMs)이 점점 커지면, GPU 메모리(VRAM)가 더 많이 필요해집니다. 로컬에서 이들을 개발하고 사용하기 위해 다중 GPU 머신을 구축하거나 획득하는 것은 첫 번째 도전일 뿐입니다. 대부분의 라이브러리와 애플리케이션은 기본적으로 단일 GPU만 사용합니다. 따라서 머신에는 다중 GPU 설정을 활용할 수 있는 적절한 드라이버와 라이브러리가 필요합니다.\n\n본 기사는 중요한 라이브러리를 갖춘 Nvidia 다중 GPU(Linux) 머신을 설정하는 방법에 대한 가이드를 제공합니다. 이를 통해 실험에 소요되는 시간을 아낄 수 있고 개발을 시작하는 데 도움이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 딥러닝을 위한 멀티 GPU 설정을 활용할 수 있는 인기 있는 오픈 소스 라이브러리로의 링크가 제공됩니다.\n\n## 대상\n\n딥러닝을 시작하기 위해 CUDA Toolkit, PyTorch 및 Miniconda를 설치하여 exllamaV2 및 torchtune과 같은 프레임워크를 사용할 것입니다.\n\n©️ 이 이야기에서 언급된 모든 라이브러리 및 정보는 오픈 소스이거나 공개적으로 사용 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 시작하기\n\n![그림](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_1.png)\n\n터미널에서 nvidia-smi 명령어를 사용하여 기계에 설치된 GPU 수를 확인하십시오. 설치된 GPU의 목록이 출력되어야 합니다. 만약 명령어가 작동하지 않거나 불일치가 있는 경우 먼저 해당 Linux 버전용 Nvidia 드라이버를 설치하십시오. nvidia-smi 명령어가 기계에 설치된 모든 GPU 목록을 출력하도록 확인하십시오.\n\n이 페이지를 따라 설치하지 않은 경우 Nvidia 드라이버를 설치하십시오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우분투 22.04에 NVIDIA 드라이버를 설치하는 방법 - Linux 자습서 - Linux 구성 학습 - (출처: linuxconfig.org)\n\n## 단계 1 CUDA-Toolkit 설치하기\n\n💡 usr/local/cuda-xx 경로에 기존 CUDA 폴더가 있는지 확인하세요. 해당 폴더가 있다면 CUDA의 버전이 이미 설치된 것입니다. 원하는 CUDA 툴킷이 이미 설치되어 있는 경우 (터미널에서 nvcc 명령으로 확인하실 수 있습니다.) 다음 단계인 단계 2로 이동해주세요.\n\nPyTorch 라이브러리용 필요한 CUDA 버전을 확인하세요: 로컬에서 시작하기 | PyTorch (저희는 CUDA 12.1을 설치하고 있습니다)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCUDA Toolkit 12.1 Downloads | NVIDIA Developer 에 방문하여 CUDA 12.1을 설치하기 위한 Linux 명령어를 얻을 수 있어요 (OS 버전과 해당하는 “deb (local)” 설치 프로그램 유형을 선택).\n\n![이미지](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_2.png)\n\n선택한 옵션에 따라 기본 설치 프로그램의 터미널 명령어가 나타날 거에요. CUDA 툴킷을 설치하려면 해당 명령어를 Linux 터미널에서 복사하여 붙여넣기하여 실행하세요. 예를 들어, x86_64 Ubuntu 22 용으로 다음 명령어를 실행하려면 다운로드 폴더에서 터미널을 열고 다음 명령어를 실행하세요:\n\n```js\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n⚠️ CUDA 툴킷을 설치하는 동안, 설치 프로그램이 커널 업데이트를 요구할 수 있습니다. 터미널에서 커널 업데이트를 요청하는 팝업이 나타나면 esc 버튼을 눌러 취소하세요. 이 단계에서는 커널을 업데이트하지 마세요! — 이렇게 하면 Nvidia 드라이버가 손상될 수 있습니다 ☠️.\n\n설치 후 리눅스 머신을 다시 시작하세요. nvcc 명령이 여전히 작동하지 않을 것입니다. CUDA 설치를 PATH에 추가해야 합니다. 나노 편집기를 사용하여 .bashrc 파일을 엽니다.\n\n```js\nnano /home/$USER/.bashrc\n```\n\n.bashrc 파일 맨 아래로 스크롤하여 다음 두 줄을 추가하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nexport PATH=\"/usr/local/cuda-12.1/bin:$PATH\"\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH\"\n```\n\n💡 참고로 설치된 CUDA 버전에 맞게 cuda-12.1을 필요에 따라 cuda-xx로 변경할 수 있습니다. 여기서 'xx'는 CUDA 버전을 나타냅니다.\n\n변경 사항을 저장하고 nano 편집기를 닫으려면:\n\n```js\n변경 사항 저장 - 키보드에서 다음을 누르세요:\n\nctrl + o             --\u003e 저장\n엔터 또는 리턴 키     --\u003e 변경 사항 수락\nctrl + x             --\u003e 편집기 닫기\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널을 닫고 다시 열어주세요. 그러면 nvcc --version 명령을 실행하면 설치된 CUDA 버전이 터미널에 표시될 겁니다.\n\n## 단계 2 - Miniconda 설치\n\nPyTorch를 설치하기 전에 Miniconda를 설치하고, 그 안에 PyTorch를 설치하는 것이 좋습니다. 또한 각 프로젝트마다 새로운 Conda 환경을 만드는 것이 편리합니다.\n\nDownloads 폴더에서 터미널을 열고 다음 명령을 실행하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n```\n\n# initiate conda\n```bash\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n\n터미널을 닫고 다시 열어주세요. 이제 conda 명령어를 사용할 수 있어요.\n\n## 단계-3 PyTorch 설치하기\n\n(선택 사항) — 프로젝트용 새로운 conda 환경을 생성하세요. `environment-name`을 원하는 이름으로 바꿀 수 있어요. 일반적으로 제 프로젝트 이름으로 지정해요. 💡 프로젝트 작업 전후에 conda activate `environment-name` 및 conda deactivate `environment-name` 명령어를 사용할 수 있어요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nconda create -n \u003cenvironment-name\u003e python=3.11\n\n# 환경 활성화\nconda activate \u003cenvironment-name\u003e\n```\n\nCUDA 버전에 맞게 PyTorch 라이브러리를 설치하세요. 아래 명령어는 우리가 cuda-12.1에 설치한 것입니다:\n\n```js\npip3 install torch torchvision torchaudio\n```\n\n위 명령어는 PyTorch 설치 가이드 — 로컬에서 시작 | PyTorch 에서 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![PyTorch Installation](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_3.png)\n\nPyTorch을 설치한 후 터미널에서 PyTorch에서 볼 수 있는 GPU의 수를 확인하세요.\n\n```python\npython\n\n\u003e\u003e import torch\n\u003e\u003e print(torch.cuda.device_count())\n8\n```\n\n이 명령은 시스템에 설치된 GPU의 수를 출력해야 합니다 (내 경우엔 8개), 그리고 nvidia-smi 명령에 나열된 GPU 수와 일치해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n바로 시작해도 괜찮아요! 여러 개의 GPU를 활용하는 딥 러닝 프로젝트에 대해 작업할 준비가 되었어요 🥳.\n\n# 다음 단계는? Multi-GPU 설정을 활용한 딥 러닝 프로젝트로 시작해보세요 (LLMs)\n\n1. 🤗 시작하려면 Hugging Face에서 인기 있는 모델을 복제해보세요:\n\n2. 💬 추론에 대한 설치 (LLM 모델 사용)를 위해 exllamav2를 별도의 환경에 복제하고 설치하세요. 이렇게 하면 모든 GPU가 사용되어 더 빠른 추론이 가능합니다: (자세한 튜토리얼은 제 미디엄 페이지를 확인하세요)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 👨‍🏫 파인 튜닝이나 훈련을 위해 torchtune을 복제하고 설치할 수 있습니다. 지침을 따라 모델을 전체 미세 조정하거나 Lora 미세 조정할 수 있습니다. GPU를 최대한 활용하세요: (자세한 튜토리얼은 내 미디엄 페이지를 확인해주세요)\n\n# 결론\n\n이 안내서는 다중 GPU 딥 러닝에 필요한 기계 설정을 안내해줍니다. 이제 torchtune과 같은 다중 GPU를 활용하는 모든 프로젝트에 대해 작업을 시작할 수 있습니다. 빠른 개발을 위한 torchtune과 exllamaV2의 자세한 튜토리얼을 기대해주세요.","ogImage":{"url":"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e다중 GPU로 딥 러닝\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e딥 러닝 모델(특히 LLMs)이 점점 커지면, GPU 메모리(VRAM)가 더 많이 필요해집니다. 로컬에서 이들을 개발하고 사용하기 위해 다중 GPU 머신을 구축하거나 획득하는 것은 첫 번째 도전일 뿐입니다. 대부분의 라이브러리와 애플리케이션은 기본적으로 단일 GPU만 사용합니다. 따라서 머신에는 다중 GPU 설정을 활용할 수 있는 적절한 드라이버와 라이브러리가 필요합니다.\u003c/p\u003e\n\u003cp\u003e본 기사는 중요한 라이브러리를 갖춘 Nvidia 다중 GPU(Linux) 머신을 설정하는 방법에 대한 가이드를 제공합니다. 이를 통해 실험에 소요되는 시간을 아낄 수 있고 개발을 시작하는 데 도움이 될 것입니다.\u003c/p\u003e\n\u003cp\u003e마지막으로, 딥러닝을 위한 멀티 GPU 설정을 활용할 수 있는 인기 있는 오픈 소스 라이브러리로의 링크가 제공됩니다.\u003c/p\u003e\n\u003ch2\u003e대상\u003c/h2\u003e\n\u003cp\u003e딥러닝을 시작하기 위해 CUDA Toolkit, PyTorch 및 Miniconda를 설치하여 exllamaV2 및 torchtune과 같은 프레임워크를 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e©️ 이 이야기에서 언급된 모든 라이브러리 및 정보는 오픈 소스이거나 공개적으로 사용 가능합니다.\u003c/p\u003e\n\u003ch2\u003e시작하기\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_1.png\" alt=\"그림\"\u003e\u003c/p\u003e\n\u003cp\u003e터미널에서 nvidia-smi 명령어를 사용하여 기계에 설치된 GPU 수를 확인하십시오. 설치된 GPU의 목록이 출력되어야 합니다. 만약 명령어가 작동하지 않거나 불일치가 있는 경우 먼저 해당 Linux 버전용 Nvidia 드라이버를 설치하십시오. nvidia-smi 명령어가 기계에 설치된 모든 GPU 목록을 출력하도록 확인하십시오.\u003c/p\u003e\n\u003cp\u003e이 페이지를 따라 설치하지 않은 경우 Nvidia 드라이버를 설치하십시오:\u003c/p\u003e\n\u003cp\u003e우분투 22.04에 NVIDIA 드라이버를 설치하는 방법 - Linux 자습서 - Linux 구성 학습 - (출처: linuxconfig.org)\u003c/p\u003e\n\u003ch2\u003e단계 1 CUDA-Toolkit 설치하기\u003c/h2\u003e\n\u003cp\u003e💡 usr/local/cuda-xx 경로에 기존 CUDA 폴더가 있는지 확인하세요. 해당 폴더가 있다면 CUDA의 버전이 이미 설치된 것입니다. 원하는 CUDA 툴킷이 이미 설치되어 있는 경우 (터미널에서 nvcc 명령으로 확인하실 수 있습니다.) 다음 단계인 단계 2로 이동해주세요.\u003c/p\u003e\n\u003cp\u003ePyTorch 라이브러리용 필요한 CUDA 버전을 확인하세요: 로컬에서 시작하기 | PyTorch (저희는 CUDA 12.1을 설치하고 있습니다)\u003c/p\u003e\n\u003cp\u003eCUDA Toolkit 12.1 Downloads | NVIDIA Developer 에 방문하여 CUDA 12.1을 설치하기 위한 Linux 명령어를 얻을 수 있어요 (OS 버전과 해당하는 “deb (local)” 설치 프로그램 유형을 선택).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e선택한 옵션에 따라 기본 설치 프로그램의 터미널 명령어가 나타날 거에요. CUDA 툴킷을 설치하려면 해당 명령어를 Linux 터미널에서 복사하여 붙여넣기하여 실행하세요. 예를 들어, x86_64 Ubuntu 22 용으로 다음 명령어를 실행하려면 다운로드 폴더에서 터미널을 열고 다음 명령어를 실행하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ewget \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\u003c/span\u003e\nsudo mv cuda-ubuntu2204.\u003cspan class=\"hljs-property\"\u003epin\u003c/span\u003e /etc/apt/preferences.\u003cspan class=\"hljs-property\"\u003ed\u003c/span\u003e/cuda-repository-pin-\u003cspan class=\"hljs-number\"\u003e600\u003c/span\u003e\nwget \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\u003c/span\u003e\nsudo dpkg -i cuda-repo-ubuntu2204-\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e-local_12\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e530.30\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.02\u003c/span\u003e-1_amd64.\u003cspan class=\"hljs-property\"\u003edeb\u003c/span\u003e\nsudo cp /\u003cspan class=\"hljs-keyword\"\u003evar\u003c/span\u003e/cuda-repo-ubuntu2204-\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e-local/cuda-*-keyring.\u003cspan class=\"hljs-property\"\u003egpg\u003c/span\u003e /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e⚠️ CUDA 툴킷을 설치하는 동안, 설치 프로그램이 커널 업데이트를 요구할 수 있습니다. 터미널에서 커널 업데이트를 요청하는 팝업이 나타나면 esc 버튼을 눌러 취소하세요. 이 단계에서는 커널을 업데이트하지 마세요! — 이렇게 하면 Nvidia 드라이버가 손상될 수 있습니다 ☠️.\u003c/p\u003e\n\u003cp\u003e설치 후 리눅스 머신을 다시 시작하세요. nvcc 명령이 여전히 작동하지 않을 것입니다. CUDA 설치를 PATH에 추가해야 합니다. 나노 편집기를 사용하여 .bashrc 파일을 엽니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003enano /home/$USER/.\u003cspan class=\"hljs-property\"\u003ebashrc\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e.bashrc 파일 맨 아래로 스크롤하여 다음 두 줄을 추가하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003ePATH\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"/usr/local/cuda-12.1/bin:$PATH\"\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eLD_LIBRARY_PATH\u003c/span\u003e=\u003cspan class=\"hljs-string\"\u003e\"/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e💡 참고로 설치된 CUDA 버전에 맞게 cuda-12.1을 필요에 따라 cuda-xx로 변경할 수 있습니다. 여기서 'xx'는 CUDA 버전을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e변경 사항을 저장하고 nano 편집기를 닫으려면:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e변경 사항 저장 - 키보드에서 다음을 누르세요:\n\nctrl + o             --\u003e 저장\n엔터 또는 리턴 키     --\u003e 변경 사항 수락\nctrl + x             --\u003e 편집기 닫기\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e터미널을 닫고 다시 열어주세요. 그러면 nvcc --version 명령을 실행하면 설치된 CUDA 버전이 터미널에 표시될 겁니다.\u003c/p\u003e\n\u003ch2\u003e단계 2 - Miniconda 설치\u003c/h2\u003e\n\u003cp\u003ePyTorch를 설치하기 전에 Miniconda를 설치하고, 그 안에 PyTorch를 설치하는 것이 좋습니다. 또한 각 프로젝트마다 새로운 Conda 환경을 만드는 것이 편리합니다.\u003c/p\u003e\n\u003cp\u003eDownloads 폴더에서 터미널을 열고 다음 명령을 실행하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n\u003cspan class=\"hljs-built_in\"\u003erm\u003c/span\u003e -rf ~/miniconda3/miniconda.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003einitiate conda\u003c/h1\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e터미널을 닫고 다시 열어주세요. 이제 conda 명령어를 사용할 수 있어요.\u003c/p\u003e\n\u003ch2\u003e단계-3 PyTorch 설치하기\u003c/h2\u003e\n\u003cp\u003e(선택 사항) — 프로젝트용 새로운 conda 환경을 생성하세요. \u003ccode\u003eenvironment-name\u003c/code\u003e을 원하는 이름으로 바꿀 수 있어요. 일반적으로 제 프로젝트 이름으로 지정해요. 💡 프로젝트 작업 전후에 conda activate \u003ccode\u003eenvironment-name\u003c/code\u003e 및 conda deactivate \u003ccode\u003eenvironment-name\u003c/code\u003e 명령어를 사용할 수 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003econda create -n \u0026#x3C;environment-name\u003e python=\u003cspan class=\"hljs-number\"\u003e3.11\u003c/span\u003e\n\n# 환경 활성화\nconda activate \u0026#x3C;environment-name\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCUDA 버전에 맞게 PyTorch 라이브러리를 설치하세요. 아래 명령어는 우리가 cuda-12.1에 설치한 것입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip3 install torch torchvision torchaudio\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위 명령어는 PyTorch 설치 가이드 — 로컬에서 시작 | PyTorch 에서 확인할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_3.png\" alt=\"PyTorch Installation\"\u003e\u003c/p\u003e\n\u003cp\u003ePyTorch을 설치한 후 터미널에서 PyTorch에서 볼 수 있는 GPU의 수를 확인하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003epython\n\n\u003e\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003e\u003e \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(torch.cuda.device_count())\n\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 명령은 시스템에 설치된 GPU의 수를 출력해야 합니다 (내 경우엔 8개), 그리고 nvidia-smi 명령에 나열된 GPU 수와 일치해야 합니다.\u003c/p\u003e\n\u003cp\u003e바로 시작해도 괜찮아요! 여러 개의 GPU를 활용하는 딥 러닝 프로젝트에 대해 작업할 준비가 되었어요 🥳.\u003c/p\u003e\n\u003ch1\u003e다음 단계는? Multi-GPU 설정을 활용한 딥 러닝 프로젝트로 시작해보세요 (LLMs)\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e🤗 시작하려면 Hugging Face에서 인기 있는 모델을 복제해보세요:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e💬 추론에 대한 설치 (LLM 모델 사용)를 위해 exllamav2를 별도의 환경에 복제하고 설치하세요. 이렇게 하면 모든 GPU가 사용되어 더 빠른 추론이 가능합니다: (자세한 튜토리얼은 제 미디엄 페이지를 확인하세요)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e👨‍🏫 파인 튜닝이나 훈련을 위해 torchtune을 복제하고 설치할 수 있습니다. 지침을 따라 모델을 전체 미세 조정하거나 Lora 미세 조정할 수 있습니다. GPU를 최대한 활용하세요: (자세한 튜토리얼은 내 미디엄 페이지를 확인해주세요)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e이 안내서는 다중 GPU 딥 러닝에 필요한 기계 설정을 안내해줍니다. 이제 torchtune과 같은 다중 GPU를 활용하는 모든 프로젝트에 대해 작업을 시작할 수 있습니다. 빠른 개발을 위한 torchtune과 exllamaV2의 자세한 튜토리얼을 기대해주세요.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024"},"buildId":"o1YmnmSuZvAX2O4TI9r41","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>