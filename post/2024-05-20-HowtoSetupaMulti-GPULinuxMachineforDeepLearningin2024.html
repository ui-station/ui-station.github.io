<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024" data-gatsby-head="true"/><meta name="twitter:title" content="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-20 17:53" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_buildManifest.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 20, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><h2>다중 GPU로 딥 러닝</h2>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png" alt="image"/></p>
<p>딥 러닝 모델(특히 LLMs)이 점점 커지면, GPU 메모리(VRAM)가 더 많이 필요해집니다. 로컬에서 이들을 개발하고 사용하기 위해 다중 GPU 머신을 구축하거나 획득하는 것은 첫 번째 도전일 뿐입니다. 대부분의 라이브러리와 애플리케이션은 기본적으로 단일 GPU만 사용합니다. 따라서 머신에는 다중 GPU 설정을 활용할 수 있는 적절한 드라이버와 라이브러리가 필요합니다.</p>
<p>본 기사는 중요한 라이브러리를 갖춘 Nvidia 다중 GPU(Linux) 머신을 설정하는 방법에 대한 가이드를 제공합니다. 이를 통해 실험에 소요되는 시간을 아낄 수 있고 개발을 시작하는 데 도움이 될 것입니다.</p>
<div class="content-ad"></div>
<p>마지막으로, 딥러닝을 위한 멀티 GPU 설정을 활용할 수 있는 인기 있는 오픈 소스 라이브러리로의 링크가 제공됩니다.</p>
<h2>대상</h2>
<p>딥러닝을 시작하기 위해 CUDA Toolkit, PyTorch 및 Miniconda를 설치하여 exllamaV2 및 torchtune과 같은 프레임워크를 사용할 것입니다.</p>
<p>©️ 이 이야기에서 언급된 모든 라이브러리 및 정보는 오픈 소스이거나 공개적으로 사용 가능합니다.</p>
<div class="content-ad"></div>
<h2>시작하기</h2>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_1.png" alt="그림"/></p>
<p>터미널에서 nvidia-smi 명령어를 사용하여 기계에 설치된 GPU 수를 확인하십시오. 설치된 GPU의 목록이 출력되어야 합니다. 만약 명령어가 작동하지 않거나 불일치가 있는 경우 먼저 해당 Linux 버전용 Nvidia 드라이버를 설치하십시오. nvidia-smi 명령어가 기계에 설치된 모든 GPU 목록을 출력하도록 확인하십시오.</p>
<p>이 페이지를 따라 설치하지 않은 경우 Nvidia 드라이버를 설치하십시오:</p>
<div class="content-ad"></div>
<p>우분투 22.04에 NVIDIA 드라이버를 설치하는 방법 - Linux 자습서 - Linux 구성 학습 - (출처: linuxconfig.org)</p>
<h2>단계 1 CUDA-Toolkit 설치하기</h2>
<p>💡 usr/local/cuda-xx 경로에 기존 CUDA 폴더가 있는지 확인하세요. 해당 폴더가 있다면 CUDA의 버전이 이미 설치된 것입니다. 원하는 CUDA 툴킷이 이미 설치되어 있는 경우 (터미널에서 nvcc 명령으로 확인하실 수 있습니다.) 다음 단계인 단계 2로 이동해주세요.</p>
<p>PyTorch 라이브러리용 필요한 CUDA 버전을 확인하세요: 로컬에서 시작하기 | PyTorch (저희는 CUDA 12.1을 설치하고 있습니다)</p>
<div class="content-ad"></div>
<p>CUDA Toolkit 12.1 Downloads | NVIDIA Developer 에 방문하여 CUDA 12.1을 설치하기 위한 Linux 명령어를 얻을 수 있어요 (OS 버전과 해당하는 “deb (local)” 설치 프로그램 유형을 선택).</p>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_2.png" alt="이미지"/></p>
<p>선택한 옵션에 따라 기본 설치 프로그램의 터미널 명령어가 나타날 거에요. CUDA 툴킷을 설치하려면 해당 명령어를 Linux 터미널에서 복사하여 붙여넣기하여 실행하세요. 예를 들어, x86_64 Ubuntu 22 용으로 다음 명령어를 실행하려면 다운로드 폴더에서 터미널을 열고 다음 명령어를 실행하세요:</p>
<pre><code class="hljs language-js">wget <span class="hljs-attr">https</span>:<span class="hljs-comment">//developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</span>
sudo mv cuda-ubuntu2204.<span class="hljs-property">pin</span> /etc/apt/preferences.<span class="hljs-property">d</span>/cuda-repository-pin-<span class="hljs-number">600</span>
wget <span class="hljs-attr">https</span>:<span class="hljs-comment">//developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb</span>
sudo dpkg -i cuda-repo-ubuntu2204-<span class="hljs-number">12</span>-<span class="hljs-number">1</span>-local_12<span class="hljs-number">.1</span><span class="hljs-number">.0</span>-<span class="hljs-number">530.30</span><span class="hljs-number">.02</span>-1_amd64.<span class="hljs-property">deb</span>
sudo cp /<span class="hljs-keyword">var</span>/cuda-repo-ubuntu2204-<span class="hljs-number">12</span>-<span class="hljs-number">1</span>-local/cuda-*-keyring.<span class="hljs-property">gpg</span> /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda
</code></pre>
<div class="content-ad"></div>
<p>⚠️ CUDA 툴킷을 설치하는 동안, 설치 프로그램이 커널 업데이트를 요구할 수 있습니다. 터미널에서 커널 업데이트를 요청하는 팝업이 나타나면 esc 버튼을 눌러 취소하세요. 이 단계에서는 커널을 업데이트하지 마세요! — 이렇게 하면 Nvidia 드라이버가 손상될 수 있습니다 ☠️.</p>
<p>설치 후 리눅스 머신을 다시 시작하세요. nvcc 명령이 여전히 작동하지 않을 것입니다. CUDA 설치를 PATH에 추가해야 합니다. 나노 편집기를 사용하여 .bashrc 파일을 엽니다.</p>
<pre><code class="hljs language-js">nano /home/$USER/.<span class="hljs-property">bashrc</span>
</code></pre>
<p>.bashrc 파일 맨 아래로 스크롤하여 다음 두 줄을 추가하세요:</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-variable constant_">PATH</span>=<span class="hljs-string">&quot;/usr/local/cuda-12.1/bin:$PATH&quot;</span>
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">LD_LIBRARY_PATH</span>=<span class="hljs-string">&quot;/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH&quot;</span>
</code></pre>
<p>💡 참고로 설치된 CUDA 버전에 맞게 cuda-12.1을 필요에 따라 cuda-xx로 변경할 수 있습니다. 여기서 &#x27;xx&#x27;는 CUDA 버전을 나타냅니다.</p>
<p>변경 사항을 저장하고 nano 편집기를 닫으려면:</p>
<pre><code class="hljs language-js">변경 사항 저장 - 키보드에서 다음을 누르세요:

ctrl + o             --&gt; 저장
엔터 또는 리턴 키     --&gt; 변경 사항 수락
ctrl + x             --&gt; 편집기 닫기
</code></pre>
<div class="content-ad"></div>
<p>터미널을 닫고 다시 열어주세요. 그러면 nvcc --version 명령을 실행하면 설치된 CUDA 버전이 터미널에 표시될 겁니다.</p>
<h2>단계 2 - Miniconda 설치</h2>
<p>PyTorch를 설치하기 전에 Miniconda를 설치하고, 그 안에 PyTorch를 설치하는 것이 좋습니다. 또한 각 프로젝트마다 새로운 Conda 환경을 만드는 것이 편리합니다.</p>
<p>Downloads 폴더에서 터미널을 열고 다음 명령을 실행하세요.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-bash"><span class="hljs-built_in">mkdir</span> -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
<span class="hljs-built_in">rm</span> -rf ~/miniconda3/miniconda.sh
</code></pre>
<h1>initiate conda</h1>
<pre><code class="hljs language-bash">~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
</code></pre>
<p>터미널을 닫고 다시 열어주세요. 이제 conda 명령어를 사용할 수 있어요.</p>
<h2>단계-3 PyTorch 설치하기</h2>
<p>(선택 사항) — 프로젝트용 새로운 conda 환경을 생성하세요. <code>environment-name</code>을 원하는 이름으로 바꿀 수 있어요. 일반적으로 제 프로젝트 이름으로 지정해요. 💡 프로젝트 작업 전후에 conda activate <code>environment-name</code> 및 conda deactivate <code>environment-name</code> 명령어를 사용할 수 있어요.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">conda create -n &lt;environment-name&gt; python=<span class="hljs-number">3.11</span>

# 환경 활성화
conda activate &lt;environment-name&gt;
</code></pre>
<p>CUDA 버전에 맞게 PyTorch 라이브러리를 설치하세요. 아래 명령어는 우리가 cuda-12.1에 설치한 것입니다:</p>
<pre><code class="hljs language-js">pip3 install torch torchvision torchaudio
</code></pre>
<p>위 명령어는 PyTorch 설치 가이드 — 로컬에서 시작 | PyTorch 에서 확인할 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_3.png" alt="PyTorch Installation"/></p>
<p>PyTorch을 설치한 후 터미널에서 PyTorch에서 볼 수 있는 GPU의 수를 확인하세요.</p>
<pre><code class="hljs language-python">python

&gt;&gt; <span class="hljs-keyword">import</span> torch
&gt;&gt; <span class="hljs-built_in">print</span>(torch.cuda.device_count())
<span class="hljs-number">8</span>
</code></pre>
<p>이 명령은 시스템에 설치된 GPU의 수를 출력해야 합니다 (내 경우엔 8개), 그리고 nvidia-smi 명령에 나열된 GPU 수와 일치해야 합니다.</p>
<div class="content-ad"></div>
<p>바로 시작해도 괜찮아요! 여러 개의 GPU를 활용하는 딥 러닝 프로젝트에 대해 작업할 준비가 되었어요 🥳.</p>
<h1>다음 단계는? Multi-GPU 설정을 활용한 딥 러닝 프로젝트로 시작해보세요 (LLMs)</h1>
<ol>
<li>
<p>🤗 시작하려면 Hugging Face에서 인기 있는 모델을 복제해보세요:</p>
</li>
<li>
<p>💬 추론에 대한 설치 (LLM 모델 사용)를 위해 exllamav2를 별도의 환경에 복제하고 설치하세요. 이렇게 하면 모든 GPU가 사용되어 더 빠른 추론이 가능합니다: (자세한 튜토리얼은 제 미디엄 페이지를 확인하세요)</p>
</li>
</ol>
<div class="content-ad"></div>
<ol start="3">
<li>👨‍🏫 파인 튜닝이나 훈련을 위해 torchtune을 복제하고 설치할 수 있습니다. 지침을 따라 모델을 전체 미세 조정하거나 Lora 미세 조정할 수 있습니다. GPU를 최대한 활용하세요: (자세한 튜토리얼은 내 미디엄 페이지를 확인해주세요)</li>
</ol>
<h1>결론</h1>
<p>이 안내서는 다중 GPU 딥 러닝에 필요한 기계 설정을 안내해줍니다. 이제 torchtune과 같은 다중 GPU를 활용하는 모든 프로젝트에 대해 작업을 시작할 수 있습니다. 빠른 개발을 위한 torchtune과 exllamaV2의 자세한 튜토리얼을 기대해주세요.</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"2024년을 위한 딥 러닝을 위한 멀티 GPU 리눅스 머신 설정하기","description":"","date":"2024-05-20 17:53","slug":"2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024","content":"\n\n## 다중 GPU로 딥 러닝\n\n![image](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png)\n\n딥 러닝 모델(특히 LLMs)이 점점 커지면, GPU 메모리(VRAM)가 더 많이 필요해집니다. 로컬에서 이들을 개발하고 사용하기 위해 다중 GPU 머신을 구축하거나 획득하는 것은 첫 번째 도전일 뿐입니다. 대부분의 라이브러리와 애플리케이션은 기본적으로 단일 GPU만 사용합니다. 따라서 머신에는 다중 GPU 설정을 활용할 수 있는 적절한 드라이버와 라이브러리가 필요합니다.\n\n본 기사는 중요한 라이브러리를 갖춘 Nvidia 다중 GPU(Linux) 머신을 설정하는 방법에 대한 가이드를 제공합니다. 이를 통해 실험에 소요되는 시간을 아낄 수 있고 개발을 시작하는 데 도움이 될 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 딥러닝을 위한 멀티 GPU 설정을 활용할 수 있는 인기 있는 오픈 소스 라이브러리로의 링크가 제공됩니다.\n\n## 대상\n\n딥러닝을 시작하기 위해 CUDA Toolkit, PyTorch 및 Miniconda를 설치하여 exllamaV2 및 torchtune과 같은 프레임워크를 사용할 것입니다.\n\n©️ 이 이야기에서 언급된 모든 라이브러리 및 정보는 오픈 소스이거나 공개적으로 사용 가능합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 시작하기\n\n![그림](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_1.png)\n\n터미널에서 nvidia-smi 명령어를 사용하여 기계에 설치된 GPU 수를 확인하십시오. 설치된 GPU의 목록이 출력되어야 합니다. 만약 명령어가 작동하지 않거나 불일치가 있는 경우 먼저 해당 Linux 버전용 Nvidia 드라이버를 설치하십시오. nvidia-smi 명령어가 기계에 설치된 모든 GPU 목록을 출력하도록 확인하십시오.\n\n이 페이지를 따라 설치하지 않은 경우 Nvidia 드라이버를 설치하십시오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우분투 22.04에 NVIDIA 드라이버를 설치하는 방법 - Linux 자습서 - Linux 구성 학습 - (출처: linuxconfig.org)\n\n## 단계 1 CUDA-Toolkit 설치하기\n\n💡 usr/local/cuda-xx 경로에 기존 CUDA 폴더가 있는지 확인하세요. 해당 폴더가 있다면 CUDA의 버전이 이미 설치된 것입니다. 원하는 CUDA 툴킷이 이미 설치되어 있는 경우 (터미널에서 nvcc 명령으로 확인하실 수 있습니다.) 다음 단계인 단계 2로 이동해주세요.\n\nPyTorch 라이브러리용 필요한 CUDA 버전을 확인하세요: 로컬에서 시작하기 | PyTorch (저희는 CUDA 12.1을 설치하고 있습니다)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCUDA Toolkit 12.1 Downloads | NVIDIA Developer 에 방문하여 CUDA 12.1을 설치하기 위한 Linux 명령어를 얻을 수 있어요 (OS 버전과 해당하는 “deb (local)” 설치 프로그램 유형을 선택).\n\n![이미지](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_2.png)\n\n선택한 옵션에 따라 기본 설치 프로그램의 터미널 명령어가 나타날 거에요. CUDA 툴킷을 설치하려면 해당 명령어를 Linux 터미널에서 복사하여 붙여넣기하여 실행하세요. 예를 들어, x86_64 Ubuntu 22 용으로 다음 명령어를 실행하려면 다운로드 폴더에서 터미널을 열고 다음 명령어를 실행하세요:\n\n```js\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n⚠️ CUDA 툴킷을 설치하는 동안, 설치 프로그램이 커널 업데이트를 요구할 수 있습니다. 터미널에서 커널 업데이트를 요청하는 팝업이 나타나면 esc 버튼을 눌러 취소하세요. 이 단계에서는 커널을 업데이트하지 마세요! — 이렇게 하면 Nvidia 드라이버가 손상될 수 있습니다 ☠️.\n\n설치 후 리눅스 머신을 다시 시작하세요. nvcc 명령이 여전히 작동하지 않을 것입니다. CUDA 설치를 PATH에 추가해야 합니다. 나노 편집기를 사용하여 .bashrc 파일을 엽니다.\n\n```js\nnano /home/$USER/.bashrc\n```\n\n.bashrc 파일 맨 아래로 스크롤하여 다음 두 줄을 추가하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nexport PATH=\"/usr/local/cuda-12.1/bin:$PATH\"\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH\"\n```\n\n💡 참고로 설치된 CUDA 버전에 맞게 cuda-12.1을 필요에 따라 cuda-xx로 변경할 수 있습니다. 여기서 'xx'는 CUDA 버전을 나타냅니다.\n\n변경 사항을 저장하고 nano 편집기를 닫으려면:\n\n```js\n변경 사항 저장 - 키보드에서 다음을 누르세요:\n\nctrl + o             --\u003e 저장\n엔터 또는 리턴 키     --\u003e 변경 사항 수락\nctrl + x             --\u003e 편집기 닫기\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n터미널을 닫고 다시 열어주세요. 그러면 nvcc --version 명령을 실행하면 설치된 CUDA 버전이 터미널에 표시될 겁니다.\n\n## 단계 2 - Miniconda 설치\n\nPyTorch를 설치하기 전에 Miniconda를 설치하고, 그 안에 PyTorch를 설치하는 것이 좋습니다. 또한 각 프로젝트마다 새로운 Conda 환경을 만드는 것이 편리합니다.\n\nDownloads 폴더에서 터미널을 열고 다음 명령을 실행하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\nmkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n```\n\n# initiate conda\n```bash\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n\n터미널을 닫고 다시 열어주세요. 이제 conda 명령어를 사용할 수 있어요.\n\n## 단계-3 PyTorch 설치하기\n\n(선택 사항) — 프로젝트용 새로운 conda 환경을 생성하세요. `environment-name`을 원하는 이름으로 바꿀 수 있어요. 일반적으로 제 프로젝트 이름으로 지정해요. 💡 프로젝트 작업 전후에 conda activate `environment-name` 및 conda deactivate `environment-name` 명령어를 사용할 수 있어요.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nconda create -n \u003cenvironment-name\u003e python=3.11\n\n# 환경 활성화\nconda activate \u003cenvironment-name\u003e\n```\n\nCUDA 버전에 맞게 PyTorch 라이브러리를 설치하세요. 아래 명령어는 우리가 cuda-12.1에 설치한 것입니다:\n\n```js\npip3 install torch torchvision torchaudio\n```\n\n위 명령어는 PyTorch 설치 가이드 — 로컬에서 시작 | PyTorch 에서 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![PyTorch Installation](/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_3.png)\n\nPyTorch을 설치한 후 터미널에서 PyTorch에서 볼 수 있는 GPU의 수를 확인하세요.\n\n```python\npython\n\n\u003e\u003e import torch\n\u003e\u003e print(torch.cuda.device_count())\n8\n```\n\n이 명령은 시스템에 설치된 GPU의 수를 출력해야 합니다 (내 경우엔 8개), 그리고 nvidia-smi 명령에 나열된 GPU 수와 일치해야 합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n바로 시작해도 괜찮아요! 여러 개의 GPU를 활용하는 딥 러닝 프로젝트에 대해 작업할 준비가 되었어요 🥳.\n\n# 다음 단계는? Multi-GPU 설정을 활용한 딥 러닝 프로젝트로 시작해보세요 (LLMs)\n\n1. 🤗 시작하려면 Hugging Face에서 인기 있는 모델을 복제해보세요:\n\n2. 💬 추론에 대한 설치 (LLM 모델 사용)를 위해 exllamav2를 별도의 환경에 복제하고 설치하세요. 이렇게 하면 모든 GPU가 사용되어 더 빠른 추론이 가능합니다: (자세한 튜토리얼은 제 미디엄 페이지를 확인하세요)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 👨‍🏫 파인 튜닝이나 훈련을 위해 torchtune을 복제하고 설치할 수 있습니다. 지침을 따라 모델을 전체 미세 조정하거나 Lora 미세 조정할 수 있습니다. GPU를 최대한 활용하세요: (자세한 튜토리얼은 내 미디엄 페이지를 확인해주세요)\n\n# 결론\n\n이 안내서는 다중 GPU 딥 러닝에 필요한 기계 설정을 안내해줍니다. 이제 torchtune과 같은 다중 GPU를 활용하는 모든 프로젝트에 대해 작업을 시작할 수 있습니다. 빠른 개발을 위한 torchtune과 exllamaV2의 자세한 튜토리얼을 기대해주세요.","ogImage":{"url":"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png","tag":["Tech"],"readingTime":6},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h2: \"h2\",\n    p: \"p\",\n    img: \"img\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\",\n    h1: \"h1\",\n    ol: \"ol\",\n    li: \"li\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h2, {\n      children: \"다중 GPU로 딥 러닝\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_0.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"딥 러닝 모델(특히 LLMs)이 점점 커지면, GPU 메모리(VRAM)가 더 많이 필요해집니다. 로컬에서 이들을 개발하고 사용하기 위해 다중 GPU 머신을 구축하거나 획득하는 것은 첫 번째 도전일 뿐입니다. 대부분의 라이브러리와 애플리케이션은 기본적으로 단일 GPU만 사용합니다. 따라서 머신에는 다중 GPU 설정을 활용할 수 있는 적절한 드라이버와 라이브러리가 필요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"본 기사는 중요한 라이브러리를 갖춘 Nvidia 다중 GPU(Linux) 머신을 설정하는 방법에 대한 가이드를 제공합니다. 이를 통해 실험에 소요되는 시간을 아낄 수 있고 개발을 시작하는 데 도움이 될 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로, 딥러닝을 위한 멀티 GPU 설정을 활용할 수 있는 인기 있는 오픈 소스 라이브러리로의 링크가 제공됩니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"대상\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"딥러닝을 시작하기 위해 CUDA Toolkit, PyTorch 및 Miniconda를 설치하여 exllamaV2 및 torchtune과 같은 프레임워크를 사용할 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"©️ 이 이야기에서 언급된 모든 라이브러리 및 정보는 오픈 소스이거나 공개적으로 사용 가능합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"시작하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_1.png\",\n        alt: \"그림\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"터미널에서 nvidia-smi 명령어를 사용하여 기계에 설치된 GPU 수를 확인하십시오. 설치된 GPU의 목록이 출력되어야 합니다. 만약 명령어가 작동하지 않거나 불일치가 있는 경우 먼저 해당 Linux 버전용 Nvidia 드라이버를 설치하십시오. nvidia-smi 명령어가 기계에 설치된 모든 GPU 목록을 출력하도록 확인하십시오.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 페이지를 따라 설치하지 않은 경우 Nvidia 드라이버를 설치하십시오:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우분투 22.04에 NVIDIA 드라이버를 설치하는 방법 - Linux 자습서 - Linux 구성 학습 - (출처: linuxconfig.org)\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단계 1 CUDA-Toolkit 설치하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"💡 usr/local/cuda-xx 경로에 기존 CUDA 폴더가 있는지 확인하세요. 해당 폴더가 있다면 CUDA의 버전이 이미 설치된 것입니다. 원하는 CUDA 툴킷이 이미 설치되어 있는 경우 (터미널에서 nvcc 명령으로 확인하실 수 있습니다.) 다음 단계인 단계 2로 이동해주세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"PyTorch 라이브러리용 필요한 CUDA 버전을 확인하세요: 로컬에서 시작하기 | PyTorch (저희는 CUDA 12.1을 설치하고 있습니다)\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CUDA Toolkit 12.1 Downloads | NVIDIA Developer 에 방문하여 CUDA 12.1을 설치하기 위한 Linux 명령어를 얻을 수 있어요 (OS 버전과 해당하는 “deb (local)” 설치 프로그램 유형을 선택).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_2.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"선택한 옵션에 따라 기본 설치 프로그램의 터미널 명령어가 나타날 거에요. CUDA 툴킷을 설치하려면 해당 명령어를 Linux 터미널에서 복사하여 붙여넣기하여 실행하세요. 예를 들어, x86_64 Ubuntu 22 용으로 다음 명령어를 실행하려면 다운로드 폴더에서 터미널을 열고 다음 명령어를 실행하세요:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"wget \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\"\n        }), \"\\nsudo mv cuda-ubuntu2204.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"pin\"\n        }), \" /etc/apt/preferences.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"d\"\n        }), \"/cuda-repository-pin-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"600\"\n        }), \"\\nwget \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\"\n        }), \"\\nsudo dpkg -i cuda-repo-ubuntu2204-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"12\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"-local_12\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".1\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".0\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"530.30\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".02\"\n        }), \"-1_amd64.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"deb\"\n        }), \"\\nsudo cp /\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"var\"\n        }), \"/cuda-repo-ubuntu2204-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"12\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \"-local/cuda-*-keyring.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"gpg\"\n        }), \" /usr/share/keyrings/\\nsudo apt-get update\\nsudo apt-get -y install cuda\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"⚠️ CUDA 툴킷을 설치하는 동안, 설치 프로그램이 커널 업데이트를 요구할 수 있습니다. 터미널에서 커널 업데이트를 요청하는 팝업이 나타나면 esc 버튼을 눌러 취소하세요. 이 단계에서는 커널을 업데이트하지 마세요! — 이렇게 하면 Nvidia 드라이버가 손상될 수 있습니다 ☠️.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"설치 후 리눅스 머신을 다시 시작하세요. nvcc 명령이 여전히 작동하지 않을 것입니다. CUDA 설치를 PATH에 추가해야 합니다. 나노 편집기를 사용하여 .bashrc 파일을 엽니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"nano /home/$USER/.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"bashrc\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \".bashrc 파일 맨 아래로 스크롤하여 다음 두 줄을 추가하세요:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"PATH\"\n        }), \"=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/usr/local/cuda-12.1/bin:$PATH\\\"\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"LD_LIBRARY_PATH\"\n        }), \"=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"💡 참고로 설치된 CUDA 버전에 맞게 cuda-12.1을 필요에 따라 cuda-xx로 변경할 수 있습니다. 여기서 'xx'는 CUDA 버전을 나타냅니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"변경 사항을 저장하고 nano 편집기를 닫으려면:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"변경 사항 저장 - 키보드에서 다음을 누르세요:\\n\\nctrl + o             --\u003e 저장\\n엔터 또는 리턴 키     --\u003e 변경 사항 수락\\nctrl + x             --\u003e 편집기 닫기\\n\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"터미널을 닫고 다시 열어주세요. 그러면 nvcc --version 명령을 실행하면 설치된 CUDA 버전이 터미널에 표시될 겁니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단계 2 - Miniconda 설치\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"PyTorch를 설치하기 전에 Miniconda를 설치하고, 그 안에 PyTorch를 설치하는 것이 좋습니다. 또한 각 프로젝트마다 새로운 Conda 환경을 만드는 것이 편리합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Downloads 폴더에서 터미널을 열고 다음 명령을 실행하세요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-bash\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"mkdir\"\n        }), \" -p ~/miniconda3\\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\\n\", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"rm\"\n        }), \" -rf ~/miniconda3/miniconda.sh\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"initiate conda\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-bash\",\n        children: \"~/miniconda3/bin/conda init bash\\n~/miniconda3/bin/conda init zsh\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"터미널을 닫고 다시 열어주세요. 이제 conda 명령어를 사용할 수 있어요.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"단계-3 PyTorch 설치하기\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"(선택 사항) — 프로젝트용 새로운 conda 환경을 생성하세요. \", _jsx(_components.code, {\n        children: \"environment-name\"\n      }), \"을 원하는 이름으로 바꿀 수 있어요. 일반적으로 제 프로젝트 이름으로 지정해요. 💡 프로젝트 작업 전후에 conda activate \", _jsx(_components.code, {\n        children: \"environment-name\"\n      }), \" 및 conda deactivate \", _jsx(_components.code, {\n        children: \"environment-name\"\n      }), \" 명령어를 사용할 수 있어요.\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"conda create -n \u003cenvironment-name\u003e python=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3.11\"\n        }), \"\\n\\n# 환경 활성화\\nconda activate \u003cenvironment-name\u003e\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"CUDA 버전에 맞게 PyTorch 라이브러리를 설치하세요. 아래 명령어는 우리가 cuda-12.1에 설치한 것입니다:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"pip3 install torch torchvision torchaudio\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위 명령어는 PyTorch 설치 가이드 — 로컬에서 시작 | PyTorch 에서 확인할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024_3.png\",\n        alt: \"PyTorch Installation\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"PyTorch을 설치한 후 터미널에서 PyTorch에서 볼 수 있는 GPU의 수를 확인하세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [\"python\\n\\n\u003e\u003e \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" torch\\n\u003e\u003e \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"print\"\n        }), \"(torch.cuda.device_count())\\n\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 명령은 시스템에 설치된 GPU의 수를 출력해야 합니다 (내 경우엔 8개), 그리고 nvidia-smi 명령에 나열된 GPU 수와 일치해야 합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"바로 시작해도 괜찮아요! 여러 개의 GPU를 활용하는 딥 러닝 프로젝트에 대해 작업할 준비가 되었어요 🥳.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"다음 단계는? Multi-GPU 설정을 활용한 딥 러닝 프로젝트로 시작해보세요 (LLMs)\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"🤗 시작하려면 Hugging Face에서 인기 있는 모델을 복제해보세요:\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"💬 추론에 대한 설치 (LLM 모델 사용)를 위해 exllamav2를 별도의 환경에 복제하고 설치하세요. 이렇게 하면 모든 GPU가 사용되어 더 빠른 추론이 가능합니다: (자세한 튜토리얼은 제 미디엄 페이지를 확인하세요)\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"👨‍🏫 파인 튜닝이나 훈련을 위해 torchtune을 복제하고 설치할 수 있습니다. 지침을 따라 모델을 전체 미세 조정하거나 Lora 미세 조정할 수 있습니다. GPU를 최대한 활용하세요: (자세한 튜토리얼은 내 미디엄 페이지를 확인해주세요)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 안내서는 다중 GPU 딥 러닝에 필요한 기계 설정을 안내해줍니다. 이제 torchtune과 같은 다중 GPU를 활용하는 모든 프로젝트에 대해 작업을 시작할 수 있습니다. 빠른 개발을 위한 torchtune과 exllamaV2의 자세한 튜토리얼을 기대해주세요.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-20-HowtoSetupaMulti-GPULinuxMachineforDeepLearningin2024"},"buildId":"ll1cGyplNwh83dpggeai1","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>