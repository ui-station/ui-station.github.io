<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR" data-gatsby-head="true"/><meta name="twitter:title" content="어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 13:03" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_buildManifest.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>전 eBPF에 상당히 오랫동안 관여해 왔습니다. 우리 팀장이 Azure CNI에서 Cilium CNI로 모든 클러스터를 라이브 이전하는 것을 제안했을 때, 기회에 바로 뛰어들었어요. 이 일은 내가 지금까지 맡았던 가장 힘든 일 중 하나였지만, 그 시간 동안 즐거웠어요.</p>
<p>하지만, 그 이야기는 다음에 하기로 해요. 이 기사의 목표는 제 개인적인 k8s.af 이야기 중 하나를 해설하여, 머리카락을 몇 일 절약할 수 있는 누군가를 돕는 것입니다.</p>
<p><img src="/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png" alt="이미지"></p>
<h2>왜 Cilium을 선택했는가?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Cilium을 선택한 우리의 주요 목표는 다음과 같습니다:</p>
<ul>
<li>더 나은 네트워크 격리: 일부 클러스터에서는 고객의 작업 부하를 실행하고 있기 때문에 출발 트래픽을 효과적으로 공유하고 제어해야 했습니다.</li>
<li>WireGuard를 사용한 투명한 암호화: 공유 클러스터에서는 제로 트러스트 접근 방식을 채택하고자 했습니다.</li>
<li>관찰 가능성: Cilium은 다양한 관찰 기능을 갖추고 있어 추가 계측없이 Kubernetes 작업 부하를 모니터링할 수 있습니다.</li>
<li>서비스 메시 기능: Cilium은 사이드카를 필요로하지 않고 다시 시도 및 회로 차단과 같은 서비스 메쉬 기능을 제공할 수 있습니다.</li>
<li>효율적이고 가벼운 네트워크 스택: 하드웨어 비용을 낮추면서 더 나은 성능을 원하신다면 저희를 선택해 주세요!</li>
<li>클러스터 매시 망: 우리는 인프라를 미래에 대비하기 위해 준비하고 싶었습니다.</li>
</ul>
<p>이주 후 모든 것이 원활했습니다 (대부분...). Cilium을 기반으로 개발한 기능을 출시하기 시작했고 고객들로부터 좋은 피드백을 받았습니다.</p>
<p>그리고 모든게 좋았는데...</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>사건</h1>
<p>일반적인 월요일 아침이었고, 매일 릴리스 주기의 일환으로 SRE 팀은 코어 서비스의 최신 업데이트를 스테이징 환경으로 프로모션했습니다.</p>
<p>그러나 프로모션 파이프라인이 완료되자마자 우리의 업타임 모니터링 솔루션이 스테이징 환경에 접근할 수 없다는 이벤트를 트리거하면서 발생했습니다. SRE 팀은 즉시 문제의 근본 원인을 조사하기 시작했습니다.</p>
<p>더 깊이 파고들기 전에, 빠른 다이어그램을 사용하여 우리의 네트워크 아키텍처(간소화된 버전)를 설명해 드리겠습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_1.png">
<p>두 개의 클러스터가 있습니다. 하나는 외부에 공개되어 있고, 다른 하나는 비공개입니다. 외부 클러스터는 방화벽을 통해 노출되어 있습니다. 비공개 클러스터의 일부 서비스는 로드 밸런서를 통해 외부 클러스터와 통신합니다.</p>
<p>초기 디버깅 후, SRE 팀은 문제가 방화벽과 클러스터 1의 인그레스 서비스 간의 연결에 있는 것으로 결론 내렸습니다. 클러스터 1 내의 모든 서비스가 실행되고 클러스터 1의 로드 밸런서를 향해 요청을 보내고 있기 때문에 클러스터 2의 pod들이 작동 중이었습니다.</p>
<p>Wireshark와 Hubble을 통해, 방화벽에서 전송된 "SYN" 패킷이 서비스에 도달했지만 서비스로부터 해당하는 "ACK" 패킷이 전송되지 않았음을 확인했습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>문제를 해결하기 위한 몇 차례의 무산된 시도 끝에 SRE 팀은 프로덕션으로의 릴리스를 승인했습니다. 중요한 수정 사항을 가능한 빨리 릴리즈해야 했기 때문입니다. 이 결정의 근거는 애플리케이션 수준의 변경이 인프라를 손상시킬 수 없으며, 이 일은 격리된 사건이었습니다.</p>
<p>그러나 릴리스가 프로덕션으로 승급되자마자 프로덕션 로드 밸런서도 응답을 중단했습니다. SRE 팀은 신속하게 변경 사항을 롤백하여 프로덕션에서 문제를 해결했습니다. 놀랍게도 스테이징 클러스터에서 변경 사항을 롤백해도 문제가 해결되지 않았습니다.</p>
<h2>재앙이 계속됩니다</h2>
<p>스테이징에서의 문제가 여전히 해결되지 않아 전문 네트워크 전문가와 함께 전투실에 호출되었습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>지금쯤 SRE 팀은 광범위한 실험을 통해 많은 데이터를 수집했습니다. 우리 VPC 내의 다른 서브넷에 VM을 배포하는 실험을 해보았는데, 로컬 노드 풀 서브넷부터 다른 클러스터에 속한 원격 서브넷, 로드 밸런서 서브넷까지 다양한 곳에 시도해봤습니다. 모든 것이 예상대로 작동되었는데, 방화벽을 통해 통과하는 트래픽에서 문제가 발생했습니다.</p>
<p>제가 노력에 합류하면서, 그들이 수집한 모든 데이터를 철저히 검토했고, 여러 종류의 워크로드를 배포하고 Hubble과 Wireshark 로그를 분석하며 더 많은 테스트를 진행했습니다. 루트 원인을 밝힐 수 있는 단서나 누락된 부분을 찾기 위해 노력했습니다.</p>
<p>Azure 네트워크 엔지니어가 합류하면서, SRE 팀은 그들이 지금까지 한 모든 단계에 대해 설명했습니다. 수집한 데이터를 분석한 후, 엔지니어는 방화벽 서브넷 내에 VM을 배포하고 문제가 있는 Kubernetes 클러스터로 TCP 연결을 시도하라는 제안을 했습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>저희 SRE 팀은 방화벽 서브넷 내에 임시 VM을 빠르게 설정하고 로드 밸런서 IP로 telnet을 시도해 보았어요. 이 테스트 중에 우리가 직면한 동일한 문제를 관찰했는데, telnet 연결이 초기화되지 않았어요. 그래서 그들은 방화벽 서브넷과 로드 밸런서 서브넷 또는 노드 풀 서브넷 간 네트워크 피어링에 문제가 있는지 조사하기로 결정했어요.</p>
<p>호기심에 저는 다른 SRE 멤버에게 서버에 다시 ping을 시도하도록 요청하고 Hubble 로그를 모니터링했어요. 놀랍게도, telnet이 시작된 순간에 SYN 패킷이 수신되었지만 "ACK"은 전달되지 않았어요.</p>
<p>이 행동은 이상하게 보였기 때문에 임시 VM에서 포트를 열도록 요청하고 쿠버네티스 클러스터에서 해당 VM으로 ping을 시도해 보았어요. 하지만 응답이 없었고, 임시 VM에 Wireshark를 설치한 후에도 들어오는 SYN 패킷을 볼 수 없었어요. 흥미로운 점은 방화벽 서브넷을 제외한 다른 목적지로 ping을 시도하면 문제가 없었다는 점이었어요.</p>
<p>의아해하며, 그들에게 노드 풀 서브넷에 생성한 임시 VM을 사용하여 동일한 테스트를 수행해 보라고 요청했더니, 드디어 작동했어요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>루트 원인</h1>
<p>이러한 동작이 이상하다고 생각해, 주요 사고 대응 팀 구성원들의 작업을 방해하고 있는 피어링을 점검 중이었던 멤버들에게 얘기했어요. 문제가 인그레스가 아니라 이그레스에 문제가 있을 수 있다는 것을 알려줬죠. 이로써 AKS-관리 노드 내의 라우팅 테이블 문제일 수도 있다는 느낌을 받았어요.</p>
<p>그래서 저희 SRE 팀은 쿠버네티스 클러스터에서 노드 중 하나로 SSH를 통해 연결하고 <code>ip route</code> 명령을 실행했어요. 이 명령을 실행하면 Cilium이 교차 노드 통신을 가능하게하기 위해 추가한 몇 가지 라우팅 규칙이 표시됐어요.</p>
<p><img src="/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_3.png" alt="Image"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>Cilium 노드 라우팅</h2>
<p>고수준 개요에서 Cilium을 클러스터 범위 IPAM 모드에서 실행할 때, Cilium에게 가상 IP를 파드에 할당하도록 지시하기 위해 CIDR 범위를 제공해야 합니다. 기본적으로 이 CIDR 범위는 10.0.0.0/8입니다.</p>
<p>새 노드가 클러스터에 가입하면, Cilium은 해당 노드에게 주어진 CIDR 블록에서 고유한 서브넷을 할당합니다. 노드의 모든 파드는 이 할당된 서브넷 범위에서 IP 주소를 받습니다.</p>
<p>예를 들어, CIDR 범위인 10.0.0.0/8을 사용한다면:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>노드 A는 서브넷 10.1.0.0/16을 받을 수 있습니다.</li>
<li>노드 B는 서브넷 10.4.0.0/16을 받을 수 있습니다.</li>
</ul>
<p>노드 간 통신을 원활하게 하기 위해 Cilium은 IP 경로를 설정하여 트래픽이 노드 간에 올바르게 전달되도록 합니다. 노드 A에 있는 IP가 10.1.5.13인 팟이 노드 B에 있는 IP가 10.4.63.38인 팟과 통신하려고 할 때, 데이터 패킷은 노드 A의 네트워크 인터페이스로 전송됩니다. 그 후 IP 라우팅 테이블을 기반으로 패킷은 노드 B로 라우팅되며, 이는 노드 B가 10.4.0.0/16 서브넷을 소유하기 때문입니다.</p>
<h2>잠자는 용</h2>
<p>보통은 정상적으로 작동합니다. 그러나 유감스럽게도, 노드에 할당된 서브넷 중 하나가 방화벽의 서브넷 범위와 겹치는 문제가 발생했습니다. 이로 인해 SYN 패킷이 팟에 성공적으로 도달하지만, 팟이 응답을 시도할 때 요청이 노드의 네트워크 인터페이스로 전달되는 상황이 발생했습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그 때, 노드의 IP 라우팅 규칙 때문에 패킷이 다른 노드로 라우팅되었습니다. 이는 방화벽의 VM IP 주소가 두 번째 노드의 서브넷 범위 내에 속했기 때문에 발생했습니다. 그러나 두 번째 노드에는 방화벽의 VM과 정확히 일치하는 IP 주소를 가진 pod가 없었기 때문에 패킷이 소멸하여 사라졌습니다.</p>
<p>이 가설을 확인하기 위해 SRE 팀은 충돌하는 노드에 대해 <code>kubectl delete node</code>을 실행했고, 그 노드가 제거되자마자 방화벽을 통한 외부 연결이 다시 작동하기 시작했습니다.</p>
<p>하지만 왜 Cilium을 배포한 후 거의 8개월이 지난 후에 이 문제가 발생했을까요? 이 모든 것은 자동 스케일링으로 귀결되었습니다. 관찰한 바에 따르면, 특정 CIDR 범위가 노드에 할당되면 해당 노드가 제거되더라도 재사용되지 않았습니다. 따라서 Cilium 운영자는 우리가 할당한 대규모 CIDR 범위를 하나씩 소비하면서 점진적으로 이동하고 있었고, 마침내 방화벽의 CIDR 범위에 다다랐습니다.</p>
<p>그 판명날인 첫 월요일 아침, SRE 팀이 개발 환경에서 스테이징으로 릴리스를 승격시키자마자 노드 스케일업을 트리거하여 충돌하는 CIDR 범위가 있는 노드가 생성되었고, 이로 인해 전체 통신 경로가 다운되었습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>문제 수정</h1>
<p>스테이징에서 발생한 이슈를 해결하기 위해, 우리는 clusterPoolIPv4PodCIDRList를 기존 내부 서브넷과 충돌하지 않는 CIDR 범위로 업데이트해 보았습니다. 헬름 업그레이드를 실행한 후에도 아무 변화가 없었습니다. 그래서 노드 스케일 업을 트리거하고 다행히도 - 새로 생성된 노드가 새로운 CIDR 범위의 서브넷으로 생성되었습니다.</p>
<p>저는 두 개의 DaemonSet을 사용하여 교차 노드 통신을 테스트하기 위해 만든 빠른 워크로드를 실행하여 두 개의 CIDR 범위가 아무 문제없이 작동하는 것을 확인했습니다. 그 후, SRE 팀은 기존 노드를 안전하게 비우고 제거하고 나쁜 CIDR 범위를 가진 모든 노드가 완전히 제거될 때까지 새로운 노드 풀을 확장하는 스크립트를 신속하게 작성했습니다. 그 스크립트를 실행하여 스테이징 클러스터를 완전히 복구했습니다.</p>
<p>추가 테스트를 실행한 후에 우리의 프로덕션 클러스터도 동일한 문제를 겪었기 때문에, Cilium 문서에서 반대하고 있던 것에도 불구하고 clusterPoolIPv4PodCIDRList를 업데이트하여 문제를 영구적으로 해결하기로 결정했습니다. 이에 이해 관계자들로부터 동의를 받고 마이그레이션을 실행했습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>결론</h1>
<p>다양한 테스트를 거친 후에도, 거의 2000개의 구성 가능한 값이 있는 Cilium과 같은 복잡한 시스템은 여전히 잘못된 구성을 빠뜨릴 수 있어 예상치 못한 실패로 이어질 수 있습니다.</p>
<p>이 사건을 통해, 네트워크 문제를 체계적으로 해결하고 클라우드 추상화에 의해 제공된 낮은 수준의 네트워킹 인프라와 기술을 이해하는 중요성을 깨달았습니다. 이 문제에 대해 협업한 후, 매우 어려운 경험이었지만 소중한 학습 기회를 제공했다는 것에 대해 모두 동의했습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"어렵게 배운 교훈 Cilium의 기본 Pod CIDR을 사용하지 마세요","description":"","date":"2024-06-19 13:03","slug":"2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR","content":"\n전 eBPF에 상당히 오랫동안 관여해 왔습니다. 우리 팀장이 Azure CNI에서 Cilium CNI로 모든 클러스터를 라이브 이전하는 것을 제안했을 때, 기회에 바로 뛰어들었어요. 이 일은 내가 지금까지 맡았던 가장 힘든 일 중 하나였지만, 그 시간 동안 즐거웠어요.\n\n하지만, 그 이야기는 다음에 하기로 해요. 이 기사의 목표는 제 개인적인 k8s.af 이야기 중 하나를 해설하여, 머리카락을 몇 일 절약할 수 있는 누군가를 돕는 것입니다.\n\n![이미지](/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png)\n\n## 왜 Cilium을 선택했는가?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nCilium을 선택한 우리의 주요 목표는 다음과 같습니다:\n\n- 더 나은 네트워크 격리: 일부 클러스터에서는 고객의 작업 부하를 실행하고 있기 때문에 출발 트래픽을 효과적으로 공유하고 제어해야 했습니다.\n- WireGuard를 사용한 투명한 암호화: 공유 클러스터에서는 제로 트러스트 접근 방식을 채택하고자 했습니다.\n- 관찰 가능성: Cilium은 다양한 관찰 기능을 갖추고 있어 추가 계측없이 Kubernetes 작업 부하를 모니터링할 수 있습니다.\n- 서비스 메시 기능: Cilium은 사이드카를 필요로하지 않고 다시 시도 및 회로 차단과 같은 서비스 메쉬 기능을 제공할 수 있습니다.\n- 효율적이고 가벼운 네트워크 스택: 하드웨어 비용을 낮추면서 더 나은 성능을 원하신다면 저희를 선택해 주세요!\n- 클러스터 매시 망: 우리는 인프라를 미래에 대비하기 위해 준비하고 싶었습니다.\n\n이주 후 모든 것이 원활했습니다 (대부분...). Cilium을 기반으로 개발한 기능을 출시하기 시작했고 고객들로부터 좋은 피드백을 받았습니다.\n\n그리고 모든게 좋았는데...\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 사건\n\n일반적인 월요일 아침이었고, 매일 릴리스 주기의 일환으로 SRE 팀은 코어 서비스의 최신 업데이트를 스테이징 환경으로 프로모션했습니다.\n\n그러나 프로모션 파이프라인이 완료되자마자 우리의 업타임 모니터링 솔루션이 스테이징 환경에 접근할 수 없다는 이벤트를 트리거하면서 발생했습니다. SRE 팀은 즉시 문제의 근본 원인을 조사하기 시작했습니다.\n\n더 깊이 파고들기 전에, 빠른 다이어그램을 사용하여 우리의 네트워크 아키텍처(간소화된 버전)를 설명해 드리겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_1.png\" /\u003e\n\n두 개의 클러스터가 있습니다. 하나는 외부에 공개되어 있고, 다른 하나는 비공개입니다. 외부 클러스터는 방화벽을 통해 노출되어 있습니다. 비공개 클러스터의 일부 서비스는 로드 밸런서를 통해 외부 클러스터와 통신합니다.\n\n초기 디버깅 후, SRE 팀은 문제가 방화벽과 클러스터 1의 인그레스 서비스 간의 연결에 있는 것으로 결론 내렸습니다. 클러스터 1 내의 모든 서비스가 실행되고 클러스터 1의 로드 밸런서를 향해 요청을 보내고 있기 때문에 클러스터 2의 pod들이 작동 중이었습니다.\n\nWireshark와 Hubble을 통해, 방화벽에서 전송된 \"SYN\" 패킷이 서비스에 도달했지만 서비스로부터 해당하는 \"ACK\" 패킷이 전송되지 않았음을 확인했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n문제를 해결하기 위한 몇 차례의 무산된 시도 끝에 SRE 팀은 프로덕션으로의 릴리스를 승인했습니다. 중요한 수정 사항을 가능한 빨리 릴리즈해야 했기 때문입니다. 이 결정의 근거는 애플리케이션 수준의 변경이 인프라를 손상시킬 수 없으며, 이 일은 격리된 사건이었습니다.\n\n그러나 릴리스가 프로덕션으로 승급되자마자 프로덕션 로드 밸런서도 응답을 중단했습니다. SRE 팀은 신속하게 변경 사항을 롤백하여 프로덕션에서 문제를 해결했습니다. 놀랍게도 스테이징 클러스터에서 변경 사항을 롤백해도 문제가 해결되지 않았습니다.\n\n## 재앙이 계속됩니다\n\n스테이징에서의 문제가 여전히 해결되지 않아 전문 네트워크 전문가와 함께 전투실에 호출되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금쯤 SRE 팀은 광범위한 실험을 통해 많은 데이터를 수집했습니다. 우리 VPC 내의 다른 서브넷에 VM을 배포하는 실험을 해보았는데, 로컬 노드 풀 서브넷부터 다른 클러스터에 속한 원격 서브넷, 로드 밸런서 서브넷까지 다양한 곳에 시도해봤습니다. 모든 것이 예상대로 작동되었는데, 방화벽을 통해 통과하는 트래픽에서 문제가 발생했습니다.\n\n제가 노력에 합류하면서, 그들이 수집한 모든 데이터를 철저히 검토했고, 여러 종류의 워크로드를 배포하고 Hubble과 Wireshark 로그를 분석하며 더 많은 테스트를 진행했습니다. 루트 원인을 밝힐 수 있는 단서나 누락된 부분을 찾기 위해 노력했습니다.\n\nAzure 네트워크 엔지니어가 합류하면서, SRE 팀은 그들이 지금까지 한 모든 단계에 대해 설명했습니다. 수집한 데이터를 분석한 후, 엔지니어는 방화벽 서브넷 내에 VM을 배포하고 문제가 있는 Kubernetes 클러스터로 TCP 연결을 시도하라는 제안을 했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저희 SRE 팀은 방화벽 서브넷 내에 임시 VM을 빠르게 설정하고 로드 밸런서 IP로 telnet을 시도해 보았어요. 이 테스트 중에 우리가 직면한 동일한 문제를 관찰했는데, telnet 연결이 초기화되지 않았어요. 그래서 그들은 방화벽 서브넷과 로드 밸런서 서브넷 또는 노드 풀 서브넷 간 네트워크 피어링에 문제가 있는지 조사하기로 결정했어요.\n\n호기심에 저는 다른 SRE 멤버에게 서버에 다시 ping을 시도하도록 요청하고 Hubble 로그를 모니터링했어요. 놀랍게도, telnet이 시작된 순간에 SYN 패킷이 수신되었지만 \"ACK\"은 전달되지 않았어요.\n\n이 행동은 이상하게 보였기 때문에 임시 VM에서 포트를 열도록 요청하고 쿠버네티스 클러스터에서 해당 VM으로 ping을 시도해 보았어요. 하지만 응답이 없었고, 임시 VM에 Wireshark를 설치한 후에도 들어오는 SYN 패킷을 볼 수 없었어요. 흥미로운 점은 방화벽 서브넷을 제외한 다른 목적지로 ping을 시도하면 문제가 없었다는 점이었어요.\n\n의아해하며, 그들에게 노드 풀 서브넷에 생성한 임시 VM을 사용하여 동일한 테스트를 수행해 보라고 요청했더니, 드디어 작동했어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 루트 원인\n\n이러한 동작이 이상하다고 생각해, 주요 사고 대응 팀 구성원들의 작업을 방해하고 있는 피어링을 점검 중이었던 멤버들에게 얘기했어요. 문제가 인그레스가 아니라 이그레스에 문제가 있을 수 있다는 것을 알려줬죠. 이로써 AKS-관리 노드 내의 라우팅 테이블 문제일 수도 있다는 느낌을 받았어요.\n\n그래서 저희 SRE 팀은 쿠버네티스 클러스터에서 노드 중 하나로 SSH를 통해 연결하고 `ip route` 명령을 실행했어요. 이 명령을 실행하면 Cilium이 교차 노드 통신을 가능하게하기 위해 추가한 몇 가지 라우팅 규칙이 표시됐어요.\n\n![Image](/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Cilium 노드 라우팅\n\n고수준 개요에서 Cilium을 클러스터 범위 IPAM 모드에서 실행할 때, Cilium에게 가상 IP를 파드에 할당하도록 지시하기 위해 CIDR 범위를 제공해야 합니다. 기본적으로 이 CIDR 범위는 10.0.0.0/8입니다.\n\n새 노드가 클러스터에 가입하면, Cilium은 해당 노드에게 주어진 CIDR 블록에서 고유한 서브넷을 할당합니다. 노드의 모든 파드는 이 할당된 서브넷 범위에서 IP 주소를 받습니다.\n\n예를 들어, CIDR 범위인 10.0.0.0/8을 사용한다면:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 노드 A는 서브넷 10.1.0.0/16을 받을 수 있습니다.\n- 노드 B는 서브넷 10.4.0.0/16을 받을 수 있습니다.\n\n노드 간 통신을 원활하게 하기 위해 Cilium은 IP 경로를 설정하여 트래픽이 노드 간에 올바르게 전달되도록 합니다. 노드 A에 있는 IP가 10.1.5.13인 팟이 노드 B에 있는 IP가 10.4.63.38인 팟과 통신하려고 할 때, 데이터 패킷은 노드 A의 네트워크 인터페이스로 전송됩니다. 그 후 IP 라우팅 테이블을 기반으로 패킷은 노드 B로 라우팅되며, 이는 노드 B가 10.4.0.0/16 서브넷을 소유하기 때문입니다.\n\n## 잠자는 용\n\n보통은 정상적으로 작동합니다. 그러나 유감스럽게도, 노드에 할당된 서브넷 중 하나가 방화벽의 서브넷 범위와 겹치는 문제가 발생했습니다. 이로 인해 SYN 패킷이 팟에 성공적으로 도달하지만, 팟이 응답을 시도할 때 요청이 노드의 네트워크 인터페이스로 전달되는 상황이 발생했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그 때, 노드의 IP 라우팅 규칙 때문에 패킷이 다른 노드로 라우팅되었습니다. 이는 방화벽의 VM IP 주소가 두 번째 노드의 서브넷 범위 내에 속했기 때문에 발생했습니다. 그러나 두 번째 노드에는 방화벽의 VM과 정확히 일치하는 IP 주소를 가진 pod가 없었기 때문에 패킷이 소멸하여 사라졌습니다.\n\n이 가설을 확인하기 위해 SRE 팀은 충돌하는 노드에 대해 `kubectl delete node`을 실행했고, 그 노드가 제거되자마자 방화벽을 통한 외부 연결이 다시 작동하기 시작했습니다.\n\n하지만 왜 Cilium을 배포한 후 거의 8개월이 지난 후에 이 문제가 발생했을까요? 이 모든 것은 자동 스케일링으로 귀결되었습니다. 관찰한 바에 따르면, 특정 CIDR 범위가 노드에 할당되면 해당 노드가 제거되더라도 재사용되지 않았습니다. 따라서 Cilium 운영자는 우리가 할당한 대규모 CIDR 범위를 하나씩 소비하면서 점진적으로 이동하고 있었고, 마침내 방화벽의 CIDR 범위에 다다랐습니다.\n\n그 판명날인 첫 월요일 아침, SRE 팀이 개발 환경에서 스테이징으로 릴리스를 승격시키자마자 노드 스케일업을 트리거하여 충돌하는 CIDR 범위가 있는 노드가 생성되었고, 이로 인해 전체 통신 경로가 다운되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 문제 수정\n\n스테이징에서 발생한 이슈를 해결하기 위해, 우리는 clusterPoolIPv4PodCIDRList를 기존 내부 서브넷과 충돌하지 않는 CIDR 범위로 업데이트해 보았습니다. 헬름 업그레이드를 실행한 후에도 아무 변화가 없었습니다. 그래서 노드 스케일 업을 트리거하고 다행히도 - 새로 생성된 노드가 새로운 CIDR 범위의 서브넷으로 생성되었습니다.\n\n저는 두 개의 DaemonSet을 사용하여 교차 노드 통신을 테스트하기 위해 만든 빠른 워크로드를 실행하여 두 개의 CIDR 범위가 아무 문제없이 작동하는 것을 확인했습니다. 그 후, SRE 팀은 기존 노드를 안전하게 비우고 제거하고 나쁜 CIDR 범위를 가진 모든 노드가 완전히 제거될 때까지 새로운 노드 풀을 확장하는 스크립트를 신속하게 작성했습니다. 그 스크립트를 실행하여 스테이징 클러스터를 완전히 복구했습니다.\n\n추가 테스트를 실행한 후에 우리의 프로덕션 클러스터도 동일한 문제를 겪었기 때문에, Cilium 문서에서 반대하고 있던 것에도 불구하고 clusterPoolIPv4PodCIDRList를 업데이트하여 문제를 영구적으로 해결하기로 결정했습니다. 이에 이해 관계자들로부터 동의를 받고 마이그레이션을 실행했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n다양한 테스트를 거친 후에도, 거의 2000개의 구성 가능한 값이 있는 Cilium과 같은 복잡한 시스템은 여전히 잘못된 구성을 빠뜨릴 수 있어 예상치 못한 실패로 이어질 수 있습니다.\n\n이 사건을 통해, 네트워크 문제를 체계적으로 해결하고 클라우드 추상화에 의해 제공된 낮은 수준의 네트워킹 인프라와 기술을 이해하는 중요성을 깨달았습니다. 이 문제에 대해 협업한 후, 매우 어려운 경험이었지만 소중한 학습 기회를 제공했다는 것에 대해 모두 동의했습니다.\n","ogImage":{"url":"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png"},"coverImage":"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e전 eBPF에 상당히 오랫동안 관여해 왔습니다. 우리 팀장이 Azure CNI에서 Cilium CNI로 모든 클러스터를 라이브 이전하는 것을 제안했을 때, 기회에 바로 뛰어들었어요. 이 일은 내가 지금까지 맡았던 가장 힘든 일 중 하나였지만, 그 시간 동안 즐거웠어요.\u003c/p\u003e\n\u003cp\u003e하지만, 그 이야기는 다음에 하기로 해요. 이 기사의 목표는 제 개인적인 k8s.af 이야기 중 하나를 해설하여, 머리카락을 몇 일 절약할 수 있는 누군가를 돕는 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e왜 Cilium을 선택했는가?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eCilium을 선택한 우리의 주요 목표는 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e더 나은 네트워크 격리: 일부 클러스터에서는 고객의 작업 부하를 실행하고 있기 때문에 출발 트래픽을 효과적으로 공유하고 제어해야 했습니다.\u003c/li\u003e\n\u003cli\u003eWireGuard를 사용한 투명한 암호화: 공유 클러스터에서는 제로 트러스트 접근 방식을 채택하고자 했습니다.\u003c/li\u003e\n\u003cli\u003e관찰 가능성: Cilium은 다양한 관찰 기능을 갖추고 있어 추가 계측없이 Kubernetes 작업 부하를 모니터링할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e서비스 메시 기능: Cilium은 사이드카를 필요로하지 않고 다시 시도 및 회로 차단과 같은 서비스 메쉬 기능을 제공할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e효율적이고 가벼운 네트워크 스택: 하드웨어 비용을 낮추면서 더 나은 성능을 원하신다면 저희를 선택해 주세요!\u003c/li\u003e\n\u003cli\u003e클러스터 매시 망: 우리는 인프라를 미래에 대비하기 위해 준비하고 싶었습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이주 후 모든 것이 원활했습니다 (대부분...). Cilium을 기반으로 개발한 기능을 출시하기 시작했고 고객들로부터 좋은 피드백을 받았습니다.\u003c/p\u003e\n\u003cp\u003e그리고 모든게 좋았는데...\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e사건\u003c/h1\u003e\n\u003cp\u003e일반적인 월요일 아침이었고, 매일 릴리스 주기의 일환으로 SRE 팀은 코어 서비스의 최신 업데이트를 스테이징 환경으로 프로모션했습니다.\u003c/p\u003e\n\u003cp\u003e그러나 프로모션 파이프라인이 완료되자마자 우리의 업타임 모니터링 솔루션이 스테이징 환경에 접근할 수 없다는 이벤트를 트리거하면서 발생했습니다. SRE 팀은 즉시 문제의 근본 원인을 조사하기 시작했습니다.\u003c/p\u003e\n\u003cp\u003e더 깊이 파고들기 전에, 빠른 다이어그램을 사용하여 우리의 네트워크 아키텍처(간소화된 버전)를 설명해 드리겠습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_1.png\"\u003e\n\u003cp\u003e두 개의 클러스터가 있습니다. 하나는 외부에 공개되어 있고, 다른 하나는 비공개입니다. 외부 클러스터는 방화벽을 통해 노출되어 있습니다. 비공개 클러스터의 일부 서비스는 로드 밸런서를 통해 외부 클러스터와 통신합니다.\u003c/p\u003e\n\u003cp\u003e초기 디버깅 후, SRE 팀은 문제가 방화벽과 클러스터 1의 인그레스 서비스 간의 연결에 있는 것으로 결론 내렸습니다. 클러스터 1 내의 모든 서비스가 실행되고 클러스터 1의 로드 밸런서를 향해 요청을 보내고 있기 때문에 클러스터 2의 pod들이 작동 중이었습니다.\u003c/p\u003e\n\u003cp\u003eWireshark와 Hubble을 통해, 방화벽에서 전송된 \"SYN\" 패킷이 서비스에 도달했지만 서비스로부터 해당하는 \"ACK\" 패킷이 전송되지 않았음을 확인했습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e문제를 해결하기 위한 몇 차례의 무산된 시도 끝에 SRE 팀은 프로덕션으로의 릴리스를 승인했습니다. 중요한 수정 사항을 가능한 빨리 릴리즈해야 했기 때문입니다. 이 결정의 근거는 애플리케이션 수준의 변경이 인프라를 손상시킬 수 없으며, 이 일은 격리된 사건이었습니다.\u003c/p\u003e\n\u003cp\u003e그러나 릴리스가 프로덕션으로 승급되자마자 프로덕션 로드 밸런서도 응답을 중단했습니다. SRE 팀은 신속하게 변경 사항을 롤백하여 프로덕션에서 문제를 해결했습니다. 놀랍게도 스테이징 클러스터에서 변경 사항을 롤백해도 문제가 해결되지 않았습니다.\u003c/p\u003e\n\u003ch2\u003e재앙이 계속됩니다\u003c/h2\u003e\n\u003cp\u003e스테이징에서의 문제가 여전히 해결되지 않아 전문 네트워크 전문가와 함께 전투실에 호출되었습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e지금쯤 SRE 팀은 광범위한 실험을 통해 많은 데이터를 수집했습니다. 우리 VPC 내의 다른 서브넷에 VM을 배포하는 실험을 해보았는데, 로컬 노드 풀 서브넷부터 다른 클러스터에 속한 원격 서브넷, 로드 밸런서 서브넷까지 다양한 곳에 시도해봤습니다. 모든 것이 예상대로 작동되었는데, 방화벽을 통해 통과하는 트래픽에서 문제가 발생했습니다.\u003c/p\u003e\n\u003cp\u003e제가 노력에 합류하면서, 그들이 수집한 모든 데이터를 철저히 검토했고, 여러 종류의 워크로드를 배포하고 Hubble과 Wireshark 로그를 분석하며 더 많은 테스트를 진행했습니다. 루트 원인을 밝힐 수 있는 단서나 누락된 부분을 찾기 위해 노력했습니다.\u003c/p\u003e\n\u003cp\u003eAzure 네트워크 엔지니어가 합류하면서, SRE 팀은 그들이 지금까지 한 모든 단계에 대해 설명했습니다. 수집한 데이터를 분석한 후, 엔지니어는 방화벽 서브넷 내에 VM을 배포하고 문제가 있는 Kubernetes 클러스터로 TCP 연결을 시도하라는 제안을 했습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e저희 SRE 팀은 방화벽 서브넷 내에 임시 VM을 빠르게 설정하고 로드 밸런서 IP로 telnet을 시도해 보았어요. 이 테스트 중에 우리가 직면한 동일한 문제를 관찰했는데, telnet 연결이 초기화되지 않았어요. 그래서 그들은 방화벽 서브넷과 로드 밸런서 서브넷 또는 노드 풀 서브넷 간 네트워크 피어링에 문제가 있는지 조사하기로 결정했어요.\u003c/p\u003e\n\u003cp\u003e호기심에 저는 다른 SRE 멤버에게 서버에 다시 ping을 시도하도록 요청하고 Hubble 로그를 모니터링했어요. 놀랍게도, telnet이 시작된 순간에 SYN 패킷이 수신되었지만 \"ACK\"은 전달되지 않았어요.\u003c/p\u003e\n\u003cp\u003e이 행동은 이상하게 보였기 때문에 임시 VM에서 포트를 열도록 요청하고 쿠버네티스 클러스터에서 해당 VM으로 ping을 시도해 보았어요. 하지만 응답이 없었고, 임시 VM에 Wireshark를 설치한 후에도 들어오는 SYN 패킷을 볼 수 없었어요. 흥미로운 점은 방화벽 서브넷을 제외한 다른 목적지로 ping을 시도하면 문제가 없었다는 점이었어요.\u003c/p\u003e\n\u003cp\u003e의아해하며, 그들에게 노드 풀 서브넷에 생성한 임시 VM을 사용하여 동일한 테스트를 수행해 보라고 요청했더니, 드디어 작동했어요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e루트 원인\u003c/h1\u003e\n\u003cp\u003e이러한 동작이 이상하다고 생각해, 주요 사고 대응 팀 구성원들의 작업을 방해하고 있는 피어링을 점검 중이었던 멤버들에게 얘기했어요. 문제가 인그레스가 아니라 이그레스에 문제가 있을 수 있다는 것을 알려줬죠. 이로써 AKS-관리 노드 내의 라우팅 테이블 문제일 수도 있다는 느낌을 받았어요.\u003c/p\u003e\n\u003cp\u003e그래서 저희 SRE 팀은 쿠버네티스 클러스터에서 노드 중 하나로 SSH를 통해 연결하고 \u003ccode\u003eip route\u003c/code\u003e 명령을 실행했어요. 이 명령을 실행하면 Cilium이 교차 노드 통신을 가능하게하기 위해 추가한 몇 가지 라우팅 규칙이 표시됐어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR_3.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003eCilium 노드 라우팅\u003c/h2\u003e\n\u003cp\u003e고수준 개요에서 Cilium을 클러스터 범위 IPAM 모드에서 실행할 때, Cilium에게 가상 IP를 파드에 할당하도록 지시하기 위해 CIDR 범위를 제공해야 합니다. 기본적으로 이 CIDR 범위는 10.0.0.0/8입니다.\u003c/p\u003e\n\u003cp\u003e새 노드가 클러스터에 가입하면, Cilium은 해당 노드에게 주어진 CIDR 블록에서 고유한 서브넷을 할당합니다. 노드의 모든 파드는 이 할당된 서브넷 범위에서 IP 주소를 받습니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, CIDR 범위인 10.0.0.0/8을 사용한다면:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e노드 A는 서브넷 10.1.0.0/16을 받을 수 있습니다.\u003c/li\u003e\n\u003cli\u003e노드 B는 서브넷 10.4.0.0/16을 받을 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e노드 간 통신을 원활하게 하기 위해 Cilium은 IP 경로를 설정하여 트래픽이 노드 간에 올바르게 전달되도록 합니다. 노드 A에 있는 IP가 10.1.5.13인 팟이 노드 B에 있는 IP가 10.4.63.38인 팟과 통신하려고 할 때, 데이터 패킷은 노드 A의 네트워크 인터페이스로 전송됩니다. 그 후 IP 라우팅 테이블을 기반으로 패킷은 노드 B로 라우팅되며, 이는 노드 B가 10.4.0.0/16 서브넷을 소유하기 때문입니다.\u003c/p\u003e\n\u003ch2\u003e잠자는 용\u003c/h2\u003e\n\u003cp\u003e보통은 정상적으로 작동합니다. 그러나 유감스럽게도, 노드에 할당된 서브넷 중 하나가 방화벽의 서브넷 범위와 겹치는 문제가 발생했습니다. 이로 인해 SYN 패킷이 팟에 성공적으로 도달하지만, 팟이 응답을 시도할 때 요청이 노드의 네트워크 인터페이스로 전달되는 상황이 발생했습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그 때, 노드의 IP 라우팅 규칙 때문에 패킷이 다른 노드로 라우팅되었습니다. 이는 방화벽의 VM IP 주소가 두 번째 노드의 서브넷 범위 내에 속했기 때문에 발생했습니다. 그러나 두 번째 노드에는 방화벽의 VM과 정확히 일치하는 IP 주소를 가진 pod가 없었기 때문에 패킷이 소멸하여 사라졌습니다.\u003c/p\u003e\n\u003cp\u003e이 가설을 확인하기 위해 SRE 팀은 충돌하는 노드에 대해 \u003ccode\u003ekubectl delete node\u003c/code\u003e을 실행했고, 그 노드가 제거되자마자 방화벽을 통한 외부 연결이 다시 작동하기 시작했습니다.\u003c/p\u003e\n\u003cp\u003e하지만 왜 Cilium을 배포한 후 거의 8개월이 지난 후에 이 문제가 발생했을까요? 이 모든 것은 자동 스케일링으로 귀결되었습니다. 관찰한 바에 따르면, 특정 CIDR 범위가 노드에 할당되면 해당 노드가 제거되더라도 재사용되지 않았습니다. 따라서 Cilium 운영자는 우리가 할당한 대규모 CIDR 범위를 하나씩 소비하면서 점진적으로 이동하고 있었고, 마침내 방화벽의 CIDR 범위에 다다랐습니다.\u003c/p\u003e\n\u003cp\u003e그 판명날인 첫 월요일 아침, SRE 팀이 개발 환경에서 스테이징으로 릴리스를 승격시키자마자 노드 스케일업을 트리거하여 충돌하는 CIDR 범위가 있는 노드가 생성되었고, 이로 인해 전체 통신 경로가 다운되었습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e문제 수정\u003c/h1\u003e\n\u003cp\u003e스테이징에서 발생한 이슈를 해결하기 위해, 우리는 clusterPoolIPv4PodCIDRList를 기존 내부 서브넷과 충돌하지 않는 CIDR 범위로 업데이트해 보았습니다. 헬름 업그레이드를 실행한 후에도 아무 변화가 없었습니다. 그래서 노드 스케일 업을 트리거하고 다행히도 - 새로 생성된 노드가 새로운 CIDR 범위의 서브넷으로 생성되었습니다.\u003c/p\u003e\n\u003cp\u003e저는 두 개의 DaemonSet을 사용하여 교차 노드 통신을 테스트하기 위해 만든 빠른 워크로드를 실행하여 두 개의 CIDR 범위가 아무 문제없이 작동하는 것을 확인했습니다. 그 후, SRE 팀은 기존 노드를 안전하게 비우고 제거하고 나쁜 CIDR 범위를 가진 모든 노드가 완전히 제거될 때까지 새로운 노드 풀을 확장하는 스크립트를 신속하게 작성했습니다. 그 스크립트를 실행하여 스테이징 클러스터를 완전히 복구했습니다.\u003c/p\u003e\n\u003cp\u003e추가 테스트를 실행한 후에 우리의 프로덕션 클러스터도 동일한 문제를 겪었기 때문에, Cilium 문서에서 반대하고 있던 것에도 불구하고 clusterPoolIPv4PodCIDRList를 업데이트하여 문제를 영구적으로 해결하기로 결정했습니다. 이에 이해 관계자들로부터 동의를 받고 마이그레이션을 실행했습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e다양한 테스트를 거친 후에도, 거의 2000개의 구성 가능한 값이 있는 Cilium과 같은 복잡한 시스템은 여전히 잘못된 구성을 빠뜨릴 수 있어 예상치 못한 실패로 이어질 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이 사건을 통해, 네트워크 문제를 체계적으로 해결하고 클라우드 추상화에 의해 제공된 낮은 수준의 네트워킹 인프라와 기술을 이해하는 중요성을 깨달았습니다. 이 문제에 대해 협업한 후, 매우 어려운 경험이었지만 소중한 학습 기회를 제공했다는 것에 대해 모두 동의했습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-LearneditthehardwayDontuseCiliumsdefaultPodCIDR"},"buildId":"-dPCbnM2yhdKNgXe92VJV","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>