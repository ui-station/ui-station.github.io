<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems" data-gatsby-head="true"/><meta name="twitter:title" content="로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-23 16:57" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/wfHLuDA3kTGBYfaM5IGXk/_buildManifest.js" defer=""></script><script src="/_next/static/wfHLuDA3kTGBYfaM5IGXk/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 23, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>데이터 엔지니어로서, 저는 몇 가지 생성적 AI 모델을 시험해 보고 로컬에서 모델을 설치/실행하는 것에 매혹을 느낍니다. Large Language Model (LLM)과 Vision-Language Model (VLM)은 가장 흥미로운 모델입니다. OpenAI는 ChatGPT 웹사이트와 모바일 앱을 제공합니다. Microsoft는 Windows 11 Copilot을 우리가 사용할 수 있도록 만들었습니다. 그러나 우리는 어떤 데이터가 인터넷으로 전송되고 그들의 데이터베이스에 저장되는지를 제어할 수 없습니다. 그들의 시스템은 오픈 소스가 아니며, 마치 신비로운 검은 상자와 같습니다.</p>
<p>일부 관대한 회사(Meta 및 Mistral AI 같은) 또는 개인들이 자신들의 모델을 오픈 소스로 공개하고, 적극적인 커뮤니티가 도구를 단계별로 구축하여 우리가 집 컴퓨터에서 LLM과 VLM을 쉽게 실행할 수 있도록 도와줍니다. Raspberry Pi 5와 8GB RAM은 이 기사에서 시험되었습니다 (Raspberry Pi에서 로컬 LLM 및 VLM 실행). 이는 신용 카드 크기의 소형 Single Board Computer (SBC)입니다. 나는 더 저렴한 컴퓨터 장비/솔루션이나 가상 머신을 찾아 성능을 시험해보고, 돈을 지불하는 대가에 좋은 가치를 제공하거나 심지어 일반 대중에게도 제공하도록 하고 싶습니다. 고려해야 할 사항은 텍스트 출력 속도, 텍스트 출력 품질 및 비용입니다.</p>
<p>생성된 내용을 평가하는 작업은 다른 연구 기관에서 수행됩니다. 예를 들어, 이 기사에서 mistral-7b가 llama2-13b보다 지식, 추론 및 이해력에서 능가한다고 언급됩니다. <a href="https://mistral.ai/news/announcing-mistral-7b/" rel="nofollow" target="_blank">https://mistral.ai/news/announcing-mistral-7b/</a>. 그래서 나는 LLM 테스트에 mistral과 llama2를 포함했습니다.</p>
<p>Ollama는 현재 macOS, Linux 및 Windows의 WSL2에서 실행할 수 있습니다. WSL2에서 메모리 사용량 및 CPU 사용량을 제어하기 어렵기 때문에 WSL2의 테스트를 제외하였습니다. 생태계에서는 여러 LLM과 VLM 모델을 다운로드할 수 있습니다. 그래서 여러 시스템에서 다양한 AI 모델을 테스트하기 위해 Ollama를 벤치마킹 테스트 베드로 사용합니다. 설치는 매우 간단합니다. 터미널에서 다음을 실행하세요:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">curl <span class="hljs-attr">https</span>:<span class="hljs-comment">//ollama.ai/install.sh | sh</span>
</code></pre>
<p>저는 Ollama LLMs에서 생성된 토큰/초의 처리량을 테스트하는 도구를 개발했습니다. 이 코드(ollama-benchmark)는 Python3로 작성되었으며 MIT 라이선스 하에 오픈 소스로 공개되어 있습니다. 추가해야 할 기능이 더 있거나 수정해야 할 버그가 있다면 알려주세요. 텍스트 출력 품질을 측정하기 어려울 수 있으므로 이 실험에서는 텍스트 출력 속도에 초점을 맞춥니다. (더 높은 토큰/초가 좋음)</p>
<p>테스트에 사용된 기계 또는 VM의 기술 사양</p>
<ul>
<li>8GB RAM이 장착된 Raspberry Pi 5 (Ubuntu 23.10 64비트 운영 체제) 쿼드코어 64비트 Arm CPU</li>
<li>Windows 11 랩탑 호스트에 설치된 VMware Player 17.5를 통해 4코어 프로세서와 8GB RAM이 장착된 Ubuntu 23.10 64비트 운영 체제</li>
<li>Windows 11 데스크톱 호스트에 설치된 VMware Player 17.5를 통해 8코어 프로세서와 16GB RAM이 장착된 Ubuntu 23.10 64비트 운영 체제</li>
<li>Apple Mac mini (Apple M1 칩) (macOS Sonoma 14.2.1 운영 체제) 8코어 CPU(성능 코어 4개, 효율성 코어 4개), 8코어 GPU, 16GB RAM</li>
<li>NVIDIA T4 GPU (Ubuntu 23.10 64비트 운영 체제), 8 vCPU, 16GB RAM</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>비교를 더 효과적으로 하기 위해 Raspberry Pi 5에 우분투 23.10 64비트 운영체제가 설치되었습니다. 아래 비디오에서 운영체제 설치 단계를 따를 수 있습니다.</p>
<p>Ollama 웹사이트 llama2 모델 페이지에는 다음과 같은 내용이 언급되어 있습니다.</p>
<h2>메모리 요구 사항</h2>
<ul>
<li>7b 파라미터 모델은 일반적으로 적어도 8GB의 RAM이 필요합니다.</li>
<li>13b 파라미터 모델은 일반적으로 적어도 16GB의 RAM이 필요합니다.</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>테스트할 모델</h2>
<ul>
<li>mistral:7b (LLM)</li>
<li>llama2:7b (LLM), llama2:13b (LLM)</li>
<li>llava:7b, llava:13b (이미지에서 텍스트로, 이미지로부터 질의응답) (VLM)</li>
</ul>
<p>메모리 제약 사항을 고려하여, 서로 다른 기기에서 성능을 테스트하고 싶은 모델입니다.</p>
<p>샘플 프롬프트 예시는 benchmark.yml에 저장되어 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-yaml"><span class="hljs-attr">version:</span> <span class="hljs-number">1.0</span>
<span class="hljs-attr">modeltypes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">instruct</span>
    <span class="hljs-attr">models:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">model:</span> <span class="hljs-string">mistral:7b</span>
    <span class="hljs-attr">prompts:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">prompt:</span> <span class="hljs-string">달걀빵을</span> <span class="hljs-string">처음부터</span> <span class="hljs-string">굽는</span> <span class="hljs-string">방법을</span> <span class="hljs-string">단계별</span> <span class="hljs-string">가이드를</span> <span class="hljs-string">작성해주세요.</span>
        <span class="hljs-attr">keywords:</span> <span class="hljs-string">요리,</span> <span class="hljs-string">레시피</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">prompt:</span> <span class="hljs-string">다음</span> <span class="hljs-string">문제를</span> <span class="hljs-string">해결하는</span> <span class="hljs-string">파이썬</span> <span class="hljs-string">함수를</span> <span class="hljs-string">개발하세요.</span> <span class="hljs-string">수도쿠</span> <span class="hljs-string">게임</span>
        <span class="hljs-attr">keywords:</span> <span class="hljs-string">파이썬,</span> <span class="hljs-string">수도쿠</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">prompt:</span> <span class="hljs-string">경제</span> <span class="hljs-string">위기에</span> <span class="hljs-string">관한</span> <span class="hljs-string">대화를</span> <span class="hljs-string">나누는</span> <span class="hljs-string">두</span> <span class="hljs-string">캐릭터의</span> <span class="hljs-string">대화를</span> <span class="hljs-string">만들어주세요.</span>
        <span class="hljs-attr">keywords:</span> <span class="hljs-string">대화</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">prompt:</span> <span class="hljs-string">숲에</span> <span class="hljs-string">용갈퀴가</span> <span class="hljs-string">살고</span> <span class="hljs-string">있습니다.</span> <span class="hljs-string">이야기를</span> <span class="hljs-string">계속해주세요.</span>
        <span class="hljs-attr">keywords:</span> <span class="hljs-string">문장</span> <span class="hljs-string">완성</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">prompt:</span> <span class="hljs-string">미국</span> <span class="hljs-string">시애틀로</span> <span class="hljs-number">4</span><span class="hljs-string">명을</span> <span class="hljs-string">위한</span> <span class="hljs-string">항공편을</span> <span class="hljs-string">예약하고</span> <span class="hljs-string">싶습니다.</span>
        <span class="hljs-attr">keywords:</span> <span class="hljs-string">항공편</span> <span class="hljs-string">예약</span>
</code></pre>
<p>각 라운드마다 5가지 다른 프롬프트가 사용되어 출력 토큰 수를 평가합니다. 이 5가지 수의 평균이 기록됩니다. 저는 처음으로 라즈베리 파이 5를 실행했고, 여기에 기록된 동영상이 있습니다.</p>
<p>다양한 모델의 시스템별 토큰 속도에 대한 벤치마크 요약</p>
<h2>AI 모델 (LLMs 및 VLM) 추론 처리량 성능 결과에 대한 생각</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>위의 동영상에서 확인할 수 있듯이, 계산 활용은 주로 GPU 코어 및 GPU VRAM에서 발생합니다.</li>
<li>추론을 더 빨리 실행하려면 강력한 GPU를 선택하세요.</li>
<li>사람과 AI 모델 간의 편안한 상호작용은 초당 7토큰의 처리량이 필요합니다 (예시는 비디오 5에서 제공됨). 대부분의 사람이 따라갈 수 없는 13토큰의 속도는 비디오 6에서 보여주었습니다.</li>
<li>미래 OS 중 Copilot이라는 AI 지원이 내장된 OS는 최소 16GB RAM이 필요할 것입니다. AI의 출력은 의미가 있는 것이며 신뢰성이 있어야 하며, 너무 빠르지도, 너무 느리지도 않아야 합니다. 이 부분은 Microsoft가 발표한 소식과 일치합니다: (Microsoft, AI PC용 RAM으로 16GB를 기본 설정 — 해당 기기에는 40 TOPS의 AI 계산 능력이 필요하다고 보도).</li>
</ul>
<h2>결론</h2>
<p>LLM을 로컬에서 실행함으로써 데이터 보안과 개인 정보 보호를 강화할 수 있을 뿐만 아니라, 전문가, 개발자 및 열정가들을 위한 무한한 가능성을 열어줍니다. 이 처리량 성능 기준에 따라, Raspberry Pi 5를 LLM 추론 기계로 사용하지 않겠습니다. 왜냐하면 너무 느리기 때문입니다. LLM과 VLM을 Apple Mac mini M1 (16GB RAM)에서 실행하는 것이 충분합니다. LLM 추론을 더 빠르게 실행하려면 강력한 기계가 필요하다면 GPU가 탑재된 클라우드 VM을 임대하세요.</p>
<p>책임 성명: 저는 Ollama나 Raspberry Pi, Apple, Google과 관련이 없습니다. 모든 의견과 견해는 제 개인적인 것이며 어떤 조직도 대표하지 않습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"로컬 LLM 및 다양한 시스템에서 VLM 실행 시 처리량 성능 비교","description":"","date":"2024-05-23 16:57","slug":"2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems","content":"\n데이터 엔지니어로서, 저는 몇 가지 생성적 AI 모델을 시험해 보고 로컬에서 모델을 설치/실행하는 것에 매혹을 느낍니다. Large Language Model (LLM)과 Vision-Language Model (VLM)은 가장 흥미로운 모델입니다. OpenAI는 ChatGPT 웹사이트와 모바일 앱을 제공합니다. Microsoft는 Windows 11 Copilot을 우리가 사용할 수 있도록 만들었습니다. 그러나 우리는 어떤 데이터가 인터넷으로 전송되고 그들의 데이터베이스에 저장되는지를 제어할 수 없습니다. 그들의 시스템은 오픈 소스가 아니며, 마치 신비로운 검은 상자와 같습니다.\n\n일부 관대한 회사(Meta 및 Mistral AI 같은) 또는 개인들이 자신들의 모델을 오픈 소스로 공개하고, 적극적인 커뮤니티가 도구를 단계별로 구축하여 우리가 집 컴퓨터에서 LLM과 VLM을 쉽게 실행할 수 있도록 도와줍니다. Raspberry Pi 5와 8GB RAM은 이 기사에서 시험되었습니다 (Raspberry Pi에서 로컬 LLM 및 VLM 실행). 이는 신용 카드 크기의 소형 Single Board Computer (SBC)입니다. 나는 더 저렴한 컴퓨터 장비/솔루션이나 가상 머신을 찾아 성능을 시험해보고, 돈을 지불하는 대가에 좋은 가치를 제공하거나 심지어 일반 대중에게도 제공하도록 하고 싶습니다. 고려해야 할 사항은 텍스트 출력 속도, 텍스트 출력 품질 및 비용입니다.\n\n생성된 내용을 평가하는 작업은 다른 연구 기관에서 수행됩니다. 예를 들어, 이 기사에서 mistral-7b가 llama2-13b보다 지식, 추론 및 이해력에서 능가한다고 언급됩니다. https://mistral.ai/news/announcing-mistral-7b/. 그래서 나는 LLM 테스트에 mistral과 llama2를 포함했습니다.\n\nOllama는 현재 macOS, Linux 및 Windows의 WSL2에서 실행할 수 있습니다. WSL2에서 메모리 사용량 및 CPU 사용량을 제어하기 어렵기 때문에 WSL2의 테스트를 제외하였습니다. 생태계에서는 여러 LLM과 VLM 모델을 다운로드할 수 있습니다. 그래서 여러 시스템에서 다양한 AI 모델을 테스트하기 위해 Ollama를 벤치마킹 테스트 베드로 사용합니다. 설치는 매우 간단합니다. 터미널에서 다음을 실행하세요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ncurl https://ollama.ai/install.sh | sh\n```\n\n저는 Ollama LLMs에서 생성된 토큰/초의 처리량을 테스트하는 도구를 개발했습니다. 이 코드(ollama-benchmark)는 Python3로 작성되었으며 MIT 라이선스 하에 오픈 소스로 공개되어 있습니다. 추가해야 할 기능이 더 있거나 수정해야 할 버그가 있다면 알려주세요. 텍스트 출력 품질을 측정하기 어려울 수 있으므로 이 실험에서는 텍스트 출력 속도에 초점을 맞춥니다. (더 높은 토큰/초가 좋음)\n\n테스트에 사용된 기계 또는 VM의 기술 사양\n\n- 8GB RAM이 장착된 Raspberry Pi 5 (Ubuntu 23.10 64비트 운영 체제) 쿼드코어 64비트 Arm CPU\n- Windows 11 랩탑 호스트에 설치된 VMware Player 17.5를 통해 4코어 프로세서와 8GB RAM이 장착된 Ubuntu 23.10 64비트 운영 체제\n- Windows 11 데스크톱 호스트에 설치된 VMware Player 17.5를 통해 8코어 프로세서와 16GB RAM이 장착된 Ubuntu 23.10 64비트 운영 체제\n- Apple Mac mini (Apple M1 칩) (macOS Sonoma 14.2.1 운영 체제) 8코어 CPU(성능 코어 4개, 효율성 코어 4개), 8코어 GPU, 16GB RAM\n- NVIDIA T4 GPU (Ubuntu 23.10 64비트 운영 체제), 8 vCPU, 16GB RAM\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n비교를 더 효과적으로 하기 위해 Raspberry Pi 5에 우분투 23.10 64비트 운영체제가 설치되었습니다. 아래 비디오에서 운영체제 설치 단계를 따를 수 있습니다.\n\nOllama 웹사이트 llama2 모델 페이지에는 다음과 같은 내용이 언급되어 있습니다.\n\n## 메모리 요구 사항\n\n- 7b 파라미터 모델은 일반적으로 적어도 8GB의 RAM이 필요합니다.\n- 13b 파라미터 모델은 일반적으로 적어도 16GB의 RAM이 필요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 테스트할 모델\n\n- mistral:7b (LLM)\n- llama2:7b (LLM), llama2:13b (LLM)\n- llava:7b, llava:13b (이미지에서 텍스트로, 이미지로부터 질의응답) (VLM)\n\n메모리 제약 사항을 고려하여, 서로 다른 기기에서 성능을 테스트하고 싶은 모델입니다.\n\n샘플 프롬프트 예시는 benchmark.yml에 저장되어 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```yaml\nversion: 1.0\nmodeltypes:\n  - type: instruct\n    models:\n      - model: mistral:7b\n    prompts:\n      - prompt: 달걀빵을 처음부터 굽는 방법을 단계별 가이드를 작성해주세요.\n        keywords: 요리, 레시피\n      - prompt: 다음 문제를 해결하는 파이썬 함수를 개발하세요. 수도쿠 게임\n        keywords: 파이썬, 수도쿠\n      - prompt: 경제 위기에 관한 대화를 나누는 두 캐릭터의 대화를 만들어주세요.\n        keywords: 대화\n      - prompt: 숲에 용갈퀴가 살고 있습니다. 이야기를 계속해주세요.\n        keywords: 문장 완성\n      - prompt: 미국 시애틀로 4명을 위한 항공편을 예약하고 싶습니다.\n        keywords: 항공편 예약\n```\n\n각 라운드마다 5가지 다른 프롬프트가 사용되어 출력 토큰 수를 평가합니다. 이 5가지 수의 평균이 기록됩니다. 저는 처음으로 라즈베리 파이 5를 실행했고, 여기에 기록된 동영상이 있습니다.\n\n다양한 모델의 시스템별 토큰 속도에 대한 벤치마크 요약\n\n## AI 모델 (LLMs 및 VLM) 추론 처리량 성능 결과에 대한 생각\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 위의 동영상에서 확인할 수 있듯이, 계산 활용은 주로 GPU 코어 및 GPU VRAM에서 발생합니다.\n- 추론을 더 빨리 실행하려면 강력한 GPU를 선택하세요.\n- 사람과 AI 모델 간의 편안한 상호작용은 초당 7토큰의 처리량이 필요합니다 (예시는 비디오 5에서 제공됨). 대부분의 사람이 따라갈 수 없는 13토큰의 속도는 비디오 6에서 보여주었습니다.\n- 미래 OS 중 Copilot이라는 AI 지원이 내장된 OS는 최소 16GB RAM이 필요할 것입니다. AI의 출력은 의미가 있는 것이며 신뢰성이 있어야 하며, 너무 빠르지도, 너무 느리지도 않아야 합니다. 이 부분은 Microsoft가 발표한 소식과 일치합니다: (Microsoft, AI PC용 RAM으로 16GB를 기본 설정 — 해당 기기에는 40 TOPS의 AI 계산 능력이 필요하다고 보도).\n\n## 결론\n\nLLM을 로컬에서 실행함으로써 데이터 보안과 개인 정보 보호를 강화할 수 있을 뿐만 아니라, 전문가, 개발자 및 열정가들을 위한 무한한 가능성을 열어줍니다. 이 처리량 성능 기준에 따라, Raspberry Pi 5를 LLM 추론 기계로 사용하지 않겠습니다. 왜냐하면 너무 느리기 때문입니다. LLM과 VLM을 Apple Mac mini M1 (16GB RAM)에서 실행하는 것이 충분합니다. LLM 추론을 더 빠르게 실행하려면 강력한 기계가 필요하다면 GPU가 탑재된 클라우드 VM을 임대하세요.\n\n책임 성명: 저는 Ollama나 Raspberry Pi, Apple, Google과 관련이 없습니다. 모든 의견과 견해는 제 개인적인 것이며 어떤 조직도 대표하지 않습니다.\n","ogImage":{"url":"/assets/img/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems_0.png"},"coverImage":"/assets/img/2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e데이터 엔지니어로서, 저는 몇 가지 생성적 AI 모델을 시험해 보고 로컬에서 모델을 설치/실행하는 것에 매혹을 느낍니다. Large Language Model (LLM)과 Vision-Language Model (VLM)은 가장 흥미로운 모델입니다. OpenAI는 ChatGPT 웹사이트와 모바일 앱을 제공합니다. Microsoft는 Windows 11 Copilot을 우리가 사용할 수 있도록 만들었습니다. 그러나 우리는 어떤 데이터가 인터넷으로 전송되고 그들의 데이터베이스에 저장되는지를 제어할 수 없습니다. 그들의 시스템은 오픈 소스가 아니며, 마치 신비로운 검은 상자와 같습니다.\u003c/p\u003e\n\u003cp\u003e일부 관대한 회사(Meta 및 Mistral AI 같은) 또는 개인들이 자신들의 모델을 오픈 소스로 공개하고, 적극적인 커뮤니티가 도구를 단계별로 구축하여 우리가 집 컴퓨터에서 LLM과 VLM을 쉽게 실행할 수 있도록 도와줍니다. Raspberry Pi 5와 8GB RAM은 이 기사에서 시험되었습니다 (Raspberry Pi에서 로컬 LLM 및 VLM 실행). 이는 신용 카드 크기의 소형 Single Board Computer (SBC)입니다. 나는 더 저렴한 컴퓨터 장비/솔루션이나 가상 머신을 찾아 성능을 시험해보고, 돈을 지불하는 대가에 좋은 가치를 제공하거나 심지어 일반 대중에게도 제공하도록 하고 싶습니다. 고려해야 할 사항은 텍스트 출력 속도, 텍스트 출력 품질 및 비용입니다.\u003c/p\u003e\n\u003cp\u003e생성된 내용을 평가하는 작업은 다른 연구 기관에서 수행됩니다. 예를 들어, 이 기사에서 mistral-7b가 llama2-13b보다 지식, 추론 및 이해력에서 능가한다고 언급됩니다. \u003ca href=\"https://mistral.ai/news/announcing-mistral-7b/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://mistral.ai/news/announcing-mistral-7b/\u003c/a\u003e. 그래서 나는 LLM 테스트에 mistral과 llama2를 포함했습니다.\u003c/p\u003e\n\u003cp\u003eOllama는 현재 macOS, Linux 및 Windows의 WSL2에서 실행할 수 있습니다. WSL2에서 메모리 사용량 및 CPU 사용량을 제어하기 어렵기 때문에 WSL2의 테스트를 제외하였습니다. 생태계에서는 여러 LLM과 VLM 모델을 다운로드할 수 있습니다. 그래서 여러 시스템에서 다양한 AI 모델을 테스트하기 위해 Ollama를 벤치마킹 테스트 베드로 사용합니다. 설치는 매우 간단합니다. 터미널에서 다음을 실행하세요:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecurl \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//ollama.ai/install.sh | sh\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e저는 Ollama LLMs에서 생성된 토큰/초의 처리량을 테스트하는 도구를 개발했습니다. 이 코드(ollama-benchmark)는 Python3로 작성되었으며 MIT 라이선스 하에 오픈 소스로 공개되어 있습니다. 추가해야 할 기능이 더 있거나 수정해야 할 버그가 있다면 알려주세요. 텍스트 출력 품질을 측정하기 어려울 수 있으므로 이 실험에서는 텍스트 출력 속도에 초점을 맞춥니다. (더 높은 토큰/초가 좋음)\u003c/p\u003e\n\u003cp\u003e테스트에 사용된 기계 또는 VM의 기술 사양\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e8GB RAM이 장착된 Raspberry Pi 5 (Ubuntu 23.10 64비트 운영 체제) 쿼드코어 64비트 Arm CPU\u003c/li\u003e\n\u003cli\u003eWindows 11 랩탑 호스트에 설치된 VMware Player 17.5를 통해 4코어 프로세서와 8GB RAM이 장착된 Ubuntu 23.10 64비트 운영 체제\u003c/li\u003e\n\u003cli\u003eWindows 11 데스크톱 호스트에 설치된 VMware Player 17.5를 통해 8코어 프로세서와 16GB RAM이 장착된 Ubuntu 23.10 64비트 운영 체제\u003c/li\u003e\n\u003cli\u003eApple Mac mini (Apple M1 칩) (macOS Sonoma 14.2.1 운영 체제) 8코어 CPU(성능 코어 4개, 효율성 코어 4개), 8코어 GPU, 16GB RAM\u003c/li\u003e\n\u003cli\u003eNVIDIA T4 GPU (Ubuntu 23.10 64비트 운영 체제), 8 vCPU, 16GB RAM\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e비교를 더 효과적으로 하기 위해 Raspberry Pi 5에 우분투 23.10 64비트 운영체제가 설치되었습니다. 아래 비디오에서 운영체제 설치 단계를 따를 수 있습니다.\u003c/p\u003e\n\u003cp\u003eOllama 웹사이트 llama2 모델 페이지에는 다음과 같은 내용이 언급되어 있습니다.\u003c/p\u003e\n\u003ch2\u003e메모리 요구 사항\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e7b 파라미터 모델은 일반적으로 적어도 8GB의 RAM이 필요합니다.\u003c/li\u003e\n\u003cli\u003e13b 파라미터 모델은 일반적으로 적어도 16GB의 RAM이 필요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e테스트할 모델\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003emistral:7b (LLM)\u003c/li\u003e\n\u003cli\u003ellama2:7b (LLM), llama2:13b (LLM)\u003c/li\u003e\n\u003cli\u003ellava:7b, llava:13b (이미지에서 텍스트로, 이미지로부터 질의응답) (VLM)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e메모리 제약 사항을 고려하여, 서로 다른 기기에서 성능을 테스트하고 싶은 모델입니다.\u003c/p\u003e\n\u003cp\u003e샘플 프롬프트 예시는 benchmark.yml에 저장되어 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-yaml\"\u003e\u003cspan class=\"hljs-attr\"\u003eversion:\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003emodeltypes:\u003c/span\u003e\n  \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003etype:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003einstruct\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003emodels:\u003c/span\u003e\n      \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003emodel:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003emistral:7b\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eprompts:\u003c/span\u003e\n      \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eprompt:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e달걀빵을\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e처음부터\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e굽는\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e방법을\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e단계별\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e가이드를\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e작성해주세요.\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ekeywords:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e요리,\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e레시피\u003c/span\u003e\n      \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eprompt:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e다음\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e문제를\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e해결하는\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e파이썬\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e함수를\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e개발하세요.\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e수도쿠\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e게임\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ekeywords:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e파이썬,\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e수도쿠\u003c/span\u003e\n      \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eprompt:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e경제\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e위기에\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e관한\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e대화를\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e나누는\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e두\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e캐릭터의\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e대화를\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e만들어주세요.\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ekeywords:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e대화\u003c/span\u003e\n      \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eprompt:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e숲에\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e용갈퀴가\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e살고\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e있습니다.\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e이야기를\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e계속해주세요.\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ekeywords:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e문장\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e완성\u003c/span\u003e\n      \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eprompt:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e미국\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e시애틀로\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e명을\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e위한\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e항공편을\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e예약하고\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e싶습니다.\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ekeywords:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e항공편\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e예약\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e각 라운드마다 5가지 다른 프롬프트가 사용되어 출력 토큰 수를 평가합니다. 이 5가지 수의 평균이 기록됩니다. 저는 처음으로 라즈베리 파이 5를 실행했고, 여기에 기록된 동영상이 있습니다.\u003c/p\u003e\n\u003cp\u003e다양한 모델의 시스템별 토큰 속도에 대한 벤치마크 요약\u003c/p\u003e\n\u003ch2\u003eAI 모델 (LLMs 및 VLM) 추론 처리량 성능 결과에 대한 생각\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e위의 동영상에서 확인할 수 있듯이, 계산 활용은 주로 GPU 코어 및 GPU VRAM에서 발생합니다.\u003c/li\u003e\n\u003cli\u003e추론을 더 빨리 실행하려면 강력한 GPU를 선택하세요.\u003c/li\u003e\n\u003cli\u003e사람과 AI 모델 간의 편안한 상호작용은 초당 7토큰의 처리량이 필요합니다 (예시는 비디오 5에서 제공됨). 대부분의 사람이 따라갈 수 없는 13토큰의 속도는 비디오 6에서 보여주었습니다.\u003c/li\u003e\n\u003cli\u003e미래 OS 중 Copilot이라는 AI 지원이 내장된 OS는 최소 16GB RAM이 필요할 것입니다. AI의 출력은 의미가 있는 것이며 신뢰성이 있어야 하며, 너무 빠르지도, 너무 느리지도 않아야 합니다. 이 부분은 Microsoft가 발표한 소식과 일치합니다: (Microsoft, AI PC용 RAM으로 16GB를 기본 설정 — 해당 기기에는 40 TOPS의 AI 계산 능력이 필요하다고 보도).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e결론\u003c/h2\u003e\n\u003cp\u003eLLM을 로컬에서 실행함으로써 데이터 보안과 개인 정보 보호를 강화할 수 있을 뿐만 아니라, 전문가, 개발자 및 열정가들을 위한 무한한 가능성을 열어줍니다. 이 처리량 성능 기준에 따라, Raspberry Pi 5를 LLM 추론 기계로 사용하지 않겠습니다. 왜냐하면 너무 느리기 때문입니다. LLM과 VLM을 Apple Mac mini M1 (16GB RAM)에서 실행하는 것이 충분합니다. LLM 추론을 더 빠르게 실행하려면 강력한 기계가 필요하다면 GPU가 탑재된 클라우드 VM을 임대하세요.\u003c/p\u003e\n\u003cp\u003e책임 성명: 저는 Ollama나 Raspberry Pi, Apple, Google과 관련이 없습니다. 모든 의견과 견해는 제 개인적인 것이며 어떤 조직도 대표하지 않습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-23-ComparingThroughputPerformanceofRunningLocalLLMsandVLMondifferentsystems"},"buildId":"wfHLuDA3kTGBYfaM5IGXk","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>