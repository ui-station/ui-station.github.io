<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker" data-gatsby-head="true"/><meta name="twitter:title" content="Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-19 12:53" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_buildManifest.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 19, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>생성적 AI 시리즈</h2>
<p>이 블로그는 생성적 AI에 관한 지속적인 시리즈이며 이전 블로그의 연장선입니다. 이 블로그 시리즈에서는 Ollama를 탐색하고 도커를 사용하여 분산 아키텍처에 배포할 수 있는 응용 프로그램을 구축할 것입니다.</p>
<p>Ollama는 강력한 언어 모델을 손쉽게 컴퓨터에서 실행할 수 있도록 도와주는 프레임워크입니다. Ollama 소개에 대해서는 2024년 2월에 A B Vijay Kumar에 의해 작성된 <a href="%EB%A7%81%ED%81%AC">Ollama — Brings runtime to serve LLMs everywhere. | by A B Vijay Kumar | Feb, 2024 | Medium</a>를 참조해주세요. 이 블로그에서는 langchain 애플리케이션을 구축하고 도커에 배포할 것입니다.</p>
<h2>Ollama를 위한 Langchain 챗봇 애플리케이션</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Langshan을 사용하여 챗봇 애플리케이션을 개발해 보겠습니다. Python 애플리케이션에서 모델에 액세스하기 위해 간단한 Streamlit 챗봇 애플리케이션을 만들 것입니다. 이 Python 애플리케이션을 컨테이너에 배포하고 다른 컨테이너에서 Ollama를 사용할 것입니다. Docker-compose를 사용하여 인프라를 구축할 것입니다. 만약 Docker 또는 docker-compose를 사용하는 방법을 모르신다면, 계속 진행하기 전에 인터넷에서 몇 가지 자습서를 참고해 주세요.</p>
<p>아래 그림은 컨테이너 간 상호 작용과 접근하는 포트를 보여주는 아키텍처를 보여줍니다.</p>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png" alt="아키텍처 그림"></p>
<p>컨테이너를 2개 생성할 것입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>Ollama 컨테이너는 모델을 저장하고 로드하기 위해 호스트 볼륨을 사용합니다 (/root/.ollama는 로컬 ./data/ollama로 매핑됩니다). Ollama 컨테이너는 내부적으로 11434로 매핑된 외부 포트인 11434에서 수신 대기합니다.</li>
<li>Streamlit 챗봇 애플리케이션은 내부적으로 8501로 매핑된 외부 포트인 8501에서 수신 대기합니다.
코딩을 시작하기 전에 Python 가상 환경을 설정하겠습니다.</li>
</ul>
<pre><code class="hljs language-shell">python3 -m venv ./ollama-langchain-venv
source ./ollama-langchain-venv/bin/activate
</code></pre>
<p>다음은 streamlit 애플리케이션의 소스 코드입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_1.png">
<p>제가 이전 블로그에서 작성한 것과 매우 유사한 소스 코드입니다. 이 코드가 어떻게 작동하는지에 대한 자세한 내용은 다른 블로그인 "Retrieval Augmented Generation(RAG) - LlamaIndex를 사용한 문서용 챗봇"을 참조하실 수 있어요. 주요 차이점은 Ollama를 사용하고 Ollama Langchain 라이브러리를 통해 모델을 호출한다는 점이에요 (이는 langchain_community의 일부입니다).</p>
<p>requirements.txt에서 종속성을 정의해 봅시다.</p>
<img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_2.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이제 Streamlit 애플리케이션의 도커 이미지를 빌드하기 위한 Dockerfile을 정의해 보겠습니다.</p>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_3.png" alt="image"></p>
<p>우리는 베이스 이미지로 python 도커 이미지를 사용하고, /app이라는 작업 디렉토리를 생성합니다. 그런 다음 애플리케이션 파일을 해당 디렉토리로 복사하고, pip를 사용하여 모든 종속성을 설치합니다. 그 후에 포트 8501을 노출하고 Streamlit 애플리케이션을 시작합니다.</p>
<p>아래에 표시된 대로 docker build 명령을 사용하여 도커 이미지를 빌드할 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_4.png" alt="docker_image_4"></p>
<p>도커 이미지가 빌드되었는지 확인할 수 있어야 합니다. 다음에 표시된 것처럼 <code>docker images</code> 명령어를 사용하세요.</p>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_5.png" alt="docker_image_5"></p>
<p>이제 스트림릿 애플리케이션 및 올라마 컨테이너의 네트워크를 정의하는 docker-compose 구성 파일을 만들어보겠습니다. 이렇게 하면 두 애플리케이션이 상호 작용할 수 있습니다. 또한 위의 그림에서 보듯이 다양한 포트 구성을 정의할 것입니다. 올라마에 대해서는 모델들이 영구적으로 유지되도록 볼륨 매핑도 수행할 것입니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_6.png">
<p>도커 컴포즈 업 명령을 실행하면 어플리케이션을 실행할 수 있습니다. 도커 컴포즈 업을 실행하면 아래 스크린샷에 표시된 대로 두 컨테이너가 모두 실행되고 있는 것을 확인할 수 있어야 합니다.</p>
<img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_7.png">
<p>아래 스크린샷에 표시된 대로 도커 컴포즈 ps 명령을 실행하여 컨테이너가 실행 중인 것을 확인할 수 있어야 합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_8.png" alt="image1"></p>
<p>올라마가 실행 중인지 확인하려면 아래 스크린샷에 표시된 대로 <a href="http://localhost:11434%EC%9D%84" rel="nofollow" target="_blank">http://localhost:11434을</a> 호출해야 합니다.</p>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_9.png" alt="image2"></p>
<p>이제 아래에 표시된 대로 docker exec 명령을 사용하여 도커 컨테이너에 로그인하여 필요한 모델을 다운로드해 봅시다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">docker exec -it ollama-langchain-ollama-container-<span class="hljs-number">1</span> ollama run phi
</code></pre>
<p>우리는 모델 phi를 사용 중이므로 해당 모델을 가져와서 실행하여 테스트 중입니다. 아래 스크린샷을 참고하세요. phi 모델이 다운로드되고 실행을 시작할 것입니다 (-it 플래그를 사용하므로 샘플 프롬프트로 상호작용 및 테스트가 가능해집니다).</p>
<img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_10.png">
<p>로컬 폴더 ./data/ollama에서 다운로드된 모델 파일 및 매니페스트를 확인할 수 있어야 합니다. 이 폴더는 내부적으로 컨테이너에 매핑되어 있으며(Ollama가 다운로드된 모델을 제공하기 위해 찾는 위치인 /root/.ollama에 매핑됨)</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>아래는 Markdown 형식으로 변경한 테이블입니다.</p>











<table><thead><tr><th><code>&#x3C;img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_11.png" /></code></th></tr></thead><tbody><tr><td>Lets now run access our streamlit application by opening <a href="http://localhost:8501" rel="nofollow" target="_blank">http://localhost:8501</a> on the browser. The following screenshot shows the interface</td></tr></tbody></table>











<table><thead><tr><th><code>&#x3C;img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_12.png" /></code></th></tr></thead><tbody><tr><td>Lets try to run a prompt “generate a story about dog called bozo”. You shud be able to see the console logs reflecting the API calls, that are coming from our Streamlit application, as shown below</td></tr></tbody></table>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>아래 스크린샷에서 확인할 수 있듯이, 내가 보낸 프롬프트에 대한 응답을 받았어요.</p>
<p>도커 컴포즈 다운을 호출하여 배포를 중지할 수 있어요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다음 스크린샷은 결과를 보여줍니다.</p>
<p><img src="/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_15.png" alt="output"></p>
<p>여기 있습니다. 이 블로그를 준비하면서 랭체인과 함께 Ollama가 작동하고 Docker-Compose를 사용하여 Docker에 배포하는 것이 정말 즐거웠어요.</p>
<p>도움이 되었기를 바랍니다. 더 많은 실험으로 돌아오겠습니다. 그 동안 즐기고 코딩하세요!!! 곧 다시 만나요!!!</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>여기서 전체 소스 코드에 액세스할 수 있습니다. abvijaykumar/ollama-langchain (github.com)</p>
<p>참고 자료</p>
<ul>
<li>Ollama</li>
<li>Docker Compose 개요 | Docker 문서</li>
<li>Docker 문서</li>
<li>Ollama — LLM을 어디서나 제공하는 런타임을 제공합니다. | 작성자: A B Vijay Kumar | 2024년 2월 | Medium</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Ollama - Langchain을 사용해 챗봇 만들기, 도커로 배포하기","description":"","date":"2024-06-19 12:53","slug":"2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker","content":"\n## 생성적 AI 시리즈\n\n이 블로그는 생성적 AI에 관한 지속적인 시리즈이며 이전 블로그의 연장선입니다. 이 블로그 시리즈에서는 Ollama를 탐색하고 도커를 사용하여 분산 아키텍처에 배포할 수 있는 응용 프로그램을 구축할 것입니다.\n\nOllama는 강력한 언어 모델을 손쉽게 컴퓨터에서 실행할 수 있도록 도와주는 프레임워크입니다. Ollama 소개에 대해서는 2024년 2월에 A B Vijay Kumar에 의해 작성된 [Ollama — Brings runtime to serve LLMs everywhere. | by A B Vijay Kumar | Feb, 2024 | Medium](링크)를 참조해주세요. 이 블로그에서는 langchain 애플리케이션을 구축하고 도커에 배포할 것입니다.\n\n## Ollama를 위한 Langchain 챗봇 애플리케이션\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLangshan을 사용하여 챗봇 애플리케이션을 개발해 보겠습니다. Python 애플리케이션에서 모델에 액세스하기 위해 간단한 Streamlit 챗봇 애플리케이션을 만들 것입니다. 이 Python 애플리케이션을 컨테이너에 배포하고 다른 컨테이너에서 Ollama를 사용할 것입니다. Docker-compose를 사용하여 인프라를 구축할 것입니다. 만약 Docker 또는 docker-compose를 사용하는 방법을 모르신다면, 계속 진행하기 전에 인터넷에서 몇 가지 자습서를 참고해 주세요.\n\n아래 그림은 컨테이너 간 상호 작용과 접근하는 포트를 보여주는 아키텍처를 보여줍니다.\n\n![아키텍처 그림](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png)\n\n컨테이너를 2개 생성할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- Ollama 컨테이너는 모델을 저장하고 로드하기 위해 호스트 볼륨을 사용합니다 (/root/.ollama는 로컬 ./data/ollama로 매핑됩니다). Ollama 컨테이너는 내부적으로 11434로 매핑된 외부 포트인 11434에서 수신 대기합니다.\n- Streamlit 챗봇 애플리케이션은 내부적으로 8501로 매핑된 외부 포트인 8501에서 수신 대기합니다.\n  코딩을 시작하기 전에 Python 가상 환경을 설정하겠습니다.\n\n```shell\npython3 -m venv ./ollama-langchain-venv\nsource ./ollama-langchain-venv/bin/activate\n```\n\n다음은 streamlit 애플리케이션의 소스 코드입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_1.png\" /\u003e\n\n제가 이전 블로그에서 작성한 것과 매우 유사한 소스 코드입니다. 이 코드가 어떻게 작동하는지에 대한 자세한 내용은 다른 블로그인 \"Retrieval Augmented Generation(RAG) - LlamaIndex를 사용한 문서용 챗봇\"을 참조하실 수 있어요. 주요 차이점은 Ollama를 사용하고 Ollama Langchain 라이브러리를 통해 모델을 호출한다는 점이에요 (이는 langchain_community의 일부입니다).\n\nrequirements.txt에서 종속성을 정의해 봅시다.\n\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_2.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 Streamlit 애플리케이션의 도커 이미지를 빌드하기 위한 Dockerfile을 정의해 보겠습니다.\n\n![image](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_3.png)\n\n우리는 베이스 이미지로 python 도커 이미지를 사용하고, /app이라는 작업 디렉토리를 생성합니다. 그런 다음 애플리케이션 파일을 해당 디렉토리로 복사하고, pip를 사용하여 모든 종속성을 설치합니다. 그 후에 포트 8501을 노출하고 Streamlit 애플리케이션을 시작합니다.\n\n아래에 표시된 대로 docker build 명령을 사용하여 도커 이미지를 빌드할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![docker_image_4](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_4.png)\n\n도커 이미지가 빌드되었는지 확인할 수 있어야 합니다. 다음에 표시된 것처럼 `docker images` 명령어를 사용하세요.\n\n![docker_image_5](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_5.png)\n\n이제 스트림릿 애플리케이션 및 올라마 컨테이너의 네트워크를 정의하는 docker-compose 구성 파일을 만들어보겠습니다. 이렇게 하면 두 애플리케이션이 상호 작용할 수 있습니다. 또한 위의 그림에서 보듯이 다양한 포트 구성을 정의할 것입니다. 올라마에 대해서는 모델들이 영구적으로 유지되도록 볼륨 매핑도 수행할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_6.png\" /\u003e\n\n도커 컴포즈 업 명령을 실행하면 어플리케이션을 실행할 수 있습니다. 도커 컴포즈 업을 실행하면 아래 스크린샷에 표시된 대로 두 컨테이너가 모두 실행되고 있는 것을 확인할 수 있어야 합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_7.png\" /\u003e\n\n아래 스크린샷에 표시된 대로 도커 컴포즈 ps 명령을 실행하여 컨테이너가 실행 중인 것을 확인할 수 있어야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image1](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_8.png)\n\n올라마가 실행 중인지 확인하려면 아래 스크린샷에 표시된 대로 http://localhost:11434을 호출해야 합니다.\n\n![image2](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_9.png)\n\n이제 아래에 표시된 대로 docker exec 명령을 사용하여 도커 컨테이너에 로그인하여 필요한 모델을 다운로드해 봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndocker exec -it ollama-langchain-ollama-container-1 ollama run phi\n```\n\n우리는 모델 phi를 사용 중이므로 해당 모델을 가져와서 실행하여 테스트 중입니다. 아래 스크린샷을 참고하세요. phi 모델이 다운로드되고 실행을 시작할 것입니다 (-it 플래그를 사용하므로 샘플 프롬프트로 상호작용 및 테스트가 가능해집니다).\n\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_10.png\" /\u003e\n\n로컬 폴더 ./data/ollama에서 다운로드된 모델 파일 및 매니페스트를 확인할 수 있어야 합니다. 이 폴더는 내부적으로 컨테이너에 매핑되어 있으며(Ollama가 다운로드된 모델을 제공하기 위해 찾는 위치인 /root/.ollama에 매핑됨)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 Markdown 형식으로 변경한 테이블입니다.\n\n| `\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_11.png\" /\u003e`                                                                   |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Lets now run access our streamlit application by opening [http://localhost:8501](http://localhost:8501) on the browser. The following screenshot shows the interface |\n\n| `\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_12.png\" /\u003e`                                                                                                   |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Lets try to run a prompt “generate a story about dog called bozo”. You shud be able to see the console logs reflecting the API calls, that are coming from our Streamlit application, as shown below |\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래 스크린샷에서 확인할 수 있듯이, 내가 보낸 프롬프트에 대한 응답을 받았어요.\n\n도커 컴포즈 다운을 호출하여 배포를 중지할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음 스크린샷은 결과를 보여줍니다.\n\n![output](/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_15.png)\n\n여기 있습니다. 이 블로그를 준비하면서 랭체인과 함께 Ollama가 작동하고 Docker-Compose를 사용하여 Docker에 배포하는 것이 정말 즐거웠어요.\n\n도움이 되었기를 바랍니다. 더 많은 실험으로 돌아오겠습니다. 그 동안 즐기고 코딩하세요!!! 곧 다시 만나요!!!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기서 전체 소스 코드에 액세스할 수 있습니다. abvijaykumar/ollama-langchain (github.com)\n\n참고 자료\n\n- Ollama\n- Docker Compose 개요 | Docker 문서\n- Docker 문서\n- Ollama — LLM을 어디서나 제공하는 런타임을 제공합니다. | 작성자: A B Vijay Kumar | 2024년 2월 | Medium\n","ogImage":{"url":"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png"},"coverImage":"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e생성적 AI 시리즈\u003c/h2\u003e\n\u003cp\u003e이 블로그는 생성적 AI에 관한 지속적인 시리즈이며 이전 블로그의 연장선입니다. 이 블로그 시리즈에서는 Ollama를 탐색하고 도커를 사용하여 분산 아키텍처에 배포할 수 있는 응용 프로그램을 구축할 것입니다.\u003c/p\u003e\n\u003cp\u003eOllama는 강력한 언어 모델을 손쉽게 컴퓨터에서 실행할 수 있도록 도와주는 프레임워크입니다. Ollama 소개에 대해서는 2024년 2월에 A B Vijay Kumar에 의해 작성된 \u003ca href=\"%EB%A7%81%ED%81%AC\"\u003eOllama — Brings runtime to serve LLMs everywhere. | by A B Vijay Kumar | Feb, 2024 | Medium\u003c/a\u003e를 참조해주세요. 이 블로그에서는 langchain 애플리케이션을 구축하고 도커에 배포할 것입니다.\u003c/p\u003e\n\u003ch2\u003eOllama를 위한 Langchain 챗봇 애플리케이션\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eLangshan을 사용하여 챗봇 애플리케이션을 개발해 보겠습니다. Python 애플리케이션에서 모델에 액세스하기 위해 간단한 Streamlit 챗봇 애플리케이션을 만들 것입니다. 이 Python 애플리케이션을 컨테이너에 배포하고 다른 컨테이너에서 Ollama를 사용할 것입니다. Docker-compose를 사용하여 인프라를 구축할 것입니다. 만약 Docker 또는 docker-compose를 사용하는 방법을 모르신다면, 계속 진행하기 전에 인터넷에서 몇 가지 자습서를 참고해 주세요.\u003c/p\u003e\n\u003cp\u003e아래 그림은 컨테이너 간 상호 작용과 접근하는 포트를 보여주는 아키텍처를 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_0.png\" alt=\"아키텍처 그림\"\u003e\u003c/p\u003e\n\u003cp\u003e컨테이너를 2개 생성할 것입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003eOllama 컨테이너는 모델을 저장하고 로드하기 위해 호스트 볼륨을 사용합니다 (/root/.ollama는 로컬 ./data/ollama로 매핑됩니다). Ollama 컨테이너는 내부적으로 11434로 매핑된 외부 포트인 11434에서 수신 대기합니다.\u003c/li\u003e\n\u003cli\u003eStreamlit 챗봇 애플리케이션은 내부적으로 8501로 매핑된 외부 포트인 8501에서 수신 대기합니다.\n코딩을 시작하기 전에 Python 가상 환경을 설정하겠습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003epython3 -m venv ./ollama-langchain-venv\nsource ./ollama-langchain-venv/bin/activate\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음은 streamlit 애플리케이션의 소스 코드입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_1.png\"\u003e\n\u003cp\u003e제가 이전 블로그에서 작성한 것과 매우 유사한 소스 코드입니다. 이 코드가 어떻게 작동하는지에 대한 자세한 내용은 다른 블로그인 \"Retrieval Augmented Generation(RAG) - LlamaIndex를 사용한 문서용 챗봇\"을 참조하실 수 있어요. 주요 차이점은 Ollama를 사용하고 Ollama Langchain 라이브러리를 통해 모델을 호출한다는 점이에요 (이는 langchain_community의 일부입니다).\u003c/p\u003e\n\u003cp\u003erequirements.txt에서 종속성을 정의해 봅시다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_2.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이제 Streamlit 애플리케이션의 도커 이미지를 빌드하기 위한 Dockerfile을 정의해 보겠습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e우리는 베이스 이미지로 python 도커 이미지를 사용하고, /app이라는 작업 디렉토리를 생성합니다. 그런 다음 애플리케이션 파일을 해당 디렉토리로 복사하고, pip를 사용하여 모든 종속성을 설치합니다. 그 후에 포트 8501을 노출하고 Streamlit 애플리케이션을 시작합니다.\u003c/p\u003e\n\u003cp\u003e아래에 표시된 대로 docker build 명령을 사용하여 도커 이미지를 빌드할 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_4.png\" alt=\"docker_image_4\"\u003e\u003c/p\u003e\n\u003cp\u003e도커 이미지가 빌드되었는지 확인할 수 있어야 합니다. 다음에 표시된 것처럼 \u003ccode\u003edocker images\u003c/code\u003e 명령어를 사용하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_5.png\" alt=\"docker_image_5\"\u003e\u003c/p\u003e\n\u003cp\u003e이제 스트림릿 애플리케이션 및 올라마 컨테이너의 네트워크를 정의하는 docker-compose 구성 파일을 만들어보겠습니다. 이렇게 하면 두 애플리케이션이 상호 작용할 수 있습니다. 또한 위의 그림에서 보듯이 다양한 포트 구성을 정의할 것입니다. 올라마에 대해서는 모델들이 영구적으로 유지되도록 볼륨 매핑도 수행할 것입니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_6.png\"\u003e\n\u003cp\u003e도커 컴포즈 업 명령을 실행하면 어플리케이션을 실행할 수 있습니다. 도커 컴포즈 업을 실행하면 아래 스크린샷에 표시된 대로 두 컨테이너가 모두 실행되고 있는 것을 확인할 수 있어야 합니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_7.png\"\u003e\n\u003cp\u003e아래 스크린샷에 표시된 대로 도커 컴포즈 ps 명령을 실행하여 컨테이너가 실행 중인 것을 확인할 수 있어야 합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_8.png\" alt=\"image1\"\u003e\u003c/p\u003e\n\u003cp\u003e올라마가 실행 중인지 확인하려면 아래 스크린샷에 표시된 대로 \u003ca href=\"http://localhost:11434%EC%9D%84\" rel=\"nofollow\" target=\"_blank\"\u003ehttp://localhost:11434을\u003c/a\u003e 호출해야 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_9.png\" alt=\"image2\"\u003e\u003c/p\u003e\n\u003cp\u003e이제 아래에 표시된 대로 docker exec 명령을 사용하여 도커 컨테이너에 로그인하여 필요한 모델을 다운로드해 봅시다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edocker exec -it ollama-langchain-ollama-container-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e ollama run phi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e우리는 모델 phi를 사용 중이므로 해당 모델을 가져와서 실행하여 테스트 중입니다. 아래 스크린샷을 참고하세요. phi 모델이 다운로드되고 실행을 시작할 것입니다 (-it 플래그를 사용하므로 샘플 프롬프트로 상호작용 및 테스트가 가능해집니다).\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_10.png\"\u003e\n\u003cp\u003e로컬 폴더 ./data/ollama에서 다운로드된 모델 파일 및 매니페스트를 확인할 수 있어야 합니다. 이 폴더는 내부적으로 컨테이너에 매핑되어 있으며(Ollama가 다운로드된 모델을 제공하기 위해 찾는 위치인 /root/.ollama에 매핑됨)\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e아래는 Markdown 형식으로 변경한 테이블입니다.\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e\u003ccode\u003e\u0026#x3C;img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_11.png\" /\u003e\u003c/code\u003e\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eLets now run access our streamlit application by opening \u003ca href=\"http://localhost:8501\" rel=\"nofollow\" target=\"_blank\"\u003ehttp://localhost:8501\u003c/a\u003e on the browser. The following screenshot shows the interface\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003e\u003ccode\u003e\u0026#x3C;img src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_12.png\" /\u003e\u003c/code\u003e\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eLets try to run a prompt “generate a story about dog called bozo”. You shud be able to see the console logs reflecting the API calls, that are coming from our Streamlit application, as shown below\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e아래 스크린샷에서 확인할 수 있듯이, 내가 보낸 프롬프트에 대한 응답을 받았어요.\u003c/p\u003e\n\u003cp\u003e도커 컴포즈 다운을 호출하여 배포를 중지할 수 있어요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다음 스크린샷은 결과를 보여줍니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker_15.png\" alt=\"output\"\u003e\u003c/p\u003e\n\u003cp\u003e여기 있습니다. 이 블로그를 준비하면서 랭체인과 함께 Ollama가 작동하고 Docker-Compose를 사용하여 Docker에 배포하는 것이 정말 즐거웠어요.\u003c/p\u003e\n\u003cp\u003e도움이 되었기를 바랍니다. 더 많은 실험으로 돌아오겠습니다. 그 동안 즐기고 코딩하세요!!! 곧 다시 만나요!!!\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e여기서 전체 소스 코드에 액세스할 수 있습니다. abvijaykumar/ollama-langchain (github.com)\u003c/p\u003e\n\u003cp\u003e참고 자료\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOllama\u003c/li\u003e\n\u003cli\u003eDocker Compose 개요 | Docker 문서\u003c/li\u003e\n\u003cli\u003eDocker 문서\u003c/li\u003e\n\u003cli\u003eOllama — LLM을 어디서나 제공하는 런타임을 제공합니다. | 작성자: A B Vijay Kumar | 2024년 2월 | Medium\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-19-OllamaBuildaChatBotwithLangchainOllamaDeployonDocker"},"buildId":"-dPCbnM2yhdKNgXe92VJV","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>