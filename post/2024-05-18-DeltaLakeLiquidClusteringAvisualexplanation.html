<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>델타 레이크 리퀴드 클러스터링 - 시각적 설명 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="델타 레이크 리퀴드 클러스터링 - 시각적 설명 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="델타 레이크 리퀴드 클러스터링 - 시각적 설명 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation" data-gatsby-head="true"/><meta name="twitter:title" content="델타 레이크 리퀴드 클러스터링 - 시각적 설명 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 16:27" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_buildManifest.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">델타 레이크 리퀴드 클러스터링 - 시각적 설명</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="델타 레이크 리퀴드 클러스터링 - 시각적 설명" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">11<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>최소한의 노력으로 레이크하우스 데이터 저장 레이아웃을 최적화하는 방법</p>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png" alt="image"/></p>
<h1>소개</h1>
<p>데이터 레이크하우스는 오픈 테이블 형식을 사용하고 특정 공급 업체에 얽매이지 않아 장점을 누립니다. 그러나 이는 특정 읽기 및 쓰기 작업을 위해 데이터 처리를 최적화하기 위해 파일 저장 레이아웃을 최적화해야 한다는 추가적인 부담과 함께 옵니다. 읽기 또는 쓰기 작업에 의해 처리되는 데이터 양을 최소화하기 위해 가능한 한 많은 파일을 제거함으로써 작업을 효율적으로 만드는 것이 핵심 아이디어입니다. 제거는 특정 파일이 해당 쿼리에 관련이 없다는 암묵적 또는 명시적 메타데이터를 사용하여 발생합니다.</p>
<div class="content-ad"></div>
<p>옛날에, Hive은 각 데이터 파티션을 HDFS 또는 클라우드 저장소의 단일 폴더로 범위를 지정하여 유명한 Hive 스타일의 파티셔닝을 소개했습니다. 그것은 작동이 잘 됩니다. 그러나 작은 파일 문제가 발생하거나 워크로드 특성 변경으로 인해 파티션 체계를 변경해야 할 때 문제가 발생합니다. 또한 Hive 스타일의 파티셔닝은 고 카디널리티 질의에 대해 도움이 되지 않습니다.</p>
<p>오픈 테이블 형식에서 제공되는 DML 지원으로 인해, 조각 모음이나 GDPR에서 잊혀져야 할 권리와 같은 경우를 관리하기 위해 단일 또는 몇 개의 레코드를 업데이트/삭제하는 것이 매우 일반적해졌습니다. 이러한 시나리오에는 고 카디널리티 질의가 효율적이어야 합니다. 이러한 요구 사항을 충족하기 위해 Delta Lake Z-Ordering과 같은 기술이 소개되었습니다. Z-Ordering은 꽤 좋지만 OPTIMIZE 명령을 다시 실행할 때 전체 테이블(또는 파티션)을 최적화하는 반복적인 노력과 많은 낭비된 컴퓨팅 파워를 도입하는 일부 제한 사항이 있습니다. Delta Lake Z-Order의 더 자세한 탐구를 위해 Z-Order에 대한 저의 글을 살펴보십시오. 그 글에서는 낮은 수준의 세부 사항도 약간 논의됩니다.</p>
<p>Hive 스타일의 파티셔닝과 Z-Ordering의 이러한 제한 사항을 완화하기 위해 Databricks 및 Delta Lake 팀은 액체 클러스터링을 소개했습니다. 작성 시점에서 Delta Lake on Databricks에서는 아직 미리보기 상태이며 OSS Delta Lake에서는 실험적인 기능인 상태입니다. 그러나 설계 문서는 누구나 읽을 수 있습니다. 액체 클러스터링은 레코드-파일 할당 방법으로 Hilbert Curve를 사용할 것으로 예상됩니다. 액체 클러스터링의 비전은 다음과 같은 단일 최적화 기술을 가지는 것입니다:</p>
<ul>
<li>저 및 고 카디널리티 질의에 모두 잘 작동합니다.</li>
<li>&quot;이미 최적화된&quot; 파일을 최적화할 필요가 없습니다.</li>
<li>클러스터링 열을 변경하면 전체 테이블을 다시 빌드할 필요가 없습니다.</li>
</ul>
<div class="content-ad"></div>
<p>이 게시물에서는 액체 클러스터링이 레코드를 파일에 할당하는 방식을 시각적으로 보여줄 것입니다. 목표는 기술적인 세부 내용을 파헤치는 대신에 해당 기술의 매우 높은 수준의 이해를 갖는 데 초점을 맞추는 것입니다. 여전히 상황이 조리실에 있기 때문에 세부 사항에 대해 심층적으로 파고들지 않습니다.</p>
<h1>기본으로 돌아가기 — 레코드를 파일에 할당하는 방법은?</h1>
<p>우리는 N개의 레코드가 있고 이를 M개의 파일에 쓰려고 한다고 가정해 봅시다. 파일 가지치기의 아이디어를 기억한다면, 비슷한 레코드를 동일한 파일에 저장하는 것이 필수적입니다. 작업 부하에 따라 비슷한 레코드는 같은 픽업 동네의 택시 여행이거나 같은 고객의 은행 거래일 수 있습니다.</p>
<p>아래 streamlit 앱은 이 문제를 처리하는 데 3가지 방법을 보여줍니다. N 및 M에 대한 다양한 값을 사용하고 배치 방식을 조정하여 레코드가 파일에 할당되는 방식을 시각적으로 확인할 수 있습니다. 이 간단한 앱에서는 모든 레코드가 필드 그룹을 갖고 있지만 우리는 2차원 평면 상 좌표인 x 및 y라는 두 개의 정수 필드에 대한 쿼리를 최적화해야 합니다. 레코드는 N개의 레코드를 생성하도록 x와 y의 쌍별 조합을 균등하게 다루기 위해 생성됩니다. 배치 방법 선택에 따라 각 지점(레코드)이 특정 파일에 할당되며 이 할당은 파일 색상을 사용하여 지정됩니다.</p>
<div class="content-ad"></div>
<ul>
<li>랜덤 할당</li>
</ul>
<p>이 방식은 사용자 정의 로직을 거의 사용하지 않습니다. 레코드가 무작위로 파일에 할당됩니다.</p>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_1.png" alt="Image"/></p>
<ol start="2">
<li>Z-Ordering 할당</li>
</ol>
<div class="content-ad"></div>
<p>다음 방법은 Z-Ordering을 사용하여 각 레코드 (x와 y의 조합)마다 Z-Order 값을 계산하는 것입니다. 이는 평면 상의 이차원 점을 선 상의 점으로 효과적으로 변환합니다. 그런 다음 선을 M개의 세그먼트로 나눌 수 있으며, 각 세그먼트는 하나의 파일을 나타냅니다. 레코드가 Z-Order 값 z를 갖고 있다면, 파일 z % M에 할당됩니다. 이제 점들은 일차원 관련 값을 갖고 있기 때문에, 그러한 값들을 선으로 연결하여 매핑이 어떻게 이루어지는지 시각적으로 확인할 수 있습니다. 각 점 위에 마우스를 올려놓으면 선형 순서 값을 볼 수 있습니다.</p>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_2.png" alt="이미지"/></p>
<p>공간적으로 서로 가까운 레코드들은 Z-Order 라인 상에서 서로 가까이 배치됩니다. 예를 들어, 위 스크린샷을 보면, 점 (2,4)와 (3,4)는 각각 36과 37의 Z-Order 값을 가지고 있습니다. (0,4)에서 (7,3)으로 이동하는 것과 같이, 공간적으로 멀리 떨어져 있지만 연이은 Z-Order 값을 가진 큰 점프가 보이기도 합니다. 그럼에도 불구하고, Z-Ordering은 좋은 데이터 로컬리티 할당을 생성합니다.</p>
<ol start="3">
<li>힐버트 곡선 할당</li>
</ol>
<div class="content-ad"></div>
<p>세 번째 방법은 레코드마다 두 차원 x와 y 값에 기초한 일차원 값을 할당하기 위해 힐버트 곡선을 사용하는 것입니다.</p>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_3.png" alt="image"/></p>
<p>이 Python 라이브러리를 사용하여 두 차원 점에 대한 힐버트 곡선 값을 도출했습니다. 이는 Z-Order와 비슷한데, 서로 가까운 포인트들은 동일한 파일에 들어가게 되지만, 일차원 할당에서 멀리 떨어진 지점이 연속적으로 배치되는 급격한 점프가 없다는 추가적인 이점이 있습니다.</p>
<p>이제 우리는 Z-Order와 힐버트 곡선과 같은 공간 채우기 곡선을 사용하여 파일에 포인트를 할당하는 방법에 대한 아이디어가 생겼으니, Databricks에서 Liquid Clustering을 살펴보겠습니다.</p>
<div class="content-ad"></div>
<h1>Liquid clustering in action</h1>
<p>이 섹션의 실험은 최소한이지만 대표적입니다. 필요한 설정은 다음과 같습니다:</p>
<ul>
<li>Azure 무료 평가판 계정 및 무료 Databricks 계정</li>
<li>Liqud 클러스터링을 지원하는 최신 DBR인 DBR 13.3을 사용하는 단일 노드 Databricks 클러스터</li>
<li>빠른 속도와 모자이크가 작동하기 위해 클러스터에서 photon을 활성화</li>
</ul>
<p>우리는 유명한 뉴욕시 택시 데이터 세트를 사용하고 아래 워크로드를 위해 최적화할 것입니다:</p>
<div class="content-ad"></div>
<ul>
<li>대부분의 쿼리는 데이터의 한 해 또는 몇 해에 대해서 작동할 것입니다.</li>
<li>많은 쿼리는 픽업 위치(위도 및 경도)를 기반으로 필터링하는 것이 포함될 것입니다.</li>
</ul>
<p>Databricks 워크스페이스에 파이썬 노트북을 만들고 샌드박스 데이터베이스를 생성하는 방법을 시작해보세요.</p>
<pre><code class="hljs language-js">%sql
<span class="hljs-variable constant_">CREATE</span> <span class="hljs-variable constant_">DATABASE</span> liquid_db;
</code></pre>
<p>다음으로, 맨해튼 섬 주변의 경계 상자를 기준으로 뉴욕시 택시 데이터셋을 기반으로 하는 테이블을 생성해보세요. 이 글의 몇 가지 미학적 이유로 테이블은 초기에 액체 클러스터링을 사용할 수 있지만, 모든 작업이 기본적으로 데이터를 클러스터링하는 것은 아님을 인식하셔야 합니다. 예를 들어, 데이터가 MERGE 작업으로 변경되면, 데이터를 클러스터링하기 위해 OPTIMIZE 작업을 실행해야 할 것입니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> liquid_db.trips 
    CLUSTER <span class="hljs-keyword">BY</span> (pickup_datetime, pickup_latitude, pickup_longitude)
<span class="hljs-keyword">AS</span>
<span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> 
<span class="hljs-keyword">FROM</span> delta.`dbfs:<span class="hljs-operator">/</span>databricks<span class="hljs-operator">-</span>datasets<span class="hljs-operator">/</span>nyctaxi<span class="hljs-operator">/</span>tables<span class="hljs-operator">/</span>nyctaxi_yellow`
<span class="hljs-keyword">WHERE</span>
    pickup_longitude <span class="hljs-keyword">between</span> <span class="hljs-number">-74.05186503267184</span> <span class="hljs-keyword">and</span> <span class="hljs-number">-73.83200446816883</span> <span class="hljs-keyword">AND</span>
    pickup_latitude <span class="hljs-keyword">between</span> <span class="hljs-number">40.69286486137213</span> <span class="hljs-keyword">and</span> <span class="hljs-number">40.91947608519337</span>
</code></pre>
<p>클러스터링 열 목록에서 첫 번째 열은 타임스탬프 열인 픽업 일시임을 주목해주세요. 우리는 하이브 스타일의 파티셔닝을 사용하기 위해 명시적으로 연도 열을 생성할 필요가 없습니다.</p>
<p>나중에 특정 Delta Lake 트랜잭션에서 생성된 파일이 클러스터링되었는지 여부를 감지하는 방법을 보여줄 텐데요, 제 경우에는 파일이 액체 클러스터링되지 않았기 때문에 직접 클러스터링해야 했습니다.</p>
<pre><code class="hljs language-sql">OPTIMIZE liquid_db.trips
</code></pre>
<div class="content-ad"></div>
<p>델타 레이크 Z-오더링과 달리, 리퀴드 클러스터링을 사용하여 테이블을 최적화할 때, 파일이 최적화되었는지 여부를 알려주는 트랜잭션 로그 메타데이터가 있습니다. 따라서 나중에 OPTIMIZE 명령을 실행하여 새로운 데이터를 클러스터링할 때 파일을 건너뛸 수 있습니다. 이러한 경우에 대해 더 많은 아이디어가 있지만, 핵심적인 차이점은 ADD 프로토콜 액션의 태그 부분에 LIQUID_METADATA_ID라는 새 메타데이터 항목이 있는 것입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">functions</span> <span class="hljs-keyword">as</span> F
second_log_file = <span class="hljs-string">&quot;dbfs:/user/hive/warehouse/liquid_db.db/trips/_delta_log/00000000000000000001.json&quot;</span>
(
    spark.<span class="hljs-property">read</span>
    .<span class="hljs-title function_">json</span>(second_log_file)
    .<span class="hljs-title function_">where</span>(<span class="hljs-string">&quot;add is not null&quot;</span>)
    .<span class="hljs-title function_">select</span>(<span class="hljs-string">&quot;add.size&quot;</span>, <span class="hljs-string">&quot;add.tags.*&quot;</span>)
    .<span class="hljs-title function_">withColumn</span>(<span class="hljs-string">&quot;size&quot;</span>, F.<span class="hljs-title function_">expr</span>(<span class="hljs-string">&quot;cast(size/1024/1024 as int)&quot;</span>))
    .<span class="hljs-title function_">withColumnRenamed</span>(<span class="hljs-string">&quot;size&quot;</span>, <span class="hljs-string">&quot;size_mb&quot;</span>)
    .<span class="hljs-title function_">display</span>()
)
</code></pre>
<p>위 스니펫의 출력에서 제 경우 191개의 파일이 나오며 대부분의 크기는 100에서 300MB 범위에 있습니다.</p>
<img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_4.png"/>
<div class="content-ad"></div>
<p>이제 OPTIMIZE 작업을 위해 트랜잭션 로그 엔트리 안에 수집된 최대 및 최소 메타데이터 값을 검토해 봅시다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">functions</span> <span class="hljs-keyword">as</span> F
def <span class="hljs-title function_">load_stats_from_commit</span>(commit_file):
  df = spark.<span class="hljs-property">read</span>.<span class="hljs-title function_">json</span>(commit_file).<span class="hljs-title function_">where</span>(<span class="hljs-string">&quot;add is not null&quot;</span>)
  add_schema = <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;
  struct
    &lt;
      numRecords:long,
      minValues: struct&lt;pickup_latitude: double,pickup_longitude: double, pickup_datetime: timestamp&gt;,
      maxValues: struct&lt;pickup_latitude: double,pickup_longitude: double, pickup_datetime: timestamp&gt;
    &gt;
  &quot;</span><span class="hljs-string">&quot;&quot;</span>

  stats = (
    df
      .<span class="hljs-title function_">select</span>(<span class="hljs-string">&quot;add.path&quot;</span>, <span class="hljs-string">&quot;add.size&quot;</span>, 
          F.<span class="hljs-title function_">from_json</span>(<span class="hljs-string">&quot;add.stats&quot;</span>, add_schema).<span class="hljs-title function_">alias</span>(<span class="hljs-string">&quot;stats&quot;</span>)
      )
      .<span class="hljs-title function_">selectExpr</span>(
        <span class="hljs-string">&quot;substring(path, 1, 10) as file&quot;</span>,
        <span class="hljs-string">&quot;size&quot;</span>, 
        <span class="hljs-string">&quot;stats.minValues.pickup_datetime as min_pickup_datetime&quot;</span>,
        <span class="hljs-string">&quot;stats.maxValues.pickup_datetime as max_pickup_datetime&quot;</span>,
        <span class="hljs-string">&quot;stats.minValues.pickup_latitude as min_pickup_latitude&quot;</span>, 
        <span class="hljs-string">&quot;stats.maxValues.pickup_latitude as max_pickup_latitude&quot;</span>,
        <span class="hljs-string">&quot;stats.minValues.pickup_longitude as min_pickup_longitude&quot;</span>,
        <span class="hljs-string">&quot;stats.maxValues.pickup_longitude as max_pickup_longitude&quot;</span>
      )
  )

  stats = (
    stats
      .<span class="hljs-title function_">withColumn</span>(<span class="hljs-string">&quot;rect&quot;</span>, F.<span class="hljs-title function_">expr</span>(
        <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;
          concat(&#x27;POLYGON ((&#x27; , 
            min_pickup_longitude, &#x27; &#x27;, min_pickup_latitude, &#x27;,&#x27; ,
            max_pickup_longitude, &#x27; &#x27;, min_pickup_latitude, &#x27;,&#x27; ,
            max_pickup_longitude, &#x27; &#x27;, max_pickup_latitude, &#x27;,&#x27; ,
            min_pickup_longitude, &#x27; &#x27;, max_pickup_latitude, &#x27;,&#x27; ,
            min_pickup_longitude, &#x27; &#x27;, min_pickup_latitude, 
          &#x27;))&#x27;
          )
      &quot;</span><span class="hljs-string">&quot;&quot;</span>))
  )

  <span class="hljs-keyword">return</span> stats

stats = <span class="hljs-title function_">load_stats_from_commit</span>(second_log_file).<span class="hljs-title function_">orderBy</span>(<span class="hljs-string">&quot;min_pickup_datetime&quot;</span>, <span class="hljs-string">&quot;max_pickup_datetime&quot;</span>)
stats.<span class="hljs-title function_">display</span>()
</code></pre>
<p>위의 &quot;난해한&quot; 코드 스니펫은 몇 가지 작업을 수행합니다:</p>
<ul>
<li>픽업 시간, 위도 및 경도에 대한 최소 및 최대 값 수집</li>
<li>가독성 목적을 위해 파일 이름의 처음 10자를 고유 식별기로 사용</li>
<li>파일 내의 모든 여행을 포함하는 경계 상자의 GeoJSON 표현 생성 (픽업 위치에 따라)</li>
</ul>
<div class="content-ad"></div>
<p>상단의 표 출력물은 특별히 흥미로운 것은 아니며, 파일이 클러스터링 열에 따라 어떻게 배치되었는지 쉽게 전달하지 않습니다. 그러나 이를 어떤 종류의 간트 차트로 시각화한다면, 파일이 시간별 범위를 포함하는 그룹으로 클러스터링되었음이 명백해질 것입니다. 파일 중첩이 발생할 수 있지만, 일반적인 주제는 시간 범위를 기반으로 한 클러스터링을 보여줍니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> plotly.<span class="hljs-property">express</span> <span class="hljs-keyword">as</span> px
fig = px.<span class="hljs-title function_">timeline</span>(stats.<span class="hljs-title function_">toPandas</span>(), 
    x_start=<span class="hljs-string">&quot;min_pickup_datetime&quot;</span>, 
    x_end=<span class="hljs-string">&quot;max_pickup_datetime&quot;</span>,
    y=<span class="hljs-string">&quot;file&quot;</span>)
fig.<span class="hljs-title function_">update_yaxes</span>(categoryorder=<span class="hljs-string">&quot;min ascending&quot;</span>)
fig.<span class="hljs-title function_">show</span>()
</code></pre>
<img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_5.png"/>
<p>Jan 2009부터 April 2010까지의 파일을 살펴보겠습니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">file_group = (
  stats
    .<span class="hljs-title function_">where</span>(<span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;
    min_pickup_datetime &gt;= &#x27;2009-01-01&#x27; AND 
    max_pickup_datetime &lt;= &#x27;2010-04-30&#x27;
    &quot;</span><span class="hljs-string">&quot;&quot;</span>
    )
)
file_group.<span class="hljs-title function_">count</span>()
# <span class="hljs-number">29</span>개의 파일이 인쇄됩니다.
</code></pre>
<p>해당 Date Range를 공유하는 이 파일들이 커버하는 지리 공간 영역을 시각화하고 싶습니다.</p>
<pre><code class="hljs language-js">%pip install databricks-mosaic==<span class="hljs-number">0.4</span><span class="hljs-number">.0</span>
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> mosaic <span class="hljs-keyword">as</span> mos
spark.<span class="hljs-property">conf</span>.<span class="hljs-title function_">set</span>(<span class="hljs-string">&quot;spark.databricks.labs.mosaic.index.system&quot;</span>, <span class="hljs-string">&quot;H3&quot;</span>)
mos.<span class="hljs-title function_">enable_mosaic</span>(spark, dbutils)
</code></pre>
<div class="content-ad"></div>
<p>%%mosaic_kepler
file_group &quot;rect&quot; &quot;geometry&quot;</p>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_6.png" alt="Image"/></p>
<p>동일한 파일 그룹에 대해 특정 날짜 범위를 포괄하는 파일들의 경우, 해당 파일들의 레코드는 잔여 클러스터링 키인 위도 및 경도를 기반으로 클러스터링됩니다. 이러한 공간 클러스터링을 통해 지구상의 특정 지점을 커버하는 파일 수가 현저히 줄어들어 파일 가지치기가 크게 향상됩니다.</p>
<h1>가지치기 혜택</h1>
<div class="content-ad"></div>
<p>아래와 같은 쿼리를 실행하면 194개 파일 중 126개 파일을 제거합니다 (첫 번째 커밋에서 최적화되지 않은 파일이 3개 발생했습니다).</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">SELECT</span> payment_type, <span class="hljs-title function_">sum</span>(total_amount) <span class="hljs-keyword">as</span> total_amount
<span class="hljs-variable constant_">FROM</span> liquid_db.<span class="hljs-property">trips</span>
<span class="hljs-variable constant_">WHERE</span> pickup_datetime &gt;= <span class="hljs-string">&#x27;2011-01-01&#x27;</span> <span class="hljs-variable constant_">AND</span> pickup_datetime &lt; <span class="hljs-string">&#x27;2012-01-01&#x27;</span>
<span class="hljs-variable constant_">GROUP</span> <span class="hljs-variable constant_">BY</span> payment_type
</code></pre>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_7.png" alt="이미지"/></p>
<p>위의 쿼리는 8년 데이터 중 1년치의 집계 결과입니다. 순수 Hive 파티셔닝이면 더 좋은 프루닝이 가능할 수도 있지만, 여전히 집계 쿼리에 대한 일정한 값은 얻을 수 있습니다.</p>
<div class="content-ad"></div>
<p>만약 같은 해를 사용하지만 타임스 스퀘어 근처의 몇 가지 특정 레코드를 찾아보려 한다면, 더 나은 가지치기를 할 수 있어요.</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">SELECT</span> *
<span class="hljs-variable constant_">FROM</span> liquid_db.<span class="hljs-property">trips</span>
<span class="hljs-variable constant_">WHERE</span> pickup_datetime &gt;= <span class="hljs-string">&#x27;2011-01-01&#x27;</span> <span class="hljs-variable constant_">AND</span> pickup_datetime &lt; <span class="hljs-string">&#x27;2012-01-01&#x27;</span>
<span class="hljs-variable constant_">AND</span> pickup_latitude <span class="hljs-variable constant_">BETWEEN</span> <span class="hljs-number">40.757816</span> <span class="hljs-variable constant_">AND</span> <span class="hljs-number">40.757832</span>
<span class="hljs-variable constant_">AND</span> pickup_longitude <span class="hljs-variable constant_">BETWEEN</span> -<span class="hljs-number">73.985143</span> <span class="hljs-variable constant_">AND</span> -<span class="hljs-number">73.985105</span>
</code></pre>
<p><img src="/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_8.png" alt="이미지"/></p>
<p>특정 워크로드에 유용한지 확인하기 위해 철저한 테스트와 벤치마킹이 필요하지만, 전반적으로 Delta Lake 테이블의 관리를 간편하게 해주는 Liquid 클러스터링은 매우 유망해 보입니다.</p>
<div class="content-ad"></div>
<h1>마무리</h1>
<p>리퀴드 클러스터링을 사용할 때 고려해야 할 측면이 많으며 특정 사용 사례에 맞는 동작을 조정하기 위해 많은 구성 값을 조정해야 할 것입니다. 본 게시물은 리퀴드 클러스터링이 어떻게 작동하는지를 높은 수준에서 시각적으로 보여주는 작은 시도입니다. 단순화된 사용 사례는 Hive 스타일의 파티셔닝과 Z-Order의 혜택을 결합하여 단일 최적화 방법을 사용하는 것입니다.</p>
<p>liquid_db를 삭제하고 정리하려면 DROP DATABASE liquid_db CASCADE를 실행하는 것을 잊지 마십시오.</p>
<h1>추가 읽을거리</h1>
<div class="content-ad"></div>
<ul>
<li>델타 테이블에 리퀴드 클러스터링 사용하기</li>
<li>[디자인 문서] [공개] 리퀴드 클러스터링 — Google Docs</li>
<li>힐버트 곡선 코딩 (youtube.com)</li>
<li>Yousry Mohamed의 미디엄에서 A부터 Z까지의 델타 레이크 Z-오더링</li>
</ul></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"델타 레이크 리퀴드 클러스터링 - 시각적 설명","description":"","date":"2024-05-18 16:27","slug":"2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation","content":"\n\n최소한의 노력으로 레이크하우스 데이터 저장 레이아웃을 최적화하는 방법\n\n![image](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png)\n\n# 소개\n\n데이터 레이크하우스는 오픈 테이블 형식을 사용하고 특정 공급 업체에 얽매이지 않아 장점을 누립니다. 그러나 이는 특정 읽기 및 쓰기 작업을 위해 데이터 처리를 최적화하기 위해 파일 저장 레이아웃을 최적화해야 한다는 추가적인 부담과 함께 옵니다. 읽기 또는 쓰기 작업에 의해 처리되는 데이터 양을 최소화하기 위해 가능한 한 많은 파일을 제거함으로써 작업을 효율적으로 만드는 것이 핵심 아이디어입니다. 제거는 특정 파일이 해당 쿼리에 관련이 없다는 암묵적 또는 명시적 메타데이터를 사용하여 발생합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n옛날에, Hive은 각 데이터 파티션을 HDFS 또는 클라우드 저장소의 단일 폴더로 범위를 지정하여 유명한 Hive 스타일의 파티셔닝을 소개했습니다. 그것은 작동이 잘 됩니다. 그러나 작은 파일 문제가 발생하거나 워크로드 특성 변경으로 인해 파티션 체계를 변경해야 할 때 문제가 발생합니다. 또한 Hive 스타일의 파티셔닝은 고 카디널리티 질의에 대해 도움이 되지 않습니다.\n\n오픈 테이블 형식에서 제공되는 DML 지원으로 인해, 조각 모음이나 GDPR에서 잊혀져야 할 권리와 같은 경우를 관리하기 위해 단일 또는 몇 개의 레코드를 업데이트/삭제하는 것이 매우 일반적해졌습니다. 이러한 시나리오에는 고 카디널리티 질의가 효율적이어야 합니다. 이러한 요구 사항을 충족하기 위해 Delta Lake Z-Ordering과 같은 기술이 소개되었습니다. Z-Ordering은 꽤 좋지만 OPTIMIZE 명령을 다시 실행할 때 전체 테이블(또는 파티션)을 최적화하는 반복적인 노력과 많은 낭비된 컴퓨팅 파워를 도입하는 일부 제한 사항이 있습니다. Delta Lake Z-Order의 더 자세한 탐구를 위해 Z-Order에 대한 저의 글을 살펴보십시오. 그 글에서는 낮은 수준의 세부 사항도 약간 논의됩니다.\n\nHive 스타일의 파티셔닝과 Z-Ordering의 이러한 제한 사항을 완화하기 위해 Databricks 및 Delta Lake 팀은 액체 클러스터링을 소개했습니다. 작성 시점에서 Delta Lake on Databricks에서는 아직 미리보기 상태이며 OSS Delta Lake에서는 실험적인 기능인 상태입니다. 그러나 설계 문서는 누구나 읽을 수 있습니다. 액체 클러스터링은 레코드-파일 할당 방법으로 Hilbert Curve를 사용할 것으로 예상됩니다. 액체 클러스터링의 비전은 다음과 같은 단일 최적화 기술을 가지는 것입니다:\n\n- 저 및 고 카디널리티 질의에 모두 잘 작동합니다.\n- \"이미 최적화된\" 파일을 최적화할 필요가 없습니다.\n- 클러스터링 열을 변경하면 전체 테이블을 다시 빌드할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 액체 클러스터링이 레코드를 파일에 할당하는 방식을 시각적으로 보여줄 것입니다. 목표는 기술적인 세부 내용을 파헤치는 대신에 해당 기술의 매우 높은 수준의 이해를 갖는 데 초점을 맞추는 것입니다. 여전히 상황이 조리실에 있기 때문에 세부 사항에 대해 심층적으로 파고들지 않습니다.\n\n# 기본으로 돌아가기 — 레코드를 파일에 할당하는 방법은?\n\n우리는 N개의 레코드가 있고 이를 M개의 파일에 쓰려고 한다고 가정해 봅시다. 파일 가지치기의 아이디어를 기억한다면, 비슷한 레코드를 동일한 파일에 저장하는 것이 필수적입니다. 작업 부하에 따라 비슷한 레코드는 같은 픽업 동네의 택시 여행이거나 같은 고객의 은행 거래일 수 있습니다.\n\n아래 streamlit 앱은 이 문제를 처리하는 데 3가지 방법을 보여줍니다. N 및 M에 대한 다양한 값을 사용하고 배치 방식을 조정하여 레코드가 파일에 할당되는 방식을 시각적으로 확인할 수 있습니다. 이 간단한 앱에서는 모든 레코드가 필드 그룹을 갖고 있지만 우리는 2차원 평면 상 좌표인 x 및 y라는 두 개의 정수 필드에 대한 쿼리를 최적화해야 합니다. 레코드는 N개의 레코드를 생성하도록 x와 y의 쌍별 조합을 균등하게 다루기 위해 생성됩니다. 배치 방법 선택에 따라 각 지점(레코드)이 특정 파일에 할당되며 이 할당은 파일 색상을 사용하여 지정됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 랜덤 할당\n\n이 방식은 사용자 정의 로직을 거의 사용하지 않습니다. 레코드가 무작위로 파일에 할당됩니다.\n\n![Image](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_1.png)\n\n2. Z-Ordering 할당\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 방법은 Z-Ordering을 사용하여 각 레코드 (x와 y의 조합)마다 Z-Order 값을 계산하는 것입니다. 이는 평면 상의 이차원 점을 선 상의 점으로 효과적으로 변환합니다. 그런 다음 선을 M개의 세그먼트로 나눌 수 있으며, 각 세그먼트는 하나의 파일을 나타냅니다. 레코드가 Z-Order 값 z를 갖고 있다면, 파일 z % M에 할당됩니다. 이제 점들은 일차원 관련 값을 갖고 있기 때문에, 그러한 값들을 선으로 연결하여 매핑이 어떻게 이루어지는지 시각적으로 확인할 수 있습니다. 각 점 위에 마우스를 올려놓으면 선형 순서 값을 볼 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_2.png)\n\n공간적으로 서로 가까운 레코드들은 Z-Order 라인 상에서 서로 가까이 배치됩니다. 예를 들어, 위 스크린샷을 보면, 점 (2,4)와 (3,4)는 각각 36과 37의 Z-Order 값을 가지고 있습니다. (0,4)에서 (7,3)으로 이동하는 것과 같이, 공간적으로 멀리 떨어져 있지만 연이은 Z-Order 값을 가진 큰 점프가 보이기도 합니다. 그럼에도 불구하고, Z-Ordering은 좋은 데이터 로컬리티 할당을 생성합니다.\n\n3. 힐버트 곡선 할당\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세 번째 방법은 레코드마다 두 차원 x와 y 값에 기초한 일차원 값을 할당하기 위해 힐버트 곡선을 사용하는 것입니다.\n\n![image](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_3.png)\n\n이 Python 라이브러리를 사용하여 두 차원 점에 대한 힐버트 곡선 값을 도출했습니다. 이는 Z-Order와 비슷한데, 서로 가까운 포인트들은 동일한 파일에 들어가게 되지만, 일차원 할당에서 멀리 떨어진 지점이 연속적으로 배치되는 급격한 점프가 없다는 추가적인 이점이 있습니다.\n\n이제 우리는 Z-Order와 힐버트 곡선과 같은 공간 채우기 곡선을 사용하여 파일에 포인트를 할당하는 방법에 대한 아이디어가 생겼으니, Databricks에서 Liquid Clustering을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Liquid clustering in action\n\n이 섹션의 실험은 최소한이지만 대표적입니다. 필요한 설정은 다음과 같습니다:\n\n- Azure 무료 평가판 계정 및 무료 Databricks 계정\n- Liqud 클러스터링을 지원하는 최신 DBR인 DBR 13.3을 사용하는 단일 노드 Databricks 클러스터\n- 빠른 속도와 모자이크가 작동하기 위해 클러스터에서 photon을 활성화\n\n우리는 유명한 뉴욕시 택시 데이터 세트를 사용하고 아래 워크로드를 위해 최적화할 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 대부분의 쿼리는 데이터의 한 해 또는 몇 해에 대해서 작동할 것입니다.\n- 많은 쿼리는 픽업 위치(위도 및 경도)를 기반으로 필터링하는 것이 포함될 것입니다.\n\nDatabricks 워크스페이스에 파이썬 노트북을 만들고 샌드박스 데이터베이스를 생성하는 방법을 시작해보세요.\n\n```js\n%sql\nCREATE DATABASE liquid_db;\n```\n\n다음으로, 맨해튼 섬 주변의 경계 상자를 기준으로 뉴욕시 택시 데이터셋을 기반으로 하는 테이블을 생성해보세요. 이 글의 몇 가지 미학적 이유로 테이블은 초기에 액체 클러스터링을 사용할 수 있지만, 모든 작업이 기본적으로 데이터를 클러스터링하는 것은 아님을 인식하셔야 합니다. 예를 들어, 데이터가 MERGE 작업으로 변경되면, 데이터를 클러스터링하기 위해 OPTIMIZE 작업을 실행해야 할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sql\nCREATE TABLE liquid_db.trips \n    CLUSTER BY (pickup_datetime, pickup_latitude, pickup_longitude)\nAS\nSELECT * \nFROM delta.`dbfs:/databricks-datasets/nyctaxi/tables/nyctaxi_yellow`\nWHERE\n    pickup_longitude between -74.05186503267184 and -73.83200446816883 AND\n    pickup_latitude between 40.69286486137213 and 40.91947608519337\n```\n\n클러스터링 열 목록에서 첫 번째 열은 타임스탬프 열인 픽업 일시임을 주목해주세요. 우리는 하이브 스타일의 파티셔닝을 사용하기 위해 명시적으로 연도 열을 생성할 필요가 없습니다.\n\n나중에 특정 Delta Lake 트랜잭션에서 생성된 파일이 클러스터링되었는지 여부를 감지하는 방법을 보여줄 텐데요, 제 경우에는 파일이 액체 클러스터링되지 않았기 때문에 직접 클러스터링해야 했습니다.\n\n```sql\nOPTIMIZE liquid_db.trips\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n델타 레이크 Z-오더링과 달리, 리퀴드 클러스터링을 사용하여 테이블을 최적화할 때, 파일이 최적화되었는지 여부를 알려주는 트랜잭션 로그 메타데이터가 있습니다. 따라서 나중에 OPTIMIZE 명령을 실행하여 새로운 데이터를 클러스터링할 때 파일을 건너뛸 수 있습니다. 이러한 경우에 대해 더 많은 아이디어가 있지만, 핵심적인 차이점은 ADD 프로토콜 액션의 태그 부분에 LIQUID_METADATA_ID라는 새 메타데이터 항목이 있는 것입니다.\n\n```js\nimport pyspark.sql.functions as F\nsecond_log_file = \"dbfs:/user/hive/warehouse/liquid_db.db/trips/_delta_log/00000000000000000001.json\"\n(\n    spark.read\n    .json(second_log_file)\n    .where(\"add is not null\")\n    .select(\"add.size\", \"add.tags.*\")\n    .withColumn(\"size\", F.expr(\"cast(size/1024/1024 as int)\"))\n    .withColumnRenamed(\"size\", \"size_mb\")\n    .display()\n)\n```\n\n위 스니펫의 출력에서 제 경우 191개의 파일이 나오며 대부분의 크기는 100에서 300MB 범위에 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_4.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 OPTIMIZE 작업을 위해 트랜잭션 로그 엔트리 안에 수집된 최대 및 최소 메타데이터 값을 검토해 봅시다.\n\n```js\nimport pyspark.sql.functions as F\ndef load_stats_from_commit(commit_file):\n  df = spark.read.json(commit_file).where(\"add is not null\")\n  add_schema = \"\"\"\n  struct\n    \u003c\n      numRecords:long,\n      minValues: struct\u003cpickup_latitude: double,pickup_longitude: double, pickup_datetime: timestamp\u003e,\n      maxValues: struct\u003cpickup_latitude: double,pickup_longitude: double, pickup_datetime: timestamp\u003e\n    \u003e\n  \"\"\"\n\n  stats = (\n    df\n      .select(\"add.path\", \"add.size\", \n          F.from_json(\"add.stats\", add_schema).alias(\"stats\")\n      )\n      .selectExpr(\n        \"substring(path, 1, 10) as file\",\n        \"size\", \n        \"stats.minValues.pickup_datetime as min_pickup_datetime\",\n        \"stats.maxValues.pickup_datetime as max_pickup_datetime\",\n        \"stats.minValues.pickup_latitude as min_pickup_latitude\", \n        \"stats.maxValues.pickup_latitude as max_pickup_latitude\",\n        \"stats.minValues.pickup_longitude as min_pickup_longitude\",\n        \"stats.maxValues.pickup_longitude as max_pickup_longitude\"\n      )\n  )\n\n  stats = (\n    stats\n      .withColumn(\"rect\", F.expr(\n        \"\"\"\n          concat('POLYGON ((' , \n            min_pickup_longitude, ' ', min_pickup_latitude, ',' ,\n            max_pickup_longitude, ' ', min_pickup_latitude, ',' ,\n            max_pickup_longitude, ' ', max_pickup_latitude, ',' ,\n            min_pickup_longitude, ' ', max_pickup_latitude, ',' ,\n            min_pickup_longitude, ' ', min_pickup_latitude, \n          '))'\n          )\n      \"\"\"))\n  )\n\n  return stats\n\nstats = load_stats_from_commit(second_log_file).orderBy(\"min_pickup_datetime\", \"max_pickup_datetime\")\nstats.display()\n```\n\n위의 \"난해한\" 코드 스니펫은 몇 가지 작업을 수행합니다:\n\n- 픽업 시간, 위도 및 경도에 대한 최소 및 최대 값 수집\n- 가독성 목적을 위해 파일 이름의 처음 10자를 고유 식별기로 사용\n- 파일 내의 모든 여행을 포함하는 경계 상자의 GeoJSON 표현 생성 (픽업 위치에 따라)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상단의 표 출력물은 특별히 흥미로운 것은 아니며, 파일이 클러스터링 열에 따라 어떻게 배치되었는지 쉽게 전달하지 않습니다. 그러나 이를 어떤 종류의 간트 차트로 시각화한다면, 파일이 시간별 범위를 포함하는 그룹으로 클러스터링되었음이 명백해질 것입니다. 파일 중첩이 발생할 수 있지만, 일반적인 주제는 시간 범위를 기반으로 한 클러스터링을 보여줍니다.\n\n```js\nimport plotly.express as px\nfig = px.timeline(stats.toPandas(), \n    x_start=\"min_pickup_datetime\", \n    x_end=\"max_pickup_datetime\",\n    y=\"file\")\nfig.update_yaxes(categoryorder=\"min ascending\")\nfig.show()\n```\n\n\u003cimg src=\"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_5.png\" /\u003e\n\nJan 2009부터 April 2010까지의 파일을 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfile_group = (\n  stats\n    .where(\"\"\"\n    min_pickup_datetime \u003e= '2009-01-01' AND \n    max_pickup_datetime \u003c= '2010-04-30'\n    \"\"\"\n    )\n)\nfile_group.count()\n# 29개의 파일이 인쇄됩니다.\n```\n\n해당 Date Range를 공유하는 이 파일들이 커버하는 지리 공간 영역을 시각화하고 싶습니다.\n\n```js\n%pip install databricks-mosaic==0.4.0\n```\n\n```js\nimport mosaic as mos\nspark.conf.set(\"spark.databricks.labs.mosaic.index.system\", \"H3\")\nmos.enable_mosaic(spark, dbutils)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n%%mosaic_kepler\nfile_group \"rect\" \"geometry\"\n\n\n![Image](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_6.png)\n\n동일한 파일 그룹에 대해 특정 날짜 범위를 포괄하는 파일들의 경우, 해당 파일들의 레코드는 잔여 클러스터링 키인 위도 및 경도를 기반으로 클러스터링됩니다. 이러한 공간 클러스터링을 통해 지구상의 특정 지점을 커버하는 파일 수가 현저히 줄어들어 파일 가지치기가 크게 향상됩니다.\n\n# 가지치기 혜택\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같은 쿼리를 실행하면 194개 파일 중 126개 파일을 제거합니다 (첫 번째 커밋에서 최적화되지 않은 파일이 3개 발생했습니다).\n\n```js\nSELECT payment_type, sum(total_amount) as total_amount\nFROM liquid_db.trips\nWHERE pickup_datetime \u003e= '2011-01-01' AND pickup_datetime \u003c '2012-01-01'\nGROUP BY payment_type\n```\n\n![이미지](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_7.png)\n\n위의 쿼리는 8년 데이터 중 1년치의 집계 결과입니다. 순수 Hive 파티셔닝이면 더 좋은 프루닝이 가능할 수도 있지만, 여전히 집계 쿼리에 대한 일정한 값은 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 같은 해를 사용하지만 타임스 스퀘어 근처의 몇 가지 특정 레코드를 찾아보려 한다면, 더 나은 가지치기를 할 수 있어요.\n\n```js\nSELECT *\nFROM liquid_db.trips\nWHERE pickup_datetime \u003e= '2011-01-01' AND pickup_datetime \u003c '2012-01-01'\nAND pickup_latitude BETWEEN 40.757816 AND 40.757832\nAND pickup_longitude BETWEEN -73.985143 AND -73.985105\n```\n\n![이미지](/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_8.png)\n\n특정 워크로드에 유용한지 확인하기 위해 철저한 테스트와 벤치마킹이 필요하지만, 전반적으로 Delta Lake 테이블의 관리를 간편하게 해주는 Liquid 클러스터링은 매우 유망해 보입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마무리\n\n리퀴드 클러스터링을 사용할 때 고려해야 할 측면이 많으며 특정 사용 사례에 맞는 동작을 조정하기 위해 많은 구성 값을 조정해야 할 것입니다. 본 게시물은 리퀴드 클러스터링이 어떻게 작동하는지를 높은 수준에서 시각적으로 보여주는 작은 시도입니다. 단순화된 사용 사례는 Hive 스타일의 파티셔닝과 Z-Order의 혜택을 결합하여 단일 최적화 방법을 사용하는 것입니다. \n\nliquid_db를 삭제하고 정리하려면 DROP DATABASE liquid_db CASCADE를 실행하는 것을 잊지 마십시오.\n\n# 추가 읽을거리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 델타 테이블에 리퀴드 클러스터링 사용하기\n- [디자인 문서] [공개] 리퀴드 클러스터링 — Google Docs\n- 힐버트 곡선 코딩 (youtube.com)\n- Yousry Mohamed의 미디엄에서 A부터 Z까지의 델타 레이크 Z-오더링","ogImage":{"url":"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png"},"coverImage":"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png","tag":["Tech"],"readingTime":11},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\",\n    ul: \"ul\",\n    li: \"li\",\n    ol: \"ol\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"최소한의 노력으로 레이크하우스 데이터 저장 레이아웃을 최적화하는 방법\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_0.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"소개\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"데이터 레이크하우스는 오픈 테이블 형식을 사용하고 특정 공급 업체에 얽매이지 않아 장점을 누립니다. 그러나 이는 특정 읽기 및 쓰기 작업을 위해 데이터 처리를 최적화하기 위해 파일 저장 레이아웃을 최적화해야 한다는 추가적인 부담과 함께 옵니다. 읽기 또는 쓰기 작업에 의해 처리되는 데이터 양을 최소화하기 위해 가능한 한 많은 파일을 제거함으로써 작업을 효율적으로 만드는 것이 핵심 아이디어입니다. 제거는 특정 파일이 해당 쿼리에 관련이 없다는 암묵적 또는 명시적 메타데이터를 사용하여 발생합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"옛날에, Hive은 각 데이터 파티션을 HDFS 또는 클라우드 저장소의 단일 폴더로 범위를 지정하여 유명한 Hive 스타일의 파티셔닝을 소개했습니다. 그것은 작동이 잘 됩니다. 그러나 작은 파일 문제가 발생하거나 워크로드 특성 변경으로 인해 파티션 체계를 변경해야 할 때 문제가 발생합니다. 또한 Hive 스타일의 파티셔닝은 고 카디널리티 질의에 대해 도움이 되지 않습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"오픈 테이블 형식에서 제공되는 DML 지원으로 인해, 조각 모음이나 GDPR에서 잊혀져야 할 권리와 같은 경우를 관리하기 위해 단일 또는 몇 개의 레코드를 업데이트/삭제하는 것이 매우 일반적해졌습니다. 이러한 시나리오에는 고 카디널리티 질의가 효율적이어야 합니다. 이러한 요구 사항을 충족하기 위해 Delta Lake Z-Ordering과 같은 기술이 소개되었습니다. Z-Ordering은 꽤 좋지만 OPTIMIZE 명령을 다시 실행할 때 전체 테이블(또는 파티션)을 최적화하는 반복적인 노력과 많은 낭비된 컴퓨팅 파워를 도입하는 일부 제한 사항이 있습니다. Delta Lake Z-Order의 더 자세한 탐구를 위해 Z-Order에 대한 저의 글을 살펴보십시오. 그 글에서는 낮은 수준의 세부 사항도 약간 논의됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Hive 스타일의 파티셔닝과 Z-Ordering의 이러한 제한 사항을 완화하기 위해 Databricks 및 Delta Lake 팀은 액체 클러스터링을 소개했습니다. 작성 시점에서 Delta Lake on Databricks에서는 아직 미리보기 상태이며 OSS Delta Lake에서는 실험적인 기능인 상태입니다. 그러나 설계 문서는 누구나 읽을 수 있습니다. 액체 클러스터링은 레코드-파일 할당 방법으로 Hilbert Curve를 사용할 것으로 예상됩니다. 액체 클러스터링의 비전은 다음과 같은 단일 최적화 기술을 가지는 것입니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"저 및 고 카디널리티 질의에 모두 잘 작동합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"\\\"이미 최적화된\\\" 파일을 최적화할 필요가 없습니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"클러스터링 열을 변경하면 전체 테이블을 다시 빌드할 필요가 없습니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 게시물에서는 액체 클러스터링이 레코드를 파일에 할당하는 방식을 시각적으로 보여줄 것입니다. 목표는 기술적인 세부 내용을 파헤치는 대신에 해당 기술의 매우 높은 수준의 이해를 갖는 데 초점을 맞추는 것입니다. 여전히 상황이 조리실에 있기 때문에 세부 사항에 대해 심층적으로 파고들지 않습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"기본으로 돌아가기 — 레코드를 파일에 할당하는 방법은?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 N개의 레코드가 있고 이를 M개의 파일에 쓰려고 한다고 가정해 봅시다. 파일 가지치기의 아이디어를 기억한다면, 비슷한 레코드를 동일한 파일에 저장하는 것이 필수적입니다. 작업 부하에 따라 비슷한 레코드는 같은 픽업 동네의 택시 여행이거나 같은 고객의 은행 거래일 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래 streamlit 앱은 이 문제를 처리하는 데 3가지 방법을 보여줍니다. N 및 M에 대한 다양한 값을 사용하고 배치 방식을 조정하여 레코드가 파일에 할당되는 방식을 시각적으로 확인할 수 있습니다. 이 간단한 앱에서는 모든 레코드가 필드 그룹을 갖고 있지만 우리는 2차원 평면 상 좌표인 x 및 y라는 두 개의 정수 필드에 대한 쿼리를 최적화해야 합니다. 레코드는 N개의 레코드를 생성하도록 x와 y의 쌍별 조합을 균등하게 다루기 위해 생성됩니다. 배치 방법 선택에 따라 각 지점(레코드)이 특정 파일에 할당되며 이 할당은 파일 색상을 사용하여 지정됩니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"랜덤 할당\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 방식은 사용자 정의 로직을 거의 사용하지 않습니다. 레코드가 무작위로 파일에 할당됩니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_1.png\",\n        alt: \"Image\"\n      })\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Z-Ordering 할당\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음 방법은 Z-Ordering을 사용하여 각 레코드 (x와 y의 조합)마다 Z-Order 값을 계산하는 것입니다. 이는 평면 상의 이차원 점을 선 상의 점으로 효과적으로 변환합니다. 그런 다음 선을 M개의 세그먼트로 나눌 수 있으며, 각 세그먼트는 하나의 파일을 나타냅니다. 레코드가 Z-Order 값 z를 갖고 있다면, 파일 z % M에 할당됩니다. 이제 점들은 일차원 관련 값을 갖고 있기 때문에, 그러한 값들을 선으로 연결하여 매핑이 어떻게 이루어지는지 시각적으로 확인할 수 있습니다. 각 점 위에 마우스를 올려놓으면 선형 순서 값을 볼 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_2.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"공간적으로 서로 가까운 레코드들은 Z-Order 라인 상에서 서로 가까이 배치됩니다. 예를 들어, 위 스크린샷을 보면, 점 (2,4)와 (3,4)는 각각 36과 37의 Z-Order 값을 가지고 있습니다. (0,4)에서 (7,3)으로 이동하는 것과 같이, 공간적으로 멀리 떨어져 있지만 연이은 Z-Order 값을 가진 큰 점프가 보이기도 합니다. 그럼에도 불구하고, Z-Ordering은 좋은 데이터 로컬리티 할당을 생성합니다.\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"힐버트 곡선 할당\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"세 번째 방법은 레코드마다 두 차원 x와 y 값에 기초한 일차원 값을 할당하기 위해 힐버트 곡선을 사용하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 Python 라이브러리를 사용하여 두 차원 점에 대한 힐버트 곡선 값을 도출했습니다. 이는 Z-Order와 비슷한데, 서로 가까운 포인트들은 동일한 파일에 들어가게 되지만, 일차원 할당에서 멀리 떨어진 지점이 연속적으로 배치되는 급격한 점프가 없다는 추가적인 이점이 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이제 우리는 Z-Order와 힐버트 곡선과 같은 공간 채우기 곡선을 사용하여 파일에 포인트를 할당하는 방법에 대한 아이디어가 생겼으니, Databricks에서 Liquid Clustering을 살펴보겠습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"Liquid clustering in action\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 섹션의 실험은 최소한이지만 대표적입니다. 필요한 설정은 다음과 같습니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Azure 무료 평가판 계정 및 무료 Databricks 계정\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Liqud 클러스터링을 지원하는 최신 DBR인 DBR 13.3을 사용하는 단일 노드 Databricks 클러스터\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"빠른 속도와 모자이크가 작동하기 위해 클러스터에서 photon을 활성화\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 유명한 뉴욕시 택시 데이터 세트를 사용하고 아래 워크로드를 위해 최적화할 것입니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"대부분의 쿼리는 데이터의 한 해 또는 몇 해에 대해서 작동할 것입니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"많은 쿼리는 픽업 위치(위도 및 경도)를 기반으로 필터링하는 것이 포함될 것입니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Databricks 워크스페이스에 파이썬 노트북을 만들고 샌드박스 데이터베이스를 생성하는 방법을 시작해보세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"%sql\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"CREATE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"DATABASE\"\n        }), \" liquid_db;\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음으로, 맨해튼 섬 주변의 경계 상자를 기준으로 뉴욕시 택시 데이터셋을 기반으로 하는 테이블을 생성해보세요. 이 글의 몇 가지 미학적 이유로 테이블은 초기에 액체 클러스터링을 사용할 수 있지만, 모든 작업이 기본적으로 데이터를 클러스터링하는 것은 아님을 인식하셔야 합니다. 예를 들어, 데이터가 MERGE 작업으로 변경되면, 데이터를 클러스터링하기 위해 OPTIMIZE 작업을 실행해야 할 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-sql\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"CREATE\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"TABLE\"\n        }), \" liquid_db.trips \\n    CLUSTER \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"BY\"\n        }), \" (pickup_datetime, pickup_latitude, pickup_longitude)\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"AS\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"SELECT\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-operator\",\n          children: \"*\"\n        }), \" \\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"FROM\"\n        }), \" delta.`dbfs:\", _jsx(_components.span, {\n          className: \"hljs-operator\",\n          children: \"/\"\n        }), \"databricks\", _jsx(_components.span, {\n          className: \"hljs-operator\",\n          children: \"-\"\n        }), \"datasets\", _jsx(_components.span, {\n          className: \"hljs-operator\",\n          children: \"/\"\n        }), \"nyctaxi\", _jsx(_components.span, {\n          className: \"hljs-operator\",\n          children: \"/\"\n        }), \"tables\", _jsx(_components.span, {\n          className: \"hljs-operator\",\n          children: \"/\"\n        }), \"nyctaxi_yellow`\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"WHERE\"\n        }), \"\\n    pickup_longitude \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"between\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"-74.05186503267184\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"and\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"-73.83200446816883\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"AND\"\n        }), \"\\n    pickup_latitude \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"between\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"40.69286486137213\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"and\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"40.91947608519337\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"클러스터링 열 목록에서 첫 번째 열은 타임스탬프 열인 픽업 일시임을 주목해주세요. 우리는 하이브 스타일의 파티셔닝을 사용하기 위해 명시적으로 연도 열을 생성할 필요가 없습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"나중에 특정 Delta Lake 트랜잭션에서 생성된 파일이 클러스터링되었는지 여부를 감지하는 방법을 보여줄 텐데요, 제 경우에는 파일이 액체 클러스터링되지 않았기 때문에 직접 클러스터링해야 했습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-sql\",\n        children: \"OPTIMIZE liquid_db.trips\\n\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"델타 레이크 Z-오더링과 달리, 리퀴드 클러스터링을 사용하여 테이블을 최적화할 때, 파일이 최적화되었는지 여부를 알려주는 트랜잭션 로그 메타데이터가 있습니다. 따라서 나중에 OPTIMIZE 명령을 실행하여 새로운 데이터를 클러스터링할 때 파일을 건너뛸 수 있습니다. 이러한 경우에 대해 더 많은 아이디어가 있지만, 핵심적인 차이점은 ADD 프로토콜 액션의 태그 부분에 LIQUID_METADATA_ID라는 새 메타데이터 항목이 있는 것입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"functions\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" F\\nsecond_log_file = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"dbfs:/user/hive/warehouse/liquid_db.db/trips/_delta_log/00000000000000000001.json\\\"\"\n        }), \"\\n(\\n    spark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"read\"\n        }), \"\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"json\"\n        }), \"(second_log_file)\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"where\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add is not null\\\"\"\n        }), \")\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"select\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add.size\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add.tags.*\\\"\"\n        }), \")\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"withColumn\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"size\\\"\"\n        }), \", F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"expr\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"cast(size/1024/1024 as int)\\\"\"\n        }), \"))\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"withColumnRenamed\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"size\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"size_mb\\\"\"\n        }), \")\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"()\\n)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위 스니펫의 출력에서 제 경우 191개의 파일이 나오며 대부분의 크기는 100에서 300MB 범위에 있습니다.\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_4.png\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이제 OPTIMIZE 작업을 위해 트랜잭션 로그 엔트리 안에 수집된 최대 및 최소 메타데이터 값을 검토해 봅시다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" pyspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"sql\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"functions\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" F\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"load_stats_from_commit\"\n        }), \"(commit_file):\\n  df = spark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"read\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"json\"\n        }), \"(commit_file).\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"where\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add is not null\\\"\"\n        }), \")\\n  add_schema = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n  struct\\n    \u003c\\n      numRecords:long,\\n      minValues: struct\u003cpickup_latitude: double,pickup_longitude: double, pickup_datetime: timestamp\u003e,\\n      maxValues: struct\u003cpickup_latitude: double,pickup_longitude: double, pickup_datetime: timestamp\u003e\\n    \u003e\\n  \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"\\n\\n  stats = (\\n    df\\n      .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"select\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add.path\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add.size\\\"\"\n        }), \", \\n          F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_json\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"add.stats\\\"\"\n        }), \", add_schema).\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"alias\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats\\\"\"\n        }), \")\\n      )\\n      .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"selectExpr\"\n        }), \"(\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"substring(path, 1, 10) as file\\\"\"\n        }), \",\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"size\\\"\"\n        }), \", \\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats.minValues.pickup_datetime as min_pickup_datetime\\\"\"\n        }), \",\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats.maxValues.pickup_datetime as max_pickup_datetime\\\"\"\n        }), \",\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats.minValues.pickup_latitude as min_pickup_latitude\\\"\"\n        }), \", \\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats.maxValues.pickup_latitude as max_pickup_latitude\\\"\"\n        }), \",\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats.minValues.pickup_longitude as min_pickup_longitude\\\"\"\n        }), \",\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"stats.maxValues.pickup_longitude as max_pickup_longitude\\\"\"\n        }), \"\\n      )\\n  )\\n\\n  stats = (\\n    stats\\n      .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"withColumn\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"rect\\\"\"\n        }), \", F.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"expr\"\n        }), \"(\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n          concat('POLYGON ((' , \\n            min_pickup_longitude, ' ', min_pickup_latitude, ',' ,\\n            max_pickup_longitude, ' ', min_pickup_latitude, ',' ,\\n            max_pickup_longitude, ' ', max_pickup_latitude, ',' ,\\n            min_pickup_longitude, ' ', max_pickup_latitude, ',' ,\\n            min_pickup_longitude, ' ', min_pickup_latitude, \\n          '))'\\n          )\\n      \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"))\\n  )\\n\\n  \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" stats\\n\\nstats = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"load_stats_from_commit\"\n        }), \"(second_log_file).\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"orderBy\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"min_pickup_datetime\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"max_pickup_datetime\\\"\"\n        }), \")\\nstats.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"display\"\n        }), \"()\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 \\\"난해한\\\" 코드 스니펫은 몇 가지 작업을 수행합니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"픽업 시간, 위도 및 경도에 대한 최소 및 최대 값 수집\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"가독성 목적을 위해 파일 이름의 처음 10자를 고유 식별기로 사용\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"파일 내의 모든 여행을 포함하는 경계 상자의 GeoJSON 표현 생성 (픽업 위치에 따라)\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"상단의 표 출력물은 특별히 흥미로운 것은 아니며, 파일이 클러스터링 열에 따라 어떻게 배치되었는지 쉽게 전달하지 않습니다. 그러나 이를 어떤 종류의 간트 차트로 시각화한다면, 파일이 시간별 범위를 포함하는 그룹으로 클러스터링되었음이 명백해질 것입니다. 파일 중첩이 발생할 수 있지만, 일반적인 주제는 시간 범위를 기반으로 한 클러스터링을 보여줍니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" plotly.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"express\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" px\\nfig = px.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"timeline\"\n        }), \"(stats.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"toPandas\"\n        }), \"(), \\n    x_start=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"min_pickup_datetime\\\"\"\n        }), \", \\n    x_end=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"max_pickup_datetime\\\"\"\n        }), \",\\n    y=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"file\\\"\"\n        }), \")\\nfig.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"update_yaxes\"\n        }), \"(categoryorder=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"min ascending\\\"\"\n        }), \")\\nfig.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"show\"\n        }), \"()\\n\"]\n      })\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_5.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Jan 2009부터 April 2010까지의 파일을 살펴보겠습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"file_group = (\\n  stats\\n    .\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"where\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\n    min_pickup_datetime \u003e= '2009-01-01' AND \\n    max_pickup_datetime \u003c= '2010-04-30'\\n    \\\"\"\n        }), _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\\\"\"\n        }), \"\\n    )\\n)\\nfile_group.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"count\"\n        }), \"()\\n# \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"29\"\n        }), \"개의 파일이 인쇄됩니다.\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"해당 Date Range를 공유하는 이 파일들이 커버하는 지리 공간 영역을 시각화하고 싶습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"%pip install databricks-mosaic==\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"0.4\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".0\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" mosaic \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" mos\\nspark.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"conf\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"set\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"spark.databricks.labs.mosaic.index.system\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"H3\\\"\"\n        }), \")\\nmos.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"enable_mosaic\"\n        }), \"(spark, dbutils)\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"%%mosaic_kepler\\nfile_group \\\"rect\\\" \\\"geometry\\\"\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_6.png\",\n        alt: \"Image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"동일한 파일 그룹에 대해 특정 날짜 범위를 포괄하는 파일들의 경우, 해당 파일들의 레코드는 잔여 클러스터링 키인 위도 및 경도를 기반으로 클러스터링됩니다. 이러한 공간 클러스터링을 통해 지구상의 특정 지점을 커버하는 파일 수가 현저히 줄어들어 파일 가지치기가 크게 향상됩니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"가지치기 혜택\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래와 같은 쿼리를 실행하면 194개 파일 중 126개 파일을 제거합니다 (첫 번째 커밋에서 최적화되지 않은 파일이 3개 발생했습니다).\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"SELECT\"\n        }), \" payment_type, \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"sum\"\n        }), \"(total_amount) \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"as\"\n        }), \" total_amount\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"FROM\"\n        }), \" liquid_db.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"trips\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"WHERE\"\n        }), \" pickup_datetime \u003e= \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'2011-01-01'\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"AND\"\n        }), \" pickup_datetime \u003c \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'2012-01-01'\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"GROUP\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BY\"\n        }), \" payment_type\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_7.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 쿼리는 8년 데이터 중 1년치의 집계 결과입니다. 순수 Hive 파티셔닝이면 더 좋은 프루닝이 가능할 수도 있지만, 여전히 집계 쿼리에 대한 일정한 값은 얻을 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"만약 같은 해를 사용하지만 타임스 스퀘어 근처의 몇 가지 특정 레코드를 찾아보려 한다면, 더 나은 가지치기를 할 수 있어요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"SELECT\"\n        }), \" *\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"FROM\"\n        }), \" liquid_db.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"trips\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"WHERE\"\n        }), \" pickup_datetime \u003e= \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'2011-01-01'\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"AND\"\n        }), \" pickup_datetime \u003c \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'2012-01-01'\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"AND\"\n        }), \" pickup_latitude \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BETWEEN\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"40.757816\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"AND\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"40.757832\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"AND\"\n        }), \" pickup_longitude \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"BETWEEN\"\n        }), \" -\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"73.985143\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"AND\"\n        }), \" -\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"73.985105\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation_8.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"특정 워크로드에 유용한지 확인하기 위해 철저한 테스트와 벤치마킹이 필요하지만, 전반적으로 Delta Lake 테이블의 관리를 간편하게 해주는 Liquid 클러스터링은 매우 유망해 보입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"마무리\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"리퀴드 클러스터링을 사용할 때 고려해야 할 측면이 많으며 특정 사용 사례에 맞는 동작을 조정하기 위해 많은 구성 값을 조정해야 할 것입니다. 본 게시물은 리퀴드 클러스터링이 어떻게 작동하는지를 높은 수준에서 시각적으로 보여주는 작은 시도입니다. 단순화된 사용 사례는 Hive 스타일의 파티셔닝과 Z-Order의 혜택을 결합하여 단일 최적화 방법을 사용하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"liquid_db를 삭제하고 정리하려면 DROP DATABASE liquid_db CASCADE를 실행하는 것을 잊지 마십시오.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"추가 읽을거리\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"델타 테이블에 리퀴드 클러스터링 사용하기\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"[디자인 문서] [공개] 리퀴드 클러스터링 — Google Docs\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"힐버트 곡선 코딩 (youtube.com)\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Yousry Mohamed의 미디엄에서 A부터 Z까지의 델타 레이크 Z-오더링\"\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-DeltaLakeLiquidClusteringAvisualexplanation"},"buildId":"PgdIX9e0tvkvkdAmDT6qR","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>