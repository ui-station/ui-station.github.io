<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile" data-gatsby-head="true"/><meta name="twitter:title" content="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 20:07" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_buildManifest.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">12<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><h1>소개</h1>
<p>2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.</p>
<p>언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.</p>
<p>하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png"/>
<p>시작하기 전에, 먼저 파이프라인을 이해해 봅시다.</p>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png"/>
<p>자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.</p>
<div class="content-ad"></div>
<h2>섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기</h2>
<p>단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.</p>
<pre><code class="hljs language-js"># 저장소를 복제합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/NSTiwari/Llama3-on-Mobile</span>
</code></pre>
<p>단계 1: MLC-LLM 설치하기</p>
<div class="content-ad"></div>
<p>모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.</p>
<pre><code class="hljs language-js">!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f <span class="hljs-attr">https</span>:<span class="hljs-comment">//mlc.ai/wheels</span>
!pip install numpy==<span class="hljs-number">1.23</span><span class="hljs-number">.5</span>
</code></pre>
<p>단계 2: 라이브러리 가져오기</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> mlc_llm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> snapshot_download
</code></pre>
<div class="content-ad"></div>
<p>Step 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요</p>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">HF</span> 계정에 로그인합니다.
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login
<span class="hljs-title function_">notebook_login</span>()

# <span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> 모델을 다운로드합니다.
<span class="hljs-title function_">snapshot_download</span>(repo_id=<span class="hljs-string">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>, local_dir=<span class="hljs-string">&quot;/content/Llama-3-8B-Instruct/&quot;</span>)
</code></pre>
<p>Step 4: GPU가 활성화되었는지 확인하세요</p>
<pre><code class="hljs language-js">!nvidia-smi

# <span class="hljs-variable constant_">CUDA</span>가 사용 가능한지 확인합니다.
torch.<span class="hljs-property">cuda</span>.<span class="hljs-title function_">is_available</span>()

device = torch.<span class="hljs-title function_">device</span>(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.<span class="hljs-property">cuda</span>.<span class="hljs-title function_">is_available</span>() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)
device
</code></pre>
<div class="content-ad"></div>
<p>Step 5: 모델 이름과 양자화 유형 구성</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">MODEL_NAME</span> = <span class="hljs-string">&quot;Llama-3-8B-Instruct&quot;</span>
<span class="hljs-variable constant_">QUANTIZATION</span>= <span class="hljs-string">&quot;q4f16_1&quot;</span>
</code></pre>
<p>Step 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환</p>
<p>다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js">!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/
</code></pre>
<p>7단계: 토큰 파일 생성</p>
<p>이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.</p>
<pre><code class="hljs language-js">!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \
    --conv-template llama-<span class="hljs-number">3</span> --context-<span class="hljs-variable language_">window</span>-size <span class="hljs-number">8192</span> --prefill-chunk-size <span class="hljs-number">1024</span>  \
    -o /content/$MODEL_NAME-$QUANTIZATION-android/
</code></pre>
<div class="content-ad"></div>
<p>8단계: Android 형식으로 모델 컴파일하기</p>
<p>여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.</p>
<pre><code class="hljs language-js">!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.<span class="hljs-property">json</span> \
    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.<span class="hljs-property">tar</span>
</code></pre>
<p>9단계: 모델을 Hugging Face에 올리기 🤗</p>
<div class="content-ad"></div>
<p>마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> whoami
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> <span class="hljs-title class_">Path</span>

# 출력 디렉토리.
output_dir = <span class="hljs-string">&quot;/content/&quot;</span> + <span class="hljs-variable constant_">MODEL_NAME</span> + <span class="hljs-string">&quot;-&quot;</span> + <span class="hljs-variable constant_">QUANTIZATION</span> + <span class="hljs-string">&quot;-android/&quot;</span>
repo_name = <span class="hljs-string">&quot;Llama-3-8B-q4f16_1-android&quot;</span>
username = <span class="hljs-title function_">whoami</span>(token=<span class="hljs-title class_">Path</span>(<span class="hljs-string">&quot;/root/.cache/huggingface/&quot;</span>))[<span class="hljs-string">&quot;name&quot;</span>]
repo_id = f<span class="hljs-string">&quot;{username}/{repo_name}&quot;</span>
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> upload_folder, create_repo

repo_id = <span class="hljs-title function_">create_repo</span>(repo_id, exist_ok=<span class="hljs-title class_">True</span>).<span class="hljs-property">repo_id</span>
<span class="hljs-title function_">print</span>(output_dir)

<span class="hljs-title function_">upload_folder</span>(
    repo_id=repo_id,
    folder_path=output_dir,
    commit_message=<span class="hljs-string">&quot;Quantized Llama-3-8B-Instruct model for Android.&quot;</span>,
    ignore_patterns=[<span class="hljs-string">&quot;step_*&quot;</span>, <span class="hljs-string">&quot;epoch_*&quot;</span>],
)
</code></pre>
<p>다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:
https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android</p>
<div class="content-ad"></div>
<p>여기 완전한 Colab 노트북을 찾을 수 있습니다.</p>
<p>좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.</p>
<h2>Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)</h2>
<p>이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.</p>
<div class="content-ad"></div>
<p>윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.</p>
<p>저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.</p>
<p>비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.</p>
<h2>섹션 III: 빌드 종속성 설치</h2>
<div class="content-ad"></div>
<h3>단계 1: Rust 설치하기</h3>
<p>안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png" alt="이미지"/></p>
<p>표준 설치를 계속하려면 옵션 1을 선택하세요.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png" alt="image"/></p>
<pre><code class="hljs language-js">sudo curl --proto <span class="hljs-string">&#x27;=https&#x27;</span> --tlsv1<span class="hljs-number">.2</span> -sSf <span class="hljs-attr">https</span>:<span class="hljs-comment">//sh.rustup.rs | sh</span>
</code></pre>
<p>Step 2: Install NDK and CMake in Android Studio</p>
<p>Open Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png"/>
<p>제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치</p>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png"/>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">MLC</span>-<span class="hljs-variable constant_">LLM</span> <span class="hljs-title class_">Python</span> 패키지와 <span class="hljs-variable constant_">TVM</span> <span class="hljs-title class_">Unity</span> 컴파일러 설치.
python3 -m pip install --pre -U -f <span class="hljs-attr">https</span>:<span class="hljs-comment">//mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly</span>

# 아래 명령어를 사용하여 설치 확인:
python3 -c <span class="hljs-string">&quot;import mlc_llm; print(mlc_llm)&quot;</span>
</code></pre>
<div class="content-ad"></div>
<p><strong>단계 4: CMake 설치하기</strong></p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png" alt="이미지"/></p>
<pre><code class="hljs language-js"># <span class="hljs-title class_">CMake</span> 설치하기.
sudo apt-get install cmake
</code></pre>
<p><strong>단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기</strong></p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png"/>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png"/>
<pre><code class="hljs language-js">cd /home/tiwarinitin1999/  

# <span class="hljs-variable constant_">MLC</span>-<span class="hljs-variable constant_">LLM</span> 저장소를 복제합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/mlc-ai/mlc-llm.git</span>
cd mlc-llm

# 저장소의 서브모듈을 업데이트합니다.
git submodule update --init --recursive

# <span class="hljs-title class_">Llama3</span>-on-<span class="hljs-title class_">Mobile</span> 저장소를 복제합니다.
cd /home/tiwarinitin1999/
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/NSTiwari/Llama3-on-Mobile.git</span>
</code></pre>
<p>단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.</p>
<div class="content-ad"></div>
<p>MLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png" alt="이미지"/></p>
<pre><code class="hljs language-js">cd /home/tiwarinitin1999/mlc-llm/android/<span class="hljs-title class_">MLCChat</span>
mkdir dist
cd dist
mkdir prebuilt
cd prebuilt
</code></pre>
<p>그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png"/>
<pre><code class="hljs language-js"># 퀀터이즈된 <span class="hljs-title class_">Llama3</span>-8B-<span class="hljs-title class_">Instruct</span> 가중치의 <span class="hljs-variable constant_">HF</span> 리포지토리를 복제합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git</span>
</code></pre>
<p>7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.</p>
<p>dist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png" alt="image"/></p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">cd</span> /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist
<span class="hljs-built_in">mkdir</span> lib
<span class="hljs-built_in">cd</span> lib/
</code></pre>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png" alt="image"/></p>
<p>Step 8: mlc-package-config.json 파일 구성하기</p>
<div class="content-ad"></div>
<p>MLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:</p>
<pre><code class="hljs language-js">{
    <span class="hljs-string">&quot;device&quot;</span>: <span class="hljs-string">&quot;android&quot;</span>,
    <span class="hljs-string">&quot;model_list&quot;</span>: [
        {
            <span class="hljs-string">&quot;model&quot;</span>: <span class="hljs-string">&quot;Llama-3-8B-q4f16_1-android&quot;</span>,
            <span class="hljs-string">&quot;bundle_weight&quot;</span>: <span class="hljs-literal">true</span>,
            <span class="hljs-string">&quot;model_id&quot;</span>: <span class="hljs-string">&quot;llama-3-8b-q4f16_1&quot;</span>,
            <span class="hljs-string">&quot;model_lib&quot;</span>: <span class="hljs-string">&quot;llama-q4f16_1&quot;</span>,
            <span class="hljs-string">&quot;estimated_vram_bytes&quot;</span>: <span class="hljs-number">4348727787</span>,
            <span class="hljs-string">&quot;overrides&quot;</span>: {
                <span class="hljs-string">&quot;context_window_size&quot;</span>:<span class="hljs-number">768</span>,
                <span class="hljs-string">&quot;prefill_chunk_size&quot;</span>:<span class="hljs-number">256</span>
            }         
        }
    ],
    <span class="hljs-string">&quot;model_lib_path_for_prepare_libs&quot;</span>: {
        <span class="hljs-string">&quot;llama-q4f16_1&quot;</span>: <span class="hljs-string">&quot;./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar&quot;</span>
    }
}
</code></pre>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png" alt="이미지"/></p>
<p>9단계: 경로에 환경 변수 설정하기</p>
<div class="content-ad"></div>
<html><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png"/></html>
<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-variable constant_">ANDROID_NDK</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/<span class="hljs-title class_">Android</span>/<span class="hljs-title class_">Sdk</span>/ndk/<span class="hljs-number">27.0</span><span class="hljs-number">.11718014</span>
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">TVM_NDK_CC</span>=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">TVM_HOME</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/mlc-llm/3rdparty/tvm
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">JAVA_HOME</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/<span class="hljs-title class_">Downloads</span>/android-studio/jbr
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">MLC_LLM_HOME</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/mlc-llm
</code></pre>
<p>Step 10: 안드로이드 빌드 파일 생성</p>
<p>마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-sh"><span class="hljs-built_in">cd</span> /home/tiwarinitin1999/mlc-llm/android/MLCChat
python3 -m mlc_llm package
</code></pre>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png" alt="Screenshot"/></p>
<p>명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png" alt="Screenshot"/></p>
<pre><code>
&lt;div class=&quot;content-ad&quot;&gt;&lt;/div&gt;

위 명령은 다음 파일을 생성합니다:

![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png)

소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:

소스 디렉토리:
/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output

&lt;div class=&quot;content-ad&quot;&gt;&lt;/div&gt;

목적지 디렉토리:
/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output

이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:

```js
{
  &quot;model_list&quot;: [
    {
      &quot;model_id&quot;: &quot;llama-3-8b-q4f16_1&quot;,
      &quot;model_lib&quot;: &quot;llama-q4f16_1&quot;,
      &quot;model_url&quot;: &quot;https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android&quot;,
      &quot;estimated_vram_bytes&quot;: 4348727787
    }
  ]
}
</code></pre>
<p>설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png"/>
<p>모든 구성이 설정되었습니다.</p>
<h2>섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기</h2>
<p>안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.</p>
<div class="content-ad"></div>
<p>[<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png" alt="image"/>]
(https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif)</p>
<p>모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.</p>
<p>오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.</p>
<div class="content-ad"></div>
<p>위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.
https://github.com/NSTiwari/Llama3-on-Mobile</p>
<p>작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.</p>
<h2>감사의 글</h2>
<p>MobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.</p>
<div class="content-ad"></div>
<h2>참고 자료 및 자원</h2>
<ul>
<li>Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북</li>
<li>MobileLlama3 GitHub 저장소</li>
<li>변환된 가중치를 위한 HuggingFace 저장소</li>
<li>Meta사의 Llama3 모델들</li>
<li>MLC-LLM</li>
<li>MLC-LLM용 Android SDK</li>
</ul></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기","description":"","date":"2024-05-18 20:07","slug":"2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile","content":"\n\n# 소개\n\n2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.\n\n언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.\n\n하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png\" /\u003e\n\n시작하기 전에, 먼저 파이프라인을 이해해 봅시다.\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png\" /\u003e\n\n자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기\n\n단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.\n\n```js\n# 저장소를 복제합니다.\ngit clone https://github.com/NSTiwari/Llama3-on-Mobile\n```\n\n단계 1: MLC-LLM 설치하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.\n\n```js\n!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f https://mlc.ai/wheels\n!pip install numpy==1.23.5\n```\n\n단계 2: 라이브러리 가져오기\n\n```js\nimport mlc_llm\nimport torch\nfrom huggingface_hub import snapshot_download\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요\n\n```js\n# HF 계정에 로그인합니다.\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n# Llama-3-8B-Instruct 모델을 다운로드합니다.\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"/content/Llama-3-8B-Instruct/\")\n```\n\nStep 4: GPU가 활성화되었는지 확인하세요\n\n```js\n!nvidia-smi\n\n# CUDA가 사용 가능한지 확인합니다.\ntorch.cuda.is_available()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 5: 모델 이름과 양자화 유형 구성\n\n```js\nMODEL_NAME = \"Llama-3-8B-Instruct\"\nQUANTIZATION= \"q4f16_1\"\n```\n\nStep 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환\n\n다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/\n```\n\n7단계: 토큰 파일 생성\n\n이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.\n\n```js\n!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \\\n    --conv-template llama-3 --context-window-size 8192 --prefill-chunk-size 1024  \\\n    -o /content/$MODEL_NAME-$QUANTIZATION-android/\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n8단계: Android 형식으로 모델 컴파일하기\n\n여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.\n\n```js\n!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.json \\\n    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.tar\n```\n\n9단계: 모델을 Hugging Face에 올리기 🤗\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.\n\n```js\nfrom huggingface_hub import whoami\nfrom pathlib import Path\n\n# 출력 디렉토리.\noutput_dir = \"/content/\" + MODEL_NAME + \"-\" + QUANTIZATION + \"-android/\"\nrepo_name = \"Llama-3-8B-q4f16_1-android\"\nusername = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\nrepo_id = f\"{username}/{repo_name}\"\n```\n  \n```js\nfrom huggingface_hub import upload_folder, create_repo\n\nrepo_id = create_repo(repo_id, exist_ok=True).repo_id\nprint(output_dir)\n\nupload_folder(\n    repo_id=repo_id,\n    folder_path=output_dir,\n    commit_message=\"Quantized Llama-3-8B-Instruct model for Android.\",\n    ignore_patterns=[\"step_*\", \"epoch_*\"],\n)\n```\n\n다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:\nhttps://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기 완전한 Colab 노트북을 찾을 수 있습니다.\n\n좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.\n\n## Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)\n\n이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.\n\n저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.\n\n비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.\n\n## 섹션 III: 빌드 종속성 설치\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n### 단계 1: Rust 설치하기\n\n안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png)\n\n표준 설치를 계속하려면 옵션 1을 선택하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png)\n\n```js\nsudo curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nStep 2: Install NDK and CMake in Android Studio\n\nOpen Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png\" /\u003e\n\n제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png\" /\u003e\n\n```js\n# MLC-LLM Python 패키지와 TVM Unity 컴파일러 설치.\npython3 -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly\n\n# 아래 명령어를 사용하여 설치 확인:\npython3 -c \"import mlc_llm; print(mlc_llm)\"\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**단계 4: CMake 설치하기**\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png)\n\n```js\n# CMake 설치하기.\nsudo apt-get install cmake\n```\n\n**단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기**  \n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png\" /\u003e\n\n```js\ncd /home/tiwarinitin1999/  \n\n# MLC-LLM 저장소를 복제합니다.\ngit clone https://github.com/mlc-ai/mlc-llm.git\ncd mlc-llm\n\n# 저장소의 서브모듈을 업데이트합니다.\ngit submodule update --init --recursive\n\n# Llama3-on-Mobile 저장소를 복제합니다.\ncd /home/tiwarinitin1999/\ngit clone https://github.com/NSTiwari/Llama3-on-Mobile.git\n```\n\n단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png)\n\n```js\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat\nmkdir dist\ncd dist\nmkdir prebuilt\ncd prebuilt\n```\n\n그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png\" /\u003e\n\n```js\n# 퀀터이즈된 Llama3-8B-Instruct 가중치의 HF 리포지토리를 복제합니다.\ngit clone https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git\n```\n\n7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.\n\ndist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png)\n\n```bash\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist\nmkdir lib\ncd lib/\n```\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png)\n\nStep 8: mlc-package-config.json 파일 구성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n    \"device\": \"android\",\n    \"model_list\": [\n        {\n            \"model\": \"Llama-3-8B-q4f16_1-android\",\n            \"bundle_weight\": true,\n            \"model_id\": \"llama-3-8b-q4f16_1\",\n            \"model_lib\": \"llama-q4f16_1\",\n            \"estimated_vram_bytes\": 4348727787,\n            \"overrides\": {\n                \"context_window_size\":768,\n                \"prefill_chunk_size\":256\n            }         \n        }\n    ],\n    \"model_lib_path_for_prepare_libs\": {\n        \"llama-q4f16_1\": \"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar\"\n    }\n}\n```\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png)\n\n9단계: 경로에 환경 변수 설정하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003chtml\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png\" /\u003e\n\u003c/html\u003e\n\n```js\nexport ANDROID_NDK=/home/tiwarinitin1999/Android/Sdk/ndk/27.0.11718014\nexport TVM_NDK_CC=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang\nexport TVM_HOME=/home/tiwarinitin1999/mlc-llm/3rdparty/tvm\nexport JAVA_HOME=/home/tiwarinitin1999/Downloads/android-studio/jbr\nexport MLC_LLM_HOME=/home/tiwarinitin1999/mlc-llm\n```\n\nStep 10: 안드로이드 빌드 파일 생성\n\n마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sh\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat\npython3 -m mlc_llm package\n```\n\n![Screenshot](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png)\n\n명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.\n\n![Screenshot](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 명령은 다음 파일을 생성합니다:\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png)\n\n소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:\n\n소스 디렉토리:\n/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n목적지 디렉토리:\n/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output\n\n이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n  \"model_list\": [\n    {\n      \"model_id\": \"llama-3-8b-q4f16_1\",\n      \"model_lib\": \"llama-q4f16_1\",\n      \"model_url\": \"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\",\n      \"estimated_vram_bytes\": 4348727787\n    }\n  ]\n}\n```\n\n설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png\" /\u003e\n\n모든 구성이 설정되었습니다.\n\n## 섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기\n\n안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n[![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png)]\n(https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif)\n\n모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.\n\n오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.\nhttps://github.com/NSTiwari/Llama3-on-Mobile\n\n작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.\n\n## 감사의 글\n\nMobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 참고 자료 및 자원\n\n- Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북\n- MobileLlama3 GitHub 저장소\n- 변환된 가중치를 위한 HuggingFace 저장소\n- Meta사의 Llama3 모델들\n- MLC-LLM\n- MLC-LLM용 Android SDK","ogImage":{"url":"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png"},"coverImage":"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png","tag":["Tech"],"readingTime":12},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h1: \"h1\",\n    p: \"p\",\n    h2: \"h2\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\",\n    h3: \"h3\",\n    img: \"img\",\n    strong: \"strong\",\n    ul: \"ul\",\n    li: \"li\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h1, {\n      children: \"소개\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"시작하기 전에, 먼저 파이프라인을 이해해 봅시다.\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# 저장소를 복제합니다.\\ngit clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//github.com/NSTiwari/Llama3-on-Mobile\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 1: MLC-LLM 설치하기\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//mlc.ai/wheels\"\n        }), \"\\n!pip install numpy==\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1.23\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".5\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 2: 라이브러리 가져오기\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" mlc_llm\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" torch\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" huggingface_hub \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" snapshot_download\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"HF\"\n        }), \" 계정에 로그인합니다.\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" huggingface_hub \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" notebook_login\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"notebook_login\"\n        }), \"()\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Llama\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \"-8B-\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Instruct\"\n        }), \" 모델을 다운로드합니다.\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"snapshot_download\"\n        }), \"(repo_id=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"meta-llama/Meta-Llama-3-8B-Instruct\\\"\"\n        }), \", local_dir=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/content/Llama-3-8B-Instruct/\\\"\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 4: GPU가 활성화되었는지 확인하세요\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"!nvidia-smi\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"CUDA\"\n        }), \"가 사용 가능한지 확인합니다.\\ntorch.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"cuda\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"is_available\"\n        }), \"()\\n\\ndevice = torch.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"device\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"cuda:0\\\"\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"if\"\n        }), \" torch.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"cuda\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"is_available\"\n        }), \"() \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"else\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"cpu\\\"\"\n        }), \")\\ndevice\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 5: 모델 이름과 양자화 유형 구성\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MODEL_NAME\"\n        }), \" = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Llama-3-8B-Instruct\\\"\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"QUANTIZATION\"\n        }), \"= \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"q4f16_1\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        className: \"hljs language-js\",\n        children: \"!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"7단계: 토큰 파일 생성\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \\\\\\n    --conv-template llama-\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"3\"\n        }), \" --context-\", _jsx(_components.span, {\n          className: \"hljs-variable language_\",\n          children: \"window\"\n        }), \"-size \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"8192\"\n        }), \" --prefill-chunk-size \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1024\"\n        }), \"  \\\\\\n    -o /content/$MODEL_NAME-$QUANTIZATION-android/\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"8단계: Android 형식으로 모델 컴파일하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"json\"\n        }), \" \\\\\\n    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"tar\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"9단계: 모델을 Hugging Face에 올리기 🤗\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" huggingface_hub \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" whoami\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" pathlib \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Path\"\n        }), \"\\n\\n# 출력 디렉토리.\\noutput_dir = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/content/\\\"\"\n        }), \" + \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MODEL_NAME\"\n        }), \" + \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"-\\\"\"\n        }), \" + \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"QUANTIZATION\"\n        }), \" + \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"-android/\\\"\"\n        }), \"\\nrepo_name = \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Llama-3-8B-q4f16_1-android\\\"\"\n        }), \"\\nusername = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"whoami\"\n        }), \"(token=\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Path\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"/root/.cache/huggingface/\\\"\"\n        }), \"))[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"name\\\"\"\n        }), \"]\\nrepo_id = f\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"{username}/{repo_name}\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" huggingface_hub \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" upload_folder, create_repo\\n\\nrepo_id = \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"create_repo\"\n        }), \"(repo_id, exist_ok=\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"True\"\n        }), \").\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"repo_id\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"print\"\n        }), \"(output_dir)\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"upload_folder\"\n        }), \"(\\n    repo_id=repo_id,\\n    folder_path=output_dir,\\n    commit_message=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Quantized Llama-3-8B-Instruct model for Android.\\\"\"\n        }), \",\\n    ignore_patterns=[\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"step_*\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"epoch_*\\\"\"\n        }), \"],\\n)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:\\nhttps://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"여기 완전한 Colab 노트북을 찾을 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"섹션 III: 빌드 종속성 설치\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"단계 1: Rust 설치하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"표준 설치를 계속하려면 옵션 1을 선택하세요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"sudo curl --proto \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"'=https'\"\n        }), \" --tlsv1\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".2\"\n        }), \" -sSf \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//sh.rustup.rs | sh\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 2: Install NDK and CMake in Android Studio\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Open Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MLC\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"LLM\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Python\"\n        }), \" 패키지와 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"TVM\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Unity\"\n        }), \" 컴파일러 설치.\\npython3 -m pip install --pre -U -f \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly\"\n        }), \"\\n\\n# 아래 명령어를 사용하여 설치 확인:\\npython3 -c \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"import mlc_llm; print(mlc_llm)\\\"\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"단계 4: CMake 설치하기\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CMake\"\n        }), \" 설치하기.\\nsudo apt-get install cmake\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.strong, {\n        children: \"단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"cd /home/tiwarinitin1999/  \\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MLC\"\n        }), \"-\", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"LLM\"\n        }), \" 저장소를 복제합니다.\\ngit clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//github.com/mlc-ai/mlc-llm.git\"\n        }), \"\\ncd mlc-llm\\n\\n# 저장소의 서브모듈을 업데이트합니다.\\ngit submodule update --init --recursive\\n\\n# \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Llama3\"\n        }), \"-on-\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Mobile\"\n        }), \" 저장소를 복제합니다.\\ncd /home/tiwarinitin1999/\\ngit clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//github.com/NSTiwari/Llama3-on-Mobile.git\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"cd /home/tiwarinitin1999/mlc-llm/android/\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"MLCChat\"\n        }), \"\\nmkdir dist\\ncd dist\\nmkdir prebuilt\\ncd prebuilt\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"# 퀀터이즈된 \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Llama3\"\n        }), \"-8B-\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Instruct\"\n        }), \" 가중치의 \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"HF\"\n        }), \" 리포지토리를 복제합니다.\\ngit clone \", _jsx(_components.span, {\n          className: \"hljs-attr\",\n          children: \"https\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"//huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"dist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-bash\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"cd\"\n        }), \" /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist\\n\", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"mkdir\"\n        }), \" lib\\n\", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"cd\"\n        }), \" lib/\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 8: mlc-package-config.json 파일 구성하기\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"{\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"device\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"android\\\"\"\n        }), \",\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"model_list\\\"\"\n        }), \": [\\n        {\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"model\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Llama-3-8B-q4f16_1-android\\\"\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"bundle_weight\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-literal\",\n          children: \"true\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"model_id\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"llama-3-8b-q4f16_1\\\"\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"model_lib\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"llama-q4f16_1\\\"\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"estimated_vram_bytes\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"4348727787\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"overrides\\\"\"\n        }), \": {\\n                \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"context_window_size\\\"\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"768\"\n        }), \",\\n                \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"prefill_chunk_size\\\"\"\n        }), \":\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"256\"\n        }), \"\\n            }         \\n        }\\n    ],\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"model_lib_path_for_prepare_libs\\\"\"\n        }), \": {\\n        \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"llama-q4f16_1\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar\\\"\"\n        }), \"\\n    }\\n}\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"9단계: 경로에 환경 변수 설정하기\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"html\", {\n      children: _jsx(\"img\", {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"ANDROID_NDK\"\n        }), \"=\", _jsx(_components.span, {\n          className: \"hljs-regexp\",\n          children: \"/home/\"\n        }), \"tiwarinitin1999/\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Android\"\n        }), \"/\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Sdk\"\n        }), \"/ndk/\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"27.0\"\n        }), _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \".11718014\"\n        }), \"\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"TVM_NDK_CC\"\n        }), \"=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"TVM_HOME\"\n        }), \"=\", _jsx(_components.span, {\n          className: \"hljs-regexp\",\n          children: \"/home/\"\n        }), \"tiwarinitin1999/mlc-llm/3rdparty/tvm\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"JAVA_HOME\"\n        }), \"=\", _jsx(_components.span, {\n          className: \"hljs-regexp\",\n          children: \"/home/\"\n        }), \"tiwarinitin1999/\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"Downloads\"\n        }), \"/android-studio/jbr\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"export\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-variable constant_\",\n          children: \"MLC_LLM_HOME\"\n        }), \"=\", _jsx(_components.span, {\n          className: \"hljs-regexp\",\n          children: \"/home/\"\n        }), \"tiwarinitin1999/mlc-llm\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Step 10: 안드로이드 빌드 파일 생성\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-sh\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"cd\"\n        }), \" /home/tiwarinitin1999/mlc-llm/android/MLCChat\\npython3 -m mlc_llm package\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png\",\n        alt: \"Screenshot\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png\",\n        alt: \"Screenshot\"\n      })\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsx(_components.code, {\n        children: \"\\n\u003cdiv class=\\\"content-ad\\\"\u003e\u003c/div\u003e\\n\\n위 명령은 다음 파일을 생성합니다:\\n\\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png)\\n\\n소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:\\n\\n소스 디렉토리:\\n/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output\\n\\n\u003cdiv class=\\\"content-ad\\\"\u003e\u003c/div\u003e\\n\\n목적지 디렉토리:\\n/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output\\n\\n이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:\\n\\n```js\\n{\\n  \\\"model_list\\\": [\\n    {\\n      \\\"model_id\\\": \\\"llama-3-8b-q4f16_1\\\",\\n      \\\"model_lib\\\": \\\"llama-q4f16_1\\\",\\n      \\\"model_url\\\": \\\"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\\\",\\n      \\\"estimated_vram_bytes\\\": 4348727787\\n    }\\n  ]\\n}\\n\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모든 구성이 설정되었습니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"[\", _jsx(_components.img, {\n        src: \"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png\",\n        alt: \"image\"\n      }), \"]\\n(https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif)\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.\\nhttps://github.com/NSTiwari/Llama3-on-Mobile\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"감사의 글\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"MobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"참고 자료 및 자원\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"MobileLlama3 GitHub 저장소\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"변환된 가중치를 위한 HuggingFace 저장소\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Meta사의 Llama3 모델들\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"MLC-LLM\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"MLC-LLM용 Android SDK\"\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile"},"buildId":"PgdIX9e0tvkvkdAmDT6qR","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>