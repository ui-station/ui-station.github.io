<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile" data-gatsby-head="true"/><meta name="twitter:title" content="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 20:07" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_buildManifest.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">18<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>소개</h1>
<p>2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.</p>
<p>언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.</p>
<p>하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png">
<p>시작하기 전에, 먼저 파이프라인을 이해해 봅시다.</p>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png">
<p>자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기</h2>
<p>단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.</p>
<pre><code class="hljs language-js"># 저장소를 복제합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/NSTiwari/Llama3-on-Mobile</span>
</code></pre>
<p>단계 1: MLC-LLM 설치하기</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.</p>
<pre><code class="hljs language-js">!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f <span class="hljs-attr">https</span>:<span class="hljs-comment">//mlc.ai/wheels</span>
!pip install numpy==<span class="hljs-number">1.23</span><span class="hljs-number">.5</span>
</code></pre>
<p>단계 2: 라이브러리 가져오기</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> mlc_llm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> snapshot_download
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Step 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요</p>
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">HF</span> 계정에 로그인합니다.
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login
<span class="hljs-title function_">notebook_login</span>()

# <span class="hljs-title class_">Llama</span>-<span class="hljs-number">3</span>-8B-<span class="hljs-title class_">Instruct</span> 모델을 다운로드합니다.
<span class="hljs-title function_">snapshot_download</span>(repo_id=<span class="hljs-string">"meta-llama/Meta-Llama-3-8B-Instruct"</span>, local_dir=<span class="hljs-string">"/content/Llama-3-8B-Instruct/"</span>)
</code></pre>
<p>Step 4: GPU가 활성화되었는지 확인하세요</p>
<pre><code class="hljs language-js">!nvidia-smi

# <span class="hljs-variable constant_">CUDA</span>가 사용 가능한지 확인합니다.
torch.<span class="hljs-property">cuda</span>.<span class="hljs-title function_">is_available</span>()

device = torch.<span class="hljs-title function_">device</span>(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.<span class="hljs-property">cuda</span>.<span class="hljs-title function_">is_available</span>() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
device
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Step 5: 모델 이름과 양자화 유형 구성</p>
<pre><code class="hljs language-js"><span class="hljs-variable constant_">MODEL_NAME</span> = <span class="hljs-string">"Llama-3-8B-Instruct"</span>;
<span class="hljs-variable constant_">QUANTIZATION</span> = <span class="hljs-string">"q4f16_1"</span>;
</code></pre>
<p>Step 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환</p>
<p>다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/
</code></pre>
<p>7단계: 토큰 파일 생성</p>
<p>이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.</p>
<pre><code class="hljs language-js">!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \
    --conv-template llama-<span class="hljs-number">3</span> --context-<span class="hljs-variable language_">window</span>-size <span class="hljs-number">8192</span> --prefill-chunk-size <span class="hljs-number">1024</span>  \
    -o /content/$MODEL_NAME-$QUANTIZATION-android/
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>8단계: Android 형식으로 모델 컴파일하기</p>
<p>여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.</p>
<pre><code class="hljs language-js">!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.<span class="hljs-property">json</span> \
    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.<span class="hljs-property">tar</span>
</code></pre>
<p>9단계: 모델을 Hugging Face에 올리기 🤗</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> whoami
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> <span class="hljs-title class_">Path</span>

# 출력 디렉토리.
output_dir = <span class="hljs-string">"/content/"</span> + <span class="hljs-variable constant_">MODEL_NAME</span> + <span class="hljs-string">"-"</span> + <span class="hljs-variable constant_">QUANTIZATION</span> + <span class="hljs-string">"-android/"</span>
repo_name = <span class="hljs-string">"Llama-3-8B-q4f16_1-android"</span>
username = <span class="hljs-title function_">whoami</span>(token=<span class="hljs-title class_">Path</span>(<span class="hljs-string">"/root/.cache/huggingface/"</span>))[<span class="hljs-string">"name"</span>]
repo_id = f<span class="hljs-string">"{username}/{repo_name}"</span>
</code></pre>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> upload_folder, create_repo

repo_id = <span class="hljs-title function_">create_repo</span>(repo_id, exist_ok=<span class="hljs-title class_">True</span>).<span class="hljs-property">repo_id</span>
<span class="hljs-title function_">print</span>(output_dir)

<span class="hljs-title function_">upload_folder</span>(
    repo_id=repo_id,
    folder_path=output_dir,
    commit_message=<span class="hljs-string">"Quantized Llama-3-8B-Instruct model for Android."</span>,
    ignore_patterns=[<span class="hljs-string">"step_*"</span>, <span class="hljs-string">"epoch_*"</span>],
)
</code></pre>
<p>다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:
<a href="https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android" rel="nofollow" target="_blank">https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android</a></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>여기 완전한 Colab 노트북을 찾을 수 있습니다.</p>
<p>좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.</p>
<h2>Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)</h2>
<p>이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.</p>
<p>저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.</p>
<p>비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.</p>
<h2>섹션 III: 빌드 종속성 설치</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h3>단계 1: Rust 설치하기</h3>
<p>안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png" alt="이미지"></p>
<p>표준 설치를 계속하려면 옵션 1을 선택하세요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png" alt="image"></p>
<pre><code class="hljs language-js">sudo curl --proto <span class="hljs-string">'=https'</span> --tlsv1<span class="hljs-number">.2</span> -sSf <span class="hljs-attr">https</span>:<span class="hljs-comment">//sh.rustup.rs | sh</span>
</code></pre>
<p>Step 2: Install NDK and CMake in Android Studio</p>
<p>Open Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png">
<p>제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치</p>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png">
<pre><code class="hljs language-js"># <span class="hljs-variable constant_">MLC</span>-<span class="hljs-variable constant_">LLM</span> <span class="hljs-title class_">Python</span> 패키지와 <span class="hljs-variable constant_">TVM</span> <span class="hljs-title class_">Unity</span> 컴파일러 설치.
python3 -m pip install --pre -U -f <span class="hljs-attr">https</span>:<span class="hljs-comment">//mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly</span>

# 아래 명령어를 사용하여 설치 확인:
python3 -c <span class="hljs-string">"import mlc_llm; print(mlc_llm)"</span>
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><strong>단계 4: CMake 설치하기</strong></p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png" alt="이미지"></p>
<pre><code class="hljs language-js"># <span class="hljs-title class_">CMake</span> 설치하기.
sudo apt-get install cmake
</code></pre>
<p><strong>단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기</strong></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png">
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png">
<pre><code class="hljs language-js">cd /home/tiwarinitin1999/

# <span class="hljs-variable constant_">MLC</span>-<span class="hljs-variable constant_">LLM</span> 저장소를 복제합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/mlc-ai/mlc-llm.git</span>
cd mlc-llm

# 저장소의 서브모듈을 업데이트합니다.
git submodule update --init --recursive

# <span class="hljs-title class_">Llama3</span>-on-<span class="hljs-title class_">Mobile</span> 저장소를 복제합니다.
cd /home/tiwarinitin1999/
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/NSTiwari/Llama3-on-Mobile.git</span>
</code></pre>
<p>단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>MLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png" alt="이미지"></p>
<pre><code class="hljs language-js">cd /home/tiwarinitin1999/mlc-llm/android/<span class="hljs-title class_">MLCChat</span>
mkdir dist
cd dist
mkdir prebuilt
cd prebuilt
</code></pre>
<p>그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png">
<pre><code class="hljs language-js"># 퀀터이즈된 <span class="hljs-title class_">Llama3</span>-8B-<span class="hljs-title class_">Instruct</span> 가중치의 <span class="hljs-variable constant_">HF</span> 리포지토리를 복제합니다.
git clone <span class="hljs-attr">https</span>:<span class="hljs-comment">//huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git</span>
</code></pre>
<p>7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.</p>
<p>dist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png" alt="image"></p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">cd</span> /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist
<span class="hljs-built_in">mkdir</span> lib
<span class="hljs-built_in">cd</span> lib/
</code></pre>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png" alt="image"></p>
<p>Step 8: mlc-package-config.json 파일 구성하기</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>MLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:</p>
<pre><code class="hljs language-js">{
    <span class="hljs-string">"device"</span>: <span class="hljs-string">"android"</span>,
    <span class="hljs-string">"model_list"</span>: [
        {
            <span class="hljs-string">"model"</span>: <span class="hljs-string">"Llama-3-8B-q4f16_1-android"</span>,
            <span class="hljs-string">"bundle_weight"</span>: <span class="hljs-literal">true</span>,
            <span class="hljs-string">"model_id"</span>: <span class="hljs-string">"llama-3-8b-q4f16_1"</span>,
            <span class="hljs-string">"model_lib"</span>: <span class="hljs-string">"llama-q4f16_1"</span>,
            <span class="hljs-string">"estimated_vram_bytes"</span>: <span class="hljs-number">4348727787</span>,
            <span class="hljs-string">"overrides"</span>: {
                <span class="hljs-string">"context_window_size"</span>:<span class="hljs-number">768</span>,
                <span class="hljs-string">"prefill_chunk_size"</span>:<span class="hljs-number">256</span>
            }
        }
    ],
    <span class="hljs-string">"model_lib_path_for_prepare_libs"</span>: {
        <span class="hljs-string">"llama-q4f16_1"</span>: <span class="hljs-string">"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar"</span>
    }
}
</code></pre>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png" alt="이미지"></p>
<p>9단계: 경로에 환경 변수 설정하기</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png">

<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-variable constant_">ANDROID_NDK</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/<span class="hljs-title class_">Android</span>/<span class="hljs-title class_">Sdk</span>/ndk/<span class="hljs-number">27.0</span><span class="hljs-number">.11718014</span>
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">TVM_NDK_CC</span>=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">TVM_HOME</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/mlc-llm/3rdparty/tvm
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">JAVA_HOME</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/<span class="hljs-title class_">Downloads</span>/android-studio/jbr
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">MLC_LLM_HOME</span>=<span class="hljs-regexp">/home/</span>tiwarinitin1999/mlc-llm
</code></pre>
<p>Step 10: 안드로이드 빌드 파일 생성</p>
<p>마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-sh"><span class="hljs-built_in">cd</span> /home/tiwarinitin1999/mlc-llm/android/MLCChat
python3 -m mlc_llm package
</code></pre>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png" alt="Screenshot"></p>
<p>명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png" alt="Screenshot"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>위 명령은 다음 파일을 생성합니다:</p>
<p><img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png" alt="이미지"></p>
<p>소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:</p>
<p>소스 디렉토리:
/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>목적지 디렉토리:
/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output</p>
<p>이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:</p>
<pre><code class="hljs language-js">{
  <span class="hljs-string">"model_list"</span>: [
    {
      <span class="hljs-string">"model_id"</span>: <span class="hljs-string">"llama-3-8b-q4f16_1"</span>,
      <span class="hljs-string">"model_lib"</span>: <span class="hljs-string">"llama-q4f16_1"</span>,
      <span class="hljs-string">"model_url"</span>: <span class="hljs-string">"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android"</span>,
      <span class="hljs-string">"estimated_vram_bytes"</span>: <span class="hljs-number">4348727787</span>
    }
  ]
}
</code></pre>
<p>설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png">
<p>모든 구성이 설정되었습니다.</p>
<h2>섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기</h2>
<p>안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>[<img src="/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png" alt="image">]
(<a href="https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif" rel="nofollow" target="_blank">https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif</a>)</p>
<p>모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.</p>
<p>오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.
<a href="https://github.com/NSTiwari/Llama3-on-Mobile" rel="nofollow" target="_blank">https://github.com/NSTiwari/Llama3-on-Mobile</a></p>
<p>작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.</p>
<h2>감사의 글</h2>
<p>MobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>참고 자료 및 자원</h2>
<ul>
<li>Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북</li>
<li>MobileLlama3 GitHub 저장소</li>
<li>변환된 가중치를 위한 HuggingFace 저장소</li>
<li>Meta사의 Llama3 모델들</li>
<li>MLC-LLM</li>
<li>MLC-LLM용 Android SDK</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"ML 이야기 MobileLlama3 모바일에서 Llama3를 로컬에서 실행하기","description":"","date":"2024-05-18 20:07","slug":"2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile","content":"\n# 소개\n\n2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.\n\n언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.\n\n하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png\" /\u003e\n\n시작하기 전에, 먼저 파이프라인을 이해해 봅시다.\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png\" /\u003e\n\n자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기\n\n단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.\n\n```js\n# 저장소를 복제합니다.\ngit clone https://github.com/NSTiwari/Llama3-on-Mobile\n```\n\n단계 1: MLC-LLM 설치하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.\n\n```js\n!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f https://mlc.ai/wheels\n!pip install numpy==1.23.5\n```\n\n단계 2: 라이브러리 가져오기\n\n```js\nimport mlc_llm\nimport torch\nfrom huggingface_hub import snapshot_download\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nStep 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요\n\n```js\n# HF 계정에 로그인합니다.\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n# Llama-3-8B-Instruct 모델을 다운로드합니다.\nsnapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"/content/Llama-3-8B-Instruct/\")\n```\n\nStep 4: GPU가 활성화되었는지 확인하세요\n\n```js\n!nvidia-smi\n\n# CUDA가 사용 가능한지 확인합니다.\ntorch.cuda.is_available()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nStep 5: 모델 이름과 양자화 유형 구성\n\n```js\nMODEL_NAME = \"Llama-3-8B-Instruct\";\nQUANTIZATION = \"q4f16_1\";\n```\n\nStep 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환\n\n다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/\n```\n\n7단계: 토큰 파일 생성\n\n이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.\n\n```js\n!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \\\n    --conv-template llama-3 --context-window-size 8192 --prefill-chunk-size 1024  \\\n    -o /content/$MODEL_NAME-$QUANTIZATION-android/\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n8단계: Android 형식으로 모델 컴파일하기\n\n여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.\n\n```js\n!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.json \\\n    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.tar\n```\n\n9단계: 모델을 Hugging Face에 올리기 🤗\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.\n\n```js\nfrom huggingface_hub import whoami\nfrom pathlib import Path\n\n# 출력 디렉토리.\noutput_dir = \"/content/\" + MODEL_NAME + \"-\" + QUANTIZATION + \"-android/\"\nrepo_name = \"Llama-3-8B-q4f16_1-android\"\nusername = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\nrepo_id = f\"{username}/{repo_name}\"\n```\n\n```js\nfrom huggingface_hub import upload_folder, create_repo\n\nrepo_id = create_repo(repo_id, exist_ok=True).repo_id\nprint(output_dir)\n\nupload_folder(\n    repo_id=repo_id,\n    folder_path=output_dir,\n    commit_message=\"Quantized Llama-3-8B-Instruct model for Android.\",\n    ignore_patterns=[\"step_*\", \"epoch_*\"],\n)\n```\n\n다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:\nhttps://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기 완전한 Colab 노트북을 찾을 수 있습니다.\n\n좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.\n\n## Section II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)\n\n이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.\n\n저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.\n\n비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.\n\n## 섹션 III: 빌드 종속성 설치\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n### 단계 1: Rust 설치하기\n\n안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png)\n\n표준 설치를 계속하려면 옵션 1을 선택하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png)\n\n```js\nsudo curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nStep 2: Install NDK and CMake in Android Studio\n\nOpen Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png\" /\u003e\n\n제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png\" /\u003e\n\n```js\n# MLC-LLM Python 패키지와 TVM Unity 컴파일러 설치.\npython3 -m pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly\n\n# 아래 명령어를 사용하여 설치 확인:\npython3 -c \"import mlc_llm; print(mlc_llm)\"\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n**단계 4: CMake 설치하기**\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png)\n\n```js\n# CMake 설치하기.\nsudo apt-get install cmake\n```\n\n**단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기**\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png\" /\u003e\n\n```js\ncd /home/tiwarinitin1999/\n\n# MLC-LLM 저장소를 복제합니다.\ngit clone https://github.com/mlc-ai/mlc-llm.git\ncd mlc-llm\n\n# 저장소의 서브모듈을 업데이트합니다.\ngit submodule update --init --recursive\n\n# Llama3-on-Mobile 저장소를 복제합니다.\ncd /home/tiwarinitin1999/\ngit clone https://github.com/NSTiwari/Llama3-on-Mobile.git\n```\n\n단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png)\n\n```js\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat\nmkdir dist\ncd dist\nmkdir prebuilt\ncd prebuilt\n```\n\n그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png\" /\u003e\n\n```js\n# 퀀터이즈된 Llama3-8B-Instruct 가중치의 HF 리포지토리를 복제합니다.\ngit clone https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git\n```\n\n7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.\n\ndist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png)\n\n```bash\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist\nmkdir lib\ncd lib/\n```\n\n![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png)\n\nStep 8: mlc-package-config.json 파일 구성하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n    \"device\": \"android\",\n    \"model_list\": [\n        {\n            \"model\": \"Llama-3-8B-q4f16_1-android\",\n            \"bundle_weight\": true,\n            \"model_id\": \"llama-3-8b-q4f16_1\",\n            \"model_lib\": \"llama-q4f16_1\",\n            \"estimated_vram_bytes\": 4348727787,\n            \"overrides\": {\n                \"context_window_size\":768,\n                \"prefill_chunk_size\":256\n            }\n        }\n    ],\n    \"model_lib_path_for_prepare_libs\": {\n        \"llama-q4f16_1\": \"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar\"\n    }\n}\n```\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png)\n\n9단계: 경로에 환경 변수 설정하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003chtml\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png\" /\u003e\n\u003c/html\u003e\n\n```js\nexport ANDROID_NDK=/home/tiwarinitin1999/Android/Sdk/ndk/27.0.11718014\nexport TVM_NDK_CC=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang\nexport TVM_HOME=/home/tiwarinitin1999/mlc-llm/3rdparty/tvm\nexport JAVA_HOME=/home/tiwarinitin1999/Downloads/android-studio/jbr\nexport MLC_LLM_HOME=/home/tiwarinitin1999/mlc-llm\n```\n\nStep 10: 안드로이드 빌드 파일 생성\n\n마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```sh\ncd /home/tiwarinitin1999/mlc-llm/android/MLCChat\npython3 -m mlc_llm package\n```\n\n![Screenshot](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png)\n\n명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.\n\n![Screenshot](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 명령은 다음 파일을 생성합니다:\n\n![이미지](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png)\n\n소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:\n\n소스 디렉토리:\n/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n목적지 디렉토리:\n/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output\n\n이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:\n\n```js\n{\n  \"model_list\": [\n    {\n      \"model_id\": \"llama-3-8b-q4f16_1\",\n      \"model_lib\": \"llama-q4f16_1\",\n      \"model_url\": \"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\",\n      \"estimated_vram_bytes\": 4348727787\n    }\n  ]\n}\n```\n\n설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png\" /\u003e\n\n모든 구성이 설정되었습니다.\n\n## 섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기\n\n안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n[![image](/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png)]\n(https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif)\n\n모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.\n\n오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.\nhttps://github.com/NSTiwari/Llama3-on-Mobile\n\n작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.\n\n## 감사의 글\n\nMobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 참고 자료 및 자원\n\n- Llama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북\n- MobileLlama3 GitHub 저장소\n- 변환된 가중치를 위한 HuggingFace 저장소\n- Meta사의 Llama3 모델들\n- MLC-LLM\n- MLC-LLM용 Android SDK\n","ogImage":{"url":"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png"},"coverImage":"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png","tag":["Tech"],"readingTime":18},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e2024년 4월, Meta가 새로운 오픈 언어 모델 패밀리인 Llama 3을 출시했습니다. 이전 모델을 발전시킨 Llama 3은 개선된 기능을 제공하며, 8B 및 70B의 사전 훈련된 버전과 명령어 튜닝된 변형을 제공합니다.\u003c/p\u003e\n\u003cp\u003e언어 모델의 지속적인 트렌드에서, 개발자들은 개인 정보 보호를 위해 API 대신 로컬 또는 오프라인 사용을 선호하고 있습니다. 올라마는 macOS 및 Linux OS에서 오프라인으로 LLMs를 실행할 수 있는 도구 중 하나로, 로컬 실행을 가능하게 합니다. 그러나 스마트폰의 제한된 하드웨어 성능으로 인해 모바일 기기에서 LLMs를 로컬로 실행하는 기능은 아직 제한적입니다.\u003c/p\u003e\n\u003cp\u003e하지만 이제는 다릅니다. MLC 덕분에 모바일 기기에서 이러한 대형 모델을 실행하는 것이 가능해졌습니다. 이 블로그는 MLC LLM을 사용하여 오프라인 추론을 위해 Llama3-8B-Instruction 모델을 모바일 폰에 직접 양자화, 변환 및 배포하는 완전한 튜토리얼을 제공합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_0.png\"\u003e\n\u003cp\u003e시작하기 전에, 먼저 파이프라인을 이해해 봅시다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_1.png\"\u003e\n\u003cp\u003e자, 더 이상 지체하지 말고, 단계별 구현을 위한 코드로 바로 넘어가 보겠습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e섹션 I: 원본 라마-3-8B-인스트럭트 모델을 MLC 호환 가중치로 양자화 및 변환하기\u003c/h2\u003e\n\u003cp\u003e단계 0: 아래 저장소를 로컬 머신에 복제하고 Google Colab에 Llama3_on_Mobile.ipynb 노트북을 업로드하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 저장소를 복제합니다.\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/NSTiwari/Llama3-on-Mobile\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e단계 1: MLC-LLM 설치하기\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e모델 가중치를 변환하려면 MLC-LLM 라이브러리가 필요합니다. 노트북을 실행하는 데 특히 NumPy 버전 1.23.5가 필요하며, 다른 버전에서 변환 프로세스에 문제가 발생했습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!pip install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-llm-nightly-cu122 -f \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//mlc.ai/wheels\u003c/span\u003e\n!pip install numpy==\u003cspan class=\"hljs-number\"\u003e1.23\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.5\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e단계 2: 라이브러리 가져오기\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e mlc_llm\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e torch\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e huggingface_hub \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e snapshot_download\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eStep 3: HF 계정에 로그인하고 원본 Llama-3-8B-Instruct 모델 가중치를 다운로드하세요\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-variable constant_\"\u003eHF\u003c/span\u003e 계정에 로그인합니다.\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e huggingface_hub \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e notebook_login\n\u003cspan class=\"hljs-title function_\"\u003enotebook_login\u003c/span\u003e()\n\n# \u003cspan class=\"hljs-title class_\"\u003eLlama\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e 모델을 다운로드합니다.\n\u003cspan class=\"hljs-title function_\"\u003esnapshot_download\u003c/span\u003e(repo_id=\u003cspan class=\"hljs-string\"\u003e\"meta-llama/Meta-Llama-3-8B-Instruct\"\u003c/span\u003e, local_dir=\u003cspan class=\"hljs-string\"\u003e\"/content/Llama-3-8B-Instruct/\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStep 4: GPU가 활성화되었는지 확인하세요\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!nvidia-smi\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eCUDA\u003c/span\u003e가 사용 가능한지 확인합니다.\ntorch.\u003cspan class=\"hljs-property\"\u003ecuda\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eis_available\u003c/span\u003e()\n\ndevice = torch.\u003cspan class=\"hljs-title function_\"\u003edevice\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"cuda:0\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e torch.\u003cspan class=\"hljs-property\"\u003ecuda\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eis_available\u003c/span\u003e() \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"cpu\"\u003c/span\u003e)\ndevice\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eStep 5: 모델 이름과 양자화 유형 구성\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eMODEL_NAME\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"Llama-3-8B-Instruct\"\u003c/span\u003e;\n\u003cspan class=\"hljs-variable constant_\"\u003eQUANTIZATION\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"q4f16_1\"\u003c/span\u003e;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStep 6: Llama-3-8B-Insruct 모델을 MLC 호환 가중치로 변환\u003c/p\u003e\n\u003cp\u003e다음 코드는 q4f16_1 양자화를 사용하여 Llama-3-8B-Instruct 모델을 양자화 및 샤딩하여 여러 청크로 변환합니다. 그런 다음 모델 가중치를 Llama-3-8B-Instruct-q4f16_1-android이란 디렉터리에 변환하고 저장합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!python -m mlc_llm convert_weight /content/$MODEL_NAME/ --quantization $QUANTIZATION -o /content/$MODEL_NAME-$QUANTIZATION-android/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e7단계: 토큰 파일 생성\u003c/p\u003e\n\u003cp\u003e이 코드 라인은 conv-template, context-window, prefill-chunk-size와 같은 매개변수를 사용하여 토큰 파일을 생성합니다. 이때 conv-template은 llama-3으로 설정되어 있으며, 이는 작업 중인 Llama-3 모델 변형을 나타냅니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!python -m mlc_llm gen_config /content/$MODEL_NAME/ --quantization $QUANTIZATION \\\n    --conv-template llama-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e --context-\u003cspan class=\"hljs-variable language_\"\u003ewindow\u003c/span\u003e-size \u003cspan class=\"hljs-number\"\u003e8192\u003c/span\u003e --prefill-chunk-size \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e  \\\n    -o /content/$MODEL_NAME-$QUANTIZATION-android/\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e8단계: Android 형식으로 모델 컴파일하기\u003c/p\u003e\n\u003cp\u003e여기서는 장치 매개변수를 사용하여 모델 가중치를 Android 호환 형식으로 컴파일하며, 이는 Llama3–8B-Instruct-q4f16_1-android.tar 파일을 생성합니다. 이 .tar 파일은 모델을 기기에 배포하기 위해 이후 단계에서 사용될 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e!python -m mlc_llm compile /content/$MODEL_NAME-$QUANTIZATION-android/mlc-chat-config.\u003cspan class=\"hljs-property\"\u003ejson\u003c/span\u003e \\\n    --device android -o /content/$MODEL_NAME-$QUANTIZATION-android/$MODEL_NAME-$QUANTIZATION-android.\u003cspan class=\"hljs-property\"\u003etar\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e9단계: 모델을 Hugging Face에 올리기 🤗\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마지막으로, 모델 가중치를 HF에 저장하세요. 이러한 가중치는 추론 중에 모바일 폰으로 다운로드될 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e huggingface_hub \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e whoami\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pathlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePath\u003c/span\u003e\n\n# 출력 디렉토리.\noutput_dir = \u003cspan class=\"hljs-string\"\u003e\"/content/\"\u003c/span\u003e + \u003cspan class=\"hljs-variable constant_\"\u003eMODEL_NAME\u003c/span\u003e + \u003cspan class=\"hljs-string\"\u003e\"-\"\u003c/span\u003e + \u003cspan class=\"hljs-variable constant_\"\u003eQUANTIZATION\u003c/span\u003e + \u003cspan class=\"hljs-string\"\u003e\"-android/\"\u003c/span\u003e\nrepo_name = \u003cspan class=\"hljs-string\"\u003e\"Llama-3-8B-q4f16_1-android\"\u003c/span\u003e\nusername = \u003cspan class=\"hljs-title function_\"\u003ewhoami\u003c/span\u003e(token=\u003cspan class=\"hljs-title class_\"\u003ePath\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"/root/.cache/huggingface/\"\u003c/span\u003e))[\u003cspan class=\"hljs-string\"\u003e\"name\"\u003c/span\u003e]\nrepo_id = f\u003cspan class=\"hljs-string\"\u003e\"{username}/{repo_name}\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e huggingface_hub \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e upload_folder, create_repo\n\nrepo_id = \u003cspan class=\"hljs-title function_\"\u003ecreate_repo\u003c/span\u003e(repo_id, exist_ok=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e).\u003cspan class=\"hljs-property\"\u003erepo_id\u003c/span\u003e\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output_dir)\n\n\u003cspan class=\"hljs-title function_\"\u003eupload_folder\u003c/span\u003e(\n    repo_id=repo_id,\n    folder_path=output_dir,\n    commit_message=\u003cspan class=\"hljs-string\"\u003e\"Quantized Llama-3-8B-Instruct model for Android.\"\u003c/span\u003e,\n    ignore_patterns=[\u003cspan class=\"hljs-string\"\u003e\"step_*\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"epoch_*\"\u003c/span\u003e],\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음 HF 🤗 리포지토리에서 샤드된 모델 가중치 및 토크나이저를 직접 찾을 수 있습니다:\n\u003ca href=\"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\u003c/a\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e여기 완전한 Colab 노트북을 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e좋아요. 우리는 양자화와 모델 가중치 변환의 초기 단계를 완료했습니다. 이제 다음 섹션에서는 GCP 인스턴스에서 추가로 모델 가중치를 컴파일하기 위한 환경을 설정할 것입니다.\u003c/p\u003e\n\u003ch2\u003eSection II: 안드로이드용 빌드 파일 생성을 위한 GCP 환경 설정(옵션)\u003c/h2\u003e\n\u003cp\u003e이 단계는 선택 사항이며 이미 Linux 또는 MacOS와 같은 UNIX 기반 시스템을 가지고 있다면 필요하지 않습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e윈도우 기기를 사용하고 있기 때문에 호환성 문제로 필요한 라이브러리와 종속성을 설치하는 것이 귀찮았어요. 그래서 귀차니즘을 피하기 위해 GCP에서 Linux VM 인스턴스를 렌트하기로 결정했어요.\u003c/p\u003e\n\u003cp\u003e저는 GCP VM 인스턴스에서 환경을 설정하고 Android Studio를 설치하는 단계를 안내하는 별도의 블로그를 작성했어요.\u003c/p\u003e\n\u003cp\u003e비슷한 문제를 겪고 계신 분이라면, 여기서 확인해보세요. 그렇지 않다면 건너뛰셔도 돼요.\u003c/p\u003e\n\u003ch2\u003e섹션 III: 빌드 종속성 설치\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch3\u003e단계 1: Rust 설치하기\u003c/h3\u003e\n\u003cp\u003e안드로이드로 HuggingFace 토크나이저를 크로스 컴파일하기 위해서는 Rust가 필요합니다. Rust를 설치하려면 아래 명령을 실행하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e표준 설치를 계속하려면 옵션 1을 선택하세요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esudo curl --proto \u003cspan class=\"hljs-string\"\u003e'=https'\u003c/span\u003e --tlsv1\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e -sSf \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//sh.rustup.rs | sh\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStep 2: Install NDK and CMake in Android Studio\u003c/p\u003e\n\u003cp\u003eOpen Android Studio → Tools → SDK Manager → SDK Tools → Install CMake and NDK.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_4.png\"\u003e\n\u003cp\u003e제 3 단계: MLC LLM Python 패키지 및 TVM Unity 컴파일러 설치\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_5.png\"\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-variable constant_\"\u003eMLC\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 패키지와 \u003cspan class=\"hljs-variable constant_\"\u003eTVM\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUnity\u003c/span\u003e 컴파일러 설치.\npython3 -m pip install --pre -U -f \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//mlc.ai/wheels mlc-llm-nightly mlc-ai-nightly\u003c/span\u003e\n\n# 아래 명령어를 사용하여 설치 확인:\npython3 -c \u003cspan class=\"hljs-string\"\u003e\"import mlc_llm; print(mlc_llm)\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cstrong\u003e단계 4: CMake 설치하기\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_6.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-title class_\"\u003eCMake\u003c/span\u003e 설치하기.\nsudo apt-get install cmake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003e단계 5: MLC-LLM 및 Llama3-on-Mobile 저장소 복제하기\u003c/strong\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_7.png\"\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_8.png\"\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecd /home/tiwarinitin1999/\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eMLC\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eLLM\u003c/span\u003e 저장소를 복제합니다.\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/mlc-ai/mlc-llm.git\u003c/span\u003e\ncd mlc-llm\n\n# 저장소의 서브모듈을 업데이트합니다.\ngit submodule update --init --recursive\n\n# \u003cspan class=\"hljs-title class_\"\u003eLlama3\u003c/span\u003e-on-\u003cspan class=\"hljs-title class_\"\u003eMobile\u003c/span\u003e 저장소를 복제합니다.\ncd /home/tiwarinitin1999/\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/NSTiwari/Llama3-on-Mobile.git\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e단계 6: 변환된 모델 가중치의 HuggingFace 저장소를 다운로드하세요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eMLCChat 디렉토리 내에 새 폴더 dist를 만들어주세요. dist 폴더 안에 prebuilt라는 하위 폴더를 생성해주세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_9.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecd /home/tiwarinitin1999/mlc-llm/android/\u003cspan class=\"hljs-title class_\"\u003eMLCChat\u003c/span\u003e\nmkdir dist\ncd dist\nmkdir prebuilt\ncd prebuilt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고 prebuilt 폴더에 HF repository(섹션 1의 단계 9에서 생성된)를 클론해주세요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_10.png\"\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 퀀터이즈된 \u003cspan class=\"hljs-title class_\"\u003eLlama3\u003c/span\u003e-8B-\u003cspan class=\"hljs-title class_\"\u003eInstruct\u003c/span\u003e 가중치의 \u003cspan class=\"hljs-variable constant_\"\u003eHF\u003c/span\u003e 리포지토리를 복제합니다.\ngit clone \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android.git\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e7단계: Llama3–8B-Instruct-q4f16_1-android.tar 파일을 복사합니다.\u003c/p\u003e\n\u003cp\u003edist 폴더 내에 lib라는 새 폴더를 만들어서 Llama3–8B-Instruct-q4f16_1-android.tar 파일 (Section I의 단계 8에서 생성된 파일)을 lib 디렉토리로 복사합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_11.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e /home/tiwarinitin1999/mlc-llm/android/MLCChat/dist\n\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e lib\n\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e lib/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_12.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eStep 8: mlc-package-config.json 파일 구성하기\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eMLCChat 폴더 내의 mlc-package-config.json 파일을 다음과 같이 구성하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e{\n    \u003cspan class=\"hljs-string\"\u003e\"device\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"android\"\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e\"model_list\"\u003c/span\u003e: [\n        {\n            \u003cspan class=\"hljs-string\"\u003e\"model\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Llama-3-8B-q4f16_1-android\"\u003c/span\u003e,\n            \u003cspan class=\"hljs-string\"\u003e\"bundle_weight\"\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003etrue\u003c/span\u003e,\n            \u003cspan class=\"hljs-string\"\u003e\"model_id\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama-3-8b-q4f16_1\"\u003c/span\u003e,\n            \u003cspan class=\"hljs-string\"\u003e\"model_lib\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama-q4f16_1\"\u003c/span\u003e,\n            \u003cspan class=\"hljs-string\"\u003e\"estimated_vram_bytes\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e4348727787\u003c/span\u003e,\n            \u003cspan class=\"hljs-string\"\u003e\"overrides\"\u003c/span\u003e: {\n                \u003cspan class=\"hljs-string\"\u003e\"context_window_size\"\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e768\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e\"prefill_chunk_size\"\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e\n            }\n        }\n    ],\n    \u003cspan class=\"hljs-string\"\u003e\"model_lib_path_for_prepare_libs\"\u003c/span\u003e: {\n        \u003cspan class=\"hljs-string\"\u003e\"llama-q4f16_1\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"./dist/lib/Llama-3-8B-Instruct-q4f16_1-android.tar\"\u003c/span\u003e\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_13.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e9단계: 경로에 환경 변수 설정하기\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_14.png\"\u003e\n\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eANDROID_NDK\u003c/span\u003e=\u003cspan class=\"hljs-regexp\"\u003e/home/\u003c/span\u003etiwarinitin1999/\u003cspan class=\"hljs-title class_\"\u003eAndroid\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003eSdk\u003c/span\u003e/ndk/\u003cspan class=\"hljs-number\"\u003e27.0\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.11718014\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eTVM_NDK_CC\u003c/span\u003e=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eTVM_HOME\u003c/span\u003e=\u003cspan class=\"hljs-regexp\"\u003e/home/\u003c/span\u003etiwarinitin1999/mlc-llm/3rdparty/tvm\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eJAVA_HOME\u003c/span\u003e=\u003cspan class=\"hljs-regexp\"\u003e/home/\u003c/span\u003etiwarinitin1999/\u003cspan class=\"hljs-title class_\"\u003eDownloads\u003c/span\u003e/android-studio/jbr\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eMLC_LLM_HOME\u003c/span\u003e=\u003cspan class=\"hljs-regexp\"\u003e/home/\u003c/span\u003etiwarinitin1999/mlc-llm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStep 10: 안드로이드 빌드 파일 생성\u003c/p\u003e\n\u003cp\u003e마지막으로, 아래 명령을 실행하여 on-device 배포를 위한 Llama3-8B-Instruct 모델의 .JAR 파일을 빌드하십시오.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-sh\"\u003e\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e /home/tiwarinitin1999/mlc-llm/android/MLCChat\npython3 -m mlc_llm package\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_15.png\" alt=\"Screenshot\"\u003e\u003c/p\u003e\n\u003cp\u003e명령어가 성공적으로 실행된 후, 아래와 같은 결과를 확인하실 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_16.png\" alt=\"Screenshot\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e위 명령은 다음 파일을 생성합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_17.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e소스 디렉토리의 출력 폴더 내용을 대상 디렉토리로 복사하세요:\u003c/p\u003e\n\u003cp\u003e소스 디렉토리:\n/home/tiwarinitin1999/mlc-llm/android/MLCChat/dist/lib/mlc4j/output\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e목적지 디렉토리:\n/home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/output\u003c/p\u003e\n\u003cp\u003e이제, home/tiwarinitin1999/Llama3-on-Mobile/mobile-llama3/MobileLlama3/dist/lib/mlc4j/src/main/assets 폴더에 있는 mlc-app-config.json 파일을 다음과 같이 구성하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e{\n  \u003cspan class=\"hljs-string\"\u003e\"model_list\"\u003c/span\u003e: [\n    {\n      \u003cspan class=\"hljs-string\"\u003e\"model_id\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama-3-8b-q4f16_1\"\u003c/span\u003e,\n      \u003cspan class=\"hljs-string\"\u003e\"model_lib\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"llama-q4f16_1\"\u003c/span\u003e,\n      \u003cspan class=\"hljs-string\"\u003e\"model_url\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"https://huggingface.co/NSTiwari/Llama-3-8B-q4f16_1-android\"\u003c/span\u003e,\n      \u003cspan class=\"hljs-string\"\u003e\"estimated_vram_bytes\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e4348727787\u003c/span\u003e\n    }\n  ]\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e설정 파일의 model_url 키는 모바일폰에서 HF 저장소로부터 모델 가중치를 다운로드합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_18.png\"\u003e\n\u003cp\u003e모든 구성이 설정되었습니다.\u003c/p\u003e\n\u003ch2\u003e섹션 IV: 안드로이드 스튜디오에서 앱 빌드하기\u003c/h2\u003e\n\u003cp\u003e안드로이드 스튜디오에서 MobileLlama3 앱을 열고 어느 정도 시간을 들여 빌드하도록 합시다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e[\u003cimg src=\"/assets/img/2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile_19.png\" alt=\"image\"\u003e]\n(\u003ca href=\"https://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://miro.medium.com/v2/resize:fit:700/1*wdN1DDl127dzmjIal0FHig.gif\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e모바일 앱이 성공적으로 빌드되면 APK를 모바일 폰에 설치하세요. 바로 설치할 수 있는 APK가 여기에 있습니다.\u003c/p\u003e\n\u003cp\u003e오프라인 사용을 위해 Llama3–8B-Instruct 모델을 모바일 기기에 구동하는 데 성공한 것을 축하드립니다. 이 기사에서 가치 있는 통찰을 얻었기를 기대합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e위의 GitHub 저장소에서 전체 프로젝트를 확인할 수 있습니다.\n\u003ca href=\"https://github.com/NSTiwari/Llama3-on-Mobile\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/NSTiwari/Llama3-on-Mobile\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e작품을 좋아하셨다면 저장소에 ⭐을 남겨주시고, 동료 온디바이스 AI 개발자들 사이에 소식을 전파해주세요. 앞으로 더욱 흥미로운 프로젝트와 블로그를 기대해주세요.\u003c/p\u003e\n\u003ch2\u003e감사의 글\u003c/h2\u003e\n\u003cp\u003eMobileLlama3는 MLC-LLM을 영감을 받아 제작되었으며, 이 프로젝트를 오픈 소스로 만들어주신 MLC-LLM에게 감사드립니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e참고 자료 및 자원\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eLlama-3-8B-Instruct 모델을 양자화하고 변환하는 Colab 노트북\u003c/li\u003e\n\u003cli\u003eMobileLlama3 GitHub 저장소\u003c/li\u003e\n\u003cli\u003e변환된 가중치를 위한 HuggingFace 저장소\u003c/li\u003e\n\u003cli\u003eMeta사의 Llama3 모델들\u003c/li\u003e\n\u003cli\u003eMLC-LLM\u003c/li\u003e\n\u003cli\u003eMLC-LLM용 Android SDK\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-MLStoryMobileLlama3RunLlama3locallyonmobile"},"buildId":"JlBEgQDLGRx6DYlBnT8eD","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>