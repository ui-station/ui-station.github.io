<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-06-30-HowtorunOllamalocallyonGPUwithDocker" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-06-30-HowtorunOllamalocallyonGPUwithDocker" data-gatsby-head="true"/><meta name="twitter:title" content="Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-06-30 22:20" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-70e5ce89f3d962ac.js" defer=""></script><script src="/_next/static/wfHLuDA3kTGBYfaM5IGXk/_buildManifest.js" defer=""></script><script src="/_next/static/wfHLuDA3kTGBYfaM5IGXk/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jun 30, 2024</span><span class="posts_reading_time__f7YPP">8<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-06-30-HowtorunOllamalocallyonGPUwithDocker&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>대형 언어 모델(LLMs)을 사용해보고 싶지만 토큰, 구독료 또는 API 키를 지불하고 싶지 않으신가요? 자신의 노트북(또는 전용 하드웨어)에서 실행하고 Gen AI의 강력함을 누리고 싶으신가요? 그렇다면 Ollama를 확인해보세요. 이 블로그에서는 다음을 안내해 드리겠습니다.</p>
<ul>
<li>도커를 사용하여 빠르게 노트북(Windows 또는 Mac)에 Ollama를 설치하는 방법</li>
<li>Ollama WebUI를 실행하고 Gen AI 플레이그라운드를 즐기는 방법</li>
<li>더 빠른 추론을 위해 노트북의 Nvidia GPU 활용</li>
<li>Ollama를 사용하여 Python Streamlit Gen AI 응용프로그램 빌드하는 방법</li>
</ul>
<h1>사전 요구 사항</h1>
<p>본 안내서를 통해 Ollama를 로컬에서 실행하려면 다음이 필요합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>도커 및 도커-컴포즈 또는 도커 데스크톱을 사용해주세요.</li>
<li>NVIDIA GPU — GPU를 사용할 경우에는 그래픽 카드를 사용하고, 그렇지 않을 경우 노트북의 CPU를 사용합니다.</li>
<li>Python 버전 3이 필요합니다.</li>
<li>Ollama를 실행하기 위해 충분한 디스크 공간이 필요합니다. 모델 파일은 적어도 10GB의 여유 공간이 필요하지만, 이것만으로는 충분하지 않습니다. 총 디스크 공간의 20%를 여유롭게 남겨 두어야 합니다. 그렇지 않으면 모델 파일에 충분한 공간이 있더라도 Ollama를 시작할 때 문제가 발생할 수 있습니다.</li>
<li>Docker 데스크톱을 구성할 때, Docker에 충분한 양의 CPU 및 메모리를 제공해주세요.</li>
</ul>
<h1>Ollama 설치 방법</h1>
<p>Ollama를 로컬에 실행하려면 다음 저장소를 클론하고 아래와 같이 docker-compose를 사용하여 실행해주세요,</p>
<pre><code class="hljs language-js">git clone git@github.<span class="hljs-property">com</span>:sujithrpillai/ollama.<span class="hljs-property">git</span>
cd ollama/ollama
docker-compose up -d
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>만약 docker-compose.yml 파일을 보면, 로컬 폴더인 models가 /root/.ollama/models로 매핑되어 있는 것을 알 수 있습니다. 이 폴더는 다운로드된 모델을 저장하는 곳입니다. 따라서 컨테이너를 다시 배포해도 모델 파일을 다시 다운로드할 필요가 없습니다.</p>
<p>또한 genai-network라는 도커 브릿지 네트워크를 사용하고 있음을 알 수 있습니다. 이는 컨테이너 간 연결성에 매우 중요합니다. 나중에 GenAI 애플리케이션을 실행할 때 Ollama 컨테이너의 DNS 이름을 프로그램에서 사용할 것입니다.</p>
<h1>모델 파일 다운로드</h1>
<p>Ollama는 다양한 LLM 모델을 지원하고 있으며 목록은 계속 늘어나고 있습니다. 현재 제공되는 모델을 확인하려면 여기를 확인하세요: <a href="https://ollama.com/library" rel="nofollow" target="_blank">https://ollama.com/library</a>. 모델 파일 크기는 다음을 확인할 수 있습니다: <a href="https://github.com/ollama/ollama?tab=readme-ov-file#model-library" rel="nofollow" target="_blank">https://github.com/ollama/ollama?tab=readme-ov-file#model-library</a></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>우리는 Gen AI 애플리케이션에 사용할 텍스트 모델 llama3과 임베딩 모델 all-minilm을 불러올 것입니다.</p>
<pre><code class="hljs language-js">docker-compose exec -it ollama bash
ollama pull llama3
ollama pull all-minilm
</code></pre>
<p>다운로드가 완료되면 단순히 exit를 입력하여 컨테이너 셸을 빠져나오세요.</p>
<p>노트북의 models 폴더로 이동하면 LLM 모델을 위한 파일 세트가 생성된 것을 확인할 수 있습니다. curl <a href="http://localhost:11434" rel="nofollow" target="_blank">http://localhost:11434</a> 로 애플리케이션이 작동 중인지 확인해보세요. 아래와 같이 Ollama가 실행되고 있는 것을 보여줘야 합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png">
<p>Ollama의 웹 사용자 인터페이스에 액세스하려면 <a href="http://localhost:3000" rel="nofollow" target="_blank">http://localhost:3000</a> 으로 이동하십시오. 처음에는 가짜 이메일 및 비밀번호로 가입해야 합니다. 그런 다음 콘솔에 로그인하십시오. 드롭다운에서 GenAI 모델을 선택하고 다양한 프롬프트로 테스트할 수 있는 플레이그라운드에서 사용할 수 있습니다.</p>
<img src="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_1.png">
<h1>추론을 위한 GPU 사용</h1>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>노트북의 GPU를 사용하여 추론하려면 docker-compose.yml 파일을 약간 수정할 수 있습니다. 도커에서 GPU를 사용하는 자세한 정보는 이 설명서를 참조하세요. 해야 할 일은 docker-compose.yml 파일에서 ollama 서비스를 수정하는 것뿐입니다. 아래와 같이 수정하세요.</p>
<pre><code class="hljs language-js">    <span class="hljs-attr">deploy</span>:
      <span class="hljs-attr">resources</span>:
        <span class="hljs-attr">reservations</span>:
          <span class="hljs-attr">devices</span>:
            - <span class="hljs-attr">driver</span>: nvidia
              <span class="hljs-attr">count</span>: all
              <span class="hljs-attr">capabilities</span>: [gpu]
</code></pre>
<p>제가 제공한 docker-compose.yml 파일에서 이러한 줄 (11번째 줄부터 17번째 줄)은 주석 처리되어 있습니다. 활성화하려면 주석 처리를 해제하세요.</p>
<p>컨테이너 내에서 ollama ps 명령을 실행하여 차이를 확인할 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Mac M1 Pro에서는 GPU가 없습니다:</p>
<p><img src="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_2.png" alt="GPU 이미지"></p>
<p>Windows에서 Nvidia GPU를 사용하는 경우:</p>
<p><img src="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_3.png" alt="GPU 이미지"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>Gen AI RAG 애플리케이션</h1>
<p>간단한 GenAI RAG 애플리케이션을 사용하여 텍스트 모델(llama3)과 임베딩 모델(all-minilm)을 모두 사용하여 애플리케이션을 구축해 봅시다.</p>
<p>이전 단계에서 설정한 Ollama가 LLM 모델을 다운로드하여 실행 중인지 확인하세요. 리포지토리의 앱 폴더로 이동하고 docker-compose up -d 명령을 실행하여 실행하세요.</p>
<p>이렇게 함으로써 RAG를 사용하는 매우 작은 Gen AI 애플리케이션이 실행됩니다. 이 애플리케이션에는 PDF 파일을 업로드할 수 있는 UI 요소가 제공됩니다. 파일을 업로드한 후 애플리케이션은 파일을 텍스트로 변환하고 벡터화하여 In-memory FAISS 벡터 데이터베이스에 저장합니다. 그런 다음 텍스트 입력 UI 요소를 사용하여 LLM에 질문을 하게 됩니다. 질문은 벡터 데이터베이스에서 유사성 검색을 수행하는 데 사용됩니다. 질문, 검색 결과 및 컨텍스트가 LLM에 전달되어 의미 있는 답변을 생성합니다. 그런 다음 UI에 답변이 표시됩니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>테스트 목적으로 이상한 데이터가 들어 있는 샘플 imaginary_world.pdf를 제공했어요. RAG가 정말 작동되는지 확인할 수 있어요.</p>
<p>애플리케이션 코드에서 LLM이 정의되는 방식을 알아볼 수 있어요. 우리는 Langchain을 사용해서 이 작업을 수행했어요 (간단한 표현을 위해 변수들을 제거했기 때문에 아래의 파이썬 파일에 작성된 방식과 차이가 있을 수 있어요.)</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">llms</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Ollama</span>
llm = <span class="hljs-title class_">Ollama</span>(base_url= <span class="hljs-attr">http</span>:<span class="hljs-comment">//ollama:11434, model=llama3)</span>
<span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">embeddings</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">OllamaEmbeddings</span>
embeddings = <span class="hljs-title class_">OllamaEmbeddings</span>(base_url= <span class="hljs-attr">http</span>:<span class="hljs-comment">//ollama:11434, model=all-minilm)</span>
</code></pre>
<p>다른 방법으로도 llm을 직접 가져와서 사용하는 방법이 있어요. 아래에 그 방법을 보여드릴게요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> ollama <span class="hljs-keyword">import</span> <span class="hljs-title class_">Client</span>
ollama_client = <span class="hljs-title class_">Client</span>(host=<span class="hljs-string">'http://ollama:11434'</span>)
user_input = “<span class="hljs-title class_">Your</span> question goes here”
llm_response = ollama_client.<span class="hljs-title function_">chat</span>(
     model=llama3, messages=[{<span class="hljs-string">'role'</span>: <span class="hljs-string">'user'</span>,<span class="hljs-string">'content'</span>: user_input,},]
)
</code></pre>
<p>애플리케이션은 <a href="http://localhost:5000%EC%97%90%EC%84%9C" rel="nofollow" target="_blank">http://localhost:5000에서</a> 액세스할 수 있습니다. 아래는 애플리케이션에서의 샘플 응답입니다,</p>
<img src="/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_4.png">
<p>다양한 프롬프트로 플레이해보세요.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>참고 자료</h1>
<p>이 내용을 작성하는 동안 사용한 참고 자료는 다음과 같습니다.</p>
<ul>
<li>Ollama 웹사이트 — <a href="https://ollama.com/" rel="nofollow" target="_blank">https://ollama.com/</a></li>
<li>Ollama Open WebUI — <a href="https://github.com/open-webui/open-webui" rel="nofollow" target="_blank">https://github.com/open-webui/open-webui</a></li>
<li>Ollama Github 저장소 — <a href="https://github.com/ollama/ollama" rel="nofollow" target="_blank">https://github.com/ollama/ollama</a></li>
<li>Streamlit — <a href="https://streamlit.io/" rel="nofollow" target="_blank">https://streamlit.io/</a></li>
<li>도커 데스크톱 GPU 지원 — docs.docker.com/desktop/gpu/</li>
</ul>
<p>자신만의 환경을 구축하여 Gen AI 프로젝트를 테스트할 수 있는 방법을 간단히 살펴봤을 거예요. 댓글로 의견을 공유해주세요. 즐거운 코딩 되세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Docker를 사용하여 GPU에서 Ollama를 로컬로 실행하는 방법","description":"","date":"2024-06-30 22:20","slug":"2024-06-30-HowtorunOllamalocallyonGPUwithDocker","content":"\n\n대형 언어 모델(LLMs)을 사용해보고 싶지만 토큰, 구독료 또는 API 키를 지불하고 싶지 않으신가요? 자신의 노트북(또는 전용 하드웨어)에서 실행하고 Gen AI의 강력함을 누리고 싶으신가요? 그렇다면 Ollama를 확인해보세요. 이 블로그에서는 다음을 안내해 드리겠습니다.\n\n- 도커를 사용하여 빠르게 노트북(Windows 또는 Mac)에 Ollama를 설치하는 방법\n- Ollama WebUI를 실행하고 Gen AI 플레이그라운드를 즐기는 방법\n- 더 빠른 추론을 위해 노트북의 Nvidia GPU 활용\n- Ollama를 사용하여 Python Streamlit Gen AI 응용프로그램 빌드하는 방법\n\n# 사전 요구 사항\n\n본 안내서를 통해 Ollama를 로컬에서 실행하려면 다음이 필요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 도커 및 도커-컴포즈 또는 도커 데스크톱을 사용해주세요.\n- NVIDIA GPU — GPU를 사용할 경우에는 그래픽 카드를 사용하고, 그렇지 않을 경우 노트북의 CPU를 사용합니다.\n- Python 버전 3이 필요합니다.\n- Ollama를 실행하기 위해 충분한 디스크 공간이 필요합니다. 모델 파일은 적어도 10GB의 여유 공간이 필요하지만, 이것만으로는 충분하지 않습니다. 총 디스크 공간의 20%를 여유롭게 남겨 두어야 합니다. 그렇지 않으면 모델 파일에 충분한 공간이 있더라도 Ollama를 시작할 때 문제가 발생할 수 있습니다.\n- Docker 데스크톱을 구성할 때, Docker에 충분한 양의 CPU 및 메모리를 제공해주세요.\n\n# Ollama 설치 방법\n\nOllama를 로컬에 실행하려면 다음 저장소를 클론하고 아래와 같이 docker-compose를 사용하여 실행해주세요,\n\n```js\ngit clone git@github.com:sujithrpillai/ollama.git\ncd ollama/ollama\ndocker-compose up -d\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 docker-compose.yml 파일을 보면, 로컬 폴더인 models가 /root/.ollama/models로 매핑되어 있는 것을 알 수 있습니다. 이 폴더는 다운로드된 모델을 저장하는 곳입니다. 따라서 컨테이너를 다시 배포해도 모델 파일을 다시 다운로드할 필요가 없습니다.\n\n또한 genai-network라는 도커 브릿지 네트워크를 사용하고 있음을 알 수 있습니다. 이는 컨테이너 간 연결성에 매우 중요합니다. 나중에 GenAI 애플리케이션을 실행할 때 Ollama 컨테이너의 DNS 이름을 프로그램에서 사용할 것입니다.\n\n# 모델 파일 다운로드\n\nOllama는 다양한 LLM 모델을 지원하고 있으며 목록은 계속 늘어나고 있습니다. 현재 제공되는 모델을 확인하려면 여기를 확인하세요: https://ollama.com/library. 모델 파일 크기는 다음을 확인할 수 있습니다: https://github.com/ollama/ollama?tab=readme-ov-file#model-library\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 Gen AI 애플리케이션에 사용할 텍스트 모델 llama3과 임베딩 모델 all-minilm을 불러올 것입니다.\n\n```js\ndocker-compose exec -it ollama bash\nollama pull llama3\nollama pull all-minilm\n```\n\n다운로드가 완료되면 단순히 exit를 입력하여 컨테이너 셸을 빠져나오세요.\n\n노트북의 models 폴더로 이동하면 LLM 모델을 위한 파일 세트가 생성된 것을 확인할 수 있습니다. curl http://localhost:11434 로 애플리케이션이 작동 중인지 확인해보세요. 아래와 같이 Ollama가 실행되고 있는 것을 보여줘야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png\" /\u003e\n\nOllama의 웹 사용자 인터페이스에 액세스하려면 http://localhost:3000 으로 이동하십시오. 처음에는 가짜 이메일 및 비밀번호로 가입해야 합니다. 그런 다음 콘솔에 로그인하십시오. 드롭다운에서 GenAI 모델을 선택하고 다양한 프롬프트로 테스트할 수 있는 플레이그라운드에서 사용할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_1.png\" /\u003e\n\n# 추론을 위한 GPU 사용\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n노트북의 GPU를 사용하여 추론하려면 docker-compose.yml 파일을 약간 수정할 수 있습니다. 도커에서 GPU를 사용하는 자세한 정보는 이 설명서를 참조하세요. 해야 할 일은 docker-compose.yml 파일에서 ollama 서비스를 수정하는 것뿐입니다. 아래와 같이 수정하세요.\n\n```js\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n```\n\n제가 제공한 docker-compose.yml 파일에서 이러한 줄 (11번째 줄부터 17번째 줄)은 주석 처리되어 있습니다. 활성화하려면 주석 처리를 해제하세요.\n\n컨테이너 내에서 ollama ps 명령을 실행하여 차이를 확인할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMac M1 Pro에서는 GPU가 없습니다:\n\n\n![GPU 이미지](/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_2.png)\n\n\nWindows에서 Nvidia GPU를 사용하는 경우:\n\n\n![GPU 이미지](/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_3.png)\n\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# Gen AI RAG 애플리케이션\n\n간단한 GenAI RAG 애플리케이션을 사용하여 텍스트 모델(llama3)과 임베딩 모델(all-minilm)을 모두 사용하여 애플리케이션을 구축해 봅시다.\n\n이전 단계에서 설정한 Ollama가 LLM 모델을 다운로드하여 실행 중인지 확인하세요. 리포지토리의 앱 폴더로 이동하고 docker-compose up -d 명령을 실행하여 실행하세요.\n\n이렇게 함으로써 RAG를 사용하는 매우 작은 Gen AI 애플리케이션이 실행됩니다. 이 애플리케이션에는 PDF 파일을 업로드할 수 있는 UI 요소가 제공됩니다. 파일을 업로드한 후 애플리케이션은 파일을 텍스트로 변환하고 벡터화하여 In-memory FAISS 벡터 데이터베이스에 저장합니다. 그런 다음 텍스트 입력 UI 요소를 사용하여 LLM에 질문을 하게 됩니다. 질문은 벡터 데이터베이스에서 유사성 검색을 수행하는 데 사용됩니다. 질문, 검색 결과 및 컨텍스트가 LLM에 전달되어 의미 있는 답변을 생성합니다. 그런 다음 UI에 답변이 표시됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n테스트 목적으로 이상한 데이터가 들어 있는 샘플 imaginary_world.pdf를 제공했어요. RAG가 정말 작동되는지 확인할 수 있어요.\n\n애플리케이션 코드에서 LLM이 정의되는 방식을 알아볼 수 있어요. 우리는 Langchain을 사용해서 이 작업을 수행했어요 (간단한 표현을 위해 변수들을 제거했기 때문에 아래의 파이썬 파일에 작성된 방식과 차이가 있을 수 있어요.)\n\n```js\nfrom langchain_community.llms import Ollama\nllm = Ollama(base_url= http://ollama:11434, model=llama3)\nfrom langchain_community.embeddings import OllamaEmbeddings\nembeddings = OllamaEmbeddings(base_url= http://ollama:11434, model=all-minilm)\n```\n\n다른 방법으로도 llm을 직접 가져와서 사용하는 방법이 있어요. 아래에 그 방법을 보여드릴게요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nfrom ollama import Client\nollama_client = Client(host='http://ollama:11434')\nuser_input = “Your question goes here”\nllm_response = ollama_client.chat(\n     model=llama3, messages=[{'role': 'user','content': user_input,},]\n)\n```\n\n애플리케이션은 http://localhost:5000에서 액세스할 수 있습니다. 아래는 애플리케이션에서의 샘플 응답입니다,\n\n\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_4.png\" /\u003e\n\n다양한 프롬프트로 플레이해보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 참고 자료\n\n이 내용을 작성하는 동안 사용한 참고 자료는 다음과 같습니다.\n\n- Ollama 웹사이트 — https://ollama.com/\n- Ollama Open WebUI — https://github.com/open-webui/open-webui\n- Ollama Github 저장소 — https://github.com/ollama/ollama\n- Streamlit — https://streamlit.io/\n- 도커 데스크톱 GPU 지원 — docs.docker.com/desktop/gpu/\n\n자신만의 환경을 구축하여 Gen AI 프로젝트를 테스트할 수 있는 방법을 간단히 살펴봤을 거예요. 댓글로 의견을 공유해주세요. 즐거운 코딩 되세요!","ogImage":{"url":"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png"},"coverImage":"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png","tag":["Tech"],"readingTime":8},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e대형 언어 모델(LLMs)을 사용해보고 싶지만 토큰, 구독료 또는 API 키를 지불하고 싶지 않으신가요? 자신의 노트북(또는 전용 하드웨어)에서 실행하고 Gen AI의 강력함을 누리고 싶으신가요? 그렇다면 Ollama를 확인해보세요. 이 블로그에서는 다음을 안내해 드리겠습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e도커를 사용하여 빠르게 노트북(Windows 또는 Mac)에 Ollama를 설치하는 방법\u003c/li\u003e\n\u003cli\u003eOllama WebUI를 실행하고 Gen AI 플레이그라운드를 즐기는 방법\u003c/li\u003e\n\u003cli\u003e더 빠른 추론을 위해 노트북의 Nvidia GPU 활용\u003c/li\u003e\n\u003cli\u003eOllama를 사용하여 Python Streamlit Gen AI 응용프로그램 빌드하는 방법\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e사전 요구 사항\u003c/h1\u003e\n\u003cp\u003e본 안내서를 통해 Ollama를 로컬에서 실행하려면 다음이 필요합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e도커 및 도커-컴포즈 또는 도커 데스크톱을 사용해주세요.\u003c/li\u003e\n\u003cli\u003eNVIDIA GPU — GPU를 사용할 경우에는 그래픽 카드를 사용하고, 그렇지 않을 경우 노트북의 CPU를 사용합니다.\u003c/li\u003e\n\u003cli\u003ePython 버전 3이 필요합니다.\u003c/li\u003e\n\u003cli\u003eOllama를 실행하기 위해 충분한 디스크 공간이 필요합니다. 모델 파일은 적어도 10GB의 여유 공간이 필요하지만, 이것만으로는 충분하지 않습니다. 총 디스크 공간의 20%를 여유롭게 남겨 두어야 합니다. 그렇지 않으면 모델 파일에 충분한 공간이 있더라도 Ollama를 시작할 때 문제가 발생할 수 있습니다.\u003c/li\u003e\n\u003cli\u003eDocker 데스크톱을 구성할 때, Docker에 충분한 양의 CPU 및 메모리를 제공해주세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003eOllama 설치 방법\u003c/h1\u003e\n\u003cp\u003eOllama를 로컬에 실행하려면 다음 저장소를 클론하고 아래와 같이 docker-compose를 사용하여 실행해주세요,\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003egit clone git@github.\u003cspan class=\"hljs-property\"\u003ecom\u003c/span\u003e:sujithrpillai/ollama.\u003cspan class=\"hljs-property\"\u003egit\u003c/span\u003e\ncd ollama/ollama\ndocker-compose up -d\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e만약 docker-compose.yml 파일을 보면, 로컬 폴더인 models가 /root/.ollama/models로 매핑되어 있는 것을 알 수 있습니다. 이 폴더는 다운로드된 모델을 저장하는 곳입니다. 따라서 컨테이너를 다시 배포해도 모델 파일을 다시 다운로드할 필요가 없습니다.\u003c/p\u003e\n\u003cp\u003e또한 genai-network라는 도커 브릿지 네트워크를 사용하고 있음을 알 수 있습니다. 이는 컨테이너 간 연결성에 매우 중요합니다. 나중에 GenAI 애플리케이션을 실행할 때 Ollama 컨테이너의 DNS 이름을 프로그램에서 사용할 것입니다.\u003c/p\u003e\n\u003ch1\u003e모델 파일 다운로드\u003c/h1\u003e\n\u003cp\u003eOllama는 다양한 LLM 모델을 지원하고 있으며 목록은 계속 늘어나고 있습니다. 현재 제공되는 모델을 확인하려면 여기를 확인하세요: \u003ca href=\"https://ollama.com/library\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library\u003c/a\u003e. 모델 파일 크기는 다음을 확인할 수 있습니다: \u003ca href=\"https://github.com/ollama/ollama?tab=readme-ov-file#model-library\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/ollama/ollama?tab=readme-ov-file#model-library\u003c/a\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e우리는 Gen AI 애플리케이션에 사용할 텍스트 모델 llama3과 임베딩 모델 all-minilm을 불러올 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edocker-compose exec -it ollama bash\nollama pull llama3\nollama pull all-minilm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다운로드가 완료되면 단순히 exit를 입력하여 컨테이너 셸을 빠져나오세요.\u003c/p\u003e\n\u003cp\u003e노트북의 models 폴더로 이동하면 LLM 모델을 위한 파일 세트가 생성된 것을 확인할 수 있습니다. curl \u003ca href=\"http://localhost:11434\" rel=\"nofollow\" target=\"_blank\"\u003ehttp://localhost:11434\u003c/a\u003e 로 애플리케이션이 작동 중인지 확인해보세요. 아래와 같이 Ollama가 실행되고 있는 것을 보여줘야 합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_0.png\"\u003e\n\u003cp\u003eOllama의 웹 사용자 인터페이스에 액세스하려면 \u003ca href=\"http://localhost:3000\" rel=\"nofollow\" target=\"_blank\"\u003ehttp://localhost:3000\u003c/a\u003e 으로 이동하십시오. 처음에는 가짜 이메일 및 비밀번호로 가입해야 합니다. 그런 다음 콘솔에 로그인하십시오. 드롭다운에서 GenAI 모델을 선택하고 다양한 프롬프트로 테스트할 수 있는 플레이그라운드에서 사용할 수 있습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_1.png\"\u003e\n\u003ch1\u003e추론을 위한 GPU 사용\u003c/h1\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e노트북의 GPU를 사용하여 추론하려면 docker-compose.yml 파일을 약간 수정할 수 있습니다. 도커에서 GPU를 사용하는 자세한 정보는 이 설명서를 참조하세요. 해야 할 일은 docker-compose.yml 파일에서 ollama 서비스를 수정하는 것뿐입니다. 아래와 같이 수정하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e    \u003cspan class=\"hljs-attr\"\u003edeploy\u003c/span\u003e:\n      \u003cspan class=\"hljs-attr\"\u003eresources\u003c/span\u003e:\n        \u003cspan class=\"hljs-attr\"\u003ereservations\u003c/span\u003e:\n          \u003cspan class=\"hljs-attr\"\u003edevices\u003c/span\u003e:\n            - \u003cspan class=\"hljs-attr\"\u003edriver\u003c/span\u003e: nvidia\n              \u003cspan class=\"hljs-attr\"\u003ecount\u003c/span\u003e: all\n              \u003cspan class=\"hljs-attr\"\u003ecapabilities\u003c/span\u003e: [gpu]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e제가 제공한 docker-compose.yml 파일에서 이러한 줄 (11번째 줄부터 17번째 줄)은 주석 처리되어 있습니다. 활성화하려면 주석 처리를 해제하세요.\u003c/p\u003e\n\u003cp\u003e컨테이너 내에서 ollama ps 명령을 실행하여 차이를 확인할 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eMac M1 Pro에서는 GPU가 없습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_2.png\" alt=\"GPU 이미지\"\u003e\u003c/p\u003e\n\u003cp\u003eWindows에서 Nvidia GPU를 사용하는 경우:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_3.png\" alt=\"GPU 이미지\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003eGen AI RAG 애플리케이션\u003c/h1\u003e\n\u003cp\u003e간단한 GenAI RAG 애플리케이션을 사용하여 텍스트 모델(llama3)과 임베딩 모델(all-minilm)을 모두 사용하여 애플리케이션을 구축해 봅시다.\u003c/p\u003e\n\u003cp\u003e이전 단계에서 설정한 Ollama가 LLM 모델을 다운로드하여 실행 중인지 확인하세요. 리포지토리의 앱 폴더로 이동하고 docker-compose up -d 명령을 실행하여 실행하세요.\u003c/p\u003e\n\u003cp\u003e이렇게 함으로써 RAG를 사용하는 매우 작은 Gen AI 애플리케이션이 실행됩니다. 이 애플리케이션에는 PDF 파일을 업로드할 수 있는 UI 요소가 제공됩니다. 파일을 업로드한 후 애플리케이션은 파일을 텍스트로 변환하고 벡터화하여 In-memory FAISS 벡터 데이터베이스에 저장합니다. 그런 다음 텍스트 입력 UI 요소를 사용하여 LLM에 질문을 하게 됩니다. 질문은 벡터 데이터베이스에서 유사성 검색을 수행하는 데 사용됩니다. 질문, 검색 결과 및 컨텍스트가 LLM에 전달되어 의미 있는 답변을 생성합니다. 그런 다음 UI에 답변이 표시됩니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e테스트 목적으로 이상한 데이터가 들어 있는 샘플 imaginary_world.pdf를 제공했어요. RAG가 정말 작동되는지 확인할 수 있어요.\u003c/p\u003e\n\u003cp\u003e애플리케이션 코드에서 LLM이 정의되는 방식을 알아볼 수 있어요. 우리는 Langchain을 사용해서 이 작업을 수행했어요 (간단한 표현을 위해 변수들을 제거했기 때문에 아래의 파이썬 파일에 작성된 방식과 차이가 있을 수 있어요.)\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain_community.\u003cspan class=\"hljs-property\"\u003ellms\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOllama\u003c/span\u003e\nllm = \u003cspan class=\"hljs-title class_\"\u003eOllama\u003c/span\u003e(base_url= \u003cspan class=\"hljs-attr\"\u003ehttp\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//ollama:11434, model=llama3)\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain_community.\u003cspan class=\"hljs-property\"\u003eembeddings\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOllamaEmbeddings\u003c/span\u003e\nembeddings = \u003cspan class=\"hljs-title class_\"\u003eOllamaEmbeddings\u003c/span\u003e(base_url= \u003cspan class=\"hljs-attr\"\u003ehttp\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//ollama:11434, model=all-minilm)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다른 방법으로도 llm을 직접 가져와서 사용하는 방법이 있어요. 아래에 그 방법을 보여드릴게요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ollama \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eClient\u003c/span\u003e\nollama_client = \u003cspan class=\"hljs-title class_\"\u003eClient\u003c/span\u003e(host=\u003cspan class=\"hljs-string\"\u003e'http://ollama:11434'\u003c/span\u003e)\nuser_input = “\u003cspan class=\"hljs-title class_\"\u003eYour\u003c/span\u003e question goes here”\nllm_response = ollama_client.\u003cspan class=\"hljs-title function_\"\u003echat\u003c/span\u003e(\n     model=llama3, messages=[{\u003cspan class=\"hljs-string\"\u003e'role'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'user'\u003c/span\u003e,\u003cspan class=\"hljs-string\"\u003e'content'\u003c/span\u003e: user_input,},]\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e애플리케이션은 \u003ca href=\"http://localhost:5000%EC%97%90%EC%84%9C\" rel=\"nofollow\" target=\"_blank\"\u003ehttp://localhost:5000에서\u003c/a\u003e 액세스할 수 있습니다. 아래는 애플리케이션에서의 샘플 응답입니다,\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-06-30-HowtorunOllamalocallyonGPUwithDocker_4.png\"\u003e\n\u003cp\u003e다양한 프롬프트로 플레이해보세요.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cp\u003e이 내용을 작성하는 동안 사용한 참고 자료는 다음과 같습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOllama 웹사이트 — \u003ca href=\"https://ollama.com/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eOllama Open WebUI — \u003ca href=\"https://github.com/open-webui/open-webui\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/open-webui/open-webui\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eOllama Github 저장소 — \u003ca href=\"https://github.com/ollama/ollama\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/ollama/ollama\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eStreamlit — \u003ca href=\"https://streamlit.io/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://streamlit.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e도커 데스크톱 GPU 지원 — docs.docker.com/desktop/gpu/\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e자신만의 환경을 구축하여 Gen AI 프로젝트를 테스트할 수 있는 방법을 간단히 살펴봤을 거예요. 댓글로 의견을 공유해주세요. 즐거운 코딩 되세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-06-30-HowtorunOllamalocallyonGPUwithDocker"},"buildId":"wfHLuDA3kTGBYfaM5IGXk","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>