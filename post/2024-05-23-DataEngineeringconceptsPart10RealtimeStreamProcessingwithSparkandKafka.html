<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka" data-gatsby-head="true"/><meta name="twitter:title" content="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-23 14:05" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_buildManifest.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 23, 2024</span><span class="posts_reading_time__f7YPP">17<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.</p>
<p>목차:</p>
<ol>
<li>스트림 처리란</li>
<li>카프카의 특징</li>
<li>카프카 구성</li>
<li>카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리</li>
<li>스파크 구조화 스트리밍 API</li>
<li>데이타브릭스 델타 레이크</li>
<li>실전 프로젝트</li>
</ol>
<p>이전 데이터 보안 부분으로 이동하는 링크입니다:</p>
<h2>스트림 처리란?</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.</p>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png" alt="데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리"></p>
<p>스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.</p>
<p>카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>카프카 특징</h2>
<ul>
<li>
<p>견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.</p>
</li>
<li>
<p>유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.</p>
</li>
<li>
<p>확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:</p>
<ul>
<li>파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.</li>
<li>수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.</li>
</ul>
</li>
</ul>
<h2>카프카 구성</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.</p>
<ul>
<li>파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.</li>
<li>복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.</li>
<li>유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.</li>
<li>자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.</li>
<li>ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.</li>
</ul>
<h2>다른 카프카 서비스</h2>
<p>Kafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png">
<p>ksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.</p>
<img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png">
<p>스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png" alt="이미지"></p>
<h2>Spark Structured Streaming API</h2>
<p>Spark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.</p>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png" alt="이미지"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.</p>
<p>또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:</p>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png" alt="이미지"></p>
<h2>Databricks Delta Lake</h2>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Databricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.</p>
<p>Delta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.</p>
<p>스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.</p>
<img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png">
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>실시간 스트리밍 아키텍처</h2>
<p>카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.</p>
<p>문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:</p>
<ul>
<li>단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성</li>
<li>단계 2: 토픽 생성</li>
<li>단계 3: 클러스터 API 키 쌍 생성</li>
</ul>
<p>위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-</p>
<ul>
<li>단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png" alt="Image"></p>
<p>Use the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:</p>
<ul>
<li>Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated</li>
</ul>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png" alt="Image"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>
<p>단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성</p>
</li>
<li>
<p>단계 7: Kafka 토픽에서 스트림 읽기</p>
</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> pyspark.<span class="hljs-property">sql</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SparkSession</span>

spark = <span class="hljs-title class_">SparkSession</span>.<span class="hljs-property">builder</span>.<span class="hljs-title function_">appName</span>(<span class="hljs-string">"Environmental Reporting"</span>).<span class="hljs-title function_">getOrCreate</span>()

kafkaDF = spark \
    .<span class="hljs-property">readStream</span> \
    .<span class="hljs-title function_">format</span>(<span class="hljs-string">"kafka"</span>) \
    .<span class="hljs-title function_">option</span>(<span class="hljs-string">"kafka.bootstrap.servers"</span>, <span class="hljs-string">"abcd.us-west4.gcp.confluent.cloud:9092"</span>) \
    .<span class="hljs-title function_">option</span>(<span class="hljs-string">"subscribe"</span>, <span class="hljs-string">"illegal_dumping"</span>) \
    .<span class="hljs-title function_">option</span>(<span class="hljs-string">"startingOffsets"</span>, <span class="hljs-string">"earliest"</span>) \
    .<span class="hljs-title function_">option</span>(<span class="hljs-string">"kafka.security.protocol"</span>,<span class="hljs-string">"SASL_SSL"</span>) \
    .<span class="hljs-title function_">option</span>(<span class="hljs-string">"kafka.sasl.mechanism"</span>, <span class="hljs-string">"PLAIN"</span>) \
    .<span class="hljs-title function_">option</span>(<span class="hljs-string">"kafka.sasl.jaas.config"</span>, <span class="hljs-string">""</span><span class="hljs-string">"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="</span><span class="hljs-string">" password="</span><span class="hljs-string">";"</span><span class="hljs-string">""</span>) \
    .<span class="hljs-title function_">load</span>()

processedDF = kafkaDF.<span class="hljs-title function_">selectExpr</span>(<span class="hljs-string">"CAST(key AS STRING)"</span>, <span class="hljs-string">"CAST(value AS STRING)"</span>)

<span class="hljs-title function_">display</span>(processedDF)
</code></pre>
<ul>
<li>단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성</li>
</ul>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">functions</span> <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">from</span>  pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">functions</span> <span class="hljs-keyword">import</span> col, struct, to_json
<span class="hljs-keyword">from</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">types</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">StructField</span>, <span class="hljs-title class_">StructType</span>, <span class="hljs-title class_">StringType</span>, <span class="hljs-title class_">MapType</span>

json_schema = <span class="hljs-title class_">StructType</span>(
  [
    <span class="hljs-title class_">StructField</span>(<span class="hljs-string">"incident_type"</span>, <span class="hljs-title class_">StringType</span>(), nullable = <span class="hljs-title class_">False</span>),
    <span class="hljs-title class_">StructField</span>(<span class="hljs-string">"location"</span>, <span class="hljs-title class_">StringType</span>(), nullable = <span class="hljs-title class_">False</span>),
    <span class="hljs-title class_">StructField</span>(<span class="hljs-string">"description"</span>, <span class="hljs-title class_">StringType</span>(), nullable = <span class="hljs-title class_">True</span>),
    <span class="hljs-title class_">StructField</span>(<span class="hljs-string">"contact"</span>, <span class="hljs-title class_">StringType</span>(), nullable = <span class="hljs-title class_">True</span>)
  ]
)

# <span class="hljs-title class_">Using</span> <span class="hljs-title class_">Spark</span> <span class="hljs-variable constant_">SQL</span> to write queries on the streaming data <span class="hljs-keyword">in</span> processedDF

query = processedDF.<span class="hljs-title function_">withColumn</span>(<span class="hljs-string">'value'</span>, F.<span class="hljs-title function_">from_json</span>(F.<span class="hljs-title function_">col</span>(<span class="hljs-string">'value'</span>).<span class="hljs-title function_">cast</span>(<span class="hljs-string">'string'</span>), json_schema))  \
      .<span class="hljs-title function_">select</span>(F.<span class="hljs-title function_">col</span>(<span class="hljs-string">"value.incident_type"</span>),F.<span class="hljs-title function_">col</span>(<span class="hljs-string">"value.location"</span>))
<span class="hljs-title function_">display</span>(query)
</code></pre>
<p>We will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.</p>
<p>Define a UDF(User Defined Function) to find out the region from the location:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">functions</span> <span class="hljs-keyword">import</span> udf
<span class="hljs-keyword">from</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">types</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">StringType</span>

# <span class="hljs-title class_">Define</span> the regions_to_states dictionary
regions_to_states = {
    <span class="hljs-string">'South'</span>: [<span class="hljs-string">'West Virginia'</span>, <span class="hljs-string">'District of Columbia'</span>, <span class="hljs-string">'Maryland'</span>, <span class="hljs-string">'Virginia'</span>,
              <span class="hljs-string">'Kentucky'</span>, <span class="hljs-string">'Tennessee'</span>, <span class="hljs-string">'North Carolina'</span>, <span class="hljs-string">'Mississippi'</span>,
              <span class="hljs-string">'Arkansas'</span>, <span class="hljs-string">'Louisiana'</span>, <span class="hljs-string">'Alabama'</span>, <span class="hljs-string">'Georgia'</span>, <span class="hljs-string">'South Carolina'</span>,
              <span class="hljs-string">'Florida'</span>, <span class="hljs-string">'Delaware'</span>],
    <span class="hljs-string">'Southwest'</span>: [<span class="hljs-string">'Arizona'</span>, <span class="hljs-string">'New Mexico'</span>, <span class="hljs-string">'Oklahoma'</span>, <span class="hljs-string">'Texas'</span>],
    <span class="hljs-string">'West'</span>: [<span class="hljs-string">'Washington'</span>, <span class="hljs-string">'Oregon'</span>, <span class="hljs-string">'California'</span>, <span class="hljs-string">'Nevada'</span>, <span class="hljs-string">'Idaho'</span>, <span class="hljs-string">'Montana'</span>,
             <span class="hljs-string">'Wyoming'</span>, <span class="hljs-string">'Utah'</span>, <span class="hljs-string">'Colorado'</span>, <span class="hljs-string">'Alaska'</span>, <span class="hljs-string">'Hawaii'</span>],
    <span class="hljs-string">'Midwest'</span>: [<span class="hljs-string">'North Dakota'</span>, <span class="hljs-string">'South Dakota'</span>, <span class="hljs-string">'Nebraska'</span>, <span class="hljs-string">'Kansas'</span>, <span class="hljs-string">'Minnesota'</span>,
                <span class="hljs-string">'Iowa'</span>, <span class="hljs-string">'Missouri'</span>, <span class="hljs-string">'Wisconsin'</span>, <span class="hljs-string">'Illinois'</span>, <span class="hljs-string">'Michigan'</span>, <span class="hljs-string">'Indiana'</span>,
                <span class="hljs-string">'Ohio'</span>],
    <span class="hljs-string">'Northeast'</span>: [<span class="hljs-string">'Maine'</span>, <span class="hljs-string">'Vermont'</span>, <span class="hljs-string">'New York'</span>, <span class="hljs-string">'New Hampshire'</span>, <span class="hljs-string">'Massachusetts'</span>,
                  <span class="hljs-string">'Rhode Island'</span>, <span class="hljs-string">'Connecticut'</span>, <span class="hljs-string">'New Jersey'</span>, <span class="hljs-string">'Pennsylvania'</span>]
}

#<span class="hljs-keyword">from</span> geotext <span class="hljs-keyword">import</span> <span class="hljs-title class_">GeoText</span>
<span class="hljs-keyword">from</span> geopy.<span class="hljs-property">geocoders</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Nominatim</span>

# <span class="hljs-title class_">Define</span> a <span class="hljs-keyword">function</span> to extract state names <span class="hljs-keyword">from</span> location text
def <span class="hljs-title function_">extract_state</span>(location_text):
    geolocator = <span class="hljs-title class_">Nominatim</span>(user_agent=<span class="hljs-string">"my_application"</span>)
    location = geolocator.<span class="hljs-title function_">geocode</span>(location_text)
    #<span class="hljs-title function_">print</span>(location)
    #<span class="hljs-title function_">print</span>(<span class="hljs-title function_">type</span>(location.<span class="hljs-property">raw</span>))
    <span class="hljs-keyword">if</span> <span class="hljs-attr">location</span>:
        state = location.<span class="hljs-property">raw</span>[<span class="hljs-string">'display_name'</span>].<span class="hljs-title function_">split</span>(<span class="hljs-string">','</span>)[-<span class="hljs-number">2</span>]
        <span class="hljs-keyword">return</span> state
    <span class="hljs-attr">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">"Unknown"</span>

# <span class="hljs-title class_">Create</span> a <span class="hljs-variable constant_">UDF</span> to map states to regions
@<span class="hljs-title function_">udf</span>(<span class="hljs-title class_">StringType</span>())
def <span class="hljs-title function_">map_state_to_region</span>(location):
    state = <span class="hljs-title function_">extract_state</span>(location).<span class="hljs-title function_">strip</span>()
    <span class="hljs-keyword">for</span> region, states <span class="hljs-keyword">in</span> regions_to_states.<span class="hljs-title function_">items</span>():
        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">in</span> <span class="hljs-attr">states</span>:
            <span class="hljs-keyword">return</span> region
    <span class="hljs-keyword">return</span> <span class="hljs-string">"Unknown"</span>  # <span class="hljs-title class_">Return</span> <span class="hljs-string">"Unknown"</span> <span class="hljs-keyword">for</span> states not found <span class="hljs-keyword">in</span> the dictionary

# <span class="hljs-title class_">Apply</span> the <span class="hljs-variable constant_">UDF</span> to map states to regions
df_with_region = query.<span class="hljs-title function_">withColumn</span>(<span class="hljs-string">"region"</span>, <span class="hljs-title function_">map_state_to_region</span>(query[<span class="hljs-string">"location"</span>]))

<span class="hljs-title function_">display</span>(df_with_region)
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">functions</span> <span class="hljs-keyword">import</span> udf
<span class="hljs-keyword">from</span> pyspark.<span class="hljs-property">sql</span>.<span class="hljs-property">types</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">StringType</span>
<span class="hljs-keyword">from</span> vaderSentiment.<span class="hljs-property">vaderSentiment</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">SentimentIntensityAnalyzer</span>

# <span class="hljs-variable constant_">VADER</span> 감정 분석기 초기화
analyzer = <span class="hljs-title class_">SentimentIntensityAnalyzer</span>()

# <span class="hljs-title class_">Description</span> 텍스트에 대한 감정 분석을 수행하는 함수 정의
def <span class="hljs-title function_">analyze_sentiment</span>(description):
    # <span class="hljs-variable constant_">VADER</span>에서 compound 감정 점수 가져오기
    sentiment_score = analyzer.<span class="hljs-title function_">polarity_scores</span>(description)[<span class="hljs-string">'neg'</span>]

    # 감정 점수를 기반으로 심각도 분류
    <span class="hljs-keyword">if</span> sentiment_score >= <span class="hljs-number">0.4</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">"High"</span>
    elif sentiment_score >= <span class="hljs-number">0.2</span> and sentiment_score &#x3C; <span class="hljs-number">0.4</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">"Medium"</span>
    <span class="hljs-attr">else</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">"Low"</span>

# 감정 분석을 위한 <span class="hljs-variable constant_">UDF</span> 생성
sentiment_udf = <span class="hljs-title function_">udf</span>(analyze_sentiment, <span class="hljs-title class_">StringType</span>())

# 처리된 <span class="hljs-title class_">DataFrame</span>(processedDF)의 description 열에 <span class="hljs-variable constant_">UDF</span> 적용
# 실제 <span class="hljs-title class_">DataFrame</span> 및 열 이름으로 <span class="hljs-string">"processedDF"</span> 및 <span class="hljs-string">"description_column"</span>을 대체합니다.
processedDF_with_severity = query.<span class="hljs-title function_">withColumn</span>(<span class="hljs-string">"severity"</span>, <span class="hljs-title function_">sentiment_udf</span>(<span class="hljs-string">"description"</span>))

# 추가된 심각도 열이 있는 <span class="hljs-title class_">DataFrame</span> 표시
<span class="hljs-title function_">display</span>(processedDF_with_severity)
</code></pre>
<p>환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.</p>
<p>데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png" alt="Real-time Stream Processing"></p>
<p>위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:</p>
<p>또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:</p>
<pre><code class="hljs language-js"># 스트리밍 <span class="hljs-title class_">DataFrame</span>을 임시 뷰로 등록
processedDF_with_severity.<span class="hljs-title function_">createOrReplaceTempView</span>(<span class="hljs-string">"incident_reports"</span>)

# 집계를 위한 <span class="hljs-variable constant_">SQL</span> 쿼리 정의
total_incidents_query = <span class="hljs-string">""</span><span class="hljs-string">"
    SELECT
        incident_type,
        COUNT(*) AS total_incidents
    FROM
        incident_reports
    GROUP BY
        incident_type
"</span><span class="hljs-string">""</span>

severity_incidents_query = <span class="hljs-string">""</span><span class="hljs-string">"
    SELECT
        incident_type,
        severity,
        COUNT(*) AS severity_incidents
    FROM
        incident_reports
    GROUP BY
        incident_type, severity
"</span><span class="hljs-string">""</span>

# 집계 수행
total_incidents_df = spark.<span class="hljs-title function_">sql</span>(total_incidents_query)
severity_incidents_df = spark.<span class="hljs-title function_">sql</span>(severity_incidents_query)

</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png" alt="Data Engineering Concepts"></p>
<p>Spark 구조화된 스트리밍 분석에 대한 일부 제한 사항:</p>
<ol>
<li>비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.</li>
<li>전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.</li>
</ol>
<ul>
<li>단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다</li>
</ul>
<pre><code class="hljs language-js"># <span class="hljs-title class_">Delta</span> <span class="hljs-title class_">Lake</span>에 스트리밍 데이터를 저장할 경로 정의
delta_table_path = <span class="hljs-string">"`result_delta_table`"</span>

# 스트리밍 쿼리에 대한 체크포인트 위치 설정
checkpoint_location = <span class="hljs-string">"/FileStore/tables/checkpoints"</span>

# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작
streaming_query = processedDF_with_severity.<span class="hljs-property">writeStream</span>\
  .<span class="hljs-title function_">outputMode</span>(<span class="hljs-string">"append"</span>)\
  .<span class="hljs-title function_">option</span>(<span class="hljs-string">"checkpointLocation"</span>, checkpoint_location)\
  .<span class="hljs-title function_">trigger</span>(processingTime=<span class="hljs-string">'10 seconds'</span>)\ # <span class="hljs-number">10</span>초ごとに 데이터의 미크로배치를 처리할 트리거 정의
  .<span class="hljs-title function_">format</span>(<span class="hljs-string">"delta"</span>)\
  .<span class="hljs-title function_">toTable</span>(delta_table_path)
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:</p>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png" alt="Delta Table as Dataframe"></p>
<p>또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:</p>
<p><img src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png" alt="Query Delta Table with SQL"></p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리","description":"","date":"2024-05-23 14:05","slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka","content":"\n이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\n\n목차:\n\n1. 스트림 처리란\n2. 카프카의 특징\n3. 카프카 구성\n4. 카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\n5. 스파크 구조화 스트리밍 API\n6. 데이타브릭스 델타 레이크\n7. 실전 프로젝트\n\n이전 데이터 보안 부분으로 이동하는 링크입니다:\n\n## 스트림 처리란?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\n\n![데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png)\n\n스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\n\n카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 카프카 특징\n\n- 견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\n- 유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\n- 확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\n\n  - 파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\n  - 수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\n\n## 카프카 구성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\n\n- 파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\n- 복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\n- 유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\n- 자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\n- ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\n\n## 다른 카프카 서비스\n\nKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\" /\u003e\n\nksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\" /\u003e\n\n스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png)\n\n## Spark Structured Streaming API\n\nSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\n\n또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png)\n\n## Databricks Delta Lake\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\n\nDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\n\n스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 실시간 스트리밍 아키텍처\n\n카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\n\n문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\n\n- 단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\n- 단계 2: 토픽 생성\n- 단계 3: 클러스터 API 키 쌍 생성\n\n위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\n\n- 단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png)\n\nUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\n\n- Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\n\n- 단계 7: Kafka 토픽에서 스트림 읽기\n\n```js\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Environmental Reporting\").getOrCreate()\n\nkafkaDF = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"abcd.us-west4.gcp.confluent.cloud:9092\") \\\n    .option(\"subscribe\", \"illegal_dumping\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\" password=\"\";\"\"\") \\\n    .load()\n\nprocessedDF = kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\ndisplay(processedDF)\n```\n\n- 단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nimport pyspark.sql.functions as F\nfrom  pyspark.sql.functions import col, struct, to_json\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\njson_schema = StructType(\n  [\n    StructField(\"incident_type\", StringType(), nullable = False),\n    StructField(\"location\", StringType(), nullable = False),\n    StructField(\"description\", StringType(), nullable = True),\n    StructField(\"contact\", StringType(), nullable = True)\n  ]\n)\n\n# Using Spark SQL to write queries on the streaming data in processedDF\n\nquery = processedDF.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n      .select(F.col(\"value.incident_type\"),F.col(\"value.location\"))\ndisplay(query)\n```\n\nWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\n\nDefine a UDF(User Defined Function) to find out the region from the location:\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the regions_to_states dictionary\nregions_to_states = {\n    'South': ['West Virginia', 'District of Columbia', 'Maryland', 'Virginia',\n              'Kentucky', 'Tennessee', 'North Carolina', 'Mississippi',\n              'Arkansas', 'Louisiana', 'Alabama', 'Georgia', 'South Carolina',\n              'Florida', 'Delaware'],\n    'Southwest': ['Arizona', 'New Mexico', 'Oklahoma', 'Texas'],\n    'West': ['Washington', 'Oregon', 'California', 'Nevada', 'Idaho', 'Montana',\n             'Wyoming', 'Utah', 'Colorado', 'Alaska', 'Hawaii'],\n    'Midwest': ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota',\n                'Iowa', 'Missouri', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana',\n                'Ohio'],\n    'Northeast': ['Maine', 'Vermont', 'New York', 'New Hampshire', 'Massachusetts',\n                  'Rhode Island', 'Connecticut', 'New Jersey', 'Pennsylvania']\n}\n\n#from geotext import GeoText\nfrom geopy.geocoders import Nominatim\n\n# Define a function to extract state names from location text\ndef extract_state(location_text):\n    geolocator = Nominatim(user_agent=\"my_application\")\n    location = geolocator.geocode(location_text)\n    #print(location)\n    #print(type(location.raw))\n    if location:\n        state = location.raw['display_name'].split(',')[-2]\n        return state\n    else:\n        return \"Unknown\"\n\n# Create a UDF to map states to regions\n@udf(StringType())\ndef map_state_to_region(location):\n    state = extract_state(location).strip()\n    for region, states in regions_to_states.items():\n        if state in states:\n            return region\n    return \"Unknown\"  # Return \"Unknown\" for states not found in the dictionary\n\n# Apply the UDF to map states to regions\ndf_with_region = query.withColumn(\"region\", map_state_to_region(query[\"location\"]))\n\ndisplay(df_with_region)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# VADER 감정 분석기 초기화\nanalyzer = SentimentIntensityAnalyzer()\n\n# Description 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef analyze_sentiment(description):\n    # VADER에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.polarity_scores(description)['neg']\n\n    # 감정 점수를 기반으로 심각도 분류\n    if sentiment_score \u003e= 0.4:\n        return \"High\"\n    elif sentiment_score \u003e= 0.2 and sentiment_score \u003c 0.4:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# 감정 분석을 위한 UDF 생성\nsentiment_udf = udf(analyze_sentiment, StringType())\n\n# 처리된 DataFrame(processedDF)의 description 열에 UDF 적용\n# 실제 DataFrame 및 열 이름으로 \"processedDF\" 및 \"description_column\"을 대체합니다.\nprocessedDF_with_severity = query.withColumn(\"severity\", sentiment_udf(\"description\"))\n\n# 추가된 심각도 열이 있는 DataFrame 표시\ndisplay(processedDF_with_severity)\n```\n\n환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\n\n데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Real-time Stream Processing](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png)\n\n위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\n\n또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\n\n```js\n# 스트리밍 DataFrame을 임시 뷰로 등록\nprocessedDF_with_severity.createOrReplaceTempView(\"incident_reports\")\n\n# 집계를 위한 SQL 쿼리 정의\ntotal_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\"\"\n\nseverity_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\"\"\n\n# 집계 수행\ntotal_incidents_df = spark.sql(total_incidents_query)\nseverity_incidents_df = spark.sql(severity_incidents_query)\n\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Data Engineering Concepts](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png)\n\nSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\n\n1. 비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\n2. 전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\n\n- 단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\n\n```js\n# Delta Lake에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \"`result_delta_table`\"\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \"/FileStore/tables/checkpoints\"\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.writeStream\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_location)\\\n  .trigger(processingTime='10 seconds')\\ # 10초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .format(\"delta\")\\\n  .toTable(delta_table_path)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\n\n![Delta Table as Dataframe](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png)\n\n또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\n\n![Query Delta Table with SQL](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png","tag":["Tech"],"readingTime":17},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\u003c/p\u003e\n\u003cp\u003e목차:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e스트림 처리란\u003c/li\u003e\n\u003cli\u003e카프카의 특징\u003c/li\u003e\n\u003cli\u003e카프카 구성\u003c/li\u003e\n\u003cli\u003e카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\u003c/li\u003e\n\u003cli\u003e스파크 구조화 스트리밍 API\u003c/li\u003e\n\u003cli\u003e데이타브릭스 델타 레이크\u003c/li\u003e\n\u003cli\u003e실전 프로젝트\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e이전 데이터 보안 부분으로 이동하는 링크입니다:\u003c/p\u003e\n\u003ch2\u003e스트림 처리란?\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png\" alt=\"데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리\"\u003e\u003c/p\u003e\n\u003cp\u003e스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\u003c/p\u003e\n\u003cp\u003e카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e카프카 특징\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\u003c/li\u003e\n\u003cli\u003e수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e카프카 구성\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\u003c/li\u003e\n\u003cli\u003e복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\u003c/li\u003e\n\u003cli\u003e유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\u003c/li\u003e\n\u003cli\u003e자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\u003c/li\u003e\n\u003cli\u003eack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e다른 카프카 서비스\u003c/h2\u003e\n\u003cp\u003eKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\"\u003e\n\u003cp\u003eksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\"\u003e\n\u003cp\u003e스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003eSpark Structured Streaming API\u003c/h2\u003e\n\u003cp\u003eSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\u003c/p\u003e\n\u003cp\u003e또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003eDatabricks Delta Lake\u003c/h2\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\u003c/p\u003e\n\u003cp\u003eDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\u003c/p\u003e\n\u003cp\u003e스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\u003c/p\u003e\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\"\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e실시간 스트리밍 아키텍처\u003c/h2\u003e\n\u003cp\u003e카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\u003c/p\u003e\n\u003cp\u003e문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\u003c/li\u003e\n\u003cli\u003e단계 2: 토픽 생성\u003c/li\u003e\n\u003cli\u003e단계 3: 클러스터 API 키 쌍 생성\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStep 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e단계 7: Kafka 토픽에서 스트림 읽기\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSparkSession\u003c/span\u003e\n\nspark = \u003cspan class=\"hljs-title class_\"\u003eSparkSession\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ebuilder\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappName\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Environmental Reporting\"\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003egetOrCreate\u003c/span\u003e()\n\nkafkaDF = spark \\\n    .\u003cspan class=\"hljs-property\"\u003ereadStream\u003c/span\u003e \\\n    .\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"kafka\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"kafka.bootstrap.servers\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"abcd.us-west4.gcp.confluent.cloud:9092\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"subscribe\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"illegal_dumping\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"startingOffsets\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"earliest\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"kafka.security.protocol\"\u003c/span\u003e,\u003cspan class=\"hljs-string\"\u003e\"SASL_SSL\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"kafka.sasl.mechanism\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"PLAIN\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"kafka.sasl.jaas.config\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\" password=\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\";\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e) \\\n    .\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e()\n\nprocessedDF = kafkaDF.\u003cspan class=\"hljs-title function_\"\u003eselectExpr\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"CAST(key AS STRING)\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"CAST(value AS STRING)\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-title function_\"\u003edisplay\u003c/span\u003e(processedDF)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003efunctions\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e F\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e  pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003efunctions\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e col, struct, to_json\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003etypes\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eStructField\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStructType\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eMapType\u003c/span\u003e\n\njson_schema = \u003cspan class=\"hljs-title class_\"\u003eStructType\u003c/span\u003e(\n  [\n    \u003cspan class=\"hljs-title class_\"\u003eStructField\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"incident_type\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e(), nullable = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e),\n    \u003cspan class=\"hljs-title class_\"\u003eStructField\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"location\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e(), nullable = \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e),\n    \u003cspan class=\"hljs-title class_\"\u003eStructField\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"description\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e(), nullable = \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e),\n    \u003cspan class=\"hljs-title class_\"\u003eStructField\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"contact\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e(), nullable = \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n  ]\n)\n\n# \u003cspan class=\"hljs-title class_\"\u003eUsing\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSpark\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eSQL\u003c/span\u003e to write queries on the streaming data \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e processedDF\n\nquery = processedDF.\u003cspan class=\"hljs-title function_\"\u003ewithColumn\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'value'\u003c/span\u003e, F.\u003cspan class=\"hljs-title function_\"\u003efrom_json\u003c/span\u003e(F.\u003cspan class=\"hljs-title function_\"\u003ecol\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'value'\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003ecast\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'string'\u003c/span\u003e), json_schema))  \\\n      .\u003cspan class=\"hljs-title function_\"\u003eselect\u003c/span\u003e(F.\u003cspan class=\"hljs-title function_\"\u003ecol\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"value.incident_type\"\u003c/span\u003e),F.\u003cspan class=\"hljs-title function_\"\u003ecol\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"value.location\"\u003c/span\u003e))\n\u003cspan class=\"hljs-title function_\"\u003edisplay\u003c/span\u003e(query)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\u003c/p\u003e\n\u003cp\u003eDefine a UDF(User Defined Function) to find out the region from the location:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003efunctions\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e udf\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003etypes\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e\n\n# \u003cspan class=\"hljs-title class_\"\u003eDefine\u003c/span\u003e the regions_to_states dictionary\nregions_to_states = {\n    \u003cspan class=\"hljs-string\"\u003e'South'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'West Virginia'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'District of Columbia'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Maryland'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Virginia'\u003c/span\u003e,\n              \u003cspan class=\"hljs-string\"\u003e'Kentucky'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Tennessee'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'North Carolina'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Mississippi'\u003c/span\u003e,\n              \u003cspan class=\"hljs-string\"\u003e'Arkansas'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Louisiana'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Alabama'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Georgia'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'South Carolina'\u003c/span\u003e,\n              \u003cspan class=\"hljs-string\"\u003e'Florida'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Delaware'\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'Southwest'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Arizona'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'New Mexico'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Oklahoma'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Texas'\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'West'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Washington'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Oregon'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'California'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Nevada'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Idaho'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Montana'\u003c/span\u003e,\n             \u003cspan class=\"hljs-string\"\u003e'Wyoming'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Utah'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Colorado'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Alaska'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Hawaii'\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'Midwest'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'North Dakota'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'South Dakota'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Nebraska'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Kansas'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Minnesota'\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e'Iowa'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Missouri'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Wisconsin'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Illinois'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Michigan'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Indiana'\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e'Ohio'\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'Northeast'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Maine'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Vermont'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'New York'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'New Hampshire'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Massachusetts'\u003c/span\u003e,\n                  \u003cspan class=\"hljs-string\"\u003e'Rhode Island'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Connecticut'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'New Jersey'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Pennsylvania'\u003c/span\u003e]\n}\n\n#\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e geotext \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGeoText\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e geopy.\u003cspan class=\"hljs-property\"\u003egeocoders\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNominatim\u003c/span\u003e\n\n# \u003cspan class=\"hljs-title class_\"\u003eDefine\u003c/span\u003e a \u003cspan class=\"hljs-keyword\"\u003efunction\u003c/span\u003e to extract state names \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e location text\ndef \u003cspan class=\"hljs-title function_\"\u003eextract_state\u003c/span\u003e(location_text):\n    geolocator = \u003cspan class=\"hljs-title class_\"\u003eNominatim\u003c/span\u003e(user_agent=\u003cspan class=\"hljs-string\"\u003e\"my_application\"\u003c/span\u003e)\n    location = geolocator.\u003cspan class=\"hljs-title function_\"\u003egeocode\u003c/span\u003e(location_text)\n    #\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(location)\n    #\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003etype\u003c/span\u003e(location.\u003cspan class=\"hljs-property\"\u003eraw\u003c/span\u003e))\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003elocation\u003c/span\u003e:\n        state = location.\u003cspan class=\"hljs-property\"\u003eraw\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e'display_name'\u003c/span\u003e].\u003cspan class=\"hljs-title function_\"\u003esplit\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e','\u003c/span\u003e)[-\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e state\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"Unknown\"\u003c/span\u003e\n\n# \u003cspan class=\"hljs-title class_\"\u003eCreate\u003c/span\u003e a \u003cspan class=\"hljs-variable constant_\"\u003eUDF\u003c/span\u003e to map states to regions\n@\u003cspan class=\"hljs-title function_\"\u003eudf\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e())\ndef \u003cspan class=\"hljs-title function_\"\u003emap_state_to_region\u003c/span\u003e(location):\n    state = \u003cspan class=\"hljs-title function_\"\u003eextract_state\u003c/span\u003e(location).\u003cspan class=\"hljs-title function_\"\u003estrip\u003c/span\u003e()\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e region, states \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e regions_to_states.\u003cspan class=\"hljs-title function_\"\u003eitems\u003c/span\u003e():\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e state \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003estates\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e region\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"Unknown\"\u003c/span\u003e  # \u003cspan class=\"hljs-title class_\"\u003eReturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"Unknown\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e states not found \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e the dictionary\n\n# \u003cspan class=\"hljs-title class_\"\u003eApply\u003c/span\u003e the \u003cspan class=\"hljs-variable constant_\"\u003eUDF\u003c/span\u003e to map states to regions\ndf_with_region = query.\u003cspan class=\"hljs-title function_\"\u003ewithColumn\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"region\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003emap_state_to_region\u003c/span\u003e(query[\u003cspan class=\"hljs-string\"\u003e\"location\"\u003c/span\u003e]))\n\n\u003cspan class=\"hljs-title function_\"\u003edisplay\u003c/span\u003e(df_with_region)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003efunctions\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e udf\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pyspark.\u003cspan class=\"hljs-property\"\u003esql\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003etypes\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e vaderSentiment.\u003cspan class=\"hljs-property\"\u003evaderSentiment\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSentimentIntensityAnalyzer\u003c/span\u003e\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eVADER\u003c/span\u003e 감정 분석기 초기화\nanalyzer = \u003cspan class=\"hljs-title class_\"\u003eSentimentIntensityAnalyzer\u003c/span\u003e()\n\n# \u003cspan class=\"hljs-title class_\"\u003eDescription\u003c/span\u003e 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef \u003cspan class=\"hljs-title function_\"\u003eanalyze_sentiment\u003c/span\u003e(description):\n    # \u003cspan class=\"hljs-variable constant_\"\u003eVADER\u003c/span\u003e에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.\u003cspan class=\"hljs-title function_\"\u003epolarity_scores\u003c/span\u003e(description)[\u003cspan class=\"hljs-string\"\u003e'neg'\u003c/span\u003e]\n\n    # 감정 점수를 기반으로 심각도 분류\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e sentiment_score \u003e= \u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"High\"\u003c/span\u003e\n    elif sentiment_score \u003e= \u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e and sentiment_score \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"Medium\"\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"Low\"\u003c/span\u003e\n\n# 감정 분석을 위한 \u003cspan class=\"hljs-variable constant_\"\u003eUDF\u003c/span\u003e 생성\nsentiment_udf = \u003cspan class=\"hljs-title function_\"\u003eudf\u003c/span\u003e(analyze_sentiment, \u003cspan class=\"hljs-title class_\"\u003eStringType\u003c/span\u003e())\n\n# 처리된 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e(processedDF)의 description 열에 \u003cspan class=\"hljs-variable constant_\"\u003eUDF\u003c/span\u003e 적용\n# 실제 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e 및 열 이름으로 \u003cspan class=\"hljs-string\"\u003e\"processedDF\"\u003c/span\u003e 및 \u003cspan class=\"hljs-string\"\u003e\"description_column\"\u003c/span\u003e을 대체합니다.\nprocessedDF_with_severity = query.\u003cspan class=\"hljs-title function_\"\u003ewithColumn\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"severity\"\u003c/span\u003e, \u003cspan class=\"hljs-title function_\"\u003esentiment_udf\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"description\"\u003c/span\u003e))\n\n# 추가된 심각도 열이 있는 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e 표시\n\u003cspan class=\"hljs-title function_\"\u003edisplay\u003c/span\u003e(processedDF_with_severity)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\u003c/p\u003e\n\u003cp\u003e데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png\" alt=\"Real-time Stream Processing\"\u003e\u003c/p\u003e\n\u003cp\u003e위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\u003c/p\u003e\n\u003cp\u003e또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 스트리밍 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e을 임시 뷰로 등록\nprocessedDF_with_severity.\u003cspan class=\"hljs-title function_\"\u003ecreateOrReplaceTempView\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"incident_reports\"\u003c/span\u003e)\n\n# 집계를 위한 \u003cspan class=\"hljs-variable constant_\"\u003eSQL\u003c/span\u003e 쿼리 정의\ntotal_incidents_query = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\nseverity_incidents_query = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n# 집계 수행\ntotal_incidents_df = spark.\u003cspan class=\"hljs-title function_\"\u003esql\u003c/span\u003e(total_incidents_query)\nseverity_incidents_df = spark.\u003cspan class=\"hljs-title function_\"\u003esql\u003c/span\u003e(severity_incidents_query)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png\" alt=\"Data Engineering Concepts\"\u003e\u003c/p\u003e\n\u003cp\u003eSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\u003c/li\u003e\n\u003cli\u003e전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-title class_\"\u003eDelta\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLake\u003c/span\u003e에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \u003cspan class=\"hljs-string\"\u003e\"`result_delta_table`\"\u003c/span\u003e\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \u003cspan class=\"hljs-string\"\u003e\"/FileStore/tables/checkpoints\"\u003c/span\u003e\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.\u003cspan class=\"hljs-property\"\u003ewriteStream\u003c/span\u003e\\\n  .\u003cspan class=\"hljs-title function_\"\u003eoutputMode\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"append\"\u003c/span\u003e)\\\n  .\u003cspan class=\"hljs-title function_\"\u003eoption\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"checkpointLocation\"\u003c/span\u003e, checkpoint_location)\\\n  .\u003cspan class=\"hljs-title function_\"\u003etrigger\u003c/span\u003e(processingTime=\u003cspan class=\"hljs-string\"\u003e'10 seconds'\u003c/span\u003e)\\ # \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"delta\"\u003c/span\u003e)\\\n  .\u003cspan class=\"hljs-title function_\"\u003etoTable\u003c/span\u003e(delta_table_path)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png\" alt=\"Delta Table as Dataframe\"\u003e\u003c/p\u003e\n\u003cp\u003e또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png\" alt=\"Query Delta Table with SQL\"\u003e\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka"},"buildId":"YUMR4jSyk_WlOHHc7UfOk","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>