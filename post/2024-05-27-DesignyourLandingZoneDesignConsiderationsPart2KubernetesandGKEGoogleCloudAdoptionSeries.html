<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries" data-gatsby-head="true"/><meta name="twitter:title" content="랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-27 17:25" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/acd99c507555fdc6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/acd99c507555fdc6.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/RZIEBQ2aNAp_DXFVTV6eL/_buildManifest.js" defer=""></script><script src="/_next/static/RZIEBQ2aNAp_DXFVTV6eL/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 27, 2024</span><span class="posts_reading_time__f7YPP">16<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>마음에 드시는 markdown 형식의 표를 아래에 참조해보세요:</p>
<table>
<thead>
<tr>
<th>구분</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr>
<td>착륙 지역이란?</td>
<td>착륙 지역의 정의 및 필요성에 대해 다룸</td>
</tr>
<tr>
<td>디자인 프로세스 개요</td>
<td>착륙 지역 디자인 프로세스 개요 소개</td>
</tr>
<tr>
<td>디자인 고려 사항 카테고리</td>
<td>착륙 지역 디자인 시 고려해야 할 7가지 카테고리 및 주요 디자인 결정 사항 소개</td>
</tr>
</tbody>
</table>
<h1>8. Google Kubernetes Engine (GKE)</h1>
<p>앞서 이 시리즈에서 말씀드렸듯이, 컨테이너는 훌륭해요! 가벼우며 빠르고 휴대성이 좋으며 쉽게 확장할 수 있어요. 이것들은 마이크로서비스 아키텍처에 잘 어울려요.</p>
<p>Google Cloud는 컨테이너를 실행하는 몇 가지 다른 방법을 제공하고 있어요. 그러나 클라우드에서 현대적인 컨테이너 기반 작업을 실행한다면 Kubernetes가 필요하겠죠. 그리고 Google Cloud에서 Kubernetes를 실행한다면 Google Kubernetes Engine을 사용해야 해요.</p>
<p>GKE은 Google의 관리형 Kubernetes 플랫폼이에요. 이를 통해 Google Cloud에서 Kubernetes를 실행할 수 있지만 자체 관리형 Kubernetes보다 여러가지 이점을 제공해요:</p>
<ul>
<li>GKE는 클러스터를 배포하고 쿠버네티스를 설치하며 노드를 등록하는 것을 처리해줍니다. 새로운 클러스터는 몇 분 안에 배포할 수 있어요!</li>
<li>쿠버네티스 제어 플레인 노드는 완전 관리되며 소비자로부터 완전히 추상화되어 있어요.</li>
<li>호스트는 구글이 투명하게 관리하고 패치하는 견고하고 미리 구성된 컨테이너 용도 OS에서 실행돼요.</li>
<li>구글은 릴리스 채널을 통해 쿠버네티스 업그레이드를 투명하게 관리해줘요.</li>
<li>클러스터는 워크로드 수요를 충족하기 위해 탄탄하게 자동으로 탄력적으로 확장돼요. (관리되는 인스턴스 그룹 및 오토스케일러와 로드 밸런서를 만들 필요 없이!)</li>
<li>비파괴적으로 자동 수리 및 불건전한 클러스터 노드의 교체를 자동으로 수행해줘요.</li>
<li>기본 제공 지역 고가용성.</li>
<li>GKE는 Google Cloud Operations에 네이티브로 통합돼 있어서 모니터링, 로깅 및 메트릭스에 쉽게 액세스할 수 있어요.</li>
<li>GKE Autopilot으로 클러스터는 기본 제공된 모베스트 프랙티스로 사전구성돼요.</li>
<li>GKE Autopilot으로 실행 중인 워크로드에 비용을 지급하게 됩니다. (POD별 청구라고도 함) 클러스터를 배포하는 데 비용을 내야 하는 대신입니다. 이것은 게임 체인저에요!</li>
<li>GKE 노드 풀 및 Autopilot에서 계산 클래스로 작업 부하에 필요한 특정 기계 유형을 할당할 수 있어요. 또한 AI/ML 학습과 같이 필요로 하는 워크로드에 GPU를 할당할 수 있어요.</li>
<li>완벽한 Google Anthos 서비스 메시와의 원활한 통합, 스스로 완전 관리 서비스로 제공할 수 있어요.</li>
</ul>
<p>(참고로, 여기에서 구글 컴퓨트 엔진에서 자체 관리 쿠버네티스 환경을 실행하려는 것이 왜 좋지 않은 아이디어인지 다양한 이유에 대해 다뤘었어요.)</p>
<p>조금 고려해야 할 설계 결정과 최상의 모베스트 프랙티스가 있어요. 이 주제에 대한 시리즈를 진행할 수도 있겠지만 간략하게 여기서 고려 사항을 요약해드릴게요.</p>
<h2>GKE Autopilot 또는 GKE 표준?</h2>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png" alt="AutoPilot"></p>
<p>안녕하세요! Autopilot은 기본 클러스터 배포 모드로 이미 장기간 사용되어 왔어요. 이 기능은 다음과 같은 몇 가지 모범 사례를 기본으로 제공합니다:</p>
<ul>
<li>릴리스 채널로의 의무적 등록. 이것은 클러스터가 자동으로 패치되고 유지되며, 오래된 보안 취약한 쿠버네티스 버전을 실행할 수 없다는 것을 의미합니다.</li>
<li>클러스터는 regional로, 클러스터의 고가용성을 보장합니다.</li>
<li>클러스터 자동 스케일링 - GKE가 기본으로 노드 수를 자동으로 조정해 줍니다.</li>
<li>노드 자동 복구가 기본으로 활성화되어 있습니다.</li>
<li>보안 부팅이 있는 shielded 인스턴스로 노드가 구축됩니다.</li>
<li>Workload identity가 사전 구성되어 있습니다. 이를 통해 쿠버네티스 서비스 계정이 구글 클라우드 IAM 서비스 계정처럼 작동할 수 있습니다. 따라서, GKE 서비스에서 구글 클라우드 API로의 세분화된 액세스 컨트롤을 제공할 수 있습니다.</li>
</ul>
<p>그리고 이전에 언급한 것처럼, 실제로 배포되고 실행 중인 파드에 대해 비용을 지불하므로, GKE 클러스터를 비효율적으로 사용하는 국면에서 지출을 낭비하는 일을 피할 수 있습니다. (GKE Standard를 실행할 때 흔히 발생하는 문제입니다.)</p>
<p>대부분의 경우 Autopilot을 권장합니다. 일부 특정한 경우에는 Standard를 사용하고 싶을 수도 있습니다. 예를 들어:</p>
<ul>
<li>TPU와 함께 노드를 배포하고 싶을 때.</li>
<li>특정 버전의 Kubernetes를 실행하고 자동 업그레이드를 사용하지 않으려는 경우. (일반적으로 이는 좋지 않은 아이디어이며, 관리형 서비스를 사용하는 의도를 상쇄시키는 것과도 같습니다.)</li>
<li>구글 Autopilot 허용 목록에 없는 특권있는 팟 (즉, 상승된 권한이 필요한 작업 부하)을 실행하고 싶을 때.</li>
<li>실험적인 작업 부하를 단일 존 클러스터에 배포하고 가용성을 보장할 필요가 없는 경우.</li>
</ul>
<h2>Multitenant 대 Single Tenant 클러스터?</h2>
<p>이것은 이전보다는 덜 검정색과 흰색의 문제입니다. Autopilot 이전에는 답이 명확했습니다: 가능한 한 많은 Multitenant 클러스터를 사용하십시오. 아이디어는 매우 적은 클러스터를 가지고 각 클러스터가 조직 내 다중 테넌트로부터 작업 부하를 호스팅한다는 것입니다. 여기서 테넌트는 일반적으로 조직 내에서 다른 팀들을 의미합니다. 그리고 각 테넌트는 하나 이상의 네임스페이스를 소유하게 됩니다.</p>
<p>멀티테넌트 GKE 클러스터를 사용하면:</p>
<ul>
<li>관리해야 할 클러스터가 하나뿐이기 때문에 클러스터 관리 부담이 비교적 적습니다.</li>
<li>개별 테넌트/애플리케이션은 클러스터를 프로비저닝할 필요가 없습니다. 기존 클러스터에 작업량을 배포하기만 하면 됩니다.</li>
<li>클러스터 자체의 관리 책임을 클라우드 플랫폼 팀에 위임할 수 있어, 애플리케이션 팀은 GKE 관리 책임을 신경 쓸 필요가 없습니다.</li>
<li>테넌트 간의 격리는 네임스페이스 사용을 통해 달성합니다. 테넌트는 자신의 네임스페이스에 배포합니다.</li>
</ul>
<p>한편, 많은 수의 소규모 단일 테넌트 GKE(표준) 클러스터가 있는 경우 — 즉, 각 클러스터가 하나의 애플리케이션 서비스를 호스팅하는 경우 — 이는 비용이 많이 소요됩니다:</p>
<ul>
<li>각 애플리케이션이 자체 클러스터를 관리해야 합니다. 이로 인해 각 팀에 상당한 클러스터 관리 부담이 발생합니다.</li>
<li>더불어 높은 가용성을 보장하기 위해 각 클러스터는 지역 전체에 최소 수의 노드를 배포해야 합니다. 단일 테넌트 애플리케이션의 경우, 이는 자주 응용프로그램의 요구 사항보다 훨씬 큰 최소 클러스터 크기에 이르게 됩니다. 결과적으로, 대규모로 과잉 프로비저닝되고 비효율적으로 사용되는 클러스터가 많이 생기게 됩니다. GKE 표준 제품을 사용하면 운영 중인 팟이 아닌 배포한 클러스터에 대한 비용을 지불하므로 많은 현금을 낭비하게 됩니다!</li>
</ul>
<p>Autopilot을 사용하면 단일 테넌트 클러스터가 많은 영향을 미치지 않습니다:</p>
<ul>
<li>실행 중인 팟만 지불하게 되므로 사용하지 않는 클러스터를 대거 배포하여 자금을 낭비하지 않습니다.</li>
<li>박스에서 사전 구성된 많은 모베스트 프랙티스로 인해 테넌트 당 클러스터 관리 부담이 비교적 적습니다.</li>
</ul>
<p>마무리로 말씀드리면, 가능한 경우에는 멀티테넌트 GKE Autopilot 클러스터를 사용하는 것이 좋습니다. 특별한 경우에만 단일 테넌트 클러스터를 사용하십시오.</p>
<p>조직에서 편안한 수준으로 배포된 몇 개의 다중 테넌트 클러스터를 설정할 수 있습니다. 예를 들어, 각 사업 라인에 맞춰 다중 테넌트 클러스터를 설정할 수 있습니다.</p>
<p>이전 부분에서 논의한 대로 공유 VPC 디자인을 중심으로 랜딩 존을 구축했다면, 일반적으로 다중 테넌트 GKE 클러스터를 공유 VPC 내에 배포하고 싶을 것입니다. 이는 다음과 같을 수 있습니다:</p>
<ul>
<li>"허브" VPC에 피어링된 다중 테넌트 GKE가 호스팅되는 공유 VPC</li>
<li>다른 공유 서비스가 호스팅되는 같은 공유 VPC</li>
</ul>
<p>어쨌든, 플랫폼 팀이 공유 VPC가 있는 호스트 프로젝트와 다중 테넌트 GKE 클러스터가 있는 호스트 프로젝트를 소유하게 될 것입니다. 테넌트는 서비스 프로젝트를 소유하게 됩니다. 그들은 다중 테넌트 클러스터(네임스페이스 내)에 직접 워크로드를 배포할 수 있으며, 비-GKE 리소스를 서비스 프로젝트로 배포할 수 있습니다.</p>
<p>Google Cloud 문서는 이 방법을 다음과 같이 설명합니다:</p>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_1.png" alt="Google Cloud"></p>
<h2>VPC-Native 또는 라우트 기반 클러스터?</h2>
<p>라우트 기반 클러스터는 pod 간 트래픽에 대한 VPC 사용자 정의 라우트에 의존하는 클러스터입니다.</p>
<p>VPC-native 클러스터는 포드 주소 지정을 위해 별칭 IP 주소 범위를 사용하는 클러스터입니다. 이는 포드가 클러스터의 VPC 네트워크 및 연결된 VPC 네트워크 내에서 네이티브로 라우팅될 수 있음을 의미합니다. 포드 주소를 위해 사용자 정의 정적 경로의 구성이 필요하지 않습니다. 네트워크 관리 부담은 비교적 낮습니다.</p>
<p>VPC-native 클러스터는 RFC 1918 IP 범위 밖의 IP 주소를 사용할 수도 있습니다. 이는 많은 포드 IP 주소가 필요할 것으로 예상될 때 유용할 수 있습니다. 예를 들어, GKE는 240.0.0.0/4 CIDR 범위를 노드, 포드 및 서비스에 사용할 수 있어 추가로 2억 6800만 개의 IP 주소를 제공합니다!</p>
<p>Google은 VPC-native 클러스터를 권장합니다. 이것은 GKE Autopilot의 기본 설정이며, 2021년에 출시된 버전 1.21.0-gke.1500부터 GKE 표준의 기본 설정이었습니다.</p>
<p>요약하자면: VPC-native 클러스터를 사용하세요!</p>
<h2>개인 클러스터?</h2>
<p>기본적으로 GKE 클러스터는 공개적입니다. 이는 제어 평면과 워커 노드가 공개 IP 주소를 가지고 있음을 의미합니다. 그러나 조직이 개인 클러스터를 생성하는 것이 모법 사례입니다. 이렇게 하면 클러스터를 인터넷에서 격리할 수 있습니다.</p>
<p>개인 클러스터에서는:</p>
<ul>
<li>워커 및 제어 평면 노드는 비공개 IP 주소만 가지고 있으며 인터넷에 노출되지 않습니다. 예를 들어 NodePort 유형의 서비스는 노드가 인터넷 라우팅 가능한 공개 IP 주소를 가지고 있지 않기 때문에 인터넷에서 클라이언트에게 접근할 수 없습니다.</li>
<li>노드는 Google API 및 서비스와 통신하기 위해 Private Google Access를 사용합니다.</li>
<li>인터넷으로의 외부 액세스는 Cloud NAT 또는 Anthos Service Mesh 이그레스 게이트웨이와 같이 구현한 특정 제어를 통해서만 가능합니다.</li>
<li>외부 클라이언트로부터의 들어오는 연결은 외부 서비스를 통해서만 허용됩니다. 일반적으로 다음을 사용합니다: 외부 Ingress를 가진 LoadBalancer 서비스 또는 Anthos Service Mesh 공개 인그레스 게이트웨이.</li>
<li>워커 노드와 GKE 제어 평면 사이의 통신은 제어 평면의 비공개 엔드포인트를 통해 이루어지며, 제어 평면에 액세스해야 하는 추가 네트워크는 승인되어야 합니다.</li>
</ul>
<h2>클러스터간 IP 주소 범위 공유</h2>
<p>VPC 네이티브 클러스터를 생성할 때, 노드, 파드 및 서비스에 대한 IP 주소를 할당해야 합니다.</p>
<ul>
<li>노드 IP 주소: 클러스터의 노드인 호스트 머신에 사용되는 주소입니다. 클러스터는 노드에 IP 주소를 할당하기 위해 서브넷의 기본 IPv4 범위를 사용합니다. 노드의 크기를 예상하는데 있어, 가장 큰 크기를 기준으로 서브넷의 크기를 설정해야 합니다.</li>
<li>파드 IP 주소: 클러스터는 파드에 IP 주소를 할당할 때 보조 IPv4 주소 범위를 사용합니다. GKE에서 가장 큰 IP 주소 요구사항이며, 각 노드는 많은 수의 파드를 호스트할 수 있습니다. 기본적으로 GKE Autopilot은 노드 당 최대 파드 수를 32개로 설정하고, 각 노드 당 64개의 IP 주소를 허용하여 파드 교체를 허용합니다. Kubernetes는 각 노드에 보조 IP 주소 범위를 할당하여 각 파드에 고유한 IP 주소를 할당합니다. 기본적으로 GKE Autopilot은 /17 보조 서브넷 범위를 할당하므로 32766개의 사용 가능한 파드가 허용됩니다. (이는 약 1000개가 넘는 노드를 가진 클러스터와 동등합니다.)</li>
<li>서비스(ClusterIP) IP 주소: 클러스터는 내부 서비스 주소를 위해 별도의 보조 IP 주소 범위를 사용합니다. 서비스 IP는 ClusterIP 서비스에 할당됩니다; 이는 클러스터 내에서만 접근 가능한 가상 IP 주소입니다. GKE Autopilot에서는 버전 1.27부터 디폴트로 Google 관리 네트워크에서 IP 주소를 할당하며 범위는 34.118.224.0/20입니다. 동일한 범위가 각 클러스터에 할당됩니다. 따라서 각 클러스터마다 4천 개 이상의 서비스 주소가 제공되며, 기관은 GKE 내의 서비스를 위해 IP 주소를 할당하거나 예약할 필요가 없습니다.</li>
</ul>
<p>동일한 공유 VPC에서 몇 개의 클러스터를 실행해야 할 수도 있습니다. 예를 들어, 업무별로 클러스터를 정의하고 싶을 수도 있습니다. (이미 언급했듯이, 많은 작은 단독 사용자 클러스터를 갖는 것은 좋지 않은 아이디어입니다.) 이 경우, 같은 서브넷에 호스팅된 클러스터 간에 기본 및 보조 IP 주소 범위를 공유할 수 있습니다. 이것은 하는 것이 좋은 일입니다:</p>
<ul>
<li>클러스터당 서브넷을 할당할 필요가 없습니다.</li>
<li>클러스터당 서브넷 크기를 고려할 필요가 없습니다.</li>
<li>네트워크 관리 오버헤드를 줄일 수 있습니다.</li>
<li>IP 주소를 효율적으로 절약하고 IP 주소 고갈 위험을 줄입니다.</li>
</ul>
<p>VPC에서 클러스터 간에 범위를 공유하기로 결정한다면, 이 점을 주의해야 합니다:</p>
<ul>
<li>미리 명명된 서브넷을 정의하세요.</li>
<li>100.64.0.0/10(약 4.2 백만 개의 파드 사용 가능) 및 240.0.0.0/4(약 2억 6천 8백만 개의 파드 사용 가능)와 같은 RFC 1918 프라이빗 CIDR 범위를 사용하는 것을 고려해보세요.</li>
</ul>
<h2>릴리스 채널</h2>
<p>여기에서는 GKE 클러스터가 업그레이드되고 유지보수되며 패치되는 전략을 결정합니다.</p>
<p>쿠버네티스 버전은 x.y.z 형식으로 표시되며, 여기서 x는 주요 버전, y는 부 버전, z는 패치 버전을 나타냅니다. 일반적으로 매년 세 번에서 네 번의 중요 (주요 또는 부) 쿠버네티스 릴리스가 있으며, 패치 릴리스는 일주일에 한 번씩 발생합니다. 주요 및 부 릴리스는 새로운 기능과 보안 패치를 모두 포함합니다. 특정 부 릴리스는 대략 1년간 지원됩니다. 따라서 언제든지 일반적으로 지원되는 부 릴리스가 약 세 개 있을 것입니다.</p>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_4.png" alt="image"></p>
<p>내 추천 (그리고 구글의 추천)은 항상 클러스터를 릴리스 채널에 등록하는 것입니다. 릴리스 채널을 사용하면 구글이 클러스터 업그레이드를 관리해줍니다. 워커 노드는 롤링 서지 업그레이드 전략을 사용하여 자동으로 업그레이드되어 워크로드에 미치는 영향을 최소화합니다. 따라서 클러스터 관리자는 클러스터 업그레이드에 시간이나 노력을 들일 필요가 없습니다.</p>
<p>GKE Standard를 사용하면 릴리스 채널을 사용하지 않을 수 있습니다. 이렇게 하려면 클러스터를 특정 버전의 Kubernetes로 유지하고 싶을 때 사용할 수 있습니다. 그러나 제 경험상, 릴리스 채널에 선택적으로 가입하지 않는 기관은 곧 자신들의 취약한 클러스터가 가득한 시스템을 운영하게 됩니다. 그러한 기관들은 큰 패치 문제를 안게 될 것입니다!</p>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_5.png" alt="image"></p>
<p>GKE Autopilot을 사용하면 릴리스 채널을 선택해 사용하지 않을 수 없습니다. (정말 좋은 점이에요!)</p>
<p>(참고로: GKE 컨트롤 플레인 노드는 항상 Google에 의해 업그레이드되며, 이 프로세스를 거부할 방법이 없습니다. 이는 릴리스 채널 등록 여부와 상관없이 해당됩니다.)</p>
<p>선택할 수 있는 세 가지 릴리스 채널이 있습니다:</p>
<ul>
<li>빠른(Rapid) — 오픈 소스 릴리스가 일반적으로 사용 가능한 후 몇 주 후에 Kubernetes 클러스터가 업그레이드됩니다. 이 채널은 가장 최신 기능을 제공하지만 가장 안정성이 낮습니다. 또한, 이 릴리스 채널의 클러스터는 GKE SLA에서 지원되지 않을 것입니다.</li>
<li>보통(Regular) — Kubernetes가 릴리스 후 2~3개월 후에 Rapid에서 실행되고 있는 릴리스 버전으로 업그레이드됩니다. 새로운 기능과 안정성 사이의 균형을 제공합니다. 기본적으로 GKE Autopilot은 노드를 이 릴리스 채널에 등록합니다.</li>
<li>안정(Stable) — Kubernetes가 Regular에서 실행되고 있는 릴리스 버전으로 업그레이드됩니다. 이 릴리스는 커뮤니티에서 약 5~6개월동안 사용 가능한 후 클러스터 노드에 적용됩니다. 이 채널은 가장 검증되었고 가장 안정적일 것이지만 최근 Kubernetes 기능을 제공하지 않을 것입니다.</li>
</ul>
<p>새로운 GKE 버전이 릴리스 채널에서 기본 버전이 되면 해당 클러스터는 일반적으로 10일 이내에 업그레이드됩니다.</p>
<p>다음은 릴리스 채널 채용에 대한 일반적인 권장 사항입니다:</p>
<ul>
<li>생산 워크로드는 클러스터의 중요성 및 위험 수용 능력에 따라 Stable 또는 Regular에 등록해야 합니다. 중요한 워크로드는 Stable에 등록하는 것을 권장합니다.</li>
<li>최종 비생산 환경(일반적으로 스테이징, Pre-Prod 또는 UAT 환경)은 생산 환경과 동일한 릴리스 채널에 등록되어야 합니다. 왜냐하면 새로 시도해보지 않은 Kubernetes 버전에서 워크로드가 생산 환경에 배치되는 것을 원치 않기 때문입니다.</li>
<li>상위 비생산 환경(예: Dev 또는 QA)은 이웃한 상위 릴리스 채널에 배포되어야 합니다.</li>
<li>새로운 기능을 실험하려는 개발 환경에서만 빠른 릴리스 채널을 사용해야 합니다. 새로운 기능은 안정적인 릴리스 채널에 몇 달 내에 나타날 것을 기억해주세요.</li>
</ul>
<p>예시:</p>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_6.png" alt="이미지"></p>
<p>릴리스 채널의 한 가지 문제는 클러스터가 언제 업그레이드될지 보장할 수 없다는 것입니다. 최종 비 프로드 환경 및 프로드 환경을 동일한 릴리스 채널(구글에서 권장하는 최상의 방법)에 등록하면 비 프로드 환경이 프로드 환경보다 먼저 업그레이드되도록 보호막을 구현하고 싶을 것입니다. GKE fleets 및 scopes를 사용하면 쉽게 이를 수행할 수 있습니다.</p>
<p>다음은 이러한 배포 순서의 예시입니다:</p>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_7.png" alt="이미지"></p>
<p>이 예에서:</p>
<ul>
<li>Staging 환경의 GKE 클러스터는 Staging 플리트에 속합니다.</li>
<li>Production에 있는 클러스터는 Production 플리트에 속합니다.</li>
<li>Staging 및 Production에 있는 모든 클러스터는 Stable 릴리스 채널에 등록되어 있습니다.</li>
<li>Production 플리트는 Staging 플리트가 7일간 실행된 후에만 업그레이드됩니다.</li>
</ul>
<h2>Workload Identity Federation</h2>
<p>워크로드 ID 페더레이션은 Kubernetes 서비스 계정이 IAM 서비스 계정으로 작동할 수 있게 합니다. 구성된 Kubernetes 서비스 계정을 사용하는 파드는 Google Cloud API에 액세스할 때 자동으로 IAM 서비스 계정으로 인증합니다. 이를 통해 GKE 애플리케이션 워크로드가 인증되고 허가되어 Google Cloud 서비스에 액세스할 수 있도록 보장할 수 있습니다.</p>
<p>오토파일럿 클러스터는 GKE에서 기본적으로 워크로드 ID 연합을 지원합니다.</p>
<h2>오토스케일링 전략</h2>
<p>오토스케일링은 클러스터가 실행 중인 응용 프로그램의 수요를 충족하기 위해 조정할 수 있는 능력을 가리킵니다.</p>
<p>GKE에는 네 가지 스케일링 차원이 있습니다:</p>
<ul>
<li>워크로드는 수요에 따라 가로 방향으로 확장됩니다. Pod를 추가하거나 제거함으로써 관리됩니다. 이는 가로 방향 팟 오토스케일러(HPA)에 의해 관리되며, 수요에 따라 빠르게 확장됩니다. HPA는 CPU 사용률과 같은 표준 메트릭 또는 초당 요청과 같은 사용자 정의 메트릭에 응답합니다.</li>
<li>인프라는 클러스터 노드를 추가하거나 제거함으로써 가로 방향으로 확장됩니다. 이는 예약된 팟을 수용하기 위해 클러스터 자동스케일러(CA)에 의해 예측적으로 관리됩니다. 예를 들어, 새로 생성된 팟을 예약할 노드가 없는 경우, 클러스터 자동스케일러가 새 노드를 만듭니다.</li>
<li>워크로드는 팟 크기를 조절함으로써 세로 방향으로 확장됩니다. 이는 세로 방향 팟 오토스케일러(VPA)에 의해 관리됩니다. VPA는 시간이 경과함에 따라 팟의 CPU 및 메모리 사용률을 모니터링하고 이에 따라 팟 크기를 조정합니다. 이로써 보다 최적화되고 비용 효율적인 팟 크기를 얻을 수 있습니다.</li>
<li>인프라는 가장 효율적인 팟의 바이너리 패킹을 달성하기 위해 최적화된 노드(VM) 사이즈로 노드 풀을 배포하거나 삭제함으로써 세로 방향으로 확장됩니다. 이를 "노드 자동 프로비저닝"이라고 하며, 다른 종류의 확장에 비해 상대적으로 느립니다.</li>
</ul>
<p>아래는 몇 가지 팁입니다:</p>
<ul>
<li>GKE Autopilot을 사용할 때는 인프라 자동스케일링 (CA 및 NAP)이 Google에 의해 설정됩니다. 당신은 팟 오토스케일링 구성만 고려하면 됩니다.</li>
<li>HPA와 VPA를 동시에 사용하지 마세요. 같은 자원 메트릭으로 HPA와 VPA를 함께 설정하는 것을 피하세요. 예를 들어, CPU 사용률에 대한 HPA와 VPA를 동시에 설정하는 것을 피하세요.</li>
<li>가로와 세로 방향 팟 오토스케일링을 관리하는 다차원 팟 오토스케일러(MPA)를 사용하면 워크로드 확장을 간단히 할 수 있습니다.</li>
</ul>
<p>결론적으로:</p>
<ul>
<li>GKE 표준 버전을 사용하면 인프라 스케일링과 워크로드(파드) 스케일링을 모두 관리해야 합니다.</li>
<li>GKE Autopilot을 사용하면 워크로드 스케일링에만 집중하면 되며, MPA를 사용하여 워크로드 자동 스케일링을 간편하게 할 수 있습니다.</li>
</ul>
<h2>인프라 및 워크로드 배포</h2>
<p>Google은 클러스터 자체를 배포할 때 클라우드 인프라의 경우와 마찬가지로 인프라를 코드로 관리하는 것을 권장합니다(Terraform과 같은). 따라서 클러스터 및 네임스페이스를 배포할 때 IaC를 사용하세요.</p>
<p>귀하의 작업 로드를 배포하려면 — 배포, 서비스, 작업, StatefufSets, 인그레스, 정책 등 — 선언적 yaml 파일을 사용하여 네이티브 Kubernetes API 호출을 해야 합니다. Helm은 쿠버네티스에서 애플리케이션 배포를 관리하는 데 도움을 줄 수 있습니다.</p>
<h2>GKE 디자인 결정 요약</h2>
<p>요약하자면: 고려해야 할 주요 Kubernetes 디자인 결정 사항과 각각에 대한 나의 권장 사항입니다.</p>
<ul>
<li>GKE, 또는 스스로 관리하는 방식? 권장 사항: GKE.</li>
<li>GKE Autopilot 대 GKE Standard. 권장 사항: Autopilot.</li>
<li>멀티 테넌트 클러스터? 어떤 수준의 클러스터? 권장 사항: 멀티 테넌트.</li>
<li>싱글 테넌트 클러스터를 만들 수 있는 능력. 권장 사항: 예외적으로만.</li>
<li>VPC 네이티브 또는 라우트 기반 클러스터? 권장 사항: VPC 네이티브.</li>
<li>프라이빗 클러스터? 권장 사항: 프라이빗 클러스터를 사용하세요.</li>
<li>클러스터 간 IP 주소 범위 공유? 추천 사항: 공유 VPC 내에 몇 개 또는 많은 클러스터가 있다면 이를 수행해야 합니다.</li>
<li>릴리스 채널? 항상 릴리스 채널에 등록하세요. 스테이징 및 프로드 클러스터를 동일한 릴리스 채널에 유지하세요. 클러스터가 올바른 순서로 업그레이드되도록 보장하기 위해 fleets 및 rollout sequences를 사용하세요.</li>
<li>Workload identity? 네 — 사용하세요.</li>
<li>오토스케일링 전략? 워크로드 오토스케일링을 지원하기 위해 클러스터 오토스케일러를 사용하세요. GKE Autopilot을 사용할 때 클러스터 오토스케일링은 자동으로 관리됩니다. 워크로드 오토스케일링을 간소화하기 위해 MPA를 사용하세요.</li>
<li>인프라 및 워크로드 배포? 클러스터 배포 및 관리에는 IaC(예: Terraform)를 사용하세요. 애플리케이션 워크로드를 배포하기 위해 선언적 Kubernetes 매니페스트를 사용하세요.</li>
</ul>
<h1>마무리</h1>
<p>GKE를 랜딩 존에서 성공적으로 디자인하는 데 고려해야 할 주요 사항들은 여기까지입니다. 다음 파트에서는 LZ 디자인 고려 사항을 완료하고, 로깅 및 모니터링 전략, 요금 청구, 인프라스트럭처 코드 (IaC)와 같은 주제를 다룰 것입니다.</p>
<h1>떠나시기 전에</h1>
<ul>
<li>관심이 있을 것으로 생각되는 분과 공유해주세요. 그들에게 도움이 될 수도 있고, 저에게 정말로 도움이 됩니다!</li>
<li>박수를 부탁드립니다! 여러분은 한 번 이상 박수를 칠 수 있다는 걸 아시나요?</li>
<li>자유롭게 댓글을 남겨주세요 💬.</li>
<li>내 컨텐츠를 놓치지 않으려면 팔로우하고 구독해주세요. 내 프로필 페이지로 이동하여 이 아이콘들을 클릭해주세요:</li>
</ul>
<p><img src="/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_9.png" alt="Image"></p>
<h1>링크</h1>
<ul>
<li>Google Cloud의 랜딩 존: 필요성 및 생성 방법</li>
<li>Google Cloud의 랜딩 존 디자인</li>
<li>GKE Autopilot</li>
<li>GKE Autopilot 특권 업무 용 업무</li>
<li>GKE 클러스터 구성 옵션</li>
<li>기업용 GKE 멀티 테넌시에 대한 모베스트 프랙티스</li>
<li>VPC 네이티브 GKE 클러스터</li>
<li>GKE의 프라이빗 클러스터</li>
<li>외부 애플리케이션 로드 밸런서를 위한 GKE Ingress</li>
<li>GKE로 이주할 때 IP 주소 계획</li>
<li>GKE 네트워크 플래닝 2023 (William Denniss)</li>
<li>GKE 릴리스 채널</li>
<li>롤아웃 순서로 GKE 클러스터 업그레이드</li>
<li>GKE SLA</li>
<li>GKE 자동 확장 (Kaslin Fields)</li>
<li>GKE에서 비용 최적화된 애플리케이션에 대한 모베스트 프랙티스</li>
<li>GKE를 위한 워크로드 ID 연합</li>
<li>Google Cloud 아키텍처 프레임워크</li>
<li>기업용 기초 설계 청사진</li>
</ul>
<h1>시리즈 내비게이션</h1>
<ul>
<li>시리즈 개요 및 구조</li>
<li>이전: 랜딩 존 설계하기 — 디자인 고려사항 파트 1</li>
<li>다음: 랜딩 존 설계하기 — 디자인 고려사항 파트 3 — 모니터링, 로깅, 빌링 및 라벨링</li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"랜딩 존 디자인하기  디자인 고려 사항 파트 2  쿠버네티스와 GKE Google Cloud 채택 시리즈","description":"","date":"2024-05-27 17:25","slug":"2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries","content":"\n마음에 드시는 markdown 형식의 표를 아래에 참조해보세요:\n\n\n| 구분               | 설명                                                                          |\n|--------------------|------------------------------------------------------------------------------|\n| 착륙 지역이란?    | 착륙 지역의 정의 및 필요성에 대해 다룸                                       |\n| 디자인 프로세스 개요 | 착륙 지역 디자인 프로세스 개요 소개                                         |\n| 디자인 고려 사항 카테고리 | 착륙 지역 디자인 시 고려해야 할 7가지 카테고리 및 주요 디자인 결정 사항 소개 |\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 8. Google Kubernetes Engine (GKE)\n\n앞서 이 시리즈에서 말씀드렸듯이, 컨테이너는 훌륭해요! 가벼우며 빠르고 휴대성이 좋으며 쉽게 확장할 수 있어요. 이것들은 마이크로서비스 아키텍처에 잘 어울려요.\n\nGoogle Cloud는 컨테이너를 실행하는 몇 가지 다른 방법을 제공하고 있어요. 그러나 클라우드에서 현대적인 컨테이너 기반 작업을 실행한다면 Kubernetes가 필요하겠죠. 그리고 Google Cloud에서 Kubernetes를 실행한다면 Google Kubernetes Engine을 사용해야 해요.\n\nGKE은 Google의 관리형 Kubernetes 플랫폼이에요. 이를 통해 Google Cloud에서 Kubernetes를 실행할 수 있지만 자체 관리형 Kubernetes보다 여러가지 이점을 제공해요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- GKE는 클러스터를 배포하고 쿠버네티스를 설치하며 노드를 등록하는 것을 처리해줍니다. 새로운 클러스터는 몇 분 안에 배포할 수 있어요!\n- 쿠버네티스 제어 플레인 노드는 완전 관리되며 소비자로부터 완전히 추상화되어 있어요.\n- 호스트는 구글이 투명하게 관리하고 패치하는 견고하고 미리 구성된 컨테이너 용도 OS에서 실행돼요.\n- 구글은 릴리스 채널을 통해 쿠버네티스 업그레이드를 투명하게 관리해줘요.\n- 클러스터는 워크로드 수요를 충족하기 위해 탄탄하게 자동으로 탄력적으로 확장돼요. (관리되는 인스턴스 그룹 및 오토스케일러와 로드 밸런서를 만들 필요 없이!)\n- 비파괴적으로 자동 수리 및 불건전한 클러스터 노드의 교체를 자동으로 수행해줘요.\n- 기본 제공 지역 고가용성.\n- GKE는 Google Cloud Operations에 네이티브로 통합돼 있어서 모니터링, 로깅 및 메트릭스에 쉽게 액세스할 수 있어요.\n- GKE Autopilot으로 클러스터는 기본 제공된 모베스트 프랙티스로 사전구성돼요.\n- GKE Autopilot으로 실행 중인 워크로드에 비용을 지급하게 됩니다. (POD별 청구라고도 함) 클러스터를 배포하는 데 비용을 내야 하는 대신입니다. 이것은 게임 체인저에요!\n- GKE 노드 풀 및 Autopilot에서 계산 클래스로 작업 부하에 필요한 특정 기계 유형을 할당할 수 있어요. 또한 AI/ML 학습과 같이 필요로 하는 워크로드에 GPU를 할당할 수 있어요.\n- 완벽한 Google Anthos 서비스 메시와의 원활한 통합, 스스로 완전 관리 서비스로 제공할 수 있어요.\n\n(참고로, 여기에서 구글 컴퓨트 엔진에서 자체 관리 쿠버네티스 환경을 실행하려는 것이 왜 좋지 않은 아이디어인지 다양한 이유에 대해 다뤘었어요.)\n\n조금 고려해야 할 설계 결정과 최상의 모베스트 프랙티스가 있어요. 이 주제에 대한 시리즈를 진행할 수도 있겠지만 간략하게 여기서 고려 사항을 요약해드릴게요.\n\n## GKE Autopilot 또는 GKE 표준?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![AutoPilot](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png)\n\n안녕하세요! Autopilot은 기본 클러스터 배포 모드로 이미 장기간 사용되어 왔어요. 이 기능은 다음과 같은 몇 가지 모범 사례를 기본으로 제공합니다:\n\n- 릴리스 채널로의 의무적 등록. 이것은 클러스터가 자동으로 패치되고 유지되며, 오래된 보안 취약한 쿠버네티스 버전을 실행할 수 없다는 것을 의미합니다.\n- 클러스터는 regional로, 클러스터의 고가용성을 보장합니다.\n- 클러스터 자동 스케일링 - GKE가 기본으로 노드 수를 자동으로 조정해 줍니다.\n- 노드 자동 복구가 기본으로 활성화되어 있습니다.\n- 보안 부팅이 있는 shielded 인스턴스로 노드가 구축됩니다.\n- Workload identity가 사전 구성되어 있습니다. 이를 통해 쿠버네티스 서비스 계정이 구글 클라우드 IAM 서비스 계정처럼 작동할 수 있습니다. 따라서, GKE 서비스에서 구글 클라우드 API로의 세분화된 액세스 컨트롤을 제공할 수 있습니다.\n\n그리고 이전에 언급한 것처럼, 실제로 배포되고 실행 중인 파드에 대해 비용을 지불하므로, GKE 클러스터를 비효율적으로 사용하는 국면에서 지출을 낭비하는 일을 피할 수 있습니다. (GKE Standard를 실행할 때 흔히 발생하는 문제입니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대부분의 경우 Autopilot을 권장합니다. 일부 특정한 경우에는 Standard를 사용하고 싶을 수도 있습니다. 예를 들어:\n\n- TPU와 함께 노드를 배포하고 싶을 때.\n- 특정 버전의 Kubernetes를 실행하고 자동 업그레이드를 사용하지 않으려는 경우. (일반적으로 이는 좋지 않은 아이디어이며, 관리형 서비스를 사용하는 의도를 상쇄시키는 것과도 같습니다.)\n- 구글 Autopilot 허용 목록에 없는 특권있는 팟 (즉, 상승된 권한이 필요한 작업 부하)을 실행하고 싶을 때.\n- 실험적인 작업 부하를 단일 존 클러스터에 배포하고 가용성을 보장할 필요가 없는 경우.\n\n## Multitenant 대 Single Tenant 클러스터?\n\n이것은 이전보다는 덜 검정색과 흰색의 문제입니다. Autopilot 이전에는 답이 명확했습니다: 가능한 한 많은 Multitenant 클러스터를 사용하십시오. 아이디어는 매우 적은 클러스터를 가지고 각 클러스터가 조직 내 다중 테넌트로부터 작업 부하를 호스팅한다는 것입니다. 여기서 테넌트는 일반적으로 조직 내에서 다른 팀들을 의미합니다. 그리고 각 테넌트는 하나 이상의 네임스페이스를 소유하게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n멀티테넌트 GKE 클러스터를 사용하면:\n\n- 관리해야 할 클러스터가 하나뿐이기 때문에 클러스터 관리 부담이 비교적 적습니다.\n- 개별 테넌트/애플리케이션은 클러스터를 프로비저닝할 필요가 없습니다. 기존 클러스터에 작업량을 배포하기만 하면 됩니다.\n- 클러스터 자체의 관리 책임을 클라우드 플랫폼 팀에 위임할 수 있어, 애플리케이션 팀은 GKE 관리 책임을 신경 쓸 필요가 없습니다.\n- 테넌트 간의 격리는 네임스페이스 사용을 통해 달성합니다. 테넌트는 자신의 네임스페이스에 배포합니다.\n\n한편, 많은 수의 소규모 단일 테넌트 GKE(표준) 클러스터가 있는 경우 — 즉, 각 클러스터가 하나의 애플리케이션 서비스를 호스팅하는 경우 — 이는 비용이 많이 소요됩니다:\n\n- 각 애플리케이션이 자체 클러스터를 관리해야 합니다. 이로 인해 각 팀에 상당한 클러스터 관리 부담이 발생합니다.\n- 더불어 높은 가용성을 보장하기 위해 각 클러스터는 지역 전체에 최소 수의 노드를 배포해야 합니다. 단일 테넌트 애플리케이션의 경우, 이는 자주 응용프로그램의 요구 사항보다 훨씬 큰 최소 클러스터 크기에 이르게 됩니다. 결과적으로, 대규모로 과잉 프로비저닝되고 비효율적으로 사용되는 클러스터가 많이 생기게 됩니다. GKE 표준 제품을 사용하면 운영 중인 팟이 아닌 배포한 클러스터에 대한 비용을 지불하므로 많은 현금을 낭비하게 됩니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:440/0*sXXDeW5_LtHqMzi4.gif\" /\u003e\n\nAutopilot을 사용하면 단일 테넌트 클러스터가 많은 영향을 미치지 않습니다:\n\n- 실행 중인 팟만 지불하게 되므로 사용하지 않는 클러스터를 대거 배포하여 자금을 낭비하지 않습니다.\n- 박스에서 사전 구성된 많은 모베스트 프랙티스로 인해 테넌트 당 클러스터 관리 부담이 비교적 적습니다.\n\n마무리로 말씀드리면, 가능한 경우에는 멀티테넌트 GKE Autopilot 클러스터를 사용하는 것이 좋습니다. 특별한 경우에만 단일 테넌트 클러스터를 사용하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n조직에서 편안한 수준으로 배포된 몇 개의 다중 테넌트 클러스터를 설정할 수 있습니다. 예를 들어, 각 사업 라인에 맞춰 다중 테넌트 클러스터를 설정할 수 있습니다.\n\n이전 부분에서 논의한 대로 공유 VPC 디자인을 중심으로 랜딩 존을 구축했다면, 일반적으로 다중 테넌트 GKE 클러스터를 공유 VPC 내에 배포하고 싶을 것입니다. 이는 다음과 같을 수 있습니다:\n\n- \"허브\" VPC에 피어링된 다중 테넌트 GKE가 호스팅되는 공유 VPC\n- 다른 공유 서비스가 호스팅되는 같은 공유 VPC\n\n어쨌든, 플랫폼 팀이 공유 VPC가 있는 호스트 프로젝트와 다중 테넌트 GKE 클러스터가 있는 호스트 프로젝트를 소유하게 될 것입니다. 테넌트는 서비스 프로젝트를 소유하게 됩니다. 그들은 다중 테넌트 클러스터(네임스페이스 내)에 직접 워크로드를 배포할 수 있으며, 비-GKE 리소스를 서비스 프로젝트로 배포할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle Cloud 문서는 이 방법을 다음과 같이 설명합니다:\n\n![Google Cloud](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_1.png)\n\n## VPC-Native 또는 라우트 기반 클러스터?\n\n라우트 기반 클러스터는 pod 간 트래픽에 대한 VPC 사용자 정의 라우트에 의존하는 클러스터입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nVPC-native 클러스터는 포드 주소 지정을 위해 별칭 IP 주소 범위를 사용하는 클러스터입니다. 이는 포드가 클러스터의 VPC 네트워크 및 연결된 VPC 네트워크 내에서 네이티브로 라우팅될 수 있음을 의미합니다. 포드 주소를 위해 사용자 정의 정적 경로의 구성이 필요하지 않습니다. 네트워크 관리 부담은 비교적 낮습니다.\n\nVPC-native 클러스터는 RFC 1918 IP 범위 밖의 IP 주소를 사용할 수도 있습니다. 이는 많은 포드 IP 주소가 필요할 것으로 예상될 때 유용할 수 있습니다. 예를 들어, GKE는 240.0.0.0/4 CIDR 범위를 노드, 포드 및 서비스에 사용할 수 있어 추가로 2억 6800만 개의 IP 주소를 제공합니다!\n\nGoogle은 VPC-native 클러스터를 권장합니다. 이것은 GKE Autopilot의 기본 설정이며, 2021년에 출시된 버전 1.21.0-gke.1500부터 GKE 표준의 기본 설정이었습니다.\n\n요약하자면: VPC-native 클러스터를 사용하세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 개인 클러스터?\n\n기본적으로 GKE 클러스터는 공개적입니다. 이는 제어 평면과 워커 노드가 공개 IP 주소를 가지고 있음을 의미합니다. 그러나 조직이 개인 클러스터를 생성하는 것이 모법 사례입니다. 이렇게 하면 클러스터를 인터넷에서 격리할 수 있습니다.\n\n개인 클러스터에서는:\n\n- 워커 및 제어 평면 노드는 비공개 IP 주소만 가지고 있으며 인터넷에 노출되지 않습니다. 예를 들어 NodePort 유형의 서비스는 노드가 인터넷 라우팅 가능한 공개 IP 주소를 가지고 있지 않기 때문에 인터넷에서 클라이언트에게 접근할 수 없습니다.\n- 노드는 Google API 및 서비스와 통신하기 위해 Private Google Access를 사용합니다.\n- 인터넷으로의 외부 액세스는 Cloud NAT 또는 Anthos Service Mesh 이그레스 게이트웨이와 같이 구현한 특정 제어를 통해서만 가능합니다.\n- 외부 클라이언트로부터의 들어오는 연결은 외부 서비스를 통해서만 허용됩니다. 일반적으로 다음을 사용합니다: 외부 Ingress를 가진 LoadBalancer 서비스 또는 Anthos Service Mesh 공개 인그레스 게이트웨이.\n- 워커 노드와 GKE 제어 평면 사이의 통신은 제어 평면의 비공개 엔드포인트를 통해 이루어지며, 제어 평면에 액세스해야 하는 추가 네트워크는 승인되어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_2.png\" /\u003e\n\n## 클러스터간 IP 주소 범위 공유\n\nVPC 네이티브 클러스터를 생성할 때, 노드, 파드 및 서비스에 대한 IP 주소를 할당해야 합니다.\n\n- 노드 IP 주소: 클러스터의 노드인 호스트 머신에 사용되는 주소입니다. 클러스터는 노드에 IP 주소를 할당하기 위해 서브넷의 기본 IPv4 범위를 사용합니다. 노드의 크기를 예상하는데 있어, 가장 큰 크기를 기준으로 서브넷의 크기를 설정해야 합니다.\n- 파드 IP 주소: 클러스터는 파드에 IP 주소를 할당할 때 보조 IPv4 주소 범위를 사용합니다. GKE에서 가장 큰 IP 주소 요구사항이며, 각 노드는 많은 수의 파드를 호스트할 수 있습니다. 기본적으로 GKE Autopilot은 노드 당 최대 파드 수를 32개로 설정하고, 각 노드 당 64개의 IP 주소를 허용하여 파드 교체를 허용합니다. Kubernetes는 각 노드에 보조 IP 주소 범위를 할당하여 각 파드에 고유한 IP 주소를 할당합니다. 기본적으로 GKE Autopilot은 /17 보조 서브넷 범위를 할당하므로 32766개의 사용 가능한 파드가 허용됩니다. (이는 약 1000개가 넘는 노드를 가진 클러스터와 동등합니다.)\n- 서비스(ClusterIP) IP 주소: 클러스터는 내부 서비스 주소를 위해 별도의 보조 IP 주소 범위를 사용합니다. 서비스 IP는 ClusterIP 서비스에 할당됩니다; 이는 클러스터 내에서만 접근 가능한 가상 IP 주소입니다. GKE Autopilot에서는 버전 1.27부터 디폴트로 Google 관리 네트워크에서 IP 주소를 할당하며 범위는 34.118.224.0/20입니다. 동일한 범위가 각 클러스터에 할당됩니다. 따라서 각 클러스터마다 4천 개 이상의 서비스 주소가 제공되며, 기관은 GKE 내의 서비스를 위해 IP 주소를 할당하거나 예약할 필요가 없습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_3.png\" /\u003e\n\n동일한 공유 VPC에서 몇 개의 클러스터를 실행해야 할 수도 있습니다. 예를 들어, 업무별로 클러스터를 정의하고 싶을 수도 있습니다. (이미 언급했듯이, 많은 작은 단독 사용자 클러스터를 갖는 것은 좋지 않은 아이디어입니다.) 이 경우, 같은 서브넷에 호스팅된 클러스터 간에 기본 및 보조 IP 주소 범위를 공유할 수 있습니다. 이것은 하는 것이 좋은 일입니다:\n\n- 클러스터당 서브넷을 할당할 필요가 없습니다.\n- 클러스터당 서브넷 크기를 고려할 필요가 없습니다.\n- 네트워크 관리 오버헤드를 줄일 수 있습니다.\n- IP 주소를 효율적으로 절약하고 IP 주소 고갈 위험을 줄입니다.\n\nVPC에서 클러스터 간에 범위를 공유하기로 결정한다면, 이 점을 주의해야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 미리 명명된 서브넷을 정의하세요.\n- 100.64.0.0/10(약 4.2 백만 개의 파드 사용 가능) 및 240.0.0.0/4(약 2억 6천 8백만 개의 파드 사용 가능)와 같은 RFC 1918 프라이빗 CIDR 범위를 사용하는 것을 고려해보세요.\n\n## 릴리스 채널\n\n여기에서는 GKE 클러스터가 업그레이드되고 유지보수되며 패치되는 전략을 결정합니다.\n\n쿠버네티스 버전은 x.y.z 형식으로 표시되며, 여기서 x는 주요 버전, y는 부 버전, z는 패치 버전을 나타냅니다. 일반적으로 매년 세 번에서 네 번의 중요 (주요 또는 부) 쿠버네티스 릴리스가 있으며, 패치 릴리스는 일주일에 한 번씩 발생합니다. 주요 및 부 릴리스는 새로운 기능과 보안 패치를 모두 포함합니다. 특정 부 릴리스는 대략 1년간 지원됩니다. 따라서 언제든지 일반적으로 지원되는 부 릴리스가 약 세 개 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_4.png)\n\n내 추천 (그리고 구글의 추천)은 항상 클러스터를 릴리스 채널에 등록하는 것입니다. 릴리스 채널을 사용하면 구글이 클러스터 업그레이드를 관리해줍니다. 워커 노드는 롤링 서지 업그레이드 전략을 사용하여 자동으로 업그레이드되어 워크로드에 미치는 영향을 최소화합니다. 따라서 클러스터 관리자는 클러스터 업그레이드에 시간이나 노력을 들일 필요가 없습니다.\n\nGKE Standard를 사용하면 릴리스 채널을 사용하지 않을 수 있습니다. 이렇게 하려면 클러스터를 특정 버전의 Kubernetes로 유지하고 싶을 때 사용할 수 있습니다. 그러나 제 경험상, 릴리스 채널에 선택적으로 가입하지 않는 기관은 곧 자신들의 취약한 클러스터가 가득한 시스템을 운영하게 됩니다. 그러한 기관들은 큰 패치 문제를 안게 될 것입니다!\n\n![image](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGKE Autopilot을 사용하면 릴리스 채널을 선택해 사용하지 않을 수 없습니다. (정말 좋은 점이에요!)\n\n(참고로: GKE 컨트롤 플레인 노드는 항상 Google에 의해 업그레이드되며, 이 프로세스를 거부할 방법이 없습니다. 이는 릴리스 채널 등록 여부와 상관없이 해당됩니다.)\n\n선택할 수 있는 세 가지 릴리스 채널이 있습니다:\n\n- 빠른(Rapid) — 오픈 소스 릴리스가 일반적으로 사용 가능한 후 몇 주 후에 Kubernetes 클러스터가 업그레이드됩니다. 이 채널은 가장 최신 기능을 제공하지만 가장 안정성이 낮습니다. 또한, 이 릴리스 채널의 클러스터는 GKE SLA에서 지원되지 않을 것입니다.\n- 보통(Regular) — Kubernetes가 릴리스 후 2~3개월 후에 Rapid에서 실행되고 있는 릴리스 버전으로 업그레이드됩니다. 새로운 기능과 안정성 사이의 균형을 제공합니다. 기본적으로 GKE Autopilot은 노드를 이 릴리스 채널에 등록합니다.\n- 안정(Stable) — Kubernetes가 Regular에서 실행되고 있는 릴리스 버전으로 업그레이드됩니다. 이 릴리스는 커뮤니티에서 약 5~6개월동안 사용 가능한 후 클러스터 노드에 적용됩니다. 이 채널은 가장 검증되었고 가장 안정적일 것이지만 최근 Kubernetes 기능을 제공하지 않을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새로운 GKE 버전이 릴리스 채널에서 기본 버전이 되면 해당 클러스터는 일반적으로 10일 이내에 업그레이드됩니다.\n\n다음은 릴리스 채널 채용에 대한 일반적인 권장 사항입니다:\n\n- 생산 워크로드는 클러스터의 중요성 및 위험 수용 능력에 따라 Stable 또는 Regular에 등록해야 합니다. 중요한 워크로드는 Stable에 등록하는 것을 권장합니다.\n- 최종 비생산 환경(일반적으로 스테이징, Pre-Prod 또는 UAT 환경)은 생산 환경과 동일한 릴리스 채널에 등록되어야 합니다. 왜냐하면 새로 시도해보지 않은 Kubernetes 버전에서 워크로드가 생산 환경에 배치되는 것을 원치 않기 때문입니다.\n- 상위 비생산 환경(예: Dev 또는 QA)은 이웃한 상위 릴리스 채널에 배포되어야 합니다.\n- 새로운 기능을 실험하려는 개발 환경에서만 빠른 릴리스 채널을 사용해야 합니다. 새로운 기능은 안정적인 릴리스 채널에 몇 달 내에 나타날 것을 기억해주세요.\n\n예시:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_6.png)\n\n릴리스 채널의 한 가지 문제는 클러스터가 언제 업그레이드될지 보장할 수 없다는 것입니다. 최종 비 프로드 환경 및 프로드 환경을 동일한 릴리스 채널(구글에서 권장하는 최상의 방법)에 등록하면 비 프로드 환경이 프로드 환경보다 먼저 업그레이드되도록 보호막을 구현하고 싶을 것입니다. GKE fleets 및 scopes를 사용하면 쉽게 이를 수행할 수 있습니다.\n\n다음은 이러한 배포 순서의 예시입니다:\n\n![이미지](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_7.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 예에서:\n\n- Staging 환경의 GKE 클러스터는 Staging 플리트에 속합니다.\n- Production에 있는 클러스터는 Production 플리트에 속합니다.\n- Staging 및 Production에 있는 모든 클러스터는 Stable 릴리스 채널에 등록되어 있습니다.\n- Production 플리트는 Staging 플리트가 7일간 실행된 후에만 업그레이드됩니다.\n\n## Workload Identity Federation\n\n워크로드 ID 페더레이션은 Kubernetes 서비스 계정이 IAM 서비스 계정으로 작동할 수 있게 합니다. 구성된 Kubernetes 서비스 계정을 사용하는 파드는 Google Cloud API에 액세스할 때 자동으로 IAM 서비스 계정으로 인증합니다. 이를 통해 GKE 애플리케이션 워크로드가 인증되고 허가되어 Google Cloud 서비스에 액세스할 수 있도록 보장할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n오토파일럿 클러스터는 GKE에서 기본적으로 워크로드 ID 연합을 지원합니다.\n\n## 오토스케일링 전략\n\n오토스케일링은 클러스터가 실행 중인 응용 프로그램의 수요를 충족하기 위해 조정할 수 있는 능력을 가리킵니다.\n\nGKE에는 네 가지 스케일링 차원이 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 워크로드는 수요에 따라 가로 방향으로 확장됩니다. Pod를 추가하거나 제거함으로써 관리됩니다. 이는 가로 방향 팟 오토스케일러(HPA)에 의해 관리되며, 수요에 따라 빠르게 확장됩니다. HPA는 CPU 사용률과 같은 표준 메트릭 또는 초당 요청과 같은 사용자 정의 메트릭에 응답합니다.\n- 인프라는 클러스터 노드를 추가하거나 제거함으로써 가로 방향으로 확장됩니다. 이는 예약된 팟을 수용하기 위해 클러스터 자동스케일러(CA)에 의해 예측적으로 관리됩니다. 예를 들어, 새로 생성된 팟을 예약할 노드가 없는 경우, 클러스터 자동스케일러가 새 노드를 만듭니다.\n- 워크로드는 팟 크기를 조절함으로써 세로 방향으로 확장됩니다. 이는 세로 방향 팟 오토스케일러(VPA)에 의해 관리됩니다. VPA는 시간이 경과함에 따라 팟의 CPU 및 메모리 사용률을 모니터링하고 이에 따라 팟 크기를 조정합니다. 이로써 보다 최적화되고 비용 효율적인 팟 크기를 얻을 수 있습니다.\n- 인프라는 가장 효율적인 팟의 바이너리 패킹을 달성하기 위해 최적화된 노드(VM) 사이즈로 노드 풀을 배포하거나 삭제함으로써 세로 방향으로 확장됩니다. 이를 \"노드 자동 프로비저닝\"이라고 하며, 다른 종류의 확장에 비해 상대적으로 느립니다.\n\n아래는 몇 가지 팁입니다:\n\n- GKE Autopilot을 사용할 때는 인프라 자동스케일링 (CA 및 NAP)이 Google에 의해 설정됩니다. 당신은 팟 오토스케일링 구성만 고려하면 됩니다.\n- HPA와 VPA를 동시에 사용하지 마세요. 같은 자원 메트릭으로 HPA와 VPA를 함께 설정하는 것을 피하세요. 예를 들어, CPU 사용률에 대한 HPA와 VPA를 동시에 설정하는 것을 피하세요.\n- 가로와 세로 방향 팟 오토스케일링을 관리하는 다차원 팟 오토스케일러(MPA)를 사용하면 워크로드 확장을 간단히 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결론적으로:\n\n- GKE 표준 버전을 사용하면 인프라 스케일링과 워크로드(파드) 스케일링을 모두 관리해야 합니다.\n- GKE Autopilot을 사용하면 워크로드 스케일링에만 집중하면 되며, MPA를 사용하여 워크로드 자동 스케일링을 간편하게 할 수 있습니다.\n\n## 인프라 및 워크로드 배포\n\nGoogle은 클러스터 자체를 배포할 때 클라우드 인프라의 경우와 마찬가지로 인프라를 코드로 관리하는 것을 권장합니다(Terraform과 같은). 따라서 클러스터 및 네임스페이스를 배포할 때 IaC를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n귀하의 작업 로드를 배포하려면 — 배포, 서비스, 작업, StatefufSets, 인그레스, 정책 등 — 선언적 yaml 파일을 사용하여 네이티브 Kubernetes API 호출을 해야 합니다. Helm은 쿠버네티스에서 애플리케이션 배포를 관리하는 데 도움을 줄 수 있습니다.\n\n## GKE 디자인 결정 요약\n\n요약하자면: 고려해야 할 주요 Kubernetes 디자인 결정 사항과 각각에 대한 나의 권장 사항입니다.\n\n- GKE, 또는 스스로 관리하는 방식? 권장 사항: GKE.\n- GKE Autopilot 대 GKE Standard. 권장 사항: Autopilot.\n- 멀티 테넌트 클러스터? 어떤 수준의 클러스터? 권장 사항: 멀티 테넌트.\n- 싱글 테넌트 클러스터를 만들 수 있는 능력. 권장 사항: 예외적으로만.\n- VPC 네이티브 또는 라우트 기반 클러스터? 권장 사항: VPC 네이티브.\n- 프라이빗 클러스터? 권장 사항: 프라이빗 클러스터를 사용하세요.\n- 클러스터 간 IP 주소 범위 공유? 추천 사항: 공유 VPC 내에 몇 개 또는 많은 클러스터가 있다면 이를 수행해야 합니다.\n- 릴리스 채널? 항상 릴리스 채널에 등록하세요. 스테이징 및 프로드 클러스터를 동일한 릴리스 채널에 유지하세요. 클러스터가 올바른 순서로 업그레이드되도록 보장하기 위해 fleets 및 rollout sequences를 사용하세요.\n- Workload identity? 네 — 사용하세요.\n- 오토스케일링 전략? 워크로드 오토스케일링을 지원하기 위해 클러스터 오토스케일러를 사용하세요. GKE Autopilot을 사용할 때 클러스터 오토스케일링은 자동으로 관리됩니다. 워크로드 오토스케일링을 간소화하기 위해 MPA를 사용하세요.\n- 인프라 및 워크로드 배포? 클러스터 배포 및 관리에는 IaC(예: Terraform)를 사용하세요. 애플리케이션 워크로드를 배포하기 위해 선언적 Kubernetes 매니페스트를 사용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마무리\n\nGKE를 랜딩 존에서 성공적으로 디자인하는 데 고려해야 할 주요 사항들은 여기까지입니다. 다음 파트에서는 LZ 디자인 고려 사항을 완료하고, 로깅 및 모니터링 전략, 요금 청구, 인프라스트럭처 코드 (IaC)와 같은 주제를 다룰 것입니다.\n\n# 떠나시기 전에\n\n- 관심이 있을 것으로 생각되는 분과 공유해주세요. 그들에게 도움이 될 수도 있고, 저에게 정말로 도움이 됩니다!\n- 박수를 부탁드립니다! 여러분은 한 번 이상 박수를 칠 수 있다는 걸 아시나요?\n- 자유롭게 댓글을 남겨주세요 💬.\n- 내 컨텐츠를 놓치지 않으려면 팔로우하고 구독해주세요. 내 프로필 페이지로 이동하여 이 아이콘들을 클릭해주세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_9.png)\n\n# 링크\n\n- Google Cloud의 랜딩 존: 필요성 및 생성 방법\n- Google Cloud의 랜딩 존 디자인\n- GKE Autopilot\n- GKE Autopilot 특권 업무 용 업무\n- GKE 클러스터 구성 옵션\n- 기업용 GKE 멀티 테넌시에 대한 모베스트 프랙티스\n- VPC 네이티브 GKE 클러스터\n- GKE의 프라이빗 클러스터\n- 외부 애플리케이션 로드 밸런서를 위한 GKE Ingress\n- GKE로 이주할 때 IP 주소 계획\n- GKE 네트워크 플래닝 2023 (William Denniss)\n- GKE 릴리스 채널\n- 롤아웃 순서로 GKE 클러스터 업그레이드\n- GKE SLA\n- GKE 자동 확장 (Kaslin Fields)\n- GKE에서 비용 최적화된 애플리케이션에 대한 모베스트 프랙티스\n- GKE를 위한 워크로드 ID 연합\n- Google Cloud 아키텍처 프레임워크\n- 기업용 기초 설계 청사진\n\n# 시리즈 내비게이션\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 시리즈 개요 및 구조\n- 이전: 랜딩 존 설계하기 — 디자인 고려사항 파트 1\n- 다음: 랜딩 존 설계하기 — 디자인 고려사항 파트 3 — 모니터링, 로깅, 빌링 및 라벨링\n","ogImage":{"url":"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png"},"coverImage":"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png","tag":["Tech"],"readingTime":16},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e마음에 드시는 markdown 형식의 표를 아래에 참조해보세요:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e구분\u003c/th\u003e\n\u003cth\u003e설명\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e착륙 지역이란?\u003c/td\u003e\n\u003ctd\u003e착륙 지역의 정의 및 필요성에 대해 다룸\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e디자인 프로세스 개요\u003c/td\u003e\n\u003ctd\u003e착륙 지역 디자인 프로세스 개요 소개\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e디자인 고려 사항 카테고리\u003c/td\u003e\n\u003ctd\u003e착륙 지역 디자인 시 고려해야 할 7가지 카테고리 및 주요 디자인 결정 사항 소개\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1\u003e8. Google Kubernetes Engine (GKE)\u003c/h1\u003e\n\u003cp\u003e앞서 이 시리즈에서 말씀드렸듯이, 컨테이너는 훌륭해요! 가벼우며 빠르고 휴대성이 좋으며 쉽게 확장할 수 있어요. 이것들은 마이크로서비스 아키텍처에 잘 어울려요.\u003c/p\u003e\n\u003cp\u003eGoogle Cloud는 컨테이너를 실행하는 몇 가지 다른 방법을 제공하고 있어요. 그러나 클라우드에서 현대적인 컨테이너 기반 작업을 실행한다면 Kubernetes가 필요하겠죠. 그리고 Google Cloud에서 Kubernetes를 실행한다면 Google Kubernetes Engine을 사용해야 해요.\u003c/p\u003e\n\u003cp\u003eGKE은 Google의 관리형 Kubernetes 플랫폼이에요. 이를 통해 Google Cloud에서 Kubernetes를 실행할 수 있지만 자체 관리형 Kubernetes보다 여러가지 이점을 제공해요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGKE는 클러스터를 배포하고 쿠버네티스를 설치하며 노드를 등록하는 것을 처리해줍니다. 새로운 클러스터는 몇 분 안에 배포할 수 있어요!\u003c/li\u003e\n\u003cli\u003e쿠버네티스 제어 플레인 노드는 완전 관리되며 소비자로부터 완전히 추상화되어 있어요.\u003c/li\u003e\n\u003cli\u003e호스트는 구글이 투명하게 관리하고 패치하는 견고하고 미리 구성된 컨테이너 용도 OS에서 실행돼요.\u003c/li\u003e\n\u003cli\u003e구글은 릴리스 채널을 통해 쿠버네티스 업그레이드를 투명하게 관리해줘요.\u003c/li\u003e\n\u003cli\u003e클러스터는 워크로드 수요를 충족하기 위해 탄탄하게 자동으로 탄력적으로 확장돼요. (관리되는 인스턴스 그룹 및 오토스케일러와 로드 밸런서를 만들 필요 없이!)\u003c/li\u003e\n\u003cli\u003e비파괴적으로 자동 수리 및 불건전한 클러스터 노드의 교체를 자동으로 수행해줘요.\u003c/li\u003e\n\u003cli\u003e기본 제공 지역 고가용성.\u003c/li\u003e\n\u003cli\u003eGKE는 Google Cloud Operations에 네이티브로 통합돼 있어서 모니터링, 로깅 및 메트릭스에 쉽게 액세스할 수 있어요.\u003c/li\u003e\n\u003cli\u003eGKE Autopilot으로 클러스터는 기본 제공된 모베스트 프랙티스로 사전구성돼요.\u003c/li\u003e\n\u003cli\u003eGKE Autopilot으로 실행 중인 워크로드에 비용을 지급하게 됩니다. (POD별 청구라고도 함) 클러스터를 배포하는 데 비용을 내야 하는 대신입니다. 이것은 게임 체인저에요!\u003c/li\u003e\n\u003cli\u003eGKE 노드 풀 및 Autopilot에서 계산 클래스로 작업 부하에 필요한 특정 기계 유형을 할당할 수 있어요. 또한 AI/ML 학습과 같이 필요로 하는 워크로드에 GPU를 할당할 수 있어요.\u003c/li\u003e\n\u003cli\u003e완벽한 Google Anthos 서비스 메시와의 원활한 통합, 스스로 완전 관리 서비스로 제공할 수 있어요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(참고로, 여기에서 구글 컴퓨트 엔진에서 자체 관리 쿠버네티스 환경을 실행하려는 것이 왜 좋지 않은 아이디어인지 다양한 이유에 대해 다뤘었어요.)\u003c/p\u003e\n\u003cp\u003e조금 고려해야 할 설계 결정과 최상의 모베스트 프랙티스가 있어요. 이 주제에 대한 시리즈를 진행할 수도 있겠지만 간략하게 여기서 고려 사항을 요약해드릴게요.\u003c/p\u003e\n\u003ch2\u003eGKE Autopilot 또는 GKE 표준?\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_0.png\" alt=\"AutoPilot\"\u003e\u003c/p\u003e\n\u003cp\u003e안녕하세요! Autopilot은 기본 클러스터 배포 모드로 이미 장기간 사용되어 왔어요. 이 기능은 다음과 같은 몇 가지 모범 사례를 기본으로 제공합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e릴리스 채널로의 의무적 등록. 이것은 클러스터가 자동으로 패치되고 유지되며, 오래된 보안 취약한 쿠버네티스 버전을 실행할 수 없다는 것을 의미합니다.\u003c/li\u003e\n\u003cli\u003e클러스터는 regional로, 클러스터의 고가용성을 보장합니다.\u003c/li\u003e\n\u003cli\u003e클러스터 자동 스케일링 - GKE가 기본으로 노드 수를 자동으로 조정해 줍니다.\u003c/li\u003e\n\u003cli\u003e노드 자동 복구가 기본으로 활성화되어 있습니다.\u003c/li\u003e\n\u003cli\u003e보안 부팅이 있는 shielded 인스턴스로 노드가 구축됩니다.\u003c/li\u003e\n\u003cli\u003eWorkload identity가 사전 구성되어 있습니다. 이를 통해 쿠버네티스 서비스 계정이 구글 클라우드 IAM 서비스 계정처럼 작동할 수 있습니다. 따라서, GKE 서비스에서 구글 클라우드 API로의 세분화된 액세스 컨트롤을 제공할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그리고 이전에 언급한 것처럼, 실제로 배포되고 실행 중인 파드에 대해 비용을 지불하므로, GKE 클러스터를 비효율적으로 사용하는 국면에서 지출을 낭비하는 일을 피할 수 있습니다. (GKE Standard를 실행할 때 흔히 발생하는 문제입니다.)\u003c/p\u003e\n\u003cp\u003e대부분의 경우 Autopilot을 권장합니다. 일부 특정한 경우에는 Standard를 사용하고 싶을 수도 있습니다. 예를 들어:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTPU와 함께 노드를 배포하고 싶을 때.\u003c/li\u003e\n\u003cli\u003e특정 버전의 Kubernetes를 실행하고 자동 업그레이드를 사용하지 않으려는 경우. (일반적으로 이는 좋지 않은 아이디어이며, 관리형 서비스를 사용하는 의도를 상쇄시키는 것과도 같습니다.)\u003c/li\u003e\n\u003cli\u003e구글 Autopilot 허용 목록에 없는 특권있는 팟 (즉, 상승된 권한이 필요한 작업 부하)을 실행하고 싶을 때.\u003c/li\u003e\n\u003cli\u003e실험적인 작업 부하를 단일 존 클러스터에 배포하고 가용성을 보장할 필요가 없는 경우.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMultitenant 대 Single Tenant 클러스터?\u003c/h2\u003e\n\u003cp\u003e이것은 이전보다는 덜 검정색과 흰색의 문제입니다. Autopilot 이전에는 답이 명확했습니다: 가능한 한 많은 Multitenant 클러스터를 사용하십시오. 아이디어는 매우 적은 클러스터를 가지고 각 클러스터가 조직 내 다중 테넌트로부터 작업 부하를 호스팅한다는 것입니다. 여기서 테넌트는 일반적으로 조직 내에서 다른 팀들을 의미합니다. 그리고 각 테넌트는 하나 이상의 네임스페이스를 소유하게 됩니다.\u003c/p\u003e\n\u003cp\u003e멀티테넌트 GKE 클러스터를 사용하면:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e관리해야 할 클러스터가 하나뿐이기 때문에 클러스터 관리 부담이 비교적 적습니다.\u003c/li\u003e\n\u003cli\u003e개별 테넌트/애플리케이션은 클러스터를 프로비저닝할 필요가 없습니다. 기존 클러스터에 작업량을 배포하기만 하면 됩니다.\u003c/li\u003e\n\u003cli\u003e클러스터 자체의 관리 책임을 클라우드 플랫폼 팀에 위임할 수 있어, 애플리케이션 팀은 GKE 관리 책임을 신경 쓸 필요가 없습니다.\u003c/li\u003e\n\u003cli\u003e테넌트 간의 격리는 네임스페이스 사용을 통해 달성합니다. 테넌트는 자신의 네임스페이스에 배포합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e한편, 많은 수의 소규모 단일 테넌트 GKE(표준) 클러스터가 있는 경우 — 즉, 각 클러스터가 하나의 애플리케이션 서비스를 호스팅하는 경우 — 이는 비용이 많이 소요됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e각 애플리케이션이 자체 클러스터를 관리해야 합니다. 이로 인해 각 팀에 상당한 클러스터 관리 부담이 발생합니다.\u003c/li\u003e\n\u003cli\u003e더불어 높은 가용성을 보장하기 위해 각 클러스터는 지역 전체에 최소 수의 노드를 배포해야 합니다. 단일 테넌트 애플리케이션의 경우, 이는 자주 응용프로그램의 요구 사항보다 훨씬 큰 최소 클러스터 크기에 이르게 됩니다. 결과적으로, 대규모로 과잉 프로비저닝되고 비효율적으로 사용되는 클러스터가 많이 생기게 됩니다. GKE 표준 제품을 사용하면 운영 중인 팟이 아닌 배포한 클러스터에 대한 비용을 지불하므로 많은 현금을 낭비하게 됩니다!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAutopilot을 사용하면 단일 테넌트 클러스터가 많은 영향을 미치지 않습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e실행 중인 팟만 지불하게 되므로 사용하지 않는 클러스터를 대거 배포하여 자금을 낭비하지 않습니다.\u003c/li\u003e\n\u003cli\u003e박스에서 사전 구성된 많은 모베스트 프랙티스로 인해 테넌트 당 클러스터 관리 부담이 비교적 적습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e마무리로 말씀드리면, 가능한 경우에는 멀티테넌트 GKE Autopilot 클러스터를 사용하는 것이 좋습니다. 특별한 경우에만 단일 테넌트 클러스터를 사용하십시오.\u003c/p\u003e\n\u003cp\u003e조직에서 편안한 수준으로 배포된 몇 개의 다중 테넌트 클러스터를 설정할 수 있습니다. 예를 들어, 각 사업 라인에 맞춰 다중 테넌트 클러스터를 설정할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e이전 부분에서 논의한 대로 공유 VPC 디자인을 중심으로 랜딩 존을 구축했다면, 일반적으로 다중 테넌트 GKE 클러스터를 공유 VPC 내에 배포하고 싶을 것입니다. 이는 다음과 같을 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"허브\" VPC에 피어링된 다중 테넌트 GKE가 호스팅되는 공유 VPC\u003c/li\u003e\n\u003cli\u003e다른 공유 서비스가 호스팅되는 같은 공유 VPC\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e어쨌든, 플랫폼 팀이 공유 VPC가 있는 호스트 프로젝트와 다중 테넌트 GKE 클러스터가 있는 호스트 프로젝트를 소유하게 될 것입니다. 테넌트는 서비스 프로젝트를 소유하게 됩니다. 그들은 다중 테넌트 클러스터(네임스페이스 내)에 직접 워크로드를 배포할 수 있으며, 비-GKE 리소스를 서비스 프로젝트로 배포할 수 있습니다.\u003c/p\u003e\n\u003cp\u003eGoogle Cloud 문서는 이 방법을 다음과 같이 설명합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_1.png\" alt=\"Google Cloud\"\u003e\u003c/p\u003e\n\u003ch2\u003eVPC-Native 또는 라우트 기반 클러스터?\u003c/h2\u003e\n\u003cp\u003e라우트 기반 클러스터는 pod 간 트래픽에 대한 VPC 사용자 정의 라우트에 의존하는 클러스터입니다.\u003c/p\u003e\n\u003cp\u003eVPC-native 클러스터는 포드 주소 지정을 위해 별칭 IP 주소 범위를 사용하는 클러스터입니다. 이는 포드가 클러스터의 VPC 네트워크 및 연결된 VPC 네트워크 내에서 네이티브로 라우팅될 수 있음을 의미합니다. 포드 주소를 위해 사용자 정의 정적 경로의 구성이 필요하지 않습니다. 네트워크 관리 부담은 비교적 낮습니다.\u003c/p\u003e\n\u003cp\u003eVPC-native 클러스터는 RFC 1918 IP 범위 밖의 IP 주소를 사용할 수도 있습니다. 이는 많은 포드 IP 주소가 필요할 것으로 예상될 때 유용할 수 있습니다. 예를 들어, GKE는 240.0.0.0/4 CIDR 범위를 노드, 포드 및 서비스에 사용할 수 있어 추가로 2억 6800만 개의 IP 주소를 제공합니다!\u003c/p\u003e\n\u003cp\u003eGoogle은 VPC-native 클러스터를 권장합니다. 이것은 GKE Autopilot의 기본 설정이며, 2021년에 출시된 버전 1.21.0-gke.1500부터 GKE 표준의 기본 설정이었습니다.\u003c/p\u003e\n\u003cp\u003e요약하자면: VPC-native 클러스터를 사용하세요!\u003c/p\u003e\n\u003ch2\u003e개인 클러스터?\u003c/h2\u003e\n\u003cp\u003e기본적으로 GKE 클러스터는 공개적입니다. 이는 제어 평면과 워커 노드가 공개 IP 주소를 가지고 있음을 의미합니다. 그러나 조직이 개인 클러스터를 생성하는 것이 모법 사례입니다. 이렇게 하면 클러스터를 인터넷에서 격리할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e개인 클러스터에서는:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e워커 및 제어 평면 노드는 비공개 IP 주소만 가지고 있으며 인터넷에 노출되지 않습니다. 예를 들어 NodePort 유형의 서비스는 노드가 인터넷 라우팅 가능한 공개 IP 주소를 가지고 있지 않기 때문에 인터넷에서 클라이언트에게 접근할 수 없습니다.\u003c/li\u003e\n\u003cli\u003e노드는 Google API 및 서비스와 통신하기 위해 Private Google Access를 사용합니다.\u003c/li\u003e\n\u003cli\u003e인터넷으로의 외부 액세스는 Cloud NAT 또는 Anthos Service Mesh 이그레스 게이트웨이와 같이 구현한 특정 제어를 통해서만 가능합니다.\u003c/li\u003e\n\u003cli\u003e외부 클라이언트로부터의 들어오는 연결은 외부 서비스를 통해서만 허용됩니다. 일반적으로 다음을 사용합니다: 외부 Ingress를 가진 LoadBalancer 서비스 또는 Anthos Service Mesh 공개 인그레스 게이트웨이.\u003c/li\u003e\n\u003cli\u003e워커 노드와 GKE 제어 평면 사이의 통신은 제어 평면의 비공개 엔드포인트를 통해 이루어지며, 제어 평면에 액세스해야 하는 추가 네트워크는 승인되어야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e클러스터간 IP 주소 범위 공유\u003c/h2\u003e\n\u003cp\u003eVPC 네이티브 클러스터를 생성할 때, 노드, 파드 및 서비스에 대한 IP 주소를 할당해야 합니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e노드 IP 주소: 클러스터의 노드인 호스트 머신에 사용되는 주소입니다. 클러스터는 노드에 IP 주소를 할당하기 위해 서브넷의 기본 IPv4 범위를 사용합니다. 노드의 크기를 예상하는데 있어, 가장 큰 크기를 기준으로 서브넷의 크기를 설정해야 합니다.\u003c/li\u003e\n\u003cli\u003e파드 IP 주소: 클러스터는 파드에 IP 주소를 할당할 때 보조 IPv4 주소 범위를 사용합니다. GKE에서 가장 큰 IP 주소 요구사항이며, 각 노드는 많은 수의 파드를 호스트할 수 있습니다. 기본적으로 GKE Autopilot은 노드 당 최대 파드 수를 32개로 설정하고, 각 노드 당 64개의 IP 주소를 허용하여 파드 교체를 허용합니다. Kubernetes는 각 노드에 보조 IP 주소 범위를 할당하여 각 파드에 고유한 IP 주소를 할당합니다. 기본적으로 GKE Autopilot은 /17 보조 서브넷 범위를 할당하므로 32766개의 사용 가능한 파드가 허용됩니다. (이는 약 1000개가 넘는 노드를 가진 클러스터와 동등합니다.)\u003c/li\u003e\n\u003cli\u003e서비스(ClusterIP) IP 주소: 클러스터는 내부 서비스 주소를 위해 별도의 보조 IP 주소 범위를 사용합니다. 서비스 IP는 ClusterIP 서비스에 할당됩니다; 이는 클러스터 내에서만 접근 가능한 가상 IP 주소입니다. GKE Autopilot에서는 버전 1.27부터 디폴트로 Google 관리 네트워크에서 IP 주소를 할당하며 범위는 34.118.224.0/20입니다. 동일한 범위가 각 클러스터에 할당됩니다. 따라서 각 클러스터마다 4천 개 이상의 서비스 주소가 제공되며, 기관은 GKE 내의 서비스를 위해 IP 주소를 할당하거나 예약할 필요가 없습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e동일한 공유 VPC에서 몇 개의 클러스터를 실행해야 할 수도 있습니다. 예를 들어, 업무별로 클러스터를 정의하고 싶을 수도 있습니다. (이미 언급했듯이, 많은 작은 단독 사용자 클러스터를 갖는 것은 좋지 않은 아이디어입니다.) 이 경우, 같은 서브넷에 호스팅된 클러스터 간에 기본 및 보조 IP 주소 범위를 공유할 수 있습니다. 이것은 하는 것이 좋은 일입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e클러스터당 서브넷을 할당할 필요가 없습니다.\u003c/li\u003e\n\u003cli\u003e클러스터당 서브넷 크기를 고려할 필요가 없습니다.\u003c/li\u003e\n\u003cli\u003e네트워크 관리 오버헤드를 줄일 수 있습니다.\u003c/li\u003e\n\u003cli\u003eIP 주소를 효율적으로 절약하고 IP 주소 고갈 위험을 줄입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eVPC에서 클러스터 간에 범위를 공유하기로 결정한다면, 이 점을 주의해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e미리 명명된 서브넷을 정의하세요.\u003c/li\u003e\n\u003cli\u003e100.64.0.0/10(약 4.2 백만 개의 파드 사용 가능) 및 240.0.0.0/4(약 2억 6천 8백만 개의 파드 사용 가능)와 같은 RFC 1918 프라이빗 CIDR 범위를 사용하는 것을 고려해보세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e릴리스 채널\u003c/h2\u003e\n\u003cp\u003e여기에서는 GKE 클러스터가 업그레이드되고 유지보수되며 패치되는 전략을 결정합니다.\u003c/p\u003e\n\u003cp\u003e쿠버네티스 버전은 x.y.z 형식으로 표시되며, 여기서 x는 주요 버전, y는 부 버전, z는 패치 버전을 나타냅니다. 일반적으로 매년 세 번에서 네 번의 중요 (주요 또는 부) 쿠버네티스 릴리스가 있으며, 패치 릴리스는 일주일에 한 번씩 발생합니다. 주요 및 부 릴리스는 새로운 기능과 보안 패치를 모두 포함합니다. 특정 부 릴리스는 대략 1년간 지원됩니다. 따라서 언제든지 일반적으로 지원되는 부 릴리스가 약 세 개 있을 것입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e내 추천 (그리고 구글의 추천)은 항상 클러스터를 릴리스 채널에 등록하는 것입니다. 릴리스 채널을 사용하면 구글이 클러스터 업그레이드를 관리해줍니다. 워커 노드는 롤링 서지 업그레이드 전략을 사용하여 자동으로 업그레이드되어 워크로드에 미치는 영향을 최소화합니다. 따라서 클러스터 관리자는 클러스터 업그레이드에 시간이나 노력을 들일 필요가 없습니다.\u003c/p\u003e\n\u003cp\u003eGKE Standard를 사용하면 릴리스 채널을 사용하지 않을 수 있습니다. 이렇게 하려면 클러스터를 특정 버전의 Kubernetes로 유지하고 싶을 때 사용할 수 있습니다. 그러나 제 경험상, 릴리스 채널에 선택적으로 가입하지 않는 기관은 곧 자신들의 취약한 클러스터가 가득한 시스템을 운영하게 됩니다. 그러한 기관들은 큰 패치 문제를 안게 될 것입니다!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_5.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eGKE Autopilot을 사용하면 릴리스 채널을 선택해 사용하지 않을 수 없습니다. (정말 좋은 점이에요!)\u003c/p\u003e\n\u003cp\u003e(참고로: GKE 컨트롤 플레인 노드는 항상 Google에 의해 업그레이드되며, 이 프로세스를 거부할 방법이 없습니다. 이는 릴리스 채널 등록 여부와 상관없이 해당됩니다.)\u003c/p\u003e\n\u003cp\u003e선택할 수 있는 세 가지 릴리스 채널이 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e빠른(Rapid) — 오픈 소스 릴리스가 일반적으로 사용 가능한 후 몇 주 후에 Kubernetes 클러스터가 업그레이드됩니다. 이 채널은 가장 최신 기능을 제공하지만 가장 안정성이 낮습니다. 또한, 이 릴리스 채널의 클러스터는 GKE SLA에서 지원되지 않을 것입니다.\u003c/li\u003e\n\u003cli\u003e보통(Regular) — Kubernetes가 릴리스 후 2~3개월 후에 Rapid에서 실행되고 있는 릴리스 버전으로 업그레이드됩니다. 새로운 기능과 안정성 사이의 균형을 제공합니다. 기본적으로 GKE Autopilot은 노드를 이 릴리스 채널에 등록합니다.\u003c/li\u003e\n\u003cli\u003e안정(Stable) — Kubernetes가 Regular에서 실행되고 있는 릴리스 버전으로 업그레이드됩니다. 이 릴리스는 커뮤니티에서 약 5~6개월동안 사용 가능한 후 클러스터 노드에 적용됩니다. 이 채널은 가장 검증되었고 가장 안정적일 것이지만 최근 Kubernetes 기능을 제공하지 않을 것입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e새로운 GKE 버전이 릴리스 채널에서 기본 버전이 되면 해당 클러스터는 일반적으로 10일 이내에 업그레이드됩니다.\u003c/p\u003e\n\u003cp\u003e다음은 릴리스 채널 채용에 대한 일반적인 권장 사항입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e생산 워크로드는 클러스터의 중요성 및 위험 수용 능력에 따라 Stable 또는 Regular에 등록해야 합니다. 중요한 워크로드는 Stable에 등록하는 것을 권장합니다.\u003c/li\u003e\n\u003cli\u003e최종 비생산 환경(일반적으로 스테이징, Pre-Prod 또는 UAT 환경)은 생산 환경과 동일한 릴리스 채널에 등록되어야 합니다. 왜냐하면 새로 시도해보지 않은 Kubernetes 버전에서 워크로드가 생산 환경에 배치되는 것을 원치 않기 때문입니다.\u003c/li\u003e\n\u003cli\u003e상위 비생산 환경(예: Dev 또는 QA)은 이웃한 상위 릴리스 채널에 배포되어야 합니다.\u003c/li\u003e\n\u003cli\u003e새로운 기능을 실험하려는 개발 환경에서만 빠른 릴리스 채널을 사용해야 합니다. 새로운 기능은 안정적인 릴리스 채널에 몇 달 내에 나타날 것을 기억해주세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e예시:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_6.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e릴리스 채널의 한 가지 문제는 클러스터가 언제 업그레이드될지 보장할 수 없다는 것입니다. 최종 비 프로드 환경 및 프로드 환경을 동일한 릴리스 채널(구글에서 권장하는 최상의 방법)에 등록하면 비 프로드 환경이 프로드 환경보다 먼저 업그레이드되도록 보호막을 구현하고 싶을 것입니다. GKE fleets 및 scopes를 사용하면 쉽게 이를 수행할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e다음은 이러한 배포 순서의 예시입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e이 예에서:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStaging 환경의 GKE 클러스터는 Staging 플리트에 속합니다.\u003c/li\u003e\n\u003cli\u003eProduction에 있는 클러스터는 Production 플리트에 속합니다.\u003c/li\u003e\n\u003cli\u003eStaging 및 Production에 있는 모든 클러스터는 Stable 릴리스 채널에 등록되어 있습니다.\u003c/li\u003e\n\u003cli\u003eProduction 플리트는 Staging 플리트가 7일간 실행된 후에만 업그레이드됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWorkload Identity Federation\u003c/h2\u003e\n\u003cp\u003e워크로드 ID 페더레이션은 Kubernetes 서비스 계정이 IAM 서비스 계정으로 작동할 수 있게 합니다. 구성된 Kubernetes 서비스 계정을 사용하는 파드는 Google Cloud API에 액세스할 때 자동으로 IAM 서비스 계정으로 인증합니다. 이를 통해 GKE 애플리케이션 워크로드가 인증되고 허가되어 Google Cloud 서비스에 액세스할 수 있도록 보장할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e오토파일럿 클러스터는 GKE에서 기본적으로 워크로드 ID 연합을 지원합니다.\u003c/p\u003e\n\u003ch2\u003e오토스케일링 전략\u003c/h2\u003e\n\u003cp\u003e오토스케일링은 클러스터가 실행 중인 응용 프로그램의 수요를 충족하기 위해 조정할 수 있는 능력을 가리킵니다.\u003c/p\u003e\n\u003cp\u003eGKE에는 네 가지 스케일링 차원이 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e워크로드는 수요에 따라 가로 방향으로 확장됩니다. Pod를 추가하거나 제거함으로써 관리됩니다. 이는 가로 방향 팟 오토스케일러(HPA)에 의해 관리되며, 수요에 따라 빠르게 확장됩니다. HPA는 CPU 사용률과 같은 표준 메트릭 또는 초당 요청과 같은 사용자 정의 메트릭에 응답합니다.\u003c/li\u003e\n\u003cli\u003e인프라는 클러스터 노드를 추가하거나 제거함으로써 가로 방향으로 확장됩니다. 이는 예약된 팟을 수용하기 위해 클러스터 자동스케일러(CA)에 의해 예측적으로 관리됩니다. 예를 들어, 새로 생성된 팟을 예약할 노드가 없는 경우, 클러스터 자동스케일러가 새 노드를 만듭니다.\u003c/li\u003e\n\u003cli\u003e워크로드는 팟 크기를 조절함으로써 세로 방향으로 확장됩니다. 이는 세로 방향 팟 오토스케일러(VPA)에 의해 관리됩니다. VPA는 시간이 경과함에 따라 팟의 CPU 및 메모리 사용률을 모니터링하고 이에 따라 팟 크기를 조정합니다. 이로써 보다 최적화되고 비용 효율적인 팟 크기를 얻을 수 있습니다.\u003c/li\u003e\n\u003cli\u003e인프라는 가장 효율적인 팟의 바이너리 패킹을 달성하기 위해 최적화된 노드(VM) 사이즈로 노드 풀을 배포하거나 삭제함으로써 세로 방향으로 확장됩니다. 이를 \"노드 자동 프로비저닝\"이라고 하며, 다른 종류의 확장에 비해 상대적으로 느립니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e아래는 몇 가지 팁입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGKE Autopilot을 사용할 때는 인프라 자동스케일링 (CA 및 NAP)이 Google에 의해 설정됩니다. 당신은 팟 오토스케일링 구성만 고려하면 됩니다.\u003c/li\u003e\n\u003cli\u003eHPA와 VPA를 동시에 사용하지 마세요. 같은 자원 메트릭으로 HPA와 VPA를 함께 설정하는 것을 피하세요. 예를 들어, CPU 사용률에 대한 HPA와 VPA를 동시에 설정하는 것을 피하세요.\u003c/li\u003e\n\u003cli\u003e가로와 세로 방향 팟 오토스케일링을 관리하는 다차원 팟 오토스케일러(MPA)를 사용하면 워크로드 확장을 간단히 할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e결론적으로:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGKE 표준 버전을 사용하면 인프라 스케일링과 워크로드(파드) 스케일링을 모두 관리해야 합니다.\u003c/li\u003e\n\u003cli\u003eGKE Autopilot을 사용하면 워크로드 스케일링에만 집중하면 되며, MPA를 사용하여 워크로드 자동 스케일링을 간편하게 할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e인프라 및 워크로드 배포\u003c/h2\u003e\n\u003cp\u003eGoogle은 클러스터 자체를 배포할 때 클라우드 인프라의 경우와 마찬가지로 인프라를 코드로 관리하는 것을 권장합니다(Terraform과 같은). 따라서 클러스터 및 네임스페이스를 배포할 때 IaC를 사용하세요.\u003c/p\u003e\n\u003cp\u003e귀하의 작업 로드를 배포하려면 — 배포, 서비스, 작업, StatefufSets, 인그레스, 정책 등 — 선언적 yaml 파일을 사용하여 네이티브 Kubernetes API 호출을 해야 합니다. Helm은 쿠버네티스에서 애플리케이션 배포를 관리하는 데 도움을 줄 수 있습니다.\u003c/p\u003e\n\u003ch2\u003eGKE 디자인 결정 요약\u003c/h2\u003e\n\u003cp\u003e요약하자면: 고려해야 할 주요 Kubernetes 디자인 결정 사항과 각각에 대한 나의 권장 사항입니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGKE, 또는 스스로 관리하는 방식? 권장 사항: GKE.\u003c/li\u003e\n\u003cli\u003eGKE Autopilot 대 GKE Standard. 권장 사항: Autopilot.\u003c/li\u003e\n\u003cli\u003e멀티 테넌트 클러스터? 어떤 수준의 클러스터? 권장 사항: 멀티 테넌트.\u003c/li\u003e\n\u003cli\u003e싱글 테넌트 클러스터를 만들 수 있는 능력. 권장 사항: 예외적으로만.\u003c/li\u003e\n\u003cli\u003eVPC 네이티브 또는 라우트 기반 클러스터? 권장 사항: VPC 네이티브.\u003c/li\u003e\n\u003cli\u003e프라이빗 클러스터? 권장 사항: 프라이빗 클러스터를 사용하세요.\u003c/li\u003e\n\u003cli\u003e클러스터 간 IP 주소 범위 공유? 추천 사항: 공유 VPC 내에 몇 개 또는 많은 클러스터가 있다면 이를 수행해야 합니다.\u003c/li\u003e\n\u003cli\u003e릴리스 채널? 항상 릴리스 채널에 등록하세요. 스테이징 및 프로드 클러스터를 동일한 릴리스 채널에 유지하세요. 클러스터가 올바른 순서로 업그레이드되도록 보장하기 위해 fleets 및 rollout sequences를 사용하세요.\u003c/li\u003e\n\u003cli\u003eWorkload identity? 네 — 사용하세요.\u003c/li\u003e\n\u003cli\u003e오토스케일링 전략? 워크로드 오토스케일링을 지원하기 위해 클러스터 오토스케일러를 사용하세요. GKE Autopilot을 사용할 때 클러스터 오토스케일링은 자동으로 관리됩니다. 워크로드 오토스케일링을 간소화하기 위해 MPA를 사용하세요.\u003c/li\u003e\n\u003cli\u003e인프라 및 워크로드 배포? 클러스터 배포 및 관리에는 IaC(예: Terraform)를 사용하세요. 애플리케이션 워크로드를 배포하기 위해 선언적 Kubernetes 매니페스트를 사용하세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e마무리\u003c/h1\u003e\n\u003cp\u003eGKE를 랜딩 존에서 성공적으로 디자인하는 데 고려해야 할 주요 사항들은 여기까지입니다. 다음 파트에서는 LZ 디자인 고려 사항을 완료하고, 로깅 및 모니터링 전략, 요금 청구, 인프라스트럭처 코드 (IaC)와 같은 주제를 다룰 것입니다.\u003c/p\u003e\n\u003ch1\u003e떠나시기 전에\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e관심이 있을 것으로 생각되는 분과 공유해주세요. 그들에게 도움이 될 수도 있고, 저에게 정말로 도움이 됩니다!\u003c/li\u003e\n\u003cli\u003e박수를 부탁드립니다! 여러분은 한 번 이상 박수를 칠 수 있다는 걸 아시나요?\u003c/li\u003e\n\u003cli\u003e자유롭게 댓글을 남겨주세요 💬.\u003c/li\u003e\n\u003cli\u003e내 컨텐츠를 놓치지 않으려면 팔로우하고 구독해주세요. 내 프로필 페이지로 이동하여 이 아이콘들을 클릭해주세요:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries_9.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003ch1\u003e링크\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Cloud의 랜딩 존: 필요성 및 생성 방법\u003c/li\u003e\n\u003cli\u003eGoogle Cloud의 랜딩 존 디자인\u003c/li\u003e\n\u003cli\u003eGKE Autopilot\u003c/li\u003e\n\u003cli\u003eGKE Autopilot 특권 업무 용 업무\u003c/li\u003e\n\u003cli\u003eGKE 클러스터 구성 옵션\u003c/li\u003e\n\u003cli\u003e기업용 GKE 멀티 테넌시에 대한 모베스트 프랙티스\u003c/li\u003e\n\u003cli\u003eVPC 네이티브 GKE 클러스터\u003c/li\u003e\n\u003cli\u003eGKE의 프라이빗 클러스터\u003c/li\u003e\n\u003cli\u003e외부 애플리케이션 로드 밸런서를 위한 GKE Ingress\u003c/li\u003e\n\u003cli\u003eGKE로 이주할 때 IP 주소 계획\u003c/li\u003e\n\u003cli\u003eGKE 네트워크 플래닝 2023 (William Denniss)\u003c/li\u003e\n\u003cli\u003eGKE 릴리스 채널\u003c/li\u003e\n\u003cli\u003e롤아웃 순서로 GKE 클러스터 업그레이드\u003c/li\u003e\n\u003cli\u003eGKE SLA\u003c/li\u003e\n\u003cli\u003eGKE 자동 확장 (Kaslin Fields)\u003c/li\u003e\n\u003cli\u003eGKE에서 비용 최적화된 애플리케이션에 대한 모베스트 프랙티스\u003c/li\u003e\n\u003cli\u003eGKE를 위한 워크로드 ID 연합\u003c/li\u003e\n\u003cli\u003eGoogle Cloud 아키텍처 프레임워크\u003c/li\u003e\n\u003cli\u003e기업용 기초 설계 청사진\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e시리즈 내비게이션\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e시리즈 개요 및 구조\u003c/li\u003e\n\u003cli\u003e이전: 랜딩 존 설계하기 — 디자인 고려사항 파트 1\u003c/li\u003e\n\u003cli\u003e다음: 랜딩 존 설계하기 — 디자인 고려사항 파트 3 — 모니터링, 로깅, 빌링 및 라벨링\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-27-DesignyourLandingZoneDesignConsiderationsPart2KubernetesandGKEGoogleCloudAdoptionSeries"},"buildId":"RZIEBQ2aNAp_DXFVTV6eL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>