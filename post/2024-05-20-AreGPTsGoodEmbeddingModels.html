<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>GPTs가 좋은 임베딩 모델인가요 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-20-AreGPTsGoodEmbeddingModels" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="GPTs가 좋은 임베딩 모델인가요 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="GPTs가 좋은 임베딩 모델인가요 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-20-AreGPTsGoodEmbeddingModels" data-gatsby-head="true"/><meta name="twitter:title" content="GPTs가 좋은 임베딩 모델인가요 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-20 20:30" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_buildManifest.js" defer=""></script><script src="/_next/static/ll1cGyplNwh83dpggeai1/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">GPTs가 좋은 임베딩 모델인가요</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="GPTs가 좋은 임베딩 모델인가요" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 20, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-20-AreGPTsGoodEmbeddingModels&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><h2>세부 사항에 귀신이 있는 놀라운 실험</h2>
<p><img src="/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png" alt="이미지"/></p>
<p>많은 임베딩 모델이 제공되고 있으므로, 기계 학습 응용 프로그램에 적합한 모델을 선택하는 것은 어려울 수 있습니다. 다행히 MTEB 리더보드는 다양한 자연어 처리 작업에 대한 포괄적인 랭킹 지표를 제공합니다.</p>
<p><img src="/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_1.png" alt="이미지"/></p>
<div class="content-ad"></div>
<p>사이트를 방문하면 상위 다섯 임베딩 모델이 Generative Pre-trained Transformers (GPTs)임을 알 수 있습니다. 이것이 GPT 모델이 임베딩에 가장 적합하다고 생각하게 할 수도 있습니다. 그러나 이것이 정말 사실인지 알아보기 위해 실험을 진행해봅시다.</p>
<h1>GPT 임베딩</h1>
<p>임베딩은 문장의 텐서 표현으로, 텍스트 토큰 ID를 변환하여 텐서 공간으로 투영하는 것입니다.</p>
<p>텍스트를 신경망 모델에 입력하고 순전파를 수행하면 임베딩 벡터를 얻을 수 있습니다. 그러나 실제 과정은 조금 더 복잡합니다. 한 단계씩 자세하게 알아봅시다:</p>
<div class="content-ad"></div>
<ul>
<li>텍스트를 토큰 ID로 변환합니다.</li>
<li>토큰 ID를 신경망에 전달합니다.</li>
<li>신경망의 출력값을 반환합니다.</li>
</ul>
<p>첫 번째 단계에서는 이를 달성하기 위해 토크나이저를 사용할 것입니다. model_inputs는 &quot;일부 질문&quot; 텍스트 내용의 텐서 표현입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoTokenizer</span>

tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span>)

messages = [
        {
            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
            <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;일부 질문.&quot;</span>,
        },
]

encodeds = tokenizer.<span class="hljs-title function_">apply_chat_template</span>(messages, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
model_inputs = encodeds.<span class="hljs-title function_">to</span>(<span class="hljs-string">&quot;cuda&quot;</span>)
</code></pre>
<p>두 번째 단계는 간단합니다. model_inputs를 신경망에 순전파합니다. 생성된 토큰의 로짓에는 .logits를 통해 액세스할 수 있습니다. torch.no_grad()는 모델 가중치를 업데이트하고 싶지 않기 때문에 모델이 추론 모드에 있음을 의미합니다.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">with</span> torch.<span class="hljs-title function_">no_grad</span>():
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">model</span>(model_inputs).<span class="hljs-property">logits</span>
</code></pre>
<p>세 번째 단계는 조금 까다롭습니다. GPT 모델은 디코더 전용이며 토큰 생성이 자기 회귀적입니다. 간단히 말해, 완료된 문장의 마지막 토큰은 문장 내의 모든 이전 토큰을 본 적이 있습니다. 따라서 마지막 토큰의 출력에는 이전 토큰들로부터의 모든 친화도 점수(어텐션)가 포함되어 있습니다.</p>
<p>Hugging Face에서 구현된 GPT의 출력 차원은 (배치 크기, 입력 토큰 크기, 어휘 크기)입니다. 모든 배치의 마지막 토큰 출력을 얻으려면 텐서 슬라이스를 수행할 수 있습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">with</span> torch.<span class="hljs-title function_">no_grad</span>():
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">model</span>(model_inputs).<span class="hljs-property">logits</span>[:, -<span class="hljs-number">1</span>, :]
</code></pre>
<div class="content-ad"></div>
<h1>이 GPT 임베딩의 품질</h1>
<p>이 GPT 임베딩의 품질을 측정하려면 코사인 유사도를 사용할 수 있어요. 코사인 유사도가 높을수록 문장의 의미가 더 가깝다는 뜻이에요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> torch
def <span class="hljs-title function_">compute_cosine_similarity</span>(vec1, vec2):
    cos = torch.<span class="hljs-property">nn</span>.<span class="hljs-title class_">CosineSimilarity</span>(dim=<span class="hljs-number">1</span>, eps=<span class="hljs-number">1e-6</span>)
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">cos</span>(vec1, vec2)
</code></pre>
<p>우리가 질문과 답변 쌍 목록을 순회하고 결과를 확인하는 유틸리티 함수를 만들어봐요. 이 실험에는 오픈소스로 공개된 위대한 모델 중 하나인 Mistral 7b v0.1이 사용돼요.</p>
<div class="content-ad"></div>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> termcolor <span class="hljs-keyword">import</span> colored
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span>
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span>)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_last_token_embeddings</span>(<span class="hljs-params">question</span>):
    messages = [
        {
            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,
            <span class="hljs-string">&quot;content&quot;</span>: question,
        },
    ]
    encodeds = tokenizer.apply_chat_template(messages, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
    model_inputs = encodeds.to(<span class="hljs-string">&quot;cuda&quot;</span>)
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">return</span> model(model_inputs).logits[:, -<span class="hljs-number">1</span>, :]

<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_similarities</span>(<span class="hljs-params">questions, answers</span>):
    <span class="hljs-keyword">for</span> question <span class="hljs-keyword">in</span> questions:
        <span class="hljs-keyword">for</span> answer <span class="hljs-keyword">in</span> answers:
            q_embedding, a_embedding = (
                generate_last_token_embeddings(question),
                generate_last_token_embeddings(answer),
            )
            similarity = compute_cosine_similarity(q_embedding, a_embedding)
            <span class="hljs-built_in">print</span>(colored(<span class="hljs-string">f&quot;question: <span class="hljs-subst">{question}</span> and ans: <span class="hljs-subst">{answer}</span>&quot;</span>, <span class="hljs-string">&quot;green&quot;</span>))
            <span class="hljs-built_in">print</span>(colored(<span class="hljs-string">f&quot;result: <span class="hljs-subst">{similarity}</span>&quot;</span>, <span class="hljs-string">&quot;blue&quot;</span>))

questions = [<span class="hljs-string">&quot;Where is the headquarter of OpenAI?&quot;</span>, <span class="hljs-string">&quot;What is GPU?&quot;</span>]
answers = [
    <span class="hljs-string">&quot;OpenAI is based at San Francisco.&quot;</span>,
    <span class="hljs-string">&quot;A graphics processing unit (GPU) is an electronic circuit that can perform mathematical calculations quickly&quot;</span>,
]
get_similarities(questions, answers)
</code></pre>
<p><img src="/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_2.png" alt="image"/></p>
<h1>결과 및 관찰</h1>
<p>첫 번째 질문과 대답 쌍에 대한 결과:```</p>
<div class="content-ad"></div>
<ul>
<li>질문: &quot;OpenAI의 본사는 어디에 있나요?&quot;</li>
<li>답변: &quot;OpenAI는 샌프란시스코에 본부를 두고 있습니다.&quot;</li>
<li>코사인 유사도: 0.96</li>
</ul>
<p>두 번째 질문과 대답 쌍에 대해:</p>
<ul>
<li>질문: &quot;GPU란 무엇인가요?&quot;</li>
<li>답변: &quot;그래픽 처리 장치 (GPU)는 빠르게 수학적 계산을 수행할 수 있는 전자 회로입니다.&quot;</li>
<li>코사인 유사도: 0.94</li>
</ul>
<p>관련 없는 쌍에 대해:</p>
<div class="content-ad"></div>
<ul>
<li>질문: “OpenAI의 본사는 어디에 있습니까?”</li>
<li>대답: “그래픽 처리 장치(GPU)는 수학적 계산을 빠르게 수행할 수 있는 전자 회로입니다.”</li>
<li>코사인 유사도: 0.90</li>
</ul>
<p>최악의 쌍의 경우:</p>
<ul>
<li>질문: “GPU가 무엇인가요?”</li>
<li>대답: “OpenAI는 샌프란시스코에 기반을 두고 있습니다.”</li>
<li>코사인 유사도: 0.93</li>
</ul>
<p>이러한 결과는 GPT 모델을 임베딩 모델로 사용하면 관련 및 관련 없는 쌍을 구별하는 면에서 큰 결과를 얻을 수 없을 수 있다는 것을 나타냅니다. 그러나 왜 GPT 모델은 여전히 상위 5위 내에 있습니까?</p>
<div class="content-ad"></div>
<h1>대조 손실이 구조에 도움이 됩니다</h1>
<pre><code class="hljs language-js">tokenizer = <span class="hljs-title class_">AutoTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">&quot;intfloat/e5-mistral-7b-instruct&quot;</span>)
model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(
  <span class="hljs-string">&quot;intfloat/e5-mistral-7b-instruct&quot;</span>
)
</code></pre>
<p>다른 모델 e5-mistral-7b-instruct을 사용하여 동일한 평가 절차를 반복했더니, 이 모델은 MTEB leaderboard의 최상위 오픈소스 모델 중 하나로, mistral 7b instruct로부터 미세 조정되었습니다. 이 모델을 사용한 결과, 관련 질문과 쌍의 코사인 유사도는 각각 오픈AI와 GPU 질문에 대해 0.88 및 0.84입니다. 관련없는 질문과 답변 쌍에 대한 유사도는 0.56 및 0.67로 감소합니다. 이 결과는 e5-mistral-7b-instruct이 임베딩에 대해 훨씬 향상된 모델이라는 것을 시사합니다. 이런 개선이 된 이유는 무엇일까요?</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_4.png" alt="Embedding Model"/></p>
<p>해당 e5-mistral-7b-instruct 논문을 살펴보면, 핵심은 contrastive loss를 사용하여 mistral 모델을 추가 조정하는 데 있습니다.</p>
<p>이 블로그 게시물에서는 이 개념을 자세히 다루었습니다. sim 함수는 두 벡터 간의 코사인 거리를 계산합니다. 대조 손실에서 분모는 양성 예와 음성 예 사이의 코사인 거리를 나타냅니다. 대조 손실의 이유는 비슷한 벡터가 가능한 한 1에 가까워지도록 하고 싶기 때문입니다. 왜냐하면 log(1) = 0이 최적의 손실을 나타내기 때문입니다.</p>
<h1>결론</h1>
<div class="content-ad"></div>
<p>이 게시물에서는 GPT를 임베딩 모델로 사용할 때 일반적인 함정을 강조했습니다. 내가 한 평가는 GPT를 대조 손실로 미세 조정할 때 임베딩이 더 의미 있고 차별적일 수 있다는 것을 제안합니다. GPT 모델의 강점과 한계를 이해하고 대조 손실과 같은 사용자 지정 손실을 활용함으로써, 머신러닝 프로젝트에 임베딩 모델을 선택하고 활용할 때 보다 정보를 얻을 수 있습니다. 이 게시물이 여러분이 응용 프로그램에 현명하게 GPT 모델을 선택하는 데 도움이 되기를 바라며 피드백을 기다리겠습니다! :)</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"GPTs가 좋은 임베딩 모델인가요","description":"","date":"2024-05-20 20:30","slug":"2024-05-20-AreGPTsGoodEmbeddingModels","content":"\n\n## 세부 사항에 귀신이 있는 놀라운 실험\n\n![이미지](/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png)\n\n많은 임베딩 모델이 제공되고 있으므로, 기계 학습 응용 프로그램에 적합한 모델을 선택하는 것은 어려울 수 있습니다. 다행히 MTEB 리더보드는 다양한 자연어 처리 작업에 대한 포괄적인 랭킹 지표를 제공합니다.\n\n![이미지](/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사이트를 방문하면 상위 다섯 임베딩 모델이 Generative Pre-trained Transformers (GPTs)임을 알 수 있습니다. 이것이 GPT 모델이 임베딩에 가장 적합하다고 생각하게 할 수도 있습니다. 그러나 이것이 정말 사실인지 알아보기 위해 실험을 진행해봅시다.\n\n# GPT 임베딩\n\n임베딩은 문장의 텐서 표현으로, 텍스트 토큰 ID를 변환하여 텐서 공간으로 투영하는 것입니다.\n\n텍스트를 신경망 모델에 입력하고 순전파를 수행하면 임베딩 벡터를 얻을 수 있습니다. 그러나 실제 과정은 조금 더 복잡합니다. 한 단계씩 자세하게 알아봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 텍스트를 토큰 ID로 변환합니다.\n- 토큰 ID를 신경망에 전달합니다.\n- 신경망의 출력값을 반환합니다.\n\n첫 번째 단계에서는 이를 달성하기 위해 토크나이저를 사용할 것입니다. model_inputs는 \"일부 질문\" 텍스트 내용의 텐서 표현입니다.\n\n```js\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"일부 질문.\",\n        },\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\nmodel_inputs = encodeds.to(\"cuda\")\n```\n\n두 번째 단계는 간단합니다. model_inputs를 신경망에 순전파합니다. 생성된 토큰의 로짓에는 .logits를 통해 액세스할 수 있습니다. torch.no_grad()는 모델 가중치를 업데이트하고 싶지 않기 때문에 모델이 추론 모드에 있음을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport torch\n\nwith torch.no_grad():\n    return model(model_inputs).logits\n```\n\n세 번째 단계는 조금 까다롭습니다. GPT 모델은 디코더 전용이며 토큰 생성이 자기 회귀적입니다. 간단히 말해, 완료된 문장의 마지막 토큰은 문장 내의 모든 이전 토큰을 본 적이 있습니다. 따라서 마지막 토큰의 출력에는 이전 토큰들로부터의 모든 친화도 점수(어텐션)가 포함되어 있습니다.\n\nHugging Face에서 구현된 GPT의 출력 차원은 (배치 크기, 입력 토큰 크기, 어휘 크기)입니다. 모든 배치의 마지막 토큰 출력을 얻으려면 텐서 슬라이스를 수행할 수 있습니다.\n\n```js\nimport torch\nwith torch.no_grad():\n    return model(model_inputs).logits[:, -1, :]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 이 GPT 임베딩의 품질\n\n이 GPT 임베딩의 품질을 측정하려면 코사인 유사도를 사용할 수 있어요. 코사인 유사도가 높을수록 문장의 의미가 더 가깝다는 뜻이에요.\n\n```js\nimport torch\ndef compute_cosine_similarity(vec1, vec2):\n    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n    return cos(vec1, vec2)\n```\n\n우리가 질문과 답변 쌍 목록을 순회하고 결과를 확인하는 유틸리티 함수를 만들어봐요. 이 실험에는 오픈소스로 공개된 위대한 모델 중 하나인 Mistral 7b v0.1이 사용돼요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport torch\nfrom termcolor import colored\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.1\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\ndef generate_last_token_embeddings(question):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": question,\n        },\n    ]\n    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n    model_inputs = encodeds.to(\"cuda\")\n    with torch.no_grad():\n        return model(model_inputs).logits[:, -1, :]\n\ndef get_similarities(questions, answers):\n    for question in questions:\n        for answer in answers:\n            q_embedding, a_embedding = (\n                generate_last_token_embeddings(question),\n                generate_last_token_embeddings(answer),\n            )\n            similarity = compute_cosine_similarity(q_embedding, a_embedding)\n            print(colored(f\"question: {question} and ans: {answer}\", \"green\"))\n            print(colored(f\"result: {similarity}\", \"blue\"))\n\nquestions = [\"Where is the headquarter of OpenAI?\", \"What is GPU?\"]\nanswers = [\n    \"OpenAI is based at San Francisco.\",\n    \"A graphics processing unit (GPU) is an electronic circuit that can perform mathematical calculations quickly\",\n]\nget_similarities(questions, answers)\n```\n\n![image](/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_2.png)\n\n# 결과 및 관찰\n\n첫 번째 질문과 대답 쌍에 대한 결과:```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 질문: \"OpenAI의 본사는 어디에 있나요?\"\n- 답변: \"OpenAI는 샌프란시스코에 본부를 두고 있습니다.\"\n- 코사인 유사도: 0.96\n\n두 번째 질문과 대답 쌍에 대해:\n\n- 질문: \"GPU란 무엇인가요?\"\n- 답변: \"그래픽 처리 장치 (GPU)는 빠르게 수학적 계산을 수행할 수 있는 전자 회로입니다.\"\n- 코사인 유사도: 0.94\n\n관련 없는 쌍에 대해:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 질문: “OpenAI의 본사는 어디에 있습니까?”\n- 대답: “그래픽 처리 장치(GPU)는 수학적 계산을 빠르게 수행할 수 있는 전자 회로입니다.”\n- 코사인 유사도: 0.90\n\n최악의 쌍의 경우:\n\n- 질문: “GPU가 무엇인가요?”\n- 대답: “OpenAI는 샌프란시스코에 기반을 두고 있습니다.”\n- 코사인 유사도: 0.93\n\n이러한 결과는 GPT 모델을 임베딩 모델로 사용하면 관련 및 관련 없는 쌍을 구별하는 면에서 큰 결과를 얻을 수 없을 수 있다는 것을 나타냅니다. 그러나 왜 GPT 모델은 여전히 상위 5위 내에 있습니까?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 대조 손실이 구조에 도움이 됩니다\n\n```js\ntokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-mistral-7b-instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"intfloat/e5-mistral-7b-instruct\"\n)\n```\n\n\n다른 모델 e5-mistral-7b-instruct을 사용하여 동일한 평가 절차를 반복했더니, 이 모델은 MTEB leaderboard의 최상위 오픈소스 모델 중 하나로, mistral 7b instruct로부터 미세 조정되었습니다. 이 모델을 사용한 결과, 관련 질문과 쌍의 코사인 유사도는 각각 오픈AI와 GPU 질문에 대해 0.88 및 0.84입니다. 관련없는 질문과 답변 쌍에 대한 유사도는 0.56 및 0.67로 감소합니다. 이 결과는 e5-mistral-7b-instruct이 임베딩에 대해 훨씬 향상된 모델이라는 것을 시사합니다. 이런 개선이 된 이유는 무엇일까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Embedding Model](/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_4.png)\n\n해당 e5-mistral-7b-instruct 논문을 살펴보면, 핵심은 contrastive loss를 사용하여 mistral 모델을 추가 조정하는 데 있습니다.\n\n이 블로그 게시물에서는 이 개념을 자세히 다루었습니다. sim 함수는 두 벡터 간의 코사인 거리를 계산합니다. 대조 손실에서 분모는 양성 예와 음성 예 사이의 코사인 거리를 나타냅니다. 대조 손실의 이유는 비슷한 벡터가 가능한 한 1에 가까워지도록 하고 싶기 때문입니다. 왜냐하면 log(1) = 0이 최적의 손실을 나타내기 때문입니다.\n\n# 결론\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물에서는 GPT를 임베딩 모델로 사용할 때 일반적인 함정을 강조했습니다. 내가 한 평가는 GPT를 대조 손실로 미세 조정할 때 임베딩이 더 의미 있고 차별적일 수 있다는 것을 제안합니다. GPT 모델의 강점과 한계를 이해하고 대조 손실과 같은 사용자 지정 손실을 활용함으로써, 머신러닝 프로젝트에 임베딩 모델을 선택하고 활용할 때 보다 정보를 얻을 수 있습니다. 이 게시물이 여러분이 응용 프로그램에 현명하게 GPT 모델을 선택하는 데 도움이 되기를 바라며 피드백을 기다리겠습니다! :)","ogImage":{"url":"/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png"},"coverImage":"/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png","tag":["Tech"],"readingTime":6},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h2: \"h2\",\n    p: \"p\",\n    img: \"img\",\n    h1: \"h1\",\n    ul: \"ul\",\n    li: \"li\",\n    pre: \"pre\",\n    code: \"code\",\n    span: \"span\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h2, {\n      children: \"세부 사항에 귀신이 있는 놀라운 실험\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_0.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"많은 임베딩 모델이 제공되고 있으므로, 기계 학습 응용 프로그램에 적합한 모델을 선택하는 것은 어려울 수 있습니다. 다행히 MTEB 리더보드는 다양한 자연어 처리 작업에 대한 포괄적인 랭킹 지표를 제공합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_1.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"사이트를 방문하면 상위 다섯 임베딩 모델이 Generative Pre-trained Transformers (GPTs)임을 알 수 있습니다. 이것이 GPT 모델이 임베딩에 가장 적합하다고 생각하게 할 수도 있습니다. 그러나 이것이 정말 사실인지 알아보기 위해 실험을 진행해봅시다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"GPT 임베딩\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"임베딩은 문장의 텐서 표현으로, 텍스트 토큰 ID를 변환하여 텐서 공간으로 투영하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"텍스트를 신경망 모델에 입력하고 순전파를 수행하면 임베딩 벡터를 얻을 수 있습니다. 그러나 실제 과정은 조금 더 복잡합니다. 한 단계씩 자세하게 알아봅시다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"텍스트를 토큰 ID로 변환합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"토큰 ID를 신경망에 전달합니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"신경망의 출력값을 반환합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"첫 번째 단계에서는 이를 달성하기 위해 토크나이저를 사용할 것입니다. model_inputs는 \\\"일부 질문\\\" 텍스트 내용의 텐서 표현입니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" transformers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"AutoTokenizer\"\n        }), \"\\n\\ntokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"AutoTokenizer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"mistralai/Mistral-7B-Instruct-v0.1\\\"\"\n        }), \")\\n\\nmessages = [\\n        {\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"role\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"user\\\"\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"일부 질문.\\\"\"\n        }), \",\\n        },\\n]\\n\\nencodeds = tokenizer.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"apply_chat_template\"\n        }), \"(messages, return_tensors=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"pt\\\"\"\n        }), \")\\nmodel_inputs = encodeds.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"to\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"cuda\\\"\"\n        }), \")\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"두 번째 단계는 간단합니다. model_inputs를 신경망에 순전파합니다. 생성된 토큰의 로짓에는 .logits를 통해 액세스할 수 있습니다. torch.no_grad()는 모델 가중치를 업데이트하고 싶지 않기 때문에 모델이 추론 모드에 있음을 의미합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" torch\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"with\"\n        }), \" torch.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"no_grad\"\n        }), \"():\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"model\"\n        }), \"(model_inputs).\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"logits\"\n        }), \"\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"세 번째 단계는 조금 까다롭습니다. GPT 모델은 디코더 전용이며 토큰 생성이 자기 회귀적입니다. 간단히 말해, 완료된 문장의 마지막 토큰은 문장 내의 모든 이전 토큰을 본 적이 있습니다. 따라서 마지막 토큰의 출력에는 이전 토큰들로부터의 모든 친화도 점수(어텐션)가 포함되어 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Hugging Face에서 구현된 GPT의 출력 차원은 (배치 크기, 입력 토큰 크기, 어휘 크기)입니다. 모든 배치의 마지막 토큰 출력을 얻으려면 텐서 슬라이스를 수행할 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" torch\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"with\"\n        }), \" torch.\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"no_grad\"\n        }), \"():\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"model\"\n        }), \"(model_inputs).\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"logits\"\n        }), \"[:, -\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \", :]\\n\"]\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"이 GPT 임베딩의 품질\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 GPT 임베딩의 품질을 측정하려면 코사인 유사도를 사용할 수 있어요. 코사인 유사도가 높을수록 문장의 의미가 더 가깝다는 뜻이에요.\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" torch\\ndef \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"compute_cosine_similarity\"\n        }), \"(vec1, vec2):\\n    cos = torch.\", _jsx(_components.span, {\n          className: \"hljs-property\",\n          children: \"nn\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"CosineSimilarity\"\n        }), \"(dim=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \", eps=\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1e-6\"\n        }), \")\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"cos\"\n        }), \"(vec1, vec2)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리가 질문과 답변 쌍 목록을 순회하고 결과를 확인하는 유틸리티 함수를 만들어봐요. 이 실험에는 오픈소스로 공개된 위대한 모델 중 하나인 Mistral 7b v0.1이 사용돼요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" torch\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" termcolor \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" colored\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" transformers \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" AutoModelForCausalLM, AutoTokenizer\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"mistralai/Mistral-7B-Instruct-v0.1\\\"\"\n        }), \"\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"mistralai/Mistral-7B-Instruct-v0.1\\\"\"\n        }), \")\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"def\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"generate_last_token_embeddings\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-params\",\n          children: \"question\"\n        }), \"):\\n    messages = [\\n        {\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"role\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"user\\\"\"\n        }), \",\\n            \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \": question,\\n        },\\n    ]\\n    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"pt\\\"\"\n        }), \")\\n    model_inputs = encodeds.to(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"cuda\\\"\"\n        }), \")\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"with\"\n        }), \" torch.no_grad():\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"return\"\n        }), \" model(model_inputs).logits[:, -\", _jsx(_components.span, {\n          className: \"hljs-number\",\n          children: \"1\"\n        }), \", :]\\n\\n\", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"def\"\n        }), \" \", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"get_similarities\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-params\",\n          children: \"questions, answers\"\n        }), \"):\\n    \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" question \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" questions:\\n        \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"for\"\n        }), \" answer \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"in\"\n        }), \" answers:\\n            q_embedding, a_embedding = (\\n                generate_last_token_embeddings(question),\\n                generate_last_token_embeddings(answer),\\n            )\\n            similarity = compute_cosine_similarity(q_embedding, a_embedding)\\n            \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"print\"\n        }), \"(colored(\", _jsxs(_components.span, {\n          className: \"hljs-string\",\n          children: [\"f\\\"question: \", _jsx(_components.span, {\n            className: \"hljs-subst\",\n            children: \"{question}\"\n          }), \" and ans: \", _jsx(_components.span, {\n            className: \"hljs-subst\",\n            children: \"{answer}\"\n          }), \"\\\"\"]\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"green\\\"\"\n        }), \"))\\n            \", _jsx(_components.span, {\n          className: \"hljs-built_in\",\n          children: \"print\"\n        }), \"(colored(\", _jsxs(_components.span, {\n          className: \"hljs-string\",\n          children: [\"f\\\"result: \", _jsx(_components.span, {\n            className: \"hljs-subst\",\n            children: \"{similarity}\"\n          }), \"\\\"\"]\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"blue\\\"\"\n        }), \"))\\n\\nquestions = [\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Where is the headquarter of OpenAI?\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"What is GPU?\\\"\"\n        }), \"]\\nanswers = [\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"OpenAI is based at San Francisco.\\\"\"\n        }), \",\\n    \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"A graphics processing unit (GPU) is an electronic circuit that can perform mathematical calculations quickly\\\"\"\n        }), \",\\n]\\nget_similarities(questions, answers)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_2.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결과 및 관찰\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"첫 번째 질문과 대답 쌍에 대한 결과:```\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"질문: \\\"OpenAI의 본사는 어디에 있나요?\\\"\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"답변: \\\"OpenAI는 샌프란시스코에 본부를 두고 있습니다.\\\"\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"코사인 유사도: 0.96\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"두 번째 질문과 대답 쌍에 대해:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"질문: \\\"GPU란 무엇인가요?\\\"\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"답변: \\\"그래픽 처리 장치 (GPU)는 빠르게 수학적 계산을 수행할 수 있는 전자 회로입니다.\\\"\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"코사인 유사도: 0.94\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"관련 없는 쌍에 대해:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"질문: “OpenAI의 본사는 어디에 있습니까?”\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"대답: “그래픽 처리 장치(GPU)는 수학적 계산을 빠르게 수행할 수 있는 전자 회로입니다.”\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"코사인 유사도: 0.90\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"최악의 쌍의 경우:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"질문: “GPU가 무엇인가요?”\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"대답: “OpenAI는 샌프란시스코에 기반을 두고 있습니다.”\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"코사인 유사도: 0.93\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이러한 결과는 GPT 모델을 임베딩 모델로 사용하면 관련 및 관련 없는 쌍을 구별하는 면에서 큰 결과를 얻을 수 없을 수 있다는 것을 나타냅니다. 그러나 왜 GPT 모델은 여전히 상위 5위 내에 있습니까?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"대조 손실이 구조에 도움이 됩니다\"\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-js\",\n        children: [\"tokenizer = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"AutoTokenizer\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"intfloat/e5-mistral-7b-instruct\\\"\"\n        }), \")\\nmodel = \", _jsx(_components.span, {\n          className: \"hljs-title class_\",\n          children: \"AutoModelForCausalLM\"\n        }), \".\", _jsx(_components.span, {\n          className: \"hljs-title function_\",\n          children: \"from_pretrained\"\n        }), \"(\\n  \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"intfloat/e5-mistral-7b-instruct\\\"\"\n        }), \"\\n)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다른 모델 e5-mistral-7b-instruct을 사용하여 동일한 평가 절차를 반복했더니, 이 모델은 MTEB leaderboard의 최상위 오픈소스 모델 중 하나로, mistral 7b instruct로부터 미세 조정되었습니다. 이 모델을 사용한 결과, 관련 질문과 쌍의 코사인 유사도는 각각 오픈AI와 GPU 질문에 대해 0.88 및 0.84입니다. 관련없는 질문과 답변 쌍에 대한 유사도는 0.56 및 0.67로 감소합니다. 이 결과는 e5-mistral-7b-instruct이 임베딩에 대해 훨씬 향상된 모델이라는 것을 시사합니다. 이런 개선이 된 이유는 무엇일까요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-20-AreGPTsGoodEmbeddingModels_4.png\",\n        alt: \"Embedding Model\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"해당 e5-mistral-7b-instruct 논문을 살펴보면, 핵심은 contrastive loss를 사용하여 mistral 모델을 추가 조정하는 데 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 블로그 게시물에서는 이 개념을 자세히 다루었습니다. sim 함수는 두 벡터 간의 코사인 거리를 계산합니다. 대조 손실에서 분모는 양성 예와 음성 예 사이의 코사인 거리를 나타냅니다. 대조 손실의 이유는 비슷한 벡터가 가능한 한 1에 가까워지도록 하고 싶기 때문입니다. 왜냐하면 log(1) = 0이 최적의 손실을 나타내기 때문입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"결론\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 게시물에서는 GPT를 임베딩 모델로 사용할 때 일반적인 함정을 강조했습니다. 내가 한 평가는 GPT를 대조 손실로 미세 조정할 때 임베딩이 더 의미 있고 차별적일 수 있다는 것을 제안합니다. GPT 모델의 강점과 한계를 이해하고 대조 손실과 같은 사용자 지정 손실을 활용함으로써, 머신러닝 프로젝트에 임베딩 모델을 선택하고 활용할 때 보다 정보를 얻을 수 있습니다. 이 게시물이 여러분이 응용 프로그램에 현명하게 GPT 모델을 선택하는 데 도움이 되기를 바라며 피드백을 기다리겠습니다! :)\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-20-AreGPTsGoodEmbeddingModels"},"buildId":"ll1cGyplNwh83dpggeai1","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>