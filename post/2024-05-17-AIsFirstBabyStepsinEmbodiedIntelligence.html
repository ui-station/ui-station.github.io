<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>살아있는 인공지능의 첫 걸음, 바디 인텔리전스 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="살아있는 인공지능의 첫 걸음, 바디 인텔리전스 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="살아있는 인공지능의 첫 걸음, 바디 인텔리전스 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence" data-gatsby-head="true"/><meta name="twitter:title" content="살아있는 인공지능의 첫 걸음, 바디 인텔리전스 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-17 19:35" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/cd012fc8787133d0.css" as="style"/><link rel="stylesheet" href="/_next/static/css/cd012fc8787133d0.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/551-3069cf29fe274aab.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-561ae49ab5aab7f5.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_buildManifest.js" defer=""></script><script src="/_next/static/PgdIX9e0tvkvkdAmDT6qR/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">살아있는 인공지능의 첫 걸음, 바디 인텔리전스</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 17, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><p>두 개의 세계가 충돌하고 있어요.</p>
<p>우리는 방금 두 가지 분야에서 로봇 공학의 상당한 도약을 목격했어요. Figure AI의 말하는 로봇과 Google의 일반적인 에이전트 SIMA가 그겁니다.</p>
<p>하지만, 아니요, 이것은 일반적인 인공 지능(AGI)은 아니에요. 일부 사치스럽지만 입증되지 않은 주장이 돌아다니고 로봇들이 우리를 다 죽일 거라고 말하진 않을 거예요.</p>
<p>그러나 두 소식이 이미 자체로 흥미롭지만, 그들 간의 시너지는 내 관점에서 AI 구현 지능의 첫 번째 발걸음이라고 생각해요.</p>
<div class="content-ad"></div>
<h1>GPT-4의 구현</h1>
<p>그러면 Figure AI는 뭔가요?</p>
<p>Figure AI는 로봇 회사로, CEO의 말에 따르면 &quot;위험하고 원하지 않는 직업이 필요하지 않도록 하여 미래 세대가 더 행복하고 의미 있는 삶을 살도록 허용하는 로봇을 구축하고 있습니다.&quot;</p>
<p>이미 유사한 웅장한 사명을 가진 회사들을 들어보셨을 것입니다. 그러나 OpenAI, NVIDIA, Jeff Bezos, 그리고 Intel이 시장에 제품이 없는 회사에 대해 26억 달러 평가액에서 6억 7500만 달러 시리즈 투자를 한다면, 그들이 무엇인가를 찾고 있다는 것을 알 수 있습니다.</p>
<div class="content-ad"></div>
<p>그러나 왜 모두가 이 회사에 대해 이야기하는 걸까요? 이 비디오 때문입니다.</p>
<p>그 링크를 클릭하는 것을 강력히 권장합니다. 간단히 말하자면, 이 비디오를 통해 사람과 상호작용하는 로봇이 여러 작업을 민첩하게 수행한다는 것을 보여줍니다.</p>
<p>흥미로운 점은 Figure AI의 로봇의 핵심에는 GPT-4V, OpenAI의 주요 다중모달 대형 언어 모델인 MLLM이 있다는 것입니다.</p>
<p>다시 말해, Figure AI의 로봇은 &#x27;화신 ChatGPT&#x27;의 처음으로, 즉 LLMs가 실체화된 작업도 할 수 있는 능력이 생긴 것입니다.</p>
<div class="content-ad"></div>
<p>그러나 대부분의 사람들이 놓칠 수도 있는데, 로봇이 이전 동작에 대한 이유를 설명하면서 사람에게 작업을 수행하도록 요청하는 순간에 주목하여 주길 바랍니다.</p>
<p>이 질문이나 요청이 사소한 것은 아니라는 점을 분명히 이해해야 합니다. 모델이 동시에 여러 작업을 수행할 수 있는 능력을 보여주기 위해 일부러 그랬던 것입니다.</p>
<p>구체적으로 말하자면, 그들은 GPT-4를 세밀하게 조정하여 텍스트와 작업 표현을 출력하도록 만들었는데, 전자는 보코더를 통해 음성으로 디코딩되고, 후자는 액추에이터 동작으로 디코딩되어 몸을 움직이도록 합니다.</p>
<p>Figure AI의 로봇의 기본 메커니즘에 대해서는 많이 알지 못하지만, Deepmind의 RT-2 모델과 같은 예시를 통해, 우리는 이미 LLM을 로봇 동작이나 텍스트를 출력하도록 훈련하는 방법을 증명한 연구자들이 있기에 꽤 좋은 아이디어를 얻을 수 있습니다.</p>
<div class="content-ad"></div>
<p><img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png" alt="image"/></p>
<p>그러나 RT-2는 GPT-4의 능력에 가까운 두 LLM인 PALM-E와 PALI-X를 사용하여 훈련된 것을 고려할 때, Figure AI의 로봇 뒤에 있는 LLM이 이 모델이 지금까지 존재한 가장 고급 비전-언어-행동 모델일 수 있다는 주장이 가능합니다.</p>
<p>하지만 현실적으로 생각해 봅시다.</p>
<p>그것은 여러 차례 연습된 것일 수 있는 매우 제한된 데모였습니다. 사실, 로봇이 일반적인 용도의 능력 측면에서 아직 매우 초기 단계에 있다는 것은 거의 확실합니다.</p>
<div class="content-ad"></div>
<p>따라서, 이 로봇들의 여정에서 다음 단계를 상상하기 위해 구글이 방금 공개한 것에서 영감을 받을 수 있습니다.</p>
<p>SIMA, 3D 총론적 에이전트.</p>
<p>하지만 이번에는 Figure AI의 경우와 달리, 에이전트에 대해 훨씬 더 많은 정보를 갖고 있습니다.</p>
<h1>SIMA, 최초의 진정한 총론적 에이전트?</h1>
<div class="content-ad"></div>
<p>Google의 확장 가능한 인공 지능 멀티월드 에이전트(SIMA) 프로젝트는 어떤 3D 환경에서 임의의 언어 명령을 이해하고 실행할 수 있는 인공 지능(AI) 시스템을 만들고자 합니다.</p>
<p>이 계획은 언어를 지각 및 다양한 가상 세계에서 실제 행동과 결합하여 처리함으로써 일반적인 AI 개발에서 중요한 과제에 대처합니다. 이는 연구 환경 및 상업 비디오 게임을 포함한 다양한 가상 세계를 대상으로 합니다.</p>
<p>간단히 말하면, SIMA는 언어 요청을 받아들이고 이를 3D 환경에서 키보드 및 마우스 조작으로 변환합니다.</p>
<p>그러나 고려해야 할 중요한 요소는 다음과 같습니다:</p>
<div class="content-ad"></div>
<p>이 에이전트들의 목표는 &quot;자원을 수집하고 집을 짓는&quot; 등 사용자가 제시하는 자연어 지침대로 작업을 수행하는 것입니다.</p>
<p>간단히 말해, 모델은 해당 환경에서 상호작용할 때 사람들과 똑같은 입력을 갖고 있으므로 사람들보다 약점이 없다는 것을 의미하며, 이는 기접근 API나 이와 유사한 것에 대한 접근 권한이 없다는 것을 의미합니다.</p>
<p>결과적으로, 요구된 작업을 수행하는 유일한 방법은 해당 작업을 수행하는 데 사람이 수행할 키보드 및 마우스 작업을 예측하는 것입니다.</p>
<p>SIMA 에이전트는 다음 구성 요소로 이루어져 있습니다:</p>
<div class="content-ad"></div>
<p>아래는 마크다운 형식으로 변경한 텍스트입니다.</p>
<p><img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_1.png" alt="이미지"/></p>
<ul>
<li>
<p>인코더:</p>
<ul>
<li>텍스트 인코더: 언어 명령을 모델이 해석할 수 있는 임베딩으로 번역합니다.</li>
<li>SPARC 개발을 기반으로 한 이미지 인코더.</li>
<li>비디오 인코더.</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Multi-modal Transformer + Transformer XL: 두 개의 트랜스포머 아키텍처로, 전자는 모달 간 교차 어텐션을 수행하고, 후자는 이전 상태를 취해 새로운 상태를 식별합니다.</li>
</ol>
<div class="content-ad"></div>
<ol start="3">
<li>정책: 선택된 행동이 600가지 가능한 기술 중 하나로 결정되는 분류 헤드</li>
</ol>
<p>여기에는 해석할 내용이 많기 때문에 단계별로 진행해 봅시다.</p>
<h2>세계 처리</h2>
<p>대부분의 최신 모델들에서와 마찬가지로, 첫 번째 단계는 입력을 &quot;인코딩&quot;하는 것입니다.</p>
<div class="content-ad"></div>
<p>일반적인 용어로 설명하자면, 입력 데이터(텍스트와 비디오)를 가져와 해당 인코더를 사용하여 이러한 데이터 포인트를 벡터 임베딩으로 변환하는 것이 아이디어입니다.</p>
<p><img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_2.png" alt="image"/></p>
<p>하지만 왜 이 작업을 하는 것일까요?</p>
<p>이 변환을 수행함으로써 각 요소가 해당 개념의 의미론을 캡처하는 밀집 벡터로 표현됩니다.</p>
<div class="content-ad"></div>
<p>다시 말해, 의미론적으로 유사한 개념은 유사한 벡터를 갖게 되어 벡터 공간에서 표현되었을 때 더 가까이 위치하게 됩니다:</p>
<p><img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_3.png" alt="Concept Vectors"/></p>
<p>또 다른 중요한 이점은 &#x27;개념을 벡터로 변환함&#x27;으로, 인간이 아주 무의식적으로 수행하는 &#x27;우리 세계를 이해하는&#x27; 행위를 수학적인 연습(컴퓨터에 이상적)으로 변환하여, 개념이 이제 수치로 표현된다는 것입니다.</p>
<p>특히, 이 모델은 이러한 벡터들 사이의 유사성(그들 사이의 거리)을 계산하여, 어떤 개념이 다른 것들과 유사한지를 알아냅니다.</p>
<div class="content-ad"></div>
<p>예를 들어, 모델은 &#x27;개&#x27;와 &#x27;고양이&#x27;가 유사한 속성 (포유류, 길들여진 동물, 네 다리 등)을 공유하는 유사한 개념을 나타낸다는 것을 알 수 있습니다.</p>
<p>분명히 말씀드리자면, AI가 각 숫자를 다루는 방식은 아니며, AI가 중요시하는 것은 벡터 간의 근접성입니다. 만약에 도움이 될 경우, 벡터의 의미를 단순히 벡터 자체로만 생각하지 마십시오 (우리는 AI가 정말 &#x27;개&#x27;가 무엇인지 아는지 확신할 수 없습니다) 대신, 가장 가까운 이웃들의 합으로 생각하고, 모델에게 다른 것들과 유사하다는 신호를 보내는 것으로 여기면 됩니다 (강아지는 고양이와 유사하며 문과는 다릅니다).</p>
<p>비슷한 이유로 유사성은 데이터 유형이 다른 상황에 집중하는 데 중요한 역할을 합니다.</p>
<p>다양한 데이터 유형에서 나온 벡터로 오는 여러 모달 상황에 집중하는 데 유용합니다.</p>
<p>기본 아이디어는 &#x27;개&#x27;라는 단어와 &#x27;개의 이미지&#x27;가 유사한 벡터를 공유하도록 원한다는 것으로, 이것은 모델에게 두 가지가 동일한 개념을 나타낸다는 것을 알려줍니다.</p>
<div class="content-ad"></div>
<p>하지만 텍스트와 이미지는 구조적으로 매우 다른 데이터 유형이기 때문에 서로 다른 인코더가 필요합니다.</p>
<p>이것은 문제입니다. 비슷한 개념에 대해 유사한 임베딩을 생성하도록 보장하며 별도의 인코더를 훈련해야 합니다.</p>
<p>이 문제를 해결하기 위해 SIMA는 SPARC 이미지 인코더를 사용합니다.</p>
<p>매우 최근에 개발된 SPARC 인코더는 대부분의 다른 이미지 인코더와 매우 유사한 방식으로 훈련됩니다(대비 학습을 사용), 그러나 미세 구체적인 세부 사항을 더 잘 캡처할 수 있습니다.</p>
<div class="content-ad"></div>
<p>가장 흔한 이미지 인코더들은 이미지의 지역 세부사항을 제대로 포착하지 못하는 문제가 있습니다.</p>
<p>네, 그들은 이미지가 무엇에 대해인지를 알려줄 수 있지만, 이미지의 전역 의미를 설명하는 데 도움이 되지 않는 중요한 작은 세부사항을 놓치기도 합니다. 그럼에도 불구하고 많은 경우에 중요합니다.</p>
<p>SPARC는 유사한 방법을 제안하지만 매우 흥미로운 요소를 추가합니다.</p>
<p>예를 들어, &quot;바구니 속 고양이와 개&quot;를 나타내는 이미지-텍스트 쌍이 있다고 가정해 봅시다.</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_4.png"/>
<ul>
<li>
<p>먼저, SPARC는 이미지를 패치로 나눕니다.</p>
</li>
<li>
<p>그런 다음 각 패치에 대해 텍스트 설명 중 하나의 토큰을 할당합니다. 예를 들어, 패치가 개의 일부를 나타내면 &quot;개&quot;라는 단어가 할당됩니다.</p>
</li>
<li>
<p>이 작업은 모든 패치(왼쪽 색상 그리드의 수직 열)에 대해 수행되며, 텍스트 설명의 여러 측면을 다루는 패치의 경우 각각에 가중치가 할당됩니다.</p>
</li>
<li>
<p>특정 개념에 할당된 모든 패치를 가지고 있으면 각 패치가 &#x27;개&#x27;와 같은 특정 단어에 부여하는 가중치를 그룹화하여 이 그룹화된 임베딩을 실제 단어와 비교합니다. 그러나 여기에는 중요한 점이 있습니다. 이미지 전역에 대해 적용하는 대신, 모든 개념에 대해 이를 다섯 번 적용합니다.</p>
</li>
</ul>
<p>하지만 모든 이 작업을 하는 이유는 무엇일까요?</p>
<div class="content-ad"></div>
<p>지역성. 이미지의 특정 부분이 이제 특정 개념에 할당되었으므로 모델은 이제 두 가지를 알게 됩니다:</p>
<ul>
<li>이미지에서 어떤 개념이 더 많이 표현되는지</li>
<li>이러한 개념이 이미지에서 어디에 위치하는지.</li>
</ul>
<p>일반인이 이해하기 쉽게 설명하면 SPARC와 다른 이미지 인코더 간의 주요 차이점은 텍스트 설명의 개별 단어를 이미지의 특정 부분에 할당한다는 사실에 있습니다.</p>
<p>이러한 방식으로 이미지의 특정 부분의 그룹화된 임베딩이 &#x27;개&#x27; 단어 토큰에 크게 편향되어 있다면, 이미지의 해당 영역에는 아마도 개가 포함되어 있을 것입니다.</p>
<div class="content-ad"></div>
<p>SIMA의 중요한 요소입니다. 3D 에이전트는 요청된 작업의 일환으로 상호 작용할 특정 객체를 식별할 수 있어야합니다.</p>
<p>마지막으로, 비디오 인코더에 대해 이야기하면, 모델이 과거 상태를 고려할 수 있도록 포함되어 있습니다. 중요한 것은 비디오 인코더가 시간적 인식을 포함한다는 것인데, 이는 텍스트나 이미지 인코더가 제공할 수 없는 기능입니다.</p>
<p>환경의 현재 상태뿐만 아니라 과거의 환경 상태와 취해진 조치에 따라 다음 조치를 취할지 결정하기 때문에 이는 중요합니다.</p>
<h2>최적의 정책 선택</h2>
<div class="content-ad"></div>
<p>제공된 정보를 바탕으로 SIMA는 다른 인코더들에 의해 생성된 표현을 사용하여 변환 모델 세트를 활용합니다. 이 모델은 LLM이 단어를 예측하는 대신 작업을 출력하여 에이전트가 실행할 키보드 및 마우스 조작을 지시합니다.</p>
<p>한편, 왜 Gemini, 구글의 MLLM을 사용하는 대신 모델의 주요 &#x27;두뇌&#x27;로 이상한 Transformers 세트를 사용했는지 궁금할 수도 있습니다.</p>
<p>이유는 아마도 예산 때문인데, 연구자들 자신들이 기술 보고서에서 SIMA의 분명한 다음 단계는 Gemini를 사용하는 것이라고 인정했기 때문입니다.</p>
<p>이것은 매우 흥미로운데, 최고의 &#x27;두뇌&#x27;를 사용하지 않았음에도 놀라운 결과를 얻었다고 하니, 이제 우리가 곧 살펴볼 것입니다.</p>
<div class="content-ad"></div>
<h1>진정한 일반화자</h1>
<p>지금쯤이면 알 수 있겠지만, 목표는 여태까지 한 게임에서도 능숙한 에이전트를 훈련시키는 것이었습니다. 게임을 해본 적이 없는 게임조차도요.</p>
<p>훈련 후 SIMA 에이전트는 다양한 범주로 구분된 600가지 다양한 기본 작업을 수행할 수 있었습니다. 이러한 범주에는 네비게이션, 동물 상호작용, 음식 등이 포함되어 있었습니다:</p>
<p><img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_5.png" alt="image"/></p>
<div class="content-ad"></div>
<p>SIMA가이러한작업을수행하고있는모습을여기에서확인할수있습니다. 게다가, SIMA는매우유망한결과를얻어 언급할만한가치가있습니다.</p>
<p>우선, 다양한게임에서훈련을받았음에도불구하고, 평균적으로SIMA는 단일게임에서전문화된 에이전트들보다 더우수한성능을보였습니다... 그특정게임안에서:</p>
<p><img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_6.png" alt="image"/></p>
<p>더욱인상적인점은, 여러가지다른게임에서, 에이전트가영이의업무(non-trivial tasks)를달성했으며, 이중대부분은얼마들이나예시없이(zero-shot tasks)성과를거두었으며, 다시한번전문화된 에이전트들을이겼습니다:</p>
<div class="content-ad"></div>
<img src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_7.png"/>
<p>이는 과거에 본 적 없는 환경에 놓여도 모델이 일반적으로 잘 수행되었으며, 때로는 Goat Simulator 3와 같은 경우에는 전문화된 에이전트(해당 게임에서만 훈련된 에이전트)보다 뛰어난 성과를 보였습니다.</p>
<p>그렇다면 이 모든 것이 의미하는 바는 무엇인가요?</p>
<p>간단히 말하자면, 우리는 게임 간의 지식 전이의 구체적인 증거를 관찰하고 있다는 것입니다.</p>
<div class="content-ad"></div>
<p>다시 말해, 모델은 일부 게임에서 유용하게 적용할 수 있는 의미 있는 기술을 학습합니다 (예: 키보드로 이동하는 방법을 배우는 것과 같이).</p>
<p>더욱이, 이러한 기술들은 상당히 높은 품질을 갖고 있어서 이 일반화된 에이전트가 많은 게임에서 특화된 에이전트들을 이기는 것을 의미하며, 이는 일반화된 접근 방식이 그들이 다양한 환경에 적용할 우수한 기술을 학습하는 데 도움이 된다는 것을 시사합니다.</p>
<p>Figure AI의 발전과 함께 이러한 결과들이 어떠한 맥락에서 중요성을 얻을 수 있는 경우, 전체적으로 매우 인상적인 결과입니다.</p>
<h1>로보틱스에게 훌륭한 주였습니다</h1>
<div class="content-ad"></div>
<p>AI 로봇 기술은 요즘 정말 빠르게 발전하고 있어요.</p>
<ul>
<li>한 쪽에서, Figure AI는 우리가 점점 더 많은 수동 작업 범위를 알리는 인간형 로봇을 만들어내는 데 탁월한 실력을 보여줍니다.</li>
<li>반면에, SIMA는 3D 환경에서 최초의 일반 적 에이전트를 보여주고 있음을 암시합니다.</li>
</ul>
<p>하지만 우리가 깨닫는 것은 시너지의 잠재력입니다.</p>
<p>이러한 에이전트들을 실생활 상황으로 가져오는 데는 아직 이르지만, 이 두 분야 사이의 융합은 다음 단계로 자연스러운 발전을 이루고 있습니다; SIMA가 훈련의 장소일 뿐만 아니라, Figure AI 로봇들이 일반적인 에이전트의 구현체 역할을 하기 때문이죠.</p>
<div class="content-ad"></div>
<p>그리고 다른 기업들도 구체적 지능에 대한 자신들의 시사를 제시하며 경쟁이 치열해지고 있습니다. 많은 기존 기업들이 인류의 기술이 충분히 준비되어 있다고 느끼고 있어서 다음 큰 도전, 인공지능을 실생활에 적용하는 것에 착수할 준비가 되어있다고 느끼는 것이 분명합니다.</p></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"살아있는 인공지능의 첫 걸음, 바디 인텔리전스","description":"","date":"2024-05-17 19:35","slug":"2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence","content":"\n\n두 개의 세계가 충돌하고 있어요.\n\n우리는 방금 두 가지 분야에서 로봇 공학의 상당한 도약을 목격했어요. Figure AI의 말하는 로봇과 Google의 일반적인 에이전트 SIMA가 그겁니다.\n\n하지만, 아니요, 이것은 일반적인 인공 지능(AGI)은 아니에요. 일부 사치스럽지만 입증되지 않은 주장이 돌아다니고 로봇들이 우리를 다 죽일 거라고 말하진 않을 거예요.\n\n그러나 두 소식이 이미 자체로 흥미롭지만, 그들 간의 시너지는 내 관점에서 AI 구현 지능의 첫 번째 발걸음이라고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# GPT-4의 구현\n\n그러면 Figure AI는 뭔가요?\n\nFigure AI는 로봇 회사로, CEO의 말에 따르면 \"위험하고 원하지 않는 직업이 필요하지 않도록 하여 미래 세대가 더 행복하고 의미 있는 삶을 살도록 허용하는 로봇을 구축하고 있습니다.\"\n\n이미 유사한 웅장한 사명을 가진 회사들을 들어보셨을 것입니다. 그러나 OpenAI, NVIDIA, Jeff Bezos, 그리고 Intel이 시장에 제품이 없는 회사에 대해 26억 달러 평가액에서 6억 7500만 달러 시리즈 투자를 한다면, 그들이 무엇인가를 찾고 있다는 것을 알 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 왜 모두가 이 회사에 대해 이야기하는 걸까요? 이 비디오 때문입니다.\n\n그 링크를 클릭하는 것을 강력히 권장합니다. 간단히 말하자면, 이 비디오를 통해 사람과 상호작용하는 로봇이 여러 작업을 민첩하게 수행한다는 것을 보여줍니다.\n\n흥미로운 점은 Figure AI의 로봇의 핵심에는 GPT-4V, OpenAI의 주요 다중모달 대형 언어 모델인 MLLM이 있다는 것입니다.\n\n다시 말해, Figure AI의 로봇은 '화신 ChatGPT'의 처음으로, 즉 LLMs가 실체화된 작업도 할 수 있는 능력이 생긴 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 대부분의 사람들이 놓칠 수도 있는데, 로봇이 이전 동작에 대한 이유를 설명하면서 사람에게 작업을 수행하도록 요청하는 순간에 주목하여 주길 바랍니다.\n\n이 질문이나 요청이 사소한 것은 아니라는 점을 분명히 이해해야 합니다. 모델이 동시에 여러 작업을 수행할 수 있는 능력을 보여주기 위해 일부러 그랬던 것입니다.\n\n구체적으로 말하자면, 그들은 GPT-4를 세밀하게 조정하여 텍스트와 작업 표현을 출력하도록 만들었는데, 전자는 보코더를 통해 음성으로 디코딩되고, 후자는 액추에이터 동작으로 디코딩되어 몸을 움직이도록 합니다.\n\nFigure AI의 로봇의 기본 메커니즘에 대해서는 많이 알지 못하지만, Deepmind의 RT-2 모델과 같은 예시를 통해, 우리는 이미 LLM을 로봇 동작이나 텍스트를 출력하도록 훈련하는 방법을 증명한 연구자들이 있기에 꽤 좋은 아이디어를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png)\n\n그러나 RT-2는 GPT-4의 능력에 가까운 두 LLM인 PALM-E와 PALI-X를 사용하여 훈련된 것을 고려할 때, Figure AI의 로봇 뒤에 있는 LLM이 이 모델이 지금까지 존재한 가장 고급 비전-언어-행동 모델일 수 있다는 주장이 가능합니다.\n\n하지만 현실적으로 생각해 봅시다.\n\n그것은 여러 차례 연습된 것일 수 있는 매우 제한된 데모였습니다. 사실, 로봇이 일반적인 용도의 능력 측면에서 아직 매우 초기 단계에 있다는 것은 거의 확실합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 이 로봇들의 여정에서 다음 단계를 상상하기 위해 구글이 방금 공개한 것에서 영감을 받을 수 있습니다.\n\nSIMA, 3D 총론적 에이전트.\n\n하지만 이번에는 Figure AI의 경우와 달리, 에이전트에 대해 훨씬 더 많은 정보를 갖고 있습니다.\n\n# SIMA, 최초의 진정한 총론적 에이전트?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle의 확장 가능한 인공 지능 멀티월드 에이전트(SIMA) 프로젝트는 어떤 3D 환경에서 임의의 언어 명령을 이해하고 실행할 수 있는 인공 지능(AI) 시스템을 만들고자 합니다.\n\n이 계획은 언어를 지각 및 다양한 가상 세계에서 실제 행동과 결합하여 처리함으로써 일반적인 AI 개발에서 중요한 과제에 대처합니다. 이는 연구 환경 및 상업 비디오 게임을 포함한 다양한 가상 세계를 대상으로 합니다.\n\n간단히 말하면, SIMA는 언어 요청을 받아들이고 이를 3D 환경에서 키보드 및 마우스 조작으로 변환합니다.\n\n그러나 고려해야 할 중요한 요소는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 에이전트들의 목표는 \"자원을 수집하고 집을 짓는\" 등 사용자가 제시하는 자연어 지침대로 작업을 수행하는 것입니다.\n\n간단히 말해, 모델은 해당 환경에서 상호작용할 때 사람들과 똑같은 입력을 갖고 있으므로 사람들보다 약점이 없다는 것을 의미하며, 이는 기접근 API나 이와 유사한 것에 대한 접근 권한이 없다는 것을 의미합니다.\n\n결과적으로, 요구된 작업을 수행하는 유일한 방법은 해당 작업을 수행하는 데 사람이 수행할 키보드 및 마우스 작업을 예측하는 것입니다.\n\nSIMA 에이전트는 다음 구성 요소로 이루어져 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마크다운 형식으로 변경한 텍스트입니다.\n\n![이미지](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_1.png)\n\n- 인코더:\n\n  - 텍스트 인코더: 언어 명령을 모델이 해석할 수 있는 임베딩으로 번역합니다.\n  - SPARC 개발을 기반으로 한 이미지 인코더.\n  - 비디오 인코더.\n\n2. Multi-modal Transformer + Transformer XL: 두 개의 트랜스포머 아키텍처로, 전자는 모달 간 교차 어텐션을 수행하고, 후자는 이전 상태를 취해 새로운 상태를 식별합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 정책: 선택된 행동이 600가지 가능한 기술 중 하나로 결정되는 분류 헤드\n\n여기에는 해석할 내용이 많기 때문에 단계별로 진행해 봅시다.\n\n## 세계 처리\n\n대부분의 최신 모델들에서와 마찬가지로, 첫 번째 단계는 입력을 \"인코딩\"하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 용어로 설명하자면, 입력 데이터(텍스트와 비디오)를 가져와 해당 인코더를 사용하여 이러한 데이터 포인트를 벡터 임베딩으로 변환하는 것이 아이디어입니다.\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_2.png)\n\n하지만 왜 이 작업을 하는 것일까요?\n\n이 변환을 수행함으로써 각 요소가 해당 개념의 의미론을 캡처하는 밀집 벡터로 표현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 말해, 의미론적으로 유사한 개념은 유사한 벡터를 갖게 되어 벡터 공간에서 표현되었을 때 더 가까이 위치하게 됩니다: \n\n![Concept Vectors](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_3.png)\n\n 또 다른 중요한 이점은 '개념을 벡터로 변환함'으로, 인간이 아주 무의식적으로 수행하는 '우리 세계를 이해하는' 행위를 수학적인 연습(컴퓨터에 이상적)으로 변환하여, 개념이 이제 수치로 표현된다는 것입니다.\n\n특히, 이 모델은 이러한 벡터들 사이의 유사성(그들 사이의 거리)을 계산하여, 어떤 개념이 다른 것들과 유사한지를 알아냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 모델은 '개'와 '고양이'가 유사한 속성 (포유류, 길들여진 동물, 네 다리 등)을 공유하는 유사한 개념을 나타낸다는 것을 알 수 있습니다.\n\n분명히 말씀드리자면, AI가 각 숫자를 다루는 방식은 아니며, AI가 중요시하는 것은 벡터 간의 근접성입니다. 만약에 도움이 될 경우, 벡터의 의미를 단순히 벡터 자체로만 생각하지 마십시오 (우리는 AI가 정말 '개'가 무엇인지 아는지 확신할 수 없습니다) 대신, 가장 가까운 이웃들의 합으로 생각하고, 모델에게 다른 것들과 유사하다는 신호를 보내는 것으로 여기면 됩니다 (강아지는 고양이와 유사하며 문과는 다릅니다).\n\n비슷한 이유로 유사성은 데이터 유형이 다른 상황에 집중하는 데 중요한 역할을 합니다.\n\n다양한 데이터 유형에서 나온 벡터로 오는 여러 모달 상황에 집중하는 데 유용합니다.\n\n기본 아이디어는 '개'라는 단어와 '개의 이미지'가 유사한 벡터를 공유하도록 원한다는 것으로, 이것은 모델에게 두 가지가 동일한 개념을 나타낸다는 것을 알려줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 텍스트와 이미지는 구조적으로 매우 다른 데이터 유형이기 때문에 서로 다른 인코더가 필요합니다.\n\n이것은 문제입니다. 비슷한 개념에 대해 유사한 임베딩을 생성하도록 보장하며 별도의 인코더를 훈련해야 합니다.\n\n이 문제를 해결하기 위해 SIMA는 SPARC 이미지 인코더를 사용합니다.\n\n매우 최근에 개발된 SPARC 인코더는 대부분의 다른 이미지 인코더와 매우 유사한 방식으로 훈련됩니다(대비 학습을 사용), 그러나 미세 구체적인 세부 사항을 더 잘 캡처할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 흔한 이미지 인코더들은 이미지의 지역 세부사항을 제대로 포착하지 못하는 문제가 있습니다.\n\n네, 그들은 이미지가 무엇에 대해인지를 알려줄 수 있지만, 이미지의 전역 의미를 설명하는 데 도움이 되지 않는 중요한 작은 세부사항을 놓치기도 합니다. 그럼에도 불구하고 많은 경우에 중요합니다.\n\nSPARC는 유사한 방법을 제안하지만 매우 흥미로운 요소를 추가합니다.\n\n예를 들어, \"바구니 속 고양이와 개\"를 나타내는 이미지-텍스트 쌍이 있다고 가정해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_4.png\" /\u003e\n\n- 먼저, SPARC는 이미지를 패치로 나눕니다.\n- 그런 다음 각 패치에 대해 텍스트 설명 중 하나의 토큰을 할당합니다. 예를 들어, 패치가 개의 일부를 나타내면 \"개\"라는 단어가 할당됩니다.\n- 이 작업은 모든 패치(왼쪽 색상 그리드의 수직 열)에 대해 수행되며, 텍스트 설명의 여러 측면을 다루는 패치의 경우 각각에 가중치가 할당됩니다.\n\n- 특정 개념에 할당된 모든 패치를 가지고 있으면 각 패치가 '개'와 같은 특정 단어에 부여하는 가중치를 그룹화하여 이 그룹화된 임베딩을 실제 단어와 비교합니다. 그러나 여기에는 중요한 점이 있습니다. 이미지 전역에 대해 적용하는 대신, 모든 개념에 대해 이를 다섯 번 적용합니다.\n\n하지만 모든 이 작업을 하는 이유는 무엇일까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지역성. 이미지의 특정 부분이 이제 특정 개념에 할당되었으므로 모델은 이제 두 가지를 알게 됩니다:\n- 이미지에서 어떤 개념이 더 많이 표현되는지\n- 이러한 개념이 이미지에서 어디에 위치하는지.\n\n일반인이 이해하기 쉽게 설명하면 SPARC와 다른 이미지 인코더 간의 주요 차이점은 텍스트 설명의 개별 단어를 이미지의 특정 부분에 할당한다는 사실에 있습니다.\n\n이러한 방식으로 이미지의 특정 부분의 그룹화된 임베딩이 '개' 단어 토큰에 크게 편향되어 있다면, 이미지의 해당 영역에는 아마도 개가 포함되어 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSIMA의 중요한 요소입니다. 3D 에이전트는 요청된 작업의 일환으로 상호 작용할 특정 객체를 식별할 수 있어야합니다.\n\n마지막으로, 비디오 인코더에 대해 이야기하면, 모델이 과거 상태를 고려할 수 있도록 포함되어 있습니다. 중요한 것은 비디오 인코더가 시간적 인식을 포함한다는 것인데, 이는 텍스트나 이미지 인코더가 제공할 수 없는 기능입니다.\n\n환경의 현재 상태뿐만 아니라 과거의 환경 상태와 취해진 조치에 따라 다음 조치를 취할지 결정하기 때문에 이는 중요합니다.\n\n## 최적의 정책 선택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제공된 정보를 바탕으로 SIMA는 다른 인코더들에 의해 생성된 표현을 사용하여 변환 모델 세트를 활용합니다. 이 모델은 LLM이 단어를 예측하는 대신 작업을 출력하여 에이전트가 실행할 키보드 및 마우스 조작을 지시합니다.\n\n한편, 왜 Gemini, 구글의 MLLM을 사용하는 대신 모델의 주요 '두뇌'로 이상한 Transformers 세트를 사용했는지 궁금할 수도 있습니다.\n\n이유는 아마도 예산 때문인데, 연구자들 자신들이 기술 보고서에서 SIMA의 분명한 다음 단계는 Gemini를 사용하는 것이라고 인정했기 때문입니다.\n\n이것은 매우 흥미로운데, 최고의 '두뇌'를 사용하지 않았음에도 놀라운 결과를 얻었다고 하니, 이제 우리가 곧 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 진정한 일반화자\n\n지금쯤이면 알 수 있겠지만, 목표는 여태까지 한 게임에서도 능숙한 에이전트를 훈련시키는 것이었습니다. 게임을 해본 적이 없는 게임조차도요.\n\n훈련 후 SIMA 에이전트는 다양한 범주로 구분된 600가지 다양한 기본 작업을 수행할 수 있었습니다. 이러한 범주에는 네비게이션, 동물 상호작용, 음식 등이 포함되어 있었습니다:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSIMA가이러한작업을수행하고있는모습을여기에서확인할수있습니다. 게다가, SIMA는매우유망한결과를얻어 언급할만한가치가있습니다.\n\n우선, 다양한게임에서훈련을받았음에도불구하고, 평균적으로SIMA는 단일게임에서전문화된 에이전트들보다 더우수한성능을보였습니다... 그특정게임안에서:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_6.png)\n\n더욱인상적인점은, 여러가지다른게임에서, 에이전트가영이의업무(non-trivial tasks)를달성했으며, 이중대부분은얼마들이나예시없이(zero-shot tasks)성과를거두었으며, 다시한번전문화된 에이전트들을이겼습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_7.png\" /\u003e\n\n이는 과거에 본 적 없는 환경에 놓여도 모델이 일반적으로 잘 수행되었으며, 때로는 Goat Simulator 3와 같은 경우에는 전문화된 에이전트(해당 게임에서만 훈련된 에이전트)보다 뛰어난 성과를 보였습니다.\n\n그렇다면 이 모든 것이 의미하는 바는 무엇인가요?\n\n간단히 말하자면, 우리는 게임 간의 지식 전이의 구체적인 증거를 관찰하고 있다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 말해, 모델은 일부 게임에서 유용하게 적용할 수 있는 의미 있는 기술을 학습합니다 (예: 키보드로 이동하는 방법을 배우는 것과 같이).\n\n더욱이, 이러한 기술들은 상당히 높은 품질을 갖고 있어서 이 일반화된 에이전트가 많은 게임에서 특화된 에이전트들을 이기는 것을 의미하며, 이는 일반화된 접근 방식이 그들이 다양한 환경에 적용할 우수한 기술을 학습하는 데 도움이 된다는 것을 시사합니다.\n\nFigure AI의 발전과 함께 이러한 결과들이 어떠한 맥락에서 중요성을 얻을 수 있는 경우, 전체적으로 매우 인상적인 결과입니다.\n\n# 로보틱스에게 훌륭한 주였습니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 로봇 기술은 요즘 정말 빠르게 발전하고 있어요.\n\n- 한 쪽에서, Figure AI는 우리가 점점 더 많은 수동 작업 범위를 알리는 인간형 로봇을 만들어내는 데 탁월한 실력을 보여줍니다.\n- 반면에, SIMA는 3D 환경에서 최초의 일반 적 에이전트를 보여주고 있음을 암시합니다.\n\n하지만 우리가 깨닫는 것은 시너지의 잠재력입니다.\n\n이러한 에이전트들을 실생활 상황으로 가져오는 데는 아직 이르지만, 이 두 분야 사이의 융합은 다음 단계로 자연스러운 발전을 이루고 있습니다; SIMA가 훈련의 장소일 뿐만 아니라, Figure AI 로봇들이 일반적인 에이전트의 구현체 역할을 하기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 다른 기업들도 구체적 지능에 대한 자신들의 시사를 제시하며 경쟁이 치열해지고 있습니다. 많은 기존 기업들이 인류의 기술이 충분히 준비되어 있다고 느끼고 있어서 다음 큰 도전, 인공지능을 실생활에 적용하는 것에 착수할 준비가 되어있다고 느끼는 것이 분명합니다.","ogImage":{"url":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png"},"coverImage":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png","tag":["Tech"],"readingTime":9},"content":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h1: \"h1\",\n    img: \"img\",\n    ul: \"ul\",\n    li: \"li\",\n    ol: \"ol\",\n    h2: \"h2\"\n  }, _provideComponents(), props.components);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"두 개의 세계가 충돌하고 있어요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우리는 방금 두 가지 분야에서 로봇 공학의 상당한 도약을 목격했어요. Figure AI의 말하는 로봇과 Google의 일반적인 에이전트 SIMA가 그겁니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만, 아니요, 이것은 일반적인 인공 지능(AGI)은 아니에요. 일부 사치스럽지만 입증되지 않은 주장이 돌아다니고 로봇들이 우리를 다 죽일 거라고 말하진 않을 거예요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 두 소식이 이미 자체로 흥미롭지만, 그들 간의 시너지는 내 관점에서 AI 구현 지능의 첫 번째 발걸음이라고 생각해요.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"GPT-4의 구현\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러면 Figure AI는 뭔가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Figure AI는 로봇 회사로, CEO의 말에 따르면 \\\"위험하고 원하지 않는 직업이 필요하지 않도록 하여 미래 세대가 더 행복하고 의미 있는 삶을 살도록 허용하는 로봇을 구축하고 있습니다.\\\"\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이미 유사한 웅장한 사명을 가진 회사들을 들어보셨을 것입니다. 그러나 OpenAI, NVIDIA, Jeff Bezos, 그리고 Intel이 시장에 제품이 없는 회사에 대해 26억 달러 평가액에서 6억 7500만 달러 시리즈 투자를 한다면, 그들이 무엇인가를 찾고 있다는 것을 알 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 왜 모두가 이 회사에 대해 이야기하는 걸까요? 이 비디오 때문입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그 링크를 클릭하는 것을 강력히 권장합니다. 간단히 말하자면, 이 비디오를 통해 사람과 상호작용하는 로봇이 여러 작업을 민첩하게 수행한다는 것을 보여줍니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"흥미로운 점은 Figure AI의 로봇의 핵심에는 GPT-4V, OpenAI의 주요 다중모달 대형 언어 모델인 MLLM이 있다는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다시 말해, Figure AI의 로봇은 '화신 ChatGPT'의 처음으로, 즉 LLMs가 실체화된 작업도 할 수 있는 능력이 생긴 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 대부분의 사람들이 놓칠 수도 있는데, 로봇이 이전 동작에 대한 이유를 설명하면서 사람에게 작업을 수행하도록 요청하는 순간에 주목하여 주길 바랍니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 질문이나 요청이 사소한 것은 아니라는 점을 분명히 이해해야 합니다. 모델이 동시에 여러 작업을 수행할 수 있는 능력을 보여주기 위해 일부러 그랬던 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"구체적으로 말하자면, 그들은 GPT-4를 세밀하게 조정하여 텍스트와 작업 표현을 출력하도록 만들었는데, 전자는 보코더를 통해 음성으로 디코딩되고, 후자는 액추에이터 동작으로 디코딩되어 몸을 움직이도록 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Figure AI의 로봇의 기본 메커니즘에 대해서는 많이 알지 못하지만, Deepmind의 RT-2 모델과 같은 예시를 통해, 우리는 이미 LLM을 로봇 동작이나 텍스트를 출력하도록 훈련하는 방법을 증명한 연구자들이 있기에 꽤 좋은 아이디어를 얻을 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 RT-2는 GPT-4의 능력에 가까운 두 LLM인 PALM-E와 PALI-X를 사용하여 훈련된 것을 고려할 때, Figure AI의 로봇 뒤에 있는 LLM이 이 모델이 지금까지 존재한 가장 고급 비전-언어-행동 모델일 수 있다는 주장이 가능합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 현실적으로 생각해 봅시다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그것은 여러 차례 연습된 것일 수 있는 매우 제한된 데모였습니다. 사실, 로봇이 일반적인 용도의 능력 측면에서 아직 매우 초기 단계에 있다는 것은 거의 확실합니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"따라서, 이 로봇들의 여정에서 다음 단계를 상상하기 위해 구글이 방금 공개한 것에서 영감을 받을 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SIMA, 3D 총론적 에이전트.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 이번에는 Figure AI의 경우와 달리, 에이전트에 대해 훨씬 더 많은 정보를 갖고 있습니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"SIMA, 최초의 진정한 총론적 에이전트?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Google의 확장 가능한 인공 지능 멀티월드 에이전트(SIMA) 프로젝트는 어떤 3D 환경에서 임의의 언어 명령을 이해하고 실행할 수 있는 인공 지능(AI) 시스템을 만들고자 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 계획은 언어를 지각 및 다양한 가상 세계에서 실제 행동과 결합하여 처리함으로써 일반적인 AI 개발에서 중요한 과제에 대처합니다. 이는 연구 환경 및 상업 비디오 게임을 포함한 다양한 가상 세계를 대상으로 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"간단히 말하면, SIMA는 언어 요청을 받아들이고 이를 3D 환경에서 키보드 및 마우스 조작으로 변환합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그러나 고려해야 할 중요한 요소는 다음과 같습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 에이전트들의 목표는 \\\"자원을 수집하고 집을 짓는\\\" 등 사용자가 제시하는 자연어 지침대로 작업을 수행하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"간단히 말해, 모델은 해당 환경에서 상호작용할 때 사람들과 똑같은 입력을 갖고 있으므로 사람들보다 약점이 없다는 것을 의미하며, 이는 기접근 API나 이와 유사한 것에 대한 접근 권한이 없다는 것을 의미합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"결과적으로, 요구된 작업을 수행하는 유일한 방법은 해당 작업을 수행하는 데 사람이 수행할 키보드 및 마우스 작업을 예측하는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SIMA 에이전트는 다음 구성 요소로 이루어져 있습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"아래는 마크다운 형식으로 변경한 텍스트입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_1.png\",\n        alt: \"이미지\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"인코더:\"\n        }), \"\\n\", _jsxs(_components.ul, {\n          children: [\"\\n\", _jsx(_components.li, {\n            children: \"텍스트 인코더: 언어 명령을 모델이 해석할 수 있는 임베딩으로 번역합니다.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"SPARC 개발을 기반으로 한 이미지 인코더.\"\n          }), \"\\n\", _jsx(_components.li, {\n            children: \"비디오 인코더.\"\n          }), \"\\n\"]\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"2\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Multi-modal Transformer + Transformer XL: 두 개의 트랜스포머 아키텍처로, 전자는 모달 간 교차 어텐션을 수행하고, 후자는 이전 상태를 취해 새로운 상태를 식별합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      start: \"3\",\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"정책: 선택된 행동이 600가지 가능한 기술 중 하나로 결정되는 분류 헤드\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"여기에는 해석할 내용이 많기 때문에 단계별로 진행해 봅시다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"세계 처리\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"대부분의 최신 모델들에서와 마찬가지로, 첫 번째 단계는 입력을 \\\"인코딩\\\"하는 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"일반적인 용어로 설명하자면, 입력 데이터(텍스트와 비디오)를 가져와 해당 인코더를 사용하여 이러한 데이터 포인트를 벡터 임베딩으로 변환하는 것이 아이디어입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_2.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 왜 이 작업을 하는 것일까요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 변환을 수행함으로써 각 요소가 해당 개념의 의미론을 캡처하는 밀집 벡터로 표현됩니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다시 말해, 의미론적으로 유사한 개념은 유사한 벡터를 갖게 되어 벡터 공간에서 표현되었을 때 더 가까이 위치하게 됩니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_3.png\",\n        alt: \"Concept Vectors\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"또 다른 중요한 이점은 '개념을 벡터로 변환함'으로, 인간이 아주 무의식적으로 수행하는 '우리 세계를 이해하는' 행위를 수학적인 연습(컴퓨터에 이상적)으로 변환하여, 개념이 이제 수치로 표현된다는 것입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"특히, 이 모델은 이러한 벡터들 사이의 유사성(그들 사이의 거리)을 계산하여, 어떤 개념이 다른 것들과 유사한지를 알아냅니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, 모델은 '개'와 '고양이'가 유사한 속성 (포유류, 길들여진 동물, 네 다리 등)을 공유하는 유사한 개념을 나타낸다는 것을 알 수 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"분명히 말씀드리자면, AI가 각 숫자를 다루는 방식은 아니며, AI가 중요시하는 것은 벡터 간의 근접성입니다. 만약에 도움이 될 경우, 벡터의 의미를 단순히 벡터 자체로만 생각하지 마십시오 (우리는 AI가 정말 '개'가 무엇인지 아는지 확신할 수 없습니다) 대신, 가장 가까운 이웃들의 합으로 생각하고, 모델에게 다른 것들과 유사하다는 신호를 보내는 것으로 여기면 됩니다 (강아지는 고양이와 유사하며 문과는 다릅니다).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"비슷한 이유로 유사성은 데이터 유형이 다른 상황에 집중하는 데 중요한 역할을 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다양한 데이터 유형에서 나온 벡터로 오는 여러 모달 상황에 집중하는 데 유용합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"기본 아이디어는 '개'라는 단어와 '개의 이미지'가 유사한 벡터를 공유하도록 원한다는 것으로, 이것은 모델에게 두 가지가 동일한 개념을 나타낸다는 것을 알려줍니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 텍스트와 이미지는 구조적으로 매우 다른 데이터 유형이기 때문에 서로 다른 인코더가 필요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이것은 문제입니다. 비슷한 개념에 대해 유사한 임베딩을 생성하도록 보장하며 별도의 인코더를 훈련해야 합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이 문제를 해결하기 위해 SIMA는 SPARC 이미지 인코더를 사용합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"매우 최근에 개발된 SPARC 인코더는 대부분의 다른 이미지 인코더와 매우 유사한 방식으로 훈련됩니다(대비 학습을 사용), 그러나 미세 구체적인 세부 사항을 더 잘 캡처할 수 있습니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"가장 흔한 이미지 인코더들은 이미지의 지역 세부사항을 제대로 포착하지 못하는 문제가 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"네, 그들은 이미지가 무엇에 대해인지를 알려줄 수 있지만, 이미지의 전역 의미를 설명하는 데 도움이 되지 않는 중요한 작은 세부사항을 놓치기도 합니다. 그럼에도 불구하고 많은 경우에 중요합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SPARC는 유사한 방법을 제안하지만 매우 흥미로운 요소를 추가합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"예를 들어, \\\"바구니 속 고양이와 개\\\"를 나타내는 이미지-텍스트 쌍이 있다고 가정해 봅시다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_4.png\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"먼저, SPARC는 이미지를 패치로 나눕니다.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"그런 다음 각 패치에 대해 텍스트 설명 중 하나의 토큰을 할당합니다. 예를 들어, 패치가 개의 일부를 나타내면 \\\"개\\\"라는 단어가 할당됩니다.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"이 작업은 모든 패치(왼쪽 색상 그리드의 수직 열)에 대해 수행되며, 텍스트 설명의 여러 측면을 다루는 패치의 경우 각각에 가중치가 할당됩니다.\"\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [\"\\n\", _jsx(_components.p, {\n          children: \"특정 개념에 할당된 모든 패치를 가지고 있으면 각 패치가 '개'와 같은 특정 단어에 부여하는 가중치를 그룹화하여 이 그룹화된 임베딩을 실제 단어와 비교합니다. 그러나 여기에는 중요한 점이 있습니다. 이미지 전역에 대해 적용하는 대신, 모든 개념에 대해 이를 다섯 번 적용합니다.\"\n        }), \"\\n\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 모든 이 작업을 하는 이유는 무엇일까요?\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"지역성. 이미지의 특정 부분이 이제 특정 개념에 할당되었으므로 모델은 이제 두 가지를 알게 됩니다:\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"이미지에서 어떤 개념이 더 많이 표현되는지\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"이러한 개념이 이미지에서 어디에 위치하는지.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"일반인이 이해하기 쉽게 설명하면 SPARC와 다른 이미지 인코더 간의 주요 차이점은 텍스트 설명의 개별 단어를 이미지의 특정 부분에 할당한다는 사실에 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이러한 방식으로 이미지의 특정 부분의 그룹화된 임베딩이 '개' 단어 토큰에 크게 편향되어 있다면, 이미지의 해당 영역에는 아마도 개가 포함되어 있을 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SIMA의 중요한 요소입니다. 3D 에이전트는 요청된 작업의 일환으로 상호 작용할 특정 객체를 식별할 수 있어야합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"마지막으로, 비디오 인코더에 대해 이야기하면, 모델이 과거 상태를 고려할 수 있도록 포함되어 있습니다. 중요한 것은 비디오 인코더가 시간적 인식을 포함한다는 것인데, 이는 텍스트나 이미지 인코더가 제공할 수 없는 기능입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"환경의 현재 상태뿐만 아니라 과거의 환경 상태와 취해진 조치에 따라 다음 조치를 취할지 결정하기 때문에 이는 중요합니다.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"최적의 정책 선택\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"제공된 정보를 바탕으로 SIMA는 다른 인코더들에 의해 생성된 표현을 사용하여 변환 모델 세트를 활용합니다. 이 모델은 LLM이 단어를 예측하는 대신 작업을 출력하여 에이전트가 실행할 키보드 및 마우스 조작을 지시합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"한편, 왜 Gemini, 구글의 MLLM을 사용하는 대신 모델의 주요 '두뇌'로 이상한 Transformers 세트를 사용했는지 궁금할 수도 있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이유는 아마도 예산 때문인데, 연구자들 자신들이 기술 보고서에서 SIMA의 분명한 다음 단계는 Gemini를 사용하는 것이라고 인정했기 때문입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이것은 매우 흥미로운데, 최고의 '두뇌'를 사용하지 않았음에도 놀라운 결과를 얻었다고 하니, 이제 우리가 곧 살펴볼 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"진정한 일반화자\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"지금쯤이면 알 수 있겠지만, 목표는 여태까지 한 게임에서도 능숙한 에이전트를 훈련시키는 것이었습니다. 게임을 해본 적이 없는 게임조차도요.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"훈련 후 SIMA 에이전트는 다양한 범주로 구분된 600가지 다양한 기본 작업을 수행할 수 있었습니다. 이러한 범주에는 네비게이션, 동물 상호작용, 음식 등이 포함되어 있었습니다:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_5.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"SIMA가이러한작업을수행하고있는모습을여기에서확인할수있습니다. 게다가, SIMA는매우유망한결과를얻어 언급할만한가치가있습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"우선, 다양한게임에서훈련을받았음에도불구하고, 평균적으로SIMA는 단일게임에서전문화된 에이전트들보다 더우수한성능을보였습니다... 그특정게임안에서:\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: _jsx(_components.img, {\n        src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_6.png\",\n        alt: \"image\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"더욱인상적인점은, 여러가지다른게임에서, 에이전트가영이의업무(non-trivial tasks)를달성했으며, 이중대부분은얼마들이나예시없이(zero-shot tasks)성과를거두었으며, 다시한번전문화된 에이전트들을이겼습니다:\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(\"img\", {\n      src: \"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_7.png\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이는 과거에 본 적 없는 환경에 놓여도 모델이 일반적으로 잘 수행되었으며, 때로는 Goat Simulator 3와 같은 경우에는 전문화된 에이전트(해당 게임에서만 훈련된 에이전트)보다 뛰어난 성과를 보였습니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그렇다면 이 모든 것이 의미하는 바는 무엇인가요?\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"간단히 말하자면, 우리는 게임 간의 지식 전이의 구체적인 증거를 관찰하고 있다는 것입니다.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"다시 말해, 모델은 일부 게임에서 유용하게 적용할 수 있는 의미 있는 기술을 학습합니다 (예: 키보드로 이동하는 방법을 배우는 것과 같이).\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"더욱이, 이러한 기술들은 상당히 높은 품질을 갖고 있어서 이 일반화된 에이전트가 많은 게임에서 특화된 에이전트들을 이기는 것을 의미하며, 이는 일반화된 접근 방식이 그들이 다양한 환경에 적용할 우수한 기술을 학습하는 데 도움이 된다는 것을 시사합니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Figure AI의 발전과 함께 이러한 결과들이 어떠한 맥락에서 중요성을 얻을 수 있는 경우, 전체적으로 매우 인상적인 결과입니다.\"\n    }), \"\\n\", _jsx(_components.h1, {\n      children: \"로보틱스에게 훌륭한 주였습니다\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"AI 로봇 기술은 요즘 정말 빠르게 발전하고 있어요.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"한 쪽에서, Figure AI는 우리가 점점 더 많은 수동 작업 범위를 알리는 인간형 로봇을 만들어내는 데 탁월한 실력을 보여줍니다.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"반면에, SIMA는 3D 환경에서 최초의 일반 적 에이전트를 보여주고 있음을 암시합니다.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"하지만 우리가 깨닫는 것은 시너지의 잠재력입니다.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"이러한 에이전트들을 실생활 상황으로 가져오는 데는 아직 이르지만, 이 두 분야 사이의 융합은 다음 단계로 자연스러운 발전을 이루고 있습니다; SIMA가 훈련의 장소일 뿐만 아니라, Figure AI 로봇들이 일반적인 에이전트의 구현체 역할을 하기 때문이죠.\"\n    }), \"\\n\", _jsx(\"div\", {\n      class: \"content-ad\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"그리고 다른 기업들도 구체적 지능에 대한 자신들의 시사를 제시하며 경쟁이 치열해지고 있습니다. 많은 기존 기업들이 인류의 기술이 충분히 준비되어 있다고 느끼고 있어서 다음 큰 도전, 인공지능을 실생활에 적용하는 것에 착수할 준비가 되어있다고 느끼는 것이 분명합니다.\"\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, props)\n  })) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence"},"buildId":"PgdIX9e0tvkvkdAmDT6qR","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>