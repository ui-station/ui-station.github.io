<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>안정적인 확산에 대한 설명 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-StableDiffusionExplained" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="안정적인 확산에 대한 설명 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="안정적인 확산에 대한 설명 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-StableDiffusionExplained_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-StableDiffusionExplained" data-gatsby-head="true"/><meta name="twitter:title" content="안정적인 확산에 대한 설명 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-StableDiffusionExplained_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 20:45" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_buildManifest.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">안정적인 확산에 대한 설명</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="안정적인 확산에 대한 설명" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-StableDiffusionExplained&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>Stable 확산이 어떻게 작동합니까? 텍스트에서 이미지 생성 기술 설명</h2>
<p><img src="/assets/img/2024-05-18-StableDiffusionExplained_0.png" alt="이미지"></p>
<p>크기가 큰 텍스트에서 이미지 모델은 텍스트 프롬프트로부터 이미지의 고품질 합성을 가능케하여 높은 성공을 거뒀습니다. 확산 모델은 텍스트에서 이미지 생성 작업에 적용되어 최첨단 이미지 생성 결과를 얻는 데 도움이 될 수 있습니다.</p>
<p>Stable 확산 모델은 이미지 생성을 위해 최첨단 결과를 달성했습니다. Stable 확산은 특정 유형의 확산 모델인 Latent 확산 모델에 기초합니다. 이 모델은 CompVis, LMU 및 RunwayML의 연구원 및 엔지니어들에 의해 제안된 'Latent 확산 모델을 사용한 고해상도 이미지 합성'에 기반합니다. 이 모델은 LAION-5B 데이터베이스의 하위 집합에서 512x512 이미지로 초기에 교육되었습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이는 주로 CLIP와 같은 사전 훈련된 언어 모델을 사용하여 텍스트 입력을 잠재 벡터로 인코딩함으로써 달성됩니다. 확산 모델은 텍스트로부터 이미지 데이터를 생성하는 데 최첨단 결과를 달성할 수 있습니다. 그러나 잡음 제거 프로세스는 고해상도 이미지를 생성할 때 매우 느리며 많은 메모리를 소비합니다. 따라서 이러한 모델을 훈련하고 추론에 사용하는 것이 어려운 과제입니다.</p>
<p>이에 따라 잠재 확산은 실제 픽셀 공간 대신 낮은 차원의 잠재 공간에서 확산 프로세스를 적용함으로써 메모리와 계산 시간을 줄일 수 있습니다. 잠재 확산에서 모델은 이미지의 잠재(압축) 표현을 생성하도록 훈련됩니다.</p>
<p>확산 모델의 훈련</p>
<p>Stable Diffusion은 수십억 장의 이미지로 훈련된 대규모 텍스트에서 이미지로 확산 모델입니다. 이미지 확산 모델은 이미지를 잡음 제거하여 출력 이미지를 생성하는 방법을 배웁니다. Stable Diffusion은 훈련 데이터에서 인코딩된 잠재 이미지를 입력으로 사용합니다. 또한 주어진 이미지 z0에 대해, 확산 알고리즘은 이미지에 점진적으로 잡음을 추가하여 소음이 섞인 이미지 zt를 생성합니다. 여기서 t는 잡음이 추가된 횟수를 나타냅니다. t가 충분히 크면 이미지는 순수한 잡음에 가까워집니다. 시간 단계 t, 텍스트 프롬프트, 이미지 확산 알고리즘 등의 입력 집합이 주어진 경우, 확산 알고리즘은 노이즈를 예측하는 네트워크를 학습합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>잠재 확산에는 주로 세 가지 주요 구성요소가 있습니다:</p>
<ul>
<li>자동 인코더 (VAE).</li>
<li>U-Net.</li>
<li>텍스트 인코더, 예를 들어 CLIP의 텍스트 인코더.</li>
</ul>
<ol>
<li>자동 인코더 (VAE)</li>
</ol>
<p>VAE 모델은 인코더와 디코더 두 부분으로 구성됩니다. 잠재 확산 학습 중에 인코더는 512<em>512</em>3 이미지를 순방향 확산 과정을 위한 사이즈가 64<em>64</em>4인 낮은 차원의 잠재 표현으로 변환합니다. 우리는 이미지의 이러한 작은 인코딩된 버전을 잠재라고 부릅니다. 학습 각 단계에서 이러한 잠재에 더 많은 잡음을 적용합니다. 이미지의 인코딩된 잠재 표현은 U-Net 모델의 입력으로 작용합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>여기서는 (3, 512, 512) 모양의 이미지를 (4, 64, 64) 모양의 잠재 이미지로 변환하여 메모리를 48배 더 적게 사용합니다. 이는 픽셀 공간 확산 모델과 비교했을 때 메모리 및 계산 요구 사항을 줄여줍니다. 따라서 16GB Colab GPU에서도 512 × 512 이미지를 매우 빠르게 생성할 수 있습니다.</p>
<p>디코더는 잠재 표현을 이미지로 다시 변환합니다. 역확산 과정에서 생성된 노이즈 제거된 잠재 이미지를 VAE 디코더를 사용하여 이미지로 변환합니다.</p>
<p>추론 중에는 노이즈가 제거된 이미지를 실제 이미지로 변환하기 위해 VAE 디코더만 필요합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms <span class="hljs-keyword">as</span> tfms
<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AutoencoderKL

<span class="hljs-comment"># Decode the latent representation into image space using the autoencoder model.</span>
vae = AutoencoderKL.from_pretrained(<span class="hljs-string">"CompVis/stable-diffusion-v1-4"</span>, subfolder=<span class="hljs-string">"vae"</span>)

<span class="hljs-comment"># Move to the GPU</span>
vae = vae.to(torch_device)

<span class="hljs-comment"># Convert PIL image to latents</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">pil_to_latent</span>(<span class="hljs-params">input_im</span>):
    <span class="hljs-comment"># Single image -> single latent in a batch (size: 1, 4, 64, 64)</span>
    <span class="hljs-keyword">with</span> torch.no_grad():
        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(<span class="hljs-number">0</span>).to(torch_device)*<span class="hljs-number">2</span>-<span class="hljs-number">1</span>) <span class="hljs-comment"># Note scaling</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">0.18215</span> * latent.latent_dist.sample()
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="2">
<li>UNet</li>
</ol>
<p>U-Net은 노이즈가 있는 잠재 변수의 더 많은 이미지 표현을 예측합니다. 여기서 노이즈가 있는 잠재 변수가 Unet의 입력으로 작용하며 UNet의 출력은 잠재 변수의 노이즈입니다. 이를 사용하여 우리는 노이즈를 노이즈가 있는 잠재 변수에서 뺌으로써 실제 잠재 변수를 얻을 수 있습니다.</p>
<p>Unet은 노이즈가 있는 잠재 변수(x)를 입력으로 사용하고 노이즈를 예측합니다. 우리는 또한 타임스텝(t)과 텍스트 임베딩을 가이드로 사용하는 조건부 모델을 사용합니다.</p>
<p>따라서, 모델은 다음과 같습니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> UNet2DConditionModel

<span class="hljs-comment"># "CompVis/stable-diffusion-v1-4" 모델을 사용하여 UNet 모델을 불러옵니다.</span>
unet = UNet2DConditionModel.from_pretrained(<span class="hljs-string">"CompVis/stable-diffusion-v1-4"</span>, subfolder=<span class="hljs-string">"unet"</span>)

<span class="hljs-comment"># GPU로 모델을 이동합니다.</span>
unet = unet.to(torch_device)
<span class="hljs-comment"># 노이즈 예측</span>
noise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[<span class="hljs-string">"sample"</span>]
</code></pre>
<p>해당 모델은 기본적으로 UNet 모델을 사용하며, 인코더(12블록), 가운데 블록 및 스킵 연결 디코더(12블록)로 구성되어 있습니다. 이 25개 블록 중 8개 블록은 다운샘플링 또는 업샘플링 컨볼루션 레이어이고, 17개 블록은 각각 네 개의 ResNet 레이어와 두 개의 Vision Transformer(ViT)를 포함하는 주요 블록입니다. 여기서 인코더는 이미지 표현을 낮은 해상도 이미지 표현으로 압축하고, 디코더는 낮은 해상도 이미지 표현을 원래의 더 높은 해상도 이미지 표현으로 해석합니다. 이로써 노이즈가 적은 이미지를 생성합니다.</p>
<ol start="3">
<li>텍스트-인코더</li>
</ol>
<p>텍스트-인코더는 입력 프롬프트를 임베딩 공간으로 변환하여 U-Net에 입력으로 제공합니다. Unet을 노이즈 제거 프로세스에 학습시킬 때 노이즈가 많은 latents를 안내하는 역할을 합니다. 텍스트-인코더는 일반적으로 간단한 트랜스포머 기반 인코더로, 입력 토큰 시퀀스를 잠재적인 텍스트-임베딩 시퀀스로 매핑합니다. Stable Diffusion은 새로운 텍스트 인코더를 학습하지 않고 이미 학습된 텍스트 인코더인 CLIP을 사용합니다. 텍스트 인코더는 입력 텍스트에 해당하는 임베딩을 생성합니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>토큰화</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">CLIPTextModel</span>, <span class="hljs-title class_">CLIPTokenizer</span>

# 토크나이저 및 텍스트 인코더를로드하여 텍스트를 토큰화하고 인코딩합니다.
tokenizer = <span class="hljs-title class_">CLIPTokenizer</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"openai/clip-vit-large-patch14"</span>)
text_encoder = <span class="hljs-title class_">CLIPTextModel</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"openai/clip-vit-large-patch14"</span>)

# <span class="hljs-variable constant_">GPU</span>로 이동
text_encoder = text_encoder.<span class="hljs-title function_">to</span>(torch_device)

prompt = <span class="hljs-string">'An astronaut riding a horse'</span>
# 텍스트를 토큰 시퀀스로 변환합니다.
text_input = <span class="hljs-title function_">tokenizer</span>(prompt, padding=<span class="hljs-string">"max_length"</span>, max_length=tokenizer.<span class="hljs-property">model_max_length</span>, truncation=<span class="hljs-title class_">True</span>, return_tensors=<span class="hljs-string">"pt"</span>)
input_ids = text_input.<span class="hljs-property">input_ids</span>.<span class="hljs-title function_">to</span>(torch_device)
</code></pre>
<p>Embedding 결과</p>
<pre><code class="hljs language-js"># 토큰에서 출력 임베딩 가져오기
output_embeddings = <span class="hljs-title function_">text_encoder</span>(text_input.<span class="hljs-property">input_ids</span>.<span class="hljs-title function_">to</span>(torch_device))[<span class="hljs-number">0</span>]
<span class="hljs-title function_">print</span>(<span class="hljs-string">'모양:'</span>, output_embeddings.<span class="hljs-property">shape</span>)
</code></pre>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다 모아보면, 모델은 추론 과정 중에 다음과 같이 작동합니다:</p>
<p><img src="/assets/img/2024-05-18-StableDiffusionExplained_1.png" alt="image"></p>
<p>스케줄러</p>
<p>위에서 언급된 3가지 외에도 이미지에 노이즈를 추가하고 모델을 사용하여 노이즈를 예측하는 데 사용되는 스케줄러가 있습니다.</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">diffusers에서 <span class="hljs-title class_">LMSDiscreteScheduler</span>를 가져와주세요
scheduler = <span class="hljs-title class_">LMSDiscreteScheduler</span>(beta_start=<span class="hljs-number">0.00085</span>, beta_end=<span class="hljs-number">0.012</span>, beta_schedule=<span class="hljs-string">"scaled_linear"</span>, num_train_timesteps=<span class="hljs-number">1000</span>)
</code></pre>
<p>위 코드는 모델을 훈련하는 데 사용되는 스케줄러를 설정합니다. 작은 단계 수에 대해 스케줄러를 설정하려면 다음과 같이 스케줄러를 설정하세요:</p>
<pre><code class="hljs language-js"># 샘플링 단계 수를 설정하세요:
scheduler.<span class="hljs-title function_">set_timesteps</span>(<span class="hljs-number">15</span>)
</code></pre>
<p>Stable Diffusion과 같은 Latent Diffusion Model은 다양한 창의적인 응용 프로그램을 가능하게 합니다:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>텍스트에서 이미지로 변환</li>
<li>이미지에서 이미지 생성 - 시작점을 기반으로 새 이미지를 생성하거나 수정</li>
<li>이미지 업스케일링 - 작은 이미지를 큰 이미지로 확대</li>
<li>인페인팅 - 이미지의 특정 영역을 마스킹하여 해당 영역에 새로운 디테일을 생성하는 것</li>
</ul>
<p>잠재 확산 모델은 훈련 비용과 추론을 줄여주어 대량의 고해상도 이미지 합성을 대중화 할 수 있는 잠재력이 있습니다.</p>
<p>다음 블로그에서는 새로운 개념이나 작업을 학습하는데 안정적인 확산을 세밀하게 조정하는 텍스트 역전법에 대해 이야기할 예정입니다.</p>
<p>참고:</p>
<!-- ui-station 사각형 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="7249294152" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>롬바흐, R., 블랫만, A., 로렌츠, D., 에셀, P., &#x26; 오머, B. (2022). 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전과 패턴 인식 컨퍼런스 논문집 (pp. 10684–10695).</li>
<li>장, L., &#x26; 아그라와라, M. (2023). 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543.</li>
<li><a href="https://huggingface.co/docs/diffusers/index" rel="nofollow" target="_blank">Hugging Face Diffusers 문서</a></li>
</ul>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"안정적인 확산에 대한 설명","description":"","date":"2024-05-18 20:45","slug":"2024-05-18-StableDiffusionExplained","content":"\n## Stable 확산이 어떻게 작동합니까? 텍스트에서 이미지 생성 기술 설명\n\n![이미지](/assets/img/2024-05-18-StableDiffusionExplained_0.png)\n\n크기가 큰 텍스트에서 이미지 모델은 텍스트 프롬프트로부터 이미지의 고품질 합성을 가능케하여 높은 성공을 거뒀습니다. 확산 모델은 텍스트에서 이미지 생성 작업에 적용되어 최첨단 이미지 생성 결과를 얻는 데 도움이 될 수 있습니다.\n\nStable 확산 모델은 이미지 생성을 위해 최첨단 결과를 달성했습니다. Stable 확산은 특정 유형의 확산 모델인 Latent 확산 모델에 기초합니다. 이 모델은 CompVis, LMU 및 RunwayML의 연구원 및 엔지니어들에 의해 제안된 'Latent 확산 모델을 사용한 고해상도 이미지 합성'에 기반합니다. 이 모델은 LAION-5B 데이터베이스의 하위 집합에서 512x512 이미지로 초기에 교육되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이는 주로 CLIP와 같은 사전 훈련된 언어 모델을 사용하여 텍스트 입력을 잠재 벡터로 인코딩함으로써 달성됩니다. 확산 모델은 텍스트로부터 이미지 데이터를 생성하는 데 최첨단 결과를 달성할 수 있습니다. 그러나 잡음 제거 프로세스는 고해상도 이미지를 생성할 때 매우 느리며 많은 메모리를 소비합니다. 따라서 이러한 모델을 훈련하고 추론에 사용하는 것이 어려운 과제입니다.\n\n이에 따라 잠재 확산은 실제 픽셀 공간 대신 낮은 차원의 잠재 공간에서 확산 프로세스를 적용함으로써 메모리와 계산 시간을 줄일 수 있습니다. 잠재 확산에서 모델은 이미지의 잠재(압축) 표현을 생성하도록 훈련됩니다.\n\n확산 모델의 훈련\n\nStable Diffusion은 수십억 장의 이미지로 훈련된 대규모 텍스트에서 이미지로 확산 모델입니다. 이미지 확산 모델은 이미지를 잡음 제거하여 출력 이미지를 생성하는 방법을 배웁니다. Stable Diffusion은 훈련 데이터에서 인코딩된 잠재 이미지를 입력으로 사용합니다. 또한 주어진 이미지 z0에 대해, 확산 알고리즘은 이미지에 점진적으로 잡음을 추가하여 소음이 섞인 이미지 zt를 생성합니다. 여기서 t는 잡음이 추가된 횟수를 나타냅니다. t가 충분히 크면 이미지는 순수한 잡음에 가까워집니다. 시간 단계 t, 텍스트 프롬프트, 이미지 확산 알고리즘 등의 입력 집합이 주어진 경우, 확산 알고리즘은 노이즈를 예측하는 네트워크를 학습합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n잠재 확산에는 주로 세 가지 주요 구성요소가 있습니다:\n\n- 자동 인코더 (VAE).\n- U-Net.\n- 텍스트 인코더, 예를 들어 CLIP의 텍스트 인코더.\n\n1. 자동 인코더 (VAE)\n\nVAE 모델은 인코더와 디코더 두 부분으로 구성됩니다. 잠재 확산 학습 중에 인코더는 512*512*3 이미지를 순방향 확산 과정을 위한 사이즈가 64*64*4인 낮은 차원의 잠재 표현으로 변환합니다. 우리는 이미지의 이러한 작은 인코딩된 버전을 잠재라고 부릅니다. 학습 각 단계에서 이러한 잠재에 더 많은 잡음을 적용합니다. 이미지의 인코딩된 잠재 표현은 U-Net 모델의 입력으로 작용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기서는 (3, 512, 512) 모양의 이미지를 (4, 64, 64) 모양의 잠재 이미지로 변환하여 메모리를 48배 더 적게 사용합니다. 이는 픽셀 공간 확산 모델과 비교했을 때 메모리 및 계산 요구 사항을 줄여줍니다. 따라서 16GB Colab GPU에서도 512 × 512 이미지를 매우 빠르게 생성할 수 있습니다.\n\n디코더는 잠재 표현을 이미지로 다시 변환합니다. 역확산 과정에서 생성된 노이즈 제거된 잠재 이미지를 VAE 디코더를 사용하여 이미지로 변환합니다.\n\n추론 중에는 노이즈가 제거된 이미지를 실제 이미지로 변환하기 위해 VAE 디코더만 필요합니다.\n\n```python\nfrom torchvision import transforms as tfms\nfrom diffusers import AutoencoderKL\n\n# Decode the latent representation into image space using the autoencoder model.\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Move to the GPU\nvae = vae.to(torch_device)\n\n# Convert PIL image to latents\n\ndef pil_to_latent(input_im):\n    # Single image -\u003e single latent in a batch (size: 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2. UNet\n\nU-Net은 노이즈가 있는 잠재 변수의 더 많은 이미지 표현을 예측합니다. 여기서 노이즈가 있는 잠재 변수가 Unet의 입력으로 작용하며 UNet의 출력은 잠재 변수의 노이즈입니다. 이를 사용하여 우리는 노이즈를 노이즈가 있는 잠재 변수에서 뺌으로써 실제 잠재 변수를 얻을 수 있습니다.\n\nUnet은 노이즈가 있는 잠재 변수(x)를 입력으로 사용하고 노이즈를 예측합니다. 우리는 또한 타임스텝(t)과 텍스트 임베딩을 가이드로 사용하는 조건부 모델을 사용합니다.\n\n따라서, 모델은 다음과 같습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nfrom diffusers import UNet2DConditionModel\n\n# \"CompVis/stable-diffusion-v1-4\" 모델을 사용하여 UNet 모델을 불러옵니다.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n# GPU로 모델을 이동합니다.\nunet = unet.to(torch_device)\n# 노이즈 예측\nnoise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n```\n\n해당 모델은 기본적으로 UNet 모델을 사용하며, 인코더(12블록), 가운데 블록 및 스킵 연결 디코더(12블록)로 구성되어 있습니다. 이 25개 블록 중 8개 블록은 다운샘플링 또는 업샘플링 컨볼루션 레이어이고, 17개 블록은 각각 네 개의 ResNet 레이어와 두 개의 Vision Transformer(ViT)를 포함하는 주요 블록입니다. 여기서 인코더는 이미지 표현을 낮은 해상도 이미지 표현으로 압축하고, 디코더는 낮은 해상도 이미지 표현을 원래의 더 높은 해상도 이미지 표현으로 해석합니다. 이로써 노이즈가 적은 이미지를 생성합니다.\n\n3. 텍스트-인코더\n\n텍스트-인코더는 입력 프롬프트를 임베딩 공간으로 변환하여 U-Net에 입력으로 제공합니다. Unet을 노이즈 제거 프로세스에 학습시킬 때 노이즈가 많은 latents를 안내하는 역할을 합니다. 텍스트-인코더는 일반적으로 간단한 트랜스포머 기반 인코더로, 입력 토큰 시퀀스를 잠재적인 텍스트-임베딩 시퀀스로 매핑합니다. Stable Diffusion은 새로운 텍스트 인코더를 학습하지 않고 이미 학습된 텍스트 인코더인 CLIP을 사용합니다. 텍스트 인코더는 입력 텍스트에 해당하는 임베딩을 생성합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n토큰화\n\n```js\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# 토크나이저 및 텍스트 인코더를로드하여 텍스트를 토큰화하고 인코딩합니다.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# GPU로 이동\ntext_encoder = text_encoder.to(torch_device)\n\nprompt = 'An astronaut riding a horse'\n# 텍스트를 토큰 시퀀스로 변환합니다.\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n```\n\nEmbedding 결과\n\n```js\n# 토큰에서 출력 임베딩 가져오기\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nprint('모양:', output_embeddings.shape)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다 모아보면, 모델은 추론 과정 중에 다음과 같이 작동합니다:\n\n![image](/assets/img/2024-05-18-StableDiffusionExplained_1.png)\n\n스케줄러\n\n위에서 언급된 3가지 외에도 이미지에 노이즈를 추가하고 모델을 사용하여 노이즈를 예측하는 데 사용되는 스케줄러가 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndiffusers에서 LMSDiscreteScheduler를 가져와주세요\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n```\n\n위 코드는 모델을 훈련하는 데 사용되는 스케줄러를 설정합니다. 작은 단계 수에 대해 스케줄러를 설정하려면 다음과 같이 스케줄러를 설정하세요:\n\n```js\n# 샘플링 단계 수를 설정하세요:\nscheduler.set_timesteps(15)\n```\n\nStable Diffusion과 같은 Latent Diffusion Model은 다양한 창의적인 응용 프로그램을 가능하게 합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 텍스트에서 이미지로 변환\n- 이미지에서 이미지 생성 - 시작점을 기반으로 새 이미지를 생성하거나 수정\n- 이미지 업스케일링 - 작은 이미지를 큰 이미지로 확대\n- 인페인팅 - 이미지의 특정 영역을 마스킹하여 해당 영역에 새로운 디테일을 생성하는 것\n\n잠재 확산 모델은 훈련 비용과 추론을 줄여주어 대량의 고해상도 이미지 합성을 대중화 할 수 있는 잠재력이 있습니다.\n\n다음 블로그에서는 새로운 개념이나 작업을 학습하는데 안정적인 확산을 세밀하게 조정하는 텍스트 역전법에 대해 이야기할 예정입니다.\n\n참고:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 롬바흐, R., 블랫만, A., 로렌츠, D., 에셀, P., \u0026 오머, B. (2022). 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전과 패턴 인식 컨퍼런스 논문집 (pp. 10684–10695).\n- 장, L., \u0026 아그라와라, M. (2023). 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543.\n- [Hugging Face Diffusers 문서](https://huggingface.co/docs/diffusers/index)\n","ogImage":{"url":"/assets/img/2024-05-18-StableDiffusionExplained_0.png"},"coverImage":"/assets/img/2024-05-18-StableDiffusionExplained_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003eStable 확산이 어떻게 작동합니까? 텍스트에서 이미지 생성 기술 설명\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-StableDiffusionExplained_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e크기가 큰 텍스트에서 이미지 모델은 텍스트 프롬프트로부터 이미지의 고품질 합성을 가능케하여 높은 성공을 거뒀습니다. 확산 모델은 텍스트에서 이미지 생성 작업에 적용되어 최첨단 이미지 생성 결과를 얻는 데 도움이 될 수 있습니다.\u003c/p\u003e\n\u003cp\u003eStable 확산 모델은 이미지 생성을 위해 최첨단 결과를 달성했습니다. Stable 확산은 특정 유형의 확산 모델인 Latent 확산 모델에 기초합니다. 이 모델은 CompVis, LMU 및 RunwayML의 연구원 및 엔지니어들에 의해 제안된 'Latent 확산 모델을 사용한 고해상도 이미지 합성'에 기반합니다. 이 모델은 LAION-5B 데이터베이스의 하위 집합에서 512x512 이미지로 초기에 교육되었습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이는 주로 CLIP와 같은 사전 훈련된 언어 모델을 사용하여 텍스트 입력을 잠재 벡터로 인코딩함으로써 달성됩니다. 확산 모델은 텍스트로부터 이미지 데이터를 생성하는 데 최첨단 결과를 달성할 수 있습니다. 그러나 잡음 제거 프로세스는 고해상도 이미지를 생성할 때 매우 느리며 많은 메모리를 소비합니다. 따라서 이러한 모델을 훈련하고 추론에 사용하는 것이 어려운 과제입니다.\u003c/p\u003e\n\u003cp\u003e이에 따라 잠재 확산은 실제 픽셀 공간 대신 낮은 차원의 잠재 공간에서 확산 프로세스를 적용함으로써 메모리와 계산 시간을 줄일 수 있습니다. 잠재 확산에서 모델은 이미지의 잠재(압축) 표현을 생성하도록 훈련됩니다.\u003c/p\u003e\n\u003cp\u003e확산 모델의 훈련\u003c/p\u003e\n\u003cp\u003eStable Diffusion은 수십억 장의 이미지로 훈련된 대규모 텍스트에서 이미지로 확산 모델입니다. 이미지 확산 모델은 이미지를 잡음 제거하여 출력 이미지를 생성하는 방법을 배웁니다. Stable Diffusion은 훈련 데이터에서 인코딩된 잠재 이미지를 입력으로 사용합니다. 또한 주어진 이미지 z0에 대해, 확산 알고리즘은 이미지에 점진적으로 잡음을 추가하여 소음이 섞인 이미지 zt를 생성합니다. 여기서 t는 잡음이 추가된 횟수를 나타냅니다. t가 충분히 크면 이미지는 순수한 잡음에 가까워집니다. 시간 단계 t, 텍스트 프롬프트, 이미지 확산 알고리즘 등의 입력 집합이 주어진 경우, 확산 알고리즘은 노이즈를 예측하는 네트워크를 학습합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e잠재 확산에는 주로 세 가지 주요 구성요소가 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e자동 인코더 (VAE).\u003c/li\u003e\n\u003cli\u003eU-Net.\u003c/li\u003e\n\u003cli\u003e텍스트 인코더, 예를 들어 CLIP의 텍스트 인코더.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e자동 인코더 (VAE)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eVAE 모델은 인코더와 디코더 두 부분으로 구성됩니다. 잠재 확산 학습 중에 인코더는 512\u003cem\u003e512\u003c/em\u003e3 이미지를 순방향 확산 과정을 위한 사이즈가 64\u003cem\u003e64\u003c/em\u003e4인 낮은 차원의 잠재 표현으로 변환합니다. 우리는 이미지의 이러한 작은 인코딩된 버전을 잠재라고 부릅니다. 학습 각 단계에서 이러한 잠재에 더 많은 잡음을 적용합니다. 이미지의 인코딩된 잠재 표현은 U-Net 모델의 입력으로 작용합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e여기서는 (3, 512, 512) 모양의 이미지를 (4, 64, 64) 모양의 잠재 이미지로 변환하여 메모리를 48배 더 적게 사용합니다. 이는 픽셀 공간 확산 모델과 비교했을 때 메모리 및 계산 요구 사항을 줄여줍니다. 따라서 16GB Colab GPU에서도 512 × 512 이미지를 매우 빠르게 생성할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e디코더는 잠재 표현을 이미지로 다시 변환합니다. 역확산 과정에서 생성된 노이즈 제거된 잠재 이미지를 VAE 디코더를 사용하여 이미지로 변환합니다.\u003c/p\u003e\n\u003cp\u003e추론 중에는 노이즈가 제거된 이미지를 실제 이미지로 변환하기 위해 VAE 디코더만 필요합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e torchvision \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e transforms \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tfms\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e diffusers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoencoderKL\n\n\u003cspan class=\"hljs-comment\"\u003e# Decode the latent representation into image space using the autoencoder model.\u003c/span\u003e\nvae = AutoencoderKL.from_pretrained(\u003cspan class=\"hljs-string\"\u003e\"CompVis/stable-diffusion-v1-4\"\u003c/span\u003e, subfolder=\u003cspan class=\"hljs-string\"\u003e\"vae\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Move to the GPU\u003c/span\u003e\nvae = vae.to(torch_device)\n\n\u003cspan class=\"hljs-comment\"\u003e# Convert PIL image to latents\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003epil_to_latent\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003einput_im\u003c/span\u003e):\n    \u003cspan class=\"hljs-comment\"\u003e# Single image -\u003e single latent in a batch (size: 1, 4, 64, 64)\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e).to(torch_device)*\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u003cspan class=\"hljs-comment\"\u003e# Note scaling\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e0.18215\u003c/span\u003e * latent.latent_dist.sample()\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eUNet\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eU-Net은 노이즈가 있는 잠재 변수의 더 많은 이미지 표현을 예측합니다. 여기서 노이즈가 있는 잠재 변수가 Unet의 입력으로 작용하며 UNet의 출력은 잠재 변수의 노이즈입니다. 이를 사용하여 우리는 노이즈를 노이즈가 있는 잠재 변수에서 뺌으로써 실제 잠재 변수를 얻을 수 있습니다.\u003c/p\u003e\n\u003cp\u003eUnet은 노이즈가 있는 잠재 변수(x)를 입력으로 사용하고 노이즈를 예측합니다. 우리는 또한 타임스텝(t)과 텍스트 임베딩을 가이드로 사용하는 조건부 모델을 사용합니다.\u003c/p\u003e\n\u003cp\u003e따라서, 모델은 다음과 같습니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e diffusers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e UNet2DConditionModel\n\n\u003cspan class=\"hljs-comment\"\u003e# \"CompVis/stable-diffusion-v1-4\" 모델을 사용하여 UNet 모델을 불러옵니다.\u003c/span\u003e\nunet = UNet2DConditionModel.from_pretrained(\u003cspan class=\"hljs-string\"\u003e\"CompVis/stable-diffusion-v1-4\"\u003c/span\u003e, subfolder=\u003cspan class=\"hljs-string\"\u003e\"unet\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# GPU로 모델을 이동합니다.\u003c/span\u003e\nunet = unet.to(torch_device)\n\u003cspan class=\"hljs-comment\"\u003e# 노이즈 예측\u003c/span\u003e\nnoise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\u003cspan class=\"hljs-string\"\u003e\"sample\"\u003c/span\u003e]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e해당 모델은 기본적으로 UNet 모델을 사용하며, 인코더(12블록), 가운데 블록 및 스킵 연결 디코더(12블록)로 구성되어 있습니다. 이 25개 블록 중 8개 블록은 다운샘플링 또는 업샘플링 컨볼루션 레이어이고, 17개 블록은 각각 네 개의 ResNet 레이어와 두 개의 Vision Transformer(ViT)를 포함하는 주요 블록입니다. 여기서 인코더는 이미지 표현을 낮은 해상도 이미지 표현으로 압축하고, 디코더는 낮은 해상도 이미지 표현을 원래의 더 높은 해상도 이미지 표현으로 해석합니다. 이로써 노이즈가 적은 이미지를 생성합니다.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e텍스트-인코더\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e텍스트-인코더는 입력 프롬프트를 임베딩 공간으로 변환하여 U-Net에 입력으로 제공합니다. Unet을 노이즈 제거 프로세스에 학습시킬 때 노이즈가 많은 latents를 안내하는 역할을 합니다. 텍스트-인코더는 일반적으로 간단한 트랜스포머 기반 인코더로, 입력 토큰 시퀀스를 잠재적인 텍스트-임베딩 시퀀스로 매핑합니다. Stable Diffusion은 새로운 텍스트 인코더를 학습하지 않고 이미 학습된 텍스트 인코더인 CLIP을 사용합니다. 텍스트 인코더는 입력 텍스트에 해당하는 임베딩을 생성합니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e토큰화\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCLIPTextModel\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eCLIPTokenizer\u003c/span\u003e\n\n# 토크나이저 및 텍스트 인코더를로드하여 텍스트를 토큰화하고 인코딩합니다.\ntokenizer = \u003cspan class=\"hljs-title class_\"\u003eCLIPTokenizer\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"openai/clip-vit-large-patch14\"\u003c/span\u003e)\ntext_encoder = \u003cspan class=\"hljs-title class_\"\u003eCLIPTextModel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"openai/clip-vit-large-patch14\"\u003c/span\u003e)\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e로 이동\ntext_encoder = text_encoder.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(torch_device)\n\nprompt = \u003cspan class=\"hljs-string\"\u003e'An astronaut riding a horse'\u003c/span\u003e\n# 텍스트를 토큰 시퀀스로 변환합니다.\ntext_input = \u003cspan class=\"hljs-title function_\"\u003etokenizer\u003c/span\u003e(prompt, padding=\u003cspan class=\"hljs-string\"\u003e\"max_length\"\u003c/span\u003e, max_length=tokenizer.\u003cspan class=\"hljs-property\"\u003emodel_max_length\u003c/span\u003e, truncation=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e, return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e)\ninput_ids = text_input.\u003cspan class=\"hljs-property\"\u003einput_ids\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(torch_device)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEmbedding 결과\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 토큰에서 출력 임베딩 가져오기\noutput_embeddings = \u003cspan class=\"hljs-title function_\"\u003etext_encoder\u003c/span\u003e(text_input.\u003cspan class=\"hljs-property\"\u003einput_ids\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eto\u003c/span\u003e(torch_device))[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'모양:'\u003c/span\u003e, output_embeddings.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다 모아보면, 모델은 추론 과정 중에 다음과 같이 작동합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-StableDiffusionExplained_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e스케줄러\u003c/p\u003e\n\u003cp\u003e위에서 언급된 3가지 외에도 이미지에 노이즈를 추가하고 모델을 사용하여 노이즈를 예측하는 데 사용되는 스케줄러가 있습니다.\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ediffusers에서 \u003cspan class=\"hljs-title class_\"\u003eLMSDiscreteScheduler\u003c/span\u003e를 가져와주세요\nscheduler = \u003cspan class=\"hljs-title class_\"\u003eLMSDiscreteScheduler\u003c/span\u003e(beta_start=\u003cspan class=\"hljs-number\"\u003e0.00085\u003c/span\u003e, beta_end=\u003cspan class=\"hljs-number\"\u003e0.012\u003c/span\u003e, beta_schedule=\u003cspan class=\"hljs-string\"\u003e\"scaled_linear\"\u003c/span\u003e, num_train_timesteps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위 코드는 모델을 훈련하는 데 사용되는 스케줄러를 설정합니다. 작은 단계 수에 대해 스케줄러를 설정하려면 다음과 같이 스케줄러를 설정하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 샘플링 단계 수를 설정하세요:\nscheduler.\u003cspan class=\"hljs-title function_\"\u003eset_timesteps\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStable Diffusion과 같은 Latent Diffusion Model은 다양한 창의적인 응용 프로그램을 가능하게 합니다:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e텍스트에서 이미지로 변환\u003c/li\u003e\n\u003cli\u003e이미지에서 이미지 생성 - 시작점을 기반으로 새 이미지를 생성하거나 수정\u003c/li\u003e\n\u003cli\u003e이미지 업스케일링 - 작은 이미지를 큰 이미지로 확대\u003c/li\u003e\n\u003cli\u003e인페인팅 - 이미지의 특정 영역을 마스킹하여 해당 영역에 새로운 디테일을 생성하는 것\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e잠재 확산 모델은 훈련 비용과 추론을 줄여주어 대량의 고해상도 이미지 합성을 대중화 할 수 있는 잠재력이 있습니다.\u003c/p\u003e\n\u003cp\u003e다음 블로그에서는 새로운 개념이나 작업을 학습하는데 안정적인 확산을 세밀하게 조정하는 텍스트 역전법에 대해 이야기할 예정입니다.\u003c/p\u003e\n\u003cp\u003e참고:\u003c/p\u003e\n\u003c!-- ui-station 사각형 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"7249294152\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e롬바흐, R., 블랫만, A., 로렌츠, D., 에셀, P., \u0026#x26; 오머, B. (2022). 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전과 패턴 인식 컨퍼런스 논문집 (pp. 10684–10695).\u003c/li\u003e\n\u003cli\u003e장, L., \u0026#x26; 아그라와라, M. (2023). 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://huggingface.co/docs/diffusers/index\" rel=\"nofollow\" target=\"_blank\"\u003eHugging Face Diffusers 문서\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-StableDiffusionExplained"},"buildId":"-dPCbnM2yhdKNgXe92VJV","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>