<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기 | ui-station</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///post/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기 | ui-station" data-gatsby-head="true"/><meta property="og:title" content="YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기 | ui-station" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///post/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer" data-gatsby-head="true"/><meta name="twitter:title" content="YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기 | ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="article:published_time" content="2024-05-18 18:03" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/162-4172e84c8e2aa747.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-4f7b40c1114f0d09.js" defer=""></script><script src="/_next/static/o1YmnmSuZvAX2O4TI9r41/_buildManifest.js" defer=""></script><script src="/_next/static/o1YmnmSuZvAX2O4TI9r41/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">UI STATION</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On May 18, 2024</span><span class="posts_reading_time__f7YPP">6<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.</p>
<p><img src="/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png" alt="YoutubeDataPipeline"></p>
<h2>사용 사례: —</h2>
<p>상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?</p>
<h2>제안된 해결책: —</h2>
<p>위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:</p>
<p>— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.</p>
<p>— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.</p>
<ul>
<li>데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.</li>
</ul>
<p>이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.</p>
<h2>구현</h2>
<p>구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.</p>
<p>도커 데스크톱을 설치해보세요 — <a href="https://www.docker.com/get-started/" rel="nofollow" target="_blank">https://www.docker.com/get-started/</a></p>
<p>머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.</p>
<pre><code class="hljs language-js">docker --version
<span class="hljs-title class_">Docker</span> version <span class="hljs-number">20.10</span><span class="hljs-number">.23</span>, build <span class="hljs-number">7155243</span>
</code></pre>
<p>최신 apache/airflow 이미지를 받아보세요</p>
<pre><code class="hljs language-sh">도커 pull apache/airflow
</code></pre>
<p>다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.</p>
<pre><code class="hljs language-sh">도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone
</code></pre>
<p>AWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기</p>
<p>config</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">[</span>default<span class="hljs-punctuation">]</span>
region = ap-south<span class="hljs-number">-1</span>
</code></pre>
<p>credentials</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">[</span>default<span class="hljs-punctuation">]</span>
aws_access_key_id = AKIB******AXPCMO
aws_secret_access_key = <span class="hljs-number">4</span>D7HkaIBsqu***********+<span class="hljs-number">0</span>AT2a8j
</code></pre>
<p>지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.</p>
<p>도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.</p>
<p><img src="/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png" alt="이미지"></p>
<p>웹 브라우저를 열고 <code>http://localhost:8080</code> 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.</p>
<p><code>aws_write_utility.py</code> 파일을 만들 때 평소처럼 진행하시면 됩니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> boto3
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> uuid

def <span class="hljs-title function_">write_json_to_s3</span>(json_data, bucket_name, key_name):

    # <span class="hljs-variable constant_">S3</span> 클라이언트 초기화
    s3 = boto3.<span class="hljs-title function_">client</span>(<span class="hljs-string">'s3'</span>)
    
    # <span class="hljs-title class_">JSON</span> 데이터를 바이트로 변환
    json_bytes = json.<span class="hljs-title function_">dumps</span>(json_data).<span class="hljs-title function_">encode</span>(<span class="hljs-string">'utf-8'</span>)
    
    # <span class="hljs-title class_">JSON</span> 데이터를 <span class="hljs-variable constant_">S3</span>에 쓰기
    s3.<span class="hljs-title function_">put_object</span>(<span class="hljs-title class_">Bucket</span>=bucket_name, <span class="hljs-title class_">Key</span>=key_name, <span class="hljs-title class_">Body</span>=json_bytes)


def <span class="hljs-title function_">generate_uuid</span>():
    <span class="hljs-string">""</span><span class="hljs-string">"UUID와 유사한 문자열 생성."</span><span class="hljs-string">""</span>
    <span class="hljs-keyword">return</span> <span class="hljs-title function_">str</span>(uuid.<span class="hljs-title function_">uuid4</span>())
</code></pre>
<p>youtube_comments.py</p>
<pre><code class="hljs language-js"># -*- <span class="hljs-attr">coding</span>: utf-<span class="hljs-number">8</span> -*-

# youtube.<span class="hljs-property">commentThreads</span>.<span class="hljs-property">list</span>를 위한 샘플 <span class="hljs-title class_">Python</span> 코드
# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:
# <span class="hljs-attr">https</span>:<span class="hljs-comment">//developers.google.com/explorer-help/code-samples#python</span>

<span class="hljs-keyword">import</span> os

<span class="hljs-keyword">import</span> googleapiclient.<span class="hljs-property">discovery</span>
<span class="hljs-keyword">import</span> aws_write_utility
<span class="hljs-keyword">from</span> aws_write_utility <span class="hljs-keyword">import</span> write_json_to_s3

def <span class="hljs-title function_">start_process</span>():
    # 로컬에서 실행 시 <span class="hljs-title class_">OAuthlib</span>의 <span class="hljs-variable constant_">HTTPS</span> 확인 비활성화
    # 제품 환경에서는 이 옵션을 활성화하지 마세요.
    os.<span class="hljs-property">environ</span>[<span class="hljs-string">"OAUTHLIB_INSECURE_TRANSPORT"</span>] = <span class="hljs-string">"1"</span>

    api_service_name = <span class="hljs-string">"youtube"</span>
    api_version = <span class="hljs-string">"v3"</span>
    <span class="hljs-variable constant_">DEVELOPER_KEY</span> = <span class="hljs-string">"AIzaS*****************PiwBdaP_IE"</span>

    youtube = googleapiclient.<span class="hljs-property">discovery</span>.<span class="hljs-title function_">build</span>(
        api_service_name, api_version, developerKey=<span class="hljs-variable constant_">DEVELOPER_KEY</span>)

    request = youtube.<span class="hljs-title function_">commentThreads</span>().<span class="hljs-title function_">list</span>(
        part=<span class="hljs-string">"snippet,replies"</span>,
        videoId=<span class="hljs-string">"r_K*****PKU"</span>
    )
    response = request.<span class="hljs-title function_">execute</span>()

    <span class="hljs-title function_">process_comments</span>(response)


def <span class="hljs-title function_">process_comments</span>(response_items):

    # 예시 <span class="hljs-variable constant_">S3</span> 버킷 및 키 이름
    bucket_name = <span class="hljs-string">'youtube-comments-analysis'</span>
    key_name = <span class="hljs-string">'data/{}.json'</span>.<span class="hljs-title function_">format</span>(aws_write_utility.<span class="hljs-title function_">generate_uuid</span>())

    comments = []
    <span class="hljs-keyword">for</span> comment <span class="hljs-keyword">in</span> response_items[<span class="hljs-string">'items'</span>]:
        author = comment[<span class="hljs-string">'snippet'</span>][<span class="hljs-string">'topLevelComment'</span>][<span class="hljs-string">'snippet'</span>][<span class="hljs-string">'authorDisplayName'</span>]
        comment_text = comment[<span class="hljs-string">'snippet'</span>][<span class="hljs-string">'topLevelComment'</span>][<span class="hljs-string">'snippet'</span>][<span class="hljs-string">'textOriginal'</span>]
        publish_time = comment[<span class="hljs-string">'snippet'</span>][<span class="hljs-string">'topLevelComment'</span>][<span class="hljs-string">'snippet'</span>][<span class="hljs-string">'publishedAt'</span>]
        comment_info = {<span class="hljs-string">'author'</span>: author, <span class="hljs-string">'comment'</span>: comment_text, <span class="hljs-string">'published_at'</span>: publish_time}
        comments.<span class="hljs-title function_">append</span>(comment_info)
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">'총 {len(comments)}개의 댓글 처리 완료.'</span>)
    <span class="hljs-title function_">write_json_to_s3</span>(comments, bucket_name, key_name)
</code></pre>
<p>youtube_dag.py</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime, timedelta
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.python_operator <span class="hljs-keyword">import</span> PythonOperator
<span class="hljs-keyword">from</span> youtube_comments <span class="hljs-keyword">import</span> start_process

<span class="hljs-comment"># 기본 인수 정의</span>
default_args = {
    <span class="hljs-string">'owner'</span>: <span class="hljs-string">'airflow'</span>,
    <span class="hljs-string">'depends_on_past'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">'start_date'</span>: datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">5</span>, <span class="hljs-number">16</span>),
    <span class="hljs-string">'email_on_failure'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">'email_on_retry'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">'retries'</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">'retry_delay'</span>: timedelta(minutes=<span class="hljs-number">5</span>),
}

<span class="hljs-comment"># DAG 객체 생성</span>
dag = DAG(
    <span class="hljs-string">'youtube_python_operator_dag'</span>,
    default_args=default_args,
    description=<span class="hljs-string">'Python 함수를 호출하는 간단한 DAG'</span>,
    schedule_interval=timedelta(days=<span class="hljs-number">1</span>),
)

<span class="hljs-comment"># PythonOperator 작업 생성</span>
python_task = PythonOperator(
    task_id=<span class="hljs-string">'my_python_task'</span>,
    python_callable=start_process,
    dag=dag,
)

<span class="hljs-comment"># 작업 간 의존성 정의</span>
python_task

<span class="hljs-comment"># DAG 등록</span>
dag
</code></pre>
<p>위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.</p>
<p>좋아요!!</p>
<p>이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.</p>
<p>이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"YouTube 데이터 파이프라인 구축하기 Docker 컨테이너에서 Airflow 사용하기","description":"","date":"2024-05-18 18:03","slug":"2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer","content":"\n\n그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.\n\n![YoutubeDataPipeline](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png)\n\n## 사용 사례: —\n\n상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 제안된 해결책: —\n\n위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:\n\n— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.\n\n— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.\n\n이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.\n\n## 구현\n\n구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n도커 데스크톱을 설치해보세요 — https://www.docker.com/get-started/\n\n머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.\n\n```js\ndocker --version\nDocker version 20.10.23, build 7155243\n```\n\n최신 apache/airflow 이미지를 받아보세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```sh\n도커 pull apache/airflow\n```\n\n다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.\n\n```sh\n도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone\n```\n\nAWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nconfig\n\n```json\n[default]\nregion = ap-south-1\n```\n\ncredentials\n\n```json\n[default]\naws_access_key_id = AKIB******AXPCMO\naws_secret_access_key = 4D7HkaIBsqu***********+0AT2a8j\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.\n\n도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.\n\n![이미지](/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png)\n\n웹 브라우저를 열고 `http://localhost:8080` 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`aws_write_utility.py` 파일을 만들 때 평소처럼 진행하시면 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```js\nimport boto3\nimport json\nimport uuid\n\ndef write_json_to_s3(json_data, bucket_name, key_name):\n\n    # S3 클라이언트 초기화\n    s3 = boto3.client('s3')\n    \n    # JSON 데이터를 바이트로 변환\n    json_bytes = json.dumps(json_data).encode('utf-8')\n    \n    # JSON 데이터를 S3에 쓰기\n    s3.put_object(Bucket=bucket_name, Key=key_name, Body=json_bytes)\n\n\ndef generate_uuid():\n    \"\"\"UUID와 유사한 문자열 생성.\"\"\"\n    return str(uuid.uuid4())\n```\n\nyoutube_comments.py\n\n```js\n# -*- coding: utf-8 -*-\n\n# youtube.commentThreads.list를 위한 샘플 Python 코드\n# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:\n# https://developers.google.com/explorer-help/code-samples#python\n\nimport os\n\nimport googleapiclient.discovery\nimport aws_write_utility\nfrom aws_write_utility import write_json_to_s3\n\ndef start_process():\n    # 로컬에서 실행 시 OAuthlib의 HTTPS 확인 비활성화\n    # 제품 환경에서는 이 옵션을 활성화하지 마세요.\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    DEVELOPER_KEY = \"AIzaS*****************PiwBdaP_IE\"\n\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, developerKey=DEVELOPER_KEY)\n\n    request = youtube.commentThreads().list(\n        part=\"snippet,replies\",\n        videoId=\"r_K*****PKU\"\n    )\n    response = request.execute()\n\n    process_comments(response)\n\n\ndef process_comments(response_items):\n\n    # 예시 S3 버킷 및 키 이름\n    bucket_name = 'youtube-comments-analysis'\n    key_name = 'data/{}.json'.format(aws_write_utility.generate_uuid())\n\n    comments = []\n    for comment in response_items['items']:\n        author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n        comment_text = comment['snippet']['topLevelComment']['snippet']['textOriginal']\n        publish_time = comment['snippet']['topLevelComment']['snippet']['publishedAt']\n        comment_info = {'author': author, 'comment': comment_text, 'published_at': publish_time}\n        comments.append(comment_info)\n    print(f'총 {len(comments)}개의 댓글 처리 완료.')\n    write_json_to_s3(comments, bucket_name, key_name)\n```\n\nyoutube_dag.py\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom youtube_comments import start_process\n\n# 기본 인수 정의\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 5, 16),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DAG 객체 생성\ndag = DAG(\n    'youtube_python_operator_dag',\n    default_args=default_args,\n    description='Python 함수를 호출하는 간단한 DAG',\n    schedule_interval=timedelta(days=1),\n)\n\n# PythonOperator 작업 생성\npython_task = PythonOperator(\n    task_id='my_python_task',\n    python_callable=start_process,\n    dag=dag,\n)\n\n# 작업 간 의존성 정의\npython_task\n\n# DAG 등록\ndag\n```\n\n위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.\n\n좋아요!!\n\n이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.","ogImage":{"url":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png"},"coverImage":"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png","tag":["Tech"],"readingTime":6},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e그게 많은 양이겠죠! 조금씩 나눠서 살펴봐요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_0.png\" alt=\"YoutubeDataPipeline\"\u003e\u003c/p\u003e\n\u003ch2\u003e사용 사례: —\u003c/h2\u003e\n\u003cp\u003e상상해봐요! 성장하는 YouTube 채널을 운영하는 콘텐츠 크리에이터라고 상상해봐요. 시청자들의 댓글과 답글을 통해 시청자를 이해하는 것은 귀중한 통찰력을 제공할 수 있어요. 그러나 수많은 동영상의 댓글을 수동으로 분류하는 것은 지칠 수 있죠. 이 프로세스를 자동화할 수 있는 방법이 있다면 어떨까요?\u003c/p\u003e\n\u003ch2\u003e제안된 해결책: —\u003c/h2\u003e\n\u003cp\u003e위의 그림을 보시면, YouTube 비디오에서 댓글과 답글을 추출하기 위한 자동화된 솔루션을 안내해 드리겠습니다. 이 과정에는 여러 가지 주요 구성 요소가 포함됩니다:\u003c/p\u003e\n\u003cp\u003e— YouTube 데이터 API용 Python 라이브러리: YouTube 데이터 API와 상호 작용하기 위해 Python 라이브러리를 사용하여 댓글과 답글을 프로그래밍 방식으로 가져올 수 있습니다.\u003c/p\u003e\n\u003cp\u003e— 작업 관리를 위한 Airflow: 데이터 추출 및 처리 작업을 체계적으로 관리하기 위해 Apache Airflow를 Docker 컨테이너 내에서 사용할 것입니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e데이터 저장을 위한 AWS S3: 마지막으로, boto3 라이브러리를 사용하여 처리된 데이터를 AWS S3에 저장하게 됩니다. 나중에 쉽게 액세스하고 분석할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 솔루션은 추출 프로세스를 자동화하는 데 그치지 않고 데이터가 구성되어 안전하게 저장되어 나중에 깊이 있는 분석을 위해 준비되어 있음을 보장합니다. 이제 이 워크플로우를 설정하고 실행하는 자세한 내용을 살펴보겠습니다.\u003c/p\u003e\n\u003ch2\u003e구현\u003c/h2\u003e\n\u003cp\u003e구현은 주로 두 가지 작업으로 구성되어 있습니다. 첫 번째는 인프라 구축, 두 번째는 코드 작업입니다. 그래서 이제 인프라 설정을 먼저 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003e도커 데스크톱을 설치해보세요 — \u003ca href=\"https://www.docker.com/get-started/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://www.docker.com/get-started/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e머신에 도커를 설치한 후에는 다음 명령어를 확인하여 설치가 성공적으로 이루어졌는지 확인하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edocker --version\n\u003cspan class=\"hljs-title class_\"\u003eDocker\u003c/span\u003e version \u003cspan class=\"hljs-number\"\u003e20.10\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.23\u003c/span\u003e, build \u003cspan class=\"hljs-number\"\u003e7155243\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e최신 apache/airflow 이미지를 받아보세요\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-sh\"\u003e도커 pull apache/airflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e다음 명령어를 사용하여 아파치 에어플로우 컨테이너를 시작하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-sh\"\u003e도커 run -p 8080:8080 -v /Users/local_user/airflow/dags:/opt/airflow/dags -v /Users/local_user/airflow/creds:/home/airflow/.aws -d apache/airflow standalone\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAWS 자격 증명을 생성하여 원격 s3 버킷과 통신하여 날짜를 쓸 수 있습니다. 다음 링크를 통해 생성하세요 — 루트 사용자를 위한 액세스 키 생성하기\u003c/p\u003e\n\u003cp\u003econfig\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-json\"\u003e\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003edefault\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e\nregion = ap-south\u003cspan class=\"hljs-number\"\u003e-1\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecredentials\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-json\"\u003e\u003cspan class=\"hljs-punctuation\"\u003e[\u003c/span\u003edefault\u003cspan class=\"hljs-punctuation\"\u003e]\u003c/span\u003e\naws_access_key_id = AKIB******AXPCMO\naws_secret_access_key = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003eD7HkaIBsqu***********+\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003eAT2a8j\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e지역 사용자의 경우 두 파일을 로컬 머신의 /Users/local_user/airflow/creds/ 폴더로 복사해주세요. 이렇게 함으로써 이 파일들이 컨테이너에서 /home/airflow/.aws/ 경로에 마운트되도록 할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e도커 데스크톱 애플리케이션을 열고 Airflow 컨테이너를 선택해주세요. 컨테이너 내에서 standalone_admin_password.txt 파일을 찾아주세요. 이 파일을 열고 Airflow 포털에 로그인하기 위한 비밀번호를 복사해주세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e웹 브라우저를 열고 \u003ccode\u003ehttp://localhost:8080\u003c/code\u003e 주소로 이동해주세요. username에 admin을 입력하고 이전 단계에서 복사한 비밀번호로 로그인해주세요.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eaws_write_utility.py\u003c/code\u003e 파일을 만들 때 평소처럼 진행하시면 됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e boto3\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e uuid\n\ndef \u003cspan class=\"hljs-title function_\"\u003ewrite_json_to_s3\u003c/span\u003e(json_data, bucket_name, key_name):\n\n    # \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 클라이언트 초기화\n    s3 = boto3.\u003cspan class=\"hljs-title function_\"\u003eclient\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e's3'\u003c/span\u003e)\n    \n    # \u003cspan class=\"hljs-title class_\"\u003eJSON\u003c/span\u003e 데이터를 바이트로 변환\n    json_bytes = json.\u003cspan class=\"hljs-title function_\"\u003edumps\u003c/span\u003e(json_data).\u003cspan class=\"hljs-title function_\"\u003eencode\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'utf-8'\u003c/span\u003e)\n    \n    # \u003cspan class=\"hljs-title class_\"\u003eJSON\u003c/span\u003e 데이터를 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e에 쓰기\n    s3.\u003cspan class=\"hljs-title function_\"\u003eput_object\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eBucket\u003c/span\u003e=bucket_name, \u003cspan class=\"hljs-title class_\"\u003eKey\u003c/span\u003e=key_name, \u003cspan class=\"hljs-title class_\"\u003eBody\u003c/span\u003e=json_bytes)\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003egenerate_uuid\u003c/span\u003e():\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"UUID와 유사한 문자열 생성.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(uuid.\u003cspan class=\"hljs-title function_\"\u003euuid4\u003c/span\u003e())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eyoutube_comments.py\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# -*- \u003cspan class=\"hljs-attr\"\u003ecoding\u003c/span\u003e: utf-\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e -*-\n\n# youtube.\u003cspan class=\"hljs-property\"\u003ecommentThreads\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003elist\u003c/span\u003e를 위한 샘플 \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e 코드\n# 이 코드 샘플을 로컬에서 실행하는 방법은 다음 링크를 참고하세요:\n# \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//developers.google.com/explorer-help/code-samples#python\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e googleapiclient.\u003cspan class=\"hljs-property\"\u003ediscovery\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e aws_write_utility\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e aws_write_utility \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e write_json_to_s3\n\ndef \u003cspan class=\"hljs-title function_\"\u003estart_process\u003c/span\u003e():\n    # 로컬에서 실행 시 \u003cspan class=\"hljs-title class_\"\u003eOAuthlib\u003c/span\u003e의 \u003cspan class=\"hljs-variable constant_\"\u003eHTTPS\u003c/span\u003e 확인 비활성화\n    # 제품 환경에서는 이 옵션을 활성화하지 마세요.\n    os.\u003cspan class=\"hljs-property\"\u003eenviron\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e\"OAUTHLIB_INSECURE_TRANSPORT\"\u003c/span\u003e] = \u003cspan class=\"hljs-string\"\u003e\"1\"\u003c/span\u003e\n\n    api_service_name = \u003cspan class=\"hljs-string\"\u003e\"youtube\"\u003c/span\u003e\n    api_version = \u003cspan class=\"hljs-string\"\u003e\"v3\"\u003c/span\u003e\n    \u003cspan class=\"hljs-variable constant_\"\u003eDEVELOPER_KEY\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"AIzaS*****************PiwBdaP_IE\"\u003c/span\u003e\n\n    youtube = googleapiclient.\u003cspan class=\"hljs-property\"\u003ediscovery\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ebuild\u003c/span\u003e(\n        api_service_name, api_version, developerKey=\u003cspan class=\"hljs-variable constant_\"\u003eDEVELOPER_KEY\u003c/span\u003e)\n\n    request = youtube.\u003cspan class=\"hljs-title function_\"\u003ecommentThreads\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\n        part=\u003cspan class=\"hljs-string\"\u003e\"snippet,replies\"\u003c/span\u003e,\n        videoId=\u003cspan class=\"hljs-string\"\u003e\"r_K*****PKU\"\u003c/span\u003e\n    )\n    response = request.\u003cspan class=\"hljs-title function_\"\u003eexecute\u003c/span\u003e()\n\n    \u003cspan class=\"hljs-title function_\"\u003eprocess_comments\u003c/span\u003e(response)\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003eprocess_comments\u003c/span\u003e(response_items):\n\n    # 예시 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 버킷 및 키 이름\n    bucket_name = \u003cspan class=\"hljs-string\"\u003e'youtube-comments-analysis'\u003c/span\u003e\n    key_name = \u003cspan class=\"hljs-string\"\u003e'data/{}.json'\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eformat\u003c/span\u003e(aws_write_utility.\u003cspan class=\"hljs-title function_\"\u003egenerate_uuid\u003c/span\u003e())\n\n    comments = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e comment \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e response_items[\u003cspan class=\"hljs-string\"\u003e'items'\u003c/span\u003e]:\n        author = comment[\u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'topLevelComment'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'authorDisplayName'\u003c/span\u003e]\n        comment_text = comment[\u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'topLevelComment'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'textOriginal'\u003c/span\u003e]\n        publish_time = comment[\u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'topLevelComment'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'snippet'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'publishedAt'\u003c/span\u003e]\n        comment_info = {\u003cspan class=\"hljs-string\"\u003e'author'\u003c/span\u003e: author, \u003cspan class=\"hljs-string\"\u003e'comment'\u003c/span\u003e: comment_text, \u003cspan class=\"hljs-string\"\u003e'published_at'\u003c/span\u003e: publish_time}\n        comments.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(comment_info)\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'총 {len(comments)}개의 댓글 처리 완료.'\u003c/span\u003e)\n    \u003cspan class=\"hljs-title function_\"\u003ewrite_json_to_s3\u003c/span\u003e(comments, bucket_name, key_name)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eyoutube_dag.py\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datetime \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datetime, timedelta\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e airflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e DAG\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e airflow.operators.python_operator \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e PythonOperator\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e youtube_comments \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e start_process\n\n\u003cspan class=\"hljs-comment\"\u003e# 기본 인수 정의\u003c/span\u003e\ndefault_args = {\n    \u003cspan class=\"hljs-string\"\u003e'owner'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'airflow'\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'depends_on_past'\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'start_date'\u003c/span\u003e: datetime(\u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e),\n    \u003cspan class=\"hljs-string\"\u003e'email_on_failure'\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'email_on_retry'\u003c/span\u003e: \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'retries'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n    \u003cspan class=\"hljs-string\"\u003e'retry_delay'\u003c/span\u003e: timedelta(minutes=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e),\n}\n\n\u003cspan class=\"hljs-comment\"\u003e# DAG 객체 생성\u003c/span\u003e\ndag = DAG(\n    \u003cspan class=\"hljs-string\"\u003e'youtube_python_operator_dag'\u003c/span\u003e,\n    default_args=default_args,\n    description=\u003cspan class=\"hljs-string\"\u003e'Python 함수를 호출하는 간단한 DAG'\u003c/span\u003e,\n    schedule_interval=timedelta(days=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e),\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# PythonOperator 작업 생성\u003c/span\u003e\npython_task = PythonOperator(\n    task_id=\u003cspan class=\"hljs-string\"\u003e'my_python_task'\u003c/span\u003e,\n    python_callable=start_process,\n    dag=dag,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# 작업 간 의존성 정의\u003c/span\u003e\npython_task\n\n\u003cspan class=\"hljs-comment\"\u003e# DAG 등록\u003c/span\u003e\ndag\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위의 .py 파일을 로컬 머신의 /Users/local_user/airflow/dags로 복사하세요. 이렇게 함으로써 컨테이너 내의 경로 /opt/airflow/dags로 마운트됩니다.\u003c/p\u003e\n\u003cp\u003e좋아요!!\u003c/p\u003e\n\u003cp\u003e이제 Airflow에서 DAG 페이지를 새로고침하세요. 위의 DAG가 표시될 것입니다. 실행해보고 문제가 있는지 로그를 확인해보세요. 녹색으로 변하면 작업이 완료된 것입니다.\u003c/p\u003e\n\u003cp\u003e이 구현은 AWS EC2 인스턴스에 Airflow를 설정하여 수행할 수도 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-05-18-YoutubeDataPipelineusingAirflowinDockerContainer"},"buildId":"o1YmnmSuZvAX2O4TI9r41","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>