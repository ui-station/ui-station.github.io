<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/50" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/50" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/o1YmnmSuZvAX2O4TI9r41/_buildManifest.js" defer=""></script><script src="/_next/static/o1YmnmSuZvAX2O4TI9r41/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label=" 쿠버네티스에서 Vault 사용 방법 안내 " href="/post/2024-05-23-AHand-OnGuidetoVaultinKubernetes"><div class="PostList_thumbnail_wrap__YuxdB"><img alt=" 쿠버네티스에서 Vault 사용 방법 안내 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt=" 쿠버네티스에서 Vault 사용 방법 안내 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl"> 쿠버네티스에서 Vault 사용 방법 안내 </strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="파이썬 제너레이터 데이터베이스에서 효율적으로 데이터를 가져오는 방법" href="/post/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="파이썬 제너레이터 데이터베이스에서 효율적으로 데이터를 가져오는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="파이썬 제너레이터 데이터베이스에서 효율적으로 데이터를 가져오는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">파이썬 제너레이터 데이터베이스에서 효율적으로 데이터를 가져오는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="컨테이너 세계에서 Runc 대 Crun" href="/post/2024-05-23-RuncvsCrunincontainersworld"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="컨테이너 세계에서 Runc 대 Crun" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-RuncvsCrunincontainersworld_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="컨테이너 세계에서 Runc 대 Crun" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">컨테이너 세계에서 Runc 대 Crun</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="도커 대 Podman 안전한 오케스트레이션의 새 시대" href="/post/2024-05-23-DockervsPodmanANewErainSecureOrchestration"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="도커 대 Podman 안전한 오케스트레이션의 새 시대" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="도커 대 Podman 안전한 오케스트레이션의 새 시대" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">도커 대 Podman 안전한 오케스트레이션의 새 시대</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="러스트 배우기 11부  빌더와 데이터베이스 상호작용" href="/post/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="러스트 배우기 11부  빌더와 데이터베이스 상호작용" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="러스트 배우기 11부  빌더와 데이터베이스 상호작용" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">러스트 배우기 11부  빌더와 데이터베이스 상호작용</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데브 컨테이너로 Rails 앱을 도커라이즈하기" href="/post/2024-05-23-DockerizeRailsappwithDevContainers"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데브 컨테이너로 Rails 앱을 도커라이즈하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데브 컨테이너로 Rails 앱을 도커라이즈하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">데브 컨테이너로 Rails 앱을 도커라이즈하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake" href="/post/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터베이스DBT 간단 정리" href="/post/2024-05-23-DBTinaNutshell"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터베이스DBT 간단 정리" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DBTinaNutshell_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터베이스DBT 간단 정리" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">데이터베이스DBT 간단 정리</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리" href="/post/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터 엔지니어링 디자인 패턴" href="/post/2024-05-23-DataEngineeringDesignPatterns"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터 엔지니어링 디자인 패턴" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터 엔지니어링 디자인 패턴" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">데이터 엔지니어링 디자인 패턴</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link posts_-active__YVJEi" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":" 쿠버네티스에서 Vault 사용 방법 안내 ","description":"","date":"2024-05-23 14:19","slug":"2024-05-23-AHand-OnGuidetoVaultinKubernetes","content":"\n## ⇢ 실용적인 예제로 HashiCorp Vault를 사용하여 k8s Secrets 관리하기\n\n![image](/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_0.png)\n\n쿠버네티스 세계에서 API 키, 비밀번호 및 기타 중요 정보와 같은 보안 정보를 관리하는 것은 매우 중요한 작업입니다. 쿠버네티스에는 내장된 보안 정보 관리 메커니즘이 있지만, 모든 조직의 보안 요구 사항을 충족시키지 못할 수도 있는 특정 제한 사항이 있습니다. 예를 들어, 쿠버네티스 보안 정보는 etcd에 저장되며, 이는 휴식 중 암호화되어 있지만, 매우 중요한 정보에 필요한 보안 수준과 접근 제어를 제공하지 못할 수 있습니다.\n\n이때 HashiCorp Vault가 등장합니다. Vault는 중요한 정보를 안전하게 저장하고 관리하기 위해 설계된 도구입니다. 동적 보안 정보, 서비스로의 암호화 및 접근 제어를 위한 견고한 메커니즘을 제공하여, 쿠버네티스 환경에서 보안 정보를 관리하기에 이상적인 솔루션입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 튜토리얼에서는 Helm을 사용하여 쿠버네티스 클러스터에 Vault를 설치하고 구성하는 단계를 안내합니다. 그리고 Pod를 배포하여 Vault에서 비밀을 액세스할 수 있도록합니다. 이 안내서를 마치면 쿠버네티스 클러스터에 작동하는 Vault 설정이 완료되어 응용 프로그램 비밀을 안전하게 관리할 수 있습니다.\n\n# 전제 조건\n\n시작하기 전에 다음 사항을 확인하세요:\n\n- 실행 중인 쿠버네티스 클러스터가 있어야 합니다.\n- 클러스터와 상호 작용하도록 구성된 kubectl이 있어야 합니다.\n- 로컬 머신에 Helm이 설치되어 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"https://miro.medium.com/v2/resize:fit:1400/1*jmt8bsoEGeVHv5ZUP7XY_Q.gif\" /\u003e\n\n# 보르트(Namespace) 네임스페이스 생성하기\n\n우선, 보르트(Vault)를 위한 별도의 네임스페이스를 생성해야 합니다. 이렇게 하면 보르트에 특화된 리소스를 독립적으로 관리할 수 있습니다.\n\n```js\n$ kubectl create ns vault\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Vault 설치\n\n우리는 HashiCorp가 제공하는 Helm 차트를 사용하여 Vault의 최신 버전을 설치할 것입니다. 이 작업을 수행하는 두 가지 방법이 있습니다: 1. HashiCorp Helm 리포지토리를 사용하여 직접 Helm 설치 명령어를 실행하거나 2. Helm 차트를 다운로드하여 로컬로 설치하는 방법이 있습니다.\n\n## 1. HashiCorp Helm 리포지토리 추가\n\nHashiCorp Helm 리포지토리를 Helm 구성에 추가해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nhelm repo add hashicorp https://helm.releases.hashicorp.com\n```\n\n## 2. 설치 방법\n\n1. 직접 Helm 설치 실행하기\n\n다음 명령어를 사용하여 HashiCorp 저장소에서 Helm 차트를 사용하여 Vault를 직접 설치할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nhelm install vault hashicorp/vault \\\n       --set='server.dev.enabled=true' \\\n       --set='ui.enabled=true' \\\n       --set='ui.serviceType=LoadBalancer' \\\n       --namespace vault\n```\n\n2. Helm 차트 다운로드 및 설치\n\n대안으로 Helm 차트를 다운로드하고 로컬로 설치할 수 있습니다:\n\n```js\n# Helm 차트 다운로드\nhelm pull hashicorp/vault --untar\n\n# 다운로드한 차트를 사용하여 Vault 설치\nhelm install vault \\\n       --set='server.dev.enabled=true' \\\n       --set='ui.enabled=true' \\\n       --set='ui.serviceType=LoadBalancer' \\\n       --namespace vault \\\n       ./vault-chart\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 설정을 사용하여 UI가 활성화된 Vault를 개발 모드로 설치하고 외부에서 액세스하기 위해 LoadBalancer 서비스를 통해 노출합니다. 이 설정은 테스트 및 개발 목적으로 이상적입니다.\n\n결과:\n\n```js\n$ kubectl get all -n vault\nNAME                                        READY   STATUS    RESTARTS   AGE\npod/vault-0                                 1/1     Running   0          2m39s\npod/vault-agent-injector-8497dd4457-8jgcm   1/1     Running   0          2m39s\n\nNAME                               TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)             AGE\nservice/vault                      ClusterIP      10.245.225.169   \u003cnone\u003e         8200/TCP,8201/TCP   2m40s\nservice/vault-agent-injector-svc   ClusterIP      10.245.32.56     \u003cnone\u003e         443/TCP             2m40s\nservice/vault-internal             ClusterIP      None             \u003cnone\u003e         8200/TCP,8201/TCP   2m40s\nservice/vault-ui                   LoadBalancer   10.245.103.246   24.132.59.59   8200:31764/TCP      2m40s\n\nNAME                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/vault-agent-injector   1/1     1            1           2m40s\n\nNAME                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/vault-agent-injector-8497dd4457   1         1         1       2m40s\n\nNAME                     READY   AGE\nstatefulset.apps/vault   1/1     2m40s\n```\n\n# Vault 구성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이번 단계에서는 Kubernetes 클러스터 내에서 안전하게 비밀을 관리하고 액세스하기 위해 Vault 정책과 인증 방법을 설정할 것입니다. 이 구성은 인증된 애플리케이션만 Vault에서 민감한 데이터를 검색할 수 있도록 보장합니다.\n\n## 1. Vault Pod에 연결하기\n\n설치가 완료된 후 Vault pod에 연결하여 초기 구성을 수행하세요:\n\n```js\nkubectl exec -it vault-0 -- /bin/sh\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 정책 생성 및 적용하기\n\n이제 비밀을 읽을 수 있는 정책을 생성하겠습니다. 이 정책은 역할에 첨부되어 특정 Kubernetes 서비스 계정에 액세스 권한을 부여하는 데 사용될 수 있습니다.\n\n정책 파일을 생성하세요:\n\n```js\ncat \u003c\u003cEOF \u003e /home/vault/read-policy.hcl\npath \"secret*\" {\n  capabilities = [\"read\"]\n}\nEOF\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래와 같이 정책을 적용해주세요:\n\n```js\n# 문법\n$ vault policy write \u003c정책명\u003e /정책/경로/여기에.hcl\n\n# 예시\n$ vault policy write read-policy /home/vault/read-policy.hcl\n```\n\n## 3. Kubernetes 인증 활성화\n\nVault에서 Kubernetes 인증 방법을 활성화하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nvault auth enable kubernetes\n```\n\n## 4. Kubernetes 인증 설정\n\nVault가 Kubernetes API 서버와 통신하도록 구성합니다:\n\n```js\nvault write auth/kubernetes/config \\\n   token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n   kubernetes_host=https://${KUBERNETES_PORT_443_TCP_ADDR}:443 \\\n   kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 5. 역할 생성\n\n특정 네임스페이스에 있는 쿠버네티스 서비스 계정(vault-serviceaccount)에 위에서 만든 정책을 바인딩하는 역할(vault-role)을 생성합니다. 이를 통해 서비스 계정이 Vault에 저장된 시크릿에 액세스할 수 있게 됩니다:\n\n```js\nvault write auth/kubernetes/role/vault-role \\\n   bound_service_account_names=vault-serviceaccount \\\n   bound_service_account_namespaces=vault \\\n   policies=read-policy \\\n   ttl=1h\n```\n\n여기서 여러 개의 서비스 계정과 네임스페이스를 전달할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\nvault write auth/kubernetes/role/\u003cmy-role\u003e \\\n   bound_service_account_names=sa1, sa2 \\\n   bound_service_account_namespaces=namespace1, namespace2 \\\n   policies=\u003cpolicy-name\u003e \\\n   ttl=1h\n```\n\n# 보안 정보 만들기\n\n이제 Vault에 일부 보안 정보를 만들어 보겠습니다:\n\n우리는 두 가지 방법으로 보안 정보를 만들 수 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Vault CLI를 사용하기\n\n## 1. Vault CLI 사용하기\n\n아래 명령어를 사용하여 시크릿을 생성하세요\n\n```js\n$ vault kv put secret/login pattoken=ytbuytbytbf765rb65u56rv\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 명령어를 사용하여 시크릿을 나열하여 비밀을 확인할 수 있습니다:\n\n```js\n$ vault kv list secret\nKeys\n----\nlogin\n```\n\n## 2. Vault UI 사용 방법\n\nVault 네임스페이스에서 서비스를 나열하여 로드 밸런서의 외부 IP를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n$ kubectl get svc -n vault\nNAME                       TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)             AGE\nvault                      ClusterIP      10.245.139.117   \u003cnone\u003e         8200/TCP,8201/TCP   28h\nvault-agent-injector-svc   ClusterIP      10.245.58.140    \u003cnone\u003e         443/TCP             28h\nvault-internal             ClusterIP      None             \u003cnone\u003e         8200/TCP,8201/TCP   28h\nvault-ui                   LoadBalancer   10.245.11.13     24.123.49.59   8200:32273/TCP      26h\n```\n\n위의 로드밸런서의 외부 IP를 사용하여 Vault UI에 액세스하실 수 있습니다.\n\n예: `external-ip`:8200\n\n제 경우: 24.123.49.59:8200\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n\u003cimg src=\"/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_1.png\" /\u003e\n\n이제 토큰 방법을 사용하여 Vault에 로그인할 수 있습니다. 초기에는 Token=root를 사용하여 로그인하십시오.\n\n이제 Vault UI에서 시크릿 대시보드를 사용하여 시크릿을 생성할 수 있습니다.\n\n시크릿 엔진으로 이동하세요 '` 시크릿`\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_2.png\" /\u003e\n\n그런 다음 오른쪽 상단의 \"비밀 생성(Create Secret)\"을 클릭하세요.\n\n\u003cimg src=\"/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_3.png\" /\u003e\n\n이제 비밀을 만들기 위해 원하는 필드를 입력하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 명령어를 사용하여 Vault CLI에서 위의 비밀을 액세스할 수도 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n$ vault kv list secret\nKeys\n----\nlogin\nmy-first-secret\n```\n\n쿠버네티스 클러스터에 Vault를 성공적으로 설치하고 구성했습니다. 이제 Vault를 사용하여 쿠버네티스에서 실행 중인 응용 프로그램의 비밀을 관리할 수 있습니다.\n\n# 쿠버네티스 Pod에서 비밀 액세스\n\n위 단계를 사용하여 Vault를 설치하고 Vault 역할(vault-role)을 구성하여 서비스 계정(vault-serviceaccount)이 Vault에 저장된 비밀에 액세스할 수 있도록 했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또한, login과 my-first-secret이라는 키-값 쌍을 가진 두 개의 시크릿을 생성했습니다. 이제 간단한 쿠버네티스 배포를 생성하고 이러한 시크릿에 액세스해 보겠습니다.\n\n먼저, vault 네임스페이스에 vault-serviceaccount라는 서비스 계정을 생성합니다. 이 서비스 계정은 위에서 정의한 \"Role 생성\" 단계에서 정의된 Vault 역할에 대한 권한이 부여됩니다.\n\n아래 매니페스트 파일을 vault-sa.yaml로 저장합니다.\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: vault-serviceaccount\n  labels:\n    app: read-vault-secret\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 명령을 사용하여 위에 제공된 매니페스트를 적용하세요.\n\n```js\nkubectl apply -f vault-sa.yaml\n```\n\n이제 아래 매니페스트 파일을 사용하여 간단한 배포(vault-secret-test-deploy.yaml)를 생성해 봅시다.\n\n이 배포 매니페스트는 Vault에서 시크릿을 안전하게 가져오도록 구성된 Nginx 파드의 단일 레플리카를 생성합니다. Vault 에이전트는 지정된 템플릿에 따라 시크릿인 login 및 my-first-secret을 파드에 주입합니다. 시크릿은 파드 파일 시스템에 저장되어 컨테이너에서 실행 중인 응용 프로그램에서 액세스할 수 있습니다. Vault와 인증하기 위해 필요한 권한을 갖고 있는 vault-serviceaccount 서비스 어카운트가 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어노테이션 섹션을 자세히 살펴보면 그 목적과 기능을 이해할 수 있습니다.\n\n```js\nannotations:\n        vault.hashicorp.com/agent-inject: \"true\"\n        vault.hashicorp.com/agent-inject-status: \"update\"\n        vault.hashicorp.com/agent-inject-secret-login: \"secret/login\"\n        vault.hashicorp.com/agent-inject-template-login: |\n          {- with secret \"secret/login\" -}\n          pattoken={ .Data.data.pattoken }\n          {- end }\n        vault.hashicorp.com/agent-inject-secret-my-first-secret: \"secret/my-first-secret\"\n        vault.hashicorp.com/agent-inject-template-my-first-secret: |\n          {- with secret \"secret/my-first-secret\" -}\n          username={ .Data.data.username }\n          password={ .Data.data.password }\n          {- end }\n        vault.hashicorp.com/role: \"vault-role\"\n```\n\n이러한 어노테이션은 Vault 에이전트를 구성하여 시크릿을 파드 볼륨에 주입하는 데 사용됩니다.\n\n- vault.hashicorp.com/agent-inject: “true”: 이 파드에 대한 Vault 에이전트 주입을 활성화합니다.\n- vault.hashicorp.com/agent-inject-status: “update”: 시크릿 주입 상태가 업데이트되도록 보장합니다.\n- vault.hashicorp.com/agent-inject-secret-login: “secret/login”: Vault에 저장된 secret/login의 시크릿을 주입해야 함을 지정합니다.\n- vault.hashicorp.com/agent-inject-template-login: 주입된 로그인 시크릿의 템플릿을 정의하여 시크릿이 기록될 형식을 지정합니다.\n- vault.hashicorp.com/agent-inject-secret-my-first-secret: “secret/my-first-secret”: Vault에 저장된 secret/my-first-secret의 시크릿을 주입해야 함을 지정합니다.\n- vault.hashicorp.com/agent-inject-template-my-first-secret: 주입된 my-first-secret에 대한 템플릿을 정의하여 시크릿이 기록될 형식을 지정합니다.\n- vault.hashicorp.com/role: “vault-role”: 인증에 사용될 Vault 역할을 지정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 명령어를 사용하여 pod 볼륨에서 Vault 시크릿을 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n$ kubectl exec -it vault-test-84d9dc9986-gcxfv -- sh -c \"cat /vault/secrets/login \u0026\u0026 cat /vault/secrets/my-first-secret\" -n vault\n```\n\n```js\n$ kubectl exec -it vault-test-84d9dc9986-gcxfv -- sh -c \"cat /vault/secrets/login \u0026\u0026 cat /vault/secrets/my-first-secret\" -n vault\n\nDefaulted container \"nginx\" out of: nginx, vault-agent, vault-agent-init (init)\npattoken=ytbuytbytbf765rb65u56rv\nusername=anvesh\npassword=anveshpassword\n```\n\n완료되었습니다! Vault에 시크릿을 성공적으로 생성하고 해당 시크릿을 팟 내에서 활용했습니다.\n\n# 소스 코드\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 한국어로 번역하였습니다.\n\n친구야! 당신을 우리의 GitHub 저장소로 초대합니다. 거기에는 Kubernetes용 소스 코드의 포괄적인 컬렉션이 저장되어 있어요.\n\n또한, 여러분의 피드백과 제안을 환영합니다! 문제가 발생하거나 개선 아이디어가 있다면, 저희의 GitHub 저장소에서 issue를 열어주세요. 🚀\n\n\u003cimg src=\"/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_6.png\" /\u003e\n\n# Connect With Me\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 블로그를 유익하게 찾으셨고 AWS, 클라우드 전략, Kubernetes 또는 관련된 모든 주제에 대해 더 깊이 알고 싶다면, LinkedIn에서 연결할 기회를 갖게 되어 기쁩니다. 의미 있는 대화를 나누고 통찰을 공유하며 함께 클라우드 컴퓨팅의 광활한 영역을 탐색해 봅시다.\n\n언제든지 연락 주시거나 생각을 공유하거나 질문을 할 자유가 있습니다. 동적인 분야에서 연결하고 함께 성장하기를 기대합니다!\n\n행복한 배포 되세요! 🚀\n\n행복한 쿠버네팅 되세요! ⎈\n\n","ogImage":{"url":"/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_0.png"},"coverImage":"/assets/img/2024-05-23-AHand-OnGuidetoVaultinKubernetes_0.png","tag":["Tech"],"readingTime":11},{"title":"파이썬 제너레이터 데이터베이스에서 효율적으로 데이터를 가져오는 방법","description":"","date":"2024-05-23 14:16","slug":"2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases","content":"\n![image](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png)\n\n# 온디맨드 코스 | 추천\n\n몇몇 독자들이 데이터 엔지니어로 성장하는 데 도움이 될 온디맨드 코스를 요청했습니다. 제가 추천하는 3가지 좋은 자원은 다음과 같습니다:\n\n- 데이터 엔지니어링 나노디그리 (UDACITY)\n- 아파치 카프카 \u0026 아파치 스파크를 이용한 데이터 스트리밍 나노디그리 (UDACITY)\n- 파이스파크를 이용한 스파크 및 파이썬 빅데이터 과정 (UDEMY)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아직 Medium 회원이 아니신가요? Medium이 제공하는 모든 것에 액세스하려면 매월 $5로 가입하는 것을 고려해보세요!\n\n# 소개\n\n데이터 엔지니어로써 우리는 종종 운영 데이터베이스에서 특히 큰 데이터 집합을 가져와서 일련의 변환을 수행한 후에 분석 데이터베이스나 S3 버킷과 같은 클라우드 객체 저장소에 기록해야 하는 상황에 직면합니다.\n\n이 경우 Airflow 인스턴스에서 사용 가능한 메모리의 큰 부분을 사용하여 데이터 팀의 다른 동료들의 작업에 영향을 주지 않고 작업을 수행하는 방법을 찾아야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n파이썬 생성기가 메모리 피크를 피하면서 데이터베이스에서 데이터를 효율적으로 가져올 때 사용될 수 있는 좋은 옵션이 될 수 있습니다.\n\n실제로, 본 자습서에서는 생성기를 사용하는 것이 데이터 엔지니어에게 현명한 접근 방식인 두 가지 실용적인 사용 사례를 살펴볼 것입니다. 이를 위해 Docker 컨테이너를 구동하여 실제 엔드 투 엔드 데이터 워크플로를 시뮬레이션하기 위해 세 가지 서비스(포스트그레스 데이터베이스, 주피터 노트북 및 MinIO)를 실행할 것입니다.\n\n# 파이썬에서 생성기의 장점\n\n파이썬에서 표준 함수는 단일 값 계산 후 종료되지만, 생성기는 필요에 따라 일시 중지하고 다시 시작하면서 시간이 지남에 따라 값 시퀀스를 생성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제너레이터는 값을 시퀀스로 생성하기 위해 return 대신 yield 문을 사용하는 특별한 함수입니다. 값은 한 번에 하나씩 생성되며 전체 시퀀스를 메모리에 저장할 필요가 없습니다.\n\n제너레이터 함수가 호출되면 제너레이터에 의해 생성된 값의 시퀀스를 반복할 수 있는 이터레이터 객체가 반환됩니다.\n\n예를 들어, 0부터 입력 변수 n 사이의 숫자들의 제곱을 생성하는 squares_generator(n) 함수를 만들어 봅시다:\n\n```js\ndef squares_generator(n):\n  num = 0\n  while num \u003c n:\n    yield num * num\n    num += 1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n함수를 호출하면 이터레이터만 반환됩니다:\n\n```js\nsquares_generator(n)\n\n#출력:\n# \u003cgenerator object squares_generator at 0x10653bdd0\u003e\n```\n\n모든 값의 시퀀스를 가져오려면 제너레이터 함수를 루프 안에서 호출해야합니다:\n\n```js\nfor num in squares_generator(5):\n  print(num)\n\n#출력:\n0\n1\n4\n9\n16\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 효율적이고 세련된 옵션은 함수 대신 한 줄로 작성된 생성기 표현식을 만드는 것입니다:\n\n```js\nn = 5\ngenerator_exp = (num * num for num in range(n))\n```\n\n이제 값을 next() 메서드를 사용하여 직접 접근할 수 있습니다:\n\n```js\nprint(next(generator_exp)) # 0\nprint(next(generator_exp)) # 1\nprint(next(generator_exp)) # 4\nprint(next(generator_exp)) # 9\nprint(next(generator_exp)) # 16\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 볼 수 있듯이, 제너레이터 함수에서 값이 반환되는 방식은 일반적인 파이썬 함수와는 즉각적으로 직관적이지 않습니다. 아마도 그것이 많은 데이터 엔지니어들이 발생해야 할 정도로 제너레이터를 사용하지 않는 이유일 것입니다.\n\n다음 섹션에서 두 가지 일반적인 사용 사례를 설명해보겠습니다.\n\n# 목표 및 설정\n\n이 자습서의 목표는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Postgres DB로부터 데이터를 가져와서 pandas 데이터프레임으로 저장합니다.\n- pandas 데이터프레임을 Parquet 형식으로 S3 버킷에 씁니다.\n\n각 목표는 일반 함수와 제너레이터 함수를 사용하여 모두 달성될 것입니다.\n\n이러한 워크플로우를 시뮬레이션하기 위해 세 가지 서비스가 있는 도커 컨테이너를 실행합니다:\n\n- Postgres DB = 데이터를 가져올 소스 운영 데이터베이스로 사용될 서비스입니다. Docker-compose가 mainDB를 생성하고 transactions이라는 테이블에 5백만 개의 모의 레코드를 삽입하는 작업을 수행합니다. 참고: 이 튜토리얼을 위한 자료를 준비하는 동안, 더 큰 데이터셋을 시뮬레이션하기 위해 5천만 개, 1억 개의 행을 시도해 보았지만 Docker 서비스의 성능에 영향을 미쳤습니다.\n- MinIO = AWS S3 버킷을 시뮬레이션하는 데 사용될 서비스로, awswrangler 패키지를 사용하여 pandas 데이터프레임을 Parquet 형식으로 쓸 때 도움이 될 것입니다.\n- Jupyter Notebook = 익숙한 컴파일러를 통해 Python 코드 조각을 대화식으로 실행하는 데 사용될 서비스입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 설명한 내용을 시각적으로 보여주는 그래프입니다:\n\n![그래프](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_1.png)\n\n첫 번째 단계로는 프로젝트의 GitHub 리포지토리를 복제하고 해당 폴더로 이동합니다:\n\n```js\ngit clone git@github.com:anbento0490/projects.git \u0026\u0026\ncd fetch_data_with_python_generators\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면 세 가지 서비스를 시작하는 도커 컴포즈를 실행할 수 있어요:\n\n```js\ndocker compose up -d\n\n[+] Running 5/5\n ⠿ Network shared-network                 Created                                                 0.0s\n ⠿ Container jupyter-notebooks            Started                                                 1.0s\n ⠿ Container minio                        Started                                                 0.7s\n ⠿ Container postgres-db                  Started                                                 0.9s\n ⠿ Container mc                           Started                                                 1.1s\n```\n\n最終적으로 확인할 수 있어요:\n\n- 포스트그레스 데이터베이스에 transactions 테이블이 생성되었고 5백만 개의 레코드가 포함되어 있어요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndocker exec -it postgres-db /bin/bash\n\nroot@9632469c70e0:/# psql -U postgres\n\npsql (13.13 (Debian 13.13-1.pgdg120+1))\n도움말을 보려면 \"help\"를 입력하세요.\n\npostgres=# \\c mainDB\n데이터베이스 \"mainDB\"에 사용자 \"postgres\"로 연결되었습니다.\n\nmainDB=# select count(*) from transactions;\n  count\n---------\n 5000000\n(1 로우)\n```\n\n- MinIO UI는 localhost:9001 포트에서 접속할 수 있습니다. 자격 증명을 요청 받을 때 (관리자 및 비밀번호를 입력)를 사용하고 generators-test-bucket이라는 빈 버킷이 생성되었습니다:\n\n![image](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_2.png)\n\n- Jupyter Notebook UI는 localhost:8889에서 접근할 수 있으며 아래에 토큰을 검색하여 액세스할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```bash\ndocker exec -it jupyter-notebooks /bin/bash\n\nroot@eae08d1f4bf6:~# jupyter server list\n\n현재 실행 중인 서버:\nhttp://eae08d1f4bf6:8888/?token=8a45d846d03cf0c0e4584c3b73af86ba5dk9e83c8ac47ee7 :: /home/jovyan\n```\n\n![Python Generators](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_3.png)\n\n좋아요! Jupyter에서 몇 가지 코드를 실행할 준비가 모두 끝났어요.\n\n하지만 그 전에 MinIO의 버킷과 상호 작용하려면 새로운 access_key와 secret_access_key를 생성해야 합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_4.png\" /\u003e\n\n알림: MinIO 버킷의 가장 멋진 기능 중 하나는 AWS S3 버킷처럼 상호 작용할 수 있다는 것입니다 (예: boto3, awswrangler 등을 사용하여). 하지만 이러한 기능은 비용이 발생하지 않으며, 로컬 환경에만 존재하므로 비밀을 노출할 걱정이 없습니다. 컨테이너가 중지될 때까지 유지되지 않으므로 데이터가 계속 유지되지 않습니다.\n\n이제 생성기 노트북에서 다음 코드를 실행해 봅시다 (비밀 정보를 꼭 교체해주세요):\n\n```python\nimport psycopg2\nimport pandas as pd\nimport boto3\nimport awswrangler as wr\n\n#######################################################\n######## PG DB에 연결하고 커서 생성 #######\nconnection = psycopg2.connect(user=\"postgres\",\n                              password=\"postgres\",\n                              port=\"5432\",\n                              database=\"mainDB\")\ncursor = connection.cursor()\n\nquery = \"select * from transactions;\"\n\n#######################################################\n######## MINIO 버킷에 연결 ###################\n\nboto3.setup_default_session(aws_access_key_id='your_access_key',\n                            aws_secret_access_key='your_secret_key')\n\nbucket = 'generators-test-bucket'\nfolder_gen = 'data_gen'\nfolder_batch = 'data_batch'\nparquet_file_name = 'transactions'\nbatch_size = 1000000\n\nwr.config.s3_endpoint_url = 'http://minio:9000'\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 mainDB에 연결하고 쿼리를 실행하기 위한 커서를 만듭니다. 또한 generators-test-bucket와 상호 작용하기 위한 기본 세션이 설정됩니다.\n\n# 사용 사례 #1: 데이터베이스에서 읽기\n\n데이터 엔지니어로서 데이터베이스 또는 외부 서비스에서 대규모 데이터 세트를 Python 파이프라인으로 가져올 때, 다음 사항 사이의 균형을 찾아야 합니다:\n\n- 메모리: 한꺼번에 전체 데이터 세트를 가져오면 OOM 오류가 발생하거나 전체 인스턴스/클러스터의 성능에 영향을 줄 수 있습니다.\n- 속도: 행을 하나씩 가져오는 것도 비싼 I/O 네트워크 작업을 초래할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 방법 #1: 일괄적으로 데이터 가져오기\n\n실무에서 자주 사용하는 합리적인 절충안은 사용 가능한 메모리와 데이터 파이프라인의 속도 요구 사항에 따라 배치로 데이터를 가져오는 것입니다:\n\n```js\n# 1.1. 배치를 사용하여 DF 생성\ndef create_df_batch(cursor, batch_size):\n\n    print('생성 중...')\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n\n    df = pd.DataFrame(columns=colnames)\n    cursor.execute(query)\n\n    while True:\n        rows = cursor.fetchmany(batch_size)\n        if not rows:\n            break\n        # 일부 변환\n        batch_df = pd.DataFrame(data = rows, columns=colnames)\n        df = pd.concat([df, batch_df], ignore_index=True)\n\n    print('DF 생성 완료!\\n')\n\n    return df\n```\n\n위 코드는 다음을 수행합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 빈 df를 생성;\n- 쿼리를 실행하고 전체 결과를 커서 객체에 캐싱;\n- while 루프를 초기화하여 매 반복마다 지정된 배치 크기(이 경우 1백만 행)와 동일한 행 수를 가져와 이 데이터를 사용하여 배치\\_df를 생성합니다.\n- 최종적으로 배치\\_df가 주 df에 추가됩니다. 전체 데이터셋이 통과될 때까지 이 프로세스가 반복됩니다.\n\n분명히 말하자면, 이것은 기본적인 예시이며, 단순히 한 번에 한 배치씩 df를 생성하는 것 외에도 while 루프의 일부로 다른 많은 작업(필터링, 정렬, 집계, 데이터를 다른 위치로 쓰기 등)을 수행할 수 있었습니다.\n\n노트북에서 함수를 실행하면 다음과 같이 결과를 얻을 수 있습니다:\n\n```js\n%%time\ndf_batch = create_df_batch(cursor, batch_size)\ndf_batch.head()\n\n결과:\n\n생성 중...\nDF 생성 완료!\n\nCPU 시간: 사용자 9.97초, 시스템: 13.7초, 총: 23.7초\n실제 시간: 25초\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Python Generators](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_5.png)\n\n## Method #2: Using Generators\n\nA less common -but powerful- strategy for data engineers is to fetch data as a stream using generators:\n\n```python\n# AUXILIARY FUNCTION\ndef generate_dataset(cursor):\n\n    cursor.execute(query)\n\n    for row in cursor.fetchall():\n        # some transformation\n        yield row\n\n# 2.1. CREATE DF USING GENERATORS\ndef create_df_gen(cursor):\n    print('Creating pandas DF using generator...')\n\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n\n    df = pd.DataFrame(data=generate_dataset(cursor), columns=colnames)\n\n    print('DF successfully created!\\n')\n\n    return df\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 코드 스니펫에서는 쿼리를 실행하고 행을 시퀀스로 반환하는 'generate_dataset' 보조 함수를 생성합니다. 이 함수는 'pd.DataFrame()' 절의 데이터 인수에 직접 전달되며, 내부적으로 모든 검색된 레코드를 순회하고 행이 소진될 때까지 요소를 생성합니다.\n\n다시 말하지만, 이 예제는 매우 기본적이며(주로 설명 목적으로), 보조 함수 내에서 어떤 종류의 필터링이나 변환을 수행할 수 있습니다. 함수를 실행하면 다음과 같은 결과가 나옵니다:\n\n```js\n%%time\ndf_gen = create_df_gen(cursor)\ndf_gen.head()\n\n팬더스 데이터프레임 생성 중...\nDF가 성공적으로 생성되었습니다!\n\nCPU 소요 시간: 사용자 9.04초, 시스템 2.1초, 총 11.1초\n실제 시간: 14.4초\n```\n\n\u003cimg src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_6.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 가지 방법 모두 데이터 프레임이 반환되기 때문에 메모리 사용량이 동일할 것 같지만, 이는 사실이 아닙니다. 데이터 프레임이 생성되는 동안 데이터 처리 방식이 다르기 때문입니다:\n\n- 방법 #1의 경우, 데이터 교환 과정이 다소 비효율적으로 이루어지고 네트워크를 통해 데이터가 교환되어 더 높은 최대 메모리가 발생합니다.\n- 방법 #2의 경우, 필요할 때만 값을 계산하고 하나씩 처리하기 때문에 더 작은 메모리 공간을 사용합니다.\n\n# 사용 사례 #2: 클라우드 객체 저장소에 쓰기\n\n가끔 데이터 엔지니어는 데이터베이스에 저장된 대량의 데이터를 가져와서 이러한 레코드를 외부(예: 규제기관, 감사인, 파트너)와 공유해야 할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 해결책은 클라우드 객체 저장소를 생성하는 것입니다. 데이터가 전달되어 제 3자(적절한 액세스 권한이 부여된)가 데이터를 읽고 자신의 시스템으로 복사할 수 있게 합니다.\n\n사실, 우리는 데이터가 parquet 형식으로 작성될 버킷인 generators-test-bucket을 생성했습니다. 이는 awswrangler 패키지를 활용하여 데이터가 저장될 것입니다.\n\nawswrangler의 장점은 pandas 데이터프레임과 매우 잘 작동하며 데이터 집합 구조를 유지한 채로 데이터프레임을 parquet 형식으로 변환할 수 있다는 것입니다.\n\n## 방법 #1: 일괄 처리를 사용하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n첫 번째 사용 사례의 경우, 일반적으로 데이터를 일괄적으로 가져와서 쓰는 것이 일반적이며 전체 데이터 집합이 순회될 때까지 계속됩니다 :\n\n```js\n# 1.2 WRITING DF TO MINIO BUCKET IN PARQUET FORMAT USING BATCHES\ndef write_df_to_s3_batch(cursor, bucket, folder, parquet_file_name, batch_size):\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n    cursor.execute(query)\n    batch_num = 1\n    while True:\n        rows = cursor.fetchmany(batch_size)\n        if not rows:\n            break\n        print(f\"Writing DF batch #{batch_num} to S3 bucket...\")\n        wr.s3.to_parquet(df= pd.DataFrame(data = rows, columns=colnames),\n                         path=f's3://{bucket}/{folder}/{parquet_file_name}',\n                         compression='gzip',\n                         mode = 'append',\n                         dataset=True)\n        print('Batch successfully written to S3 bucket!\\n')\n        batch_num += 1\n```\n\nwrite_df_to_s3_batch() 함수를 실행하면 각각 100만 개의 레코드를 포함하는 5개의 파케이 파일이 해당 버킷에 생성됩니다 :\n\n```js\nwrite_df_to_s3_batch(cursor, bucket, folder_batch, parquet_file_name, batch_size)\n\nWriting DF batch #1 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #2 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #3 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #4 to S3 bucket...\nBatch successfully written to S3 bucket!\n\nWriting DF batch #5 to S3 bucket...\nBatch successfully written to S3 bucket!\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_7.png\" /\u003e\n\n## 방법 #2: 제너레이터 사용하기\n\n대안으로, 제너레이터를 활용하여 데이터를 추출하고 버킷에 작성할 수 있습니다. 제너레이터는 데이터를 가져오고 이동하는 동안 메모리 비효율성을 야기하지 않으므로 전체 DataFrame을 한 번에 쓰기를 결정할 수도 있습니다:\n\n```js\n# 2.2 GENERATOR를 사용하여 PARQUET 형식으로 DF를 MINIO 버킷에 쓰기\ndef write_df_to_s3_gen(cursor, bucket, folder, parquet_file_name):\n    print('DF를 S3 버킷에 쓰는 중...')\n\n    colnames = ['transaction_id',\n                'user_id',\n                'product_name',\n                'transaction_date',\n                'amount_gbp']\n\n    wr.s3.to_parquet(df=pd.DataFrame(data=generate_dataset(cursor), columns=colnames),\n             path=f's3://{bucket}/{folder}/{parquet_file_name}',\n             compression='gzip',\n             mode='append',\n             dataset=True)\n    print('데이터가 성공적으로 S3 버킷에 쓰여졌습니다!\\n')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef wirte_df_to_s3_gen(cursor, bucket, folder_gen, parquet_file_name):\n\nWriting DF to S3 bucket...\nData successfully written to S3 bucket!\n```\n\n![Python Generators](/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_8.png)\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 Python 함수보다 직관성이 떨어지는 제너레이터는 메모리를 적게 차지하면서도 좋은 성능을 제공하기 때문에 덜 사용되지만 이점이 많습니다.\n\n실제로 이 자습서에서는 데이터 엔지니어가 Python 제너레이터를 활용해 데이터베이스에서 데이터를 효율적으로 검색하는 방법을 연구하기 위해 세 가지 로컬 서비스(포스트그레스DB, 주피터 노트북, MinIO)를 도커를 통해 구동하여 데이터를 일괄로 처리하는 대신 데이터를 효율적으로 가져올 수 있는 두 가지 실제 예시를 공유했습니다.\n","ogImage":{"url":"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png"},"coverImage":"/assets/img/2024-05-23-PythonGeneratorsHowToEfficientlyFetchDataFromDatabases_0.png","tag":["Tech"],"readingTime":14},{"title":"컨테이너 세계에서 Runc 대 Crun","description":"","date":"2024-05-23 14:15","slug":"2024-05-23-RuncvsCrunincontainersworld","content":"\n\n만약 컨테이너화 기술에 흥미가 있다면, runc에 대해 들어봤을 수도 있어요.\n\n자세한 내용을 설명해 들어가기 전에 컨테이너가 정확히 무엇인지 알려드릴게요.\n\n주로 두 가지 리눅스 커널 모듈, 네임스페이스와 cgroup으로 만들어진 환경 안에서 가상 감옥 또는 격리된 환경을 만들어낸답니다.\n\n- 네임스페이스: 볼 수 있거나 접근할 수 있는 것을 제어합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCgroup: 리소스(예: RAM 및 CPU) 얼마나 사용하는지 확인하는 데 사용됩니다.\n\n![Image](/assets/img/2024-05-23-RuncvsCrunincontainersworld_0.png)\n\n컨테이너 런타임이 무엇인가요?\n\n컨테이너 런타임은 이미 이야기한 컨테이너라고 불리는 격리된 환경을 관리하는 데 도움을 주는 소프트웨어입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRunc과 Crun은 현재 사용되는 두 가지 주요 컨테이너 런타임입니다.\n\n그렇다면 containerd 또는 cri-o란 무엇인가요?\n\n이 둘은 모두 runc 위에서 작동하는 컨테이너 런타임의 추상화된 레이어입니다. 이것들은 컨테이너를 관리하기 위한 프론트 엔드로, 즉 컨테이너를 생성, 제거, 시작 및 중지하는 역할을 담당합니다.\n\n![이미지](/assets/img/2024-05-23-RuncvsCrunincontainersworld_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n크리올(Containerd)과 크라이오(CRI-O)는 둘 다 컨테이너 런타임으로 runc를 사용합니다.\n\n지금까지는 runc와 crun이 무엇인지 알아보았습니다; 이제 차이를 살펴봅시다.\n\n둘 다 컨테이너 런타임이며 컨테이너를 처리하는 데 동일한 작업을 수행합니다.\n\n하지만 차이점은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1) runc은 Go로 작성되었고 crun은 C로 작성되어서 Linux 커널과 더 호환성이 높아요.\n\n2) runc는 도커에 의해 개발되었고 crun은 RedHat에 의해 개발되었어요.\n\n3) crun은 더 가벼워서 메모리 소비가 낮아요. crun은 300K이고 runc는 15M이에요.\n\nPodman은 어떠세요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n포드맨은 컨테이너를 관리하는 데 책임이 있는 crun의 frontend 역할을 하는 도커와 비슷한 유틸리티입니다.\n\n## 요약\n\n![RuncvsCrunincontainersworld_2](/assets/img/2024-05-23-RuncvsCrunincontainersworld_2.png)\n\n결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 것을 사용해야 할까요?\n\n개발 및 프로덕션 환경 모두에서 추가 구성 요소 없이 가볍게 사용할 컨테이너 엔진이 필요하다면 Podman을 선택하세요.\n\n특히 이미지를 생성할 필요가 없는 프로덕션 환경에서는 Podman을 선택하는 것이 좋습니다. 이미지 빌드 구성 요소는 불필요하고 자원을 소비하는 소프트웨어로 간주될 수 있습니다.\n\n그리고 이것이 Docker를 프로덕션 환경에서 사용하지 않아야 하는 또 다른 이유입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 쿠버네티스를 사용 중이라면 cri-o를 선택하세요. 그리고 runc에 만족스럽지 않다면 C 언어로 된 가벼운 런타임을 원한다면 crun으로 런타임을 쉽게 전환할 수 있습니다.\n\nkloudino.com (메디 탈레가니)가 작성함","ogImage":{"url":"/assets/img/2024-05-23-RuncvsCrunincontainersworld_0.png"},"coverImage":"/assets/img/2024-05-23-RuncvsCrunincontainersworld_0.png","tag":["Tech"],"readingTime":2},{"title":"도커 대 Podman 안전한 오케스트레이션의 새 시대","description":"","date":"2024-05-23 14:14","slug":"2024-05-23-DockervsPodmanANewErainSecureOrchestration","content":"\n\n탐구하는 Root vs Rootless Orchestration: 보안 관점에서\n\n안녕하세요, 기술 애호가 여러분! 😊 오늘은 컨테이너 오케스트레이션의 매혹적인 세계로 빠져들어보겠습니다. 이 도구들이 나오기 전에는 개발자들이 수동 배포의 고통, 표준화 부족 (내 컴퓨터에서는 동작하는)으로 인한 복잡하고 오류가 발생하기 쉬운 과정을 겪어야 했습니다. 이러한 기술을 개발한 사람들에게 이해와 감사의 마음을 전해봅시다.\n\n이를 염두에 두고, 우리의 관심을 보안 오케스트레이션의 세계에서 뜨거운 반향을 일으키는 새로운 도구인 Podman으로 돌려봅시다. 이 도구는 특히 안전한 오케스트레이션 분야에서 Docker와 10년간 사랑받아온 기존 강자에 도전하고 있습니다 💪. 이 흥미진진한 발전에 대해 더 깊이 파헤치기 위해 기대해 주세요!\n\n# 🚀 컨테이너 이해: 왜 필요한가부터 어떻게 하는가까지\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨테이너는 코드, 런타임, 라이브러리 및 시스템 설정을 포함한 응용 프로그램 실행에 필요한 모든 것이 포함된 독립 실행 가능한 패키지입니다. \n\n이는 응용 프로그램이 어디에서 시작하더라도 동일하게 실행된다는 것을 의미합니다. 즉, 당신의 랩탑, 클라우드 서버 또는 동료의 컴퓨터 등 어디에서든 실행할 수 있습니다. 이 일관성은 '내 컴퓨터에서는 작동하는데'라는 오랜 문제를 해결합니다.\n\n컨테이너 오케스트레이션의 핵심은 컨테이너 실행 환경이며, 컨테이너 생성, 관리 및 실행에 도움을 줍니다.\n\n- 컨테이너가 시작되면 실행 환경은 저장소에서 지정된 컨테이너 이미지를 요청합니다. 이 이미지는 응용 프로그램 및 종속성에 대한 청사진 역할을 합니다.\n- 실행 환경은 Linux 네임스페이스를 사용하여 안전하고 분리된 환경을 제공하여 시스템 리소스(CPU, 메모리, 디스크, 네트워크 등)에 대한 독특한 이해를 제공합니다.\n- Linux 커널의 컨트롤 그룹(cgroups)은 리소스 공정한 분배를 보장하며 어떤 컨테이너도 리소스를 독차지하거나 시스템 성능을 저하시키지 않습니다.\n- 한 번에 한 컨테이너가 격리되면 실행 환경은 해당 환경 내에서 프로그램을 실행하여 호스트 시스템과 쉽게 통신합니다.\n- 데몬 프로세스로 작동하는 컨테이너 실행 환경 도구는 리눅스 커널과 상호작용하여 컨테이너를 관리하며, 관리를 위해 루트 액세스가 필요합니다. 이 상호작용은 효율적인 컨테이너 관리에 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n잘 알려진 컨테이너 런타임 중에는 Docker, k8s, nerdctl 등에서 사용되는 containerd와 cri-o가 있습니다.\n\n# 🏆 도커의 지배 속에서 Podman의 부상\n\n\u003cimg src=\"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png\" /\u003e\n\n컨테이너 관리 세계에서 인기가 Podman으로 변화되고 있으며, 그 이유에는 몇 가지가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Rootless Architecture: 도커와는 달리 루트 액세스가 있는 데몬 프로세스로 작동하는 Podman과 달리, Podman은 루트리스 접근 방식을 채택합니다. 이 기본적인 차이가 Podman의 인기 증진에 상당한 기여를 합니다.\n- 보안 취약점: Docker의 루트 액세스는 파일을 읽고 프로그램을 설치하며 애플리케이션을 편집하는 등 컨테이너를 관리할 수 있게 합니다. 그러나 이는 시스템에 보안 취약점을 도입하여 헤커들로 하여금 유혹적인 대상이 되게 합니다.\n- 해커들의 타깃: 해커가 데몬을 compromise하는 데 성공하면 민감한 데이터에 접근하거나 악성 코드를 실행하거나 컨테이너 구성을 변경하거나 시스템 전체를 다운시킬 가능성이 있습니다.\n- SELinux로 보강된 보안: Docker와는 다르게 Podman은 각 컨테이너를 Security-Enhanced Linux (SELinux) 레이블과 함께 시작하여 보안을 강화합니다.\n- 다른 도구에 의존: 루트리스 접근 방식으로 Podman은 컨테이너를 직접 관리하지 않습니다. 대신 이 아래서 설명하는 다른 도구들을 사용하여 컨테이너 관리를 수행합니다.\n- Buildah: OCI (Open Container Initiative) 호환 컨테이너를 빌드하는 데 사용되는 오픈 소스 리눅스 기반 도구입니다. Buildah는 전체 컨테이너 런타임이나 데몬을 설치하지 않고도 컨테이너를 생성하고 관리할 수 있습니다.\n- Skopeo: 컨테이너 이미지 및 이미지 레지스트리를 사용하여 다양한 작업을 수행하기 위한 명령줄 유틸리티입니다. 전체 이미지를 다운로드하지 않고 원격 레지스트리의 이미지를 검사할 수 있어 컨테이너 작업에 대한 가벼운 솔루션입니다.\n- Systemd: Podman은 설정된 컨테이너 런타임을 호출하여 실행 중인 컨테이너를 생성합니다. 그렇지만 전용 데몬이 없는 Podman은 시스템 및 서비스 관리자인 systemd를 사용하여 업데이트를 수행하고 컨테이너를 백그라운드에서 유지합니다.\n\n# 🔒 Podman의 보안에 대한 행동: Docker에 대한 안전한 대체품\n\n\u003cimg src=\"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_1.png\" /\u003e\n\n- 주어진 시나리오에서 세 명의 리눅스 사용자인 Bob, Dawg, BadBoy가 생성되었습니다. Bob과 Dawg는 Podman을 사용하여 컨테이너를 생성하며, 이러한 컨테이너들은 각 사용자 네임스페이스 내의 리소스에만 액세스할 수 있습니다. 이러한 설정은 각 컨테이너의 액세스를 해당하는 네임스페이스로 제한하여 보안을 강화합니다.\n- BadBoy는 Docker를 사용하며 루트 액세스를 가지고 있어 호스트 시스템의 모든 리소스에 대한 가시성을 허용합니다. 네임스페이스 밖에 있는 리소스까지도 볼 수 있어 시스템에 잠재적인 공격 가능성을 노출시킵니다. 이에 반해 루트리스 아키텍처인 Podman은 사용자 개별 네임스페이스에만 액세스 권한을 제한하여 보안을 강화합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Podman 설정\n\nPodman을 설정하는 실용적인 예제에 대해 알아보겠습니다. macOS에서 설정을 진행할 것이지만 필요에 따라 다른 환경에서 설정하는 방법에 대한 해당 문서를 참조할 수 있습니다.\n\n- Podman 설치: Homebrew를 이용하여 Podman을 설치하려면 brew install podman을 실행하세요.\n- Podman Machine 초기화: Podman 머신을 초기화하려면 podman machine init을 사용하세요.\n- Podman-Compose 설치: Docker Compose를 Podman으로 실행하는 스크립트인 Podman-Compose를 설치하려면 brew install podman-compose을 사용하세요.\n- Podman-Desktop 설치: Podman에 대한 Docker Desktop과 유사한 경험을 제공하는 Podman-Desktop을 설치하려면 brew install podman-desktop을 사용하세요.\n- Podman 세부 정보 확인: 마지막으로, podman info를 사용하여 Podman의 설치 및 구성 세부사항을 확인할 수 있습니다. 아래는 중요한 몇 가지 필드가 강조된 예시입니다.\n\n```js\nhost:\n  arch: amd64\n  buildahVersion: 1.32.0\n  cgroupControllers:\n  - cpu\n  - io\n  - memory\n  - pids\n  cgroupManager: systemd\n  cgroupVersion: v2\n  ociRuntime:\n    name: crun\n    package: crun-1.12-1.fc39.x86_64\n    path: /usr/bin/crun\n    version: |-\n      crun version 1.12\n      commit: ce429cb2e277d001c2179df1ac66a470f00802ae\n      rundir: /run/user/501/crun\n      spec: 1.0.0\n      +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL\n  os: linux\n  security:\n    apparmorEnabled: false\n    capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT\n    rootless: true\n    seccompEnabled: true\n    seccompProfilePath: /usr/share/containers/seccomp.json\n    selinuxEnabled: true\n  serviceIsRemote: true\nregistries:\n  search:\n  - docker.io\nversion:\n  APIVersion: 4.7.2\n  Built: 1698762721\n  BuiltTime: Tue Oct 31 20:02:01 2023\n  GitCommit: \"\"\n  GoVersion: go1.21.1\n  Os: linux\n  OsArch: linux/amd64\n  Version: 4.7.2\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Podman의 세부 정보를 확인하고 이미지를 검사하며 실행 중인 컨테이너를 관리하는 데 Podman CLI도 사용할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_2.png)\n\n- 아래 제공된 Docker Compose 파일을 사용하여 컨테이너를 시작하려면 다음 명령을 실행하세요: podman compose up -d\n\n```yaml\nservices:\n  postgres:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_DB: podman-psql\n      POSTGRES_USER: podman-psql-user\n      POSTGRES_PASSWORD: podman-pass\n    ports:\n      - '5432:5432'\n\n  redis:\n    image: 'redis:6.0.14'\n    restart: always\n    command: redis-server\n    ports:\n      - '6379:6379'\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 다음은 Podman 데스크톱 내에서 실행 중인 컨테이너와 이미지를 검사할 수 있습니다.\n\n![image1](/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_3.png)\n\n![image2](/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_4.png)\n\n# 마지막으로\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n디지턈 시대에는 보안이 매우 중요합니다. 침입이 발생하면 심각한 결과를 가져올 수 있습니다. Docker와 Podman은 각각 강점과 약점을 가지고 있습니다. Podman은 안전한 오케스트레이션의 기초를 바탕으로 만들어졌지만 Docker와 같은 기능(예: Docker Swarm)이 부족할 수 있습니다. 반면 Docker는 사용 편의성을 강조하지만 보안 측면에서는 미흡하다고 여겨집니다.\n\n이 토론이 유익했고 안전한 오케스트레이션에 대한 이해력을 높일 수 있었기를 바랍니다. 이 정보가 유용했다면 더 많은 글을 읽고 싶다면 저를 팔로우해주세요. 즐거운 학습되세요! 🚀\n\n# 참고 자료\n\n- Podman이란? (redhat.com)\n- Podman 설치 | Podman\n- Docker를 대체할 도구 및 그 이유 | mkdev의 프로그래밍 글\n- Alfresco와 함께 Podman 사용하기 — Alfresco Hub","ogImage":{"url":"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png"},"coverImage":"/assets/img/2024-05-23-DockervsPodmanANewErainSecureOrchestration_0.png","tag":["Tech"],"readingTime":7},{"title":"러스트 배우기 11부  빌더와 데이터베이스 상호작용","description":"","date":"2024-05-23 14:11","slug":"2024-05-23-LearningRustPart11BuildersandDatabaseInteraction","content":"\n다음 시리즈로 넘어가보겠습니다; 이 부분에서는 데이터 구조에 빌더 패턴을 구현하는 방법을 살펴보겠습니다. 그런 다음 sqlx와 Postgres를 사용한 데이터베이스 상호작용으로 넘어가겠습니다.\n\n![이미지](/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png)\n\n# Rust 시리즈\n\n부분 1 — 기본 개념\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPart 2 — 메모리\n\nPart 3 — 흐름 제어와 함수\n\nPart 4 — 옵션/결과 및 컬렉션\n\nPart 5 — 트레이트, 제네릭 및 클로저\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 6부 — 매크로, 반복자 및 파일 처리\n\n제 7부 — 스레드 공유 상태 및 채널\n\n제 8부 — Cargo, 크레이트, 모듈 및 라이브러리\n\n제 9부 — 명령행 인수, 워크스페이스 및 테스팅\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 10부 — 상자 포인터 및 웹 앱\n\n제 11부 — 빌더와 데이터베이스 상호작용 (이 기사)\n\n# 소개\n\n이것은 러스트 학습 시리즈의 열한 번째 섹션입니다. 이번에는 러스트에서 빌더 패턴을 다룰 것입니다. 이 공통된 패턴은 구조체를 안전하고 투명하게 초기화하는 좋은 방법입니다. 다음으로, 우리는 포스트그레스와 SQLX 프레임워크를 사용하여 데이터베이스에서 CRUD 작업을 수행하는 방법을 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 준비물\n\n이 글의 데이터베이스 부분을 위한 유일한 준비물은 Rust와 Cargo가 설치되어 있고 시스템에 Docker가 설치되어 있는 것입니다. 만약 Docker를 가지고 있지 않지만 로컬 Postgres DB가 이미 설치되어 있거나 다른 서버의 DB에 액세스할 수 있다면 Docker Postgres 설정을 건너뛰고 DB에 연결하기 위한 연결 속성만 수정하면 됩니다.\n\n# 빌더 패턴\n\n빌더 패턴은 복잡한 객체의 구성을 해당 표현에서 분리하는 디자인 패턴입니다. 이 패턴을 사용하면 유효성 검사를 수행하고 기본값으로 대체하며 값을 부분적으로 할당한 후에 항목을 생성할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRust로 그 구현하는 방법을 살펴볼 거예요. 이것이 우리의 코드입니다.\n\n```js\n#[derive(Debug)]\nstruct ChargingSession {\n    id: String,\n    watts: u32,\n    vin: String,\n}\n\nstruct ChargingSessionBuilder {\n    id: String,\n    watts: Option\u003cu32\u003e,\n    vin: Option\u003cString\u003e,\n}\n\nimpl ChargingSessionBuilder {\n    fn new(id: \u0026str) -\u003e ChargingSessionBuilder {\n        ChargingSessionBuilder {\n            id: id.to_string(),\n            watts: None,\n            vin: None,\n        }\n    }\n\n    fn watts(mut self, watts: u32) -\u003e ChargingSessionBuilder {\n        self.watts = Some(watts);\n        self\n    }\n\n    fn vin(mut self, vin: \u0026str) -\u003e ChargingSessionBuilder {\n        self.vin = Some(vin.to_string());\n        self\n    }\n\n    fn build(self) -\u003e ChargingSession {\n        ChargingSession {\n            id: self.id,\n            watts: self.watts.unwrap_or_else(|| 0),\n            vin: self.vin.unwrap_or_else(|| \"Unknown\".to_string()),\n        }\n    }\n}\n\nfn main() {\n    // 이것은 ChargingSession을 생성하는 표준적인 방법입니다.\n    let cs_old_way = ChargingSession {\n        id: String::from(\"11111\"),\n        watts: 420,\n        vin: String::from(\"4Y1SL65848Z411439\"),\n    };\n    println!(\"Regular way to create struct: {:?}\", cs_old_way);\n\n    // 빌더를 사용해서 생성하는 방법입니다.\n    let cs = ChargingSessionBuilder::new(\"11111\")\n        .watts(420)\n        .vin(\"4Y1SL65848Z411439\")\n        .build();\n    println!(\"Builder pattern to create struct: {:?}\", cs);\n\n    // ID만 제공하여 생성하는 예시입니다.\n    let cs_lean = ChargingSessionBuilder::new(\"11111\")\n    .build();\n     println!(\"Builder pattern to create struct (default values): {:?}\", cs_lean);\n}\n```\n\n이걸 한 번에 이해하기에 많지만, 단계적으로 진행해 봅시다.\n\n먼저, 충전 세션 정보를 저장하는 구조체를 정의했습니다. 이 시리즈의 이전 예제에서 하나의 필드인 세션용 차량 ID 번호인 vin을 추가했어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면 데이터 구조체와 동일한 필드를 가진 빌더를 위한 구조체를 정의합니다.\n\n빌더는 impl 블록에서 구현되며, id 및 각 추가 필드를 설정하는 함수를 정의하는 새 함수가 있습니다. 그러나 몇 가지 중요한 사항이 있습니다.\n\n- 각 함수는 ChargingSessionBuilder 유형을 반환합니다. 기본적으로 self입니다.\n- 추가 속성 필드에는 mut self를 첫 번째 매개변수로 사용하는 메서드가 있습니다. 이는 이러한 함수 호출의 체이닝을 허용하는 데 중요합니다. 또한 여기에 유효성 검사 논리를 코딩할 수 있습니다.\n- build 함수가 모두 통합되는 곳입니다. 존재하는 값들을 할당하고 나면 기본값을 결정하고 대상 구조체를 생성할 수 있습니다.\n\n이를 통해 빌더 패턴의 우아함과 Rust 내에서의 구현 방법을 살펴보았습니다. 이것이 앱이나 라이브러리에 코드를 구현하는 훌륭한 방법임을 알 수 있기를 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 섹션인 데이터베이스로 넘어가 봅시다.\n\n# 데이터베이스 — sqlx\n\n이 섹션에서는 Rust 프로그램에서 sqlx를 사용하여 데이터베이스 작업을 살펴볼 것입니다. 먼저 일부 설정이 필요합니다.\n\n## 설정\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n새 프로젝트를 시작해봅시다. db_app이라고 이름 짓고, cargo new db_app으로 생성할 수 있어요. 기본 디렉토리에 몇 개의 파일을 생성할 거에요. 첫 번째 파일은 docker-compose.yml이라고 하며 다음 내용이 있어야 합니다. Postgres와 Pgadmin이 노출되는 임의의 포트를 선택했으며, 컴퓨터에 설치된 다른 앱들과 충돌하지 않도록 했어요.\n\n```js\nversion: '3'\nservices:\n  postgres:\n    image: postgres:latest\n    container_name: postgres\n    ports:\n      - '6500:5432'\n    volumes:\n      - postgresDB:/data/postgres\n    env_file:\n      - ./.env\n  pgAdmin:\n    image: dpage/pgadmin4\n    container_name: pgAdmin\n    env_file:\n      - ./.env\n    ports:\n      - \"5050:80\"\nvolumes:\n  postgresDB:\n```\n\n.env이라는 파일이 하나 더 필요하며, 다음 내용이 있어야 해요. 이 파일은 docker-compose 파일과 나중에 Rust 애플리케이션에서 모두 사용할 거에요.\n\n```js\nPOSTGRES_HOST=127.0.0.1\nPOSTGRES_PORT=6500\nPOSTGRES_USER=admin\nPOSTGRES_PASSWORD=password123\nPOSTGRES_DB=charging_session\n\nDATABASE_URL=postgresql://admin:password123@localhost:6500/charging_session?currentSchema=public\n\nPGADMIN_DEFAULT_EMAIL=admin@admin.com\nPGADMIN_DEFAULT_PASSWORD=password123\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 내용과 함께 데이터베이스 사용자, 비밀번호, 그리고 Pgadmin에 대한 연결 정보를 제공해 드렸습니다.\n\n이 두 가지 항목을 만들고 위의 내용을 사용하여 Docker Compose를 사용하여 로컬 DB 인스턴스를 시작할 수 있습니다. 처음에는 항상 전경에서 시작하는 것을 좋아합니다. 이미지 다운로드 및 기타 작업을 하기 때문에 여러분의 컴퓨터 및 네트워크 속도에 따라 시간이 소요될 수 있습니다. docker-compose.yml 및 .env 파일이 있는 디렉토리와 동일한 위치에서 다음을 실행하십시오.\n\n```js\ndocker-compose up\n```\n\n시작된 모든 것을 확인한 후에는 언제든지 -d 스위치를 사용하여 데몬 모드로 시작할 수 있습니다. 완료되면 두 컨테이너가 시작되었는지 확인해 봅시다. 다음과 같이 명령을 실행합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndocker ps\n```\n\n비슷한 결과가 표시됩니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_1.png\" /\u003e\n\n만약 두 개의 컨테이너가 표시되지 않는다면, docker-compose를 실행한 터미널에서 에러를 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 번 이들이 실행되면 DB에 연결해 봅시다. Pgadmin을 포트 5050에서 실행하도록 구성했으니 브라우저에서 http://localhost:5050을 입력하면 pgadmin의 로그인 화면이 표시됩니다. .env 파일에 구성된 자격 증명(관리자@관리자.com/비밀번호123)을 사용하여 pgadmin에 로그인할 수 있습니다. 그런데 아직 데이터베이스에 연결되지 않았습니다. 이를 위해 다음을 실행해야 합니다.\n\n```js\ndocker inspect postgres\n```\n\n출력을 확인하여 \"NetworkSettings\" 섹션으로 이동하고 \"IPAddress\" 속성의 값을 복사합니다. 이 값은 DB에 연결하는 데 사용할 호스트(IP)입니다. 제 컴퓨터에서 이 값은 172.23.0.1 이었습니다.\n\n로그인한 후 \"새 서버 추가\" 버튼을 클릭하고, \"호스트 이름/주소\"로 이전에 복사한 IP 주소를 포함한 필수 자격 증명을 제공하고 \"저장\" 버튼을 클릭하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n서버에 로그인하면 데이터베이스 charging_session을 볼 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_2.png)\n\n잘 했어요. 데이터베이스를 사용할 준비가 되었고 Pgadmin에서 관리할 수 있습니다.\n\n이제 Rust 앱의 종속성을 구성해 봅시다. 처음에는 모두 필요하지 않지만 결국 필요하게 될 것이므로 지금 추가해 두는 것이 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCargo.toml 파일의 종속성 부분을 다음과 같이 수정하세요.\n\n```toml\n[dependencies]\nchrono = { version = \"0.4.31\", features = [\"serde\"] }\ndotenv = \"0.15.0\"\nenv_logger = \"0.10.1\"\nlog = \"0.4.20\"\nserde = { version = \"1.0.193\", features = [\"derive\"] }\nserde_json = \"1.0.108\"\nsqlx = { version = \"0.7.3\", features = [\"runtime-tokio-native-tls\", \"postgres\", \"uuid\", \"chrono\"] }\ntokio = { version = \"1.35.0\", features = [\"macros\", \"rt-multi-thread\"]}\nuuid = { version = \"1.6.1\", features = [\"serde\", \"v4\"] }\n```\n\n다양한 종속성에 대해 활성화된 기능을 검토하는 데도 시간을 할애하는 것이 좋습니다.\n\n다음으로, 필요한 테이블을 만들기 위해 sqlx-cli 마이그레이션 기능을 사용할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우선 명령 줄에서 다음을 실행하여 CLI를 설치해야 합니다.\n\n```js\ncargo install sqlx-cli --no-default-features --features rustls,postgres\n```\n\n그런 다음 마이그레이션 파일을 초기화해야 합니다. 다음과 같이 실행합니다.\n\n```js\nsqlx migrate add initial-tables\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 명령어는 우리가 마이그레이션 스크립트를 작성하기 위해 새 파일 migrations/`timestamp`\\_initial-tables.sql을 생성합니다.\n\n![이미지](/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_3.png)\n\n이 파일을 열고 아래 SQL 문을 추가하여 테이블을 생성하세요.\n\n\ncreate table locations (\n  id bigserial primary key,\n  name varchar(255) unique not null\n);\n\ncreate table sessions (\n  id bigserial primary key,\n  location_id bigint not null,\n  watts bigint not null,\n  vin varchar(255) not null,\n  constraint fk_location foreign key (location_id) references locations(id) on delete cascade\n);\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 예제에서 사용할 두 개의 테이블 정의입니다. 이 시리즈의 충전 세션에 대한 표준 예제를 확장하여, 충전 장치의 위치를 저장할 locations 테이블을 추가했습니다.\n\n이제 다음을 실행하여 테이블을 생성하세요.\n\n```js\nsqlx migrate run\n```\n\n그러면 즉시 20231212235431/migrate initial-tables (타임스탬프 부분은 달라질 수 있음)과 같은 메시지가 표시됩니다. 이제 Pgadmin에 가서 테이블을 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_4.png\" /\u003e\n\n위에서 보듯이 두 개의 테이블이 생성되었고, 문서 목적을 위해 Pgadmin 내에 ERD 다이어그램도 생성했습니다. 이제 초기 설정과 프로젝트 구성을 완료했습니다.\n\n# 데이터베이스 함수\n\n이 섹션에서는 데이터베이스 상호 작용의 다양한 유형과 Rust 구현을 살펴볼 것입니다. 이는 데이터베이스에 연결하는 것으로 시작됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDB에 연결하기\n\n데이터베이스에 연결하는 방법은 PgPoolOptions connect 함수를 통해 연결을 생성하는 것입니다. 우리의 .env 파일에서 데이터베이스 연결 문자열을 읽고 로깅을 구성하며 성공 또는 오류를 기록하는 코드 전체는 아래와 같습니다.\n\n```js\nuse sqlx::{postgres::PgPoolOptions, Pool, Postgres};\nuse dotenv::dotenv;\nuse log::{info, error};\n\n#[tokio::main]\nasync fn main() {\n    if std::env::var_os(\"RUST_LOG\").is_none() {\n        std::env::set_var(\"RUST_LOG\", \"info\");\n    }\n    dotenv().ok();\n    env_logger::init();\n\n    let database_url = std::env::var(\"DATABASE_URL\").expect(\"DATABASE_URL must be set\");\n    let pool = match PgPoolOptions::new()\n        .max_connections(10)\n        .connect(\u0026database_url)\n        .await\n    {\n        Ok(pool) =\u003e {\n            info!(\"✅ 데이터베이스에 연결되었습니다!\");\n            pool\n        }\n        Err(err) =\u003e {\n            error!(\"🔥 데이터베이스 연결에 실패했습니다: {:?}\", err);\n            std::process::exit(1);\n        }\n    };\n}\n```\n\n시작 부분의 #[tokio::main] 매크로를 주목해주세요. 이는 async fn main()을 동기 fn main()로 변환하여 런타임 인스턴스를 초기화하고 async main 함수를 실행하게 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 코드를 실행하면 출력 로깅에서 다음 메시지가 표시됩니다.\n\n[2023-12-13T00:16:42Z INFO db_app] ✅ 데이터베이스에 연결되었습니다!\n\n우리는 데이터베이스에 연결할 수 있습니다. .env 파일에서 구성한 모든 연결 정보를 기억해 주세요. 이 정보는 docker-compose, sqlx-cli, 그리고 우리의 어플리케이션에서 공유되었습니다.\n\n## Inserts\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 연결 풀이 준비되었으니, 우리가 살펴볼 첫 번째 데이터베이스 로직은 삽입입니다. sqlx를 사용하여 삽입하는 방법은 다음과 같습니다.\n\n```rust\nlet insert_result = sqlx::query_as!(\n    Locations,\n    \"INSERT INTO locations (id,name) VALUES (1, 'Location A') RETURNING *\"\n)\n.fetch_one(\u0026pool)\n.await;\n\nmatch insert_result {\n    Ok(location) =\u003e {\n        info!(\"✓Inserted: {:?}\", location);\n    }\n    Err(e) =\u003e {\n        error!(\"Error Insert: {}\", e.to_string())\n    }\n}\n```\n\n이 코드는 하나의 레코드를 삽입할 것입니다. 물론 매개변수를 사용할 수도 있으며, 이는 업데이트를 살펴볼 때 살펴볼 것입니다.\n\n## 질의하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 데이터베이스를 조회하는 방법을 살펴보겠습니다.\n\n```js\n    let query_result = sqlx::query_as!(Locations, \"SELECT * FROM Locations\")\n        .fetch_all(\u0026pool)\n        .await;\n    if query_result.is_err() {\n        let message = \"모든 위치를 가져오는 동안 문제가 발생했습니다.\";\n        error!(\"{}{}\", message, query_result.err().unwrap());\n    } else {\n        info!(\"😎 위치에 대한 쿼리 결과 {:?}\", query_result.unwrap());\n    }\n```\n\n이것은 성공적으로 작동했다면 unwrap을 통해 결과에 접근할 수 있는 결과를 반환합니다. 쿼리 내의 이슈가 발생했다면 해당 에러에 접근할 수도 있습니다.\n\n## 업데이트\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 작업은 데이터베이스를 업데이트하는 것입니다. 이는 이전에 삽입한 것과 비슷할 것입니다.\n\n```js\n    let update_result = sqlx::query_as!(\n        Sessions,\n        \"UPDATE sessions SET watts = 415 WHERE id = $1 RETURNING *\",\n        1i64,\n    )\n    .fetch_one(\u0026pool)\n    .await;\n\n    match update_result {\n        Ok(session) =\u003e {\n            info!(\"✓Update: {:?}\", session);\n        }\n        Err(e) =\u003e {\n            error!(\"Error Update: {}\", e.to_string())\n        }\n    }\n```\n\n이 예제에서 주목할 점은 $1이라는 매개변수 자리 표시자를 사용하는 준비된 문(statement)를 사용하고 있다는 것입니다. 그런 다음 SQL 문자열 뒤에 매개변수를 전달합니다.\n\n## Deletes\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막으로, 데이터베이스에서 레코드를 삭제하는 CRUD 작업을 완료합니다. 우리의 목적은 데이터베이스를 정리하는 데 사용할 것이며, 이렇게 하면 언제든지 응용 프로그램을 실행할 수 있습니다.\n\n```js\nlet rows_deleted = sqlx::query!(\"DELETE from sessions\")\n    .execute(\u0026pool)\n    .await\n    .unwrap()\n    .rows_affected();\n\ninfo!(\"✕ 세션 테이블에서 {}개의 행 삭제됨\", rows_deleted);\n```\n\n여기서는 연산에서처럼 sqlx::query 대신 sqlx::query_as를 사용합니다. 또한 언랩 이후 .rows_affected를 추가하여 삭제된 행 수를 얻습니다.\n\n## 트랜잭션\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```rust\n   let tx = pool.begin().await.expect(\"트랜잭션을 시작할 수 없습니다\");\n\n   // 데이터베이스 작업 수행(데이터 삽입 또는 변경)\n\n   tx.commit().await.expect(\"트랜잭션을 커밋할 수 없습니다\");\n```\n\n커밋을 호출하지 않으면 트랜잭션이 범위를 벗어나면 자동으로 롤백됩니다.\n\n## 완전한 응용 프로그램\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 코드는 우리가 이전에 논의한 모든 다양한 기능이 하나의 애플리케이션에 모두 포함된 완전한 애플리케이션입니다.\n\n```js\nuse dotenv::dotenv;\nuse log::{error, info};\nuse sqlx::{postgres::PgPoolOptions, Pool, Postgres};\n\n#[derive(Debug)]\nstruct Locations {\n    id: i64,\n    name: String,\n}\n\n#[derive(Debug)]\nstruct Sessions {\n    id: i64,\n    location_id: i64,\n    watts: i64,\n    vin: String,\n}\nasync fn insert_into_locations(pool: Pool\u003cPostgres\u003e) {\n    let tx = pool.begin().await.expect(\"Unable to begin transaction\");\n\n    let insert_result = sqlx::query_as!(\n        Locations,\n        \"INSERT INTO locations (id,name) VALUES (1, 'Location A') RETURNING *\"\n    )\n    .fetch_one(\u0026pool)\n    .await;\n\n    match insert_result {\n        Ok(location) =\u003e {\n            info!(\"✓Inserted: {:?}\", location);\n        }\n        Err(e) =\u003e {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n    let insert_result = sqlx::query_as!(\n        Locations,\n        \"INSERT INTO locations (id,name) VALUES (2, 'Location B') RETURNING *\"\n    )\n    .fetch_one(\u0026pool)\n    .await;\n\n    match insert_result {\n        Ok(location) =\u003e {\n            info!(\"✓Inserted: {:?}\", location);\n        }\n        Err(e) =\u003e {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n\n    tx.commit().await.expect(\"Unable to commit the transaction\");\n}\n\nasync fn insert_into_sessions(pool: Pool\u003cPostgres\u003e) {\n    let tx = pool.begin().await.expect(\"Unable to begin transaction\");\n\n    let insert_result = sqlx::query_as!(\n        Sessions,\n        \"INSERT INTO sessions (id,location_id, watts, vin) VALUES (1, 1, 420, '2FMZA52286BA02033') RETURNING *\"\n    )\n    .fetch_one(\u0026pool)\n    .await;\n\n    match insert_result {\n        Ok(session) =\u003e {\n            info!(\"✓Inserted: {:?}\", session);\n        }\n        Err(e) =\u003e {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n    let insert_result = sqlx::query_as!(\n        Sessions,\n        \"INSERT INTO sessions (id,location_id, watts, vin) VALUES (2, 2, 393, '1GMYA52286BA04055') RETURNING *\"\n)\n    .fetch_one(\u0026pool)\n    .await;\n\n    match insert_result {\n        Ok(session) =\u003e {\n            info!(\"✓Inserted: {:?}\", session);\n        }\n        Err(e) =\u003e {\n            error!(\"Error Insert: {}\", e.to_string())\n        }\n    }\n\n    tx.commit().await.expect(\"Unable to commit the transaction\");\n}\n\nasync fn update_sessions(pool: Pool\u003cPostgres\u003e) {\n    let tx = pool.begin().await.expect(\"Unable to begin transaction\");\n\n    let update_result = sqlx::query_as!(\n        Sessions,\n        \"UPDATE sessions SET watts = 415 WHERE id = $1 RETURNING *\",\n        1i64,\n    )\n    .fetch_one(\u0026pool)\n    .await;\n\n    match update_result {\n        Ok(session) =\u003e {\n            info!(\"✓Update: {:?}\", session);\n        }\n        Err(e) =\u003e {\n            error!(\"Error Update: {}\", e.to_string())\n        }\n    }\n\n    tx.commit().await.expect(\"Unable to commit the transaction\");\n}\n\nasync fn clean_db(pool: Pool\u003cPostgres\u003e) {\n    let rows_deleted = sqlx::query!(\"DELETE from sessions\")\n        .execute(\u0026pool)\n        .await\n        .unwrap()\n        .rows_affected();\n\n    info!(\"✕Deleted {} rows from sessions table\", rows_deleted);\n\n    let rows_deleted = sqlx::query!(\"DELETE from locations\")\n        .execute(\u0026pool)\n        .await\n        .unwrap()\n        .rows_affected();\n    info!(\"✕Deleted {} rows from locations table\", rows_deleted);\n}\n\nasync fn query_locations(pool: Pool\u003cPostgres\u003e) {\n    let query_result = sqlx::query_as!(Locations, \"SELECT * FROM Locations\")\n        .fetch_all(\u0026pool)\n        .await;\n    if query_result.is_err() {\n        let message = \"Something bad happened while fetching all locations\";\n        error!(\"{}\", message);\n    } else {\n        info!(\"😎Query Result For Locations {:?}\", query_result);\n    }\n}\n\nasync fn query_sessions(pool: Pool\u003cPostgres\u003e) {\n    let query_result = sqlx::query_as!(Sessions, \"SELECT * FROM Sessions\")\n        .fetch_all(\u0026pool)\n        .await;\n    if query_result.is_err() {\n        let message = \"Something bad happened while fetching all sessions\";\n        error!(\"{}\", message);\n    } else {\n        info!(\"😎Query Result for Sessions {:?}\", query_result);\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    if std::env::var_os(\"RUST_LOG\").is_none() {\n        std::env::set_var(\"RUST_LOG\", \"info\");\n    }\n    dotenv().ok();\n    env_logger::init();\n\n    let database_url = std::env::var(\"DATABASE_URL\").expect(\"DATABASE_URL must be set\");\n    let pool = match PgPoolOptions::new()\n        .max_connections(10)\n        .connect(\u0026database_url)\n        .await\n    {\n        Ok(pool) =\u003e {\n            info!(\"✅Connection to the database is successful!\");\n            pool\n        }\n        Err(err) =\u003e {\n            error!(\"🔥 Failed to connect to the database: {:?}\", err);\n            std::process::exit(1);\n        }\n    };\n    clean_db(pool.clone()).await;\n\n    insert_into_locations(pool.clone()).await;\n    query_locations(pool.clone()).await;\n\n    insert_into_sessions(pool.clone()).await;\n    query_sessions(pool.clone()).await;\n\n    update_sessions(pool.clone()).await;\n    query_sessions(pool.clone()).await;\n}\n```\n\n위 애플리케이션을 실행한 결과는 다음과 같습니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_5.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위와 같이 데이터베이스 상호작용 토론을 마쳤습니다. sqlx를 사용하여 데이터베이스 작업을 수행하는 방법에 대해 좋은 개요를 제공했을 겁니다.\n\n# 요약\n\n저희 러스트 학습 시리즈의 이 부분을 즐기셨기를 바랍니다. 시리즈 이번 섹션에서는 먼저 러스트에서 객체 생성에 대한 매우 유용한 패턴인 빌더 패턴을 살펴보았습니다. 이는 다른 언어에서 익숙할 수 있지만, 러스트에서 어떻게 구현하는지 살펴보았습니다.\n\n다음으로, 우리는 Rust를 사용하여 데이터베이스인 특히 Postgres와 상호작용하는 방법을 검토했습니다. 우리는 마이그레이션을 실행하고 데이터베이스에 연결하는 방법을 보았으며, 그 후 DB에 대해 여러 가지 CRUD 작업을 수행하는 방법을 살펴보았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n러스트 학습 여정에 함께해줘서 고마워요.\n\n좋은 여행 되세요!\n","ogImage":{"url":"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png"},"coverImage":"/assets/img/2024-05-23-LearningRustPart11BuildersandDatabaseInteraction_0.png","tag":["Tech"],"readingTime":18},{"title":"데브 컨테이너로 Rails 앱을 도커라이즈하기","description":"","date":"2024-05-23 14:10","slug":"2024-05-23-DockerizeRailsappwithDevContainers","content":"\n\n![이미지](/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png)\n\n만약 Rails 프로젝트 또는 다른 프레임워크에 대해 Docker를 시도해보려고 망설이고 있다면, 꼭 한 번 시도해보기를 추천합니다. VSCode의 Dev Containers 확장 프로그램을 사용하면 프로세스가 놀랄 만큼 스무스해지고 로컬 개발 과정이 간편해집니다.\n\n이미 CI/CD 파이프라인이나 배포에 Docker를 사용해본 적이 있다면 컨테이너화의 이점에 익숙할 것입니다. 그러나 솔직히 말해서 로컬 개발에 있어서 Docker는 항상 편리하지는 않습니다. 모든 것에 docker 또는 docker-compose 명령을 추가해야 한다는 것은 조금 귀찮은 일일 수 있습니다.\n\n이때 DevContainers가 등장합니다. 이를 통해 로컬 Rails 개발 환경 (및 다른 프레임워크)을 위한 전체 Docker 설정이 간단해집니다. 더 이상 명령줄이 혼잡해지지 않고, 매끄럽고 일관된 개발 경험만을 가져다줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nVSCode에서 개발 컨테이너를 사용하여 Rails를 도커화하는 간단한 튜토리얼을 제공합니다. 또한 도커나 도컴포즈 명령어를 실행할 필요 없이 컨테이너 환경 내에서 작업하는 방법을 안내합니다.\n\n본 튜토리얼에서는 다음을 배울 수 있습니다:\n\n- VSCode 편집기의 DevContainers 확장 프로그램을 사용하여 Docker 컨테이너 내에서 새로운 Rails 프로젝트를 신속하게 생성하는 방법\n- 터미널에서 복잡한 Docker 명령어를 필요로하지 않도록 함\n- 데이터 지속성을 위해 MySQL 데이터베이스 컨테이너를 사용하여 이중 구조 아키텍처 구축\n- Rails와 함께 MySQL을 컨테이너 내에서 실행\n- 멀티 스테이지 도커 파일을 사용하여 개발 및 프로덕션 환경에 대한 이미지 빌드 최적화\n- VSCode 내에서 간편하고 효율적인 Rails 개발 워크플로우 달성\n\n이 글과 관련된 튜토리얼을 통해 Rails 프로젝트에서 Dev Containers를 사용해보기를 고려하게끔 도와드리기를 바랍니다. 개발 프로세스를 최적화하고, 협업을 개선하며, 일관된 환경을 유지하는 환상적인 방법입니다. 즐거운 코딩 되세요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 도움이 되었다면, 추가적인 팁과 튜토리얼을 보기 위해 LinkedIn을 팔로우하고 YouTube 채널을 구독해주세요.\n\n## 추가 학습 내용:\n\n- 멀티 스테이지 도커 빌드 배우기\n- DevContainers로 로컬 DevOps 환경 설정하는 방법 배우기\n- VSCode 확장 프로그램 DevContainers","ogImage":{"url":"/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png"},"coverImage":"/assets/img/2024-05-23-DockerizeRailsappwithDevContainers_0.png","tag":["Tech"],"readingTime":2},{"title":"꿈팀 조합 구축 스트림라인된 데이터 파이프라인을 위한 Snowflake, Databricks 및 Delta Lake","description":"","date":"2024-05-23 14:09","slug":"2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines","content":"\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png\" /\u003e\n\n빅 데이터 시대에는 정보 관리가 지속적인 싸움입니다. 데이터 양이 급증함에 따라 저장뿐만 아니라 가치 있는 통찰을 추출하기 위한 효율적인 처리와 분석도 필요합니다. 이것이 꿈의 팀이 나타나는 곳입니다: Snowflake, Databricks 및 Delta Lake. 각각이 강력한 솔루션으로 클라우드 기반의 솔루션을 결합하여 효율적인 데이터 파이프라인을 만들어내어 당신을 챔피언처럼 데이터를 관리할 수 있게 합니다.\n\n플레이어와 그들의 역할:\n\n- Snowflake: 주역 쿼터백. 구조화된 데이터를 저장하고 분석하는 데 뛰어난 클라우드 기반 데이터 웨어하우스인 Snowflake는 데이터 분석가에게 친숙한 사용자 친화적 SQL 인터페이스를 통해 있는대로 데이터 탐색 및 효율적인 쿼리를 가능하게 합니다. 또한, Snowflake는 확장성과 보안을 자랑하며 데이터가 항상 접근 가능하고 보호되도록 보장합니다.\n- Databricks: 만회하는 와이드 리시버. Apache Spark를 기반으로 한 Databricks는 대규모 데이터 처리와 고급 분석의 달인입니다. 다양한 소스에서 데이터 수집, 데이터 변환 (정제 및 가공) 및 심지어 머신러닝 모델을 구축하고 배포하는 것과 같은 작업에서 빛을 발합니다. Databricks는 분석을 위해 데이터를 준비하는 일꾼 역할을 합니다.\n- Delta Lake: 신뢰할 수 있는 러닝 백. 데이터 호수 위에 구축된 오픈 소스 저장 레이어인 Delta Lake는 데이터 안정성의 챔피언입니다. 기존 또는 반구조적 데이터에 구조와 관리성을 추가하여 데이터 호수 안에 위치한 Delta Lake는 ACID 트랜잭션 (데이터 일관성 보장), 스키마 강제 사항 (데이터 구조 정의), 데이터 버전 관리 (데이터 변경 추적)와 같은 기능을 제공합니다. 데이터의 정확성과 신뢰성을 보장하는 데이터의 기초로 생각할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n승리의 플레이북: 간소화된 데이터 파이프라인 구축\n\n성능과 효율성을 최적화하여 각 단계를 효율적으로 흐르는 데이터 파이프라인을 상상해보세요. Snowflake, Databricks 및 Delta Lake가 이를 달성하는 방법은 다음과 같습니다:\n\n- Touchdown — 데이터 수집: Databricks는 다양한 소스(데이터베이스, API 또는 데이터 레이크에 있는 raw 파일 등)에서 데이터를 가져오는데 사용되는 다양한 커넥터를 통해 데이터를 추출합니다.\n- The Handoff — 데이터 변환: Databricks가 중심에 위치합니다. Spark의 처리 능력이 빛나며 주입된 데이터를 노트북을 사용하여 정제, 변환 및 준비합니다. 이 단계에서는 누락된 값을 처리하고 데이터 유형을 변환하거나 새로운 기능을 도출하는 등 분석 전에 모두 중요한 과정입니다.\n- The Choice of Plays — ETL 대 ELT: 여기서 데이터 저장을 위해 두 가지 옵션이 있습니다:\n\n  - ETL (추출, 변환, 로드): 이 플레이에서 Databricks는 변환된 데이터를 데이터 웨어하우스인 Snowflake로 이관하는 중심 역할을 합니다. Snowflake는 데이터를 안전하게 저장하여 쿼리 및 분석에 쉽게 사용할 수 있게 합니다.\n  - ELT (추출, 로드, 변환): 이 플레이는 Delta Lake의 강점을 활용합니다. 원본 데이터는 데이터 레이크에 직접 들어가며, Databricks는 클라우드 환경 내에서 데이터를 직접 Delta 테이블에 변환합니다. 이 접근 방식은 데이터 이동을 최소화하지만 분석을 위해 Delta 테이블에 의존하기 전에 데이터 품질을 보장해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n4. 승리의 춤 — 데이터 분석 및 소비: Snowflake가 슬기롭게 떠나다. 데이터 분석가 및 데이터 과학자들은 Snowflake의 SQL 인터페이스를 활용하여 Snowflake에 저장된 데이터에 대한 철저한 탐색, 시각화 및 고급 분석을 할 수 있습니다. 또한, 구체적인 사용 사례를 위해 Delta 테이블을 직접 쿼리하거나 실시간 스코어링과 분석을 위해 훈련된 머신러닝 모델을 Snowflake에 배포할 수 있습니다. 이를 통해 데이터로부터 가치 있는 통찰력을 얻을 수 있습니다.\n\n샘플 코드 실행 (Delta Lake를 사용한 ETL):\n\n```js\n# Databricks 노트북 - Python\n\n# 1. Snowflake에 연결\nsnowflake_conn = {\n    \"host\": \"your_snowflake_account.snowflakecomputing.com\",\n    \"user\": \"your_username\",\n    \"password\": \"your_password\",\n    \"database\": \"your_database\",\n    \"schema\": \"your_schema\"\n}\n\n# 2. CSV 파일에서 데이터 로드 (데이터 레이크에 저장된 것으로 가정)\ndf = spark.read.csv(\"path/to/your/data.csv\", header=True)  # 헤더가 있는 CSV를 가정\n\n# 3. 데이터 변환 (예시: 결측값 처리)\ndf = df.fillna(0, subset=[\"column_with_missing_values\"])  # 결측값을 0으로 대체\n\n# 4. 변환된 데이터를 Snowflake 테이블에 작성\ndf.write.format(\"snowflake\").options(**snowflake_conn).mode(\"overwrite\").saveAsTable(\"your_snowflake_table_name\")\n```\n\n게임 넘어서: 파이프라인 최적화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n팀의 승리를 위해서는 지속적인 최적화가 필요해요. 여기 몇 가지 추가 팁이 있어요:\n\n- 올바른 전략 선택하기: ETL 및 ELT 방법 중 선택할 때 데이터 양, 처리 복잡성 및 원하는 대기 시간과 같은 요소를 신중하게 고려해주세요.\n- 데이터 품질이 중요해요: 통찰력의 무결성을 보장하기 위해 데이터 품질 점검을 파이프라인 전체에 구현해주세요.\n- 전략 실행하기: Airflow나 Databricks Workflows와 같은 도구를 활용하여 데이터 파이프라인의 실행을 일정화하고 조정하여 정보가 원활하게 흐를 수 있도록 해주세요.\n- 비용 관리: Snowflake와 Databricks는 사용량 기반의 가격 모델을 갖고 있어요. 자원 사용량을 모니터링하고 Databricks의 유휴 클러스터를 중지하는 등 비용 절감 방안을 도입해주세요.\n\n최종 엔딩: 데이터의 힘을 발휘해보세요\n\nSnowflake, Databricks, 그리고 Delta Lake를 결합하여 견고하고 확장 가능한 데이터 관리 시스템을 만들 수 있어요. 이 꿈의 팀은 대용량 데이터의 과제에 대처하고, 원시 정보를 실행 가능한 통찰력으로 변화시키는 능력을 부여해줘요. 상상해보세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정보에 대한 빠른 통찰력: 간소화된 데이터 파이프라인은 유용한 데이터에보다 빠르게 액세스할 수 있도록 도와주어 데이터 기반의 결정을 이전보다 빨리 내릴 수 있습니다.\n- 개선된 데이터 거버넌스: Delta Lake 및 Snowflake의 보안 기능은 데이터의 보호와 신뢰성을 보장하여 데이터 분석에 대한 신뢰를 증진시킵니다.\n- 미래를 위한 유연성: 이 아키텍처는 확장 가능하게 설계되었습니다. 데이터 요구 사항이 발전함에 따라, 새로운 데이터 소스, 처리 요구 사항 및 분석 요구 사항을 수용하기 위해 파이프라인을 조정하고 확장할 수 있습니다.\n\n그러니 자신감 있게 필드에 발을 들이세요. Snowflake, Databricks 및 Delta Lake가 곁에 있는 당신은 데이터를 관리하고 그 참된 잠재력을 개방할 수 있는 승리팀입니다.","ogImage":{"url":"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png"},"coverImage":"/assets/img/2024-05-23-BuildingtheDreamTeamSnowflakeDatabricksandDeltaLakeforStreamlinedDataPipelines_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터베이스DBT 간단 정리","description":"","date":"2024-05-23 14:08","slug":"2024-05-23-DBTinaNutshell","content":"\n\n이 게시물은 일반적으로 DBT(Data Build Tool)에 대한 내 이해와 누가 사용해야 하고 사용하지 말아야 하는지에 중점을 둘 것입니다. 앞으로의 게시물에서 더 자세히 다룰 예정입니다.\n\nDBT(Data Build Tool)와 Snowflake❄️를 통해 비즈니스 문제 해결하기👨‍💻\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_0.png)\n\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 1 : [링크](https://lnkd.in/gASCckRR)\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 2 : [링크](https://lnkd.in/gMqfKZRW)\n❖ Snowflake❄️ + DBT👨‍💻 프로젝트 전체 파트 3 : [링크](https://lnkd.in/g8BuWy66)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n◉ 참여하기:\n👉 GitHub: [링크](https://lnkd.in/ggt3ZzUx)\n🚀 기여하고, 복제하고, 공유하세요!\n\n◉ YouTube 채널 찾기🎥: [링크](https://lnkd.in/esW5M3vb)\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_1.png)\n\n- DBT란 무엇인가요?\n- DBT는 무료인가요?\n- DBT가 해결하고 있는 문제는 무엇인가요?\n- ETL의 어느 부분에서 DBT를 사용하나요?\n- DBT가 지원하는 데이터 어댑터(데이터 플랫폼)는 무엇인가요?\n- DBT가 Databricks와 같은 기존 제품과 다른 점은 무엇인가요?\n- DBT를 배우고 사용하는 방법은 무엇인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DBT이란 무엇인가요?\n\nDBT(data build tool)는 SQL 스크립트(.sql)와 YAML 스크립트(.yml)를 사용하여 워크플로우를 변환하는 데 사용되는 오픈 소스 도구(Core 버전)입니다.\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_2.png)\n\n- SQL 스크립트는 CTE를 사용하여 데이터를 모듈화된 방식으로 변환하는 데 도움이 됩니다.\n- YAML 스크립트는 스키마, 설명 및 열에 대한 테스트 규칙(Null이 아닌 값, 고유 값 등)을 정의하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DBT은 무료인가요?\n\nDBT에는 DBT 코어와 DBT 클라우드 두 가지 버전이 있습니다.\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_3.png)\n\n- DBT 코어: CLI(Command Line Interface) 버전으로, 간단한 pip 명령어 `pip install dbt-core`를 사용하여 설치할 수 있습니다. 연결을 위해 어댑터(snowflakes, SQL Server)를 설치하려면 `pip install dbt-snowflake`를 사용하세요.\n- DBT 클라우드: GIT 및 어댑터(데이터 소스)를 통합하고 드래그 앤 드롭 기능을 사용하여 워크스페이스를 구성할 수 있는 GUI(Graphical User Interface)를 제공하며, 모델(당신의 .sql 파일)을 예약할 수 있는 기능이 추가되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## DBT가 해결하려는 문제는 무엇인가요?\n\n![DBTinaNutshell_4](/assets/img/2024-05-23-DBTinaNutshell_4.png)\n\n원활한 협업:\n\n- 협업을 위한 통합 플랫폼을 즐기세요.\n- 효율적인 팀워크를 위한 CI/CD-Git 통합을 활용하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n편리한 데이터 변환:\n\n- 번거로움 없이 간단한 SQL 선택 문을 사용하여 데이터를 변환하세요.\n\n테스트로 신뢰성 확보:\n\n- 사용자 정의 테스트 케이스를 포함하여 데이터 변환을 테스트하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n편리하게 배포하고 일정을 계획하세요:\n\n- 코딩을 개발 및 프로덕션과 같은 다양한 환경에 배포하고 일정을 조정하세요.\n\n간편하게 작업 문서화하세요:\n\n- 간단한 .yml 파일로 전체 프로세스를 문서화하세요.\n- 데이터 변환 여정에 대한 포괄적인 문서를 작성하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ETL에서 DBT는 어느 부분에 사용되나요?\n\nDBT는 다른 ELT✅ 방법을 사용합니다❌ ETL이 아닌데요, 여기서는 데이터 플랫폼으로 모든 데이터를 추출 및 로드하고 DBT를 사용하여 변형한 후 다양한 사용 사례를 위해 다시 데이터 플랫폼에 로드합니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-DBTinaNutshell_5.png\" /\u003e\n\n## DBT는 어떤 데이터 어댑터(데이터 플랫폼)를 지원하나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금은 다음 데이터 플랫폼을 직접 지원합니다:\n\n- Snowflakes\n- Google Big Query\n- Data Bricks\n- AWS Redshift\n- Trino\n\n![이미지](/assets/img/2024-05-23-DBTinaNutshell_6.png)\n\nTrino는 직접적으로가 아닌 Trino를 통해 여러 데이터 소스에 연결하는 데 사용됩니다. 자세한 내용은 이미지를 참조하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-DBTinaNutshell_7.png\" /\u003e\n\n## DBT와 Databricks와 같은 다른 기존 제품과의 차이점은 무엇인가요?\n\n- DBT: DBT는 주로 데이터웨어하우스 내에서 데이터 변환 및 문서화 문제를 .sql 및 .yml 파일을 사용하여 CICD-git 환경에서 간단하고 효과적으로 처리하는 것에 중점을 둡니다.\n- Databricks (유명한 예시): Databricks는 대용량 데이터 분석을 위한 협업 환경을 제공하는 통합 분석 플랫폼입니다. 데이터 엔지니어링, 머신 러닝, 협업 데이터 과학 기능이 포함되어 있으며 주로 분산 데이터 처리를 위해 Spark와 함께 사용됩니다.\n\n## 어떻게 DBT를 학습하고 사용할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- DBT를 배우기 시작하려면 해당 웹사이트에서 시작할 수 있어요. 총 16개의 다양한 코스가 초급, 중급 및 고급 수준으로 분류되어 있어요.\n\n![DBTinaNutshell_8](/assets/img/2024-05-23-DBTinaNutshell_8.png)\n\n- DBT를 더 잘 배울 수 있도록 많은 매체 기사와 유튜브 비디오들이 있어요 (DBT 콘텐츠에 대한 업데이트 받으시려면 지켜봐주세요! 다가오는 게시물 및 DBT 관련 비디오에 대한 업데이트를 받으시려면 저를 팔로우해주세요).\n\nLeo Godin의 DBT 시리즈 게시물: 링크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나에 대해 더 알아보기:\n\n저는 데이터 과학 애호가🌺이며, 수학, 비즈니스 및 기술이 데이터 과학 분야에서 더 나은 결정을 내리는 데 어떻게 도움이 될 수 있는지에 대해 배우고 탐구하고 있습니다.\n\n더 많은 내용 보기: https://medium.com/@ravikumar10593/\n\n내 모든 핸들 찾기: https://linktr.ee/ravikumar10593\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나의 뉴스레터를 찾아보세요: [https://substack.com/@ravikumar10593](https://substack.com/@ravikumar10593)","ogImage":{"url":"/assets/img/2024-05-23-DBTinaNutshell_0.png"},"coverImage":"/assets/img/2024-05-23-DBTinaNutshell_0.png","tag":["Tech"],"readingTime":4},{"title":"데이터 엔지니어링 개념 제10부, 스파크와 카프카를 활용한 실시간 스트림 처리","description":"","date":"2024-05-23 14:05","slug":"2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka","content":"\n이것은 데이터 엔지니어링 개념 10부작 시리즈의 마지막 부분입니다. 이번에는 스트림 처리에 대해 이야기할 것입니다.\n\n목차:\n\n1. 스트림 처리란\n2. 카프카의 특징\n3. 카프카 구성\n4. 카프카 서비스 — 카프카 스트림, ksqlDB, 스키마 레지스트리\n5. 스파크 구조화 스트리밍 API\n6. 데이타브릭스 델타 레이크\n7. 실전 프로젝트\n\n이전 데이터 보안 부분으로 이동하는 링크입니다:\n\n## 스트림 처리란?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일괄 처리는 데이터 처리를 일정한 간격으로, 한 번에 대량의 데이터를 처리하는 데 사용되며 즉각적인 통찰력이 필요하지 않은 경우에 사용됩니다. 한편, 스트림 처리는 이 기사에서 논의할 다른 시나리오에 적합할 것입니다.\n\n![데이터 엔지니어링 개념 시리즈 10부: Spark와 Kafka를 이용한 실시간 스트림 처리](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png)\n\n스트림 처리는 사기 탐지, IoT 센서 모니터링, 실시간 맞춤형 추천, 교통 데이터 분석을 통한 혼잡 감지 및 교통 흐름 최적화와 같은 사용 사례에 필수적입니다. 이러한 작업들은 신속한 응답, 실용성 및 자원 이용 효율성을 필요로 합니다. 그러나 이러한 시스템을 구현하는 것은 높은 지연 환경에서 기대되는 데이터 무결성, 보안 및 장애 허용성 때문에 복잡할 수 있습니다. 또한 항상 켜져 있는 하드웨어 때문에 확장/세밀 조정 및 모니터링이 비싸게 들 수 있습니다. 그러므로 최고 수준의 신뢰할 수 있는 인프라 및 저 레이턴시를 유지할 수 있도록 잘 분할된 데이터베이스가 필요합니다.\n\n카프카는 스트리밍 기능을 제공하는 가장 널리 사용되는 도구 중 하나이며, 여기에서 자세히 알아보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 카프카 특징\n\n- 견고함 — 한 대의 브로커(서버)가 다운되어도 데이터 손실을 방지하기 위해 복제됨.\n- 유연성 — AVRO, JSON과 같은 다양한 데이터 형식을 처리할 수 있으며, 다양한 크기와 생산자 및 소비자 수가 다른 토픽을 가질 수 있음.\n- 확장성 — 데이터 흐름이 증가함에 따라 성장함. 이 기능을 용이하게 하기 위해 다음 기술들이 구현됨:\n\n  - 파티셔닝: 병렬 처리를 위해 토픽을 추가로 파티션(샤드)으로 분할할 수 있음. 사람들을 다른 섹션으로 나누어 꽉 차지 않게하고, 모든 사람이 음악을 들을 수 있도록 하는 것과 같다고 생각해보세요.\n  - 수평 확장: 클러스터에 더 많은 브로커를 추가하여 증가하는 데이터 양을 처리할 수 있음. 관객이 증가함에 따라 콘서트 장소에 더 많은 스피커를 추가한다고 상상해보세요.\n\n## 카프카 구성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n카프카 구성은 모든 카프카 클러스터의 속성 및 동작 설정을 지원하여 성능, 신뢰성 및 확장성을 향상시킵니다.\n\n- 파티션 — 토픽을 병렬 처리하기 위해 여러 파티션으로 나눌 수 있습니다. 각 파티션은 변경할 수 없는 메시지의 순서가 지정된 세트입니다.\n- 복제 — 여러 브로커 노드에 걸쳐 Kafka 토픽 파티션의 복제 횟수를 제공하여 장애 허용성을 제공합니다.\n- 유지 기간 — 보관할 로그 세그먼트 파일의 최대 크기 또는 메시지 보관 기간 시간을 나타냅니다.\n- 자동 오프셋 재설정 — 파티션에서 읽기 위해 오프셋은 시작점으로 제공되며, 자동 오프셋이 처음일 경우 시작부터 모든 메시지를 읽고, 최신으로 설정되어 있으면 최신 메시지만 읽습니다.\n- ack — ack 구성은 생산자가 메시지를 수신한 후 소비자로부터 확인을 받는지 여부를 결정합니다. acks=0은 확인 없음, ack=1은 리더가 확인을 보내고, ack=all은 파티션의 모든 복제본이 확인을 보내는 것을 의미합니다.\n\n## 다른 카프카 서비스\n\nKafka Streams — 이는 스트리밍 애플리케이션을 작성하는 데 사용되는 Java 라이브러리입니다. 선언적 언어 형식을 사용하여 스트리밍 처리 논리를 작성할 수 있는 API로, 다양한 기능을 포함하여 부가 코드를 피하고 낮은 대기 시간, 탄력성 및 암호화 기능을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_1.png\" /\u003e\n\nksqlDB — ksqlDB은 스트림 데이터를 처리하고 SQL 쿼리를 통해 상호 작용하는 데 특별히 설계된 데이터베이스로, 필터링, 집계 및 다른 토픽을 결합하는 기능을 포함합니다. ksqlDB에서 사용되는 두 가지 주요 구조는 변경할 수 없는 append only 이벤트의 이전 이벤트의 컬렉션인 스트림과 데이터 스트림의 현재 상태를 나타내는 데이터의 불변한 스냅샷인 테이블입니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_2.png\" /\u003e\n\n스키마 레지스트리 — 스키마 레지스트리는 생산자와 소비자 서비스가 준수해야 하는 사용자가 정의한 스키마의 중앙 저장소입니다. 버전 관리, 데이터 유효성 검사, 데이터 손실 또는 손상 위험 감소 및 호환성 검사와 같은 기능을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_3.png)\n\n## Spark Structured Streaming API\n\nSpark Structured Streaming은 Spark SQL 엔진을 기반으로하며, 이 스트리밍 모델은 DataFrame/Dataset API를 기반으로합니다. 입력 데이터 스트림은 미리 정의된 간격으로 흡수되며 사용자가 정의한 쿼리의 결과는 무한한 테이블에서 업데이트(증분)됩니다. 출력 모드(append, complete, update)는 사용자가 구현하고자 하는 사용 사례에 따라 정의할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 이 접두어 무결성(어떤 시점에서든 응용 프로그램의 출력 = 데이터의 접두어 일괄 작업)은 일관성(출력물은 항상 접두어의 결과이며 순서대로 처리됨), 장애 내성(시스템이 실패해도 결과 상태를 유지) 및 스트림 데이터의 순서를 유지하는 여러 이점이 있습니다.\n\n또한, 이는 다른 스트리밍 시스템과 비교했을 때 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_5.png)\n\n## Databricks Delta Lake\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nDatabricks는 스파크의 창조자들에의해 설계된 클라우드 기반 플랫폼으로, 생산 등급 솔루션을 구축, 배포, 공유 및 유지하는 데 사용되는 통합 데이터 분석 처리 매체로 Spark 생태계의 모든 기능과 비즈니스 인텔리전스, 머신러닝 및 AI를 위한 추가 리소스가 구비되어 있습니다. Databricks의 모든 하위 시스템의 기본 저장 형식은 Delta Lake입니다.\n\nDelta Lake 테이블은 Databricks에 구현된 Delta Lakehouse 아키텍쳐의 기반입니다. 이를 통해 일괄 및 스트리밍 데이터의 병렬 처리가 공유 파일 저장소에서 가능하며, delta lake는 원시 형식에서 구조화된 형식으로 데이터의 연속적인 흐름을 가능하게 하며, 들어오는 신선한 데이터와 함께 작업할 수 있도록 분석 및 ML 응용프로그램을 지원합니다.\n\n스키마 강제 및 데이터 버전 관리를 제공하여 유효성 검사를 지원하고 재사용성 및 감사 기능을 제공합니다. 더불어, Spark 구조화 스트리밍 API와 웰 매칭하여 규모 있는 점진적 처리를 가능하도록 최적화되었습니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_6.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 실시간 스트리밍 아키텍처\n\n카프카, 스파크 및 델타 레이크와 같은 스트리밍 기술을 활용하여 실시간 스트리밍 플랫폼을 구축할 예정입니다.\n\n문제 정의: 실시간 환경 이슈 보고서를 수집하고, 변환 및 분석해서 심각성 수준 및 위치 기준에 따라 관련 환경 당국이 사용할 수 있는 데이터를 만드는 응용 프로그램을 개발 중입니다. 중복 보고서를 필터링하고 트렌드를 파악하기 위해 역사적 분석도 수행할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 아키텍처를 구현하려면 다음 단계를 따를 것입니다:\n\n- 단계 1: Confluent Cloud 계정 및 Kafka 클러스터 생성\n- 단계 2: 토픽 생성\n- 단계 3: 클러스터 API 키 쌍 생성\n\n위 3 단계를 구현하기 위해 아래 링크를 사용하십시오-\n\n- 단계 4: 환경 보고서를 생성하는 Streamlit 웹 앱을 만듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_8.png)\n\nUse the following link to set up a Python client in streamlit app to push the incidents to the Kafka topics:\n\n- Step 5: Go to the Confluent Cloud dashboard and verify if the topics are being populated\n\n![Image](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_9.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 단계 6: Databricks Community Edition 계정 및 컴퓨팅 인스턴스 생성\n\n- 단계 7: Kafka 토픽에서 스트림 읽기\n\n```js\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Environmental Reporting\").getOrCreate()\n\nkafkaDF = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"abcd.us-west4.gcp.confluent.cloud:9092\") \\\n    .option(\"subscribe\", \"illegal_dumping\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"\" password=\"\";\"\"\") \\\n    .load()\n\nprocessedDF = kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\ndisplay(processedDF)\n```\n\n- 단계 8: 변환 적용 — 처리된 데이터프레임에서 데이터를 가져 오기 위해 SQL 쿼리 작성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nimport pyspark.sql.functions as F\nfrom  pyspark.sql.functions import col, struct, to_json\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\n\njson_schema = StructType(\n  [\n    StructField(\"incident_type\", StringType(), nullable = False),\n    StructField(\"location\", StringType(), nullable = False),\n    StructField(\"description\", StringType(), nullable = True),\n    StructField(\"contact\", StringType(), nullable = True)\n  ]\n)\n\n# Using Spark SQL to write queries on the streaming data in processedDF\n\nquery = processedDF.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n      .select(F.col(\"value.incident_type\"),F.col(\"value.location\"))\ndisplay(query)\n```\n\nWe will now add another column called Region which would be mapped from the Location column, helping the authorities assign the issues to appropriate regional centres.\n\nDefine a UDF(User Defined Function) to find out the region from the location:\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n# Define the regions_to_states dictionary\nregions_to_states = {\n    'South': ['West Virginia', 'District of Columbia', 'Maryland', 'Virginia',\n              'Kentucky', 'Tennessee', 'North Carolina', 'Mississippi',\n              'Arkansas', 'Louisiana', 'Alabama', 'Georgia', 'South Carolina',\n              'Florida', 'Delaware'],\n    'Southwest': ['Arizona', 'New Mexico', 'Oklahoma', 'Texas'],\n    'West': ['Washington', 'Oregon', 'California', 'Nevada', 'Idaho', 'Montana',\n             'Wyoming', 'Utah', 'Colorado', 'Alaska', 'Hawaii'],\n    'Midwest': ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota',\n                'Iowa', 'Missouri', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana',\n                'Ohio'],\n    'Northeast': ['Maine', 'Vermont', 'New York', 'New Hampshire', 'Massachusetts',\n                  'Rhode Island', 'Connecticut', 'New Jersey', 'Pennsylvania']\n}\n\n#from geotext import GeoText\nfrom geopy.geocoders import Nominatim\n\n# Define a function to extract state names from location text\ndef extract_state(location_text):\n    geolocator = Nominatim(user_agent=\"my_application\")\n    location = geolocator.geocode(location_text)\n    #print(location)\n    #print(type(location.raw))\n    if location:\n        state = location.raw['display_name'].split(',')[-2]\n        return state\n    else:\n        return \"Unknown\"\n\n# Create a UDF to map states to regions\n@udf(StringType())\ndef map_state_to_region(location):\n    state = extract_state(location).strip()\n    for region, states in regions_to_states.items():\n        if state in states:\n            return region\n    return \"Unknown\"  # Return \"Unknown\" for states not found in the dictionary\n\n# Apply the UDF to map states to regions\ndf_with_region = query.withColumn(\"region\", map_state_to_region(query[\"location\"]))\n\ndisplay(df_with_region)\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 단계 9: 분석 수행 — 우리는 Description의 감정을 분석하여 사건 심각도를 나타내는 다른 열을 추가합니다. 이는 당국이 노력을 우선순위로 정하는 데 도움이 될 것입니다.\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# VADER 감정 분석기 초기화\nanalyzer = SentimentIntensityAnalyzer()\n\n# Description 텍스트에 대한 감정 분석을 수행하는 함수 정의\ndef analyze_sentiment(description):\n    # VADER에서 compound 감정 점수 가져오기\n    sentiment_score = analyzer.polarity_scores(description)['neg']\n\n    # 감정 점수를 기반으로 심각도 분류\n    if sentiment_score \u003e= 0.4:\n        return \"High\"\n    elif sentiment_score \u003e= 0.2 and sentiment_score \u003c 0.4:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# 감정 분석을 위한 UDF 생성\nsentiment_udf = udf(analyze_sentiment, StringType())\n\n# 처리된 DataFrame(processedDF)의 description 열에 UDF 적용\n# 실제 DataFrame 및 열 이름으로 \"processedDF\" 및 \"description_column\"을 대체합니다.\nprocessedDF_with_severity = query.withColumn(\"severity\", sentiment_udf(\"description\"))\n\n# 추가된 심각도 열이 있는 DataFrame 표시\ndisplay(processedDF_with_severity)\n```\n\n환경 위험 데이터로 훈련된 ML 분류 모델을 사용하거나 심각도 수준을 식별하기 위해 LLMs를 사용함으로써 심각도 UDF가 크게 개선될 수 있습니다. 그러나 간단함을 위해 여기에서는 vaderSentiment 분석기를 사용한 감정 분석을 사용했습니다.\n\n데이터에 대한 다양한 통계 분석을 수행할 수도 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Real-time Stream Processing](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_10.png)\n\n위와 같이 위치별로 그룹화하고 심각도를 기준으로 정렬하여 해당 카운트의 빈도를 얻을 수도 있습니다:\n\n또한 임시 뷰를 생성하고 해당 뷰에서 SQL 쿼리를 수행할 수도 있습니다:\n\n```js\n# 스트리밍 DataFrame을 임시 뷰로 등록\nprocessedDF_with_severity.createOrReplaceTempView(\"incident_reports\")\n\n# 집계를 위한 SQL 쿼리 정의\ntotal_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        COUNT(*) AS total_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type\n\"\"\"\n\nseverity_incidents_query = \"\"\"\n    SELECT\n        incident_type,\n        severity,\n        COUNT(*) AS severity_incidents\n    FROM\n        incident_reports\n    GROUP BY\n        incident_type, severity\n\"\"\"\n\n# 집계 수행\ntotal_incidents_df = spark.sql(total_incidents_query)\nseverity_incidents_df = spark.sql(severity_incidents_query)\n\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n![Data Engineering Concepts](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_11.png)\n\nSpark 구조화된 스트리밍 분석에 대한 일부 제한 사항:\n\n1. 비 시간 열에 대한 윈도우 함수를 수행할 수 없습니다.\n2. 전체 출력 모드에서 조인을 수행할 수 없습니다. 추가 모드에서만 가능합니다.\n\n- 단계 10: 델타 테이블을 만들고 분석 결과를 해당 테이블에 저장합니다\n\n```js\n# Delta Lake에 스트리밍 데이터를 저장할 경로 정의\ndelta_table_path = \"`result_delta_table`\"\n\n# 스트리밍 쿼리에 대한 체크포인트 위치 설정\ncheckpoint_location = \"/FileStore/tables/checkpoints\"\n\n# 체크포인트 및 트리거를 사용하여 스트리밍 쿼리 시작\nstreaming_query = processedDF_with_severity.writeStream\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_location)\\\n  .trigger(processingTime='10 seconds')\\ # 10초ごとに 데이터의 미크로배치를 처리할 트리거 정의\n  .format(\"delta\")\\\n  .toTable(delta_table_path)\n```\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러면 Delta 테이블에서 데이터프레임으로 스트림을 읽을 수 있습니다:\n\n![Delta Table as Dataframe](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_12.png)\n\n또는 Delta 테이블을 쿼리하기 위해 SQL을 사용할 수도 있습니다:\n\n![Query Delta Table with SQL](/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_13.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n읽어 주셔서 감사합니다 :) 실시간 스트리밍 데이터 아키텍처에 대한 빠른 통찰이 마음에 들었으면 좋겠네요. 데이터 엔지니어링 모험을 위해 행운을 빕니다! 여기 GitHub 저장소 링크입니다:\n\n","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringconceptsPart10RealtimeStreamProcessingwithSparkandKafka_0.png","tag":["Tech"],"readingTime":13},{"title":"데이터 엔지니어링 디자인 패턴","description":"","date":"2024-05-23 14:04","slug":"2024-05-23-DataEngineeringDesignPatterns","content":"\n\n디자인 패턴은 소프트웨어 엔지니어들만을 위한 것은 아닙니다. 최신 데이터 솔루션을 구축하는 데 도움이 되는 인기있는 데이터 엔지니어링 디자인 패턴을 알아봅시다.\n\n![이미지](/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png)\n\nELT 패턴: 추출, 로드, 변환\n\n이는 RDBMS 세계에서 인기있던 ETL 패턴의 후속입니다. 데이터 엔지니어들이 다양한 소스(RDBMS, API 또는 스크래핑)에서 데이터를 추출하고, S3, ADLS Gen 2, 또는 GCS와 같은 객체 스토어에 로드한 후 Databricks와 같은 현대적인 도구를 사용하여 효율적으로 변환하는 인기있는 공통 패턴입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Lakehouse Pattern](/assets/img/2024-05-23-DataEngineeringDesignPatterns_1.png)\n\nDatabricks은 Lakehouse 아키텍처의 선두주자입니다. 이는 데이터 레이크(Data Lake)와 데이터 웨어하우스(Datawarehouse)를 통합합니다. 원시 또는 가공된, 구조화된 또는 반구조화된 데이터가 모두 하나의 환경 안에 모두 사용 가능하며 하나의 플랫폼(Databricks)에서 액세스할 수 있습니다.\n\n![Lakehouse Pattern](/assets/img/2024-05-23-DataEngineeringDesignPatterns_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n중개 아키텍처 패턴:\n\n메달리온 아키텍처 패턴은 주로 Databricks와 함께 사용되며 이제는 사실상의 표준이 되었습니다. 데이터 처리를 위해 원본 데이터인 청동, 정리 및 풍부화된 데이터인 은, 그리고 비즈니스 수준의 집계한 데이터인 금 레이어로 이루어져 있습니다.\n\nDeltaLake 아키텍처 패턴:\n\nDelta Lake은 초기에 Databricks에서 개발된 오픈 소스 프로젝트입니다. 이 프로젝트는 데이터 호수에 안정성을 제공합니다. Deltalake는 ACID 트랜잭션, 타임 트래벌, z-order, CDC, 스키마 진화 및 기타 최적화로 데이터 호수를 개선합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nKappa Architecture Pattern:\n\nKappa Architecture에서는 모든 데이터가 스트림으로 처리됩니다. 이는 기존에 배치 처리로 다뤄졌을 것들을 연속적인 데이터 스트림으로 처리하는 것을 의미합니다. 시스템은 데이터가 도착하는 즉시 실시간으로 처리하며 별도의 배치로 처리하지 않습니다. Kappa Architecture는 실시간 처리에 최적화되어 있지만, \"배치\" 데이터로 간주될 수 있는 대규모의 과거 데이터를 스트림으로 재생하여 처리할 수 있습니다. Databricks Autoloader는 Kappa Architecture를 구현하는 데 가장 적합합니다.\n\nServerless Architecture Pattern:\n\n데이터 엔지니어링에서 서버리스는 클라우드 지역/IP 제약 사항이나 기타 기본 인프라 제약 조건에 대해 걱정하지 않고 데이터 파이프라인을 구축할 수 있습니다. 특히 변수 사용 패턴을 갖는 워크로드에 대한 즉시 클러스터 가용성과 비용 효율적인 스케일링을 위해 유용합니다. 많은 클라우드 제공업체가 이를 제공하고 있습니다. Databricks SQL은 Serverless를 제공하며, SQL 웨어하우스를 3초 미만의 시간 안에 론칭할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMLFlow 아키텍처 패턴:\n\nMLflow는 Databricks가 개발한 오픈 소스 프로젝트로, 실험, 재현성, 모델 배포 및 모델 제공을 포함한 기계 학습 라이프사이클을 관리하는 데 사용됩니다.\n\n이러한 패턴은 견고하고 확장 가능한 데이터 시스템을 설계하는 데 중요합니다. Databricks를 이용하면 이러한 패턴을 더욱 간편하게 구현할 수 있으며 효율적인 데이터 처리 및 분석을 위한 통합 도구를 제공합니다.\n\n좋아하는 패턴이 빠졌나요? 댓글 섹션에서 알려주세요. 흥미로운 내용이라고 생각되면 반가워 하지 마세요.","ogImage":{"url":"/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png"},"coverImage":"/assets/img/2024-05-23-DataEngineeringDesignPatterns_0.png","tag":["Tech"],"readingTime":3}],"page":"50","totalPageCount":98,"totalPageGroupCount":5,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"50"},"buildId":"o1YmnmSuZvAX2O4TI9r41","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>