<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/34" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/34" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_buildManifest.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="마이크로소프트 패브릭과 데이타브릭스 유니티 카탈로그 - 통합 시나리오 해석하기" href="/post/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="마이크로소프트 패브릭과 데이타브릭스 유니티 카탈로그 - 통합 시나리오 해석하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="마이크로소프트 패브릭과 데이타브릭스 유니티 카탈로그 - 통합 시나리오 해석하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">마이크로소프트 패브릭과 데이타브릭스 유니티 카탈로그 - 통합 시나리오 해석하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터브릭스, 데브옵스 및 파이테스트" href="/post/2024-06-19-DatabricksDevOpsandpytest"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터브릭스, 데브옵스 및 파이테스트" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터브릭스, 데브옵스 및 파이테스트" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">데이터브릭스, 데브옵스 및 파이테스트</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="단위 테스트 및 코드 모듈화를 위한 Databricks" href="/post/2024-06-19-UnitTestingandCodeModularizationinDatabricks"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="단위 테스트 및 코드 모듈화를 위한 Databricks" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="단위 테스트 및 코드 모듈화를 위한 Databricks" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">단위 테스트 및 코드 모듈화를 위한 Databricks</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="데이터  AI 서밋 by Databricks 2024의 주요 통찰 결과" href="/post/2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="데이터  AI 서밋 by Databricks 2024의 주요 통찰 결과" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="데이터  AI 서밋 by Databricks 2024의 주요 통찰 결과" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">데이터  AI 서밋 by Databricks 2024의 주요 통찰 결과</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Azure Databricks와 Microsoft Fabric 통합하기" href="/post/2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Azure Databricks와 Microsoft Fabric 통합하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Azure Databricks와 Microsoft Fabric 통합하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">Azure Databricks와 Microsoft Fabric 통합하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="2024년 데이타브릭스 데이터  AI 서밋에서의 소감들" href="/post/2024-06-19-ReflectionsfromDatabricksDataAISummit2024"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="2024년 데이타브릭스 데이터  AI 서밋에서의 소감들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="2024년 데이타브릭스 데이터  AI 서밋에서의 소감들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">2024년 데이타브릭스 데이터  AI 서밋에서의 소감들</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="DSPy와 Amazon Bedrock을 사용하여 견고한 AI 시스템 구축하기" href="/post/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="DSPy와 Amazon Bedrock을 사용하여 견고한 AI 시스템 구축하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="DSPy와 Amazon Bedrock을 사용하여 견고한 AI 시스템 구축하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">DSPy와 Amazon Bedrock을 사용하여 견고한 AI 시스템 구축하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="5개의 무료 엔드 투 엔드 데이터 엔지니어링 프로젝트" href="/post/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="5개의 무료 엔드 투 엔드 데이터 엔지니어링 프로젝트" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="5개의 무료 엔드 투 엔드 데이터 엔지니어링 프로젝트" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">5개의 무료 엔드 투 엔드 데이터 엔지니어링 프로젝트</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="미래 예약된 지연 크론 작업 처리를 위해 DynamoDB Streams 사용하기" href="/post/2024-06-19-UsingDynamoDBStreamstoHandleFutureScheduledDelayedCronJobs"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="미래 예약된 지연 크론 작업 처리를 위해 DynamoDB Streams 사용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-UsingDynamoDBStreamstoHandleFutureScheduledDelayedCronJobs_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="미래 예약된 지연 크론 작업 처리를 위해 DynamoDB Streams 사용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">미래 예약된 지연 크론 작업 처리를 위해 DynamoDB Streams 사용하기</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AWS에서 Terraform을 사용하여 Lambda 함수를 배포하는 방법" href="/post/2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AWS에서 Terraform을 사용하여 Lambda 함수를 배포하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AWS에서 Terraform을 사용하여 Lambda 함수를 배포하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">AWS에서 Terraform을 사용하여 Lambda 함수를 배포하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">Jun 19, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/21">21</a><a class="link" href="/posts/22">22</a><a class="link" href="/posts/23">23</a><a class="link" href="/posts/24">24</a><a class="link" href="/posts/25">25</a><a class="link" href="/posts/26">26</a><a class="link" href="/posts/27">27</a><a class="link" href="/posts/28">28</a><a class="link" href="/posts/29">29</a><a class="link" href="/posts/30">30</a><a class="link" href="/posts/31">31</a><a class="link" href="/posts/32">32</a><a class="link" href="/posts/33">33</a><a class="link posts_-active__YVJEi" href="/posts/34">34</a><a class="link" href="/posts/35">35</a><a class="link" href="/posts/36">36</a><a class="link" href="/posts/37">37</a><a class="link" href="/posts/38">38</a><a class="link" href="/posts/39">39</a><a class="link" href="/posts/40">40</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"마이크로소프트 패브릭과 데이타브릭스 유니티 카탈로그 - 통합 시나리오 해석하기","description":"","date":"2024-06-19 12:25","slug":"2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios","content":"\n올해 첫 번째 Microsoft Fabric 커뮤니티 컨퍼런스가 개최되었습니다. 첫 번째 날 키노트에서 Fabric와 Databricks Unity Catalog (UC) 통합을 쇼케이스하는 두 가지 미리보기가 있었어요.\n\n이전 블로그 게시물에서는 Databricks에서 OneLake로 쓰는 옵션 및 Fabric Spark에서 ADLS Gen2로 쓰는 옵션(Unity Catalog가 활성화된 클러스터가 아닌 경우)에 대해 알아보았습니다. 이 블로그 게시물은 Fabric + Unity Catalog 통합(Unity Catalog가 활성화된 클러스터와 관련된 다양한 시나리오를 밝히는 것을 목표로 합니다. Lakehouse 시나리오에 대한 자세한 내용은 Piethein의 게시물에서 확인해주세요.\n\n- Unity Catalog 테이블을 OneLake 카탈로그로 동기화할 수 있을까요? 어떻게요?\n- Unity Catalog가 활성화된 클러스터에서 OneLake로 쓸 수 있을까요?\n- Unity Catalog를 OneLake와 통합할 수 있을까요? SQL 엔드포인트 / Fabric 데이터 웨어하우스에 대해 페더레이티드 쿼리를 실행할 수 있을까요?\n\n참고: 본 글은 제 개인적인 경험과 견해를 반영한 것이며, Microsoft나 Databricks의 공식 입장을 대변하는 것은 아닙니다. 또한, 이 블로그 게시물은 잠재적인 시나리오를 개요로 제시하였지만, Fabric 로드맵이나 의도를 반영하는 것은 아닙니다. 미래에 모든 언급된 옵션이 운영되지는 않을 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# Databricks Unity Catalog and Microsoft Fabric\n\n통합 시나리오는 기본적으로 진입점에 따라 볼 수 있습니다. Unity Catalog 및 Fabric:\n\n- Fabric에서 Unity Catalog에 액세스하기 (Fabric → Unity Catalog): 이 기능을 통해 사용자는 Fabric 내에서 Unity Catalog 카탈로그, 스키마 및 테이블에 원활하게 액세스할 수 있습니다.\n- Unity Catalog에서 Fabric 활용하기 (DBX/Unity Catalog → Fabric): 이 기능을 사용하면 사용자는 Unity Catalog 내부에서 OneLake에 직접 액세스하고 SQL 엔드포인트 또는 Fabric 데이터 웨어하우스를 통해 연합 쿼리를 실행할 수 있습니다.\n\n이러한 시나리오를 자세히 살펴보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# Fabric → Unity Catalog\n\n## Fabric에서 Unity Catalog 사용하기\n\nFabric에서 Unity Catalog 테이블에 액세스할 수 있는 몇 가지 옵션이 있습니다. Fabric Spark에서 ADLS Gen2로 직접 읽기 및 쓰기도 가능합니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n현재 옵션\nUC 테이블에 바로 가기를 생성할 수 있는 옵션은 현재 두 가지 있습니다: 수동(지루한) 또는 반자동, 후자는 노트북을 통해 가능합니다. 반자동 방법을 사용하면 사용자들은 UC Delta 외부 테이블을 OneLake에 통합하여 바로 가기를 생성할 수 있습니다. 동기화를 위해 카탈로그와 스키마 이름을 지정하면, 해당 스키마 내 테이블에 대한 바로 가기가 Fabric 레이크하우스 내에 생성됩니다. 실행 유틸리티 노트북에 대한 추가 지침을 참조하세요.\n\n```js\n# configuration\ndbx_workspace = \"\u003cdatabricks_workspace_url\u003e\"\ndbx_token = \"\u003cpat_token\u003e\"\ndbx_uc_catalog = \"catalog1\"\ndbx_uc_schemas = '[\"schema1\", \"schema2\"]'\n\nfab_workspace_id = \"\u003cworkspace_id\u003e\"\nfab_lakehouse_id = \"\u003clakehouse_id\u003e\"\nfab_shortcut_connection_id = \"\u003cconnection_id\u003e\"\nfab_consider_dbx_uc_table_changes = True\n\n# sync UC tables to lakehouse\nsc.addPyFile('https://raw.githubusercontent.com/microsoft/fabric-samples/main/docs-samples/onelake/unity-catalog/util.py')\nfrom util import *\ndatabricks_config = {\n    'dbx_workspace': dbx_workspace,\n    'dbx_token': dbx_token,\n    'dbx_uc_catalog': dbx_uc_catalog,\n    'dbx_uc_schemas': json.loads(dbx_uc_schemas)\n}\nfabric_config = {\n    'workspace_id': fab_workspace_id,\n    'lakehouse_id': fab_lakehouse_id,\n    'shortcut_connection_id': fab_shortcut_connection_id,\n    \"consider_dbx_uc_table_changes\": fab_consider_dbx_uc_table_changes\n}\nsync_dbx_uc_tables_to_onelake(databricks_config, fabric_config)\n```\n\n가능한 미래 옵션\n\n- Fabric에서 Unity Catalog 네이티브 항목: 하이브 메타스토어 메타데이터 이동과 유사하게, Unity Catalog 메타데이터를 Fabric 레이크하우스로 동기화하여 Unity Catalog 테이블에 액세스할 수 있게 합니다. 이 시나리오의 한 예시는 FabCon에서 시연되었는데, 사용자들이 Fabric UI를 통해 Unity Catalog 테이블에 직접 액세스하고 쿼리할 수 있는 것을 보여주었습니다.\n- Fabric에 Unity Catalog 바로 가기: Dataverse 바로 가기와 유사하게, OneLake 바로 가기 UX는 잠재적으로 Unity Catalog 테이블에 대한 바로 가기 생성을 지원할 수 있을 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n참고: 델타 공유, Databricks에서 JDBC/ODBC, Fabric 데이터 파이프라인 Databricks 활동 등의 옵션은 여기에 언급되지 않았습니다.\n\n# Databricks/Unity 카탈로그 → Fabric\n\n## Databricks/Unity 카탈로그에서 Fabric 및 OneLake 사용하기\n\nUnity 카탈로그는 클라우드 객체 저장소 연결(예: ADLS Gen2)을 활용하고 외부 데이터 시스템에 연결하여 연합 쿼리를 실행하는 다양한 방법을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 현재 옵션입니다. 사용자들은 UC 활성화된 클러스터에서 OneLake를 다음과 같이 사용할 수 있습니다: (i) Service Principal (SPN) 기반 인증을 사용하여 OneLake에 r/w, 그리고 (ii) SPN 인증을 사용하여 mount 지점으로 OneLake에 r/w.\n\n```js\n# spn을 사용한 r/w\nworkspace_name = \"\u003c워크스페이스_이름\u003e\"\nlakehouse_name = \"\u003c레이크하우스_이름\u003e\"\ntenant_id = \"\u003c테넌트_ID\u003e\"\nservice_principal_id = \"\u003c서비스_프린시펄_ID\u003e\"\nservice_principal_password = \"\u003c서비스_프린시펄_비밀번호\u003e\"\n\nspark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id\", service_principal_id)\nspark.conf.set(\"fs.azure.account.oauth2.client.secret\", service_principal_password)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n\n# 읽기\ndf = spark.read.format(\"parquet\").load(f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse/Files/data\")\ndf.show(10)\n\n# 쓰기\ndf.write.format(\"delta\").mode(\"overwrite\").save(f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse/Tables/dbx_delta_spn\")\n```\n\n```js\n# spn으로 Mount\nworkspace_id = \"\u003c워크스페이스_ID\u003e\"\nlakehouse_id = \"\u003c레이크하우스_ID\u003e\"\ntenant_id = \"\u003c테넌트_ID\u003e\"\nservice_principal_id = \"\u003c서비스_프린시펄_ID\u003e\"\nservice_principal_password = \"\u003c서비스_프린시펄_비밀번호\u003e\"\n\nconfigs = {\n    \"fs.azure.account.auth.type\": \"OAuth\",\n    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n    \"fs.azure.account.oauth2.client.id\": service_principal_id,\n    \"fs.azure.account.oauth2.client.secret\": service_principal_password,\n    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n}\n\nmount_point = \"/mnt/onelake-fabric\"\ndbutils.fs.mount(\n    source = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com\",\n    mount_point = mount_point,\n    extra_configs = configs\n)\n\n# 읽기\ndf = spark.read.format(\"parquet\").load(f\"/mnt/onelake-fabric/{lakehouse_id}/Files/data\")\ndf.show(10)\n\n# 쓰기\ndf.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/onelake-fabric/{lakehouse_id}/Tables/dbx_delta_mount_spn\")\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n알림: OneLake abfss 경로 또는 마운트 경로를 사용하여 외부 테이블을 만들 경우 UC에서 예외가 발생할 수 있습니다. 현재, OneLake를 기본 저장소로 사용하여 UC에 외부 테이블을 등록할 수 없습니다. 이는 잠재적인 미래 시나리오로 이어질 수 있습니다.\n\n- INVALID_PARAMETER_VALUE: 클라우드 파일 시스템 스키마 누락\n- 목록을 위한 SAS 토큰을 획득하지 못했습니다. 유효하지 않은 Azure 경로\n\n잠재적인 미래 옵션\nADLS Gen2와 Azure Synapse와 유사하게, 미래에는 다른 옵션이 존재할 수 있습니다:\n\n- 기본 관리 저장소로서 OneLake: Databricks는 Unity Catalog의 자동 활성화를 시작했습니다. 즉, 자동으로 ADLS Gen2와 같이 Databricks 관리 저장소로 Unity Catalog 메타스토어를 자동으로 프로비저닝합니다. 그러나 사용자는 Unity Catalog 메타스토어를 OneLake를 가리키도록하면서 사용자 관리 수준 저장소를 생성할 수도 있습니다. 참고: 아직 이 기능은 불가능합니다.\n- 외부 위치로서 OneLake: 외부 위치는 카탈로그 및 스키마의 관리 저장소 위치를 정의하고, 외부 테이블 및 외부 볼륨의 위치를 정의하는 데 사용됩니다. 예를 들어, Spark에서 외부 테이블을 사용하는 경우 OneLake를 외부 위치로 활용할 수 있습니다. 참고: 아직 이 기능은 불가능합니다.\n- 볼륨용 OneLake: 볼륨은 클라우드 객체 저장소 위치의 논리적 저장 볼륨을 나타내며 비 탭식 데이터셋에 대한 관리 방식을 추가합니다. ADLS Gen2와 같이 OneLake를 사용하여 외부 및 관리 볼륨을 생성할 수 있습니다. 참고: 아직 이 기능은 불가능합니다.\n- 연합 Lakehouse: SQL 엔드포인트나 Fabric Data Warehouse의 데이터에 대한 읽기 전용 액세스는 UC 외부 카탈로그를 사용하여 미래 옵션으로 가능할 수 있습니다. 현재 Azure Synapse 및 SQL 액세스 인증은 사용자 이름/암호를 기반으로 하며 SPN은 아직 지원되지 않으므로 이 옵션은 아직 불가능합니다. 외부 카탈로그는 현재 객체 저장소를 지원하지 않으므로, 아직 OneLake/Lakehouse로의 외부 카탈로그 연결이 가능한지 여전히 불분명합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n표 태그를 Markdown 형식으로 변경해주세요.\n\n## 잠깐, 액세스 정책은 어떻게 되나요?\n\n다른 옵션으로는 Databricks의 ODBC를 SQL 엔드포인트로 사용하거나 Partner Connect( Power BI + Databricks) 및 스트리밍 옵션이 여기에 언급되지 않았네요.\n\n계속해서 Unity 카탈로그 액세스 정책이 OneLake RBAC 및 OneSecurity와 어떻게 조화를 이루고 있는지, Unity 카탈로그에서 Fabric로 보안 및 액세스 정책을 이동하거나 그 반대로 이동할 수 있는지에 대해 살펴볼 수 있을 것입니다.\n\n참고 문헌:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- Non-UC 시나리오: Databricks와 Fabric — OneLake 및 ADLS Gen2에 쓰기 | Aitor Murguzur 작성 | Medium\n- Lakehouse 시나리오: Azure Databricks와 Microsoft Fabric 통합 | Piethein Strengholt 작성 | 2024년 6월 | Medium\n- Databricks Unity Catalog를 OneLake와 통합 — Microsoft Fabric | Microsoft Learn\n","ogImage":{"url":"/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png"},"coverImage":"/assets/img/2024-06-19-MicrosoftFabricandDatabricksUnityCatalogunravelingtheintegrationscenarios_0.png","tag":["Tech"],"readingTime":10},{"title":"데이터브릭스, 데브옵스 및 파이테스트","description":"","date":"2024-06-19 12:24","slug":"2024-06-19-DatabricksDevOpsandpytest","content":"\nDatabricks에서 코드 품질을 지속적으로 보장하고 DevOps 작업 프로세스에 통합하는 방법에 궁금증을 풀어 보셨나요? 더 이상 망설이지 마세요.\n\n다음 예시에서는 Databricks에서 pytest와 DevOps를 사용하여 구현된 테스트 기능을 쉽게 시작하는 방법에 대해 살펴볼 것입니다.\n\n# 목표\n\n이 글을 마치면 Databricks에 구현된 함수를 테스트하고, DevOps에서 pull request가 제출될 때마다 테스트 스위트를 실행할 수 있게 될 것입니다. 아래에서 이를 어떻게 구현할지 살펴보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png\" /\u003e\n\n# DevOps에서 프로젝트 설정하기\n\nPytest는 두 가지 테스트 레이아웃을 지원하는데, 이 예시에서는 테스트가 애플리케이션 코드 외부에 배치되는 테스트 레이아웃을 사용할 것입니다. 이 분리는 나중에 데브옵스와 데이타브릭스 자산 번들을 사용하여 자동 릴리스를 다루는 기사에서 유용할 것입니다.\n\n```js\nproject.toml;\npipelines / pipeline_pytest.yml;\nsrc / functions / column_funtions.py;\nsoultion / demo_notebook.py;\ntests / test_column_funtions.py;\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 예시에서는 src(소스 코드)와 tests(테스트)로 구성된 두 개의 주요 폴더를 갖춘 간단한 설정이 있습니다. src 폴더는 지원하는 함수를 포함하는 functions와 Lakehouse를 구현하는 노트북을 포함하는 solution 폴더로 구분됩니다.\n\ncolumn_functions.py에서는 주어진 열을 제곱하는 간단한 함수를 구현했습니다.\n\n```python\n# Databricks notebook source\ndef column_squared(df, columnname):\n    df_squared = df.withColumn(columnname + \"_squared\", df[columnname] * df[columnname])\n    return df_squared\n```\n\ntest_column_functions.py에서는 column_functions.py의 함수 기능을 유효성 검사하는 간단한 테스트를 구현했습니다. 여기서 중요한 부분은 외부 데이터 소스나 스파크 세션에 의존하지 않고 독립적으로 유닛 테스트를 구현하고 있다는 것입니다. 입력과 예상 출력을 비교하기 위해 Databricks의 내장 기능을 사용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.testing import assertDataFrameEqual\nfrom src.functions.column_functions import column_squared\n\nclass TestColumnFuntions(object):\n    def test_column_squared(self):\n        spark = SparkSession.builder.getOrCreate()\n\n        source_data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 35)]\n        source_df = spark.createDataFrame(source_data, [\"name\", \"age\"])\n\n        df_actual = column_squared(source_df,'age')\n\n        expected_data = [(\"John\", 25, 625), (\"Alice\", 30, 900), (\"Bob\", 35, 1225)]\n        df_expected = spark.createDataFrame(expected_data, [\"name\", \"age\", \"age_squared\"])\n\n        assertDataFrameEqual(df_actual, df_expected)\n```\n\n# DevOps 파이프라인\n\n함수와 관련된 테스트를 구현한 후에는 이제 Azure DevOps에서 파이프라인을 설정할 수 있습니다.\n\n파이프라인(pipeline*pytest.yml)에 대해 가상 환경을 Python용으로 생성하고 필요한 패키지를 설치한 다음 'tests' 디렉토리에서 pytest를 실행합니다. 이 경우 pytest-azurepipelines를 사용하여 pytest를 DevOps 파이프라인에 통합합니다. 이제 pytest는 'test*.py'로 시작하거나 '\\_test.py'로 끝나는 모든 테스트를 찾아 이 경로를 따라 이동합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```yaml\ntrigger: none\n\nsteps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: \"3.9\"\n      addToPath: true\n\n  - script: |\n      python -m venv .venv\n      source .venv/bin/activate\n      python -m pip install --upgrade pip\n      pip install numpy==1.22.4\n      pip install pyspark\n      pip install pandas\n      pip install pyarrow\n      pip install pytest-azurepipelines\n      python -m pytest -vv tests\n    displayName: \"pytest\"\n```\n\n이제 메인 브랜치에서 빌드 검증을 설정하여, 개발자가 풀 리퀘스트를 만들 때마다 정의된 테스트가 실행되도록 할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-06-19-DatabricksDevOpsandpytest_1.png\" /\u003e\n\n# 결론\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 예시에서는 Databricks에서 구현된 기능에 대한 테스트 슈트를 설정하고 DevOps 워크플로에 통합하는 것이 얼마나 간단한지 살펴보았습니다. 이제 프로젝트에 필요한 테스트를 구현하여 지속적으로 고품질 코드를 제공할 수 있도록 만들어 보세요.\n","ogImage":{"url":"/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png"},"coverImage":"/assets/img/2024-06-19-DatabricksDevOpsandpytest_0.png","tag":["Tech"],"readingTime":5},{"title":"단위 테스트 및 코드 모듈화를 위한 Databricks","description":"","date":"2024-06-19 12:22","slug":"2024-06-19-UnitTestingandCodeModularizationinDatabricks","content":"\n노트북은 Databricks에서 데이터를 다루는 인기 있는 방법입니다. 노트북 사용자는 데이터를 빠르게 읽고 변환하며 상호적으로 탐색할 수 있습니다. 게다가, 노트북을 공유하고 협업하는 것은 간단합니다. 그러나 프로젝트가 확장될수록 코드 중복을 방지하고 재사용성을 용이하게 하는 모듈화 기능이 필요해집니다.\n\n이를 달성하는 한 가지 방법은 공유 함수를 포함하는 노트북을 생성하고 각 노트북의 시작 부분에서 실행하는 것입니다. 또는 모듈을 만들어 일반적인 Python 개발과 유사한 Python import 명령어를 사용할 수 있습니다. 긴 코드 블록을 함수로 나누면 코드의 재사용을 촉진할 뿐만 아니라 테스트도 용이해집니다.\n\n![이미지](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png)\n\n## 모듈화\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n파이썬에서 모듈화란 프로그램을 작은 관리 가능한 모듈로 나누는 것을 말합니다. 파이썬에서 코드를 모듈화하는 것에는 여러 가지 이점이 있습니다:\n\n- 재사용성: 모듈은 다른 프로젝트에서 다시 사용할 수 있어 재작성이 필요하지 않습니다.\n- 유지보수성: 작은 중점적인 모듈로 인해 업데이트와 디버깅이 쉬워집니다.\n- 확장성: 프로젝트가 성장할 때 효율적인 확장이 가능합니다.\n- 협업: 다른 개발자들이 동시에 작업하기를 용이하게 합니다.\n- 테스트: 단위 테스트가 간소화되어 더 신뢰할 수 있는 코드를 작성할 수 있습니다.\n- 가독성: 특정 작업에 집중함으로써 코드 이해가 향상됩니다.\n\nDatabricks에서 모듈을 사용하기 위해서는 클래스 또는 함수를 포함한 파일들로 구성된 폴더와 **init**.py 파일을 생성해야 합니다. 이는 Databricks에 모듈임을 알려줍니다. 아래는 공통 모듈과 함께 공유 함수, 변환 로직을 포함한 변환 모듈, 그리고 테스트 데이터가 포함된 내 솔루션의 구조입니다.\n\n코드 구조:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n워크스페이스\n├── test_data\n│    └── testdata.csv\n├── common\n│    └── __init__.py\n│    └── utilis.py\n├── transform\n│    └── __init__.py\n│    └── operations.py\n├── test_utils.py\n├── test_tran.py\n├── test\n```\n\ntestdata.csv:\n\n```js\nentity,iso_code,date,indicator,value\nUnited States,USA,2022-04-17,Daily ICU occupancy,\nUnited States,USA,2022-04-17,Daily ICU occupancy per million,4.1\nUnited States,USA,2022-04-17,Daily hospital occupancy,10000\nUnited States,USA,2022-04-17,Daily hospital occupancy per million,30.3\nUnited States,USA,2022-04-17,Weekly new hospital admissions,11000\nUnited States,USA,2022-04-17,Weekly new hospital admissions per million,32.8\n```\n\nulits.py:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef mask_func(col_val):\n    if col_val is not None:\n        if len(col_val)\u003e=16:\n            charList=list(col_val)\n            charList[4:12]='x'*8\n            return \"\".join(charList)\n        else:\n            return col_val\n    else:\n        return col_val\n```\n\noperations.py:\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, col\n\ndef deduplicate(df, uniq_col, orderby_col):\n    df = df.withColumn(\"rn\", row_number()\n        .over(Window.partitionBy(uniq_col)\n        .orderBy(col(orderby_col).desc())))\n\n    df = df.filter(col(\"rn\") == 1).drop(\"rn\")\n    return df\n\ndef clean_clients(df):\n    df = df.where(col(\"name\") != \"\").withColumn(\"timestamp\", col(\"timestamp\").cast(\"date\"))\n\n    return df\n```\n\n모듈에서 이러한 함수를 사용하려는 사람은 아래 예시와 같이 import 명령을 사용하여 노트북에 쉽게 추가할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![UnitTestingandCodeModularizationinDatabricks1](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_1.png)\n\nSimilarly, it’s possible to import transformation functions from the module and remove duplicated records from the DataFrame.\n\n![UnitTestingandCodeModularizationinDatabricks2](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_2.png)\n\n# Unit Testing in Databricks\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n소프트웨어 개발에서 단위 테스트는 코드의 정확성, 안정성, 유지 보수 가능성을 보장하는 중요한 요소입니다. 데이터 처리를 위해 노트북이 일반적으로 사용되는 Databricks에서는 단위 테스트가 더욱 중요해집니다.\n\nDatabricks에서 단위 테스트를 시작하려면 코드를 테스트할 수 있는 함수로 분해해야 합니다. 이 프로세스는 코드의 모듈성을 향상시키는 것뿐만 아니라 포괄적인 테스트 스위트를 작성하는 데 도움이 됩니다. Python에서 유닛 테스트를 수행하는 두 가지 인기있는 프레임워크인 Unittest와 pytest가 있습니다.\n\nUnittest 예시:\n\n```python\nimport unittest\n\nclass ExampleTestSuite(unittest.TestCase):\n\n    def test_import(self):\n        self.assertTrue(True)\n\n    def test_addition(self):\n        self.assertEqual(1 + 2, 3)\n\n    def test_subtraction(self):\n        self.assertNotEqual(1 - 2, 0)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n테이블 태그를 Markdown 형식으로 변경하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 왜 단위 테스팅을 해야 할까요?\n\n처음 테스트 중에 코드가 정상적으로 작동하는 것처럼 보이더라도, 단위 테스트는 여러 가지 이유로 중요한 역할을 합니다:\n\n- 정확성 확인: 단위 테스트는 코드의 개별 단위 기능을 확인하여 다양한 조건에서 예상대로 작동하는지 확인합니다.\n- 초기 버그 탐지: 개발 과정 초기에 버그를 식별함으로써, 개발자는 이를 신속히 해결하여 시스템의 다른 부분으로 전파되는 가능성을 줄일 수 있습니다.\n- 리팩토링 및 유지보수: 단위 테스트는 코드 리팩토링 및 유지보수 과정에서 안전망 역할을 하며, 개발자가 확신을 갖고 변경을 가할 수 있으면서도 일관된 동작을 보장합니다.\n- 회귀 테스트: 단위 테스트는 회귀 테스트로 작용하여 새로운 변경사항이나 기능이 기존의 기능을 망가뜨리지 않도록 하여 시스템의 안정성을 유지합니다.\n\n마스킹 기능에 대한 단위 테스트의 간단한 예제를 살펴보겠습니다. 이 단위 테스트는 입력 숫자가 올바르게 마스킹되거나 None을 반환하는지를 확인하여, 함수가 변경되더라도 예상되는 동작이 유지되도록 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\ntest_utils.py\n\n```python\nfrom common.utils import mask_func\n\ndef test_mask_func():\n    assert \"1234xxxxxxxx4568\" == mask_func(\"1234567891234568\")\n    assert mask_func(None) is None\n```\n\nETL(Extract, Transform, Load)과 같은 복잡한 프로세스의 경우, 데이터 변환 과정의 다양한 측면을 확인하는 데 개선된 테스트를 개발할 수 있습니다. 이러한 테스트에는 스키마 확인, 데이터프레임 비교, 행 수 유효성 검사 또는 특정 값의 존재 여부 확인이 포함될 수 있습니다.\n\ntest_tran.py:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nimport pytest\nfrom transform.operations import *\nfrom pyspark.testing.utils import assertDataFrameEqual\nfrom pyspark.sql import SparkSession\nfrom pyspark.testing import assertDataFrameEqual, assertSchemaEqual\nfrom pyspark.sql.types import *\nimport pandas as pd\n\n@pytest.fixture()\ndef spark():\n    return SparkSession.builder.appName(\"integrity-tests\").getOrCreate()\n\n@pytest.fixture()\ndef raw_input_df(spark):\n    df = pd.read_csv('test_data/testdata.csv')\n    return spark.createDataFrame(df)\n\n@pytest.fixture()\ndef test_df(spark):\n\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Los Angeles\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df = spark.createDataFrame(input_data, schema)\n\n    return df\n\ndef test_deduplicate(test_df, spark):\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df = spark.createDataFrame(input_data, schema)\n\n    df1 = deduplicate(test_df, \"Name\", \"timestamp\")\n    assertDataFrameEqual(df1, df)\n\ndef test_schema_deduplicated(test_df, spark):\n    schema = \"name STRING, age INTEGER, city STRING, timestamp STRING\"\n    input_data = [\n        (\"John\", 25, \"New York\", \"20210101\"),\n        (\"Jane\", 30, \"Chicago\", \"20220101\"),\n        (\"Doe\", 40, \"New York\", \"20210101\"),\n        (\"\", 39, \"New York\", \"20210101\"),\n    ]\n    df_expected = spark.createDataFrame(input_data, schema)\n\n    test_df = deduplicate(test_df, \"Name\", \"timestamp\")\n    assertSchemaEqual(test_df.schema, df_expected.schema)\n\ndef test_clean_clients(test_df, spark):\n    df = clean_clients(test_df)\n    assert df.where(\"name == '' \").count() == 0\n\ndef test_readfromfile(raw_input_df):\n    assert raw_input_df.count() \u003e 0\n```\n\n## Initializing Spark Session for Tests:\n\n테스트 파일은 주피터 노트북이 아니기 때문에, Spark 세션을 초기화하는 것이 필요합니다. 이를 위해서 `spark` 함수와 `fixture` 데코레이터를 사용해서 Spark 세션을 만들 수 있습니다. `fixture` 데코레이터는 자동으로 실행되며 각 테스트 함수에 해당하는 테스트 객체를 제공해주어 테스트 데이터의 생성 및 공유를 간편하게 할 수 있습니다.\n\n```js\n@pytest.fixture()\ndef spark():\n    return SparkSession.builder.appName(\"integrity-tests\").getOrCreate()\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n선언했던 fixture 데코레이터의 동작 방식을 주목하는 것이 중요합니다. 이 기법을 사용하면 테스트 데이터를 원활하게 실행하고 전달할 수 있습니다. 이 기법을 이용하면 Spark 세션을 생성하여 테스트 데이터를 로드하고 이를 테스트 함수 사이에서 공유할 수 있습니다. 테스트 데이터는 목록을 기반으로 생성하거나 테스트 파일에서 로드할 수 있습니다.\n\n```js\n@pytest.fixture()\ndef raw_input_df(spark):\n df = pd.read_csv('test_data/testdata.csv')\n\n return spark.createDataFrame(df)\n```\n\n테스트용 데이터 원본으로 샘플 파일을 사용하는 것도 가능합니다. 그러나 워크스페이스에서 로드해야 하는 경우에는 Spark가 워크스페이스로부터 파일을 직접 로드하는 것을 지원하지 않기 때문에 Pandas를 사용해야 합니다.\n\n## 모듈의 지연 변경 사항 처리하기:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모듈을 다룰 때 변경 내용을 구현하는 데 지연이 발생하는 것은 일반적입니다. 이 동작을 해결하기 위해 매직 함수를 사용할 수 있습니다:\n\n```js\n%load_ext autoreload\n\n%autoreload 2\n\n%aimport test_tran\n```\n\n# Databricks에서 단위 테스트 실행\n\nDatabricks에서 단위 테스트를 실행하려면 pytest 모듈을 호출하는 노트북을 생성해야 합니다. 아래는 지정된 저장소에서 테스트 실행을 트리거하는 코드 스니펫입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n테스트 노트북:\n\n```js\n%pip install pytest\n```\n\n```js\nimport pytest\nimport sys\nimport os, sys\n\nrepo_name = \"\u003c저장소 위치\u003e\"\n\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nrepo_root = os.path.dirname(os.path.dirname(notebook_path))\n\nos.chdir(f\"/Workspace/{repo_root}/{repo_name}\")\nprint(os.getcwd())\n# 읽기 전용 파일 시스템에 pyc 파일을 쓰지 않도록 설정합니다.\nsys.dont_write_bytecode = True\n\n# pytest 실행.\nretcode = pytest.main([\".\", \"-v\", \"-p\", \"no:cacheprovider\"])\n\n# 테스트 실패가 있는 경우 셀 실행 실패 처리합니다.\nassert retcode == 0, \"pytest 호출에 실패했습니다. 자세한 내용은 로그를 확인하세요.\"\n```\n\npytest를 실행하면 현재 디렉토리와 서브디렉토리에서 이름이 test\\__.py 또는 _\\_test.py 패턴을 따르는 모든 파일을 자동으로 실행합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n스크립트를 실행하면 다음과 비슷한 보고서가 표시됩니다:\n\n![보고서](/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_4.png)\n\n# 요약\n\n요약하면 모듈화와 유닛 테스팅은 소프트웨어 개발에서 널리 사용되는 관행이며, 이러한 적용은 데이터 엔지니어링 활동에 매끄럽게 확장됩니다. 코드의 모듈화 및 유닛 테스트를 구현하여 데이터 처리 솔루션이 더욱 신뢰성 있고 유연해집니다. 모듈화는 코드 구성 요소의 더 나은 조직화와 재사용을 가능하게 하며, 유닛 테스트는 각 구성 요소가 다양한 조건에서 예상대로 동작하는지 확인합니다. 이러한 기술들이 함께 사용되면 데이터 엔지니어링 솔루션의 전체적인 견고성과 유지보수성에 기여하며, 마지막으로 데이터 처리 파이프라인 및 워크플로의 품질을 향상시킵니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 이 기사를 유익하게 여기셨다면, 'clap' 버튼을 클릭하거나 LinkedIn에서 좋아요를 표시해 주시면 감사하겠습니다. 여러분의 지원을 감사히 여깁니다. 궁금한 점이나 조언이 있으시다면 언제든 LinkedIn에서 연락해 주세요.\n","ogImage":{"url":"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png"},"coverImage":"/assets/img/2024-06-19-UnitTestingandCodeModularizationinDatabricks_0.png","tag":["Tech"],"readingTime":13},{"title":"데이터  AI 서밋 by Databricks 2024의 주요 통찰 결과","description":"","date":"2024-06-19 12:21","slug":"2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024","content":"\nData + AI Summit by Databricks에서 얻은 주요 인사이트가 공개되었습니다! 다음과 같은 데이터 및 분석 공간의 흥미로운 업데이트가 있습니다.\n\n🚀 Unity Catalog가 이제 오픈 소스로 공개되었습니다! Duck DB와 같은 도구에서 Unity Catalog의 Delta 테이블에 외부에서 액세스할 수 있습니다. Azure Synapse, Azure SQL, Amazon Redshift, Snowflake를 추가하여 새로운 외부 데이터 소스에 연결할 수 있습니다.\n\n🛡️ Delta 테이블에 대한 규칙을 만들어 Unity Catalog에서 행 수준 보안 및 열 수준 가리기가 일반적으로 사용 가능해졌습니다.\n\n빌트인 데이터 품질 세부 정보로 GA로 된 Lake House 모니터링은 Delta 테이블을 모니터링하는 데 도움이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n📊 Unity Catalog metrics가 Databricks 및 Power BI, Tableau와 같은 외부 사용자를 위해 도입되어 데이터를 신뢰하고 사용할 수 있습니다.\n\n🔥 Delta 4.0은 파티셔닝과 관련된 문제를 극복하기 위해 더 나은 성능을 제공하는 Liquid 클러스터링을 도입했습니다. 작성 시 7배, 읽기 시 12배 빠른 성능을 제공합니다. JSON을 variant 데이터 유형으로 저장하는 VARIANT도 제공됩니다.\n\n💻 7월 1일부터 스파크 클러스터를 위한 100% 서버리스 인프라를 제공합니다. 초고속 클러스터를 즐기고 사용한 만큼 지불하며, 유휴 시간 요금은 없습니다. 클러스터 크기 조정, 자동 확장성, 스팟 인스턴스와 같은 복잡성에 작별을 고하세요.\n\n🔄 Databricks Lakeflow (곧 미리 보기 예정)는 DLT와 워크플로에 기반하여 데이터 수집, 변환, 오케스트레이션, 데이터 파이프라인의 모니터링을 간편화하는 솔루션입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n🔍 Databricks SQL은 이제 빠른 클러스터 시작, 향상된 성능, SQL UDF, 세션 변수, 그리고 SQL 분석가를 위한 AI 기능을 갖춘 데이터 웨어하우징 핵심 기능을 제공합니다.\n\n🤖 Databricks AI/BI에는 챗봇 \"Genie\"가 포함되어 있으며, Gen AI는 자연어 쿼리에서 자동으로 BI 대시보드를 만들어서 셀프 서비스 보고를 가능하게 합니다.\n\nApache Iceberg Tabular의 인수로 Delta lake에 이제 UniForm 형식을 통합하여 Parquet 및 Delta 형식과 함께 제공됩니다.\n\n스파크 4.0이 곧 출시될 예정이며 흥미로운 업데이트가 예정되어 있습니다. 더 많은 혁신을 기대해 주세요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n새로운 기능들을 탐험하는 것을 기대하고 있어요!\n\n만약 이 기사를 좋아하신다면, 저의 링크드인 페이지에서 저를 팔로우해주세요. https://www.linkedin.com/in/kaviprakash-selvaraj/\n","ogImage":{"url":"/assets/img/2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024_0.png"},"coverImage":"/assets/img/2024-06-19-KeyInsightsfromDataAISummitbyDatabricks2024_0.png","tag":["Tech"],"readingTime":3},{"title":"Azure Databricks와 Microsoft Fabric 통합하기","description":"","date":"2024-06-19 12:19","slug":"2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric","content":"\n고지: 본 글은 Microsoft나 Databricks의 공식 입장이 아닌 저의 개인적인 경험과 견해를 반영하고 있습니다.\n\n본 글은 고객 상호작용 중 자주 언급되는 핫 토픽인 Azure Databricks와 Microsoft Fabric의 조합과 통합에 대해 다룹니다. 두 서비스는 각자의 분야에서 최고 수준을 자랑합니다. Azure Databricks는 데이터 엔지니어링, 데이터 과학 및 머신 러닝 워크로드의 확장에 능합니다. 마찬가지로 Microsoft Fabric은 다양한 데이터 사용을 위한 간편성과 셀프 서비스 기능으로 빛을 발합니다. 보통 제기되는 핵심 질문은: 이 두 강자를 어떻게 통합할 수 있을까요?\n\n현재 고려해야 할 다섯 가지 옵션이 있습니다. 이 글은 새로운 기능이 추가됨에 따라 발전할 수 있음을 염두에 두십시오.\n\n- 보고 및 분석 레이어를 추가하여 Databricks를 활용한 아키텍처를 더욱 강화합니다.\n- OneLake 골드 레이어를 통합하여 Databricks를 활용한 아키텍처를 보완합니다.\n- Databricks가 모든 데이터를 OneLake에 기록하도록 합니다. 권장되지는 않지만 논의할 가치가 있습니다.\n- V-ORDERED 활성화된 소비 레이어를 통해 Databricks를 확장합니다.\n- 추가 구성 요소를 추가하여 Databricks 및 Microsoft Fabric의 데이터 처리 효율성을 향상시킵니다. 이는 좀 더 개인적인 접근입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n금일 제공되는 옵션은 다음 섹션에서 철저히 검토될 것입니다. 미묘한 차이점을 제공하고 장단점을 고려하며 관련 문서를 참조할 것입니다. 그러나 그보다 앞서, 두 강력한 도구를 활용하기로 선택하는 조직이 그 이유를 이해하는 데 도움이 됩니다.\n\n## 왜 이 조합을 선택하는가?\n\n조직은 Azure Databricks와 Microsoft Service Fabric을 결합하는 이유 때문에 이 조합이 제공하는 독특한 기능들을 선호합니다.\n\n다양한 규모의 조직에서 선호하는 종합 데이터 처리, 분석 및 데이터 과학 플랫폼인 Azure Databricks는 긴 역사와 다양한 조직에서의 성공적인 채택으로 신뢰할 수 있는 플랫폼으로 자리매김했습니다. Spark의 창시자들에 의해 설립된 Databricks는 주로 엔지니어들을 위해 제공되며, 대규모로 Spark 워크로드를 관리하고 노트북 작성 및 복잡한 작업을 처리할 수 있는 플랫폼을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마이크로소프트 패브릭의 매력은 그 간단함에 있습니다. 2023년에 출시되어 파워 BI에서 진화한 이 제품은 기존 파워 BI 사용자에게 쉬운 전환을 제안합니다. 사용자 친화적인 인터페이스, 통합 셀프 서비스 기능, 그리고 마이크로소프트 365와의 심플한 통합으로 비즈니스 사용자들의 특히 매력을 끌고 있습니다. 마이크로소프트 패브릭은 데이터 사용을 민주화하고 진입 장벽을 낮추기 위해 설계되어 있어, 모든 사용자에게 접근성 있는 플랫폼으로 인기를 끌고 있습니다.\n\n본질적으로, Azure Databricks와 마이크로소프트 서비스 패브릭의 결합은 기술적, 비즈니스적 요구를 모두 충족하는 종합적인 솔루션을 제공하여, 많은 조직들 사이에서 인기를 얻고 있습니다.\n\n이제 조직들이 종종 이 결합을 선택하는 이유를 알게 되었습니다. 이제 두 서비스를 어떻게 통합할 수 있는지 알아보겠습니다.\n\n## 리포팅 및 분석 레이어를 추가하여 데이터브릭스 지원 아키텍처를 강화하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n첫 번째 디자인 고려 사항은 일반적인 Azure Databricks Medallion Lakehouse 아키텍처를 개선하는 데 관여합니다. 이 아키텍처는 Azure Data Lake Storage (ADLS) gen2, Azure Data Factory 및 Azure Databricks와 같은 서비스를 활용합니다. 이 설정에서 Databricks는 데이터 투입, 처리, 검증 및 보강의 모든 측면을 관리합니다. PowerBI는 일반적으로 보고와 분석적인 통찰을 전달하는 것을 포함한 나머지 작업을 처리합니다.\n\nMicrosoft Fabric을 포함한 Databricks 중심 아키텍처를 확장하여 자기 서비스 기능을 강화하고 비즈니스 사용자를 위한 사용자 경험을 향상시키는 것은 인기 있는 전략입니다. Databricks와 PowerBI에 새로운 기능과 능력을 갖추어 더욱 매력적이고 효율적인 경험을 제공한다고 생각해보세요.\n\nMicrosoft는 최근 Microsoft Fabric을 위한 '바로 가기'라는 새로운 기능을 소개했습니다. 이 기능은 다양한 소스에서 데이터를 읽어내어 데이터 중복을 제거하고 직접 데이터를 사용할 수 있게 하는 가벼운 데이터 가상화 엔진 역할을 합니다. 예를 들어, PowerBI를 사용할 때 PowerBI에 데이터를 복사하거나 가져오지 않고 필요한 데이터에 즉시 액세스할 수 있습니다.\n\n우리가 이전에 이야기한 Databricks 중심 디자인과 관련해서, Databricks가 모든 데이터를 ADLS에 쓰기 때문에 ADLS Gen2 바로 가기 기능을 활용할 수 있습니다. 그러나 주의해야 할 중요한 고려 사항이 몇 가지 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 단축키를 사용하려면 패브릭 레이크하우스가 필요합니다. 이미 보유하고 있지 않다면, 하나를 만드는 것을 잊지 마세요.\n- 테이블에 대한 바로 가기는 Delta Lake 형식의 데이터에만 액세스할 수 있습니다.\n- 다브릭 관리 테이블 대신 외부 테이블에 대한 바로 가기를 가능한 한 사용하세요. 다음 설계 고려 사항을 논의할 때 이 부분에 다시 언급하겠습니다.\n- 각 바로 가기는 단일 Delta 폴더를 참조할 수 있습니다. 그러므로 여러 Delta 폴더에서 데이터에 액세스해야 한다면, 각 폴더에 대해 개별적인 바로 가기를 만들어야 할 것입니다.\n- 이러한 테이블 디렉토리에서 파일을 직접 조작하지 마세요. 대신, ADLS에서 Delta 파일을 읽는 읽기 전용 접근 방식을 사용하세요. 이 접근 방식에서 ADLS는 중간 저장소로 작동합니다. Databricks에서 테이블을 직접 읽지 않습니다.\n- Lakehouse에 바로 가기를 만드는 것은 Fabric UI를 통해 수동으로 수행해야 합니다. 또는 REST API를 사용하여 모든 바로 가기를 자동으로 제공할 수 있습니다. 여기 튜토리얼 및 노트북 스크립트 링크가 있습니다.\n- ADLS에서 데이터를 직접 읽을 때는 Unity 카탈로그의 보안 모델의 데이터 액세스 정책이 적용되지 않습니다.\n\nDatabricks와 Microsoft Fabric을 통합하는 과정에서 흥미로운 발전이 동행되고 있습니다! 이러한 기능들은 Microsoft Build 2024 컨퍼런스에서 발표되었습니다. 곧 Azure Databricks Unity Catalog를 Fabric과 통합할 수 있을 것입니다. Fabric 포털을 통해 새로운 Azure Databricks Unity Catalog 항목을 만들고 구성할 수 있을 것입니다. 이 단계를 거치면 Unity Catalog에서 관리되는 모든 테이블이 즉시 바로 가기로 업그레이드될 수 있을 것입니다. 이 예정된 통합은 Azure Databricks 데이터를 Fabric에 효율적으로 통합하여 Fabric 워크로드 전반에 걸쳐 원활한 운영을 지원할 것입니다. 이 새로운 기능의 데모는 여기에서 찾아볼 수 있습니다: https://www.youtube.com/watch?v=BYob0cGW0Nk\u0026t=4434s\n\n데이터 사용을 위해 Microsoft Fabric을 사용하는 확장된 Databricks 중심 아키텍처는 Databricks에 아주 만족한 고객들 사이에서 흔히 관찰됩니다. 이들 고객은 이미 Databricks를 사용하여 레이크하우스를 구축하는 데 상당한 시간과 자원을 투자하였으며, 이를 계속 활용할 계획입니다. Microsoft Fabric는 Delta 형식을 사용하는 레이크하우스 접근 방식의 강점과 다재다능성을 인식합니다. 이는 데이터 소비에 최적화된 레이어를 추가하여 (기존) 아키텍처를 향상시킬 수 있게 해줍니다. 이를 통해 조직은 Databricks 중심 설정에 데이터 사용을 위한 추가적인 레이어를 특별히 추가함으로써 기존 아키텍처를 강화할 수 있습니다.\n\n## OneLake 골드 레이어를 통합하여 Databricks가 가능한 아키텍처를 칭찬하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n두 번째 디자인은 초기 디자인 패턴을 수정하여 OneLake 골드 레이어를 구조에 통합합니다. 이것은 Azure Databricks의 Azure Blob 파일 시스템 (ABFS) 드라이버 덕분에 가능합니다. 이 드라이버는 ADLS와 OneLake를 모두 지원합니다. 아래에 이 접근 방식의 그림을 보실 수 있고 MS Learn 페이지에서 Notebook 예제를 찾을 수 있습니다.\n\n이 구조 내에서 전반적인 워크플로 및 데이터 처리 단계 — 데이터 수집, 처리, 유효성 검사, 데이터 보강 — 는 크게 변경되지 않습니다. 모든 것은 Azure Databricks 내에서 관리됩니다. 핵심 차이점은 이제 데이터 사용을 위한 데이터가 Microsoft Fabric에 더 가까워졌다는 것입니다. 왜냐하면 Databricks는 데이터를 OneLake에 저장된 Gold 레이어에 기록하기 때문입니다. 이것이 최선의 방법이며 이것이 왜 유익한지 궁금할 수 있습니다.\n\n중요한 점은 이 통합 방식이 Databricks에서 공식적으로 지원되지 않는다는 것이며, 데이터 관리에 영향을 미칩니다. 이에 관해 다음에 자세히 다룰 것입니다. 더 많은 정보를 원하시면 Databricks 문서를 참조해 주세요.\n\nDatabricks는 관리형 테이블과 외부 테이블 두 가지 유형의 테이블을 구분합니다. 관리형 테이블은 기본적으로 생성되며 Unity Catalog에 의해 관리되며 수명주기 및 파일 레이아웃을 제어합니다. 외부 도구를 사용하여 이러한 테이블에서 파일을 직접 조작하는 것은 권장되지 않습니다. 이에 반해, 외부 테이블은 메타스토어, 카탈로그 또는 스키마에 지정된 관리형 저장 위치 외부에 데이터를 저장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그래서, 문서에서 제공한 지침을 기반으로, 이 접근 방식을 사용하여 OneLake에 직접 쓰여진 모든 테이블은 외부 테이블로 분류하는 것이 권장됩니다. 이는 데이터가 메타스토어의 범위 바깥에서 관리되기 때문입니다. 그 결과, 이러한 테이블의 관리는 Fabric 내부와 같은 다른 곳에서 수행해야 합니다. 이 방식의 동기는 다음과 같을 수 있습니다:\n\n첫째, OneLake에 데이터를 물리적으로 저장하는 것은 Microsoft Fabric 내에서 성능을 향상시킵니다. 이는 OneLake 테이블이 특히 조인 및 집계와 관련된 쿼리에 최적화되어 있기 때문입니다. 그에 반해, ADLS Gen2를 통해 바로가기를 통해 데이터를 읽는 경우, 이러한 작업을 포함하는 쿼리에 대해 성능이 느릴 수 있습니다.\n\n둘째, OneLake에서 데이터를 관리하는 것은 Microsoft Fabric 내에서 보안 조치를 적용하는 데 유용합니다. 예를 들어, OneLake 테이블은 역할 기반 액세스 제어 (RBAC)를 사용하여 보안할 수 있어 데이터 액세스를 관리하는 과정이 단순해집니다. 그러나 ADLS Gen2를 사용할 경우, ADLS Gen2 저장소 계정의 권한을 처리해야 하므로 더 복잡할 수 있습니다.\n\n셋째, OneLake 테이블은 정책에 따라 관리될 수 있어 데이터가 규정에 따라 사용되도록 보장하기가 더 쉽습니다. 예를 들어, 다른 곳에 있는 도메인과 테이블을 (외부적으로) 공유할 때입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n데이터를 읽는 것 이외에도 Microsoft Fabric 내에서 새로운 데이터를 생성하는 것을 고려해 볼 수 있습니다. 만약 이것이 여러분의 계획 중 일부라면, 다가오는 기능이 매우 흥미로울 수 있습니다. 곧 Fabric 사용자들은 Unity 카탈로그를 통해 Azure Databricks에서 lakehouses와 같은 데이터 항목에 액세스할 수 있게 될 것입니다. 데이터는 여전히 OneLake에 남아 있겠지만, Azure Databricks에서 해당 데이터의 계보 및 다른 메타데이터에 직접 액세스하고 보는 능력을 갖게 될 것입니다. 이 향상된 기능은 Fabric에서 Databricks로 다시 데이터를 읽는 것을 용이하게 할 것입니다. 예를 들어, Azure Databricks의 Mosaic AI를 활용해 AI를 활용하려는 경우 Microsoft Fabric에서 다시 읽음으로써 가능할 것입니다. 이 기술은 아마도 Lakehouse Federation일 것입니다. 이 내용에 대한 자세한 정보는 이 비디오의 해당 부분에서 확인할 수 있습니다: https://youtu.be/BYob0cGW0Nk?t=4125\n\n마지막으로, 모든 통합 및 데이터 처리를 Databricks에서 처리하고 소비 레이어를 Fabric에서 관리하는 전략은 각 응용 프로그램 영역에서 가장 좋은 기능을 활용할 수 있도록 하는 편리함을 기관들에게 제공합니다. 이 접근 방식은 데이터 처리의 최적 성능과 보안을 보장합니다.\n\n## Databricks가 모든 데이터를 OneLake에 기록하도록 설정 (권장되지 않음)\n\nDatabricks를 OneLake와 통합하는 경험을 바탕으로, OneLake가 ADLS Gen2와 동일한 API를 지원한다는 것을 알고 있습니다. 이를 감안해 보면, 가상의 설계 가능성을 고려해 보겠습니다: 모든 Medallion 레이어를 OneLake에 저장하는 것입니다. 이 가능할까요? 알아보도록 하죠.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 접근 방식에 대한 인센티브는 신규 배포로부터 비롯될 수 있습니다. 여기서의 목표는 데이터 엔지니어링 작업을 효율적으로 확장하면서 Microsoft Fabric을 사용하여 모든 계층에서 데이터 사용 및 소비에 대한 설계 간결성과 셀프 서비스를 장려하는 데 Databricks의 기본 기능을 활용하는 것입니다.\n\n유감스럽게도, 이 설계는 효율적인 데이터 관리에 적합하지 않습니다. 이 구성은 각 워크스페이스 계층이 Microsoft Fabric의 자체 Lakehouse 엔터티를 필요로 하기 때문에 작업공간이 증가함에 따른 관리 오버헤드로 이어질 수 있습니다. 이 증식은 데이터 공유 시 통제, 메타데이터 관리 및 협업 오버헤드와 같은 추가적인 도전 과제를 야기할 수 있습니다. 또한 Databricks는 관리형 테이블을 사용할 때 이 접근 방식을 지원하지 않습니다. 따라서, 비록 이 아키텍처가 이론적으로 매력적으로 보일 수 있지만, 저는 최선의 실천으로 이를 사용하는 것을 강력히 비추합니다.\n\n## V-ORDERED 활성화 소비 계층으로 Databricks 확장\n\n다음 설계 고려사항은 Microsoft Fabric의 사용과 V-Order 기능을 활용하는 데 더 중점을 둘 것입니다. 이 기능은 파케이 파일 형식에 대한 라이트 타임 최적화로, Microsoft Fabric 컴퓨팅 엔진(예: Power BI)에서 빠른 데이터 읽기를 가능케 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nDatabricks와 Microsoft은 오픈 소스 열 기반 파일 형식인 Delta Lake를 채택하기로 선택했습니다. 그러나 Microsoft은 V-Order 압축의 추가 레이어를 통합했는데, 이것은 최대 50%의 더 많은 압축을 제공합니다. V-Order는 오픈 소스 parquet 형식과 완전히 호환되며, 모든 parquet 엔진이 일반 parquet 파일처럼 읽을 수 있습니다.\n\n참고로 V-Order를 적용하여 Fabric의 유지 관리 기능을 활용하면 V-Order가 없는 테이블에 적용할 수 있습니다.\n\nV-Order는 Microsoft Fabric에 중요한 이점을 제공하는데, 특히 Power BI 및 SQL 엔드포인트와 같은 구성 요소에게 도움이 됩니다. 예를 들어, Power BI를 사용하여 데이터 쿼리 중에 뛰어난 성능을 유지하면서 실시간 데이터에 직접 연결할 수 있습니다. 데이터 소스의 변경 사항이 Power BI에 즉시 반영되므로 새로 고침을 기다릴 필요가 없어집니다.\n\nV-Order로 최적화된 테이블을 사용하는 것은 현재 Microsoft Fabric에게만 제한되어 있습니다. Databricks는 아직이 기능을 통합하지 않았습니다. 따라서 그런 경우에는 V-Order로 최적화된 테이블을 활용하기 위해 Microsoft Fabric 내의 서비스를 활용해야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n먼저 실버(Silver)와 골드(Gold) 단계 사이의 Databricks를 통한 처리 단계가 V-Order 최적화가 필요하지 않을 경우에도 여전히 중요하다는 주장이 가능하다는 점을 감안해 볼 수 있습니다. 이 부분은 반복적으로 보일 수 있지만, Databricks를 이용한 지속적인 데이터 처리를 가능하게 하는 타당한 선택지입니다.\n\n다른 주목할만한 측면은 왜 조직이 이 설계를 선택하는지에 대한 것이며, 이는 여러 테이블 간의 거래 일관성을 유지하는 것입니다. 특히, 이러한 일관성을 Gold에서 유지하는 것은 매우 중요합니다. 현재 Spark는 개별 테이블에 대한 트랜잭션만 지원합니다. 따라서, 테이블 간에 데이터 불일치가 있는 경우 보상 조치를 통해 해결해야 할 수 있습니다. 예를 들어, 구매 주문에 대한 세 가지 테이블에 영향을 미치는 변경 사항을 하는 경우, 이러한 변경 사항을 하나의 트랜잭션으로 묶을 수 있습니다. 이것은 해당 테이블을 쿼리할 때 모든 변경 사항이 있거나 전혀 없을 것임을 의미합니다. 이러한 무결성 문제는 다수의 테이블에 걸쳐 복잡한 트랜잭션을 관리할 수 있는 환경의 중요성을 강조하며, Delta Lake 위에서 이를 지원할 수 있는 유일한 플랫폼은 Microsoft Fabric Warehouse입니다. 자세한 내용은 여기에서 확인할 수 있습니다.\n\n위 이미지에서 나타난 업데이트된 아키텍처에서는 Synapse Engineering이 이제 실버(Silver)에서 골드(Gold)로의 처리 엔진으로 작동합니다. 이 접근 방식은 모든 테이블이 V-Order 최적화되도록 보장합니다. 추가로, 트랜잭션 기능이 필요한 사용 사례를 위해 Synapse Warehouse가 추가되었습니다. 그러나 이러한 아키텍처 변경 사항은 데이터 엔지니어가 서로 다른 독특한 데이터 처리 서비스를 탐색해야 한다는 의미입니다. 따라서 모든 팀에게 명확한 지침을 제공하는 것이 중요합니다. 예를 들어, Databricks의 기본 기능인 AutoLoader와 Delta Live Tables를 활용한 데이터 품질 검증을 위해 브론즈(Bronze) 및 실버(Silver)에 대한 원칙을 수립하고, 골드(Gold)에서는 Microsoft Fabric와만 연동되는 소비특화 통합 로직 구축에 초점을 맞출 수 있습니다.\n\n## 추가 구성 요소를 추가하여 Databricks 및 Microsoft Fabric의 데이터 처리 효율성 향상하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리 이전 토론에서는 엔지니어들이 다른 데이터 처리 서비스를 탐험해야 하는 과제에 대해 논의했습니다. 이 문제는 메타데이터 주도 접근법과 DBT와 같은 템플릿 프레임워크를 채택하여 해결할 수 있습니다. 새로운 아키텍처에서는 Databricks와 Microsoft Fabric에 추가 구성 요소를 결합했습니다. 이 변경 사항에 대해 자세히 알아보도록 하겠습니다.\n\nDatabricks 측면에서는 메타데이터 주도 프레임워크(메타데이터 저장소), Great Expectations 및 Data Build Tool (DBT)를 추가했습니다. 메타데이터 주도 프레임워크는 작성하고 유지해야 하는 코드 양을 크게 줄일 수 있습니다. 다중 노트북을 생성하는 대신 이 접근 방식을 통해 모든 데이터의 수집 및 유효성 검사를 위한 범용 파이프라인을 활용할 수 있으며 Great Expectations라는 다른 오픈 소스 프레임워크로 데이터를 수집하고 유효성 검사할 수 있습니다. 이 접근 방식은 메타데이터 저장소에서 읽어와 다른 스크립트를 동적으로 호출함으로써 달성됩니다. 이 접근 방식에 대해 더 알고 싶다면 이 주제에 대한 또 다른 블로그 글을 추천합니다.\n\n다음으로 DBT에 대해 이야기해 봅시다. 이 오픈 소스 명령줄 도구인 데이터 빌드 툴(DBT)은 Python으로 작성되었습니다. 그 강점은 SQL의 SELECT 문과 유사한 구문을 사용하여 템플릿을 활용해 변환을 정의하는 범용 인터페이스를 제공하는 데 있습니다. Databricks는 dbt-databricks 패키지를 통해 지원됩니다. DBT와 Databricks를 사용하는 데 대한 자세한 정보를 원한다면 이 주제에 대한 다른 블로그 글을 읽는 것을 권장합니다.\n\nMicrosoft Fabric 측면에서도 DBT가 중요한 역할을 할 수 있습니다. Microsoft Fabric 내에서 Synapse Warehousing을 위해 dbt-fabric 또는 Synapse Spark을 위해 dbt-fabricspark 중에서 선택할 수 있습니다. 이 템플릿 접근 방식의 장점은 개발자가 모든 데이터 변환 사용 사례를 위한 단일 프론트엔드에 익숙해지기만 하면 두 서비스를 모두 활용할 수 있다는 점입니다. 이 방법론은 프로세스를 간소화하고 효율성을 높일 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 결론\n\nAzure Databricks와 Microsoft Fabric의 통합은 조직에 수많은 혜택과 가능성을 제공합니다. Azure Databricks의 유연성과 확장성이 Microsoft Fabric의 간편하고 사용자 친화적인 기능과 결합되면 모든 계층에서 데이터 사용 및 관리를 혁신적으로 개선할 수 있습니다. Databricks 중심 아키텍처를 Microsoft Fabric 레이어로 향상시키거나 성능 및 보안을 향상시키기 위해 아키텍처에 OneLake 골드 레이어를 통합하기까지 여러 아키텍처 디자인 선택지가 있습니다.\n\n또한, Microsoft Fabric의 V-Order 최적화 소개와 추가 구성 요소 사용은 데이터 처리 효율성을 현격히 향상시키고 간소화할 수 있습니다. 그러나 이러한 조합 또는 통합은 서비스 탐색과 유연성, 데이터 보안 및 격리를 균형있게 고려해야 할 수도 있습니다.\n\n요약하자면, Azure Databricks와 Microsoft Fabric의 통합은 Microsoft Build 2024 Conference에서 발표된 흥미로운 혁신과 함께 빅 데이터 처리 워크로드를 위한 희망찬 미래를 암시합니다.\n","ogImage":{"url":"/assets/img/2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric_0.png"},"coverImage":"/assets/img/2024-06-19-IntegratingAzureDatabricksandMicrosoftFabric_0.png","tag":["Tech"],"readingTime":15},{"title":"2024년 데이타브릭스 데이터  AI 서밋에서의 소감들","description":"","date":"2024-06-19 12:18","slug":"2024-06-19-ReflectionsfromDatabricksDataAISummit2024","content":"\n![Reflections from Databricks Data + AI Summit 2024](/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png)\n\n# 개요:\n\n2024년 Databricks Data + AI Summit에 참석한 것은 눈을 뜨게 하는 경험이었어요. 놀라운 주제 발표에서부터 분과 세션, 그리고 연구 내용까지 깊게 들여다보며, 이번 세미나는 오늘날의 디지턈 환경에서 높은 품질의 데이터와 AI의 변혁적인 힘을 강조했어요.\n\n이 개인적인 관점에서, 나에게 정말 기억에 남는 이벤트로 만든 몇 가지 주요 포인트와 영감을 주는 이야기들을 탐구해보겠어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 주요 연설:\n\n데이터브릭스는 이러한 연설들이 대부분 자가 마케팅과 자기 홍보에 관한 것이라는 내 생각을 바꿨어.\n\n- 나는 첫 번째 주요 연설에 마음이 없이 참석했어. 시간에 맞춰 도착했음에도 불구하고, 객석에는 16,000명 이상이 참석하여 수용 능력을 초과했어. 두 연설 모두 놀라운 발표와 데모로 이뤄진 롤러코스터 경험이었어 (나는 다음 섹션에서 조금 더 자세히 다룰 거야).\n- 데이터브릭스는 간단하게 유지했고, 그들이 실제 세계와 실제 사용 사례/문제를 해결하려고 했다는 것을 깨달았어. \"쉽게 얻을 수 있는 열매(l﻿ow-hanging fruit)\" 라는 용어를 들어봤어? 데이터 세계의 모든 사람들 — 엔지니어, 데이터 과학자, 분석가, 제품 관리자 및 리더들 — 매일 이러한 문제에 직면해. 그들의 모든 발표와 연설은 이 주제와 일치하며, Gen AI 시대의 기회와 혁신을 강조했어.\n- 대부분의 연설자는 분명히 데이터브릭스 출신이었어. 그들은 유용한 통찰, 새로운 구현 및 산업 트렌드를 공유했어. 몇 명의 다른 기술 기관 출신인 추가적인 중요 인물들로부터 이러한 트렌드가 확인되었어. 가죽 자켓을 입은 록스타 CEO를 포함한 다른 기술 기관 출신 중요 인물들도 포함돼. 내 새로운 조언, 내 3살 아이에게까지 하는 것은 이제 \"고통을 바라\"야!\n\n# 기술적 발표:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nDatabricks에서는 상당수의 릴리스가 있었는데, 그중 몇 가지를 발견해서 흥미롭다고 생각했습니다. 몇 개를 소개해 드리겠습니다. 뭔가 놓친 부분이 있을 수 있으니 양해 부탁드립니다.\n\n- 완전히 서버리스: 휴, 정말 감사합니다. 엔지니어의 업무를 훨씬 쉽게 만들어 줍니다. 오랫동안 기다려왔고 유지보수 및 비용 절감 가능성에 흥분하고 있어요!\n- AI BI: 이제 새로 발표된 GA 기능이 제공하는 것을 달성하기 위해 제3자 시각화 도구를 적게 사용할 것이라 확신합니다. 추가 보너스로 제공되는 Genie 기능을 활용하여 NLP를 사용해 제품용으로 준비된 즉석 대시보드를 생성할 수 있습니다!\n- 노코드 파인 튜닝: 데이터의 이 측면에 대해 전문가는 아니지만, 노코드를 사용한 모델 파인 튜닝이 게임 체인저가 될 것이라는 동료 데이터 과학자로부터 흥미로운 확언을 받았습니다!\n- LakeFlow ETL: 이제 다른 도구보다 Databricks를 선호하지 않았던 엔지니어들을 위한 노코드 솔루션입니다. 일괄 처리 또는 스트리밍 데이터의 가져오기를 위해 많은 코드를 작성해야 하는가요? 이제 그럴 필요가 없어졌습니다!\n- LLMOps 및 Mosaic AI 에이전트 프레임워크/평가: MlFlow의 ML 모델 프레임워크가 게임 체인저였다고 생각한다면, 이 기능도 즐길 것입니다. MLOps와 유사한 LLMOps(ML 오퍼레이션)가 공개되었을 때 Databricks는 이 분야에서 각 플랫폼보다 앞서 있다는 것을 증명했습니다.\n- 데이터 거버넌스, 모니터링 및 운영 탐지: 개인적으로 이것은 모든 ETL 도구에 내장되어야 한다고 생각하는 제가 특별히 좋아하는 부분입니다. Lakehouse로 자동화된 방식으로 이를 얼마나 구현했는지에 대해 너무나 흥미롭습니다!\n- Spark 4.0 발표: 현재는 이에 대해 넘어갈 거예요. Spark 3.x에서 제공되는 최신 기능을 모두 사용하지는 않겠다고 자신합니다. 그래도 제공되는 기능을 따르는 것에는 여전히 흥미를 느끼고 있습니다!\n- Unity Catalog OSS: \"마지막이지만 최고의\" 또는 여기에서 맥락을 두기 위해 \"위에 소개된 기능을 즐기고 싶다면 필수\"라고 말할 수 있습니다. Unity 표를 통해 모든 주요 SQL 엔진 및 데이터 형식에 액세스할 수 있습니다. 게임 체인저! 채택은 어려울 수 있지만 Databricks는 필요한 모든 지원을 제공하기 위해 열정적인 것으로 보입니다. 그리고, 'Spark'ing CTO가 라이브 영상에서 Git 리포를 공개한 것도 그만한 멋진 일이었습니다.\n\nLakeFlow 엔지니어링 디렉터와 쿠키 광고에 관한 마케팅 캠페인 제품 관리자의 멋진 데모에 큰 찬사를 보냅니다(실시간 디버깅이 재미있었고 잘 작동해서 기뻤습니다!). 또한 ‘X 배 더 저렴하고/빠르다’와 같은 기능 수준의 KPI가 매우 참신했습니다.\n\n# 특별한 언급 — Compound AI Systems Workshop:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 것들은 키노트 중에 고수준 발표/데모였습니다. 우리가 다 알다시피, \"악마는 세부 사항에 있다\"고 합니다. 그러한 세션 중에서 더 자세히 다루는 유용한 세션들이 몇 가지 있었습니다.\n\n- 모든 세션 또는 최고의 세션을 참석하는 것은 인간적으로 불가능하지만, 많은/대부분은 가상으로도 이용 가능할 것입니다, 이미 제공되고 있지 않다면.\n- 특별히, '워크샵'이라고 명시된 세션 중 어떤 것이라도 참석하시는 것을 강력히 권장합니다. 그리고 또한 7시간 동안 진행되는 세션 중 어떤 것이라도 참석해보세요. 이것이 개인적으로 저에게 가장 깨우침을 주는 경험이었으며 제게 Gen AI 분야의 놀라운 현재 연구자들의 마음에 들어가볼 수 있는 기회를 제공해주었습니다.\n- 영감을 주는 20-30개의 연구 주제 포스터가 있었습니다. 초록문을 차분히 읽거나 작가들의 이야기를 듣거나 심지어 데이터 애호가들로부터의 질문들을 관찰하는 것은 새로운 경지로 나를 인도해주었습니다.\n- 또한 교육, 로봇 공학 등 분야의 선도적인 연설자들이 참석하였으며 Anthropic, DeepMind, OpenAI, Microsoft, LangChain 등 최신 트렌드의 Gen AI 기관에서 온 패널리스트들과 함께 논의가 마무리되었습니다.\n\n# 마지막으로:\n\n가죽자켓을 입은 CEO가 정확하게 지적했습니다 - \"무엇이든 시작해보세요. 이것은 빠르게 움직이는 기차입니다. 지수적인 추세를 기다려보거나 관찰하고 있기 원치 않으실 겁니다.\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저번 주에 이 서밋 전에, 제가 따라가며 읽어본 상대적인 AI 구현을 강화하기 위해 Snowflake가 발표한 멋진 기능들에 대해 알아보았어요. 이 글을 큰 생각과 함께 마무리 짓는군요 — \"다음 단계는 너야, Snowflake. 이미!\"\n\nPS: 서밋을 통해 제게 지지를 보내준 리더, 팀 그리고 조직에게 정말 큰 감사를 표합니다!\n","ogImage":{"url":"/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png"},"coverImage":"/assets/img/2024-06-19-ReflectionsfromDatabricksDataAISummit2024_0.png","tag":["Tech"],"readingTime":5},{"title":"DSPy와 Amazon Bedrock을 사용하여 견고한 AI 시스템 구축하기","description":"","date":"2024-06-19 12:16","slug":"2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock","content":"\n## 프롬프트 매직에서 프롬프트 엔지니어링으로 변경\n\n![이미지](/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png)\n\n인공 지능이 다양한 산업을 혁신하면서, AI 모델을 개발하고 배포하기 위한 견고하고 확장 가능한 도구에 대한 필요성은 이제껏 없었습니다. 이 분야의 주목할만한 발전로는 Stanford의 최신 데이터 과학 도구인 DSpy와 AWS의 기계 학습을 위한 혁신적인 기반인 Amazon Bedrock이 있습니다. 이 블로그 글은 DSpy와 Amazon Bedrock 사이의 특징, 기능 및 독특한 시너지에 대해 파헤치며, 개발자와 데이터 과학자가 AI의 경계를 넓히는 데 어떻게 도움을 주는지 강조합니다.\n\n# DSPy란 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n%pip 설치 dspy-ai\n```\n\nDSPy는 스탠포드 NLP에서 개발한 오픈 소스 라이브러리로, 데이터 과학 워크플로우를 만들고 관리하는 프로세스를 간소화하기 위해 설계되었습니다. 이는 세 가지 핵심 구성 요소인 Signatures, Modules 및 Optimizers을 중심으로 구축되어 있습니다.\n\n## Signatures\n\nDSPy의 서명은 언어 모델(LM) 작업의 입력/출력 동작을 모듈식이고 적응적인 방식으로 정의합니다. Signatures는 길고 취약한 프롬프트에 의존하는 대신, 깨끗하고 재현 가능한 코드를 허용합니다. 서명의 예로는 질문 답변을 위한 `»question -` answer»`나 요약을 위한 `»document -` summary»`가 있습니다. 작업 요구 사항에 따라 서명은 간단하거나 복잡할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 모듈\n\nDSPy의 모듈은 LM 프로그램의 구성 요소입니다. 각 모듈은 chain-of-thought나 retrieval-augmented generation과 같은 특정 프롬프팅 기술을 추상화합니다. 모듈은 다양한 시그니처를 처리할 수 있으며, PyTorch와 같은 프레임워크의 신경망 레이어처럼 더 큰 프로그램으로 구성될 수 있습니다. 이를 통해 유연하고 확장 가능한 프로그램 구성이 가능해집니다.\n\n## 옵티마이저\n\nDSPy의 옵티마이저는 DSPy 프로그램의 매개변수를 세밀하게 조정하여 프로그램의 출력을 최적화합니다. 그들은 기울기 하강법과 이산 최적화 기술의 조합을 사용하여 메트릭을 최대화하거나 일반적으로 프로그램의 출력을 평가하는 함수에 점수를 부여합니다. 다양한 종류의 옵티마이저가 제공되며, 각각 다른 데이터 시나리오와 최적화 요구에 맞게 맞춤화됩니다. 옵티마이저가 가장 잘 작동하도록하려면 일부 학습 입력을 제공해야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# Amazon Bedrock을 사용하는 방법\n\n## 구성\n\n첫 번째 단계는 DSPy를 구성하여 기본적으로 Amazon Bedrock을 사용하도록 설정하는 것입니다:\n\n```js\nimport dspy\n\nbedrock_haiku = dspy.AWSAnthropic(\n    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n)\ndspy.configure(lm=bedrock_haiku)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLLM 구성이 마무리되었으니 문제 해결을 시작할 수 있어요.\n\n## 서명 및 모듈\n\n공식 DSPy 설명서에서 제안하는 대로 \"DSPy를 사용하는 8 단계\"를 따라서 작업을 정의해 보겠습니다. 처음에는 간단하게 질문과 답변 프로그램을 만들어 보죠. 따라서 우리의 입력은 질문이 되고, 출력은 답변이 될 거예요. 이를 위해 우리의 서명과 모듈을 정의할 수 있습니다:\n\n\\js\nqa = dspy.Predict(\"question -\u003e answer\")\n\\\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래 예시에서 예측은 우리의 모듈이며, 예측을 생성하는 것이 목표이며, 서명은 질문 - 답변입니다. 이는 우리가 DSPy에게 질문에서 답변을 찾고 있다는 것을 간결하게 설명하는 줄임표기법입니다. qa를 출력하면 다음 출력이 나타납니다:\n\n```js\nPredict(StringSignature(question -\u003e answer\n    instructions='주어진 필드 `question`으로 `answer` 필드를 생성하십시오.'\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n))\n```\n\nDSPy는 질문과 답변이 문자열임을 추론하고, 지시사항에서 강조된 프롬프트를 언어 모델의 입력으로 사용합니다. 타입을 직접 제어할 수도 있습니다:\n\n```js\ndspy.TypedPredictor(\"question:str -\u003e answer:int\")\n\n# 출력\nTypedPredictor(StringSignature(question -\u003e answer\n    instructions='주어진 필드 `question`으로 `answer` 필드를 생성하십시오.'\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n    answer = Field(annotation=int required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n))\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nModule Predict에서 정의된 프롬프트를 통해, DSPy는 프롬프트 엔지니어링 프로세스를 반복하고 제어할 수 있는 개념을 소개합니다. 이 클래스를 사용하여 다음 질문에 대한 답변을 생성해보겠습니다:\n\n```js\nqa(question=\"Sergio Mattarella는 누구인가?\").answer\n\n# 출력\n주어진 질문에 대한 답변은 다음과 같습니다:\n질문: Sergio Mattarella는 누구인가?\n답변: Sergio Mattarella는 현재 이탈리아의 대통령입니다. 그는 2015년부터 대통령으로 재직하고 있습니다.\n```\n\n## 서명 및 모듈을 위한 고급 구성\n\n이제 프로그램의 동작을 수정하기 위해 서명과/또는 모듈을 사용자 정의할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\ndspy.Modules에 대해 이야기해보겠습니다. 이 라이브러리에서 제공하는 다른 모듈을 사용하거나 사용자 정의 모듈을 만들 수 있습니다:\n\n- dspy.Predict: 기본 예측자입니다. 서명을 수정하지 않습니다. 학습의 주요 형태(즉, 지시 및 데모의 저장 및 LM 업데이트)를 처리합니다.\n- dspy.ChainOfThought: 서명의 응답을 확정하기 전 단계별로 생각하도록 LM에 가르칩니다.\n- dspy.ProgramOfThought: 실행 결과에 따라 응답을 결정할 코드를 출력하도록 LM에 가르칩니다.\n- dspy.ReAct: 주어진 서명을 구현하기 위해 도구를 사용할 수 있는 에이전트입니다.\n- dspy.MultiChainComparison: ChainOfThought에서 여러 출력을 비교하여 최종 예측을 생성할 수 있습니다.\n\n이전 출력인 dspy.Predict와 dspy.ChainOfThought를 비교해보겠습니다. 그러나 질문을 바꿔볼까요:\n\n```js\nquestion = \"True or False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\"\n\npredictor = dspy.Predict(\"question -\u003e answer\")\npredictor(question=question)\n\n# 결과\nPrediction(\n    answer='Question: True of False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\\nAnswer: True. The numbers 17, 9, 10, 12, 13, 4, and 2 add up to 67, which is an even number.'\n)\n\n------\n\ncot = dspy.ChainOfThought(\"question -\u003e answer\")\ncot(question=question)\n\n# 결과\nPrediction(\n    rationale=\"Question: True or False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\\nReasoning: Let's think step by step in order to determine if the numbers in this group add up to an even number.\\n1. We need to add up all the numbers in the group: 17 + 9 + 10 + 12 + 13 + 4 + 2 = 67.\\n2. 67 is an odd number, not an even number.\",\n    answer='False, the numbers in this group do not add up to an even number.'\n)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n두 출력 결과를 보면, 답변이 다르며, 후자가 올바른 것을 알 수 있습니다. 이는 DSPy가 Chain of Thought (CoT)를 통해 우리가 제공한 프롬프트를 확장하기 때문입니다. CoT를 사용하면 LM(Language Model)에게 답변을 제공하기 전에 \"단계별로\" 추론하도록 강요합니다. 이 근거는 답변에서 제공되며, 더 자세한 지침은 cot.extended_signature에서 확인할 수 있습니다.\n\n```js\ncot.extended_signature\n\n# 출력\nStringSignature(question -\u003e rationale, answer\n    instructions='`question` 필드를 주어 `answer` 필드를 생성하십시오.'\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '질문:', 'desc': '${question}'})\n    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"추론: 정답을 만들기 위해 '단계별로 생각해 봅시다. ${produce the answer}. We ...\", '__dspy_field_type': 'output'})\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': '답변:', 'desc': '${answer}'})\n)\n```\n\ndspy.Signature의 경우, 예를 들어 RAG에 매우 유용한 컨텍스트를 소개하려면 축약 표기를 확장할 수 있습니다:\n\n```js\ndspy.Predict(\"context, question -\u003e answer\")\n\n# 출력\nPredict(StringSignature(context, question -\u003e answer\n    instructions='`context`, `question` 필드를 주어 `answer` 필드를 생성하십시오.'\n    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '컨텍스트:', 'desc': '${context}'})\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '질문:', 'desc': '${question}'})\n    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': '답변:', 'desc': '${answer}'})\n))\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아니면 더 많은 제어를 위해 더 긴 표기를 사용해보세요:\n\n```js\nclass BasicQA(dspy.Signature):\n    \"\"\"문맥에 기반한 짧은 답변으로 질문에 대답합니다\"\"\"\n    context = dspy.InputField()\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"문맥에서 추출된 짧은 답변\")\n\n# 출력\nBasicQA(context, question -\u003e answer\n    instructions='문맥에 기반한 짧은 답변으로 질문에 대답합니다'\n    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '문맥:', 'desc': '${context}'})\n    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': '질문:', 'desc': '${question}'})\n    answer = Field(annotation=str required=True json_schema_extra={'desc': '문맥에서 추출된 짧은 답변', '__dspy_field_type': 'output', 'prefix': '답변:'})\n)\n```\n\n서명은 TypedPredictor 모듈 덕분에 pydantic 표기를 지원합니다:\n\n```js\nimport dspy\nfrom pydantic import BaseModel, Field\nfrom dspy.functional import TypedPredictor\nfrom datetime import datetime\nfrom textwrap import dedent\n\nclass TravelInformation(BaseModel):\n    origin: str = Field(pattern=r\"^[A-Z]{3}$\")\n    destination: str = Field(pattern=r\"^[A-Z]{3}$\")\n    date: str\n    confidence: float = Field(gt=0, lt=1)\n\nclass TravelSignature(dspy.Signature):\n    \"\"\" 주어진 이메일에서 모든 여행 정보를 추출합니다 \"\"\"\n    email: str = dspy.InputField()\n    flight_information: list[TravelInformation] = dspy.OutputField()\n\npredictor = TypedPredictor(TravelSignature)\npredictor(email=dedent(\"\"\"\n    Amazon Web Services Airlines로 예약해 주셔서 감사합니다.\n    2024년 6월 18일 바리에서 라스베이거스로 가는 XYZ123 편에 예약이 완료되었으며 탑승을 환영합니다.\n    즐거운 여행 되세요.\n\"\"\"))\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 리트리버\n\n프로그램은 dspy.Retrieve 클래스를 확장하여 검색 시스템을 구현할 수도 있습니다. 사용 가능한 리트리버의 최신 목록을 확인하려면 DSPy GitHub 저장소의 dspy.retrievers 모듈을 참조해주세요.\n\nAmazon Bedrock와 함관해서 리트리버를 사용하려면 사용자 정의 SentenceVectorizer 클래스를 만들어야 합니다. 미리 해당 작업을 수행해 두었습니다. (그런데, 이를 DSPy 팀이 공식적으로 구현하길 원하시면 PR #1151에 +1을 부탁드립니다):\n\n```python\nimport boto3\nimport json\nimport numpy as np\nfrom typing import List, Optional\nfrom dsp.modules.sentence_vectorizer import BaseSentenceVectorizer\n\nclass AmazonBedrockVectorizer(BaseSentenceVectorizer):\n    '''\n    이 벡터화기는 텍스트를 임베딩으로 변환하기 위해 Amazon Bedrock API를 사용합니다.\n    '''\n    SUPPORTED_MODELS = [\n        \"amazon.titan-embed-text-v1\", \"amazon.titan-embed-text-v2:0\",\n        \"cohere.embed-english-v3\", \"cohere.embed-multilingual-v3\"\n    ]\n\n    def __init__(\n        self,\n        model_id: str = 'amazon.titan-embed-text-v2:0',\n        embed_batch_size: int = 128,\n        region_name: str = 'us-west-2',\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n    ):\n        self.model_id = model_id\n        self.embed_batch_size = embed_batch_size\n\n        # Bedrock 클라이언트 초기화\n        self.bedrock_client = boto3.client(\n            service_name='bedrock-runtime',\n            region_name=region_name,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key\n        )\n\n    def __call__(self, inp_examples: List[\"Example\"]) -\u003e np.ndarray:\n        text_to_vectorize = self._extract_text_from_examples(inp_examples)\n        embeddings_list = []\n\n        n_batches = (len(text_to_vectorize) - 1) // self.embed_batch_size + 1\n        for cur_batch_idx in range(n_batches):\n            start_idx = cur_batch_idx * self.embed_batch_size\n            end_idx = (cur_batch_idx + 1) * self.embed_batch_size\n            cur_batch = text_to_vectorize[start_idx: end_idx]\n\n            # Bedrock API Body 구성\n            if self.model_id not in self.SUPPORTED_MODELS:\n                raise Exception(f\"지원하지 않는 모델: {self.model_id}\")\n\n            if self.model_id == \"amazon.titan-embed-text-v1\":\n                if self.embed_batch_size == 1:\n                    body = json.dumps({\"inputText\": cur_batch[0]})\n                else:\n                    raise Exception(f\"모델 {self.model_id}은 배치 크기 1을 전용으로 지원합니다.\")\n            elif self.model_id == \"amazon.titan-embed-text-v2:0\":\n                if self.embed_batch_size == 1:\n                    body = json.dumps({\n                        \"inputText\": cur_batch[0],\n                        \"dimensions\": 512\n                    })\n                else:\n                    raise Exception(f\"모델 {self.model_id}은 배치 크기 1을 전용으로 지원합니다.\")\n            elif self.model_id.startswith(\"cohere.embed\"):\n                body = json.dumps({\n                    \"texts\": cur_batch,\n                    \"input_type\": \"search_document\"\n                })\n            else:\n                raise Exception(\"여기서 어떻게 나타났나요?\")\n\n\n            # Bedrock API 호출\n            response = self.bedrock_client.invoke_model(\n                body=body,\n                modelId=self.model_id,\n                accept='application/json',\n                contentType='application/json'\n            )\n\n            response_body = json.loads(response['body'].read())\n            if self.model_id.startswith(\"cohere.embed\"):\n                cur_batch_embeddings = response_body['embeddings']\n            elif self.model_id.startswith(\"amazon.titan-embed-text\"):\n                cur_batch_embeddings = response_body['embedding']\n            else:\n                raise Exception(f\"아직 구현되지 않았습니다! Amazon Bedrock 문서에서 모델 {self.model_id}의 응답 형식을 확인하세요: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\")\n            embeddings_list.extend(cur_batch_embeddings)\n\n        embeddings = np.array(embeddings_list, dtype=np.float32)\n        return embeddings\n\n    def _extract_text_from_examples(self, inp_examples: List) -\u003e List[str]:\n        if isinstance(inp_examples[0], str):\n            return inp_examples\n        return [\" \".join([example[key] for key in example._input_keys]) for example in inp_examples]\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래와 같이 코드를 사용하여 선호하는 DSPy 검색기에서 이 클래스를 사용할 수 있습니다:\n\n```js\nfrom dspy.retrieve.faiss_rm import FaissRM\n\ndocument_chunks = [\n    \"...\"\n]\n\nfrm = FaissRM(\n    document_chunks=document_chunks,\n    vectorizer=AmazonBedrockVectorizer(\n        embed_batch_size=128, model_id=\"cohere.embed-english-v3\"\n        # OR:\n        # embed_batch_size=1, model_id=\"amazon.titan-embed-text-v2:0\"\n    )\n)\nprint(frm([\"여기에 질문을 입력하세요\"]))\n```\n\n## 사용자 정의 프로그램\n\n이 지식을 활용하여 프로그램의 동작을 정의하는 사용자 정의 클래스를 정의할 수 있습니다! 예를 들어, RAG 클래스는 다음과 같이 보일 것입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        # 'Retrieve' will use the user's default retrieval settings unless overriden.\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        # 'ChainOfThought' with signature that generates answers given retrieval \u0026 question.\n        self.generate_answer = dspy.ChainOfThought(\"context, question -\u003e answer\")\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        return self.generate_answer(context=context, question=question)\n```\n\n이 코드를 실행하기 전에 선호하는 검색기를 구성해야 합니다.\n\n# 결론\n\nDSPy와 Amazon Bedrock은 인공지능(AI) 개발 도구의 진화에서 중요한 발전을 나타냅니다. DSPy의 데이터 과학 능력과 Bedrock의 확장 가능하고 효율적인 모델 관리를 결합하여 개발자와 데이터 과학자는 복잡한 AI 과제에 대처할 강력한 도구 상자를 갖추게 됩니다. 이러한 도구들이 계속 발전함에 따라, 그들은 의심할 여지 없이 AI의 미래를 형성하는 데 중추적인 역할을 할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n자세한 정보는 DSPy GitHub 저장소와 Amazon Bedrock 문서를 살펴보세요. 이 흥미로운 분야에서의 미래 업데이트와 진전에 주목해 주세요!\n","ogImage":{"url":"/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png"},"coverImage":"/assets/img/2024-06-19-BuildingRobustAISystemswithDSPyandAmazonBedrock_0.png","tag":["Tech"],"readingTime":17},{"title":"5개의 무료 엔드 투 엔드 데이터 엔지니어링 프로젝트","description":"","date":"2024-06-19 12:15","slug":"2024-06-19-5FREEEnd-To-EndDataEngineeringProjects","content":"\n이 5개의 프로젝트를 수행함으로써 AWS, GCP 및 Azure를 배울 수 있습니다.\n\n이 프로젝트들은 총 200만 회 이상의 조회수를 갖고 있습니다. 그 이유가 있을 텐데요.\n\n![이미지](/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png)\n\n데이터 엔지니어링은 복잡한 분야로 들릴 수 있지만, 솔직히 말해서 그렇습니다. 하지만 자전거를 타는 것을 배우는 것처럼 연습을 통해 더 쉽고 직관적으로 되는 법이죠.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n시작하거나 기술을 향상시키는 데 도움이 되기 위해, 무료로 작업할 수 있는 멋진 다섯 가지 프로젝트를 골라왔어요. 이 프로젝트들은 어떤 것이든 아니에요; 즐겁고 매력적이며 다양한 주제와 도구를 다루고 있어요. 함께 시작해봐요!\n\n# 1. YouTube 데이터 분석\n\n유튜브 비디오가 인기있는 이유가 궁금했던 적이 있나요? 이 프로젝트에서 실제 유튜브 데이터를 활용하여 그것을 알아볼 수 있어요. 데이터 마법사가 되어야 하는 것은 아니에요. Python이라는 학습이 쉬운 프로그래밍 언어와 큰 데이터를 처리하는데 사용되는 PySpark을 익힐 수 있을 거예요. 또한 데이터를 관리하는 언어인 SQL과 Athena, Glue, Redshift, S3와 같은 다양한 AWS(아마존 웹 서비스) 도구를 사용해볼 수 있어요. 이 도구들은 데이터 세계에서 큰 이름이며, 어떻게 함께 문제를 해결하는지 배울 수 있을 거예요.\n\n개발할 수 있는 기술:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- Python 및 PySpark으로 코딩하기\n- 데이터 관리를 위한 기본 SQL\n- 현실 세계 문제의 이해 및 해결\n- 데이터 프로젝트를 위한 AWS 도구 사용\n\n## 2. Airflow를 활용한 Twitter 데이터 파이프라인\n\nTwitter는 데이터의 보고이에요. 이 프로젝트에서는 Twitter 데이터 수집, 처리 및 저장 프로세스를 자동화하는 방법을 배울 거예요. Airflow를 사용하여 이러한 작업을 예약하고 조직화하는 방법을 직접 확인하게 될 거예요. 또한, 이 프로젝트는 Twitter 데이터에 액세스하기 위한 Tweepy 및 데이터 분석을 위한 Python 라이브러리인 Pandas를 소개합니다. 데이터 엔지니어들에게 널리 사용되는 ETL (추출, 변환, 로드) 작업 작성에 익숙해지는 좋은 기회가 될 거예요.\n\n개발할 기술:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- Python 프로그래밍\n- Airflow를 사용하여 작업 자동화\n- 데이터 수집 및 분석\n- API 및 클라우드 저장소 사용\n\n## 3. 실시간 주식 시장 분석\n\n주식 시장 트렌드를 실시간으로 예측할 수 있다면 어떨까요? 이 프로젝트는 당신을 그 현실에 더 가깝게 이끌어줍니다. Python과 Kafka(실시간 데이터 스트림 처리 플랫폼)를 사용하여 주식 시장 데이터를 실시간으로 분석하는 애플리케이션을 구축할 것입니다. EC2 인스턴스(서버 유형)에 Kafka를 설정하고 데이터 파이프라인을 생성하는 것이 재미있는 부분입니다. 마법 같은 프로젝트이지만 실용적인 기술에 근거해 있습니다.\n\n개발할 기술:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 실시간 애플리케이션 구축\n- 데이터 스트림을 위한 Kafka 이해\n- 클라우드 서버 설정\n- 실시간 데이터 분석\n\n## 4. GCP에서 Uber 데이터 분석\n\nUber의 데이터는 방대하고 다양하여 분석 프로젝트에 이상적입니다. 원시 데이터를 이해하고 데이터 모델을 작성하며 ETL 작업을 위한 스크립트를 작성하는 방법을 배울 수 있습니다. 이 프로젝트에서는 데이터 파이프라인을 구축하는 현대적인 도구인 mage와 데이터 분석을 위한 SQL도 소개됩니다. 게다가 Google Cloud Platform (GCP)에서 작업하게 되어 선도적인 클라우드 서비스 중 하나인 GCP의 최신 기술 스택을 직접 경험할 수 있습니다.\n\n개발할 수 있는 기술:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 데이터 모델링 및 분석\n- ETL 스크립트 작성 및 자동화\n- SQL 쿼리 작성\n- 클라우드 기반 데이터 도구 사용\n\n# 5. Azure에서 올림픽 데이터 분석\n\n올림픽은 많은 데이터를 생성합니다. 이 프로젝트에서는 이 데이터를 API에서 추출하고 Microsoft의 클라우드 플랫폼인 Azure를 사용하여 분석하는 방법을 배울 수 있습니다. DataBricks와 같은 대용량 데이터용 서비스, 데이터 통합용 DataFactory, 대규모 데이터 분석용 Synapse Analytics 등을 사용할 수 있습니다. 이 프로젝트는 규모에 맞는 데이터 처리 방법을 가르치며 향후 올림픽 전략에 영향을 줄 수 있는 통찰을 제공할 것입니다.\n\n개발할 수 있는 기술:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- API에서 데이터 추출하기\n- 데이터 엔지니어링을 위해 Azure 서비스 사용하기\n- 데이터 처리를 위한 Spark 코드 작성하기\n- SQL을 사용한 고급 데이터 분석\n\n도움이 되었다면 저의 게시물을 팔로우하는 것을 잊지 마세요 :)\n\n관심이 있다면 여기에서 데이터 엔지니어링 기초 과정을 확인할 수 있습니다 —\n\n데이터 엔지니어링/과학/분석/LLM에 관한 놀라운 블로그를 더 많이 게시할 예정입니다\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n환영합니다!\n","ogImage":{"url":"/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png"},"coverImage":"/assets/img/2024-06-19-5FREEEnd-To-EndDataEngineeringProjects_0.png","tag":["Tech"],"readingTime":5},{"title":"미래 예약된 지연 크론 작업 처리를 위해 DynamoDB Streams 사용하기","description":"","date":"2024-06-19 12:13","slug":"2024-06-19-UsingDynamoDBStreamstoHandleFutureScheduledDelayedCronJobs","content":"\n크론 작업, \"백그라운드 작업\" 또는 클라우드워치 이벤트 대신 유연한 일정에 필요한 작업을 수행하는 대체 수단입니다.\n\n많은 소프트웨어 시스템은 작업을 예약할 수 있는 메커니즘이 필요하며, \"크론\" (또는 AWS 클라우드의 클라우드워치 이벤트/예약된 작업) 또는 \"지연 작업\" 시스템은 일반적인 해결책입니다. 그러나 특정 유형의 작업에 대해 꽤 좋거나 분명히 크론보다 나은 대안이 있습니다. 해당 대안은 TTL(생존 기간)이 있는 DynamoDB 레코드 및 람다 함수로 처리된 DynamoDB 스트림을 사용하는 것입니다. 레코드의 TTL이 만료될 때 람다가 트리거되며, 그때 레코드를 필요에 따라 처리할 수 있습니다. 이러한 종류의 작업에는 어떤 작업을 할 수 있을까요? 어떤 작업에서는 잠재적으로 변경 가능하거나 동적이며 또는 \"x 분 후\" 유형의 일정이 필요한 경우가 있습니다. 가능한 고정된 크론 스타일 일정(예: 매주 화요일 정오에 실행) 대신에 유연한 일정을 원하는 작업에 적합합니다. 또한 개별화되거나 희소한 상황에 대해 매우 효율적이고 비용 효율적인 개별 레코드를 기반으로 트리거하는 것이 모든 레코드를 대상으로 프로세스를 실행하는 것보다 우수합니다.\n\n몇 가지 사용 사례 예시:\n\n- 이벤트 전 X일 또는 마감일 이후 X일 후에 메시지를 전송하는 알림 메시지 보내기.\n- X 시간 동안 독자적 상태/상태를 변경하거나 읽지 않은 경우 상태 변경하기.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nTTL의 정밀도에 관한 중요한 주의 사항이 있습니다. 아래 중요한 주의 사항 섹션을 참조해주세요.\n\n# 기본 사용법\n\n기본 패턴은 특정 이벤트를 기반으로 DynamoDB 레코드를 생성/업데이트하고, 그 후 미래의 특정 시간에 어떤 일이 발생하길 원할 때입니다. 레코드에 TTL(생존 기간)을 설정하여 그 시점에 만료되도록 하고, 만료되면 DynamoDB가 레코드를 삭제합니다. 그런 다음 DynamoDB 테이블에 스트림을 활성화하고, 해당 삭제 이벤트를 처리할 람다를 스트림에 연결합니다. 람다에는 이 상황에 대한 특정 이벤트만 수신하도록 필터가 있어야 합니다(REMOVE 및 TTL 삭제 vs. 코드 삭제를 구분하는 필드 및 주 키에 대한 필터 설정이 있어야 합니다).\n\n![image](/assets/img/2024-06-19-UsingDynamoDBStreamstoHandleFutureScheduledDelayedCronJobs_0.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 경우 DynamoDB의 추가적인 이점은 이러한 레코드에 대해 PUT을 수행함으로써 이것이 \"업서트\"로 작동한다는 것입니다. 즉, 이를 유지하려면 이미 존재하는 레코드인지 확인하고 TTL을 업데이트하거나 새로운 레코드를 만들 필요가 없습니다 (예를 들어 대부분의 RDBMS에서 해야 하는 작업과 같은).\n\n# 예시\n\n위의 경우 #2를 기준으로 예시를 살펴보겠습니다. 이 경우가 더 복잡하여 이 기술의 장점을 잘 보여줍니다. 장치 읽기를 수신하는 시스템을 상상해보세요. 보통 하루에 한 번씩 읽기가 발생하며, 읽기가 일주일 동안 없는 경우 상태를 업데이트하고 경고를 생성하고자 합니다. 이러한 장치들은 높은 빈도로 읽기가 발생하지 않습니다(이 경우 농업 관수 시스템이거나, 원격 환자 모니터링 장치일 수 있습니다. 즉, 환자가 집에서 매일 혈압을 측정하겠다는 것입니다). 다음을 수행해야 합니다:\n\n- 읽기가 없는 경우 3일 후에 장치 상태를 \"연결 불가\"로 설정\n- 장치 관리자에게 7일 후에 경고를 발생시키고, 10일 후에도 여전히 오프라인인 경우에 다시 경고를 보냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nPUT 메소드를 사용하여 시간 프레임(3, 7, 10일)마다 DynamoDB(DDB) 레코드를 세 개 PUT하면 이를 달성할 수 있습니다. 첫 번째 레코드는 도달할 수 없는 상태를 트리거하기 위한 것이고, 나머지 두 개는 경고를 위한 것입니다. 기기에서 읽기 값을 받을 때마다 이 작업을 수행하는데, 이렇게 함으로써 TTL을 연장합니다 (PUT은 같은 기본 키의 기존 레코드를 덮어씁니다). 그러나 갑자기 몇 일 동안 읽기 값을 받지 못하게 되었을 때, 즉 레코드가 업데이트되지 않았을 때는 우선 3일 항목의 TTL이 만료되어 DynamoDB 스트림으로 REMOVE 이벤트를 보내고, 람다로 전달하여 프로세스를 시작합니다.\n\n그래서 우리는 파티션 키(PK)와 소트 키(SK) 설계가 모두 있는 DDB 기본 키를 사용할 수 있는데, 이는 다음과 같습니다:\n\nPK: DEVICE#`소유자 ID`\n\nSK: `기기 ID`#`이벤트 ID`\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nTTL: 유닉스 시대 시간, 초 단위 해상도입니다.\n\n당신의 요구에 따라 추가적인 속성을 갖고 있을 수 있으며, 일반적으로 PK 및 SK의 값에 대한 명시적 속성도 가지고 있습니다. 그렇게 함으로써 PK/SK에서 그 값을 분리해 내지 않아도 됩니다. 예를 들어, OwnerID, DeviceID, EventID와 종종 데브리 책에서 설명한 것처럼 유용한 유형 필드가 있을 것입니다.\n\n게다가, 장치가 더 이상 사용되지 않는 경우, 단순히 PK = DEVICE#`ownerid` 및 디바이스 ID의 begins_with를 사용하는 SK에 대한 DDB 레코드를 모두 삭제하여 이를 처리할 수 있습니다.\n\nDynamoDB 테이블에서 스트림을 활성화하고, 스트림에 람다를 연결하여 이벤트를 처리해야 합니다. AWS 문서 \"DynamoDB Streams\"와 \"AWS Lambda Trigger\"를 참조하세요. 기본적으로 람다는 DynamoDB \"이벤트\"를 모두 받아오게 되므로 삽입, 업데이트 및 삭제를 다루게 됩니다. 여기서 필터링이 필요합니다. 여기서는 최소한 REMOVE(삭제) 이벤트만 필터링하고자 합니다. 필터링은 람다에서 수행할 수 있지만, 이는 소요 비용이 큽니다. 왜냐하면 매번 레코드를 유지하기 위해 이 디자인으로 정기적으로 수행하고 있는 삽입/업데이트가 발생할 때마다 람다가 호출되기 때문입니다. 다행히도 \"Lambda Event Filtering\"을 통해 이벤트와 일치하지 않으면 람다가 호출되지 않도록 할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 이벤트 전용 DynamoDB 테이블을 따로 사용한다면, 삭제 이벤트에 대한 필터만 필요합니다. 그러나 테이블 내에 다른 아이템이 있는 경우 (예: 단일 테이블 디자인을 사용하는 경우) 특정 레코드로 제한하기 위해 필터를 추가해야 합니다. 이 경우 PK가 'DEVICE#' 접두사를 가진 레코드만 일치하도록 \"접두사\" 필터를 사용하여 이 작업을 수행할 수 있습니다. AWS의 Lambda 이벤트 필터링을 다룬 이 튜토리얼을 참고하시기 바랍니다. 마지막으로, 반드시 해야 할 중요한 추가 필터링 사항이 있습니다. userIdentity 필드를 확인해야 합니다. DynamoDB Streams 및 Time To Live 문서에서 이에 대해 설명되어 있으며, 필터 구문을 보여줍니다. 예를 들어, Serverless Framework를 사용하는 경우, DynamoDB 스트림을 처리하는 람다는 다음과 같은 이벤트 정의를 갖게 됩니다:\n\n```js\n    events:\n      - stream:\n          type: dynamodb\n          batchSize: 20\n          enabled: true\n          arn:\n            Fn::GetAtt: [DeviceMonitorTable, StreamArn]\n          filterPatterns:\n            - eventName: [REMOVE]\n              dynamodb:\n                Keys:\n                  PK:\n                    S:\n                      - prefix: 'DEVICE#'\n              userIdentity:\n                type:\n                  - Service\n                principalId:\n                  - dynamodb.amazonaws.com\n```\n\n이 필터를 통해 람다가 처리해야 할 이벤트만 받게 됩니다. 그 후, 람다는 받은 레코드에 적절한 처리를 수행합니다 (아마도 이벤트 ID나 레코드 내의 다른 관련 데이터에 기반하여). 그 후에 장치가 계속해서 측정치를 갖지 않는 상태로 유지되거나 시리즈의 다음 DDB 레코드가 TTL에 도달하거나 등의 상황이 발생할 수 있습니다. 장치가 다시 사용되고 모든 레코드가 업데이트되거나 (첫 번째 레코드부터 시작하여 TTL에 도달한 레코드의 수에 따라) 다시 생성될 수 있습니다.\n\n# 중요한 한 가지 주의사항!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 솔루션에서 주의해야 할 중요한 점은 TTL이 정확하지 않고 만료 후 \"만료일로부터 몇 일 이내에\" 발생한다는 것이다. 이 문서에 따르면 정확한 타이밍을 기대하는 중요한 작업에는 사용하지 않는 것이 좋다. 그런데, 실제로, 두 가지 서로 다른 앱에서 확인했을 때, 이 작업은 대개 TTL만료 후 몇 분 안에 트리거된다. 테이블의 사용량에 따라 (즉, 정기 사용은 만료된 레코드의 정기적인 정리를 의미한다) 결정되는 것 같다. 이에 대해 더 나은 정보가 있다면, 의견을 달아주시거나 말씀해주세요!\n\n# 마지막으로\n\n이러한 지연 작업 시스템을 구축하는 것은 크론 스타일 접근 방식으로는 정말 고통스럽습니다. 하루에 한 번 또는 모든 시간 간격을 확실히 포함할 때의 빈도로 크론 작업을 수행해야 하기 때문입니다. 위의 예와 같은 \"일\" 단위 간격의 경우에는 그렇게 나쁘지 않을 수도 있습니다. 그러나 더 정확한 타이밍이 필요한 경우, 그냥 작동하지 않을 수도 있습니다. 게다가, 이러한 이벤트가 발생하는 빈도가 더 드문 경우, 필요 이상으로 크론 작업을 실행할 수도 있습니다. AWS 생태계에 속해 있다면, 저장 실행 작업에 대신 제3자나 패키지를 가져올 경우에 비해 사용하기가 매우 매력적으로 보입니다.\n\n게다가, 타이밍 간격이 구성 가능하면, 이것은 크론 스타일 시스템보다 처리하기가 훨씬 쉽습니다. 여러분의 수용할 수 있는 한계와 지정된 동작에 따라서, 기존 항목을 그대로 둘 수 있고 다음 장치 읽기(또는 DDB 레코드 쓰기를 트리거하는 것)에서 TTL을 간단히 업데이트하거나, 영향을 받는 레코드만 조정할 수도 있습니다(PK+SK 콤보를 사용하여 실제 시간 양에 의존하지 않는 것을 확인하세요. 따라서 저는 SK에서 이 측면을 \"이벤트 ID\"로 지정했습니다. \"3일\" 또는 다른 것이 변할 수 있는 \"3일\" 같은 것 대신 \"alert1\"을 사용할 것입니다).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 방식은 이벤트 주도 스타일을 제공하며 놀라운 확장성을 제공하고 물론 서버리스 시스템과 잘 어울립니다.\n","ogImage":{"url":"/assets/img/2024-06-19-UsingDynamoDBStreamstoHandleFutureScheduledDelayedCronJobs_0.png"},"coverImage":"/assets/img/2024-06-19-UsingDynamoDBStreamstoHandleFutureScheduledDelayedCronJobs_0.png","tag":["Tech"],"readingTime":8},{"title":"AWS에서 Terraform을 사용하여 Lambda 함수를 배포하는 방법","description":"","date":"2024-06-19 12:12","slug":"2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform","content":"\n서버리스 함수는 인프라 걱정 할 필요 없이 DevOps 및 SysOps에게 필수적입니다.\n\n우리는 Amazon Web Services (AWS) 람다 함수를 살펴보고, 어떻게 테라폼을 사용하여 AWS 람다를 배포할 수 있는지 알아볼 것입니다.\n\n![이미지](/assets/img/2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform_0.png)\n\n모든 자료에 대한 Github 링크: [https://github.com/batuhan-bulut/terraform-aws-lambda](https://github.com/batuhan-bulut/terraform-aws-lambda)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# AWS Lambda란 무엇인가요?\n\nAWS Lambda를 사용하면 서버에 대해 걱정하지 않고 지원되는 언어로 스크립트를 실행할 수 있습니다. Node.JS, Python, C# 등으로 작성된 코드를 실행할 수 있습니다.\n\nLambda를 사용하면 다음을 수행할 수 있습니다.\n\n- 지역별 EC2 인스턴스 상태 확인\n- AWS CLI를 사용하여 일부 자동화 실행\n- AWS SQS로 작업 예약\n- 기타\n\n이런 가능성들로 AWS Lambda는 데브옵스 및 시스옵스 팀에 매우 중요한 역할을 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAWS 웹 사이트에서 GUI를 사용하여 AWS Lambda를 쉽게 배포할 수 있어요.\n\n여러 계정이 있고 동일한 Lambda를 다른 지역에 배포해야 한다면 어떻게 할 건가요? 하나씩 Lambda를 배포할까요?\n\n이때 IaC와 Terraform이 게임에 합류해요.\n\n# IaC란? (Infrastructure as Code)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nIaC를 사용하면 우리는 쉽게 환경 (개발, 테스트, 스테이징, 프로덕션 등)을 설정할 수 있어요. 인기 있는 공급 업체들은 요구 사항에 맞는 CDK(Cloud Development Kit)를 가지고 있어요.\n\n## 왜 테라폼이 중요한가요?\n\n예를 들어, AWS에 앱이 있고 (EC2, RDS, Lambda, SQS 등을 사용하는) 다양한 서비스를 사용한다고 가정해봅시다. AWS CDK로 이 애플리케이션을 쉽게 배포할 수 있어요.\n\n그런데 만약 경영진이 Azure, GCP 또는 다른 클라우드 공급업체로 전환하기로 결정한다면 어떻게 될까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAWS CDK에 대한 지식 대부분은 중요하지 않아요. 왜냐하면 해당 공급업체가 자체 CDK를 가지고 있거든요.\n\n여기서 Terraform이 우리의 워크플로에 합류하는 곳이에요.\n\n# Terraform이란?\n\n지식과 경험을 통해 Terraform을 사용하면 간단한 명령어로 많은 리소스를 관리할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 문서에서는 Terraform의 AWS 쪽에 중점을 둘 것입니다.\n\n# 불이 켜지면, 카메라 맞춰, 액션!\n\n주의하세요: 이 작업은 AWS 측에 비용을 발생시킬 수 있습니다.\n\n## 우리의 Terraform 코드는 무엇을 할까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- IAM 역할 제공\n- IAM 프로필 제공\n- 프로필에 IAM 정책 부착\n- 업로드용 ZIP 파일 생성\n- 람다 함수 생성\n\n이 문서에서는 하드코딩된 값 대신 변수를 사용할 것입니다. 이렇게 하면 다양한 설정으로 동일한 스크립트를 실행할 수 있습니다.\n\n이것은 우리의 terraform/main.tf 파일입니다. 우리 코드의 구조를 담고 있습니다.\n\n```js\nprovider \"aws\" {\n  region = var.region\n}\n\n# AWS Lambda를 위한 IAM 역할\nresource \"aws_iam_role\" \"terraform_lambda_iam_role\" {\nname               = var.terraform_lambda_iam_role.name\nassume_role_policy = var.terraform_lambda_iam_role.assume_role_policy\n}\n\n# 람다를 위한 새로운 정책 생성\nresource \"aws_iam_policy\" \"iam_policy_for_lambda\" {\nname         = \"iam_policy_${var.terraform_lambda_iam_role.name}\"\ndescription  = var.iam_policy_for_lambda.description\npolicy       = var.iam_policy_for_lambda.policy\n}\n\n# IAM 정책을 IAM 역할에 부착\nresource \"aws_iam_role_policy_attachment\" \"attach_iam_policy_to_iam_role\" {\nrole        = aws_iam_role.terraform_lambda_iam_role.name\npolicy_arn  = aws_iam_policy.iam_policy_for_lambda.arn\n}\n\n# Lambda에 업로드할 ZIP 파일 생성\ndata \"archive_file\" \"zip_python_lambda_code\" {\ntype        = \"zip\"\noutput_path = \"${path.module}/python/${var.zip_python_lambda_code.name}.zip\"\nsource_file = \"${path.module}/../${var.zip_python_lambda_code.name}.py\"\n}\n\n# Lambda 함수 생성\nresource \"aws_lambda_function\" \"terraform_lambda\" {\nfilename     = \"${path.module}/python/${var.zip_python_lambda_code.name}.zip\"\nfunction_name  = var.terraform_lambda.name\nrole        = aws_iam_role.terraform_lambda_iam_role.arn\nruntime     = var.terraform_lambda.runtime\nhandler     = \"${var.zip_python_lambda_code.name}.lambda_handler\"\n}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 코드 블록에서는 terraform/terraform.tf 파일에서 모든 변수를 읽습니다. 동일한 Terraform 파일을 프로덕션, 스테이징 또는 다른 AWS 지역과 같이 다른 환경에 대해 다양한 구성으로 실행할 수 있습니다.\n\n이 변수들을 읽기 위해서는 변수를 정의해야 합니다. “.tf” 파일을 사용하여 변수를 선언할 수 있습니다.\n\n위 코드에 대한 terraform.tf 파일이 다음과 같이 보입니다.\n\n```js\n변수 \"region\" {\n  type = string\n  설명 = \"AWS 지역\"\n  기본 = \"eu-central-1\"\n}\n\n변수 \"zip_python_lambda_code\" {\n  type = map(string)\n  설명 = \"Python 파일의 이름\"\n  기본 = {\n    name = \"index\"\n  }\n}\n\n변수 \"terraform_lambda_iam_role\" {\n  type = map(string)\n  설명 = \"aws_iam_role - terraform_lambda_iam_role 변수\"\n  기본 = {\n    name = \"Lambda-from-terraform\"\n    assume_role_policy = \u003c\u003cEOF\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n   {\n     \"Action\": \"sts:AssumeRole\",\n     \"Principal\": {\n       \"Service\": \"lambda.amazonaws.com\"\n     },\n     \"Effect\": \"Allow\",\n     \"Sid\": \"\"\n   }\n ]\n}\nEOF\n  }\n}\n\n변수 \"iam_policy_for_lambda\" {\n  type = map(string)\n  설명 = \"람다를 위한 IAM 정책\"\n  기본 = {\n    description = \"Terraform으로 생성된 IAM 정책\"\n    policy = \u003c\u003cEOF\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n   {\n     \"Action\": [\n       \"logs:CreateLogGroup\",\n       \"logs:CreateLogStream\",\n       \"logs:PutLogEvents\"\n     ],\n     \"Resource\": \"arn:aws:logs:*:*:*\",\n     \"Effect\": \"Allow\"\n   }\n ]\n}\nEOF\n  }\n}\n\n변수 \"terraform_lambda\" {\n  type = map(string)\n  설명 = \"람다를 위한 변수\"\n  기본 = {\n    name = \"Lambda_Terraform\"\n    runtime = \"python3.12\"\n    handler = \"index.lambda_handler\"\n  }\n}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 파일에서는 변수와 그 기본 값들을 선언합니다. 테라폼에서는 다양한 유형의 변수를 선언할 수 있어요.\n\n더 많은 정보를 원하신다면 테라폼 문서를 확인해보세요.\n\n코드에 기본 값을 사용하고 싶지 않다면 기본 변수를 덮어쓸 terraform/vars.tfvars 파일을 추가할 수도 있어요. 이 파일의 내용은 일반적으로 \"key = value\" 형식입니다.\n\n여기 .tfvars 파일의 예시가 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nregion = \"us-east-1\"\n\nterraform_lambda= {\n    name = \"Override_Name\"\n    runtime = \"python3.9\"\n    handler = \"override.lambda_handler\"\n}\n```\n\n이렇게 하면 Terraform이 region 및 terraform_lambda 변수의 기본값을 재정의할 것입니다.\n\n참고: 이 예와 같이 terraform_lambda와 같은 객체에서 변수를 변경하려면 객체의 모든 변수를 전달해야 합니다. 그렇지 않으면 오류가 발생합니다.\n\n그리고 루트 폴더에 간단한 Python 앱을 index.py라는 이름으로 추가해보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n# 간단한 Hello 함수\nimport json\n\ndef lambda_handler(event, context):\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Lambda에서 안녕하세요!')\n    }\n```\n\n다음은 폴더가 보이는 예시입니다\n\n\u003cimg src=\"/assets/img/2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform_1.png\" /\u003e\n\n# 쇼타임!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n코드를 실행할 준비가 되었습니다.\n\n먼저 테라폼 구성 파일이 포함된 작업 디렉터리를 초기화하기 위해 terraform init 명령을 실행해야 합니다.\n\n```js\n$terraform init\n\n백엔드 초기화 중...\n\n공급자 플러그인 초기화 중...\n- 이전 의존성 락 파일에서 hashicorp/aws의 이전 버전 재사용 중\n- 이전 의존성 락 파일에서 hashicorp/archive의 이전 버전 재사용 중\n- 이전에 설치한 hashicorp/aws v5.54.1 사용 중\n- 이전에 설치한 hashicorp/archive v2.4.2 사용 중\n\n테라폼이 성공적으로 초기화되었습니다!\n\n이제 테라폼을 사용할 수 있습니다. 인프라에 필요한 변경 사항을 볼려면 \"terraform plan\"을 실행해보세요. 이제 모든 테라폼 명령이 작동해야 합니다.\n\n테라폼의 모듈 또는 백엔드 구성을 설정하거나 변경한 경우, 작업 디렉터리를 다시 초기화하려면이 명령을 다시 실행하십시오. 잊어버릴 경우 다른 명령어가 감지하여 필요시 상기시켜줄 것입니다.\n```\n\n이후, 인프라 구조의 변경 사항을 확인하기 위해 terraform plan을 실행할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndata.archive_file.zip_python_lambda_code: 읽는 중...\ndata.archive_file.zip_python_lambda_code: 읽기 완료 소요 시간 0초 [id=111]\n\n테라폼은 선택한 공급업체를 사용하여 다음 실행 계획을 생성했습니다. 자원 작업은 다음 기호로 표시됩니다:\n  + 생성\n\n테라폼은 다음 작업을 수행할 것입니다:\n\n  # aws_iam_policy.iam_policy_for_lambda가 생성됩니다\n  + resource \"aws_iam_policy\" \"iam_policy_for_lambda\" {\n      + arn              = (적용 후 알려짐)\n      + attachment_count = (적용 후 알려짐)\n      + description      = \"테라폼에 의해 생성된 IAM 정책\"\n      + id               = (적용 후 알려짐)\n      + name             = \"iam_policy_Lambda-from-terraform\"\n      + name_prefix      = (적용 후 알려짐)\n      + path             = \"/\"\n      + policy           = jsonencode(\n            {\n              + Statement = [\n                  + {\n                      + Action   = [\n                          + \"logs:CreateLogGroup\",\n                          + \"logs:CreateLogStream\",\n                          + \"logs:PutLogEvents\",\n                        ]\n                      + Effect   = \"Allow\"\n                      + Resource = \"arn:aws:logs:*:*:*\"\n                    },\n                ]\n              + Version   = \"2012-10-17\"\n            }\n        )\n      + policy_id        = (적용 후 알려짐)\n      + tags_all         = (적용 후 알려짐)\n    }\n\n  # aws_iam_role.terraform_lambda_iam_role가 생성됩니다\n  + resource \"aws_iam_role\" \"terraform_lambda_iam_role\" {\n      + arn                   = (적용 후 알려짐)\n      + assume_role_policy    = jsonencode(\n            {\n              + Statement = [\n                  + {\n                      + Action    = \"sts:AssumeRole\"\n                      + Effect    = \"Allow\"\n                      + Principal = {\n                          + Service = \"lambda.amazonaws.com\"\n                        }\n                      + Sid       = \"\"\n                    },\n                ]\n              + Version   = \"2012-10-17\"\n            }\n        )\n      + create_date           = (적용 후 알려짐)\n      + force_detach_policies = false\n      + id                    = (적용 후 알려짐)\n      + managed_policy_arns   = (적용 후 알려짐)\n      + max_session_duration  = 3600\n      + name                  = \"Lambda-from-terraform\"\n      + name_prefix           = (적용 후 알려짐)\n      + path                  = \"/\"\n      + tags_all              = (적용 후 알려짐)\n      + unique_id             = (적용 후 알려짐)\n    }\n\n  # aws_iam_role_policy_attachment.attach_iam_policy_to_iam_role 생성됩니다\n  + resource \"aws_iam_role_policy_attachment\" \"attach_iam_policy_to_iam_role\" {\n      + id         = (적용 후 알려짐)\n      + policy_arn = (적용 후 알려짐)\n      + role       = \"Lambda-from-terraform\"\n    }\n\n  # aws_lambda_function.terraform_lambda 생성됩니다\n  + resource \"aws_lambda_function\" \"terraform_lambda\" {\n      + architectures                  = (적용 후 알려짐)\n      + arn                            = (적용 후 알려짐)\n      + code_sha256                    = (적용 후 알려짐)\n      + filename                       = \"./python/index.zip\"\n      + function_name                  = \"Override_Name\"\n      + handler                        = \"index.lambda_handler\"\n      + id                             = (적용 후 알려짐)\n      + invoke_arn                     = (적용 후 알려짐)\n      + last_modified                  = (적용 후 알려짐)\n      + memory_size                    = 128\n      + package_type                   = \"Zip\"\n      + publish                        = false\n      + qualified_arn                  = (적용 후 알려짐)\n      + qualified_invoke_arn           = (적용 후 알려짐)\n      + reserved_concurrent_executions = -1\n      + role                           = (적용 후 알려짐)\n      + runtime                        = \"python3.9\"\n      + signing_job_arn                = (적용 후 알려짐)\n      + signing_profile_version_arn    = (적용 후 알려짐)\n      + skip_destroy                   = false\n      + source_code_hash               = (적용 후 알려짐)\n      + source_code_size               = (적용 후 알려짐)\n      + tags_all                       = (적용 후 알려짐)\n      + timeout                        = 3\n      + version                        = (적용 후 알려짐)\n    }\n\n계획: 4개 추가, 0개 변경, 0개 제거.\n\n앞서 설명한 변경 사항을 적용시키기 위해 terraform apply를 실행할 수 있습니다.\n\n그 후 콘솔에 성공 메시지가 표시될 것입니다. 이는 모든 리소스가 AWS 측에 생성된 것을 의미합니다. AWS GUI에서 람다 함수를 확인하고 실행하거나 CLI에서 람다 함수를 확인할 수 있습니다.\n\naws lambda list-functions --region us-east-1 | grep Override_Name\n\n\"FunctionName\": \"Override_Name\",\n\"FunctionArn\": \"arn:aws:lambda:us-east-1:11111:function:Override_Name\",\n\"LogGroup\": \"/aws/lambda/Override_Name\"\n```\n","ogImage":{"url":"/assets/img/2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform_0.png"},"coverImage":"/assets/img/2024-06-19-HowtodeployLambdafunctiononAWSusingTerraform_0.png","tag":["Tech"],"readingTime":14}],"page":"34","totalPageCount":110,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":1},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"34"},"buildId":"YUMR4jSyk_WlOHHc7UfOk","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>