<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/99" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/99" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-8ce515d2b46d0f43.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_buildManifest.js" defer=""></script><script src="/_next/static/JlBEgQDLGRx6DYlBnT8eD/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="인간 중심 AI 핀 부적절한 전략과 실행 사례 연구" href="/post/2024-05-18-TheHumaneAIPinACaseStudyinPoorStrategyandPoorExecution"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간 중심 AI 핀 부적절한 전략과 실행 사례 연구" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-TheHumaneAIPinACaseStudyinPoorStrategyandPoorExecution_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간 중심 AI 핀 부적절한 전략과 실행 사례 연구" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">인간 중심 AI 핀 부적절한 전략과 실행 사례 연구</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT-4의 비밀 슈퍼파워 YouTube 데모에서 보이지 않은 것들 " href="/post/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT-4의 비밀 슈퍼파워 YouTube 데모에서 보이지 않은 것들 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT-4의 비밀 슈퍼파워 YouTube 데모에서 보이지 않은 것들 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">ChatGPT-4의 비밀 슈퍼파워 YouTube 데모에서 보이지 않은 것들 </strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="BiTCN 컨볼루션 네트워크를 활용한 다변수 시계열 예측" href="/post/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BiTCN 컨볼루션 네트워크를 활용한 다변수 시계열 예측" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BiTCN 컨볼루션 네트워크를 활용한 다변수 시계열 예측" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">BiTCN 컨볼루션 네트워크를 활용한 다변수 시계열 예측</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="시간을 통해 전파하는 역전파  RNN이 학습하는 방법" href="/post/2024-05-18-BackpropagationThroughTimeHowRNNsLearn"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="시간을 통해 전파하는 역전파  RNN이 학습하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="시간을 통해 전파하는 역전파  RNN이 학습하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">시간을 통해 전파하는 역전파  RNN이 학습하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LSTM 시계열 예측에서 흔히 발생하는 오류를 수정하는 방법" href="/post/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LSTM 시계열 예측에서 흔히 발생하는 오류를 수정하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LSTM 시계열 예측에서 흔히 발생하는 오류를 수정하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">LSTM 시계열 예측에서 흔히 발생하는 오류를 수정하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2" href="/post/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">39<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="고객 이탈 예측" href="/post/2024-05-18-CUSTOMERCHURNPREDICTION"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="고객 이탈 예측" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="고객 이탈 예측" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">고객 이탈 예측</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="휴머노이드는 여기에 머물러 있을까요" href="/post/2024-05-18-AretheHumanoidsHeretoStay"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="휴머노이드는 여기에 머물러 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-AretheHumanoidsHeretoStay_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="휴머노이드는 여기에 머물러 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">휴머노이드는 여기에 머물러 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">10<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="필수 인공지능" href="/post/2024-05-18-EssentialAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="필수 인공지능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-EssentialAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="필수 인공지능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">필수 인공지능</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="SLAM을 처음부터 구현해 보기" href="/post/2024-05-18-ImplementSLAMfromscratch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="SLAM을 처음부터 구현해 보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-ImplementSLAMfromscratch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="SLAM을 처음부터 구현해 보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">SLAM을 처음부터 구현해 보기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link posts_-active__YVJEi" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"인간 중심 AI 핀 부적절한 전략과 실행 사례 연구","description":"","date":"2024-05-18 19:45","slug":"2024-05-18-TheHumaneAIPinACaseStudyinPoorStrategyandPoorExecution","content":"\n휴메인 인공 지능 핀은 디지털 어시스턴트가 탑재된 착용형 음성 제어 장치로, 일반적인 사용 사례에서 스마트폰을 대체하기 위해 만들어졌으며 가격은 700달러입니다. 그러나 최근에 나온 기술 제품 중에서 가장 부정적인 평가를 받은 제품 중 하나입니다.\n\n이 게시물은 Marques Brownlee의 리뷰에서 영감을 받았습니다. 그분은 제품이 해야 할 일(즉, 제품의 전략)과 제품을 사용하는 실제 경험(즉, 제품 팀의 실행)으로 리뷰를 구분하여 작성했습니다.\n\n# 휴메인 인공 지능 핀 제품 전략\n\n전략은 목표를 달성하기 위해 독특한 단계를 취하여 내구성 있는 경쟁 우위를 제공하는 방식입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n좋은 전략의 조건은 경쟁사가 귀하의 제품을 복제하거나 대체하기 어려운 점이 있어야 하며, 이는 귀사만이 이를 할 수 있는 특별한 능력을 갖고 있거나 성공적인 실행으로 구조적 이점을 창출하여 그것을 그냥 복제할 수 없게 만드는 것입니다.\n\n핵심 테제 제품의 목적은 사람들이 스마트한 개인 비서를 가지고 질문을 할 수 있어 편하게 사용할 수 있는 것이 유용할 것이라는 것입니다. 왜냐하면 핸드폰을 꺼내기는 종종 불편하기 때문입니다. 사실, 이것은 애플의 Siri의 핵심 가치 제안이었으며, 이는 2011년부터 광고를 진행해 왔으며, 오늘날에는 Airpods Pro와 함께 Siri를 사용할 때 가능합니다. 따라서 Humane AI Pin의 전체 가치 제안은 Siri가 별로라는 것입니다.\n\n제품 기회는 애플이 Siri에 대해 성공적으로 실행하는 경우 사라집니다. 리스크가 있는 베팅이지만, 그것이 불가능한 것은 아닙니다. Zoom은 사용하기 쉽고 사용이 검증된 비디오 회의 소프트웨어에 대해 마이크로소프트나 구글이 잘 실행하지 않을 것을 베팅함으로써 거대한 기업이 되었고 옳았습니다.\n\n전략을 평가하는 방법은 가정이 사실이고 완벽하게 실행된다고 가정했을 때 무엇이 발생하는지 고려하는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMarkdown 형식으로 표를 수정하십시오.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 인간 친화적 AI 핀 제품 실행\n\n제품 실행은 측정 가능하고 집중된 것입니다. 이는 당신의 전략을 달성하는 것을 가능케 하는 단기적인 조치들입니다. 실행은 분기별 OKR 또는 매 반기마다의 목표와 같은 활동 유형입니다.\n\n완벽한 전략이라도 실행이 미흡하다면 중요하지 않습니다. 인간 친화적 AI 핀에서는 많은 미완벽한 실행이 있습니다.\n\n- “태양 빛을 받을 때 프로젝터는 기본적으로 읽을 수 없습니다.” — 워싱턴 포스트\n- “핀은 연속적인 요청을할 때 또는 프로젝터를 너무 오래 사용할 때 매우 빨리 과열되기 시작합니다. 그런 경우에 핀이 갑자기 연락을 끊고 식을 때는 놀랍지 마세요. 이 일은 2주 동안 4~5회 발생했습니다” — 워싱턴 포스트\n- “아, 카메라도요? 최선의 경우, 결과는 만족스러울 수 있지만, 어두운 곳에 있을 경우, 많은 노이즈와 흐릿한 얼굴을 기대하십시오.” — 워싱턴 포스트\n- “하지만, 아 차라리, AI 핀은 알람이나 타이머를 설정할 수 없습니다. 또한 캘린더에 항목을 추가하거나 이미 있는 항목을 알려줄 수도 없습니다. 노트와 목록을 만들 수는 있지만 (이는 당신이 장치를 연결하고 연락처를 관리하고 업로드한 사진을 확인할 수 있는 인류 센터 웹 앱에 나타납니다), 나중에 목록에 항목을 추가하려고 하면 거의 항상 어떤 이유로 실패 할 것입니다.” — 더 버지\n- “그리고 모든 것이 방해가 됩니다. 내 배낭 끈이 그것에 닿았고, 메신저 백이 그것에 걸려갑니다. 내 아들과 강아지가 날 덮치면서 실수로 AI 핀을 다시 시작했습니다.” — 더 버지\n- “AI 핀이 무언가를 시도할 때마다 Humane의 서버를 통해 쿼리를 처리해야 하며, 최상의 경우에는 상대적으로 느리고 최악의 경우에는 완전히 실패합니다. AI 핀에게 도서 매매가 다음 주에 있다고 말하면: 편리합니다! 10초를 기다리면서 처리하고 처리하고 일반적인 \"추가할 수 없음\" 오류 메시지를 표시하는 것은 덜 편리합니다. 누군가에게 전화를 걸려고 할 때 절반의 경우 전화가 걸리지 않을 것입니다. 누군가가 나를 호출했을 때 절반이상의 시간, AI 핀은 전화를 울리지도 않고 거의 바로 음성 사서함으로 연결했습니다. 여러 일 테스트를 거쳐, AI 핀이 할 수 있는 단 하나의 일은 시간을 알려주는 것뿐입니다.” — 더 버지\n- “AI 핀의 언어 모델과 기능에 대한 문제는 여기서 끝나지 않습니다. 때때로 다시 시작하거나 종료하는 것과 같이 요청한 내용을 거부할 때가 있습니다. 다른 경우에는 완전히 예상치 못한 일을 할 수도 있습니다. \"Julian Chokkattu에게 텍스트 메시지 보내기\"라고 말했을 때, 그는 Wired의 친구이자 AI 핀 리뷰어인데, 무엇을 어떻게 말하고 싶은지 물어볼 줄 알았습니다. 대신, 장치는 단순히 OK라고 말하고, \"안녕 Julian, 오늘 하루 어떻게 지내고 있니?\" 라며 Chokkattu에게 보낸다고 말했습니다. 우리가 친구사이인 몇 년 동안 저는 그에게 그런 말을 한 적이 없습니다. 그럼에도 불구하고 기술적으로 AI 핀이 내가 요청한 대로 행동 한 것이라고 할 수 있겠죠.” — 엔가젯\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n좋은 실행은 잘 설계된 기능을 제공하거나 적어도 사람들이 버그를 참을 가치가 있는 기능을 제공하는 것입니다. 인간적인 AI 핀은 첫 번째로 많은 것을 하지도 않는 제품인데도 불구하고 많은 버그 경험과 설계가 미흡한 기능들을 가지고 있습니다.\n\n마지막으로, 전략과 실행이 제품 결과에 어떻게 영향을 미치는지에 대한 훌륭한 예시를 제공해 준 인간적인 팀에 감사의 말씀을 전하고, 그들의 제품의 다음 버전에서 행운을 빕니다.\n","ogImage":{"url":"/assets/img/2024-05-18-TheHumaneAIPinACaseStudyinPoorStrategyandPoorExecution_0.png"},"coverImage":"/assets/img/2024-05-18-TheHumaneAIPinACaseStudyinPoorStrategyandPoorExecution_0.png","tag":["Tech"],"readingTime":4},{"title":"ChatGPT-4의 비밀 슈퍼파워 YouTube 데모에서 보이지 않은 것들 ","description":"","date":"2024-05-18 19:43","slug":"2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou","content":"\n테크 세계는 오픈AI의 AI 기술의 최신 발전인 ChatGPT-4o의 출시 이후에 흥분으로 가득 찼습니다. 이전에 본 적이 없는 ChatGPT-4o는 \"토마토\"를 의미하는 \"o\"와 함께 천문적인 자연스러운 인간-컴퓨터 상호작용 분야의 혁명적인 한 걸음을 나아갑니다. YouTube 발표에서 많은 기능을 강조했지만, 미술되지 않은 놀라운 기능이 더 많이 있습니다. ChatGPT-4o가 게임 체인저인 이유와 그의 혁신적인 숨겨진 기능을 깊게 들여다보겠습니다.\n\n![ChatGPT-4o의 비밀 능력: YouTube 데모에 나오지 않은 것들](/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_0.png)\n\n## 진정한 멀티모달 놀라움\n\nChatGPT-4o는 텍스트, 오디오, 이미지 및 비디오의 모든 조합을 입력으로 받아들이고, 텍스트, 오디오 및 이미지 형식으로 출력을 생성할 수 있도록 설계되었습니다. 이 유연성은 인간과 기계 간 보다 직관적이고 원활한 상호작용을 위한 새로운 지평을 열어줍니다. 놀랍게도, 모델의 오디오 입력에 대한 응답 시간이 인간의 대화 속도를 흉내내는 232밀리초로 매우 빠릅니다. 또한, 영어 텍스트 및 코드에서 GPT-4 Turbo의 성능과 맞먹아, 비영어 언어의 텍스트 처리를 현저히 향상시킵니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 다중 모달 기능은 사용자들이 말로 된 지시와 시각적 단서의 조합을 제공할 수 있도록 해줍니다. 이를 통해 더 풍부하고 세밀한 의사 소통이 가능해지며, 여러 가지 입력을 동시에 처리하고 응답할 수 있는 능력은 사용자 경험을 향상시켜 AI와의 상호 작용이 사람과 대화하는 듯한 느낌을 줍니다.\n\n# 향상된 이해를 위한 통합 모델\n\nGPT-3.5와 GPT-4와 같은 기존 모델은 음성 상호 작용을 위해 여러 단계의 과정을 거치면서 지연시간과 맥락 정보의 손실을 초래했습니다. 이에 반해, ChatGPT-4o는 모든 입력과 출력을 위한 단일 엔드-투-엔드 신경망을 사용합니다. 이를 통해 ChatGPT-4o는 톤, 배경 소음, 다수의 화자 등과 같은 뉴안스를 이해하고 더 자연스러운 감정적 표현을 생성할 수 있습니다. 즉, 웃음이나 노래와 같은 표현이 가능해집니다.\n\n이 통합된 방법은 모델이 오랜 대화 동안 문맥을 유지하고 복잡한 대화를 처리하는 능력을 향상시킵니다. 그룹 토론 중에 다른 화자를 구별하거나 대화에서 감정적 함의를 이해하는 것과 같은 작업에서, ChatGPT-4o의 통합된 설계는 대화 능력을 크게 향상시킵니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 다양한 형태에서 우수한 성능\n\nChatGPT-4o의 평가 지표는 인상적입니다:\n\n- 추론력: 0-shot CoT MMLU(일반 지식 질문)에서 88.7%의 점수를 기록하며 5-shot no-CoT MMLU에서 87.2%를 달성하여 우수한 추론 능력을 자랑합니다.\n- 오디오 및 비전: 음성 인식에서 Whisper-v3를 능가하며 다국어 및 시각 인식 평가에서 새로운 기준을 세웁니다.\n\n이러한 지표는 ChatGPT-4o의 다양한 형태를 이해하고 생성하는 고급 기능을 강조합니다. 추론 테스트에서의 성과는 복잡하고 미묘한 쿼리를 처리하는 능력을 보여주며, 교육 및 전문 분야에 강력한 도구로 사용될 수 있음을 나타냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 비디오에서 소개되지 않은 혁신적 기능\n\n유튜브 발표에서 강조되지 않은 ChatGPT-4o의 두드러지는 기능들을 소개합니다:\n\n## 시각적 서술\n\n- 로봇 작가의 차단: ChatGPT-4o는 텍스트 설명을 일관된 이미지 시퀀스로 전환할 수 있습니다. 이를 통해 로봇이 종이를 입력하고 찢어내는 과정을 단계별로 시각화할 수 있습니다. 사건 시퀀스를 이해하고 묘사할 수 있는 능력으로, 이는 이야기와 시각 요소를 결합하는 내용 작성 및 블렌딩에 뛰어난 기능을 제공합니다.\n- 우편집사 샐리: 이 기능은 상세한 텍스트 설명을 해석하고 해당하는 시각적 표현물을 생성할 수 있습니다. 샐리가 우편을 배달하거나 강아지와 상호 작용하며 이동하는 장면 등을 묘사할 수 있으며, 캐릭터 일관성과 상세한 시각적 서술을 보장합니다. 이 능력은 ChatGPT-4o의 교육, 엔터테인먼트, 마케팅과 같은 분야에서 텍스트 입력으로부터 매력적이고 정확한 시각적 이야기를 만드는 것을 강조하며, 이를 통해 유틸리티가 향상됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_1.png\" /\u003e\n\n## 창의적 콘텐츠 생성\n\n- 영화 '탐정'을 위한 포스터 제작: ChatGPT-4o는 텍스트 설명에서 전문적인 품질의 영화 포스터를 만들 수 있습니다. 예를 들어, 두 캐릭터의 얼굴을 신중한 표현과 함께 포함하여 특정 시나리오를 정확하게 나타낼 수 있는 포스터를 생성할 수 있습니다. 이 능력은 ChatGPT-4o의 고급 디자인 기술과 세심한 주의를 보여주며, 그래픽 디자인 및 마케팅에 탁월한 도구로 사용될 수 있습니다.\n- 캐릭터 디자인 — 로봇 Geary: 이 기능은 ChatGPT-4o가 요리, 바이올린 연주 또는 프로그래밍과 같은 다양한 활동 중에서도 일관적으로 캐릭터를 시각화할 수 있게 합니다. 텍스트에서 특정 세부 사항을 포함하여 캐릭터 무결성을 유지하면서, ChatGPT-4o는 애니메이션, 비디오 게임 및 교육 콘텐츠에 중요한 캐릭터 디자인과 스토리텔링 능력을 입증합니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_2.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 예술적 및 타이포그래픽 스킬\n\n- 반복 편집을 통한 시적 타이포그래피: ChatGPT-4o는 시를 시각적으로 매력적인 디자인으로 변환할 수 있으며, 사용자 피드백을 기반으로 출력물을 반복적으로 개선합니다. 예를 들어 시를 명확한 필체로 변환하고 초현실주의적인 도안으로 가득한 것으로 만든 다음, 디자인을 다크 모드로 조정하거나 요청에 따라 색상을 변경할 수 있습니다. 이 능력은 모델의 유연성과 예술적 스킬을 강조하며, 사용자 맞춤형 및 예술적 시각 콘텐츠를 만들기 위한 강력한 도구로 만들어줍니다.\n- 글꼴 디자인: ChatGPT-4o는 상세한 텍스트 설명에서 사용자 정의 글꼴을 만들 수 있습니다. 화려한 빅토리아 양식의 글꼴이나 현대적이고 세련된 디자인을 생성할 수 있어서 다양한 타이포그래픽 스타일과 디테일 수준을 처리할 수 있는 능력을 보여줍니다. 이 능력은 브랜딩, 마케팅 및 크리에이티브 프로젝트에 대한 독특하고 맞춤형 글꼴을 만들고자 하는 디자이너들에게 특히 유용합니다.\n\n![ChatGPT-4o's Secret Superpowers What-the-YouTube-Demo-DIDN'T-Show-You](/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_3.png)\n\n## 브랜딩 및 마케팅\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 코스터 위의 로고: ChatGPT-4o는 명확한 소재 및 디자인 명세를 반영하여 코스터와 같은 제품에 브랜드 로고를 정확하게 배치할 수 있습니다. 예를 들어, 나무와 대리석 코스터에 새겨진 OpenAI 로고를 묘사하여 브랜딩 및 제품 디자인 작업을 처리하는 데 정확성을 보여줍니다. 이 기능은 브랜드 제품을 시각화하고 마케팅, 디자인 및 프레젠테이션에 도움이 되는 데 매우 가치가 있습니다.\n\n![이미지](/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_4.png)\n\n## 고급 텍스트 렌더링\n\n- 여러 줄 렌더링 — 로봇 문자 입력: 이 기능을 통해 ChatGPT-4o는 로봇이 문자를 보내는 것을 자세히 보여주는 능력이 있습니다. 명확하고 가독성 있는 여러 줄 메시지를 생성할 수 있습니다. 로봇이 핸드폰의 메시지 앱을 보는 일인칭 시점을 묘사하여 텍스트가 말풍선 안에서 정확하게 표현되도록 합니다. 이 기능은 교육 자료, 이야기 전달 및 디자인 개념에 대한 시각적 콘텐츠를 작성하는 데 모델의 유틸리티를 향상시킵니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_5.png\" /\u003e\n\n# 포괄적인 멀티미디어 콘텐츠 생성\n\n- 다양한 스피커가 있는 회의록: ChatGPT-4o는 오디오 입력을 해석하여 다양한 스피커를 인식하고 자세한 필기를 생성하는 데 능숙합니다. 이는 목소리를 구별하고 대화를 정확하게 속성 지정하여 필기를 명확하고 읽기 쉬운 형식으로 제공할 수 있습니다. 이 기능은 회의, 인터뷰 및 정확한 음성 상호작용 기록이 필요한 다른 시나리오에 유용합니다.\n- 강의 요약: ChatGPT-4o는 긴 강의나 프레젠테이션을 간결하고 정보량 풍부한 요약으로 만들 수 있습니다. 내용을 글머리 기호나 번호 목록으로 구조화하여 요약이 쉽게 읽고 이해할 수 있도록 합니다. 이 기능은 교육 목적, 회의록 및 콘텐츠 큐레이션에 유용하며, 긴 콘텐츠에서 중요한 포인트를 추려내는 것이 중요한 상황에 가치가 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_6.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 변수 바인딩 - 큐브 쌓기: 이 능력을 통해 ChatGPT-4o는 텍스트 설명을 기반으로 특정 특성과 배열을 가진 객체를 시각화할 수 있습니다. 예를 들어, 다양한 색상과 문자가 특정 순서로 쌓인 큐브를 정확하게 묘사할 수 있어 복잡한 변수 할당을 다루고 일관성을 유지하는 능력을 보여줍니다. 이 기능은 교육 자료, 안내서 및 창의적인 프로젝트를 시각화하는 데 유용합니다.\n- 콘크리트 시: ChatGPT-4o는 시와 시각 예술을 결합한 시각적으로 매력적이고 구조적으로 정확한 콘크리트 시를 만들 수 있습니다. 단어를 모양을 형성하도록 배열하거나 OpenAI 로고와 같은 모양을 형성하는 등의 단어를 배치하고 추가 지침에 따라 디자인을 사용자 정의할 수 있습니다. 이 기능은 텍스트를 시각적 요소와 결합하여 매력적이고 의미 있는 예술을 만드는 프로젝트에 대한 유틸리티를 향상시킵니다.\n\n![이미지](/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_7.png)\n\n# 무결한 안전성과 사용 편의성\n\n안전성은 ChatGPT-4o의 설계의 중요한 요소입니다. 내장된 안전 메커니즘과 사후 교육을 통한 다듬어진 동작으로 모델은 다양한 모달리티에서 책임 있는 사용을 보장합니다. ChatGPT-4o가 사이버 보안 및 편향을 포함한 다양한 안전 범주에서 중간 위험 수준 이하를 유지하는 것을 보여 주는 포괄적인 평가가 이루어졌습니다. 사회심리학과 오진 정보에 대한 외부 전문가들은 잠재적인 위험을 식별하고 완화하기 위해 모델을 엄격하게 테스트했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델의 안전 프로토콜을 통해 사용자 보안이나 개인 정보 보호를 저해하지 않고 다양한 환경에서 배치할 수 있습니다. 견고한 디자인과 지속적인 개선으로 교육, 의료 및 전문 서비스 분야의 민감한 응용 프로그램에 신뢰할 만한 선택지가 됩니다.\n\n# 가용성 및 향후 출시 계획\n\nChatGPT-4o는 무료 티어 및 플러스 사용자에게 확장된 메시지 제한이 적용된 상태로 현재 사용 가능합니다. 개발자들은 API를 통해 텍스트 및 비전 작업에 액세스할 수 있습니다. 앞으로 몇 주 내에 음성 및 비디오 기능은 일부 신뢰할 수 있는 파트너 그룹에게 출시될 예정입니다.\n\n이 단계적 출시를 통해 OpenAI는 피드백을 수집하고 필요한 조정을 수행함으로써 모델의 전체 잠재력을 실현하고 성능 및 신뢰성의 높은 기준을 유지하는 것을 보장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 새로운 지평을 탐험 중\n\nChatGPT-4o는 성능을 향상시키는 데 그치지 않습니다. 인공지능이 자연스럽고 창의적으로 사람들과 상호 작용하는 잠재력을 탐험하는 데 중점을 두고 있습니다. ChatGPT-4o는 상세한 시각적 서술을 만들거나 사용자 정의 글꼴을 디자인하거나 정확한 오디오 전사를 제공하는 것과 같은 일들을 수행하는 데 있어 인공지능 기술의 새로운 기준을 정립하고 있습니다.\n\n멀티모달리티를 원활하게 통합하고 해석하며, 고급 추론 능력과 창의적 능력을 결합하여 ChatGPT-4o는 인공지능의 진화 과정에서 중요한 이정표로 자리매김하고 있습니다. ChatGPT-4o가 이끄는 인간-컴퓨터 상호작용의 미래는 촉망받으며, 인공지능이 어디까지 성취할 수 있는지의 한계를 넓히는 데 도움이 되고 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_0.png"},"coverImage":"/assets/img/2024-05-18-ChatGPT-4osSecretSuperpowersWhattheYouTubeDemoDIDNTShowYou_0.png","tag":["Tech"],"readingTime":9},{"title":"BiTCN 컨볼루션 네트워크를 활용한 다변수 시계열 예측","description":"","date":"2024-05-18 19:41","slug":"2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks","content":"\n![image](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_0.png)\n\n시계열 예측 분야에서는 모델의 아키텍처가 주로 다층 퍼셉트론(MLP) 또는 트랜스포머 아키텍처에 의존합니다.\n\nN-HiTS, TiDE 및 TSMixer와 같은 MLP 기반 모델은 훈련 속도가 빠르면서 매우 좋은 예측 성능을 달성할 수 있습니다.\n\n한편, PatchTST 및 iTransformer와 같은 트랜스포머 기반 모델도 좋은 성능을 달성하지만 더 많은 메모리를 소비하고 더 많은 훈련 시간이 필요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아직도 예측에서 널리 활용되고 있지 않은 아키텍처 하나가 있습니다: 합성곱 신경망(CNN).\n\n전통적으로 CNN은 컴퓨터 비전에 적용되었지만, 예측 분야에서는 TimesNet이 최근의 예만 있습니다.\n\n그러나 CNN은 순차 데이터를 처리하는 데 효과적임이 입증되었으며, 그들의 아키텍처는 병렬 계산을 허용하여 훈련 속도를 크게 높일 수 있습니다.\n\n따라서 본 기사에서는 2023년 3월 논문 'Parameter-efficient deep probabilistic forecasting'에서 제안된 BiTCN을 탐색합니다. 두 개의 시계열 합성곱 신경망(TCN)을 활용하여 이 모델은 과거와 미래의 변수를 인코딩하면서도 계산 효율적인 특징을 유지합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n더 자세한 내용은 원본 논문을 꼭 읽어보세요.\n\n시작해봅시다!\n\n## BiTCN 탐험\n\n이전에 언급된대로, BiTCN은 두 개의 시계열 합성곱 신경망을 활용하므로 그 이름이 BiTCN입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n한 TCN은 미래 공변량을 인코딩하고, 다른 하나는 과거 공변량 및 시계열의 역사적 값들을 인코딩합니다. 이렇게 함으로써 모델은 데이터로부터 시간 정보를 배울 수 있고, 합성곱의 사용으로 계산 효율성을 유지할 수 있습니다.\n\n여기에는 분석할 것이 많기 때문에 아키텍처를 좀 더 자세히 살펴보겠습니다.\n\n## BiTCN 아키텍처\n\nBiTCN의 아키텍처는 많은 시계열 블록으로 구성되어 있습니다. 각 블록은 다음과 같이 구성됩니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 확장된 합성곱\n- GELU 활성화 함수\n- 드롭아웃 단계\n- 완전 연결 레이어\n\n시계열 블록의 일반적인 아키텍처는 아래에 표시됩니다.\n\n![temporal block](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_1.png)\n\n위 그림에서 각 시계열 블록이 출력 O를 생성함을 볼 수 있습니다. 최종 예측은 N개의 레이어에 쌓인 각 블록의 모든 출력을 더하여 얻습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n드롭아웃과 댄스 레이어는 신경망에서 흔한 구성 요소입니다. 그러나 이번에는 확장 컨볼루션(dilated convolution)과 GELU 활성화 함수에 대해 좀 더 자세히 살펴봅시다.\n\n## 확장 컨볼루션\n\n확장 컨볼루션의 목표를 더 잘 이해하기 위해, 기본적인 컨볼루션이 어떻게 작동하는지 상기해 봅시다.\n\n![이미지](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 그림에서 일차원 입력에 대한 전형적인 합성곱이 어떻게 보이는지 볼 수 있습니다. 출력의 길이를 동일하게 유지하기 위해 입력 시리즈는 왼쪽에 0으로 채워집니다.\n\n세 개의 커널 크기와 한 개의 스트라이드를 가정할 때, 위 그림에 나와 있는대로 출력 텐서도 네 개의 길이를 가집니다.\n\n출력의 각 요소가 세 개의 입력 값을 기반으로 한다는 것을 볼 수 있습니다. 다시 말해 출력은 색인의 값과 이전 두 값에 의존합니다.\n\n이를 수용 영역(Receptive Field)이라고 합니다. 시계열 데이터를 다루고 있으므로 출력 계산이 더 긴 이력을 볼 수 있도록 수용 영역을 증가시키는 것이 유익할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그렇게 하려면, 커널 크기를 크게 하거나 더 많은 합성곱 계층을 쌓을 수 있습니다. 커널 크기를 크게 하는 것은 최선의 선택이 아닙니다. 정보를 손실하고 모델이 데이터의 유용한 관계를 학습하지 못할 수 있습니다. 그래서 더 많은 합성곱을 쌓아보겠습니다.\n\n위 그림에서 볼 수 있듯이, 커널 크기가 3인 두 개의 합성곱 작업을 쌓으면 출력의 마지막 요소는 이제 입력의 다섯 요소에 의존합니다. 따라서 수용 영역이 3에서 5로 증가했습니다.\n\n안타깝게도 이것도 문제가 됩니다. 이러한 방식으로 수용 영역을 증가시키면 아주 깊은 신경망이 생성될 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n따라서, 우리는 모델에 너무 많은 레이어를 추가하지 않으면서 수용 영역을 증가시키기 위해 확장된 합성곱을 사용합니다.\n\n![이미지](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_4.png)\n\n위의 그림에서 우리는 2-확장된(convolution)을 실행한 결과를 볼 수 있습니다. 기본적으로 매 두 요소가 하나의 출력을 생성하는 것으로 간주됩니다. 따라서 우리는 이제 컨벌루션을 쌓지 않고도 수용 영역이 5임을 볼 수 있습니다.\n\n실제로 수용 영역을 더 증가시키기 위해, 주로 2로 설정된 확장 베이스를 사용하여 많은 희석커널(diluted kernel)을 쌓습니다. 이는 첫 번째 레이어가 2¹-확장 커널이되고, 그다음에 2²-확장 커널이 따르며, 그런 다음 2³로 이어지는 방식입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n수용 영역이 늘어나면 모델은 더 긴 입력 시퀀스를 고려하여 출력을 생성할 수 있습니다. Dilated convolutions을 사용하면 합리적인 수의 레이어를 유지할 수도 있습니다.\n\n이제 Dilated convolutions의 내부 작업을 이해했으니 GELU 활성화 함수를 알아보겠습니다.\n\n## GELU 활성화 함수\n\n많은 딥러닝 아키텍처에서는 ReLU(Recitified Linear Unit) 활성화 함수를 사용합니다. ReLU의 방정식은 아래와 같이 표시됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 식을 보면 ReLU는 간단히 0과 입력 중 최대 값을 취하는 것을 알 수 있습니다. 다시 말해, 입력이 양수이면 입력이 반환되고, 입력이 음수이면 0이 반환됩니다.\n\nReLU는 사라지는 그래디언트 문제를 완화하는 데 도움이 되지만 죽은 ReLU 문제를 만들 수도 있습니다.\n\n이는 네트워크에서 일부 뉴런이 오직 0만 출력하여 모델의 학습에 더 이상 기여하지 않는 경우에 발생합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n해당 상황에 대처하기 위해 가우시안 에러 선형 유닛 또는 GELU를 사용할 수 있습니다. GELU 방정식은 아래와 같이 나타낼 수 있습니다.\n\n![GELU Equation](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_6.png)\n\n이 함수를 사용하면 입력 값이 0보다 작을 때 작은 음수 값을 활성화 함수로 사용할 수 있습니다.\n\n![Activation Function with GELU](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_7.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이렇게 하면 신경세포가 소멸하지 않게 되어 음수 입력 값을 사용하여 0이 아닌 값이 반환될 수 있습니다. 이는 역전파에 대해 더 풍부한 그래디언트를 제공하며 모델의 기능을 유지할 수 있습니다.\n\n## BiTCN에서 모두 모아보기\n\n이제 BiTCN의 시간 블록의 내부 작업을 이해했으니, 우리는 모델에서 모든 것이 어떻게 함께 동작하는지 살펴봅시다.\n\n![이미지](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_8.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 그림에서는 늦게 발생한 값들이 밀도 레이어를 통과하고 시간 블록 스택을 거친 후 모든 이전 공변량과 결합된 것을 볼 수 있습니다.\n\n상단에는 범주형 공변량이 다른 공변량과 결합되기 전에 먼저 임베딩된 것을 볼 수 있습니다. 여기서는 미래와 과거 공변량이 아래에 표시된 대로 모두 결합됨에 유의해주세요.\n\n그럼 그 값들은 밀도 레이어와 시간 블록 스택을 거쳐 이끌어집니다.\n\n최종 출력은 아래에 표시된 것과 같이 늦게 발생한 값과 공변량에서 나온 정보가 결합된 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![그림](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_9.png)\n\n위의 그림은 하나의 시간 블록이 공변량의 미래 값을 활용하여 모델 출력을 결정하는 아이디어를 강조합니다 (빨간 점으로 표시됨).\n\n마지막으로, BiTCN은 예측 주변에 신뢰 구간을 구성하기 위해 Student’s t-분포를 사용합니다.\n\n이제 BiTCN의 내부 작업을 이해했으니, Python을 사용하여 소규모 예측 프로젝트에 적용해 봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# BiTCN을 사용한 예측\n\n이 실험에서는 BiTCN을 N-HiTS 및 PatchTST와 함께 사용하여 장기 예측 작업을 수행합니다.\n\n구체적으로, 블로그 웹사이트의 일일 조회수를 예측하는 데 사용합니다. 데이터셋에는 일일 조회수와 새로운 글이 게시된 날짜를 나타내는 지표, 미국의 공휴일을 나타내는 지표와 같은 외생 특성이 포함되어 있습니다.\n\n이 데이터셋은 제가 직접 제 웹사이트의 트래픽을 사용하여 컴파일했습니다. 데이터셋은 여기서 공개적으로 이용 가능합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n본 부분에서는 외부 기능을 지원하는 BiTCN의 사용 준비 구현을 제공하는 것으로 내가 알기로는 유일한 라이브러리인 neuralforcast를 사용합니다.\n\n언제나 GitHub에 이 실험의 전체 소스 코드가 있습니다.\n\n시작해 봅시다!\n\n## 초기 설정\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 프로젝트에 필요한 라이브러리를 가져오는 것이 첫 번째 단계입니다.\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NHITS, PatchTST, BiTCN\n```\n\n그런 다음, 데이터를 DataFrame으로 읽어옵니다.\n\n```js\ndf = pd.read_csv('https://raw.githubusercontent.com/marcopeix/time-series-analysis/master/data/medium_views_published_holidays.csv')\ndf['ds'] = pd.to_datetime(df['ds]')\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n데이터를 그래프로 나타낼 수도 있습니다.\n\n```js\npublished_dates = df[df[\"published\"] == 1];\nholidays = df[df[\"is_holiday\"] == 1];\n\nfig, (ax = plt.subplots((figsize = (12, 8))));\n\nax.plot(df[\"ds\"], df[\"y\"]);\nax.scatter(\n  published_dates[\"ds\"],\n  published_dates[\"y\"],\n  (marker = \"o\"),\n  (color = \"red\"),\n  (label = \"새 기사\")\n);\nax.scatter(\n  holidays[\"ds\"],\n  holidays[\"y\"],\n  (marker = \"x\"),\n  (color = \"green\"),\n  (label = \"미국 공휴일\")\n);\nax.set_xlabel(\"날짜\");\nax.set_ylabel(\"총 조회수\");\nax.legend((loc = \"best\"));\n\nfig.autofmt_xdate();\n\nplt.tight_layout();\n```\n\n![이미지](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_10.png)\n\n위 그림에서, 주중에 주말보다 더 많은 방문이 발생하는 주별 계절성이 명확히 나타납니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한, 방문 횟수의 급증은 일반적으로 새로운 기사가 게시된 후 발생합니다(빨간 점으로 표시됨). 새로운 콘텐츠가 더 많은 트래픽을 유도하기 때문에 새로운 기사가 게시될 때 일반적으로 트래픽이 증가합니다. 마지막으로, 미국 공휴일(녹색 십자로 표시됨)은 종종 낮은 트래픽을 시사합니다.\n\n따라서, 외부 요인의 영향을 명확히 볼 수 있는 시리즈이며, BiTCN을 위한 훌륭한 사용 사례입니다.\n\n## 데이터 처리\n\n이제 데이터를 학습 세트와 테스트 세트로 분할해 봅시다. 테스트를 위해 마지막 28개 항목을 예약합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\ntrain = df[:-28]\ntest = df[-28:]\n```\n\n그런 다음, 예보 기간에 대한 날짜 및 외생 변수의 미래 값이 포함된 DataFrame을 생성합니다.\n\n미래의 외생 변수 값을 제공하는 것이 의미가 있다는 점에 유의해야 합니다. 미래의 미국의 공휴일 날짜는 미리 알려져 있으며, 기사의 발행 또한 계획할 수 있기 때문입니다.\n\n```python\nfuture_df = test.drop(['y'], axis=1)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n좋아요! 이제 시리즈를 모델링할 준비가 되었습니다.\n\n## 모델링\n\n언급했듯이, 이 프로젝트에서는 N-HiTS(MLP 기반), BiTCN(CNN 기반) 및 PatchTST(Transformer 기반)를 사용합니다.\n\nN-HiTS와 BiTCN은 둘 다 외부 특성을 사용한 모델링을 지원하지만, PatchTST는 지원하지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 실험의 수평선은 테스트 세트의 전체 길이를 포함하기 위해 28로 설정됩니다.\n\n```js\nhorizon = len(test);\n\nmodels = [\n  NHITS(\n    (h = horizon),\n    (input_size = 5 * horizon),\n    (futr_exog_list = [\"published\", \"is_holiday\"]),\n    (hist_exog_list = [\"published\", \"is_holiday\"]),\n    (scaler_type = \"robust\")\n  ),\n  BiTCN(\n    (h = horizon),\n    (input_size = 5 * horizon),\n    (futr_exog_list = [\"published\", \"is_holiday\"]),\n    (hist_exog_list = [\"published\", \"is_holiday\"]),\n    (scaler_type = \"robust\")\n  ),\n  PatchTST(\n    (h = horizon),\n    (input_size = 2 * horizon),\n    (encoder_layers = 3),\n    (hidden_size = 128),\n    (linear_hidden_size = 128),\n    (patch_len = 4),\n    (stride = 1),\n    (revin = True),\n    (max_steps = 1000)\n  ),\n];\n```\n\n그런 다음, 훈련 세트에 모델을 적용합니다.\n\n```js\nnf = NeuralForecast((models = models), (freq = \"D\"));\nnf.fit((df = train));\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그럼, 우리는 외부 요인의 미래 값을 사용하여 예측을 생성할 수 있어요.\n\n```js\npreds_df = nf.predict((futr_df = future_df));\n```\n\n좋아요! 지금 이 시점에서, preds_df에 저장된 예측이 있어요. 각 모델의 성능을 평가할 수 있어요.\n\n## 평가\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예측값과 실제 값들을 하나의 DataFrame으로 합치는 것으로 시작합니다.\n\n```python\ntest_df = pd.merge(test, preds_df, 'left', 'ds')\n```\n\n선택적으로, 예측값을 실제 값과 비교해서 아래 그림과 같이 시각화할 수도 있습니다.\n\n![예제 그림](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_11.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 그림에서는 모든 모델이 실제 트래픽을 전반적으로 과대 예측한 것으로 보입니다.\n\n그럼 최상의 성능을 발휘하는 모델을 찾기 위해 평균 절대 오차 (MAE)와 대칭 평균 절대 백분율 오차 (sMAPE)를 측정해보겠습니다.\n\n```js\nfrom utilsforecast.losses import mae, smape\nfrom utilsforecast.evaluation import evaluate\n\nevaluation = evaluate(\n    test_df,\n    metrics=[mae, smape],\n    models=[\"NHITS\", \"BiTCN\", \"PatchTST\"],\n    target_col=\"y\",\n)\n\nevaluation = evaluation.drop(['unique_id'], axis=1)\nevaluation = evaluation.set_index('metric')\n\nevaluation.style.highlight_min(color='blue', axis=1)\n```\n\n![이미지](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_12.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 표에서 BiTCN이 최상의 성능을 달성했음을 확인할 수 있습니다. 해당 모델의 MAE 및 sMAPE가 가장 낮기 때문입니다.\n\n이 실험만으로는 BiTCN의 강력한 벤치마크는 아니지만, 외생 변수를 활용한 예측 문맥에서 가장 우수한 결과를 달성하는 것을 볼 수 있어 흥미로운 실험입니다.\n\n# 결론\n\nBiTCN 모델은 이전 값과 미래 값을 함께 인코딩하기 위해 두 개의 시간 합성곱 신경망을 활용하여 효율적인 다변량 시계열 예측을 수행합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n시계열 분야에서 컨볼루션 신경망의 성공적인 응용을 보는 것은 흥미로운 일이죠. 대부분의 모델은 MLP 또는 트랜스포머를 기반으로 하지만요.\n\n저희의 소규모 실험에서 BiTCN이 가장 우수한 성능을 발휘했습니다. 하지만 저는 각 문제에는 독특한 해결책이 필요하다고 믿습니다. 이제 BiTCN을 도구 상자에 추가하고 여러분의 프로젝트에 적용해 보세요.\n\n독자 여러분, 읽어 주셔서 감사합니다! 즐기셨기를 바라며 무엇인가 새로운 것을 배우셨기를 기대합니다.\n\n건배 🍻\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 저를 지지해주세요\n\n제 작품을 즐기고 계신가요? Buy me a coffee로 제게 지지를 보여주세요. 그러면 여러분은 제게 격려를 주고, 저는 커피 한 잔을 즐길 수 있어요! 만약 그러고 싶다면, 아래 버튼을 클릭해주세요 👇\n\n![Buy me a coffee](/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_13.png)\n\n# 참고자료\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n**Parameter-efficient deep probabilistic forecasting** by Olivier Sprangers, Sebastian Schelter, Maarten de Rijke\n\nExplanation of **dilated convolution** and figures of **dilates convolutions** inspired: **Temporal convolutional networks and forecasting** by Unit8\n","ogImage":{"url":"/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_0.png"},"coverImage":"/assets/img/2024-05-18-BiTCNMultivariateTimeSeriesForecastingwithConvolutionalNetworks_0.png","tag":["Tech"],"readingTime":18},{"title":"시간을 통해 전파하는 역전파  RNN이 학습하는 방법","description":"","date":"2024-05-18 19:38","slug":"2024-05-18-BackpropagationThroughTimeHowRNNsLearn","content":"\n![RNN](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_0.png)\n\n순환 신경망(RNN)은 시계열 및 자연어와 같은 순차 데이터를 처리하는 정규 피드포워드 신경망 변형입니다.\n\n과거 입력 및 출력에서 다음 단계로 정보를 전달할 수 있도록 \"순환\" 뉴런을 추가하여 이를 달성합니다. 아래 다이어그램은 전통적인 RNN을 보여줍니다:\n\n![RNN Diagram](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n왼쪽에는 반복 뉴런이 있고, 오른쪽에는 시간을 거듭할수록 펼쳐진 반복 뉴런이 있습니다. 이전 실행이 이어지는 계산에 전달되는 방식을 주목해주세요.\n\n이것은 시스템에 어느정도의 \"기억력\"을 추가하여 모델이 이전 시간에 발생한 역사적 패턴을 잡는 데 도움이 됩니다.\n\nY_1을 예측할 때, 반복 뉴런은 X_1의 입력과 이전 시간 단계의 출력인 Y_0을 사용합니다. 이는 Y_0이 Y_1에 직접적인 영향을 미치고, 이는 Y_2에 간접적으로 영향을 미침을 의미합니다.\n\nRNN에 대한 완벽한 소개와 몇 가지 실습 예제를 원하신다면, 이전 포스트를 확인해보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 이 기사에서는 RNN이 어떻게 학습하는지를 뒷방향 시간으로 이해하자구!\n\n# 역전파란?\n\nBPTT에 대해 들어가기 전에, 일반적인 역전파를 다시 확인하는 것이 중요하다. 역전파는 일반적인 피드포워드 신경망을 훈련하는 데 사용되는 알고리즘이야.\n\n역전파의 본질은 손실 함수를 기반으로 신경망의 각 매개변수를 조정하여 오차를 최소화하려는 것이야. 이 조정은 편도함수와 연쇄법칙을 사용해 이루어져.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\ncompute 그래프를 통해 간단한 예제를 보여드릴게요. compute 그래프는 신경망과 매우 닮은데요.\n\n다음 함수를 살펴보세요:\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_2.png)\n\n이것을 compute 그래프로 그릴 수 있습니다. 이는 함수를 시각화하는 방법일 뿐이에요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n`\u003ctable\u003e` 태그를 마크다운 형식으로 변경해 주세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 먼저 p=x-y 및 f=pz에 대한 편미분을 계산할 수 있습니다:\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_5.png)\n\n하지만, 어떻게 얻을까요?\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_6.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요기서는 chain rule을 사용해요! x에 대한 예시가 있어요:\n\n![chain rule example](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_7.png)\n\n서로 다른 편미분을 결합하면 원하는 표현을 얻을 수 있어요. 그래서 위 예시에서:\n\n![partial derivatives example](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_8.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nf에 대한 x의 출력 그라디언트는 z입니다. 이게 말이 되지요. z는 x를 곱하는 유일한 값이기 때문이죠.\n\ny와 z에 대해서도 반복합니다:\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_9.png)\n\n이러한 그라디언트들과 그에 해당하는 값들을 계산 그래프에 적어볼 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Gradient Descent Image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_10.png)\n\nGradient descent works by updating the values (x, y, z) by a small amount in the opposite direction of the gradient. The goal of gradient descent is to try and minimize the output function. For example, for x:\n\n![Equation for x](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_11.png)\n\nWhere h is called the learning rate, it decides how much the parameter will get updated. For this case, let’s define h=0.1, so x=3.7.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금 출력 결과는 무엇인가요?\n\n![이미지](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_12.png)\n\n결과가 작아졌죠. 다시 말해, 최소화 중이에요!\n\n이것이 역전파가 어떻게 작동하는지에 대한 직관을 제공해줬으면 좋겠어요. 기본적으로 그것은 그라디언트 강하와 같지만, 연쇄 법칙을 사용하여 상류 그라디언트를 전달한다는 거죠.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n백프로패게이션에 대한 전문 기사가 있어요. 더 읽고 싶으시면 참고하세요.\n\n# 시간을 통한 역전파란?\n\n## 개요\n\n우리는 방금 백프로패게이션이 그래디언트 강하법이라는 것을 보았습니다. 그러나 우리는 네트워크 층마다 오류(도함수)를 역방향으로 전파하고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nBPTT은 각 시점에서 역전파를 수행하여 이러한 정의를 확장합니다. 예제를 함께 살펴보겠습니다.\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_13.png)\n\n다음 다이어그램에서:\n\n- Y는 출력 벡터입니다.\n- X는 피처의 입력 벡터입니다.\n- h는 숨겨진 상태입니다.\n- V는 출력을 위한 가중치 행렬입니다.\n- U는 입력을 위한 가중치 행렬입니다.\n- W는 숨겨진 상태를 위한 가중치 행렬입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n어떤 시간 t에서 다음은 계산된 출력입니다:\n\n![2024-05-18-BackpropagationThroughTimeHowRNNsLearn_14.png](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_14.png)\n\n여기서 σ는 일반적으로 tanh 또는 sigmoid인 활성화 함수입니다.\n\n우리의 손실 함수가 평균 제곱 오차인 경우를 가정해 봅시다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_15.png)\n\nA_t is the actual value that we want our prediction to equal.\n\n## Backpropagation Through Time\n\nNow, we are in a position to start doing BPTT after this problem has been set up.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n백프로파게이션의 목표는 모델의 가중치와 매개변수를 조정하여 오차를 최소화하는 것입니다. 이것은 가중치와 매개변수에 대한 오차의 편미분을 통해 수행됩니다.\n\n시간 단계 3의 업데이트를 계산해 봅시다.\n\nV 가중치 행렬에 대해:\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_16.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요번은 꽤 간단해요. E_3은 Y_3의 함수에요. 그래서 Y_3에 대해 E_3을 미분하고, Y_3을 V에 대해 미분해요. 여기서는 너무 복잡한 일은 없어요.\n\nW 가중 행렬에 대해:\n\n![이미지](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_17.png)\n\n이제 조금 멋진 것들이 나타나네요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 식에서 첫 번째 용어는 비교적 간단합니다. E_3는 Y_3의 함수이며, Y_3는 h_3의 함수이며, h_3는 W의 요소입니다. V 행렬에서 보았던 것과 동일한 프로세스입니다.\n\n그러나 h_2와 h_1에 대한 이전 단계에서도 행렬 W가 사용되었으므로 그 이전 단계에 대한 미분을 고려해야 합니다.\n\nRNN에서 상태 h_3가 이전 상태에 종속되므로 W의 영향을 모든 시간 단계에 걸쳐 고려해야 합니다.\n\nU 가중치 행렬에 대해:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_18.png)\n\nU 행렬에 대한 오류는 W에 대한 것과 매우 유사하며, 다른 점은 숨겨진 상태 h를 U로 미분한다는 점입니다.\n\n숨겨진 상태는 이전 숨겨진 상태와 새 입력의 복합 함수입니다.\n\n## 일반화된 공식\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nBPTT는 다음과 같이 일반화될 수 있습니다:\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_19.png)\n\n여기서 J는 RNN 내의 임의의 가중치 행렬이며, U, W 또는 V가 될 수 있습니다.\n\nRNN의 총 오차(손실)는 각 시간 스텝에서의 오차인 E_t의 합입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![2024-05-18-BackpropagationThroughTimeHowRNNsLearn_20.png](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_20.png)\n\nAnd, that’s pretty much all there is to training an RNN! However, there is one problem ...\n\n# Exploding \u0026 Vanishing Gradient Problem\n\n## Overview\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nRNN의 중요한 문제 중 하나는 사라지는 그래디언트와 폭발하는 그래디언트 문제입니다. 이 문제는 BPTT를 수행할 때 네트워크를 T번 시간 단계만큼 펼치기 때문에 발생합니다. 이로 인해 네트워크는 사실상 T개의 레이어를 갖게 됩니다.\n\n흔히 사용되는 타임스탬프의 수가 많기 때문에, 펼쳐진 네트워크는 보통 아주 깊어집니다. 그래디언트가 역방향으로 전파됨에 따라 지수적으로 증가하거나 감소할 수 있습니다.\n\n이것은 활성화 함수가 일반적으로 tanh 또는 sigmoid인 경우에 발생합니다. 이러한 함수들은 입력을 작은 출력 범위로 압축시킵니다: sigmoid는 0에서 1, tanh는 -1에서 1까지입니다.\n\n이러한 함수의 미분 값은 큰 절댓값 입력에 대해 작고 거의 0에 가깝습니다. RNN과 같이 깊은 네트워크에서 이러한 미분 값이 연쇄 법칙에 사용될 때, 위에서 보았듯이 많은 작은 숫자들이 곱해지게 됩니다. 이는 매우 작은 숫자가 되어 초기 레이어에서 거의 0에 가까운 그래디언트를 생성하게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 수학적 추론\n\n이전 내용을 참고하면, 이전 시간 단계의 다른 숨은 상태에 대한 숨은 상태의 편미분을 계산하는 많은 경우가 있습니다.\n\n![image](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_21.png)\n\n그런 다음 우리의 숨은 상태(시간 단계) 수에 따라 여러 번 곱해집니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 Markdown 형식의 텍스트입니다.\n\n![이미지1](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_22.png)\n\n여기에 일어나는 일입니다:\n\n![이미지2](/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_23.png)\n\n이제 RNN이 경험하는 기울기 소실과 폭주에 대한 이유를 이해할 수 있습니다. 순차 길이에 따라 기울기가 지수 함수적으로 소실되는 것은 숨은 상태의 편도함수를 반복적으로 곱하기 때문인 체인 규칙 효과입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 문제\n\n만약 그래디언트가 소멸한다면, RNN은 매우 나쁜 장기 기억력을 가지고 있어 과거에서 많은 것을 배울 수 없게 됩니다. 이는 정말 좋지 않은 상황인데, RNN은 순차 데이터를 다룰 수 있도록 메모리를 갖추도록 설계되었기 때문입니다.\n\n이로 인해 그래디언트가 매우 작아지게 되는데, 이는 가중치가 업데이트되는 값도 작아진다는 것을 의미합니다. 따라서 네트워크가 훈련하는 데 시간이 오래 걸리고 더 많은 컴퓨팅 자원을 사용하게 됩니다.\n\n물론, 많은 현명한 사람들이 이 문제를 해결하기 위한 방법을 개발해 왔는데, 다음 글에서 그에 대해 논의할 예정입니다!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이전 게시물에서는 폭망 경사 문제와 이를 극복하는 데 사용되는 도구에 대해 더 많이 읽을 수 있어요.\n\n# 요약 및 추가적인 생각\n\nRNN은 일반 피드포워드 신경망과 비슷한 알고리즘을 사용하여 학습합니다. 시간을 통한 역전파는 일반적인 역전파와 매우 유사하지만, 각 오류와 가중치 행렬에 대해 해당 가중치 행렬이 사용된 모든 과거의 시간을 고려해야 합니다. 이로 인해 불안정한 경사를 초래할 수 있습니다. RNN은 종종 매우 깊은 구조를 가지므로 도함수를 여러 번 곱하게 되어 초기 레이어에 도달했을 때 그 값을 증가시키거나 감소시킬 수 있습니다.\n\n# 저와 소통해요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- LinkedIn, X(Twitter) 또는 Instagram\n- 기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 내 YouTube 채널!\n\n## 참고 및 더 읽을거리\n\n- Stanford RNN CheatSheet\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. 2019년 9월. 출판사: O’Reilly Media, Inc. ISBN: 9781492032649.\n","ogImage":{"url":"/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_0.png"},"coverImage":"/assets/img/2024-05-18-BackpropagationThroughTimeHowRNNsLearn_0.png","tag":["Tech"],"readingTime":14},{"title":"LSTM 시계열 예측에서 흔히 발생하는 오류를 수정하는 방법","description":"","date":"2024-05-18 19:35","slug":"2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting","content":"\r\nLSTM을 시계열 예측에 사용할 때, 사람들은 흔히 범할 수 있는 함정에 빠지곤 합니다. 이를 설명하기 위해서는 회귀자와 예측자의 작동 방식을 살펴볼 필요가 있습니다. 예측 알고리즘은 시계열 데이터를 다루는 방법을 아래와 같이 보여줍니다:\r\n\r\n![How a forecasting algorithm works](/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_0.png)\r\n\r\n한편, 회귀 문제는 다음과 같이 보일 것입니다:\r\n\r\n![How a regression problem looks](/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_1.png)\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\nLSTM이 회귀기이므로 시계열을 회귀 문제로 변환해야 합니다. 이를 수행하는 여러 방법이 있지만, 이 섹션에서는 창(Window) 및 다중 단계(Multi-Step) 방법에 대해 설명하고, 어떻게 작동하는지와 특히 이를 활용할 때 발생할 수 있는 일반적인 실수를 피하는 방법에 대해 논의할 것입니다.\r\n\r\n창 방법(Window Method)에서, 시계열은 이전 각 시간 단계의 값과 결합되어 창이라고 불리는 가상 특성으로 되어 있습니다. 여기서 창 크기가 3인 창이 있습니다:\r\n\r\n![창 방법 이미지](/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_2.png)\r\n\r\n다음 함수는 단일 시계열에서 창 방법 데이터 세트를 생성합니다. 사용자는 이전 값의 수(보통 look back이라고 함)를 선택해야 합니다. 결과 데이터 세트에는 대각선 반복이 있으며, look-back 값에 따라 샘플의 수가 달라집니다:\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n```js\r\ndef window(sequences, look_back):\r\n    X, y = [], []\r\n    for i in range(len(sequences)-look_back-1):\r\n        x = sequences[i:(i+look_back)]\r\n        X.append(x)\r\n        y.append(sequences[i + look_back])\r\n    return np.array(X), np.array(y)\r\n```\r\n\r\n이제 결과를 살펴보겠습니다. 모델을 훈련한 후에는 테스트 세트에서 테스트됩니다. 다양한 소스와 튜토리얼에서는 비슷한 방법을 사용하여 결과를 컴파일하는 것을 제안했습니다. 그러나 나중에 설명할 것처럼 이 방법은 신빙성이 없습니다. 그럼 지금은 코드와 결과가 어떻게 보이는지 살펴보겠습니다:\r\n\r\n```js\r\nlook_back = 3\r\nX, y = window(ts_data, look_back)\r\n\r\n# 훈련-테스트 분할\r\ntrain_ratio = 0.8\r\ntrain_size = int(train_ratio * len(ts_data))\r\nX_train, X_test = X[:train_size-look_back], X[train_size-look_back:]\r\ny_train, y_test = y[:train_size-look_back], y[train_size-look_back:]\r\n\r\n# LSTM 모델 생성 및 훈련\r\nmodel = Sequential()\r\nmodel.add(LSTM(units=72, activation='tanh', input_shape=(look_back, 1)))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='mean_squared_error', optimizer='Adam', metrics=['mape'])\r\n\r\nmodel.fit(x=X_train, y=y_train, epochs=500, batch_size=18, verbose=2)\r\n\r\n# 예측 생성\r\nforecasts = model.predict(X_test)\r\nlstm_fits = model.predict(X_train)\r\n\r\n# 메트릭스 계산\r\nmape = mean_absolute_percentage_error(y_test, forecasts)\r\nr2 = r2_score(y_train, lstm_fits)\r\n\r\n# 날짜 초기화\r\ndate_range = pd.date_range(start='1990-01-01', end='2023-09-30', freq='M')\r\n\r\n# 맞춤 값에 원래 시계열과 맞추기 위한 비어있는 값 추가\r\nfits = np.full(train_size, np.nan)\r\nfor i in range(train_size-look_back):\r\n    fits[i+look_back] = lstm_fits[i]\r\n\r\n# 실제값, 맞춤값, 예측값 플롯\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(date_range, ts_data, label='Actual', color='blue')\r\nplt.plot(date_range[:train_size], fits, label='Fitted', color='green')\r\nplt.plot(date_range[train_size:], forecasts, label='Forecast', color='red')\r\nplt.title('FSC - Short - Passengers\\nOne Step Forward Forecast')\r\nplt.xlabel('Date')\r\nplt.ylabel('Passengers')\r\nplt.legend()\r\nplt.text(0.05, 0.05, f'R2 = {r2*100:.2f}%\\nMAPE = {mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12)\r\nplt.grid(True)\r\nplt.show()\r\n```\r\n\r\n\u003cimg src=\"/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_3.png\" /\u003e\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n문제: 결과는 훌륭해 보입니다. 그러나 샘플 테스트 세트를 살펴보면 특이한 결함이 보입니다:\r\n\r\n![이미지](/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_4.png)\r\n\r\n예를 들어 y9를 생성하는 데에는 y8이 입력으로 사용되었습니다. 훈련에는 사용되지 않았지만 미래 값을 포함하는 것은 이상합니다. 왜냐하면 우리는 미래의 시점을 예측하고 있기 때문입니다.\r\n\r\n해결책: 직접적으로 이전 값을 예측 값으로 대체하는 반복적 테스트 세트를 사용하면 이 문제를 해결할 수 있습니다. 이러한 배치 방식에서 모델은 자체 예측에 기반을 둔다. 일반적인 예측 알고리즘과 유사합니다.\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n아래 루프에서 다음을 수행합니다:\r\n\r\n```js\r\n# 반복적인 예측 및 대체\r\nfor i in range(len(X_test)):\r\n    forecasts[i] = model.predict(X_test[i].reshape(1, look_back, 1))\r\n    if i != len(X_test)-1:\r\n        X_test[i+1,look_back-1] = forecasts[i]\r\n        for j in range(look_back-1):\r\n            X_test[i+1,j] = X_test[i,j+1]\r\n```\r\n\r\n결과는 완벽하지는 않지만 적어도 정허하다고 할 수 있습니다:\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n\u003cimg src=\"/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_6.png\" /\u003e\r\n\r\n다중 단계 방법은 창 방법과 유사하지만 더 많은 대상 단계를 갖습니다. 다음은 두 개의 전방 단계의 샘플입니다:\r\n\r\n\u003cimg src=\"/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_7.png\" /\u003e\r\n\r\n사실, 이 방법에서 사용자는 n_steps_in과 n_steps_out을 선택해야 합니다. 다음 코드는 단순 시계열을 다중 단계 LSTM 훈련을 위해 준비된 데이터 세트로 변환합니다:\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n```js\r\n# 단변량 시퀀스를 다단계로 샘플링하기\r\ndef split_sequences(sequences, n_steps_in, n_steps_out):\r\n X, y = list(), list()\r\n for i in range(len(sequences)):\r\n     # 해당 패턴의 끝을 찾음\r\n     end_ix = i + n_steps_in\r\n     out_end_ix = end_ix + n_steps_out\r\n     # 시퀀스를 벗어나는지 확인\r\n     if out_end_ix \u003e len(sequences):\r\n         break\r\n     # 패턴의 입력 및 출력 부분 수집\r\n     seq_x, seq_y = sequences[i:end_ix], sequences[end_ix:out_end_ix]\r\n     X.append(seq_x)\r\n     y.append(seq_y)\r\n return np.array(X), np.array(y)\r\n```\r\n\r\n이제, 특성 뿐만 아니라 타겟도 대각선 반복을 가지고 있어 시계열과 비교하려면 그들을 평균화하거나 예측 중 하나를 선택해야 합니다. 아래 코드에서는 첫 번째, 마지막 및 평균 예측의 결과가 생성되며, 그에 이어 플롯이 제시됩니다. 여기서 첫 번째 예측은 한 달 전 예측을 의미하며, 마지막 예측은 12개월 전 예측을 의미합니다.\r\n\r\n```js\r\nn_steps_in = 12\r\nn_steps_out = 12\r\n\r\nX, y = split_sequences(ts_data, n_steps_in, n_steps_out)\r\nX = X.reshape(X.shape[0], X.shape[1], 1)\r\ny = y.reshape(y.shape[0], y.shape[1], 1)\r\n\r\n# 훈련 및 테스트 세트 분리\r\ntrain_ratio = 0.8\r\ntrain_size = int(train_ratio * len(ts_data))\r\nX_train, X_test = X[:train_size-n_steps_in-n_steps_out+1], X[train_size-n_steps_in-n_steps_out+1:]\r\ny_train = y[:train_size-n_steps_in-n_steps_out+1]\r\ny_test = ts_data[train_size:]\r\n\r\n# LSTM 모델 생성 및 훈련\r\nmodel = Sequential()\r\nmodel.add(LSTM(units=72, activation='tanh', input_shape=(n_steps_in, 1)))\r\nmodel.add(Dense(units=n_steps_out))\r\nmodel.compile(loss='mean_squared_error', optimizer='Adam', metrics=['mape'])\r\n\r\nmodel.fit(x=X_train, y=y_train, epochs=500, batch_size=18, verbose=2)\r\n\r\n# 예측 생성\r\nlstm_predictions = model.predict(X_test)\r\nlstm_fitted = model.predict(X_train)\r\n\r\nforecasts = [np.diag(np.fliplr(lstm_predictions), i).mean() for i in range(0, -lstm_predictions.shape[0], -1)]\r\nfits = [np.diag(np.fliplr(lstm_fitted), i).mean() for i in range(lstm_fitted.shape[1]+n_steps_in - 1, -lstm_fitted.shape[0], -1)]\r\nforecasts1 = lstm_predictions[n_steps_out-1:,0]\r\nfits1 = model.predict(X)[:train_size-n_steps_in,0]\r\nforecasts12 = lstm_predictions[:,n_steps_out-1]\r\nfits12 = lstm_fitted[:,n_steps_out-1]\r\n\r\n# Metric\r\nav_mape = mean_absolute_percentage_error(y_test, forecasts)\r\nav_r2 = r2_score(ts_data[n_steps_in:train_size], fits[n_steps_in:])\r\none_mape = mean_absolute_percentage_error(y_test[:-n_steps_out+1], forecasts1)\r\none_r2 = r2_score(ts_data[n_steps_in:train_size], fits1)\r\ntwelve_mape = mean_absolute_percentage_error(y_test, forecasts12)\r\ntwelve_r2 = r2_score(ts_data[n_steps_in+n_steps_out-1:train_size], fits12)\r\n\r\ndate_range = pd.date_range(start='1990-01-01', end='2023-09-30', freq='M')\r\n\r\n# 실제, 적합 결과 및 예측을 플롯\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(date_range, ts_data, label='Actual', color='blue')\r\nplt.plot(date_range[:train_size], fits, label='Fitted', color='green')\r\nplt.plot(date_range[train_size:], forecasts, label='Forecast', color='red')\r\nplt.title('FSC - Short - Passengers\\n. LSTM 12 Month Average Forecast')\r\nplt.xlabel('Date')\r\nplt.ylabel('Passengers')\r\nplt.legend()\r\nplt.text(0.05, 0.05, f'R2 = {av_r2*100:.2f}%\\nMAPE = {av_mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12)\r\nplt.grid(True)\r\nplt.show()\r\n\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(date_range, ts_data, label='Actual', color='blue')\r\nplt.plot(date_range[n_steps_in:train_size], fits1, label='Fitted', color='green')\r\nplt.plot(date_range[train_size:-n_steps_out+1], forecasts1, label='Forecast', color='red')\r\nplt.title('FSC - Short - Passengers\\n LSTM 1 Month in advance Forecast')\r\nplt.xlabel('Date')\r\nplt.ylabel('Passengers')\r\nplt.legend()\r\nplt.text(0.05, 0.05, f'R2 = {one_r2*100:.2f}%\\nMAPE = {one_mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12)\r\nplt.grid(True)\r\nplt.show()\r\n\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(date_range, ts_data, label='Actual', color='blue')\r\nplt.plot(date_range[n_steps_in+n_steps_out-1:train_size], fits12, label='Fitted', color='green')\r\nplt.plot(date_range[train_size:], forecasts12, label='Forecast', color='red')\r\nplt.title('FSC - Short - Passengers\\n LSTM 12 Months in advance Forecast')\r\nplt.xlabel('Date')\r\nplt.ylabel('Passengers')\r\nplt.legend()\r\nplt.text(0.05, 0.05, f'R2 = {twelve_r2*100:.2f}%\\nMAPE = {twelve_mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12)\r\nplt.grid(True)\r\nplt.show()\r\n```\r\n\r\n이슈: 창 메서드와 동일한 문제가 여기에도 있습니다:\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n![이미지](/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_8.png)\r\n\r\n해상도: 창 법과 비슷한 방법을 사용할 수 있습니다. 그러나 n_steps_out을 test_size와 동일하게 선택할 수도 있습니다. 이렇게 하면 테스트 세트가 하나로 축소됩니다:\r\n\r\n![이미지](/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_9.png)\r\n\r\n다음 함수는 이를 정확히 수행합니다. 이 함수는 시계열, 학습 크기 및 샘플 수를 사용합니다. 이 버전은 다른 예측 알고리즘과 비교할 수 있기 때문에 comparable로 이름 지었습니다:\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n```js\r\ndef split_sequences_comparable(sequences, n_samples, train_size):\r\n # 단계\r\n n_steps_out = len(sequences) - train_size\r\n n_steps_in = train_size - n_steps_out - n_samples + 1\r\n # 끝 세트\r\n X_test = sequences[n_samples + n_steps_out - 1:train_size]\r\n X_forecast = sequences[-n_steps_in:]\r\n X, y = list(), list()\r\n for i in range(n_samples):\r\n     # 이 패턴의 끝을 찾습니다\r\n     end_ix = i + n_steps_in\r\n     out_end_ix = end_ix + n_steps_out\r\n     # 패턴의 입력 및 출력 부분을 수집합니다\r\n     seq_x, seq_y = sequences[i:end_ix], sequences[end_ix:out_end_ix]\r\n     X.append(seq_x)\r\n     y.append(seq_y)\r\n return np.array(X), np.array(y), np.array(X_test), np.array(X_forecast), n_steps_in, n_steps_out\r\n```\r\n\r\n이 함수에서는 단계 수가 이미 고정되었기 때문에 샘플 수와 훈련 크기는 사용자가 선택하도록 하고, 최대 가능한 단계 수를 계산하도록 결정했습니다. 아래는 실행된 코드와 그 결과입니다:\r\n\r\n```js\r\nn_samples = 12\r\ntrain_size = 321\r\nX_train, y_train, X_test, X_forecast, n_steps_in, n_steps_out = split_sequences_comparable(ts_data, n_samples, train_size)\r\ny_test = ts_data[train_size:]\r\n\r\n# Reshaping\r\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\r\nX_test = X_test.reshape(X_test.shape[1], X_test.shape[0], 1)\r\ny_train = y_train.reshape(y_train.shape[0], y_train.shape[1])\r\ny_test = y_test.reshape(y_test.shape[1], y_test.shape[0], 1)\r\n\r\n# LSTM 모델 생성 및 훈련\r\nmodel = Sequential()\r\nmodel.add(LSTM(units=154, activation='tanh', input_shape=(n_steps_in, 1)))\r\nmodel.add(Dense(units=n_steps_out))\r\nmodel.compile(loss='mean_squared_error', optimizer='Adam', metrics=['mape'])\r\n\r\nmodel.fit(x=X_train, y=y_train, epochs=500, batch_size=18, verbose=2)\r\n\r\n# 예측\r\nlstm_predictions = model.predict(X_test)\r\npredictions = lstm_predictions.reshape(lstm_predictions.shape[1])\r\nlstm_fitted = model.predict(X_train)\r\nfits = [np.diag(np.fliplr(lstm_fitted), i).mean() for i in range(lstm_fitted.shape[1]+n_steps_in - 1, -lstm_fitted.shape[0], -1)]\r\n\r\n# 메트릭스\r\nmape = mean_absolute_percentage_error(y_test, predictions)\r\nr2 = r2_score(ts_data[n_steps_in:train_size], fits[n_steps_in:])\r\n\r\n# 실제, 적합 및 예측 플롯\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(date_range, ts_data, label='Actual', color='blue')\r\nplt.plot(date_range[:train_size], fits, label='Fitted', color='green')\r\nplt.plot(date_range[train_size:], predictions, label='Forecast', color='red')\r\nplt.title('FSC - Short - Passengers\\n12 Sample Comparable LSTM Forecast')\r\nplt.xlabel('Date')\r\nplt.ylabel('Passengers')\r\nplt.legend()\r\nplt.text(0.05, 0.05, f'R2 = {r2*100:.2f}%\\nMAPE = {mape*100:.2f}%\\', transform=plt.gca().transAxes, fontsize=12)\r\nplt.grid(True)\r\nplt.show()\r\n```\r\n\r\n\u003cimg src=\"/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_10.png\" /\u003e\r\n\r\n\u003c!-- ui-station 사각형 --\u003e\r\n\r\n\u003cins class=\"adsbygoogle\"\r\nstyle=\"display:block\"\r\ndata-ad-client=\"ca-pub-4877378276818686\"\r\ndata-ad-slot=\"7249294152\"\r\ndata-ad-format=\"auto\"\r\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\r\n\r\n\u003cscript\u003e\r\n(adsbygoogle = window.adsbygoogle || []).push({});\r\n\u003c/script\u003e\r\n\r\n지금까지 우리가 얻은 결과는 가장 신뢰할만한 것입니다. 그러나 제가 개발한 혁신적인 방법을 사용하면 더 나은 결과를 얻을 수 있습니다. 이 방법은 나중에 시리즈에서 (순환 방법) 자세히 설명하겠습니다. 먼저 LSTM 네트워크의 하이퍼파라미터를 조정하는 방법에 대해 설명하겠습니다.\r\n\r\nLSTM은 모든 시간 단계를 특성으로 집계하기 때문에 시계열 데이터가 모든 이러한 방법에서 손실됩니다. 나중에 시리즈에서 (인코더/디코더 방법) 시계열 입력의 구조를 유지하는 다른 방법을 사용할 것입니다.\r\n\r\n계속 주목해 주세요!\r\n","ogImage":{"url":"/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_0.png"},"coverImage":"/assets/img/2024-05-18-HowtofixacommonmistakeinLSTMtimeseriesforecasting_0.png","tag":["Tech"],"readingTime":15},{"title":"기계 학습을 배우는 용기 사라지는 그래디언트와 폭주하는 그래디언트에 대처하기 파트 2","description":"","date":"2024-05-18 19:28","slug":"2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2","content":"\n\"“Courage to Learn ML”의 새로운 장을 찾아주신 여러분, 환영합니다. 이 시리즈는 복잡한 주제들을 쉽고 재미있게 다루고, 멘토와 학습자 간의 캐주얼 대화처럼 친밀한 분위기를 제공하기 위해 만들어졌습니다. “용기로 방어하다”의 쓰기 스타일에서 영감을 받아 기계 학습에 특히 집중하고 있어요.\n\n이번 시간에는 사라지는 그래디언트와 폭발하는 그래디언트의 어려움을 극복하는 방법에 대해 계속해서 탐구할 거예요. 첫 번째 세그먼트에서 우리는 네트워크 내에서 효율적인 학습을 보장하기 위해 안정적인 그래디언트 유지가 왜 중요한지에 대해 이야기했어요. 불안정한 그래디언트가 우리 네트워크의 심화를 방해할 수 있고 결국 깊은 \"학습\"의 잠재력을 제한할 수 있다는 것을 밝혀냈죠. 이러한 개념을 살려내기 위해 DNN(맛있고 영양가 있는 얹힌 작은 얼음 공장)이라는 소형 아이스크림 공장을 운영하는 비유를 사용하고 수렴한 팩토리 생산 라인을 조율하는 것과 유사한 DNN 훈련을 위한 강력한 전략을 명료하게 보여줬어요.\n\n이제, 두 번째 이야기에서는 각 제안된 솔루션에 대해 더 심층적으로 탐구하며, 아이스크림 공장을 활기차게 만든 것과 같은 명확성과 창의성으로 그들을 살펴볼 거에요. 여기 이번 부분에서 다룰 주제 목록입니다:\n\n- 활성화 함수\n- 가중치 초기화\n- 배치 정규화\n- 실제 적용(개인 경험)\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 활성화 함수\n\n활성화 함수는 우리 \"공장\" 설정의 핵심입니다. 이 함수들은 우리의 DNN 조립 라인 내에서 전방 및 후방 전파를 통해 정보를 전달하는 역할을 합니다. 적절한 활성화 함수를 선택하는 것은 우리의 DNN 조립 라인 및 이에 따라 우리의 DNN 훈련 과정이 원활하게 작동하는 데 중요합니다. 이 부분은 활성화 함수의 장닿과 단점을 간단히 설명하는 것이 아닙니다. 여기서는 다양한 활성화 함수의 생성 배경을 파악하고 종종 간과되는 중요한 질문에 대답하기 위해 Q\u0026A 형식을 사용할 것입니다.\n\n이러한 함수들을 우리 아이스크림 생산 비유의 블렌더로 생각해보세요. 이용 가능한 블렌더 목록을 제공하는 대신, 각각의 혁신과 특정 개선 사항 뒤에 있는 이유를 심층적으로 검토하고 이해하는 데 도움을 드리겠습니다.\n\n## 활성화 함수란 무엇이며, 어떻게 적절한 함수를 선택할 수 있을까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Activation functions](/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png)\n\n활성화 함수는 신경망 모델에 선형 및 비선형 관계를 모두 포착할 수 있는 유연성과 강력함을 부여하는 주요 요소입니다. 로지스틱 회귀와 DNN의 주요 차이점은 이러한 활성화 함수들과 여러 층을 결합하는 데 있습니다. 이들은 NN이 다양한 함수를 근사할 수 있게 합니다. 그러나 이러한 능력은 도전과제와 함께 제공됩니다. 활성화 함수 선택에는 더 주의를 기울여야 합니다. 잘못된 선택은 모델이 특히 역전파 중에 효과적으로 학습하는 것을 막을 수 있습니다.\n\n당신이 당사 DNN 아이스크림 공장의 매니저로 상상해보세요. 당신은 생산 라인을 위해 적절한 활성화 함수(아이스크림 믹서로 생각해보세요)를 섬세하게 선택하고 싶을 것입니다. 즉, 당신의 요구 사항에 가장 적합한 것을 찾는 데 신중을 기울이고 최적의 선택지를 찾아내야 합니다.\n\n따라서 효과적인 활성화 함수를 선택하는 첫 번째 단계는 두 가지 핵심 질문에 대한 대답을 찾는 것입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 활성화 함수의 선택이 소멸 그래디언트나 폭발 그래디언트와 같은 문제에 어떤 영향을 미치나요? 어떤 기준이 좋은 활성화 함수를 정의하나요?\n\n은닉층에서 활성화 함수를 선택할 때, 주로 소멸 그래디언트와 관련된 문제가 발생합니다. 이는 전통적인 시그모이드 활성화 함수(가장 전통적이고 기본적인 모델)로 거슬러 올라갈 수 있습니다. 시그모이드 함수는 입력값을 확률 범위(0부터 1)에 매핑할 수 있는 능력으로 널리 사용되었습니다. 이는 이진 분류 작업에서 특히 유용합니다. 이 능력 덕분에 연구자들은 예측을 분류하기 위한 확률 임계값을 조정하여 모델의 유연성과 성능을 향상할 수 있었습니다.\n\n그러나 이를 은닉층에 적용하는 것은 주로 소멸 그래디언트 문제를 야기했습니다. 이는 주로 두 가지 주요 요인으로 설명할 수 있습니다:\n\n- 순방향 전파 과정에서 시그모이드 함수는 입력을 0과 1 사이의 매우 좁은 범위로 압축합니다. 한 네트워크가 은닉층에서 활성화 함수로 시그모이드만 사용하는 경우, 여러 층을 거칠수록 이 범위가 더욱 좁아지게 됩니다. 이 압축 효과로 인해 출력의 변동성이 감소하고 양수 값으로의 편향이 도입됩니다. 입력 부호에 관계없이 출력은 0과 1 사이에 유지되기 때문에.\n- 역전파 과정에서 시그모이드 함수의 도함수(종모양 곡선)는 0과 0.25 사이의 값을 생성합니다. 이 작은 범위는 입력을 가로지르는 그래디언트가 여러 층을 통과함에 따라 급속하게 감소할 수 있도록 할 수 있습니다. 이것은 앞선 층 그래디언트가 연속된층의 도함수의 곱으로 이루어지기 때문인데, 이러한 낮은 도함수의 복합 곱은 점점 더 작은 그래디언트를 결과로 가져와서 초기 층에서의 효과적인 학습을 방해합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이러한 제약 사항을 극복하기 위해 이상적인 활성화 함수는 다음과 같은 특성을 보여야 합니다:\n\n- 비선형성. 네트워크가 복잡한 패턴을 포착할 수 있도록 함.\n- 비포화. 함수와 그 도함수가 입력 범위를 과도하게 압축하지 않아서 gradient 소멸을 방지해야 함.\n- 중심이 0인 출력. 함수는 양수 및 음수 출력 둘 다를 허용해야 하며, 각 노드 사이의 평균 출력이 특정 방향으로의 편향을 도입하지 않도록 해야 함.\n- 계산 효율성. 함수와 그 도함수가 계산적으로 간단하여 효율적인 학습을 용이하게 해야 함.\n\n## 이러한 기본 특성들을 고려할 때, 인기있는 활성화 함수들이 기본 모델인 Sigmoid를 어떻게 개선하고 뛰어나게 만드는지 알아봅시다.\n\n이 섹션은 거의 모든 현재 활성화 함수에 대한 일반적인 개요를 제공하려고 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n테이블 태그를 마크다운 형식으로 변경하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nReLU의 고려 사항 중 하나는 선형 세그먼트 간의 급격한 전환으로 인해 x=0에서 미분 불가능하다는 것입니다. 실제로 PyTorch와 같은 프레임워크는 subgradient 개념을 사용하여 이를 해결하며, 종종 x=0에서 도함수를 0.5 또는 [0, 1] 내의 다른 값으로 설정합니다. 이는 보통 정확한 제로 입력이 드물고 데이터의 변동성 때문에 문제가 되지 않습니다.\n\n그래서, ReLU가 여러분에게 적합한 선택일까요? 많은 연구자들은 그렇다고 말합니다. 이는 그 간결함, 효율성 및 주요 DNN 프레임워크의 지원 덕분입니다. 게다가 https://arxiv.org/abs/2310.04564 같은 최근 연구들이 ReLU의 계속된 중요성을 강조하며, ML 분야에서의 부활과 같은 시대를 맞이한다고 강조하고 있습니다.\n\nLeaky ReLUs는 클래식적인 ReLU에 약간의 변화를 준겳이며, ReLU를 더 자세히 살펴보면 몇 가지 문제점이 드러납니다. 음수 입력에 대한 제로 출력으로 이어지는 것은 'dying ReLU' 문제로 이어지며, 뉴런들이 훈련 중 업데이트되지 않게 됩니다. 또한, ReLU는 양수 값을 선호하여 모델에 방향성 편향을 도입할 수 있습니다. 이러한 단점을 극복하면서 ReLU의 이점을 유지하기 위해, 여러 연구자들이 'leaky' ReLU와 같은 여러 변형을 개발했습니다.\n\nLeaky ReLU는 ReLU의 음수 부분을 수정하여 작고 0이 아닌 기울기를 부여합니다. 이 조정은 음수 입력이 작은 음수 출력을 생성하도록하며, 효과적으로 그 외의 0 출력 영역을 '누출'시킵니다. 이 누출의 기울기는 하이퍼파라미터 알파(α)에 의해 제어되며, 전형적으로 뉴런을 활성 유지와 희소성 사이의 균형을 유지하기 위해 0에 가깝게 설정됩니다. 작은 음수 출력을 허용함으로써, Leaky ReLU는 활성 함수의 출력을 0 주변으로 중앙 집중시키고 뉴런이 비활성화되지 않게 하여 'dying ReLU' 문제에 대응합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 하이퍼파라미터로 α를 도입하면 모델 튜닝에 대한 복잡성이 추가됩니다. 이를 관리하기 위해 원본 Leaky ReLU의 변형이 개발되었습니다:\n\n- Randomized Leaky ReLU (RReLU): 이 버전은 훈련 중에 α를 지정된 범위 내에서 무작위로 지정하고 평가 중에는 고정합니다. 무작위성은 모델을 정규화하고 과적합을 방지하는 데 도움이 될 수 있습니다.\n- Parametric Leaky ReLU (PReLU): PReLU는 훈련 중에 α를 학습할 수 있도록 하며, 활성화 함수를 데이터셋의 특정 요구에 맞게 조정할 수 있습니다. 이는 α를 훈련 데이터에 맞게 조정하여 모델 성능을 향상시킬 수 있지만, 과적합의 위험도 증가시킵니다.\n\nLeaky ReLU를 개선한 Exponential Linear Unit (ELU). Leaky ReLU와 ELU는 음의 값을 허용하여 평균 유닛 활성화를 제로에 가깝게 밀어내고 활성화 함수의 활력을 유지하는 데 도움이 됩니다. Leaky ReLU의 문제점은 이 음의 값의 범위를 조절할 수 없다는 것입니다. 이론적으로 이 값들은 작게 유지하려는 의도에도 불구하고 음의 무한대로 확장될 수 있습니다. ELU는 이를 해결하기 위해 비선형 지수 곡선을 비정상적인 입력에 통합하여 음의 출력 범위를 최대 -𝛼(일반적으로 1로 설정되는 새로운 하이퍼파라미터)로 좁히고 제어합니다. 또한 ELU는 매끄러운 함수입니다. 그 지수 요소 덕분에 음과 양 값 사이에서 매끄러운 전환을 가능하게 하며, 입력 값에 대한 잘 정의된 기울기를 보장하여 기울기 기반 최적화에 유리합니다. 이 기능은 ReLU와 Leaky ReLU에서 보이는 미분 불가능 문제를 해결합니다.\n\nSelf-Normalizing 속성을 갖춘 향상된 ELU인 Scaled Exponential Linear Unit (SELU). SELU는 신경망 내에서 제로 평균 및 단위 분산을 유지하도록 설계된 ELU의 확장된 버전입니다. 양의 순입력의 기울기가 1을 초과하도록 고정 스케일 요인 λ(1보다 큰 값)을 통합함으로써 SELU는 하위 레이어의 기울기가 줄어드는 상황에서 기울기를 증폭하여 딥 뉴럴 네트워크에서 자주 발생하는 소멸하는 기울기 문제를 예방하는 데 특히 유용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSELU에 대해, 매개변수(α 및 λ)는 고정된 값이며 학습할 수 없으므로 조정해야 할 매개변수가 적어 튜닝 과정이 간소화됩니다. SELU 구현에서 이러한 특정 값들을 찾을 수 있습니다.\n\nSELU는 실제로 활성화 함수 세계에서 정교한 \"믹서\"인데요, 특정 요구 사항이 딸려옵니다. 단방향 또는 순차 네트워크에서 가장 효과적이며 RNN, LSTM 또는 건너뛰기 연결을 갖는 아키텍처에서는 그 설계 때문에 그런만큼 성능이 좋지 않을 수 있습니다.\n\nSELU의 자기 정규화 기능을 위해서는 입력 피처가 표준화되어야 합니다. 평균이 0이고 표준 편차가 1인 것이 중요합니다. 또한, 매 숨겨진 레이어의 가중치는 LeCun 정규 초기화를 사용하여 초기화되어야 합니다. 여기서 가중치는 평균이 0이고 분산이 1/fan_in인 정규 분포에서 샘플링됩니다. \"fan_in\"이란 용어가 익숙하지 않다면, 가중치 초기화에 대한 전용 세션에서 설명하겠습니다.\n\n요약하면 SELU의 자기 정규화가 효과적으로 기능하려면 입력 피처가 정규화되고 네트워크 구조가 끊기지 않는 것을 보장해야 합니다. 이 일관성은 네트워크 전체에서 자기 정규화 효과가 유지되도록 도와주며 누출 없이 계속 유지되도록 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nGELU (Gaussian Error Linear Unit)은 Dropout으로부터 규제 아이디어를 통합한 혁신적인 활성화 함수입니다. 기존 ReLU가 음수 입력에 대해 0을 출력하는 반면, leaky ReLU, ELU 및 SELU는 음수 출력을 허용합니다. 이를 통해 활성화의 평균을 0에 가깝게 이동시켜 편향을 줄이는데 도움을 줍니다. 이는 ReLU와 비슷한 방식으로 편향을 줄이지만 음수 입력을 완전히 0으로 만들지 않고 음의 값을 허용한다는 것을 의미합니다. 그러나 이러한 누출은 \"죽어 가는 ReLU\"의 일부 이점을 잃어버릴 수 있음을 의미합니다. 여기서는 일부 뉴런의 비활성으로 더 sparse하고 일반화된 모델을 얻을 수 있습니다.\n\n죽어 가는 ReLU 및 Dropout의 희소성 이점을 고려할 때, GELU는 여기에 한 발 더 나아간 것입니다. GELU는 0 출력의 특성을 가진 죽어 가는 ReLU를 무작위적인 요소와 결합하여 뉴런이 재활성화될 수 있는 가능성을 열어줍니다. 이 접근은 유익한 희소성을 유지하는 것뿐만 아니라 뉴런 활동을 재도입하여 GELU를 견고한 해결책으로 만듭니다. 이 메커니즘을 완전히 이해하기 위해 GELU의 정의를 자세히 살펴보겠습니다:\n\n![이미지](/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_1.png)\n\nGELU 활성화 함수에서 CDF인 Φ(x) 또는 표준 가우스 누적 분포 함수가 중요한 역할을 합니다. 이 함수는 표준 정규 분포를 따를 때 x보다 작거나 같은 값을 갖는 것으로 나타내는 확률을 나타냅니다. Φ(x)는 음수 입력에 대해 0부터 양수 입력에 대해 1로 매끄럽게 전환되어, 입력의 스케일링을 효과적으로 제어합니다. Dan Hendrycks 외(출처)의 논문에 따르면 뉴런 입력은 배치 정규화를 사용할 때 특히 정규 분포를 따르는 경향이 있어 정규 분포의 사용이 정당화됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n해당 함수의 디자인은 x 값이 줄어들수록 입력이 더 자주 \"떨어지도록\" 허용하여 변환을 확률적이면서 입력 값에 의존적으로 만듭니다. 이 메커니즘은 흔히 사용되는 직선 함수인 f(x) = x를 더 부드럽게 만들어 ReLU 함수와 유사한 형태를 유지하며, 조각별 선형 함수에서 발생하는 갑작스러운 변화를 피합니다. GELU의 가장 중요한 특징 중 하나는 뉴런을 완전히 비활성화할 수 있다는 것으로, 이를 통해 입력 값의 변화에 따라 다시 활성화될 수 있습니다. 이러한 확률적 성질은 입력 값에 의존하지만 완전히 무작위적이지 않아 뉴런이 다시 활성화될 기회를 제공합니다.\n\n아래는 GELU가 ReLU보다 두드러지는 이점이라고 요약할 수 있습니다. GELU는 어떤 입력 값이 양수인지 음수인지에 관계없이 전체 입력 값 범위를 고려합니다. Φ(x) 값이 감소함에 따라 GELU 함수의 출력이 0에 가까워지는 확률이 증가하여 뉴런을 부드럽게 \"떨어뜨리게\" 됩니다. 이 방법은 전형적인 드롭아웃 방식보다 더 정교하며, 무작위적으로 하는 것이 아니라 데이터에 따라 뉴런의 비활성화를 결정하도록 되어 있습니다. 이 방식은 매우 매력적으로 느껴지며, 마치 고급 디저트에 부드러운 크림을 추가하여 조금 더 향상된 미각을 경험하는 것과 같다고 생각합니다.\n\nGELU는 GPT-3, BERT 및 다른 Transformers와 같은 모델에서 효율적이며 언어 처리 작업에서 강력한 성능을 보여 인기 있는 활성화 함수가 되었습니다. 확률적 성질 때문에 계산 위주이지만, 표준 가우스 누적 분포인 Φ(x)의 곡선은 시그모이드와 tanh 함수와 유사합니다. 흥미로운 점은 GELU가 tanh를 사용하거나 x(1.702\\*x) 공식을 사용하여 근사할 수 있다는 것입니다. 이러한 단순화 가능성에도 불구하고, PyTorch의 GELU 구현은 그러한 근사가 종종 불필요할 정도로 빠르게 진행됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n더 깊이 들어가기 전에 먼저 정리해보자면,\n\n## ReLU를 검토하고 이로부터 영감받은 다른 활성화 함수가 무엇인지를 살펴보면 어떤 활성화 함수가 좋을지 정확히 알아볼까요?\n\nGünter Klambauer 등의 논문에서 SELU가 소개된 적이 있습니다. 여기서는 효과적인 활성화 함수의 중요한 특성을 강조했는데요.\n\n- 범위: 네트워크 전체의 평균 활성화 수준을 조절하는 데 도움이 되기 위해 음수와 양수 값을 출력해야 합니다.\n- 포화 영역: 도함수가 제로에 가까워지는 영역으로, 하위층의 너무 높은 분산을 안정화하는 데 도움을 줍니다.\n- 증폭 슬로프: 하위 층에서 너무 낮은 분산을 높이기 위해 중요한 기울기가 있어야 합니다.\n- 연속성: 연속적인 곡선은 분산의 변화를 안정화하고 증가시키는 효과를 균형있게 유지하는 고정점을 보장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 \"이상적인\" 활성화 함수에 대한 두 가지 추가 기준을 제안하고 싶습니다:\n\n- 비선형성: 이것은 명백하고 필수적입니다. 왜냐하면 선형 함수는 복잡한 패턴을 효과적으로 모델링할 수 없기 때문입니다.\n- 동적 출력: 출력이 제로이고 입력 데이터에 따라 출력을 변경할 수 있는 능력은 동적 뉴런 활성화와 비활성화를 가능하게 합니다. 이렇게 하면 네트워크가 변화하는 데이터 조건에 효율적으로 적응할 수 있습니다.\n\n## 활성화 함수가 음수를 출력하는 이유에 대해 더 직관적인 설명을 부탁드려도 될까요?\n\n활성화 함수를 입력 데이터를 변환하는 블렌더로 생각해 보세요. 일부 재료를 선호하는 블렌더처럼, 활성화 함수는 그들의 본질적인 특성에 따라 편향을 도입할 수 있습니다. 예를 들어, 시그모이드 및 ReLU 함수는 일반적으로 입력과 관계없이 비음수 출력만 나타냅니다. 이는 블렌더가 어떤 재료를 넣어도 항상 동일한 맛을 내는 것과 유사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_3.png)\n\n이 편향을 최소화하려면 부정적이고 긍정적인 값을 출력할 수 있는 활성화 함수를 가지는 것이 좋습니다. 기본적으로 우리는 중심이 제로인 출력을 목표로 합니다. 활성화 함수의 출력을 나타내는 놀이터를 상상해보세요. Sigmoid나 ReLU와 같은 함수로는, 이 놀이터는 부정적인 입력을 무시하거나 제로로 바꾸기 때문에 긍정적인 쪽으로 크게 기울어져 있습니다. Leaky ReLU는 음수 입력이 약간 음수 출력을 생성하도록 허용함으로써 이 놀이터를 균형잡게 시도하지만, 부정적 기울기의 선형 및 상수적 성격 때문에 조정이 미미합니다. 반면에 Exponential Linear Unit (ELU)은 지수 구성 요소로 음수 측면에 더 다이나믹한 밀어넣기를 제공하여, 더 균형 잡힌 상태에 가까워질 수 있도록 돕습니다. 이 균형은 긍정적 및 부정적 업데이트가 훈련에 기여하도록 보장함으로써 신경망에서 건강한 그레이디언트 플로우와 효율적인 학습을 유지하는 데 중요합니다, 단방향 업데이트의 제한을 피하기 위해.\n\n## ReLU와 유사하게 양수 입력을 제로화하는 활성화 함수를 생성할 수 있을까요, min(0, x)를 사용하여 양수 입력을 제로화하는 함수를 선호하는 이유는 무엇인가요?\n\n확실히, ReLU의 양수 값을 제로화하고 음수 값을 그대로 통과시키는 버전을 설계할 수 있습니다. 이것은 기술적으로 실행 가능한데, 중요한 점은 여기서 값의 부호가 아니라 네트워크에 비선형성을 도입하는 것입니다. 이 활성화 함수들이 일반적으로 출력 레이어가 아닌 숨겨진 레이어에서 사용된다는 것을 기억하는 것이 중요합니다. 즉, 이 네트워크 내의 이러한 활성화 함수의 존재는 최종 출력의 부호에 영향을 미치지 않고 이 레이어의 특성에 의해 직접적으로 영향을 받지 않더라도 최종 출력이 여전히 양수와 음수 모두가 될 수 있다는 것을 의미합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n무슨 상황에서든, 네트워크의 가중치와 편향은 출력의 필요한 부호에 맞게 조정될 수 있습니다. 예를 들어, 전통적인 ReLU에서 출력이 1이고 다음 레이어의 가중치가 1이면 출력은 여전히 1로 유지됩니다. 마찬가지로, 제안된 ReLU 변형이 -1을 출력하고 가중치가 -1이면 결과는 여전히 1이 됩니다. 본질적으로, 우리는 출력의 부호보다는 크기에 더 신경을 씁니다.\n\n따라서, ReLU가 부정적인 쪽에서 포화되는 것은 양수 쪽에서 포화되는 것과 근본적으로 다르지 않습니다. 그러나 우리가 영 중심 활성화 함수를 중요시하는 이유는 양수 또는 음수 값에 대한 내재적인 선호도를 방지하여 모델에서 불필요한 편향을 피하기 위한 것입니다. 이 균형은 네트워크 전체에 걸쳐 중립성과 효과적인 학습을 유지하는 데 도움이 됩니다.\n\n## Leaky ReLU와 같은 함수들은 출력을 영 중심 주변에 유지하기 위해 음수값을 출력할 필요가 있습니다. 그렇다면 ELU, SELU, GELU는 왜 음수 입력에 포화되도록 특별히 설계되었을까요?\n\n이를 이해하기 위해, ReLU 뒤에 있는 생물학적 영감을 살펴볼 수 있습니다. ReLU는 생물학적 뉴런을 모방하는데, 이들은 한계값을 가지고 있습니다. 이 한계값을 초과하는 입력은 뉴런을 활성화시키고, 그 이하는 그렇지 않습니다. 활성 및 비활성 상태 간 전환 가능성은 신경 기능에서 중요합니다. ELU, SELU, GELU와 같은 변형을 고려할 때, 이들의 설계가 두 가지 다른 필요에 부합함을 알 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 긍정적 영역: 임계값을 초과하는 신호가 전달될 때 전달되는 원하는 신호를 전송하는 것을 의미합니다.\n- 부정적 영역: 원치 않는 신호를 최소화하거나 걸러내며 대형 부정적 값의 영향을 완화하여 누수하는 게이트처럼 작동합니다.\n\n이러한 기능은 입력에 대한 게이트 역할을 하며, 뉴런의 출력에 영향을 미쳐야 하는 사항과 그렇지 말아야 하는 사항을 관리합니다. 예를 들어, SELU는 다음 두 가지 측면을 구분하여 활용합니다:\n\n- 긍정적 영역: 스케일링 인자 λ (1보다 큼)는 신호를 전달하지 않을 뿐만 아니라 약간 증폭시킵니다. 역전파 중 이 영역의 도함수는 일정하게 유지됩니다 (약 1.0507), 작지만 유용한 기울기를 증가하여 사그라들기 기울기를 희석시키기 위해 사용됩니다.\n- 부정적 영역: 도함수는 0과 λα 사이의 값 사이를 이동합니다 (일반적인 값은 λ ≈ 1.0507 그리고 α ≈ 1.6733), 약 1.7583에 달하는 최대 도함수를 이끌어 냅니다. 여기서 함수는 거의 0에 가깝게 접근하며, 지나치게 큰 기울기를 줄여 폭발 문제를 해결하기 위해 돕습니다.\n\n이 설계는 이러한 활성화 함수들이 유용한 신호를 증가시키면서 잠재적으로 유해한 극단을 억제해 안정적인 학습 환경을 제공할 수 있도록 균형을 맞춘다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n활성화 함수가 게이트로 작용하는 개념은 새로운 아이디어가 아닙니다. 시그모이드 함수가 무엇을 기억하거나 업데이트하거나 잊어버릴지를 결정하는 LSTM과 같은 구조에서 강력한 전례가 있습니다. 이 게이팅 개념은 ReLU의 변형이 특정한 방식으로 설계된 이유를 이해하는 데 도움이 됩니다. 예를 들어 GELU는 표준 정규 분포의 누적 분포 함수(CDF)에서 유도된 스케일 계수를 사용하는 동적 게이트 역할을 합니다. 이 스케일링을 통해 입력의 작은 부분이 0에 가까울 때 통과되도록 하고, 더 큰 양수 값은 대부분 변경되지 않고 통과할 수 있게 합니다. 입력이 다음 레이어에 얼마나 많은 영향을 미치는지 제어함으로써, GELU는 정보 흐름의 효과적인 관리를 용이하게 해주며, 특히 transformer와 같은 구조에서 유용합니다.\n\n언급된 ELU, SELU, 그리고 GELU 모두 음수 측면을 부드럽게 만듭니다. 음수 입력의 부드러운 포화는 큰 음수 값의 영향을 완화하는 것뿐만 아니라, 네트워크가 입력 데이터의 변동에 덜 민감해지도록 만듭니다. 이를 통해 더 안정적인 특징 표현이 이뤄지게 됩니다.\n\n요약하면, 양수인지 음수인지에 상관없이 포화 영역이 구체적으로 중요하지 않습니다. 왜냐하면 이러한 활성화 함수들은 네트워크의 중간 레이어에서 작동하며, 여기서 가중치와 편향이 적절하게 조정될 수 있습니다. 하지만, 한쪽이 신호를 변경하지 않고 전달하거나 심지어 증폭할 수 있도록 허용하는 이러한 함수의 설계가 중요합니다. 이러한 배치는 신호를 조직화하고 효과적인 역전파를 용이하게 도와 전체 네트워크의 성능과 학습 안정성을 향상시킵니다.\n\n## 언제 각 활성화 함수를 선택해야 할까요? 왜 ReLU가 여전히 실무에서 가장 인기 있는 활성화 함수인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n올바른 활성화 함수를 선택하는 데는 계산 리소스, 네트워크 아키텍처의 특정 요구 사항 및 이전 모델로부터의 경험적 증거 등 여러 요소가 관련됩니다.\n\n- 계산 리소스: 충분한 계산 리소스가 있다면 교차 검증을 사용하여 다양한 활성화 함수를 실험해 보는 것이 유익할 수 있습니다. 이를 통해 모델과 데이터셋에 특화된 활성화 함수를 만들 수 있습니다. SELU를 사용할 때 배치 정규화가 필요 없는 경우가 대부분이며, 이는 다른 함수들과 달리 배치 정규화가 필요하지 않아 아키텍처를 간단하게 만들어 줍니다.\n- 경험적 증거: 특정 응용 프로그램에는 특정 함수가 표준으로 사용될 수 있습니다. 예를 들어, 트랜스포머 모델을 훈련시키기 위해 GELU를 선호하는 경우가 많은데, 이는 해당 아키텍처에서 효과적이기 때문입니다. SELU는 자기 정규화 특성과 조절해야 할 하이퍼파라미터가 없다는 장점으로, 훈련 안정성이 핵심인 깊은 네트워크에 특히 유용합니다.\n- 계산 효율성과 간결성: 계산 효율성과 간결성이 중요한 경우, ReLU 및 PReLU, ELU와 같은 변형들이 우수한 선택지입니다. 이들은 매개변수 조정의 필요성을 피하고 모델의 희소성 및 일반화를 지원하여 과적합을 줄이는 데 도움을 줍니다.\n\n더 정교한 함수가 등장했지만, ReLU는 여전히 간결하고 효율적이어서 매우 인기가 있습니다. 구현이 간단하고 이해하기 쉬우며 계산을 복잡하게 하지 않고 비선형성을 소개하는 명확한 방법을 제공합니다. 음수 부분을 제로 처리하는 함수의 능력으로 계산을 단순화하고 계산 속도를 향상시키므로, 특히 대규모 네트워크에서 매우 유리합니다.\n\nReLU의 설계는 음수 활성화를 제로처리하여 모델의 희소성을 기본적으로 증가시키며, 이는 일반화를 개선할 수 있습니다 — 훈련 중심의 과적합이 심각한 문제인 딥 뉴럴 네트워크에서 매우 중요한 요소입니다. 게다가 ReLU는 추가적인 하이퍼파라미터가 필요 없으며, PReLU나 ELU와 같은 함수와 달리 모델 훈련에 추가 복잡성을 도입하지 않습니다. 또한 ReLU가 널리 채택된 상태이므로, 많은 머신러닝 프레임워크와 라이브러리가 이를 위해 특화된 최적화를 제공하여, 많은 개발자에게 실용적인 선택이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요약하자면, 새로운 활성화 함수는 특정 시나리오에 특정 이점을 제공하지만, ReLU의 간단함, 효율성, 효과적인 측면의 균형은 많은 응용 프로그램에서 선호하는 선택지가 되고 있습니다. 어떤 활성화 함수를 선택한다 하더라도, 그 특성을 철저히 이해하는 것이 중요하며 모델의 요구 사항과 일치하고 모델 훈련 중 문제 해결을 용이하게 하는 데 필수적입니다.\n\n# 가중치 초기화\n\n그래, 우리는 기욁할 기울기를 안정화시킬 완벽한 활성화 함수를 찾으려는 것을 그만두고, 가중치를 효율적으로 초기화하여 우리의 신경망을 올바르게 설정하는 다른 중요한 측면에 초점을 맞출 시간입니다.\n\n가중치 초기화에 대한 가장 인기 있는 방법들에 대해 자세히 살펴보기 전에, 기본적인 질문을 하나 다루어 보겠습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 가중치 초기화는 왜 중요하며 불안정한 그래디언트를 완화하는 데 어떻게 도움이 될까요?\n\n적절한 가중치 초기화는 모델 전체를 따라 정확하게 그래디언트가 흐를 수 있도록 보장합니다. 이는 아이스크림 공장에서 반제품이 전달되는 방식과 유사합니다. 초기 기계 설정이 올바른 것만 중요한 것이 아니라 각 부서가 효율적으로 작동하는 것도 중요합니다.\n\n가중치 초기화는 네트워크를 통해 전진 및 역방향으로 정보가 안정적으로 흐를 수 있도록 목표를 합니다. 너무 크거나 너무 작은 가중치는 문제를 일으킬 수 있습니다. 지나치게 큰 가중치는 전진 패스 중 출력을 지나치게 증가시켜 예측을 과대추정하게 할 수 있습니다. 반면 아주 작은 가중치는 출력을 지나치게 줄일 수 있습니다. 이러한 가중치의 크기는 역전파 중에 중요해집니다. 가중치가 너무 크면 그래디언트가 폭발할 수 있고, 너무 작으면 그래디언트가 사라질 수 있습니다. 이를 이해하여 우리는 출력 및 그래디언트를 무효화하는 영옵션 (zero)과 지나치게 높은 값과 같은 극단적인 초기화를 피합니다. 이 균형 잡힌 접근법은 네트워크의 효과성을 유지하고 불안정한 그래디언트와 관련된 문제를 방지하는 데 도움이 됩니다.\n\n## 가중치를 초기화하는 좋은 방법은 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가장 중요한 것은 최적 가중치 초기화는 이미 학습된 가중치를 사용하는 것이 가장 좋습니다. 이미 일부 학습을 거친 가중치를 얻을 수 있다면 손실을 최소화하는 방향으로 진행 중인 이 가중치를 계속 사용하는 것이 이상적입니다.\n\n그러나 처음부터 시작하는 경우 가중치를 초기화하는 방법을 신중하게 고려해야 합니다, 특히 불안정한 기울기를 방지하기 위해. 좋은 가중치 초기화에는 다음을 목표로 하는 것이 중요합니다:\n\n- 극단적인 값은 피해야 합니다. 이전에 논의했던 대로, 가중치는 너무 크거나 작지 않고 0도 아니어야 합니다. 적절히 조절된 가중치는 네트워크 훈련의 전진 및 역진행 중 안정성을 유지하는 데 도움이 됩니다.\n- 대칭을 깨야 합니다. 가중치가 다양한 행동을 하도록 하는 것은 매우 중요합니다. 이렇게 하면 뉴런이 서로 거울에 비친 행동을 하지 않고 동일한 특성만 학습하게 되는 것을 방지합니다. 이러한 차별이 없으면 네트워크가 복잡한 패턴을 모델링하는 능력이 심각하게 제한될 수 있습니다. 각각의 다른 초기 가중치가 각 뉴런이 데이터의 다른 측면을 학습하기 시작하도록 도와줍니다. 이는 아이스크림 공장의 다양한 종류의 생산 라인을 가지고 다양한 맛을 생산할 수 있는 범위를 확대하는 것과 비슷합니다.\n- 손실 표면에서 유리한 위치에 가중치를 배치해야 합니다. 초기 가중치는 모델이 글로벌 최솟값으로 향하는 여정을 더 쉽게 만들기 위해 손실 표면에서 양호한 시작 위치에 모델을 위치시켜야 합니다. 손실 랜드스케이프가 어떻게 보이는지 명확한 그림을 가지고 있지 않기 때문에 가중치 초기화에 약간의 무작위성을 도입하는 것이 유익할 수 있습니다.\n\n모든 가중치를 0으로 설정하는 것이 문제가 되는 이유입니다. 이는 모든 뉴런이 동일하게 행동하고 동일한 속도로 학습하기 때문에 대칭 문제를 발생시킵니다. 다양한 패턴을 효과적으로 포착하지 못하게되는 네트워크의 능력을 제한합니다. ReLU 및 그 변형과 함께 0 가중치는 출력이 0이 되어 학습이 멈추고 모든 뉴런이 비활성화되는 결과를 초래합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 모든 가중치를 작은 무작위 숫자로 초기화해야 하지 않을까요?\n\n가중치를 초기화할 때 작은 무작위 숫자를 사용하는 것은 도움이 될 수 있지만, 종종 충분한 제어가 없을 수 있습니다. 무작위로 할당된 가중치는 너무 작을 수 있어서 기욹기 소멸 문제로 이어질 수 있습니다. 이는 훈련 중 업데이트가 무의미하게 작아져 학습 과정이 정체될 수 있습니다. 또한, 완전히 무작위 초기화는 대칭을 깨는 것을 보장하지 않습니다. 예를 들어, 초기화된 값이 너무 유사하거나 모두 같은 부호를 가지는 경우, 뉴런들도 여전히 너무 유사하게 작동하여 데이터의 다양한 측면을 배우지 못할 수 있습니다.\n\n실무에서는 초기화에 대해 더 구조화된 방법을 사용합니다. 유명한 방법에는 Glorot (또는 Xavier) 초기화, He (또는 Kaiming) 초기화, LeCun 초기화 등이 있습니다. 이러한 기술은 일반적으로 정규 분포나 균일 분포를 기반으로 하지만, 이전 및 다음 레이어의 크기를 고려하여 균형을 제공하는 것으로 조절됩니다. 이는 기울기 소실 또는 폭발의 위험이 없이 효과적인 학습을 촉진합니다.\n\n## 그렇다면, 가중치 초기화에 표준 정규 분포(N(0,1))를 사용하지 않는 이유는 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n일반적인 정규 분포(N(0,1))를 사용하면 무작위화 과정을 어느 정도 제어할 수 있지만, 분산을 효과적으로 제어할 수 없어 최적 가중치 초기화에는 부족합니다. 제로 평균은 가중치가 모두 동일한 부호를 공유하지 않도록 보장하여 대칭을 깨는 데 효과적입니다. 그러나 분산이 1인 것은 문제가 될 수 있습니다.\n\n활성화 함수 입력 𝑍이 가중치에 의존하는 시나리오를 고려해 봅시다. 이전 레이어의 𝑁개 뉴런의 출력을 합산하여 계산된다고 가정해보면, 각각의 가중치는 표준 정규 분포에서 초기화됩니다. 여기서 𝑍도 평균이 0인 정규 분포를 따르지만, 분산은 𝑁이 됩니다. 예를 들어 𝑁=100인 경우, 𝑍의 분산은 100이 되어 너무 크기 때문에 활성화 함수로 입력이 불안정하게 전달되어 역전파 과정에서 그래디언트가 불안정해질 수 있습니다. 아이스크림 공장을 비유하면, 각 기계의 설정에서 오차 허용을 높게 설정하는 것은 품질 관리 부재로 인해 원하는 결과와 크게 벗어나는 최종 제품을 만드는 것과 같습니다.\n\n그렇다면 왜 𝑍의 분산에 신경을 쓸까요? 분산은 𝑍 값의 퍼짐을 제어합니다. 분산이 너무 작으면 𝑍의 출력이 충분히 다양하지 않아 대칭을 깨는 데 효과적이지 못할 수 있습니다. 그러나 너무 큰 분산은 값이 너무 높거나 낮아질 수 있습니다. 시그모이드와 같은 활성화 함수의 경우, 극단적으로 높거나 낮은 입력값은 함수의 포화 극으로 출력을 밀어 넣어 그래디언트 소실 문제를 야기할 수 있습니다.\n\n따라서, 분포에서 무작위로 가중치를 초기화할 때 평균과 분산 둘 다 중요합니다. 효과적으로 대칭을 깨기 위해 평균을 0으로 설정하고, 동시에 분산을 최소화하여 중간 제품(즉, 뉴런 출력)이 너무 크거나 작지 않도록 해야 합니다. 올바른 초기화는 네트워크를 통과하는 정보의 안정된 흐름을 보장하고, 전방 및 역방향으로 효율적인 학습 과정을 유지하며, 그래디언트에 불안전성을 도입하지 않습니다. 신중한 초기화 접근은 효과적이고 견고하게 학습하는 네트워크로 이어질 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 그래서, 신경망의 중간 층에서 출력 값을 제어하기 위해 연속된 층에도 입력으로 사용되는 가중치 초기화에 대해 신중히 선택한 평균과 분산을 사용합니다. 그렇다면, 가장 인기 있는 방법들이 어떻게 이 분산을 제어할 수 있는 걸까요?\n\n가중치를 초기화하는 가장 흔한 방법들을 살펴보기 전에, 𝑍Z의 분산은 가중치 초기화의 분산뿐만 아니라 𝑍Z를 계산하는 데 참여하는 뉴런의 수도 영향을 받는다는 점이 중요합니다. 만약 16개의 뉴런만 사용된다면, 𝑍Z의 분산은 16이 되고, 100개의 뉴런이 사용된다면 100이 됩니다. 이 변동은 가중치가 뽑히는 분포만이 아니라 계산에 기여하는 뉴런의 수, 즉 \"팬-인\"이라고도 불리는 요소에 의해 영향을 받습니다. \"팬-인\"은 뉴런으로 들어오는 입력 연결의 수를 의미하며, 비슷하게 \"팬-아웃\"은 뉴런이 가지는 출력 연결의 수를 나타냅니다.\n\n예시를 통해 설명해드리겠습니다: 신경망의 중간 층에 200개의 뉴런이 있고, 이전 층의 100개 뉴런 및 다음 층의 300개 뉴런과 연결되어 있다고 가정해봅시다. 이 경우, 이 층의 팬-인은 100이고, 팬-아웃은 300입니다.\n\n팬-인과 팬-아웃을 이용하면 가중치 초기화 중 분산을 제어할 수 있는 메커니즘을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 팬-인은 전방 전파 중 현재 레이어의 출력 𝑍의 분산을 조절하는 데 도움을 줍니다.\n- 팬-아웃은 역전파 중 후속 레이어의 가중치가 얼마나 영향을 미치는지 조정합니다.\n\n현재 레이어로 전방 및 역방향에서 공급되는 뉴런의 수를 고려하여, 연구자들은 다양한 초기화 방법들을 아이디어 위에 구축해냈습니다. Lecun, Xavier/Glorot 초기화 및 He/Kaiming 초기화가 이러한 방법들 중 일부입니다. 이러한 방법들의 아이디어는 꽤 유사합니다. 가중치를 생성할 때 균일 분포 또는 정규 분포 중 하나를 사용하고, 분산을 조절하기 위해 팬-인 또는 팬-아웃을 사용합니다. 이 분포들의 평균은 모두 0으로 설정하여 출력 값의 평균을 0으로 만듭니다.\n\n```js\n# 초기화의 다양한 유형\n\n| 초기화          | 활성화 함수             | σ² (정규)  |\n| -------------- | ----------------------------- | --------------- |\n| Xavier/Glorot  | None, tanh, logistic, softmax | 1 / 팬_평균 |\n| He/Kaiming     | ReLU 및 변형                  | 2 / 팬-인    |\n| LeCun          | SELU                          | 1 / 팬-인    |\n```\n\nLecun 초기화는 가중치 분포에 작은 분산을 사용하여 𝑍의 분산을 축소하는 것에 기반합니다. 𝑍의 분산이 팬-인과 각 가중치의 분산의 곱이라면, 𝑍가 분산이 1이 되도록 보장하려면 각 가중치의 분산은 1/팬-인이어야 합니다. 따라서 Lecun 초기화는 가중치를 𝑁(0,1/팬-인)에서 무작위로 선택합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n자비에/글로뤼 초기화는 이전 레이어의 가중치(fan-in)의 영향을 고려할 뿐만 아니라 역전파 중 이러한 가중치가 후속 레이어에 미치는 영향(fan-out)도 고려합니다. 순방향 및 역방향 전파 중 분산을 균형있게 유지하기 위해 분산에 대한 공식인 2/(fan_in + fan_out)을 사용하여 가중치를 그려놓을 수 있습니다. 이 때의 분산은 Normal 분포, N(0,2/(fan_in + fan_out)) 또는 Uniform 분포(- sqrt(6/ (fan_in + fan_out)), sqrt(6/ (fan_in + fan_out))) 중에서 선택할 수 있습니다.\n\n희/카이밍 초기화는 ReLU 및 그 변형에 특히 맞추어져 있습니다. ReLU는 음수 입력을 제로로 처리하므로 뉴런 활성화의 절반은 0이 아닌 것으로 예상되며, 이는 분산을 줄이고 그라디언트 소멸을 유발할 수 있습니다. 이에 대비하여 희 초기화는 Lecun 방법에서 사용된 분산을 두 배로 늘리는데, 이를 통해 ReLU를 사용하는 레이어에 필요한 균형을 유지합니다. Leaky ReLU 및 ELU의 경우 약간의 조정이 필요하지만(예: ELU의 경우 2 대신 1.55 배 사용), 원칙은 그라디언트를 안정화하기 위해 분산을 조정하고자 한다는 것입니다. 반면 SELU의 경우 자체 정규화 속성을 활용하기 위해 모든 숨겨진 레이어에 Lecun 초기화를 사용해야 합니다.\n\n이 토론은 PyTorch와 같은 프레임워크에서 가중치 초기화가 어떻게 구현되는지에 대한 흥미로운 측면을 엽니다. 이는 다음과 같은 질문으로 제시될 수 있습니다 —\n\n## PyTorch에서 가중치 초기화는 어떻게 구현되고, 그것이 특별한 이유는 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n파이토치에서는 선형 레이어의 가중치 초기화에 대한 기본 접근 방식이 르쿤 초기화 방법을 기반으로 합니다. 반면 케라스에서는 기본 초기화 기술로 제비어/글로럿 초기화가 사용됩니다.\n\n그러나 파이토치는 가중치 초기화에 대해 매우 유연한 접근 방식을 제공합니다. 사용자는 모델에서 사용된 다양한 활성화 함수의 특정 요구 사항과 일치하도록 프로세스를 세밀하게 조정할 수 있습니다. 이 세밀한 조정은 두 가지 주요 구성 요소를 고려하여 달성됩니다:\n\n- 모드: 이 구성 요소는 레이어의 입력 연결 수(fan-in) 또는 출력 연결 수(fan-out)에 따라 초기화된 가중치의 분산이 조정되는지를 결정합니다.\n- 게인: 이는 모델에서 사용된 활성화 함수에 따라 초기화된 가중치의 스케일을 조정하는 스케일링 계수입니다. 파이토치는 가중치 초기화 프로세스를 최적화하기 위해 맞춤형 게인 값을 계산하는 torch.nn.init.calculate_gain 함수를 제공합니다.\n\n가중치 초기화 매개변수를 사용자 정의하는 이 유연성을 통해 모델에서 사용된 특정 활성화 함수와 비교 가능하고 호환되는 초기화 접근 방식을 설정할 수 있습니다. 흥미로운 점은 파이토치의 가중치 초기화 구현이 서로 다른 초기화 방법 간의 어떤 관계를 나타낼 수 있는데, 이를 통해 신경망의 전반적인 기능을 향상시키기 위한 초기화 프로세스를 활용할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어, SELU 활성화 함수에 대한 PyTorch 문서를 검토하면 가중치 초기화의 흥미로운 측면을 발견할 수 있습니다. 문서에는 SELU 활성화와 함께 kaiming*normal 또는 kaiming_normal*을 사용하여 초기화할 때, nonlinearity=`selu` 대신 nonlinearity=`linear`을 선택해야 자가 정규화를 달성할 수 있다고 언급되어 있습니다. 이 세부 사항은 흥미로운데, PyTorch의 기본 Lecun 초기화가 Kaiming 방법을 선형 비선형성에서 gain이 1로 설정했을 때 Lecun 초기화 방법을 효과적으로 복제한다는 점을 강조합니다. 이는 Lecun 초기화가 보다 일반적인 Kaiming 초기화 접근법의 특정 응용이라는 것을 보여줍니다. 마찬가지로, Xavier 초기화 방법은 입력 연결의 수(fan-in)와 출력 연결의 수(fan-out)를 모두 고려하는 Lecun 초기화의 다른 변형으로 볼 수 있습니다.\n\n## 가중치를 분포로부터 초기화할 때 평균과 분산을 신중하게 선택해야 하는 점에 동의합니다. 그러나 왜 초기 가중치를 정규 분포 대신 균일 분포에서 추출하려고 하는지에 대한 이유는 여전히 명확하지 않습니다. 무엇 때문에 한 가지를 다른 것보다 선호하게 되는지 설명해주실 수 있나요?\n\n가중치를 초기화할 때 분포로부터 추출할 때 평균과 분산을 신중하게 선택하는 중요성에 대한 귀하의 주장은 타당합니다. 신경망에서 가중치를 초기화할 때 중요한 고려 사항 중 하나는 정규 분포나 균일 분포 중에서 추출할지 결정하는 것입니다. 명확한 연구 결과를 지지하는 답변이 없지만, 이러한 선택을 하는 이유에는 몇 가지 타당한 이유가 있습니다:\n\n균일 분포는 엔트로피가 가장 높은 분포로, 범위 내의 모든 값이 동등하게 가능성이 있습니다. 이 공정한 접근은 초기화에 어떤 값이 더 잘 작동할지에 대한 사전 지식이 부족할 때 유용할 수 있습니다. 각 잠정적인 가중치 값에 공정하게 대우하고 균일한 확률을 할당합니다. 이는 한정된 정보로 게임에서 모든 팀에 공평하게 건 게임과 비슷합니다 - 선호되는 결과의 가능성을 최대화합니다. 어떤 구체적인 값이 좋은 초기 가중치인지 알 수 없기 때문에 균일 분포를 사용하면 편향되지 않은 시작점을 보장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n한편, 정규 분포는 일반적으로 가중치를 0에 가까운 작은 값으로 초기화하는 것이 더 자주 일어납니다. 초기 가중치가 작을수록 출력의 분산이 줄어들고 학습 중 안정적인 기울기를 유지하는 데 도움이 되기 때문에 작은 초기 가중치가 일반적으로 선호됩니다. 이는 가중치 초기화 방법에서 단위 분산 대신 작은 분산을 선호하는 이유와 유사합니다. 게다가, 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수는 작은 초기 가중치 값에서 더 나은 성능을 발휘하며 이러한 활성화 함수가 숨겨진 레이어가 아닌 최종 출력 레이어에서만 사용되더라도 그렇습니다.\n\n근본적으로, 균일 분포는 사전 지식이 부족한 상황에서 공평한 시작점을 제공하여 모든 잠재적인 가중치 값들을 동등하게 가능성 있는 것으로 간주합니다. 반면 정규 분포는 0에 가까운 작은 초기 가중치를 선호하여 기울기 안정성을 돕고 시그모이드나 하이퍼볼릭 탄젠트와 같은 특정 활성화 함수와 잘 맞습니다. 이러한 분포 사이의 선택은 종종 다른 신경 아키텍처와 작업에 걸쳐 경험적인 결과에 따라 이루어집니다. 보편적으로 최적의 방법은 존재하지 않지만, 균일 및 정규 분포의 특성을 이해하면 더 많이 발견되고 문제에 특화된 초기화 결정을 할 수 있게 됩니다.\n\n## 우리는 편향 항에 대해서도 이러한 가중치 초기화 방법을 사용합니까? 편향 항을 어떻게 초기화합니까?\n\n좋은 질문입니다. 우리는 편향 항에 대해서는 가중치와 동일한 초기화 기술을 반드시 사용하지는 않습니다. 사실, 편향 값을 모두 간단히 0으로 초기화하는 것이 흔한 실천입니다. 그 이유는 가중치가 각 뉴런이 기본 데이터를 근사하는 함수의 모양을 결정하는 반면, 편향은 각 함수를 위아래로 이동시키는 오프셋 값을 제공하기 때문입니다. 그래서 편향은 가중치가 학습하는 전반적인 형태에 직접적으로 영향을 주지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n초기화의 주요 목표는 대칭을 깨고 가중치 학습에 좋은 시작점을 제공하는 것이므로 편향이 어떻게 초기화되는지에 대해 크게 걱정할 필요가 없습니다. 그들을 모두 0으로 설정하는 것이 일반적으로 충분합니다. 이에 대해 더 자세한 논의는 CS231n 강의 노트에서 찾아볼 수 있습니다.\n\n# 배치 정규화\n\n선택한 활성화 함수와 적절하게 초기화된 가중치로 신경망을 훈련 시작할 수 있습니다 (우리의 미니 아이스크림 공장 생산 라인을 가동시키는 것과 같습니다). 그러나 품질 통제가 필요합니다. 초기에는 물론 훈련 반복 중에도요. 두 가지 주요 기술은 특성 정규화와 배치 정규화입니다.\n\n이전 포스트에서 경사 하강법에 대해 논의한 것처럼, 이러한 기술은 빠른 수렴을 위해 손실 풍경을 재구성합니다. 특성 정규화는 초기 데이터 입력에 이를 적용하며, 배치 정규화는 에폭 사이에 숨겨진 레이어의 입력을 정규화합니다. 두 기술 모두 다른 단계에서 품질 보증 점검을 구현하는 것과 유사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n배치 정규화는 훈련 중에 각 레이어의 입력을 평균이 0이고 분산이 1인 값으로 정규화하여 내부 공변량 이동을 줄이어 경사 소실/폭발 문제를 완화하는 데 도움을 줍니다. 내부 이동이 발생하는 이유에 대해 생각해 보죠. 각 레이어의 매개변수를 기울기에 기반하여 업데이트하는 과정은 신경망의 각 레이어가 공장의 서로 다른 부서라고 생각할 수 있습니다. 한 부서의 매개변수(또는 설정)를 업데이트할 때마다 다음 부서의 입력이 변경됩니다. 이로 인해 각 레이어마다 새로운 변화에 대한 조정이 필요하며 이를 심층 학습에서 내부 공변량 이동이라고 합니다. 그렇다면 이러한 이동이 자주 발생할 때 어떻게 될까요? 네트워크가 안정화하기 어려워지며 각 레이어의 입력이 계속 변화함에 따라 문제가 발생합니다. 이는 공장의 한 부분에서 지속적인 변화가 제품 품질에 일관성 없이 영향을 미치는 것과 유사합니다. 이는 작업자들을 혼란스럽게 하고 작업 흐름을 망치는 결과를 초래할 수 있습니다.\n\n배치 정규화는 훈련 중 미니 배치 전체에서 각 레이어의 입력을 정규화하여 평균이 0이고 분산이 1인 값으로 설정하는 것을 목표로 합니다. 레이어가 예상할 수 있는 일관된, 통제된 입력 분포를 강요합니다. 공장 비유로 돌아가서, 다음 부서로 전달되기 전 각 부서의 출력에 엄격한 품질 기준을 설정하는 것과 유사합니다. 예를 들어, 베이킹 부서가 일관된 크기와 모양의 아이스크림콘을 생산해야 한다는 규칙을 설정하는 것입니다. 다음 장식 부서는 콘의 변화량을 고려할 필요가 없게 되며, 각 일반화된 콘에 동일한 양의 아이스크림을 추가할 수 있습니다.\n\n정규화를 통해 내부 공변량 이동을 줄이는 것으로 배치 정규화는 훈련 과정 중에 기울기가 엉망이 되는 것을 방지합니다. 레이어들이 신속히 변하는 입력 분포에 계속해서 재조정할 필요가 없어져서 기울기가 더 안정적으로 유지됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 정규화는 정규화자 역할을 하며 목적 함수 랜드스케이프를 부드럽게 만듭니다. 이를 통해 더 높은 학습 속도를 사용하여 수렴 속도를 높일 수 있습니다. 일반적으로 배치 정규화는 내부 분산 이동을 줄이고 그래디언트를 안정화시키며 목적함수를 정규화하고 훈련 가속화를 가능하게 합니다.\n\n## 배치 정규화를 어떻게 적용해야 하나요? 활성화 함수 이전 또는 이후에 적용해야 하나요? 훈련 및 테스트 중에 어떻게 처리해야 하나요?\n\n배치 정규화는 그래디언트를 안정화시키는 추가 레이어를 통해 DNN을 훈련하는 방식을 실제로 바꿨습니다. DL 영역에서 활성화 함수 이전 또는 이후에 적용해야 하는지에 대한 논쟁이 있습니다. 솔직히 말해서, 이는 모델에 따라 다르며 조금은 실험해 봐야 할 수도 있습니다. 그냥 방법을 일정하게 유지하도록 하고 변경하면 예상치 못한 문제가 발생할 수 있습니다.\n\n훈련 중에 배치 정규화 레이어는 각 미니 배치를 통해 각 차원에 대한 평균과 표준편차를 계산합니다. 이러한 통계량은 출력을 정규화하는 데 사용되어 평균이 0이고 분산이 1임을 보장합니다. 이 프로세스는 입력 분포를 표준 정규 분포로 변환하는 것으로 생각할 수 있습니다. 전체 훈련 데이터 세트를 사용하여 특징 정규화를 하는 것과는 달리 배치 정규화는 각 미니 배치에 기초하여 조정되어 처리되는 데이터에 동적이며 반응성을 가지게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제, 테스트는 다른 이야기입니다. 테스트 데이터에서 평균과 분산을 정규화에 사용하면 안 됩니다. 대신에 이러한 매개변수는 학습된 특징으로 간주되어 훈련 과정에서 유지되어야 합니다. 훈련 중 각 미니 배치는 고유의 평균과 분산을 가지지만, 일반적인 실천 방법은 이러한 값들의 이동 평균을 훈련 단계 동안 사용하는 것입니다. 이를 통해 안정된 추정값을 제공하여 테스트 중에 적용할 수 있게 됩니다. 다른 적은 일반적인 방법은 전체 훈련 데이터 세트를 사용하여 포괄적인 평균과 분산을 계산하는 추가 에포크를 실행하는 방법도 있습니다.\n\nPyTorch로 DNN 프레임워크로 훈련할 때, 조정 가능한 하이퍼파라미터인 γ와 β를 사용할 수 있습니다. 이러한 파라미터를 조정하여 배치 정규화 과정을 세밀하게 조정할 수 있습니다. 일반적으로 기본 설정은 매우 효과적입니다. 그러나 훈련 중에 PyTorch는 분산을 계산하기 위해 편향 추정량을 사용하지만, 테스트 중에 이동 평균을 위해 불편 추정량을 사용합니다. 이러한 조정은 모델이 미처 못 본 조건에서 인구 표준 편차를 더 정확하게 근사하고 모델의 신뢰성을 향상하는 데 도움이 됩니다.\n\n배치 정규화를 올바르게 적용하는 것은 네트워크에서 효율적인 학습에 중요합니다. 네트워크가 잘 학습하는 것뿐만 아니라 다양한 데이터 집합과 테스트 시나리오에서 성능을 유지할 수 있게 합니다. 생산 라인의 각 세그먼트를 정확하게 교정하여 운전을 원활하고 일관되게 유지하는 것으로 생각해보세요.\n\n## 왜 역전파 중에 그래디언트에 직접 배치 정규화를 적용하는 대신 순전파 중에 배치 정규화가 적용되나요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n일반적으로 순방향 패스 중 입력 또는 활성화에 배치 정규화를 적용하는 이유가 역전파 중에 기울기 자체에 직접 배치 정규화를 적용하는 것보다 더 일반적입니다.\n\n먼저, 기울기에 배치 정규화를 직접 적용하는 이점을 보여주는 실증적 증거나 실무가 부족합니다. 내부 공변량 이동의 개념은 주로 순방향 패스 중에 발생하며, 계층 입력의 분포가 매개변수 업데이트로 인해 변경됩니다. 따라서, 후속 계층에서 처리되기 전에 이러한 입력을 안정화시키기 위해 이 단계에서 배치 정규화를 적용하는 것이 합리적입니다. 또한, 기울기에 배치 정규화를 직접 적용하는 것은 기울기의 크기와 방향이 나르는 중요한 정보를 왜곡할 수 있습니다. 이는 내재적 의미를 변경하는 방식으로 고객 피드백을 변조하는 것과 유사하며, 이는 미니 아이스크림 공장의 제조 프로세스에 대한 향후 조정을 잘못 이끌 수 있습니다.\n\n그러나, 기울기를 경사 클리핑과 같은 마이너 조정을 하는 것은 일반적으로 허용되며 유익합니다. 이 기법은 기울기를 지나치게 크게 만들지 않고 안전한 범위 내에 유지하여 기울기를 제한하는 도구입니다. 이는 피드백에서 극단적 아웃라이어를 걸러내는 것과 유사하며, 이는 프로세스를 방해할 수 있는 급격한 반응을 방지하면서 전체 피드백의 무결성을 유지하는 데 도움이 됩니다. PyTorch에서는 기울기 노름을 모니터링하는 것이 일반적이며, 기울기가 폭발하기 시작하면 경사 클리핑과 같은 기법을 사용할 수 있습니다. PyTorch는 torch.nn.utils.clip*grad_norm* 및 torch.nn.utils.clip*grad_value*와 같은 함수를 제공하여 이를 관리할 수 있습니다.\n\n## 직접 정규화 대신 기울기를 클리핑하는 옵션을 언급했습니다. 왜 기울기를 클리핑하는 대신 바닥값을 설정하지 않는지 정확히 선택하는 이유가 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기울기 클리핑은 폭발하는 기울기 문제를 방지하는 데 도움이 되는 간단하면서도 효율적인 기술입니다. 종종 기울기의 최대값을 수동으로 제한합니다. 예를 들어 ReLU 활성화 함수는 상한값을 6으로 설정할 수 있으며, PyTorch에서는 ReLU6로 알려져 있습니다. 이 상한을 설정함으로써, 각 층에서 역전파 과정 중 기울기가 연쇄 법칙에 따라 곱해질 때 값이 지나치게 커지지 않도록 보장합니다. 이러한 클리핑은 기울기가 학습 과정을 방해할 정도로 급격하게 증가하는 것을 방지하여 그 값을 관리 가능한 한도 내에 유지합니다.\n\n한편, 기울기를 억제하는 것은 너무 작아지지 않도록 하기 위해 하한값을 설정하는 것입니다. 그러나 이는 사그라들어 가는 기울기 문제의 근본적인 해결책이 되지는 않습니다. 일부 활성화 함수인 시그모이드나 tanh 같은 경우 입력이 0에서 멀어질수록 기울기 값을 매우 심각하게 축소시키기 때문에 기울기의 사그라들음 문제가 발생합니다. 이는 학습 속도가 극도로 느려지거나 정체되는 매우 작은 기울기 값을 야기합니다. 기울기를 억제해도 해결되지 않는 이유는 문제의 근본이 활성화 함수의 성질에 기인하기 때문입니다. 즉, 단순히 값이 너무 작은데만 있지 않고 활성화 함수가 기울기 값을 압축하는 것에 있습니다. 따라서 사그라드는 기울기 문제를 효과적으로 해결하기 위해서는 네트워크 아키텍처나 활성화 함수 선택을 조정하는 것이 더 유익합니다. 기울기가 사그라들지 않도록 하는 활성화 함수 사용(ReLU같은), ResNet 아키텍처에서 볼 수 있는 스킵 연결 추가, LSTM이나 GRU 같은 RNN에서 게이트 메커니즘을 사용하는 등의 기술을 통해 기울기는 역전파 중 네트워크 전반에 걸쳐 더 건강한 흐름을 보장하여 자연스럽게 사그라드는 것을 방지할 수 있습니다.\n\n요약하면, 기울기 클리핑은 지나치게 큰 기울기를 효과적으로 관리하지만, 하한값을 설정하는 기울기 억제는 지나치게 작은 기울기 문제를 효과적으로 다루지 못합니다. 대신, 사그라드와 관련된 문제를 해결하려면 일반적으로 구조적인 조정이 필요합니다.\n\n#실무에서의 경험(개인 경험)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요약할 때, 모든 논의된 방법이 사라지는 그래디언트 문제와 폭주하는 그래디언트 문제를 해결하는 데 유용하다는 것은 명백합니다. 이들은 모두 모델의 학습 과정을 향상시킬 수 있는 실용적인 접근 방법입니다. 이 글을 마무리하며 한 가지 질문으로 마무리하고 싶습니다 -\n\n## 현실은 무엇인가요? 실무에서는 어떤 일반적인 과정이 있나요?\n\n실무에서 좋은 소식은 가능한 모든 해결책을 실험할 필요가 없다는 것입니다. 활성화 함수를 선택할 때, ReLU가 종종 선택되는 것이며 매우 비용 효율적입니다. ReLU는 양의 입력의 크기를 변경하지 않고 전달합니다 (시그모이드나 tanh는 큰 값을 크기와 관계없이 항상 1로 압축합니다) 그리고 계산 및 미분 측면에서 간단합니다. 주요 프레임워크에서 잘 지원되며 dead ReLU 문제를 우려한다면 Leaky ReLU, ELU, SELU, 또는 GELU와 같은 대안을 고려할 수 있지만 일반적으로 시그모이드와 tanh를 피해야 하는 사라지는 그래디언트 문제를 피하기 위해 명확을 지켜야 합니다.\n\n선호되는 활성화 함수인 ReLU로 인해 가중치 초기화가 지나치게 민감하게 작용하는 문제에 대해 덜 걱정해도 됩니다. 시그모이드, tanh 및 SELU와 같은 함수에서 주로 발생하는 문제일 뿐입니다. 대신, 선택한 활성화 함수에 권장되는 가중치 초기화 방법에 집중하는 것이 적당합니다 (예를 들어, ReLU에 대해 He/Kaiming 초기화를 사용하는 이유는 ReLU의 비선형성을 고려하기 때문입니다).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n네트워크에는 항상 배치 정규화를 포함하세요. 활성화 함수 전 또는 후에 적용할지 결정(또는 실험)하고, 모델 전체에서 일관되게 그 선택을 유지하세요. 배치 정규화는 규제 효과와 높은 학습률 사용이 가능해지는 등 여러 가지 이점을 제공합니다. 이는 교육 및 수렴 속도를 높일 수 있습니다.\n\n그래서 어떤 것을 실험해볼 가치가 있을까요? 옵티마이저는 탐구할 가치가 있습니다. 이전 글에서 그라디언트 디센트 및 그 인기 있는 변형 등 다양한 옵티마이저를 논의했습니다. Adam은 빠르지만 과적합을 유발하고 학습률을 너무 빨리 감소시킬 수 있습니다. SGD는 신뢰성이 있고 병렬 컴퓨팅 환경에서 특히 효과적일 수 있습니다. 느릴 수 있지만 모델로부터 최대 성능을 뽑아내려면 확실한 선택입니다. 때로는 RMSprop이 더 나은 대안일 수 있습니다. 저는 Adam으로 시작하여 속도를 이유로 한 후에 더 나은 최소값을 찾고 과적합을 방지하기 위해 후기 에포크에서 SGD로 전환하는 것이 좋은 전략으로 생각합니다.\n\n만약 이 시리즈를 즐기고 계시다면, 상호작용(박수, 댓글 및 팔로우)이 지지뿐만 아니라 시리즈를 이어가는 원동력이자 저의 계속된 공유를 영감받는 기반이 됩니다.\n\n이 시리즈의 다른 게시물:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- ML 학습의 용기: L1 및 L2 정규화 해독하기 (파트 1)\n- ML 학습의 용기: 우도, MLE 및 MAP 해독하기\n- ML 학습의 용기: F1, 재현율, 정밀도 및 ROC 곡선에 대한 심층 탐구\n- ML 학습의 용기: 가장 일반적인 손실 함수에 대한 상세 가이드\n- ML 학습의 용기: 경사 하강법과 인기 있는 옵티마이저에 대한 심층 탐구\n- ML 학습의 용기: 수학적 이론부터 코딩 실무까지 백프로파게이션 설명\n\n## 참고 자료\n\n활성화 함수\n\n- [가우시안 에러 선형 유닛 (GeLU) 설명](https://ml-explained.com/blog/activation-functions-explained#gaussian-error-linear-unit-gelu)\n- [ReLU 활성화 함수](https://www.mldawn.com/relu-activation-function/)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가중치 초기화\n\n- [normal glorot initialization(일반 글로럿 초기화)의 원천](https://datascience.stackexchange.com/questions/102036/where-does-the-normal-glorot-initialization-come-from)\n- [파이토치(PyTorch)에서의 기본 초기화에 대한 명확한 이해](https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/2)\n\n그래디언트 클리핑\n\n- [파이토치(PyTorch)에서 그래디언트 클리핑 하는 방법](https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch)\n","ogImage":{"url":"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png"},"coverImage":"/assets/img/2024-05-18-CouragetoLearnMLTacklingVanishingandExplodingGradientsPart2_0.png","tag":["Tech"],"readingTime":39},{"title":"고객 이탈 예측","description":"","date":"2024-05-18 19:26","slug":"2024-05-18-CUSTOMERCHURNPREDICTION","content":"\n이것은 이진 분류 문제이며, 은행 데이터 세트를 사용했습니다. 고객이 은행을 떠날 때에 대한 정보가 포함되어 있으며, 이를 사용하여 미래에 은행을 떠날 가능성이 있는 고객을 예측해야 합니다.\n\n![이미지](/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_0.png)\n\n우리는 인공 신경망을 구축할 것입니다. 이러한 문제에 접근하는 단계는 다음과 같습니다 —\n\n- 특정 라이브러리 가져오기\n- 데이터 세트 로드, 데이터 세트에 대한 가능한 정보 찾기(예: 데이터 세트에 결측값이 있는지, 중복된 값의 존재 여부)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_1.png\" /\u003e\n\n- 고객이 나간 수를 확인하기 위해 동일한 것을 나타내는 이 코드를 사용했습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_2.png\" /\u003e\n\n- 이제 데이터 세트를 분석하고 ('RowNumber', 'CustomerId', 'Surname')와 같은 열이 예측에 큰 영향을 미치지 않으므로 삭제할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![데이터1](/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_3.png)\n\n- 추가로, ONE HOT ENCODING을 사용하여 범주형 값들을 변환하겠습니다. get_dummies() 및 (drop_first=True)를 사용하면 지리와 성별에서 다른 하나를 삭제할 수 있습니다(예: 프랑스 및 여성).\n\n![데이터2](/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_4.png)\n\n- 모델을 훈련 및 테스트 데이터셋으로 분할합니다.\n- 이제 값들을 스케일링할 것입니다. 'balance'와 'estimated_salary'의 값이 매우 크기 때문에 발생하는 문제를 방지하기 위해 StandardScaler()를 사용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_5.png\" /\u003e\n\n- 케라스 라이브러리를 사용하여 순차적 모델에 대한 'model' 객체를 만듭니다.\n- 그런 다음 레이어(은닉, 출력)를 추가합니다.\n- 시그모이드 활성화 함수를 사용하고, 입력이 11(탈퇴 제외)인 3개 노드를 갖는 밀집 은닉 레이어를 추가합니다.\n- 출력 레이어를 추가합니다.\n- summary를 확인하면 매개변수(가중치 + 편향)를 제공합니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_6.png\" /\u003e\n\n- 이제 모델을 컴파일해야 합니다. 어떤 손실 함수, 옵티마이저를 사용할 것인지 지정해야 합니다. 바이너리 분류 문제이므로 사용되는 손실 함수는 크로스 엔트로피/로그 손실입니다. 다양한 옵티마이저(경사 하강법, 확률적 경사 하강법, RMSprop 등)를 사용할 수 있지만 아담(적응 모멘트 추정)이 잘 작동합니다.\n- 모델을 적합하고 10회 반복(에포크)하며 'history'라는 딕셔너리에 저장합니다. Validation_split은 모델을 훈련하는 지점을 처리하는데 사용됩니다. 예를 들어 8000개의 항목이있는 경우 이를 나누고 2000개의 항목을 제거하며 실행 중에 2000개의 포인트를 동시에 확인하고 정확성을 알려줍니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Customer Churn Prediction 7](/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_7.png)\n\n- 이제 배열 안에 가중치(weights)와 편향(biases)을 얻을 수 있습니다.\n\n![Customer Churn Prediction 8](/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_8.png)\n\n- 이제 예측이 나오게 됩니다. 시그모이드 함수를 사용하기 때문에 출력은 (0-1)의 범위에 있을 것입니다, 확률입니다. 우리는 이 확률을 0 또는 1로 변환해야 합니다, 그러기 위해 임계값을 정해야 합니다 (예를 들면 0.5, 만약 확률이 0.5보다 작으면 고객이 은행을 떠나지 않고, 확률이 0.5보다 크면 그들은 은행을 떠날 것입니다.) 임계값은 일반적으로 도표를 통해 결정되지만, 여기서는 추측하고, 0.5로 설정되어 있습니다.\n- 그러면 모델의 정확도 점수를 찾을 준비가 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_9.png\" /\u003e\n\n- 또한 matplotlib을 사용하여 그래프를 그릴 수 있습니다.\n\n## 참고 — 정확도를 높이기 위해 다음을 증가시킬 수 있습니다:\n\n- epoch의 수를 증가시킴.\n- 은닉층의 활성화 함수를 relu로 설정.\n- 은닉층의 노드 수를 증가시킴.\n- 또는 은닉층의 수를 증가시킴(과적합이 발생할 수 있으므로 적당히).\n","ogImage":{"url":"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_0.png"},"coverImage":"/assets/img/2024-05-18-CUSTOMERCHURNPREDICTION_0.png","tag":["Tech"],"readingTime":4},{"title":"휴머노이드는 여기에 머물러 있을까요","description":"","date":"2024-05-18 19:24","slug":"2024-05-18-AretheHumanoidsHeretoStay","content":"\n매주 이런 식으로 새로운 업데이트를 내놓는 인간형 회사들을 볼 수 없네요. 옵티머스가 걸을 수 있어요? 디지트가 빈 토트백을 옮겼다구요? 피거도 그렇게 하는군요! 드디어 실제 회사들도 흥미를 느끼기 시작한 것 같아요. 테슬라부터 시작해서 아마존과 BMW에서도 이제는 \"작동 중\"이랍니다. 마치 집과 정원에서 우리에게 한 발짝 떨어진 것 같아요.\n\n하지만 정말로 일하고 있는 걸까요? 보여지는 데모들은 보스턴 다이내믹스의 아틀라스가 파크our을 하는 것만큼 흥미로운 것이 아니라 humanoids가 생산적인 것 같지도 않아요. 그래서 시장이 정말로 흥분한 것일까요? Humanoids가 무언가를 준비하고 있는 걸까요? 저는 두 가지 이유로 humanoids에 흥분해요:\n\n1. 인간형 로봇은 마침내 \"브라운필드\" 문제를 해결할 수도 있어요. 이것이 로봇 솔루션들이 실험 단계에 머무는 주된 이유이기도 하거든요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2. 2023년에 기계 학습은 큰 발전을 이루었습니다. 컴퓨터들이 이번에 처음으로 노력의 스킬을 발휘하여 오픈 월드 환경에서 작동하고 접촉 시키는 것이 가능해졌습니다.\n\n# Greenfield vs. Brownfield\n\n로봇 공학은 아직 큰 산업이 아니며 대부분의 산업은 “Greenfield” 배치로만 성공을 거둡니다. 기존 공정을 개조하는 대신, 공장과 그 제품을 로봇 솔루션 주위에 설계합니다. 이것이 ABB, Fanuc 및 Kuka와 같은 기업들이 수익을 올리는 방식이며 자동차 산업을 위한 생산 라인과 같은 전문 솔루션을 구축합니다. 아마존도 이와 같은 원리로 Kiva 자동화 시스템과 함께 작동하는 건물 구축을 하고 있습니다. 반면, 기존 공정과 통합되는 솔루션 (기존 토지 또는 \"갈색\" 영역에)은 종종 생산적으로 성공하지 못하고 버려지는 경우가 많습니다.\n\n아래 이미지는 이러한 딜레마를 설명합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![2024-05-18-AretheHumanoidsHeretoStay_1.png](/assets/img/2024-05-18-AretheHumanoidsHeretoStay_1.png)\n\n작지만 성공한 커피 사업을 상상해보세요. 이 사업은 역사적으로 분리된 분쇄기, 커피 포트 및 열판을 사용하여 커피를 만들어 왔습니다. 여기에 \"협력 로봇\"이라는 자동화된 과정이 도입됩니다. 이런 로봇은 커피 포트를 열판 위에 놓는 등 일부 작업만 수행할 수 있고, 추가적인 장비들을 필요로 합니다. 이런 해결책은 기존 과정을 준비된 자동화 솔루션으로 교체하는 것이 실제로 쉽고 저렴하게 가능합니다. 에스프레소 메이커만 사면 끝이죠. 수동으로 만든 커피에 집착하는 사람들처럼, 산업 환경에서의 공정은 종종 다른 공정과 깊게 연결되어있어 하류 공정을 변경해야 할 수도 있습니다.\n\n(계속)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이러한 매우 기본적인 작업들 다음에는 조립 작업장을 위한 키트를 생성하고 배포하고, 슈퍼마켓 선반이 깔끔하고 적절히 구비되어 있는지 확인하거나 식기 세척기, 커피 메이커 및 진공 청소기와 같은 가정용 가전제품을 작동하는 등 보다 복잡한 작업을 빠르게 수행하게 될 것입니다.\n\n하지만 또 다른 이점도 있습니다: 전력에 연결되었거나 무선으로 충전 중이라면, 인간형 로봇은 휴식 없이 세 번의 교대 근무를 할 수 있으며, 학습한 모든 것은 즉시 동종 로봇들 모두에게 전달될 수 있습니다. 더욱 좋은 점은, 일단 인간형 로봇이 프로세스에 통합되면, 알고리즘을 통해 작업자들로부터 과도한 정직을 요구하는 Lean 및 Six-Sigma의 모든 기술을 완전히 디지털 방식으로 구현할 수 있게 되어 엄청난 생산성 향상을 이끌어낼 수도 있습니다.\n\n![이미지](/assets/img/2024-05-18-AretheHumanoidsHeretoStay_2.png)\n\n투자자, 기업가 및 과학자들을 흥분하게 만드는 것은 이런 전망이며, 비록 게임이 오래 소요될 지라도요. 그럼에도 불구하고, 2023년에 역사책에 기록된 또 다른 Durchbruch 덕분에 아마도 많은 사람들이 모두 출자하지 않을까 합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 기계 학습이 이끄는 전례 없는 능력\n\n2023년은 ChatGPT의 해였습니다. ChatGPT의 명백한 이점 외에, 트랜스포머 신경망 구조는 텍스트에 국한되지 않고 훨씬 더 강력해졌다는 것이 밝혀졌습니다. 그것은 이미지와 언어를 결합하는 능력으로 인해 기계 학습이 미리 정의된 클래스로의 지도 학습을 벗어나게 하였고, 로봇이 이전에 본 적이없는 물체를 다루도록 허용하였습니다. 예를 들어, \"나사\"라는 물체를 이미지에서 제로샷 방식으로 찾을 수 있는 Owl-VIT [1] 비전-언어 모델이 있습니다. \"나사\"가 무엇이며 어떻게 생겼는지에 대해 명시적으로 학습되지 않고도 가능합니다.\n\n![이미지](/assets/img/2024-05-18-AretheHumanoidsHeretoStay_3.png)\n\n라벨링 및 물체 탐지가 완벽하지는 않지만, 비전 임베딩은 원격 조작된 데모와 결합하여 확산을 사용하여 시각 운동 표현을 학습할 수 있도록 허용합니다[2]. 마치 DallE나 Midjourney에서 이미지를 생성할 때 사용되는 방식과 유사합니다. 로봇은 텍스트를 프롬프트로 변환하는 대신, 센서 관측치를 궤적으로 변환합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n인간을 훈련시킬 때와 달리 현장에 배치된 로봇이 생산한 경험은 쉽게 다른 로봇으로 전달될 수 있습니다. 여기서 심지어 소수의 인간형 프로토타입만으로도 다양한 제조 및 가정 업무에 대한 전문 지식을 통한 무료롭 처리 양을 만들어낼 수 있습니다. 트랜스포머 모델이 자연스럽게 멀티모달이기 때문에 시각과 텍스트/음성 설명만 적용하는 것이 아니라 촉각적 정보, 소리 또는 진동을 받아들이고, 시맨틱한 구조화 정보와 연결하는 데 도움이 될 것입니다.\n\n대형 언어 모델은 또한 인간 언어와 컴퓨터 코드 사이를 매끄럽게 오가며 소프트웨어 습득 및 인간 피드백을 기반으로 코드를 적절하게 수정할 수도 있을 것입니다. 최근 이 논문[3]과 이 비디오에서 보여준 것처럼, 로봇의 가능성에 대한 \"API\"를 제공받음으로써 ChatGPT는 합리적인 코드를 생성하고 인간 피드백에 따라 조정할 수 있습니다. 인간 지침서, 책 지식 또는 이 두 가지의 조합에서 위와 같은 예시인 전문적인 로봇 임무를 빠르게 학습할 수 있는 능력을 가진 LLM을 인터랙션 훈련을 통해 미세조정함으로써 더 향상시킬 수 있을 것입니다.\n\n# 다음은 무엇일까요?\n\n그래서 우리는 대규모로 인간형 로봇을 배치할 준비가 되어 있고, 곧 더 매력적인 사용 사례들을 보게 되겠죠? 많은 기업들이 하드웨어 중심 접근 방식을 선택하여 동적 보행과 기본 조작이 가능하다는 것을 입증했습니다. 아직은 이 로봇들이 많은 것을 실제로 하거나 더 많은 가치를 창출하지는 못하고 있습니다. 고가치 임무인 자율 키팅, 조립 또는 선반 보충과 같은 임무는 이미 어느 정도 시간이 경과했으며 이전 창업 시절에도 가능했습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 비디오에 나오는 산업용 하드웨어와 현재 볼 수 있는 휴머노이드 사이에 중요한 차이가 있습니다. 처음부터 로봇을 만드는 것은 심각한 시스템 공학적 도전이 따르며 현재의 프로토 타입은 안정적인 기지에 장착된 협력 로봇의 0.1mm 정확도에서 현재는 상당히 멀리 떨어져 있을 것으로 보입니다. 관성과 진동을 제어하는 어려운 작업은 물론 토크 감지를 사용하여 이를 가능케하지만 대부분의 휴머노이드는 아직 이 기능이 없는 액추에이터에 의존하지만 저렴한 비용 접근 방식을 택하여 엔지니어링 아츠의 아름다운 로봇이 뻣뻣한 산업 시스템과 유사하다고 할 수 있을 정도의 선택을 했습니다.\n\n따라서 다리가 없는 휴머노이드는?\n\n누구나 로봇이 바로 다리가 필요할 것이라고 믿지 않습니다. 이러한 기업은 고가치의 조작 작업, 훈련 용이성 및 매끄러운 배치에 중점을 둡니다. 이 분야의 초기 사례 중 하나는 Rodney Brook의 \"배터\" (안식을 바라며) 로봇이며 나중에는 그의 한 팔로 된 후속자인 소이어가 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nBaxter가 슈퍼 저렴한 비용(`$25k에 이중 팔로봇)을 정밀성과 강성으로 바꾸는 지나치게 야 amb 계약의 하드웨어 디자인을 겪던 동안, Sawyer는 보다 전통적인 로봇 드라이브를 사용하여 최소한의 조립물과 기본적인 프로그래밍만으로도 다양한 응용 분야에서 성능을 발휘할 수 있습니다. Sawyer는 아직 몇몇 국가에서 판매 중이지만, 모든 “Cobots”이 겪는 브라운필드 문제에 시달립니다: 작업이 자동화 솔루션이 정당화할만큼 반복적인 경우, 이미 해당 솔루션이 만들어졌으며 상당히 더 나은, 빠르고 저렴할 가능성이 높습니다.\n\n“정체된 상반신” 방식의 또 다른 예는 Giant AI인데, 이는 2023년에 공개되고 (사업을 종료한 채) 잠잠하게 알려진 비디오 시리즈로 나타났습니다:\n\nBaxter와 마찬가지로, Giant의 Universal Worker는 기본 조작에 중점을 둔 정적 솔루션이었습니다. Baxter와 같이, Giant는 모든 하드웨어를 처음부터 개발했으며 힘줄 기반 접근법을 구현하여 (잠재적인) 비용 절감을 굉장한 개발 관리부담과 정확성으로 교환했으며, 제 시간에 진정한 고객 가치를 제공하지 못했습니다.\n\nGiant의 일부 지적재산권은 Sanctuary.ai에서 살아 있으며, 여기서도 상체 민첩성에 중점을 두지만, 유압 구동 재래를 복원함으로써, 로봇이 정밀한 조작부터 무거운 들기까지 다양한 작업 범위에 대처할 수 있도록 해줍니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 인간 모양은 맞지만 걷지는 않아요?\n\n로봇을 특정 장소에 제한하는 것은 이전에 좋은 생각이 아니었습니다. 왜냐하면 브라운필드 문제를 해결하지 못했기 때문입니다. 동일한 장소에서 상당한 시간을 보내는 로봇은 보다 효율적으로 자동화할 수 있는 작업에 종사하는 것일 가능성이 높습니다. 또한 이동성은 배치의 다양성만을 고려하는 것이 아니라, 로봇이 더 넓은 작업 공간에 대응하고 도구와 부품을 스스로 가져올 수 있게 해줍니다. 그렇다고 해서 즉시 다리가 필요한 건 아닐까요?\n\n회사들은 이 가설을 테스트하기 위해 인간형 상반신과 저렴하고 견고한 구동 장치를 결합해봅니다. 예를 들어, 1X 로보틱스(1X robotics)…\n\n…영상에서 보여지는 것 이상의 작업을 수행하려면 소프트웨어 업데이트 이상이 필요할 것입니다)과 바닥에서 물건을 줍는 능력을 결합한 상반신의 민첩성과 인간의 발자국만 조금 더 큰 바퀴 플랫폼을 함께 사용합니다. 산업용 공동 로봇의 성능을 얻는 것은 여전히 매우 어려울 것이며, 작은 바퀴 기반으로 인해 로봇이 운반할 수 있는 하중이 제한될 것입니다. 이러한 로봇은 따라서 인간과 로봇의 흥미로운 상호 작용을 창출하는 데 굉장히 뛰어난 '페퍼'처럼 많이 능력있지는 않지만, 다리가 달린 플랫폼의 이동성이나 협업 로봇의 일군 능력만큼은 갖춘 것이 아닙니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n작은 휠베이스는 동적 안정성을 제한하는 기회를 줄입니다. 이는 Boston Dynamics의 - 극도로 익살스러운 - 핸들 로봇들에서 나타납니다. 이 로봇들은 카운터 웨이트를 움직여 세그웨이와 같은 드라이브 체인에서 균형을 맞춰 다양한 하중 조건에 적응할 수 있습니다.\n\n아마도 주위에 있는 사람들이 이 두 친구 가까이 다가가지 못한 것을 눈치챘을 겁니다. 실제로, 어떠한 형태의 동적 활동도 일반적으로 안전하지 않습니다. 이것이 신뢰할 만한 동적 보행을 시연하는 것이 결핵되는 연결고리이자 많은 데모의 중심 주제인 이유입니다.\n\n# 인간형 로봇 경주에서 우승하기\n\n하지만 이 파도가 줄어들지 않으려면, 인간형 로봇은 가능한 빨리 생산 환경으로 이동해야 합니다. 이는 동시에 어느 정도의 소프트웨어와 하드웨어 혼합을 제공해야만 가능합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 기존 설치물, 도구, 및 장치를 쉽게 활용할 수 있는 인간 근로자의 형태 요인,\n- 개방형 세계에 대한 쉬운 범용화와 훈련 가능성을 갖춘 인간 근로자의 기능성,\n- 그리고 최소한의 조작 기술 세트,\n\n몇 가지 특정 사용 사례에 대한 브라운필드 문제를 해결하는 데 충분합니다. 언제나 새로운 하드웨어를 개발하는 것은 일반적으로 좋지 않은 생각입니다. 학습과 교육을 더욱 쉽게 만드는 것이 아니라 어렵게 하는 경우가 많습니다. 인간형 로봇이 고객 가치를 창출하고, 책지식과 시각-촉각적 경험을 결합한 다중 모드 기반 모델을 위한 데이터 기초를 먼저 제공할수록 좋습니다.\n\n# 참고 문헌\n\n[1] Minderer, M., Gritsenko, A., Stone, A., Neumann, M., Weissenborn, D., Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M. and Shen, Z., Simple open-vocabulary object detection with vision transformers. arXiv 2022. arXiv preprint arXiv:2205.06230.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n[2] Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burchfiel, B. and Song, S., 2023. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137.\n\n[3] Liang J, Xia F, Yu W, Zeng A, Arenas MG, Attarian M, Bauza M, Bennice M, Bewley A, Dostmohamed A, Fu CK. Learning to Learn Faster from Human Feedback with Language Model Predictive Control. arXiv preprint arXiv:2402.11450. 2024 Feb 18.\n","ogImage":{"url":"/assets/img/2024-05-18-AretheHumanoidsHeretoStay_0.png"},"coverImage":"/assets/img/2024-05-18-AretheHumanoidsHeretoStay_0.png","tag":["Tech"],"readingTime":10},{"title":"필수 인공지능","description":"","date":"2024-05-18 19:21","slug":"2024-05-18-EssentialAI","content":"\n르네상스 시대에는 1480년경 레오나르도 다 빈치가 헬리콥터에 대한 창의적인 개념을 그린 것으로 알려져 있지만, 그 꿈을 상용화되고 신뢰할 수 있는 제품으로 만드는 데에는 별도의 혁명인 산업혁명이 필요했습니다. 마찬가지로, 생성 적 인공지능이 우리 사무실에 딸린 로봇이 딸기를 수확하고 있는 매력적인 오일 페인팅을 선사했지만, 농업 산업에 수익성 있고 신뢰할 수 있는 수확 로봇을 제공하기 위해서는 인공지능에 대해 근본적으로 다른 접근 방식이 필요하다고 믿고 있습니다.\n\n![이미지](/assets/img/2024-05-18-EssentialAI_0.png)\n\n인공지능 르네상스가 도래했습니다\n\n우리 모두가 경험하는 것처럼, 인공지능 르네상스가 왔습니다. “Attention Is All You Need”이라는 위대한 논문이 트랜스포머 모델의 붐을 일으키기 시작했지만, 최초의 대중적인 붐은 ChatGPT 3와 4로 시작되었으며, 그 뒤에는 생성 적 인공지능의 대규모 확장이 이어졌습니다. 이번 주에는 OpenAI의 GPT-4o와 Google Gemini이 실시간 언어 번역, 코드 분석 및 다양한 흥미로운 \"AI 어시스턴트\" 응용 프로그램을 포함한 경이로운 실시간 시연을 선보였습니다. 인간형 로봇도 뜨거운 관심을 받고 있습니다 - 투자자들이 지난 12개월 동안 인공지능 주도의 인간형 로봇에 수십억 달러를 투자했습니다. RT-2 및 RT-X와 같은 프로젝트는 인간형 로봇을 위한 폭넓은 텍스트 - 행동 및 제로샷 학습 응용 프로그램을 꿈꾸고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n토르투가에서는 AI 르네상스를 신뢰합니다: 우리는 AI 로봇을 사용하여 과일을 따거나 다른 중요한 농업 서비스를 제공합니다. 또한, 현재의 AI 붐은 실제 현실 자동화의 거대한 경제적 결과로 이끄는 것은 아니라고 믿습니다. 우리가 AI 산업 혁명으로 전환할 때에 이뤄질 것이라고 생각합니다. 다 빈치의 르네상스는 \"공중 나사\"와 같은 혁신과 학습의 폭발을 가져왔지만, 이러한 인상적인 논문상 아이디어들이 실제로 수익 창출 규모에서 이륙할 수 있었던 것은 산업 혁명이 일어날 때였습니다.\n\nAI 르네상스의 문제\n\n대부분의 주요 AI 프로젝트는 “인공 일반 지능”에 대해 구축하고 있습니다. 르네상스처럼, 그들은 다 빈치와 같은 모델을 만들고 있습니다. 그들은 예술가, 건축가, 의사 또는 엔지니어의 역할을 수행할 수 있습니다. 그리고, 그들은 “초심주의” 방법을 사용하여 구축하고 있습니다 — 큰 모델, 큰 훈련 인프라, 큰 팀 및 큰 돈이라는 것을 의미합니다. 이는 대규모 언어 모델(LLMs)과 Foundation 모델에도 적용됩니다. 이들의 훈련 비용은 수천만 달러에서 수억 달러로 증가하고 있으며, 인간형 로봇에서도 마찬가지입니다. 비싼 \"모든 것의 로봇\"을 목표로 삼아서 같은 변압기 기반 강화 학습 방식을 백업하고 있습니다.\n\n최근 OpenAI의 Sam Altman이 말한 것처럼,\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\"매년 5억 달러, 50억 달러 또는 500억 달러를 태울지라도 상관없어요. 정말로 상관하지 않아요. 사람들에게 정말 능력있는 도구를 제공하고 이를 활용해 미래를 만들어가도록 하는 것이 정말 좋은 일이라고 생각해요. 여러분과 세계의 모든 다른 사람들의 창의력에 베팅하고 싶어요. 이 문제에 대해 어떻게 대처할지 찾아내기 위해.\n\n막강한 접근법은 AGI를 구축하는 데 맞을 수 있지만, Altman이 가치를 창출될 것으로 가정하는 곳에서, 우리는 많은 노력을 본다는 것을 알아요. 인상적인 프로토타입에서 경제적으로 실현 가능한 제품까지 혁신하는 데는 엄청난 노력이 필요해요. AI 레온아르도 다빈치가 상상한 헬리콥터의 아이디어를 생성한 후, 실제 헬리콥터를 어떻게 만들어야 할까요? 누가 7만 대를 제작하고 판매하며 유지할 것인가요? Tortuga에서는 첫 번째 프로토타입 로봇부터 150대의 저렴하고 특수화된 로봇으로 구성된 상업용 농장 규모의 플릿을 구축했어요. 수백만 개의 딸기를 수확했고, 굉장히 효과적이면서도 저렴한 로봇과 AI/ML 스택을 통해 그렇게 했어요.\n\n우리의 첫 번째 신념은 더 열린, 종합적인 방식으로 혁신하는 것이 가치가 있다는 것이지만, 산업 경제의 기반이 되는 일들을 해결하려고 할 때, 빅 AI는 깊고 구체적이며 반복 가능한 물리적 작업을 해결하려 할 때 덜 효과적이라는 것입니다.\n\n전문화: 르네상스는 산업 혁명이 필요합니다.\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nChatGPT4의 다양한 테스트 결과는 정말 인상적입니다. 마찬가지로 Waymo는 피닉스와 샌프란시스코에서 운영 중입니다. AI 로봇들은 오늘날 현실 세계에서 운전할 수 있습니다. 하지만 3D 물리 세계의 작업은 순수 텍스트나 2차원 도로 시스템의 작업에 비해 자동화하기가 훨씬 더 복잡한 것으로 입증되었습니다. 메타의 주요 AI 과학자인 Yann LeCun은 이 복잡성 차이를 언급했습니다. 현실 세계가 쓰인 세계보다 훨씬 더 복잡하고 \"심층적\"이라고 말했습니다.\n\n토르투가의 맥락에서, 만약 최대주의 AI 휴머노이드인 AI 레오나르도가 ChatGPT4처럼 딸기를 고르는 데 90%의 점수를 얻을 수 있다 해도 충분하지 않습니다. 토륄투가 로봇이 10%의 실수를 하면, 우리는 고객의 수익의 10%를 파괴하고 자동화의 가치에 반대하게 됩니다. 인간은 완벽하지 않습니다. 그래서 우리의 기준은 100%가 아닌 대략 97% 정확도입니다. 그러나 97%는 단순히 \"90%보다 7점 높은 것\"이 아닙니다. 물리적 세계에서는 한 차원 이상 더 어려운 문제입니다. 산업 프로세스에서는 우리 사회의 기초를 이루는 일과 작업들이 매주 수백만 번 실행되어야 하며, 매우 높은 정밀도(95% 이상)로 실행되어야 합니다. 그 이유는 비용, 이윤 및 효율성이 깊게 중요하기 때문입니다.\n\n르네상스 방식의 대규모 AI는 광범위한 기반 모델 기반 접근법을 사용하고 극도로 견고한 데이터셋, 시뮬레이션 환경, 합성 데이터 생성 및 대규모 교육 파이프라인을 사용하여 엣지 케이스와 성능 개선을 위해 큰 비용의 투자주기를 갖습니다. 그러나 거의 모든 로봇 환경에서 특히 농장에서는 매우 미묘하고 동적인 엣지 케이스가 많이 존재합니다. 농장에서는 과일과 식물/농업 구조, 해충, 과일 종류, 그리고 온도, 습도, 햇빛 세기, 바람, 비와 같은 환경 조건과의 상호 작용이 크게 다르고 변하기 때문입니다. 물리학적 모델은 개별 과일이 바람에 흔들리거나 로봇에 닿거나 하나씩 딸릴 때의 상호 작용과 같은 필요한 복잡성을 충분히 전달하지 못하기 때문에 시뮬레이션의 효과는 제한됩니다. 특히 유기적 시스템에 대한 합성 데이터는 우리에게 가장 기본적인 사용 사례를 넘어서는 것이 너무 단순해서 우리에게 혜택을 주지 못합니다. 서로 다른 과일에 대한 견고한 실제 세계 데이터셋은 없고, 현존하는 공개 노력은 우리의 특정 센서, 로봇 구조 또는 인지 방식에 적용되지 않는 \"최소 공통 분모\" 데이터셋입니다. 로봇 공학에서 일반화 학습 접근 방식을 추구하려는 많은 학문적 노력이 있지만, 이러한 도전에 아직까지 좋은 답을 찾지 못했습니다. 이 도전 과제의 섹션에서 다루었던 것처럼요.\n\n이제 두 번째 믿음으로 넘어가봅시다. 인공지능 르네상스는 모든 가능성과 창의성에 대한 것입니다. 그러나 인공지능 산업 혁명은 자원과 전문화에 대해 새롭게 고찰해야 합니다. 저렴하고 확장 가능한 인공지능을 만들기 위한 더 나은 방법은 해결하려는 문제에 대해 더 깊이 파고들어 근본적인 구성 요소들로 줄이고, 그 핵심 문제만 심층적이고 전문화된 방식으로 해결하는 것입니다. 우리는 그것을 꼭 필요한 인공 지능(Essential AI)이라고 부르는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n필수 인공지능: 인공지능 산업혁명을 위한 원칙 세트\n\n필수 인공지능은 세 가지 원칙에 기반을 두고 있습니다:\n\n- 전문화 및 동적입니다. 우리는 동적이지만 구체적인 방법을 사용하여 여러 독립 추론 시스템을 연결하여 특정 지능 로봇 에이전트인 '특정 인공지능'을 만듭니다. 이들 디버들된 모델은 모두 매우 높은 F1 점수를 가지고 있고, 우리의 방식은 각각의 모델에서 정밀도와 재현율 사이의 미묘한 균형을 맞추는 것을 가능하게 합니다. 모델 체인의 정확도에 대한 곱셈 효과에도 불구하고, 움직임 계획 및 생물 환경과의 상호작용만 고려할 때도 전체 품질 값이 97%를 초과합니다. 모델 체이닝을 통해 환각과 블랙박스 효과를 피하고, 각 모델의 재학습 목표가 명확합니다. 보상 학습을 넘어서, 우리의 로봇은 또한 라이브 성능에 대한 피드백을 받아 품질과 성능을 최적화하기 위해 실행할 모델을 결정합니다. 이것은 실제 분야에서 결과를 강화하는 적응적 행동입니다.\n- 자습 및 유연합니다. 우리는 우리의 MLOps 접근 방식에서 빅 에이에이의 최고를 채용하여 특정 모델의 이상치를 자동으로 식별하고 효율적이고 특정한 라벨링 파이프라인을 사용하여 빠르게 다시 훈련시킵니다. 우리는 견고하고 전문화된 Tortuga 데이터를 수집하며, 개발 프로세스에서 가장 효과적인 곳에 시뮬레이션 및 합성 데이터를 적용하며, 실제 세계를 충분히 반영하지 못하는 경우에는 중지합니다. 이는 표준 격리된 특이 케이스뿐만 아니라 새로운 식물 품종 및 새로운 농장 환경과 같은 완전히 새로운 맥락에도 적용되며, 우리 모델의 내장된 이해를 업데이트합니다. 우리는 모든 이를 파트너와 무관한 특정 도구로 수행합니다.\n- 효율적입니다. 특수 모델 체인 방식과 특정하고 동적인 MLOps 도구를 사용하여 우리는 지원 임원의 연봉보다 적은 비용으로 지상 실리콘의 모든 참변 사실 주석을 지원하고, 단지 3명의 엔지니어 지원 스텝으로 모든 모델에 대한 이터레이션을 24시간 이내에 완료할 수 있습니다. 우리는 영원히 변화하는 월계도 툴의 일괄 실행형 생태계를 조합하는 대신, 우리에게 동작하는 매우 집중된 매우 저렴한 시스템 위에 개발함으로써 이를 수행할 수 있습니다. 큰 에이에이 방법보다 수십 배 낮은 비용으로 가능합니다.\n\n우리는 필수 인공지능 로봇들이 현실 세계의 산업 규모 문제를 해결할 것이라 믿으며, 장기간 이 문제에 대한 유일한 올바른 접근 방식일 것이라고 확신합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n인공지능 레오나르도 다빈치 휴머노이드 르네상스 로봇은 가정에서 놀라운 자원이 될 수 있습니다. 비서로서, 대화 상대로서. 심지어 창고 환경에서 더 많은 저거량 작업을 해낼 수도 있을지도 모릅니다. 하지만 하루에 수백만 개의 베리를 수확해야 한다면, 산업혁명처럼 생각하고 그 일을 탁월하게 수행하는 비용 효율적인 로봇을 만들어야 합니다. 제대로 된 산업 작업은 우리 사회의 기반을 이루며, 제조업, 농업, 거래 분야에서의 \"지루하고 더러운, 위험한\" 직업들에 대해 Essential AI가 비용과 효율성 면에서 Big AI나 심지어 AGI보다 우월하게 성과를 내게 될 것입니다.\n\n더 많은 정보를 원하신다면, Big AI에 대해 아래의 글을 참고해 보세요:\n\n- 포브스: 트랜스포머가 인공지능을 혁신했다. 그들을 대체할 것은 무엇인가?\n- Cobot의 Brad Porter: 인간이 할 수 있는 로봇을 위한 위대한 인공지능으로 가는 길\n- Jacob Grow: 대형 언어 모델의 경계와 인공지능의 전진 방향\n","ogImage":{"url":"/assets/img/2024-05-18-EssentialAI_0.png"},"coverImage":"/assets/img/2024-05-18-EssentialAI_0.png","tag":["Tech"],"readingTime":7},{"title":"SLAM을 처음부터 구현해 보기","description":"","date":"2024-05-18 19:19","slug":"2024-05-18-ImplementSLAMfromscratch","content":"\nSLAM (Simultaneous Localization and Mapping)을 위한 솔루션을 구현하는 다양한 방법이 있지만, 구현하기 가장 간단한 알고리즘은 Graph SLAM입니다.\n\nGraph SLAM은 로봇공학에서 사용되는 기술로, 로봇의 궤적을 시간에 따라 동시에 추정하고 환경 안의 랜드마크 위치를 노드와 제약조건으로 나타내는 그래프입니다. 그래프는 로봇의 자세 및 랜드마크 위치를 나타내는 노드와 간격을 제약 조건으로 나타내는 에지로 구성됩니다. 제약 조건은 초기 위치, 상대 움직임 및 상대 측정 제약 조건과 같은 것들을 나타냅니다. 그래프를 최적화함으로써, Graph SLAM은 센서 측정을 가장 잘 설명하는 가장 확률적인 궤적과 랜드마크 위치를 찾으려고 합니다.\n\n## 예제\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nGraph SLAM으로 파고들기 전에, Graph SLAM을 어떻게 구현할지 탐구하는 과정에서 도움이 될 예제를 소개하겠습니다. 이 예제에서는 하나의 차원적인 세계에서 로봇이 이동하는 상황을 살펴보겠습니다. 로봇의 첫 번째 자세는 시간 단계 t0에서이며, 로봇의 자세는 x=2입니다. 이 위치에서 로봇은 랜드마크 L0(예: 나무)를 보고 거리가 9단위 떨어져 있습니다. 그런 다음 로봇은 5단위만큼 앞으로 이동합니다. 이 시점에서 로봇은 x=7에 있어야 하며 랜드마크는 4단위 떨어져 있어야 합니다. 그러나 시간 단계 t1에서 로봇은 랜드마크까지의 거리를 보거나 측정하지 않습니다. 시간 단계 t1에서의 랜드마크 측정 부재는 센서 오류, 가리개, 또는 다른 이유로 인할 수 있습니다. 마지막으로, 로봇은 3단위 앞으로 이동하고 랜드마크를 1단위로 떨어져서 볼 수 있습니다. 이 시점에서 로봇은 x=10에 있어야 하며 랜드마크는 x=11에 있어야 합니다.\n\n![이미지](/assets/img/2024-05-18-ImplementSLAMfromscratch_1.png)\n\n## 제약 조건\n\nGraph SLAM에서는 세 가지 중요한 유형의 제약 조건이 있습니다. 각각의 제약 조건을 자세히 살펴보겠습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 초기 위치 제약: 초기 위치 제약은 로봇이 환경 속 초기 위치에 대한 지식을 나타냅니다. 이는 로봇의 궤적 추정에 고정된 참조점을 제공합니다. 이 제약은 초기 위치 및 방향 추정을 캡처하기 위해 `x, y, θ`로 표현될 수 있습니다. 이 제약을 그래프에 통합함으로써 로봇의 궤적 추정을 기준으로 잡고 추가적인 제약 최적화를 위한 시작점을 제공할 수 있습니다.\n- 상대 운동 제약: 상대 운동 제약은 연이은 시간 단계 간 로봇의 자세 변화에 대한 정보를 캡처합니다. 이러한 제약은 통상 휠 엔코더 또는 IMU와 같은 오도메트리 센서에서 얻어집니다. 오도메트리 센서는 로봇의 움직임에 대한 추정을 제공하며, 위치 및 방향의 변화와 같은 로봇의 움직임을 제공합니다. 연이은 시간 단계 간 오도메트리 측정을 비교함으로써 로봇의 이동을 기술하는 상대 운동 제약을 유도할 수 있습니다. 이러한 제약은 움직임 추정 값의 불확실성을 포착하는 가우시안 분포로 표현됩니다.\n- 상대 측정 제약: 상대 측정 제약은 환경 속 서로 다른 랜드마크나 특징들 간의 상대 위치 또는 거리에 대한 정보를 캡처합니다. 이 제약은 레이저 거리계 또는 카메라와 같은 센서 측정을 통해 얻어집니다. 예를 들어, 로봇이 랜드마크를 관찰하고 해당 랜드마크까지의 거리를 측정한 경우, 이 정보는 상대 측정 제약으로 사용될 수 있습니다. 이러한 제약은 로봇의 궤적에 상대적으로 랜드마크 위치를 추정하는 데 도움을 줍니다.\n\n그래프 SLAM에서는 이러한 모든 제약을 함께 사용하여 환경과 로봇의 궤적에 대한 그래프 표현을 구축합니다. 그래프는 서로 다른 시간 단계의 로봇 자세와 랜드마크 위치를 나타내는 노드 및 그들 사이의 제약을 나타내는 엣지로 구성됩니다.\n\n이것은 테이블 태그를 마크다운 형식으로 변경한 것입니다.\n\n이미지는 [여기](/assets/img/2024-05-18-ImplementSLAMfromscratch_2.png)에서 확인할 수 있습니다.\n\n우리의 예제에서는 5개의 총 제약이 있습니다: 초기 위치 제약 1개, 상대 운동 제약 2개 및 상대 측정 제약 2개입니다. 4개의 상대 제약은 그래프 내의 엣지로 표시됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 그래프 표현\n\n그래프 SLAM에서 환경과 로봇의 궤적은 그래프 구조를 사용하여 표현됩니다. 그래프는 노드와 엣지로 구성되어 있으며, 노드는 로봇의 포즈(위치 및 방향)를 시간에 따라 다른 지점에서 나타냅니다. 엣지는 이러한 포즈 간의 제약 조건이나 측정값을 나타냅니다.\n\n로봇의 포즈뿐만 아니라 환경에 있는 랜드마크나 특징을 나타내는 노드도 그래프에 포함됩니다. 이러한 랜드마크는 로봇이 인식하고 로컬리제이션 및 맵핑에 사용할 수 있는 객체, 관심 지점 또는 기타 독특한 특징일 수 있습니다.\n\n그래프 표현은 엣지를 통해 로봇의 포즈와 랜드마크를 연결하여 센서로부터 얻은 측정값이나 제약 조건을 나타냅니다. 이러한 측정값에는 거리 측정, 방위 측정 또는 로봇과 랜드마크의 상대적인 위치와 방향에 대한 정보를 제공하는 기타 유형의 센서 데이터가 포함될 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어, 로봇이 현재 자세에서 landmark를 관측한다고 가정해 봅시다. 이 관측은 그래프에서 로봇의 자세 노드와 landmark 노드 사이에 엣지를 생성합니다. 이 엣지는 센서로부터 얻은 측정값을 나타내며, 이를 통해 로봇과 landmark 간의 상대적인 위치와 방향에 대한 정보를 제공합니다.\n\n이러한 측정값을 그래프에 통합함으로써 SLAM 알고리즘은 로봇의 가장 가능성 있는 궤적과 측정값에 의해 적용된 제한 조건을 가장 잘 만족하는 환경 지도를 추정할 수 있습니다. 그래프 최적화 과정은 예측된 측정값과 센서로부터 실제로 얻은 측정값 간의 오차를 최소화하기 위해 그래프 내의 자세와 landmark 위치를 조정하는 것을 포함합니다.\n\n## 행렬과 벡터 표현\n\n그래프 SLAM에서는 로봇의 자세와 landmark 간의 관계를 모델링하기 위해 행렬과 벡터 표현을 사용합니다. 이러한 표현은 SLAM 문제를 해결하는 데 도움이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n행렬 표현부터 시작해봅시다. Graph SLAM에서 우리는 정보 행렬이라고 하는 행렬을 만듭니다. 이 행렬은 Ω (오메가)로 표시되며 서로 다른 변수들 간의 제약 조건이나 관계를 나타냅니다. 각 변수는 지도상의 로봇 pose나 landmarke에 해당합니다.\n\n정보 행렬은 정사각 행렬이며, 그 크기는 SLAM 문제에서 변수의 수에 따라 달라집니다. 그래프에 n개의 노드가 있는 Graph SLAM 문제의 경우, n x n 크기의 정보 행렬을 갖게 됩니다. 이 행렬의 요소들은 변수들 간의 관계에 대한 정보를 인코딩합니다. 예를 들어, 두 변수가 높은 상관 관계를 가진 경우, 정보 행렬의 해당 요소는 더 높은 값을 갖게 됩니다.\n\n이제 벡터 표현으로 넘어가 봅시다. Graph SLAM에서 우리는 정보 벡터라고 하는 벡터를 생성합니다. 이것은 ξ (크싸이)로 표시되며, SLAM 문제에서 우리가 한 측정치나 관측치를 나타냅니다. 벡터의 각 요소는 특정 측정치나 관측치에 해당합니다. 그래프에 n개의 노드가 있는 Graph SLAM 문제의 경우, n x 1 크기의 정보 벡터를 갖게 됩니다.\n\n정보 벡터에는 SLAM 문제의 측정치와 변수들과의 관계에 대한 정보가 포함되어 있습니다. 이는 우리가 측정치를 SLAM 문제에 통합하고 로봇 포즈와 랜드마크의 추정치를 업데이트하는 데 도움이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저희 예제에서는 그래프에 4개의 노드가 있으므로 4 x 4 행렬을 초기화합니다. 다음은 정보 행렬입니다:\n\n```js\n// 0으로 채워진 4x4 행렬\n     + -- + -- + -- + -- +\n     | t0 | t1 | t2 | L0 |\n+ -- + -- + -- + -- + - -+\n| t0 | 0  | 0  | 0  | 0  |\n| t1 | 0  | 0  | 0  | 0  |\n| t2 | 0  | 0  | 0  | 0  |\n| L0 | 0  | 0  | 0  | 0  |\n```\n\n저희 예제에서는 또한 그래프에 4개의 노드가 있으므로 4 x 1 벡터를 초기화합니다. 다음은 정보 벡터입니다:\n\n```js\n// 0으로 채워진 4x1 벡터\n+---+\n| 0 |\n| 0 |\n| 0 |\n| 0 |\n+---+\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 그래프 SLAM 알고리즘\n\n우리가 정보 행렬과 벡터를 선언하면, 초기 위치 제약을 행렬과 벡터에 적용해야 합니다. 예를 들어, 초기 위치인 2를 사용하여 정보 행렬을 업데이트하려면, 간단한 선형 방정식을 만들 것입니다:\n\n이제 우리의 간단한 선형 방정식과 그 계수 `1,0,0,0;2`를 가지고 t0에 해당하는 행에 추가해봅시다:\n\n```js\n// 오메가 행렬 (결과)\n     + -- + -- + -- + -- +\n     | t0 | t1 | t2 | L0 |\n+ -- + -- + -- + -- + - -+\n| t0 | 1  | 0  | 0  | 0  |\n| t1 | 0  | 0  | 0  | 0  |\n| t2 | 0  | 0  | 0  | 0  |\n| L0 | 0  | 0  | 0  | 0  |\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n// Xi vector (결과)\n+---+\n| 2 |\n| 0 |\n| 0 |\n| 0 |\n+---+\n```\n\n일반화하기 위해 여기에 초기 의사 코드가 있습니다:\n\n```js\nvoid GraphSLAM(G, startPose) {\n    // Omega와 Xi 선언\n    Omega = new Matrix(n,n)\n    Xi = new Vector(n)\n\n    // 초기 위치 제약\n    Omega['t0','t0'] = 1\n    Xi['t0'] = startPose\n\n    // 그래프 최적화\n    Mu = GraphOptimization(Omega, Xi, G)\n    return Mu\n}\n```\n\n여기서부터 그래프 최적화에 대해 논의해야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 그래프 최적화\n\n그래프의 초기 매트릭스 및 벡터 표현이 준비되면, 그래프 최적화를 수행해야 합니다. 그래프 SLAM에서의 그래프 최적화는 센서 측정을 기반으로 그래프를 반복적으로 업데이트하여 로봇 자세와 랜드마크 위치의 추정치를 개선하는 과정입니다. 이 과정은 측정 업데이트와 상태 업데이트라는 두 가지 주요 단계로 구성됩니다.\n\n- 측정 업데이트: 측정 업데이트 단계에서는 그래프의 엣지를 반복하며 정보 매트릭스에 제약 조건을 추가합니다. 이러한 제약 조건은 센서에서 얻은 측정치(예: 거리 측정 또는 방향 측정)를 나타냅니다.\n- 상태 업데이트: 상태 업데이트 단계에서는 선형 방정식 체계를 해결하여 그래프의 오차를 최소화하는 최적 로봇 자세와 랜드마크 위치를 추정합니다. 이는 정보 매트릭스의 역행렬을 취하고 정보 벡터와 곱하여 수행됩니다.\n\n다음은 그래프 최적화를 위한 고수준의 의사 코드 예시입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nvoid GraphOptimization(Omega, Xi, G) {\n    Omega, Xi = MeasurementUpdate(Omega, Xi, G);\n    Mu = StateUpdate(Omega, Xi);\n    return Mu;\n}\n```\n\n측정 및 상태 업데이트에 대해 더 자세히 알아보겠습니다.\n\n## 측정 업데이트\n\n측정 업데이트에서는 그래프 데이터를 사용하여 정보 행렬 및 벡터 데이터를 정의합니다. Omega는 선형 방정식의 계수를 나타내는 정보 행렬이고, Xi는 해당 방정식의 상수항을 나타내는 정보 벡터입니다. G는 측정치(예: 거리)를 나타내는 엣지 가중치를 포함하는 그래프입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n각 모서리는 일련의 선형 방정식을 정의하는 데 도움이 됩니다. 예를 들어, 모서리 t0-t1로 정보 행렬을 업데이트하려면 두 개의 선형 방정식을 만들겠죠:\n\n이제, 첫 번째 선형 방정식과 그 계수 `1,-1,0,0;-5`를 가져와서 t0에 해당하는 행에 추가해봅시다:\n\n```js\n// 오메가 행렬 (결과)\n     + -- + -- + -- + -- +\n     | t0 | t1 | t2 | L0 |\n+ -- + -- + -- + -- + - -+\n| t0 | 2  |-1  | 0  | 0  |\n| t1 | 0  | 0  | 0  | 0  |\n| t2 | 0  | 0  | 0  | 0  |\n| L0 | 0  | 0  | 0  | 0  |\n```\n\n```js\n// 시 벡터 (결과)\n+---+\n|-3 |\n| 0 |\n| 0 |\n| 0 |\n+---+\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 두 번째 선형 방정식의 계수인 `-1, 1, 0, 0; 5`를 가져와 t1에 대응하는 열에 추가해 봅시다:\n\n```js\n// 오메가 행렬 (결과)\n     + -- + -- + -- + -- +\n     | t0 | t1 | t2 | L0 |\n+ -- + -- + -- + -- + - -+\n| t0 | 2  |-1  | 0  | 0  |\n| t1 |-1  | 1  | 0  | 5  |\n| t2 | 0  | 0  | 0  | 0  |\n| L0 | 0  | 0  | 0  | 0  |\n```\n\n```js\n// 크시 벡터 (결과)\n+---+\n|-3 |\n| 5 |\n| 0 |\n| 0 |\n+---+\n```\n\n일반적인 상황을 이해하기 위해 의사 코드를 보여 드리겠습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nvoid MeasurementUpdate(Omega, Xi, G) {\n    for each edge:\n        // LinEq 1: src - dst = -weight\n        Omega[edge.src, edge.src] += 1\n        Omega[edge.src, edge.dst] += -1\n        Xi[edge.src] += -edge.weight\n        // LinEq 2: dst - src = weight\n        Omega[edge.dst, edge.dst] += 1\n        Omega[edge.dst, edge.src] += -1\n        Xi[edge.dst] += edge.weight\n    return Omega, Xi\n}\n```\n\n보시다시피, 측정 업데이트 프로세스는 그래프 G의 각 엣지에 대해 반복하는 것을 포함합니다. 각 엣지마다 코드는 두 단계를 수행합니다. 먼저, 엣지의 원본 노드에 해당하는 행에 대한 Omega 및 Xi 값을 업데이트합니다. 그 다음, 엣지의 대상 노드에 해당하는 행에 대한 Omega 및 Xi 값을 업데이트합니다. 두 단계 모두 엣지의 가중치는 두 노드 사이(예: 거리)의 측정을 나타냅니다.\n\n이러한 단계는 측정에 의해 부과된 제약 조건이 Omega 및 Xi 행렬에 올바르게 표현되도록 보장합니다. 모든 엣지를 반복한 후 함수는 업데이트된 Omega 및 Xi 행렬을 반환합니다.\n\n## State Update\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 시점에서, Omega와 Xi를 완전히 정의했습니다. 그러니 Mu를 해결하기 위해 방정식 체계를 해결하기만 하면 됩니다:\n\n여기서 Mu는 업데이트된 상태 추정을 나타냅니다. Mu를 구하기 위해서는 단순히 Omega를 역행렬로 변환해야 합니다:\n\n```js\nvoid StateUpdate(Omega, Xi) {\n    Mu = Omega.invert() * Xi\n    return Mu\n}\n```\n\n제공된 의사 코드는, 공분산 행렬(Omega)의 역행렬을 측정 벡터(Xi)로 곱하여 상태를 업데이트합니다. 결과인 Mu는 로봇의 자세 및 랜드마크 위치의 상태 추정을 나타냅니다. 이는 시스템의 상태를 정의하는 모든 변수의 값이 포함된 벡터입니다. 우리의 예시에서, 이는 Mu의 예상 값입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n// Mu vector (result)\n+----+\n|  2 |\n|  7 |\n| 10 |\n| 11 |\n+----+\n```\n\n이 예에서 첫 번째 요소는 t0에서 로봇의 위치이며, 두 번째 요소는 t1에서 로봇의 위치이고, 세 번째 요소는 t2에서 로봇의 위치이며, 네 번째 요소는 landmark(L0)의 위치입니다.\n\n## 토론\n\nGraph SLAM 알고리즘은 정확한 답변을 제공하지 않을 수 있지만, 근접한 결과를 제공합니다. SLAM 알고리즘의 결과는 알고리즘에 공급되는 측정값의 품질에 따라 달라집니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 내용은 간단한 1차원 예시에 불과했습니다. 이를 쉽게 3차원 공간으로 확장할 수 있고 로봇이 바라보는 방향을 설명하는 추가적인 방향 차원도 포함할 수 있습니다.\n\n만약 SLAM 용어 중 이해되지 않는 것이 있다면, SLAM 개요를 다시 참고해 주세요.\n\n이 글이 마음에 드셨다면, ❤를 눌러 다른 사람들이 이를 발견하는 데 도움을 주세요!\n","ogImage":{"url":"/assets/img/2024-05-18-ImplementSLAMfromscratch_0.png"},"coverImage":"/assets/img/2024-05-18-ImplementSLAMfromscratch_0.png","tag":["Tech"],"readingTime":13}],"page":"99","totalPageCount":120,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"99"},"buildId":"JlBEgQDLGRx6DYlBnT8eD","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>