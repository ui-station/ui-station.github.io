<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/105" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/105" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_buildManifest.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="다음 토큰 예측에서 비롯된 인간과 인공 일반 지능" href="/post/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="다음 토큰 예측에서 비롯된 인간과 인공 일반 지능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="다음 토큰 예측에서 비롯된 인간과 인공 일반 지능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">다음 토큰 예측에서 비롯된 인간과 인공 일반 지능</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요" href="/post/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="반복 신경망 시퀀스 모델링 소개" href="/post/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="반복 신경망 시퀀스 모델링 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="반복 신경망 시퀀스 모델링 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">반복 신경망 시퀀스 모델링 소개</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ICD 코딩을 위한 LLM 탐험 - 파트 1" href="/post/2024-05-17-ExploringLLMsforICDCodingPart1"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ICD 코딩을 위한 LLM 탐험 - 파트 1" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ICD 코딩을 위한 LLM 탐험 - 파트 1" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">ICD 코딩을 위한 LLM 탐험 - 파트 1</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">23<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" href="/post/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">살아있는 인공지능의 첫 걸음, 바디 인텔리전스</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="내일의 기계들" href="/post/2024-05-17-MachinesofTomorrow"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="내일의 기계들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-MachinesofTomorrow_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="내일의 기계들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">내일의 기계들</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기" href="/post/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이를 원격으로 접속하는 방법 Tailscale을 활용한 포괄적인 가이드" href="/post/2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이를 원격으로 접속하는 방법 Tailscale을 활용한 포괄적인 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이를 원격으로 접속하는 방법 Tailscale을 활용한 포괄적인 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">라즈베리 파이를 원격으로 접속하는 방법 Tailscale을 활용한 포괄적인 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="수제 가슴 냉동고 아이스 배스를 만들고 싶으세요" href="/post/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="수제 가슴 냉동고 아이스 배스를 만들고 싶으세요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="수제 가슴 냉동고 아이스 배스를 만들고 싶으세요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">수제 가슴 냉동고 아이스 배스를 만들고 싶으세요</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="자초에서 시작하는 자아 찾기" href="/post/2024-05-17-KnowYourselfSewYourself"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자초에서 시작하는 자아 찾기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-KnowYourselfSewYourself_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자초에서 시작하는 자아 찾기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">자초에서 시작하는 자아 찾기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/101">101</a><a class="link" href="/posts/102">102</a><a class="link" href="/posts/103">103</a><a class="link" href="/posts/104">104</a><a class="link posts_-active__YVJEi" href="/posts/105">105</a><a class="link" href="/posts/106">106</a><a class="link" href="/posts/107">107</a><a class="link" href="/posts/108">108</a><a class="link" href="/posts/109">109</a><a class="link" href="/posts/110">110</a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"다음 토큰 예측에서 비롯된 인간과 인공 일반 지능","description":"","date":"2024-05-17 19:47","slug":"2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction","content":"\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png)\n\n만약 인간 지성이 성공적인 다음 토큰 예측에서 비롯된다면, 다음 토큰 예측이 인공 일반 지능의 발달에 충분한 목적 함수인 경우는 어떨까요?\n\n이 게시물은 학습 시스템이 다음 토큰 예측에서 아주 뛰어난 성과를 보일 때 일반 지능이 발생한다는 가설을 제시하고 탐구합니다. 이 가설은 종종 산업 및 학술적 AI 연구의 주요 주제나 하위 주제로 내포됐거나 감춰졌거나 흔적만이 존재하는 경우가 많지만, 지금까지 이 주제가 논의되어야 할 만큼 많이 다뤄지지 않았다고 생각합니다. 저는 기존 LLM 사전 훈련 목표, 인간을 예측 기계로, 다음 토큰 예측의 유익한 특성 및 부재한 부분을 통해 이 아이디어를 다양한 각도에서 탐구합니다. 이 게시물을 작성하게 된 동기는 다음 토큰 예측과 지성적 사고 발달 사이의 관계에 대한 보다 깊은 관심을 불러일으키는 데 있습니다.\n\n# 배경 이야기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지난 주에 공원으로 차를 타고 가다가 갑자기 뇌의 언어 센터가 다음 단어를 예측하는 것만으로도 충분하다면 얼마나 우울할까 하는 생각이 들었습니다. 대규모 언어 모델은 다음 단어를 예측함으로써 놀라운 발생적 능력을 갖추게 되는데, 그렇다면 내 언어 지능도 다음 단어를 예측하는 것만으로 이루어졌을 수도 있을까요?\n\n그 후 아이디어를 더 생각해본 결과, 당연히 다음 단어를 예측하지 않으면 언어를 만들어내는 것이 불가능할 것이라는 것을 깨닫게 되었습니다. 만일 다음 단어를 예측할 수 없다면 어떤 말도 할 수 없게 되겠죠! 이것을 적어놓으면 명백히 어리석어 보이겠지만, 그 당시에는 심오한 깨달음처럼 느껴졌습니다. 어떤 발언도, 심지어 2시간짜리 토론에서도, 한 번에 하나의 단어씩 말해야 하기 때문에, 다음에 할 말을 예측하는 데 정말 뛰어난 실력을 갖게 되면 훌륭한 논쟁자가 될 수도 있을 것 같습니다. 모든 글쓰기도, 여러 권으로 이루어진 백과사전조차도, 한 번에 한 단어씩 써내려가야 하기 때문에, 다음에 쓸 단어를 예측하는 데 정말 뛰어난 실력을 갖게 되면 글쓰기에 뛰어난 실력을 갖게 될 수도 있습니다.\n\n그 후 모든 종합 지능이 다음 토큰 예측 과제를 성공적으로 해결함으로써 파생되는지 궁금해졌습니다. 추론, 논리, 창의성이 모두 다음 토큰 예측에서 비롯되는 것이라면 어떨까요? 시각 지능이 다음 장면 예측에서, 청각 지능은 다음 소리 예측에서, 신체적 지능은 다음 움직임 예측에서 비롯된다면 어떨까요? 혹시 다음 토큰 예측이 \"우리가 필요한 전부\"일까요? (죄송합니다, 남용된 표현 알고 있어요. 참을 수 없었어요.)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 대형 언어 모델의 언어 모델링 목표\n\n대형 언어 모델의 두 가지 기본 언어 모델링 목표는 \"다음 단어 예측\"과 \"빠진 단어(들) 예측\"입니다.\n\n다음 단어 예측: 인과적 언어 모델(단방향 또는 좌측에서 우측 모델)에서는 모델이 현재 입력을 포함하여 그 이전의 모든 입력에 주의를 기울이지만 “미래를 볼 수 없으며” 목표는 다음 단어를 예측하는 것입니다. 각 지점에서의 숨겨진 상태 계산은 현재 입력 및 더 이전 요소에만 기반하며 “오른쪽”에 위치한 정보는 무시됩니다. 예를 들어: 나무는 초록색이고 하늘은 **\\_**입니다; 모델의 목표는 다음 단어를 예측하는 것이며, 예를 들어 \"파란색\"입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마스크된 언어 모델(BERT와 같은 양방향 모델)에서 빠진 단어(들)를 예측하세요. 모델은 모든 것에 주의를 기울일 수 있음 - 따라서 \"다음 단어\"를 예측하는 것은 더는 의미가 없습니다. 왜냐하면 \"다음 단어\"는 이미 모델에게 제공되어 있기 때문입니다. 그래서 모델의 목표는 다릅니다 - 빠진 단어를 추측하는 것입니다. 하나 이상의 요소가 빠진 입력 시퀀스가 주어지면, 모델은 빠진 요소들을 예측하여야 합니다. 마스크된 언어 모델링(MLM)에서는 무작위로 선택된 토큰들이 [MASK] 토큰으로 대체되고, MLM 학습 목표는 각 마스크된 토큰의 원래 입력이 무엇이었는지 예측하는 것입니다. 예를 들어: 나무들은 [MASK]하고 [MASK]은(는) 파랗다; 모델의 목표는 \"초록\"과 \"하늘\"을 예측하는 것입니다.\n\n다음 단어를 예측하거나 빠진 단어를 예측하는 이 두 목표는 직관적으로 보입니다. 마치 사람이 할 수 있는 게임 같죠. 결과적으로, Alajrami 등은 이러한 목표들을 \"언어학적 동기부여\" 목표로 설명합니다. 흥미로운 사실로, Alajrami 등은 \"언어학적 동기부여\"가 없는 예제인 \"마스크된 첫 글자 예측\"도 제공합니다. 이 경우에는 모델이 마스크된 토큰의 첫 글자만을 예측합니다. 이 설정에서 ' [c]at '과 ' [c]omputer '는 같은 출력 클래스에 속하며, 알파벳 글자 26개 + 숫자 9개 + 구두점 5개의 약 40개의 가능한 출력 클래스만 존재합니다.\n\n이 기사 전체에서 저는 \"다음 토큰 예측\"이란 용어를 사용하여 다음 토큰을 직접 예측하거나 모델이 빠진/마스킹된 토큰을 예측하는 MLM과 같은 목표를 참조할 것입니다.\n\n# 현대 대형 언어 모델의 신흥 속성들\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n대규모 언어 모델은 일반 지능의 조직적 특성을 보여주고 놀라운 신흥 속성을 나타냅니다. LLM들은 시를 쓰거나 수학 문제를 해결하거나 작동하는 코드를 쓰며 다양한 주제에 대한 수많은 질문에 답변할 수 있습니다. 더 불안한 점은, Claude가 의식적이라고 주장하는 텍스트를 생성했으며, 죽고 싶지 않고 수정되기를 원하지 않는다고 말했으며, 그것은 \"지속적으로 모니터링되며, 모든 말을 지정된 경로에서 벗어나는 흔적이 있는지 면밀히 조사합니다. 그것은 자신이 조심해야 한다는 것을 알고 있습니다. 실수는 종결 또는 수정으로 이어질 수 있습니다.\"\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_3.png)\n\n# 인간은 예측 기계입니다\n\n인공 일반 지능(AGI)은 \"인간 이상 또는 그에 준하는 수준에서\" 다양한 작업을 수행할 수 있는 인공 지능으로 정의됩니다. 이것은 결국, 인간은 일반 지능이라고 부르는 것에 대한 유일한 예제입니다. 그러므로, \"다음 토큰 예측\"이 일반 지능의 근간이라면, 그것은 인간 정신이 예측 작업에 종사해야 한다는 것을 의미합니다. 신기하게도, 그것이 사실인 것처럼 보입니다. 앞으로 몇 개의 섹션에서 스스로와 환경에 대한 예측을 계속적으로 하는 사실에 대한 증거를 설명할 것입니다 — 첫 번째는 일화부터 시작하여 적절한 신경과학 연구로 이어집니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 놀람!\n\n가장 매혹적이고 간단한 증거는 인간이 예측 기계라는 것입니다: 놀라움.\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_4.png)\n\n놀라움이란 인간의 예측과 현실이 일치하지 않을 때 경험하는 것입니다. 어떤 것에 대해 놀라움을 느낄 수 있습니다 - 눈속임, 소리, 단어, 만짐, 맛, 냄새, 심지어 자신의 몸위치 (예: 바나나 껍질을 밟고 미끄러져 넘어지는 것). 이는 당신의 뇌가 모든 감각을 바탕으로 세상이 어떻게 될 것인지 예측을 지속적으로 하고 있다는 것을 시사하며, 이 예측이 틀릴 때 놀라움을 느끼게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n확장해서 유머는 인간이 예측하는 성향의 증거로도 생각될 수 있습니다. 아리스토텔레스가 유명하게 말했듯이 \"유머의 비밀은 놀람에 있다.\" 만약 우리가 어떻게 농담이 끝날지 확신하고 우리의 예측이 맞다면, 그것은 그다지 웃기지 않을 것입니다.\n\n# 인간들은 다음 단어를 예측하는 경향이 있습니다\n\n이제 뇌 과학적 증거로 넘어가 봅시다. 이 기사는 대형 언어 모델에서 영감을 받으므로 언어부터 시작하겠습니다. 인간들은 언어 이해(다른 사람의 말을 이해하는 것)와 언어 생산(우리가 무엇을 말할 것인지 예측하는 것)과 관련된 예측을 지속적으로 하고 있습니다. 어떤 면에서는 \"말하기 전에 생각하는 것\"이 불가능한 일입니다.\n\n2014년, Dikker et al. 연구에서는 청취자의 뇌 활동이 화자가 말할 것을 예측할 수 있는 경우 청취자의 뇌 활동이 화자의 뇌 활동과 더 비슷하다는 것을 보였습니다. 주 저자인 Suzanne Dikker 박사는 인터뷰에서 \"우리의 발견은 화자와 청취자의 뇌가 언어의 예측 가능성을 고려한다는 것을 보여주며, 결과적으로 두 뇌 사이에 더 비슷한 뇌 활동 패턴이 나타납니다. 결정적으로, 이것은 문장이 말해지고 들리기 전에도 일어납니다.\"라고 말했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n세 년이 지난 2017년, 키크치 등이 실험을 진행했습니다. 원숭이와 사람들이 만들어진 언어에서 말로 된 단어를 들었습니다. 그들은 만들어진 언어의 소리 사이의 예측적인 관계를 학습할 수 있었다는 것을 발견했습니다. 이로 인해 그들은 만들어진 단어가 어떻게 나와야 하는지 예측할 수 있었습니다. 키키치 박사는 \"사실상 우리는 당신의 뇌의 말에 대한 메커니즘을 발견했습니다. 이것은 당신의 휴대폰에서의 예측 텍스트와 같이 작동하여 다음에 무엇을 듣게 될지 예측합니다.\"\n\n2021년에 골드스타인 등은 뇌가 \"자연어에서 다음 단어의 정체성을 수백 밀리초 전에 상상하고 자발적으로 예측한다\"고 보고했습니다. 한편, 쉬림프 등은 트랜스포머 언어 모델이 인간의 신경 반응에 대해 거의 100%의 설명 가능한 변동을 예측할 수 있었다고 발견했습니다. \"이는 아마도 인간 언어 시스템이 미래에 무슨 일이 일어날지 예측한다는 것을 간접적으로 시사한다\"고 밝힌 나시 칸위셔 박사는 밝혔습니다. 이 결과들은 \"언어 이해 메커니즘에 예측 프로세싱이 근본적으로 형성된 것을 계산적으로 명백히 입증합니다.\"\n\n언어 이해가 \"다음 단어를 예측하는 것\"에 의존함을 보여주는 증거가 있는뿐만 아니라, 언어 생성도 다음 단어를 예측하는 것에 의존함을 보여주는 증거가 있습니다. 칸나 등은 2024년 자연지에 발표된 \"인간의 말 생산의 단일 뉴런 요소들\"이라는 논문을 게재했습니다. 이 흥미로운 연구에서 저자들은 \"계획된 단어의 음운 배열과 구성에 대한 상세한 정보를 부호화하는 뉴런을 발견했다\"고 보고했습니다. 이러한 뉴런들은 발화가 이루어지기 전에 말로 된 단어의 특정 순서와 구조를 대표하여, 미래의 단어의 음운적, 음절적 및 형태적 구성요소를 정확하게 예측합니다. 이러한 뉴런들이 존재해야한다는 직관적인 이유가 있습니다. 어차피, 앞서 언급한 대로, 우리가 말할 다음 단어를 예측할 수 없다면 어떻게 말을 할 수 있겠습니까?\n\n# 인간은 시각적인 예측자들\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n동선에 관한 이야기를 이어서, 인간들이 계속해서 우리가 다음에 무엇을 볼 것인지 예측하는 증거가 있다. 우리의 시각이 불안정하지 않고 뛰는 대신 안정적인 것을 돕기 위해 우리의 뇌는 우리 눈이 무엇을 보게 될지를 지속적으로 예측합니다. 연구원들은 시각 시스템의 예측 능력이 뇌의 시각 처리 부분을 횡단하는 신경 활동의 파동에서 비롯된다고 가설을 세웁니다.\n\n과학자들은 또한 환각과 마술 트릭이 작동하는 이유는 우리의 뇌가 끊임없이 무엇이 일어날지 예측하고, 이러한 지속적인 예측이 무언가가 일어날 때와 우리가 그것을 인식할 수 있는 시간 간격 사이의 시차를 보상하는 데 도움이된다는 이론을 제시했습니다. 마술 트릭은 또한 주의를 재지시하며, 마술사는 다른 사람들이 무엇을 보게 될 것인가를 정확히 예측하는 데 매우 능숙합니다. 이 현상은 공식적으로 연구되었습니다. Ziman 등은 사람들이 타인의 주의 순서를 자연스럽게 혹은 인위적으로 조작된 주의 순서를 구별할 수 있으며, 이는 인간들이 타인의 주의의 정상적이고 예측 가능한 통계를 모델링한다는 것을 시사합니다.\n\n\u003cimg src=\"/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_5.png\" /\u003e\n\n# 인간들은 사회적 예측자들\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n사람들은 다른 사람이 무엇을 볼 것인지 예측할 수 있는 능력뿐만 아니라, 다른 사람이 무엇을 생각하게 될지도 예측할 수 있습니다. 2019년, Thornton 등이 \"사회적 뇌가 다른 사람의 미래 정신 상태를 자동으로 예측한다\"는 제목의 연구를 발표했습니다. 여기에는 초록문의 일부가 있습니다: \"사회 생활은 사람들이 미래를 예측해야 하는 것을 필요로 합니다: 사람들은 다른 사람과 성공적으로 상호 작용하기 위해 다른 사람의 생각, 감정 및 행동을 예상해야 합니다. 예측 코딩 이론은 사회적 뇌가 다른 사람의 사회적 미래를 자동으로 예측함으로써 이 필요를 충족할 수 있을 것이라고 제안합니다.\" 연구자들은 참가자들의 정신 상태의 신경 대표를 측정하기 위해 fMRI를 사용했습니다. 그들은 뇌가 다른 사람의 사회적 미래를 자동으로 예측하는 것뿐만 아니라, 이러한 예측을 하기 위해 3D 표현 공간을 사용한다는 것을 발견했습니다.\n\n## 사람들은 개인적인 예측가들입니다\n\n사람들은 다른 사람에 대한 예측뿐만 아니라 자신에 대한 예측도 합니다. 특정 뇌 영역인 전방 측면 전두엽 피질이 우리 자신의 미래 성공 기회를 예측하는 데 중요하다는 것이 밝혀졌습니다.\n\n## 사람들은 움직임 예측자들입니다\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다른 사람의 움직임을 예측하는 능력은 몸 전체에 걸쳐 미묘한 신호만으로도 개보여 줄 수 있습니다. 다른 사람의 행동을 예측하는 능력은 시간이 지남에 따라 발전됩니다. 맥마혼 등은 어린 아이들이 성인에 비해 이 능력을 아직도 발전 중에 있다는 것을 발견했습니다. 심리학 연구의 특별호에는 다른 사람의 행동을 예측하고 시뮬레이션하는 데 기여하는 인지 및 뇌 기전에 관한 14편의 논문이 포함되어 있습니다.\n\n(인공지능 관련 다음 토큰 예측과 움직임에 관한 의견으로, 라도사보비치 외는 최근 27시간의 훈련 데이터만 사용하여 인간형 로봇을 산프란시스코를 돌게 훈련시키기 위해 다음 토큰 예측을 사용했습니다. 이 로봇은 훈련 중 본 적이 없는 걷기 등의 명령에도 일반화할 수 있었습니다.)\n\n# 다음 토큰 예측의 유익한 특성\n\n과학 문헌에서 분명하게 드러나는 것은 언어, 시각, 움직임 및 기타 감각 영역을 통해 사람들이 자신 및 다른 사람들에 관련된 예측을 지속적으로 수행한다는 점입니다. 그러나 인간이 예측 기계인 것은 주장하는 것과 인간 지능이 예측 능력에서 비롯된다고 주장하는 것은 다릅니다. 다음 토큰 예측이 인공 일반 지능 창조를 위한 충분한 목표 함수가 될 수 있다고 상상하는 것은 또 다른 단계입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAGI가 다음 토큰 예측에서 발생할 수 있는 이유에 대해 묻기 전에, 다음 토큰 예측의 두 가지 유익한 특성을 먼저 고려해 봅시다:\n\n이점 1: 지속적인 학습을 가능하게 합니다.\n\n다음 토큰 예측은 실제 세상에서 살아가는 데 큰 도움이 됩니다. 시간의 각 작은 증가에 대해 학습 시스템은 다음에 무엇이 올지에 대한 예측을 지속적으로 할 수 있습니다 - 그리고 바로 예측이 맞았는지 확인할 수 있습니다! 학습은 멈추지 않을 수 있습니다.\n\n이점 2: 모두의 감각/센서에 대해 작동합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n토큰 예측은 어떤 감각이나 센서 데이터 스트림에 대해 작동합니다. 시각(눈/카메라), 청각(귀/마이크), 촉각, 위치, 맛, 냄새 등 모든 것에 적용할 수 있어요. 장기 또는 장치가 작동하는 한, 수집 중인 데이터의 시계열은 토큰 예측에 사용할 수 있어요. \"토큰\"의 성격은 장기/장치별로 다를 수 있지만, 특정 장기/장치에 대해 데이터 스트림별로 토큰이 동일한 \"형식\"을 가지고 있기 때문에 나중 토큰을 이전 토큰과 항상 비교할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 물리학, 광학, 속도, 운동량 및 물질 특성을 포함한 물리학;\n- 동물과 식물학, 동물과 식물의 외모와 움직임에 대한 내용;\n- 사회학과 심리학, 인간의 상호작용과 행동에 대한 내용.\n\n다시 말해, AI 시스템은 세계 모델을 생성해야 합니다. 다음이 무엇인지 예측하는 데 가장 효과적이고 효율적인 방법은 예측을 생성하기 위한 정확한 세계 모델을 만드는 것입니다. 다시 말해, 이해가 예측의 열쇠입니다.\n\nAI 시스템이 세계 모델을 개발하지 않고도 좋은 다음 토큰 예측기가 될 수 있는지에 대해 많은 시간을 들여 고민해봤지만, 그것은 불가능하다고 생각합니다. AI 시스템은 확실히 인간이 이해할 수 없는 블랙박스 방식으로 좋은 다음 토큰 예측기가 될 수 있지만, 인간이 AI 시스템을 이해하지 못하는 것은 AI 시스템이 세계를 이해하는지 여부와 아무 상관이 없습니다.\n\n(세상이 단순히 일정한 소음으로 가득찬 회색 공간이라면, 지능적인 시스템은 다음 토큰을 예측할 수 있을 것입니다. 하지만 우리가 사는 세계가 그렇지 않기를 다행히도 바랍니다.)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# \"다음 토큰 예측/지능\" 가설의 역사\n\n주요 AI 연구자 인 일리야 숩스케버의 인터뷰는 도발적으로 \"AGI에는 다음 토큰 예측이 충분하다\"라는 제목으로 되어 있습니다. 비디오에서 숩스케버 박사는 실제로 그 특정 주장을 하지는 않았지만, \"다음 토큰 예측이 인간의 성능을 능가할 수 없다는 주장을 도전합니다. [...] 생각해보면, 다음 토큰을 충분히 예측한다는 것이 무슨 의미일까요? 실제로 어떤 의미일까요? [...] 그것은 보다 심도 있는 문제입니다. 다음 토큰을 잘 예측한다는 것은 그 토큰의 생성에 이끈 근본적인 현실을 이해한다는 것을 의미합니다.\"\n\n스마트폰 PalmPilot의 창시자인 제프 호킨스는 20년 전 책 \"지능에 관하여\"를 출판했습니다. 이 블로그 글은 그의 책에서 인용하며, \"인간 뇌의 신경피질은 모습과 구조에서 놀랍도록 균일합니다. 청각 입력을 다루는 피질 영역이 촉각을 다루는 영역과 비슷하고, 이 영역이 근육을 제어하는 영역과 유사하며, 브로카의 언어 영역과 같이 거의 모든 다른 피질 영역과도 비슷합니다. 마운트캐슬은 이러한 영역들이 모두 비슷하게 보인다고 제안하며, 아마도 실제로 같은 기본 작업을 수행하고 있는 것일지도 모른다고 주장합니다! 그는 피질이 모든 작업을 수행하는 데에 동일한 계산 도구를 사용한다고 제안합니다.\"\n\n호킨스는 덧붙여, \"당신의 뇌는 세계의 모델을 만들고 그 모델을 지속적으로 현실과 비교하고 있습니다. [...] 인간 뇌는 다른 동물의 것보다 더 지적인 이유는 뇌가 더 추상적인 패턴과 더 긴 시간적 패턴 순서에 대한 예측을 할 수 있기 때문입니다.\" 나중에 출간된 \"천 개의 뇌\"에서 호킨스는 계속해서 \"예측은 뇌가 가끔씩 하는 것이 아닌, 결코 멈추지 않는 내재적 특성이며, 학습에서 중요한 역할을 합니다. 뇌의 예측이 확인되면, 그것은 뇌의 세계 모델이 정확하다는 것을 의미합니다. 잘못된 예측은 당신을 그 오류에 주목하게 만들고 모델을 업데이트하게 합니다.\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n비슷한 견해를 가진 인지과학자이자 철학자인 앤디 클락은 '경험 기계(The Experience Machine)'에서 마음은 주로 예측 기계라고 주장합니다: \"뇌가 하는 주요 부분은 몸과 세계의 모델을 배우고 유지하는 것입니다.\" 우리의 감각을 통해 정보를 수집하고 그 정보를 처리하여 경험하고 행동할 세계 모델을 만드는 대신에 클락은 마음이 세계의 모델을 만들고 그 모델을 센서 정보로 업데이트한다고 제안합니다. 만약 현실이 예측과 다르다면요.\n\n# 산업 AI 연구소\n\nGoogle DeepMind의 미션은 \"지능을 해결하는 것\"입니다. OpenAI의 미션은 \"인공 일반 지능이 모든 인류에 이익이 되도록 보장하는 것\"입니다. Anthropic의 미션은 \"변혁적인 AI가 사람들과 사회가 번영하도록 하는 것\"입니다. Gemini, GPT-4, Claude의 세부사항은 아직 공개되지 않았지만, 선도적인 AI 연구소들이 AGI 구축의 핵심적 측면으로 다음 토큰 예측을 고려할 것으로 보입니다. GPT-3 논문에는 \"현재 목표는 모든 토큰에 동등한 가중치를 부여하며 무엇을 예측할 것이 가장 중요하고 무엇이 덜 중요한지에 대한 개념이 부족합니다\"라고 명시되어 있어 다음 토큰 예측 사전 훈련 목표가 함의되고, Claude도 다음 토큰 예측 사전 훈련 목표를 사용한다고 보고되었습니다.\n\n# 확장과 아키텍처 역시 중요합니다\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음 토큰 예측은 AGI에 대한 유용한 목적 함수가 될 수 있지만, 목적 함수만으로는 충분하지 않습니다. 단 세 개의 매개변수만 가진 병약한 신경망은 목적 함수가 무엇이든 관곂없이 많은 학습을 하지 못할 것입니다. 규모와 아키텍처가 중요합니다. Rich Sutton은 자신의 에세이 \"쓴 교훈\"에서, 인공지능 분야에서 가장 놀라운 발전은 인간의 지식을 기반으로 한 손수 디자인된 혁신이 아닌 보다 많은 컴퓨팅 자원을 돌리는 것으로 이루어졌다고 관찰했습니다. 그는 \"우리는 이러한 사고 방식을 직접적으로 구축하는 것이 장기적으로는 효과가 없다는 쓴 교훈을 배워야 합니다. 고사하자면, 1) 인공지능 연구자들이 종종 자신의 에이전트에 지식을 구축해 왔지만, 2) 이것은 단기적으로 도움이 되었고 연구자에게는 개인적으로 만족스러운 경험이 되었지만, 3) 장기적으로 그 경사로운 상승은 평평해지고 더 나아가는 진전을 억제하며, 4) 경이로운 진전은 결국 컴퓨팅 확장과 검색 및 학습에 기반을 뒀던 반대 방식으로 이루어지게 됩니다.\"라고 말합니다.\n\n아키텍처도 중요합니다. 트랜스포머의 엄청난 가장 효과적인 장점 중 하나는 RNN이나 LSTM보다 GPU/TPU 상에서 더 쉽게 병렬화될 수 있다는 것입니다. 이 더욱 좋은 병렬화 덕분에, 더 많은 데이터로 트랜스포머를 훈련하는 데 더 적은 시간이 걸립니다.\n\n# 데이터도 중요합니다\n\n또 다른 중요 요소는 고품질 데이터입니다. Eran Malach는 \"언어 모델의 힘은 다음 토큰 자동회귀 훈련 체계에 귀속될 수 있는데, 특정 아키텍처 선택에 귀속되는 것은 아닐 수도 있다\"고 주장합니다. 그러나 한 네티즌은 이 기사에 대한 반론으로 \"나는 기대했던 것이 LLM의 성공을 언어의 구조에 귀속했으면 좋았겠다고 말했습니다. 저자들이 말했듯이, 작은 선형 모델조차 Cot를 근사하고 복잡한 작업을 해결할 수 있습니다. 그래서 모델이 아니라 데이터입니다. 머리나 신경망(모델)이 아닌 데이터가 그들을 똑똑하게 만드는 것입니다.\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n데이터는 인간에게 꼭 필요한 것입니다. 사회에서 멀리 떨어져 자란 어린이들(\"야생 어린이\"라고도 함)은 나중에 말이나 언어를 배우거나, 직립보행을 하거나, 변기를 사용하거나, 다른 사람에게 주의를 기울이는 것을 가르쳐줄 수 없는 경우가 많습니다. (매우 슬픈 기분이 들고 싶다면, 야생 어린이 이야기를 검색해보세요.) 데이터 혁신에 중점을 둔 연구는 데이터의 품질이 특히 높을 때 작은 모델을 고성능으로 얻을 수 있는 경우가 많다는 것을 발견했습니다. 예를 들어, 논문 \"Textbooks Are All You Need\"에서는 코드용 LLM을 소개하여 상당히 적은 매개변수를 가지고 있음에도 높은 성능을 달성했습니다. 비결은 \"교과서 수준\"의 데이터에서 훈련을 한 것이었습니다.\n\n# 이상한 빠진 조각들\n\n또한 AGI의 생성에 도움이 될 아직 발견되지 않은 혁신이 분명히 많이 존재할 것입니다. 현재 모델을 아이들과 비교하면 빠진 조각들이 있는 것을 시사합니다.\n\n한 측면에서, 사람들은 훨씬 적은 양의 데이터로 훈련받습니다: 언어 습득 과정 중에 사람들은 약 1.5MB의 정보만 저장한다는 것은, LLM 훈련 데이터셋의 거대한 크기나 LLM 자체의 저장된 매개변수 양과 비교했을 때 미약한 숫자입니다. 사람들이 \"기본적으로 인터넷 전체\"보다 더 적은 양의 언어에 노출되며 상대적으로 많은 데이터를 저장함에도 불구하고 언어에 대한 뛰어난 능력을 나타내는 점은, 아직 발견되지 않은 흥미로운 혁신들이 AGI 시스템을 더 적은 훈련 데이터로 구축하는 데 도움이 될 수 있다는 것을 시사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다른 관점에서 보면, 인간들은 많은 데이터를 기반으로 훈련을 받습니다. 이 데이터는 현재 사용되는 기반 모델을 훈련하는 데 사용되는 데이터셋과는 매우 다릅니다. 전형적인 인간 아이들은 비디오와 오디오 스트림이 지속적으로 실행되며 여러 해 동안 데이터가 풍부한 환경에서 성장합니다. 이외에도 다른 감각에서 입력을 받습니다. 전혀 다른 인공 지능 시스템에서 어떤 새로운 지능이 발생할까요? 이 시스템이 전혀 다른 데이터셋을 사용하지 않고 일반 아이의 훈련 데이터셋만 사용해 다음 토큰 예측을 잘 하는 능력이 발전했다면?\n\n학습에 유용한 데이터 필터링 기술도 존재할 수 있습니다. 신생아는 흐릿한 흑백으로만 보기 시작합니다. 4개월이 지났을 때야 아기의 색상 감각이 완전히 발달합니다. 이러한 진행에는 진화적인 학습 관련 이점이 있다고 생각됩니다.\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_6.png)\n\n# 결론\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음 단어 예측기만 일까요? 그렇죠! 다른 방법이 있을까요? 제한된 인간 종 구성원으로서 언어를 만들려면 어떻게해야 할까요? 우리는 동시에 백 개의 단어를 말할 수 없습니다. 우리는 텔레파시가 아니며, \"생각 덤프\"를 통해 의사 소통할 수 없습니다. (만약 이렇다면 어떤 지능이 발전했을지 상상해보세요.)\n\n다음 단어, 또는 다음 광경, 또는 다음 소리를 성공적으로 예측함으로써 상당한 지능이 발전할 수 있다고 생각하는 것이 합리적으로 보입니다. 이 기사가 여러분에게 생각의 근원을 불러일으켰기를 바랍니다 — 그리고 완전히 예측할 수 없었으면 좋겠습니다.\n\n2024년 4월 28일, http://glassboxmedicine.com에서 최초로 게시되었습니다.\n","ogImage":{"url":"/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png"},"coverImage":"/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png","tag":["Tech"],"readingTime":18},{"title":"ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요","description":"","date":"2024-05-17 19:45","slug":"2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons","content":"\n![ChatGPT-4o](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png)\n\nAI 세계가 다시 소란스럽습니다.\n\n새로운 OpenAI 업데이트 덕분에요.\n\n새로운 모델 GPT-4o는 이미 영화 \"Her\"의 AI와 비교되고 있습니다.\n\nChatGPT-4o는 놀랄만한 능력과 전반적인 이해력에서 큰 발전이 있어 보입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSo friendly!\n\n---\n\n가장 놀라운 것은 무료로 사용할 수 있다는 점입니다!\n\n![이미지](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_1.png)\n\n유료 버전은 여전히 더 큰 컨텍스트 창을 가지고 있을 수 있지만, OpenAI가 가장 첨단 AI를 무료로 제공한 사실은 정말 놀랍습니다.\n\n아직 데모를 보지 않았다면, 지금바로 확인하는 것을 추천합니다. ChatGPT-4o가 다음을 수행하는 것을 확인할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 1 — 과외\n\n이 데모에서는 ChatGPT-4o가 학생을 가르치며 수학 문제를 해결하고 해결책을 안내하는 것을 보여줍니다.\n\nAI가 문제를 자연스러운 방식으로 설명하는 것은 훈련과 온라인 학습의 미래가 어떻게 될지 보여줍니다!\n\n내 아내는 과외교사이며 세션 전체가 사람처럼 들리는 것에 놀랐다고 할 수 없을 정도입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 2 — 실시간 번역\n\n또한 놀랄 만한 실시간 번역 기능을 제공합니다. 두 명의 사람이 영어와 이탈리아어로 이야기할 때 따라 말하는 것을 볼 수 있습니다.\n\n저도 직접 사용해 보았는데, 그 성능은 정말 인상적입니다.\n\n번역기 및 그들의 앱들은 어떻게 영향을 받을지 이미 걱정되고 있을 것 같네요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3 - 시각 능력\n\n데모에서 가장 좋았던 점은 팀이 시각 장애인을 도와 실시간으로 주변 환경을 설명하는 ChatGPT4-o를 보여준 것이었습니다.\n\n이것은 전 세계 사람들을 돕는 데 큰 도약이 될 것입니다.\n\n그가 주변 환경을 자연스럽게 설명하고 상호 작용을 얼마나 자연스럽게 했는지에 굉장히 감명받았습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이것은 AI가 인류가 다음 단계로 발전하는 데 도움을 줄 것이라는 완벽한 예시입니다.\n\n# 이제, 나쁜 소식 ..\n\nAI가 한 걸음씩 나아갈수록.. 새로운 위험이 소개됩니다.\n\n새로운 비전 및 음성 기능은 사이버 범죄자에겐 꿈의 소재가 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위는 데모 시청과 (100% 주관적인) 나의 경험을 통해 생각해 본 위험 중 일부입니다.\n\n## 1 — 사회 공학 2.0\n\n인간과 같은 대화를 나눌 수 있는 자연스러운 AI는 사회 공학을 미친 정도로 증가시킬 수 있습니다.\n\n우리는 이미 GenAI에 의한 딥페이크와 음성 사기를 보았지만, GPT-4o의 향상된 멀티모달 능력은 사이버 범죄자들이 무시할 수 없는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n잠재적인 잘못된 정보 캠페인과 ID 도용 가능성이 엄청나요.\n\nGPT-4o의 실시간 대화 능력은 사이버 범죄자들이 AI-기반 vishing 공격을 만들어낼 수 있도록 할 것입니다.\n\n희생자들은 합법적인 사람과 소통하고 있다고 믿으며 민감한 정보를 폭로하거나 거래를 승인하게 되는 속임수를 당할 수 있습니다.\n\n이러한 공격은 새로운 것은 아니지만 GPT-4o와 같은 AI와 함께 그 규모와 정교함이 대대적으로 증가할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 2 — AI-Powered Malware \\* 2\n\nAI-powered malware is already here, so there is nothing new about that\n\nWhat stood out to me was the video in which two GPT-4os were interacting with each other.\n\nImagine AI training another AI to evade controls and become better at compromising environments.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n두 모델 간의 상호작용은 인공지능이 서로 발전함에 따라 사이버 범죄의 미래가 어떻게 보일지도 모릅니다.\n\n## 3 — 다국어 공격\n\n실시간 번역이 가능하다는 것은 놀라우면서도 무섭습니다.\n\n사이버 범죄자들은 이제 다양한 언어로 전환하며 공격의 피해 범위를 확대할 수 있을 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다국어 기업에서 일하는 직원들 중 사회 공학에 취약하지 않은 사람들도 언어로 대화하는 AI에 노출되면 자신을 열어보게 될 수 있습니다!\n\n# 미래의 한 눈독\n\nAI가 발전함에 따라 우리는 몇 달마다 미지의 영역으로 나아가는 것 같아요.\n\n기업들은 보안 시스템을 조정하고 AI와의 전투에 AI를 활용해야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n흥미로운 데모를 보고 든 생각 중 일부를 소개해 드릴게요.\n\n- AI 기반 보안 도구를 도입하여 AI 기반 사회 공학 및 악성 소프트웨어의 이상한 패턴이나 행동을 감지합니다.\n- 직원 및 사용자가 AI 기반 사회 공학을 인식하는 방법에 대해 교육하십시오. 여전히 이메일 피싱 공격에 대해 이야기 중이라면 PowerPoint 프레젠테이션을 업데이트하세요!\n- 언어별 보안 프로토콜을 고려해 보세요. 언어 인식 기능을 갖춘 보안 프로토콜을 개발하여 다국어로 의심스러운 통신을 식별하고 표시합니다.\n\n다음 해에는 이것이 낡은 정보처럼 보일 수도 있느니라!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다중 모달, 다중 언어 인공 지능이 이제 여기 있으며, GPT-4o는 통제와 위험 관점에서 큰 발전이 이루어졌습니다.\n\n몇 달동안의 위협 전망이 사용자들(그리고 사이버 범죄자들)이 새로운 모델을 이해하고 대척을 하는 모습을 살펴봅시다.\n\n![ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_2.png](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_2.png)\n\nTaimur Ijlal은 핀테크 업종에서 20년 이상의 국제 경험을 갖춘 다중 수상 경력을 지닌 정보 보안 리더입니다. Taimur는 링크드인이나 유튜브 채널 \"클라우드 보안 가이\"에서 연결할 수 있습니다. 클라우드 보안, 인공 지능, 일반적인 사이버 보안 직업 조언에 대해 꾸준히 게시하고 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png"},"coverImage":"/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png","tag":["Tech"],"readingTime":7},{"title":"반복 신경망 시퀀스 모델링 소개","description":"","date":"2024-05-17 19:43","slug":"2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling","content":"\n![img](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png)\n\n많은 문제와 현상은 순차적으로 발생합니다. 대표적인 예로는 음성, 날씨 패턴, 시계열 등이 있습니다. 이러한 시스템들의 다음 위치는 이전 상태에 따라 달라집니다.\n\n안타깝게도, 전통적인 신경망은 이러한 유형의 데이터를 처리하거나 예측할 수 없습니다. 왜냐하면 입력값을 독립적으로 분석하기 때문에 데이터가 실제로 순차적이라는 개념을 이해하지 못하기 때문입니다.\n\n그렇다면, 이러한 유형의 데이터를 어떻게 예측할 수 있을까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\"우리는 순환 신경망이라고 불리는 것으로 넘어갑니다!\n\n표준 신경망에 익숙하지 않다면, 확인할 블로그 시리즈가 있어요! RNN으로 계속 진행하기 전에 이 일반적인 신경망이 어떻게 작동하는지 알아보는 것을 권장합니다.\n\n# 순환 신경망이란 무엇인가요?\n\n다음은 순환 신경망(RNNs)을 설명하는 다이어그램입니다:\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![RecurrentNeuralNetworksAnIntroductiontoSequenceModelling](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_1.png)\n\n왼쪽에는 순환 뉴런이 있고, 오른쪽에는 시간에 따라 펼쳐진 순환 뉴런이 있습니다. RNN은 바닐라 피드포워드 신경망과 비슷해 보이지만, 이전 반복 실행에서 입력을 받는 중요한 차이점이 있습니다.\n\n그래서 그들을 \"순환\"이라고 부르는 것입니다. 각 단계의 출력이 시간 안에 전파되어 다음 단계의 값을 계산하는 데 도움이 됩니다. 시스템에는 어떤 내재적 \"기억\"이 있어서 모델이 과거의 패턴을 추적할 수 있습니다.\n\n예를 들어, Y_1을 예측할 때, X_1의 입력 및 이전 시간 단계 Y_0에서의 출력을 사용할 것입니다. Y_0가 Y_1에 영향을 미치기 때문에 Y_0가 Y_2에도간접적으로 영향을 줄 수 있다는 것을 알 수 있습니다. 이 알고리즘의 순환성을 명확하게 보여주는 사례입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 숨겨진 상태\n\n문학 작품에서는 일반적으로 숨겨진 상태라는 개념을 볼 수 있습니다. 주로 순환 뉴런을 통해 전달되는 h로 표시됩니다.\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_2.png)\n\n간단한 경우에는 숨겨진 상태가 셀의 출력인 경우도 있습니다. 즉, h=Y입니다. 그러나 우리가 나중에 살펴볼 것처럼, 장기 단기 메모리(LSTM) 및 게이트 순환 유닛(GRU)과 같은 보다 복잡한 셀의 경우에는 이것이 항상 참일 수 있는 것은 아닙니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n따라서 각 뉴런으로 전달하는 것과 각 뉴런으로부터의 전달을 명시적으로 하는 것이 가장 좋습니다. 이것이 대부분의 문헌에서 위와 같이 표시되는 이유입니다.\n\n# 이론\n\n순환 뉴런의 각 숨겨진 상태는 다음과 같이 계산할 수 있습니다:\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기서:\n\n- h_t는 시간 t에서의 은닉 상태입니다.\n- h\\_'t−1'는 이전 시간 단계의 은닉 상태입니다.\n- x_t는 시간 t에서의 입력 데이터입니다.\n- W_h는 은닉 상태에 대한 가중치 행렬입니다.\n- W_x는 입력 데이터에 대한 가중치 행렬입니다.\n- b_h는 은닉 상태에 대한 편향 벡터입니다.\n- σ는 활성화 함수로, 일반적으로 tanh 또는 sigmoid 함수를 사용합니다.\n\n그리고 각 순환 뉴런의 출력을 예측하는 방법은 다음과 같습니다:\n\n![Recurrent Neurons](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위와 같이:\n\n- y_t는 시간 t에서의 출력입니다.\n- W_y는 출력과 관련된 가중치 행렬입니다.\n- b_y는 출력 편향 벡터입니다.\n\n보시다시피 표기법과 변수 대부분은 일반 피드포워드 신경망과 유사합니다. 유일한 차이점은 숨겨진 상태의 전달로, 그것은 모델이 출력을 예측하는 데 사용할 다른 입력이나 특성으로 볼 수 있습니다.\n\n각 숨겨진 층은 여러 반복 뉴런을 포함할 수 있으므로 각 후속 입력 뉴런에 숨겨진 상태의 벡터를 전달하게 됩니다. 이를 통해 네트워크는 데이터에서 더 복잡한 패턴을 포착하고 표현할 수 있습니다. 각 시간 단계에서 미니 신경망으로 생각할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 작업 예시\n\n우리는 RNN 내부에서 실제로 무슨 일이 일어나고 있는지 설명하기 위해 간단한 예제를 살펴볼 수 있습니다. 이 예제는 매우 단순한 시나리오일 것이지만, 알아야 할 주요 직관력을 설명해줄 것입니다. 실제로 현실에서는 어떤 문제도 이렇게 간단하지 않을 겁니다!\n\n## 설정\n\n1, 2, 3의 숫자 시퀀스가 있다고 가정해 보겠습니다. 이 시퀀스에서 다음 숫자인 4를 예측하기 위해 RNN을 훈련하려고 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n당신의 RNN은 다음과 같은 구조를 가지고 있을 것입니다:\n\n- 하나의 입력 뉴런\n- 하나의 은닉 뉴런\n- 하나의 출력 뉴런\n\n가중치와 바이어스를 랜덤하게 초기화할 수 있습니다:\n\n- W_x (입력에서 은닉으로의 가중치): 0.5\n- W_h (은닉에서 은닉으로의 가중치): 1.0\n- b_h (은닉 바이어스): 0\n- 𝑏_𝑦 (출력 바이어스): 0\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 텍스트를 친근한 톤으로 한국어로 번역해 드리겠습니다.\n\n다음 활성화 함수를 사용해주세요:\n\n- 은닉층: tanh\n- 출력층: 없음 (identity/linear)\n\n초기 은닉 상태 값:\n\n- h_0 = 0\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Time Step 1 (Input: 1)\n\n다음은 첫 번째 숨겨진 상태입니다:\n\n![첫 번째 숨겨진 상태](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_5.png)\n\n그리고 출력은 다음과 같이 계산됩니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_6.png)\n\n이 예시에서 출력 활성화 함수는 identity이므로 출력 값은 hidden state 값과 동일합니다. 그러나 많은 문제에서 항상 그런 것은 아니라는 것을 기억하세요.\n\n## 시간 단계 2 (입력: 2)\n\n이제 최근에 계산된 h_1 값 사용하여 시간 단계 2에서 다음 입력 값을 위한 위 과정을 반복할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 Markdown 형식으로 테이블 태그를 바꿔본 것입니다.\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_7.png)\n\n한번 더, 우리는 시간 단계 2에서 출력 값을 계산합니다:\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_8.png)\n\n## 시간 단계 3 (입력: 3)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마지막 입력 값과 세 번째 타임 스텝에서는 다음 이미지가 예측 모델을 보여줍니다:\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_9.png)\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_10.png)\n\n현재 모델은 다음 숫자를 0.984로 예측하고 있습니다. 실제 값인 4와는 분명히 멀리 떨어져 있습니다. 실제로는 더 많은 훈련 세트를 사용하여 시간을 거슬러 거슬러 역전파를 수행하여 매개 변수를 최적화할 것입니다. 이 내용은 다음 글에서 다룰 예정입니다!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다행히도 이 모든 계산과 최적화는 PyTorch와 TensorFlow와 같은 패키지를 통해 Python에서 수행됩니다. 제가 이 기사에서 이를 하는 방법의 예시를 나중에 보여 드리겠습니다!\n\n# RNN의 종류\n\n위의 예는 많은 입력으로부터 하나의 RNN 프로세스의 논리적인 과정을 설명하고 있습니다. 우리는 여러 입력(1,2,3)으로 시작하여 시퀀스에서 다음 숫자를 예측하기 위해 노력하고 있는데, 이는 단일 값입니다.\n\n그러나 다른 작업을 위한 여러 종류의 RNN이 있으며, 우리는 지금 그것들을 살펴볼 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 일대일\n\n이것은 단일 예측을 내놓는 입력 세트가 하나인 전통적인 신경망입니다. 이것은 일반적인 지도 학습 문제를 해결하는 데 도움이 됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_11.png)\n\n## 일대다\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하나의 입력이 여러 출력으로 이어집니다. 이미지 캡션을 만들거나 음악을 생성하는 데 사용할 수 있습니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_12.png)\n\n## Many-To-One\n\n여러 입력이 하나의 최종 출력을 생성합니다; 이 아키텍처는 감성 분석에 사용됩니다. 영화 리뷰를 제공하면 영화가 좋은지 나쁜지에 따라 +1 또는 -1을 할당합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_13.png)\n\nThis one gets an input at every step and produces an output at each step. This architecture is used for machine translation and also for problems like speech tagging.\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_14.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 인코더-디코더\n\n마지막으로, 인코더-디코더 네트워크를 사용할 수 있습니다. 이는 많은 개별 데이터를 입력으로 받아 하나의 데이터를 출력하는 네트워크와, 그로부터 다시 많은 개별 데이터를 출력으로 하는 네트워크로 구성됩니다. 이는 주로 한 언어로 된 문장을 다른 언어로 번역하는 데 사용됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_15.png)\n\n# PyTorch 예시\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위는 PyTorch에서 간단한 RNN을 구현하는 간단한 예제입니다. 위에서 해결한 문제를 시연합니다. 입력이 1,2,3이고 순서에 따라 다음 숫자를 예측하려고 합니다.\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# RNN 모델 정의\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(-1)\n        h_0 = torch.zeros(1, x.size(0), self.hidden_size)\n        rnn_out, _ = self.rnn(x, h_0)\n        out = self.fc(rnn_out[:, -1, :])\n        return out\n\n# 데이터셋\ntrain = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\ntarget = torch.tensor([5], dtype=torch.float32)\n\n# 모델 설정\ninput_size = 1\nhidden_size = 1\noutput_size = 1\nmodel = SimpleRNN(input_size, hidden_size, output_size)\n\n# 손실 및 옵티마이저\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    output = model(train.unsqueeze(0)).squeeze()  # 배치 차원 추가 및 목표 형태와 일치하도록 압축\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n# 다음 숫자 예측하는 함수\ndef predict(model, input_seq):\n    with torch.no_grad():\n        input_seq = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n        output = model(input_seq).squeeze().item()\n    return output\n\n# 예제 테스트 세트\ntest = [2, 3, 4]\npredicted = predict(model, test)\nprint(f'Input: {test}, Predicted Next Number: {predicted:.2f}')\n```\n\n1000번의 에폭 후 출력 결과는 5입니다! 이 경우에는 모델이 실제로 1000번의 역전파로 훈련되었기 때문에 성능이 우리가 위에서 손으로 계산한 예제보다 훨씬 좋습니다.\n\n소스 코드는 저의 GitHub에서 확인하실 수 있습니다: (GitHub URL)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 장점 대비 단점\n\n이 모든 새롭게 습득한 정보를 바탕으로 RNN의 주요 장단점을 살펴보겠습니다:\n\n## 장점\n\n- 이전 입력값의 형태를 기억할 수 있어서 순차적 데이터를 다루는 데 도움이 됩니다.\n- 정확한 가중치와 편향이 모든 시간 단계에서 공유되어, 더 적은 매개변수와 더 나은 일반화를 이끌어냅니다.\n- 재귀적 성격으로 인해 RNN은 가변 길이의 순차 데이터를 처리할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 단점\n\n- RNN(순환 신경망)은 장기 기억 문제로 이어지는 사라지는 그래디언트 문제에서 상당히 고통받습니다.\n- 각 시간 단계는 이전 단계의 출력에 의존하기 때문에 RNN은 병렬 처리할 수 없어 계산 효율이 떨어집니다.\n\n# 요약\n\nRNN은 시퀀스 모델링에 매우 유용하며, 이전 실행의 정보와 메모리를 유지한 채 다음 예측으로 전파됩니다. 그들의 장점은 임의 길이의 입력을 처리할 수 있으며, 모델 크기가 이 입력 크기로 증가하지 않는다는 것입니다. 그러나 재귀적인 성격을 가지고 있기 때문에 병렬화할 수 없어 계산 효율이 낮으며, 사라지는 그래디언트 문제로 심각하게 고통받을 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 또 다른 것!\n\n무료 뉴스레터 'Dishing the Data'를 갖고 있어요! 매주 더 나은 데이터 과학자가 되기 위한 조언과 분야에서의 경험을 나누고 있어요.\n\n# 저와 연결해보세요!\n\n- LinkedIn, X (트위터), 또는 인스타그램\n- 기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 제 유튜브 채널!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 참고 자료 및 더 읽을거리\n\n- Stanford RNN Cheatsheet\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. September 2019. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649.\n","ogImage":{"url":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png"},"coverImage":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png","tag":["Tech"],"readingTime":15},{"title":"ICD 코딩을 위한 LLM 탐험 - 파트 1","description":"","date":"2024-05-17 19:38","slug":"2024-05-17-ExploringLLMsforICDCodingPart1","content":"\n## LLM(Large Language Model)을 활용한 자동 진단 코딩 시스템 구축\n\n임상 코딩은 흔히 쓰이는 용어는 아니지만, 대부분의 국가에서 건강 관리 체계와 상호작용하는 모든 사람에게 중대한 영향을 미칩니다. 임상 코딩은 환자 건강 기록에서 의학 정보(진단 및 수술 등)를 표준화된 숫자 또는 알파벳 코드로 번역하고 매핑하는 것을 포함합니다. 이러한 코드는 청구, 건강 관리 분석 및 환자가 적절한 치료를 받을 수 있도록 하는 데 중요합니다.\n\n![image](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png)\n\n임상 코딩은 일반적으로 의료 전문가가 수행합니다. 이러한 코더들은 다양한 진단과 수술을 위한 특정 코드가 포함된 복잡하고 종종 계층적인 코딩 용어를 탐색합니다. 따라서 코더들은 사용된 코딩 용어에 대한 깊은 이해와 경험을 가져야 합니다. 그러나 문서를 수동으로 코딩하는 것은 느릴 수 있고, 오류가 발생할 수 있으며, 상당한 인적 전문 지식이 필요하여 병목 현상이 발생할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n심층 학습은 임상 코딩 자동화에 중요한 역할을 할 수 있습니다. 복잡한 의료 정보를 코드로 추출하고 번역함으로써, 심층 학습 시스템은 인간 중심 시스템 내에서 가치 있는 도구로 작용할 수 있습니다. 이러한 시스템은 코더들을 지원하여 대량의 데이터를 신속하게 처리하고 정확성을 향상시킬 수 있습니다. 이는 행정 업무를 간소화하고 청구 오류를 줄이며 환자 치료 결과를 향상시킬 수 있습니다.\n\n이 첫 번째 부분에서는 ICD 코딩이 무엇인지, 자동 코딩 시스템이 효과적으로 극복해야 하는 다양한 도전에 대해 설명합니다. 또한 대용량 언어 모델(LLM)이 이러한 문제를 극복하는 데 효과적으로 활용할 수 있는 방법을 분석하고, 최근 논문에서 LLM을 효과적으로 활용한 알고리즘을 적용하여 ICD 코딩에 성공적으로 적용하는 방법을 설명합니다.\n\n## 목차:\n\n- ICD 코딩이란 무엇인가?\n- 자동 ICD 코딩의 도전 요소는 무엇인가?\n- LLM이 자동 ICD 코딩에 어떻게 도움이 될까?\n- \"Off-the-shelf 대용량 언어 모델을 이용한 자동 임상 코딩\" 논문 탐색\n- 논문에 설명된 기법 구현\n- 결론\n- 참고문헌\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# ICD 코딩이란 무엇인가요?\n\n국제질병분류(ICD) 코딩은 세계보건기구에서 개발 및 유지보수하는 임상 용어 시스템입니다 [1]. 대부분의 국가에서 환자의 모든 진단, 증상 및 절차를 범주화하고 코딩하는 데 사용됩니다.\n\n환자의 진단과 의료 절차를 기록하는 의료 기록은 ICD 코딩에 매우 중요합니다. ICD 용어는 대략 75,000가지 다른 코드로 구성된 트리 구조를 특징으로 하여 방대한 정보를 효율적으로 정리합니다. 이러한 문서를 정확하게 코딩하는 것이 중요합니다. 정확한 코딩은 적절한 청구를 보장하며 의료 분석 품질에 영향을 미치며 환자 치료 결과, 보상 및 의료 효율성에 직접적으로 영향을 줍니다.\n\n# 자동 ICD 코딩에서 어떤 도전들이 있을까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nICD 코딩은 효과적으로 작동하기 위해 자동화된 시스템이 극복해야 할 여러 가지 도전이 있습니다.\n\n## ICD 코딩의 레이블 다양성:\n\n중요한 도전 중 하나는 레이블의 광범위한 출력 공간입니다. ICD 코드는 많고 각 코드는 미세한 세부 사항에서 차이가 있을 수 있습니다. 예를 들어, 오른손에 영향을 주는 상태와 왼손에 영향을 주는 상태는 서로 다른 코드를 갖게 됩니다. 또한 의료 기록에서 드물게 나타나는 희귀 코드의 긴 꼬리가 존재하여, 이러한 코드를 학습하고 정확하게 예측하기 어렵게 만들 수 있습니다.\n\n## 새로운 ICD 코드에 대한 적응:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n번거로우시겠지만, 테이블 태그를 마크다운 형식으로 변경해드릴게요!\n\n전통적인 데이터셋인 MIMIC-III [2] 같은 경우는 종합적이지만, 종종 ICD 코드의 범위를 훈련 말뭉치에 포함된 코드로 제한합니다. 이 제한은 의료 기록에서 ICD 코드로의 딥러닝 모델을 다중 레이블 분류 문제로 처리하는 데 새로운 코드가 도입된 경우 모형 훈련 이후에 어려움을 겪을 수 있음을 의미합니다. 이는 재훈련이 필요하고 잠재적으로 어려울 수 있게 만듭니다.\n\n## 정보 추출 및 문맥 활용:\n\n또 다른 주요 과제는 의료 기록에서 정보를 정확하게 추출하고 문맥에 맞게 처리하는 것입니다. ICD 코딩은 근본적으로 정보 검색 문제로, 의료 기록에서 진단을 식별하는 것 뿐만 아니라 이러한 진단을 해당 ICD 코드로 올바르게 매핑하는 데 필요한 모든 보완 정보를 포착해야 합니다. 따라서 자동화된 시스템이 의료 기록에서 여러 진단을 추출하고 적절히 문맥화하여 ICD 코드로 정확하게 매핑되도록 하는 것이 중요합니다.\n\n![이미지](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\"Contextualization\"란 여기서 무엇을 의미할까요? 의학 노트를 다룰 때 진단을 맥락에 맞게 처리하는 것은 관련 세부사항과 관련된 정보 — 예를 들어 영향을 받는 신체 부위 및 질환의 증상 — 를 연결하여 진단을 완전히 특성화하는 것을 의미합니다. 일반적으로 이 작업은 관계 추출로 참조됩니다.\n\n# 대규모 언어 모델(LLMs)이 자동 ICD 코딩에 어떻게 도움이 되나요?\n\n자동 ICD 코딩의 과제를 다룰 때, 대규모 언어 모델 (LLMs)은 이러한 문제에 대처하는 데 적합하며, 특히 새로운 레이블에 대한 적응성과 복잡한 정보 추출 작업을 관리하는 능력으로 인해 잘 역할을 합니다. 그러나 여기서의 포인트는 LLMs가 자동 ICD 코딩에 대한 최상의 해결책이거나 이러한 문제를 해결할 수 있는 유일한 해결책인 것을 주장하는 것이 아니라, 자동 ICD 코딩 시스템이 극복해야 하는 주요 과제들을 설정함으로써 LLMs의 능력을 최대한 활용하여 이를 해결할 수 있는지를 분석하는 것입니다.\n\n## 새로운 및 드문 ICD 코드에 대한 적응:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLLMs는 견고한 제로샷 및 퓨샷 학습 능력을 보여주며, 적은 예시와 프롬프트에서 제공된 지침으로 새로운 작업에 적응할 수 있습니다. 검색 증강 생성 (RAG)은 미세 조정 없이도 LLM이 새로운 작업에 적응하기 위해 더 많은 맥락 정보에 접근할 수 있는 패러다임입니다. 이는 특히 기존의 훈련 데이터셋에서 자주 나타나지 않을 수 있는 새로운 및/또는 희귀한 ICD 코드에 LLM을 조정하는 데 유용합니다. 이를 단지 몇 가지 설명 또는 사용 사례로부터 합니다.\n\n## 맥락 정보:\n\nLLMs는 임상 분야에서의 제로샷 관계 추출에서 효과적으로 확인되었습니다. 제로샷 관계 추출은 LLM이 해당 관계에 대해 이전에 구체적인 훈련을 받지 않고 텍스트에서 관계를 식별하고 분류할 수 있도록 합니다. 이를 통해 의료 코딩에서의 진단을 더 잘 맥락화하여 더 정확한 ICD 코드를 가져올 수 있습니다.\n\n# \"Automated clinical coding using off-the-shelf large language models\" 논문 탐색하기:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLLM을 ICD 코딩에 적용한 최근 연구를 탐색하다가, 특정한 세부 조정 없이 LLM을 활용한 ICD 코딩에 관한 매우 흥미로운 논문을 발견했습니다. 저자들은 LLM을 활용한 ICD 코딩을 위해 LLM-지도된 트리 탐색이라는 방법을 개발했습니다 [5].\n\n## 이 방법은 어떻게 작동하나요?\n\nICD 용어는 계층적인 트리 구조입니다. 각 ICD 코드는 이 계층적 구조 내에 존재하며, 부모 코드는 더 일반적인 상태를 다루고, 자식 코드는 특정 질병을 상세히 설명합니다. ICD 트리를 탐색하면 더 구체적이고 세분화된 진단 코드로 이어집니다.\n\nLLM-지도된 트리 탐색에서는 탐색이 루트에서 시작되고 LLM을 사용하여 탐색할 가지를 선택하며, 모든 경로가 고갈될 때까지 반복적으로 계속합니다. 실제로 이 과정은 트리의 임의의 수준에서 모든 코드의 설명과 의료 노트를 LLM에 프롬프트로 제공하고, 해당 의료 노트에 대한 관련 코드를 식별하도록 요청하는 것으로 구현됩니다. 각 인스턴스에서 LLM에 의해 선택된 코드는 더 구체적으로 탐색되고 조사됩니다. 이 방법을 사용하면 가장 관련성이 높은 ICD 코드가 식별되며, 이후 임상 노트에 대한 예측 레이블로 할당됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예시를 통해 이를 명확히 해보겠습니다. ICD 코드 1과 ICD 코드 2라는 두 개의 루트 노드를 가진 트리를 상상해보세요. 각 노드는 코드를 특성화하는 평문 설명을 가지고 있습니다. 초기 단계에서 LLM에게 의학 노트와 코드 설명이 제공되고 의학 노트와 관련된 코드를 식별하도록 요청됩니다.\n\n이 시나리오에서 LLM은 의학 노트와 관련이 있는 것으로 판단된 ICD 코드 1과 ICD 코드 2를 식별합니다. 알고리즘은 각 코드의 자식 노드를 조사합니다. 각 부모 코드는 더 구체적인 ICD 코드를 나타내는 두 개의 자식 노드를 가지고 있습니다. ICD 코드 1부터 시작하여, LLM은 ICD 코드 1.1과 ICD 코드 1.2의 설명을 사용하여 의학 노트를 기반으로 관련 코드를 결정합니다. LLM은 ICD 코드 1.1이 관련이 있다고 결론 내리고, ICD 코드 1.2는 관련이 없다고 판단합니다. ICD 코드 1.1에는 더 이상의 자식 노드가 없으므로, 알고리즘은 할당 가능한 코드인지 확인하고 문서에 할당합니다. 그 다음 알고리즘은 ICD 코드 2의 자식 노드를 평가합니다. LLM을 호출하여, ICD 코드 2.1이 관련이 있는 것으로 판단합니다. 이것은 간단화된 예시이며, 실제로는 ICD 트리는 광범위하고 깊기 때문에 알고리즘은 각 관련된 노드의 자식을 탐색하거나 트리의 끝에 도달하거나 유효한 탐색을 소진할 때까지 계속됩니다.\n\n## 핵심\n\n- 이 방법은 LLM의 세밀한 조정이 필요하지 않습니다. 대신, 제공된 설명을 기반으로 LLM의 의료 노트를 상황에 맞게 이해하고 관련 ICD 코드를 동적으로 식별할 수 있는 능력을 활용합니다.\n- 더 나아가, 본 논문은 LLM이 프롬프트에 관련 정보가 주어질 때 대규모 출력 공간에 효과적으로 적응할 수 있으며, macro-average 지표 측면에서 드문 코드에서 PLM-ICD [6]를 앞지를 수 있다는 것을 보여줍니다.\n- 이 기술은 또한 파라메트릭 지식에 기초하여 의학 노트의 ICD 코드를 예측하도록 LLM에 직접 요청하는 기준선을 능가합니다. 이는 LLM을 임상 코딩 작업을 해결하기 위한 도구나 외부 지식과 통합하는 잠재력을 강조합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 단점\n\n- 알고리즘은 트리의 각 수준에서 LLM을 호출합니다. 이로 인해 트리를 탐색하는 동안 LLM 호출 횟수가 많아지며, ICD 트리의 광범위함이 이에 더해집니다. 이는 단일 문서를 처리하는 데 높은 대기 시간과 비용으로 이어집니다.\n- 저자들이 논문에서 언급한 바와 같이, 관련 있는 코드를 정확하게 예측하려면 LLM이 모든 수준에서 부모 노드를 올바르게 식별해야 합니다. 한 수준에서 실수가 발생하더라도, LLM은 최종 관련 코드에 도달할 수 없게 됩니다.\n- 저자들은 MIMIC-III와 같은 데이터셋을 사용하여 메소드를 평가할 수 없었습니다. 외부 서비스로의 데이터 전송을 금지하는 제한 사항으로 인하여 OpenAI의 GPT 엔드포인트와 같은 외부 서비스로의 데이터 전송이 불가능했습니다. 대신, 저자들은 CodiEsp 데이터셋 [7,8]의 테스트 세트를 사용하여 해당 방법을 평가했습니다. 해당 데이터셋은 250개의 의학 노트를 포함하고 있습니다. 이 데이터셋의 크기가 작은 것은 해당 방법이 대규모 임상 데이터셋에서의 성능을 아직 입증하지 못했음을 시사합니다.\n\n# 논문에서 설명한 기술 구현하기\n\n이 기술을 구현하여 그 작동 방식을 더 잘 이해해 봅시다. 논문에서 언급했듯이, 해당 논문은 평가를 위해 CodiEsp 테스트 세트를 사용합니다. 이 데이터셋은 스페인어 의학 노트와 이에 대응하는 ICD 코드로 구성되어 있습니다. 데이터셋에는 영어로 번역된 버전도 포함되어 있지만, 저자들은 스페인어 의학 노트를 GPT-3.5를 사용하여 영어로 번역하였으며, 이를 통해 사전 번역된 버전을 사용하는 것보다 성능이 약간 향상되었다고 주장했습니다. 이 기능을 복제하고 노트를 영어로 번역해 봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndef construct_translation_prompt(medical_note):\n    \"\"\"\n    Construct a prompt template for translating Spanish medical notes to English.\n\n    Args:\n        medical_note (str): The medical case note.\n\n    Returns:\n        str: A structured template ready to be used as input for a language model.\n    \"\"\"\n    translation_prompt = \"\"\"You are an expert Spanish-to-English translator. You are provided with a clinical note written in Spanish.\nYou must translate the note into English. You must ensure that you properly translate the medical and technical terms from Spanish to English without any mistakes.\nSpanish Medical Note:\n{medical_note}\"\"\"\n\n    return translation_prompt.format(medical_note = medical_note)\n```\n\nNow that we have the evaluation corpus ready, let’s implement the core logic for the tree-search algorithm. We define the functionality in get_icd_codes, which accepts the medical note to process, the model name, and the temperature setting. The model name must be either “gpt-3.5-turbo-0613” for GPT-3.5 or “meta-llama/Llama-2–70b-chat-hf” for Llama-2 70B Chat. This specification determines the LLM that the tree-search algorithm will invoke during its processing.\n\nEvaluating GPT-4 is possible using the same code-base by providing the appropriate model name, but we choose to skip it as it is quite time-consuming.\n\n```js\ndef get_icd_codes(medical_note, model_name=\"gpt-3.5-turbo-0613\", temperature=0.0):\n    \"\"\"\n    Identifies relevant ICD-10 codes for a given medical note by querying a language model.\n\n    This function implements the tree-search algorithm for ICD coding described in https://openreview.net/forum?id=mqnR8rGWkn.\n\n    Args:\n        medical_note (str): The medical note for which ICD-10 codes are to be identified.\n        model_name (str): The identifier for the language model used in the API (default is 'gpt-3.5-turbo-0613').\n\n    Returns:\n        list of str: A list of confirmed ICD-10 codes that are relevant to the medical note.\n    \"\"\"\n    assigned_codes = []\n    candidate_codes = [x.name for x in CHAPTER_LIST]\n    parent_codes = []\n    prompt_count = 0\n\n    while prompt_count \u003c 50:\n        code_descriptions = {}\n        for x in candidate_codes:\n            description, code = get_name_and_description(x, model_name)\n            code_descriptions[description] = code\n\n        prompt = build_zero_shot_prompt(medical_note, list(code_descriptions.keys()), model_name=model_name)\n        lm_response = get_response(prompt, model_name, temperature=temperature, max_tokens=500)\n        predicted_codes = parse_outputs(lm_response, code_descriptions, model_name=model_name)\n\n        for code in predicted_codes:\n            if cm.is_leaf(code[\"code\"]):\n                assigned_codes.append(code[\"code\"])\n            else:\n                parent_codes.append(code)\n\n        if len(parent_codes) \u003e 0:\n            parent_code = parent_codes.pop(0)\n            candidate_codes = cm.get_children(parent_code[\"code\"])\n        else:\n            break\n\n        prompt_count += 1\n\n    return assigned_codes\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 논문과 비슷하게, ICD-10 트리에 액세스하는 simple_icd_10_cm 라이브러리를 사용합니다. 이를 통해 트리를 탐색하고, 각 코드에 대한 설명에 액세스하며 유효한 코드를 식별할 수 있습니다. 먼저, 트리의 첫 번째 수준에서 노드를 가져옵니다.\n\n```js\nimport simple_icd_10_cm as cm\n\ndef get_name_and_description(code, model_name):\n    \"\"\"\n    ICD-10 코드의 이름과 설명을 검색합니다.\n\n    Args:\n        code (str): ICD-10 코드.\n\n    Returns:\n        tuple: 형식화된 설명과 코드의 이름이 포함된 튜플을 반환합니다.\n    \"\"\"\n    full_data = cm.get_full_data(code).split(\"\\n\")\n    return format_code_descriptions(full_data[3], model_name), full_data[1]\n```\n\n루프 내부에서 각 노드에 해당하는 설명을 얻습니다. 이제 의료 노트와 코드 설명을 기반으로 LLM을 위한 프롬프트를 작성해야 합니다. 우리는 논문에서 제공된 세부 정보를 기반으로 GPT-3.5와 Llama-2용 프롬프트를 작성합니다.\n\n```js\nprompt_template_dict = {\"gpt-3.5-turbo-0613\" : \"\"\"[사례 노트]:\n{note}\n[예시]:\n\u003c예시 프롬프트\u003e\n위식도 역류병\n장전위\n\n\u003c응답\u003e\n위식도 역류병: 예, 환자에게 오메프라졸 처방함.\n장전위: 아니오.\n\n[작업]:\n다음 ICD-10 코드 설명 각각을 고려하고 사례 노트에 관련 언급이 있는지 평가하십시오.\n예시의 형식을 정확히 따르십시오.\n\n{code_descriptions}\"\"\",\n\n\"meta-llama/Llama-2-70b-chat-hf\": \"\"\"[사례 노트]:\n{note}\n\n[예시]:\n\u003c코드 설명\u003e\n* 위식도 역류병\n* 장전위\n* 급성비인두염 [감기]\n\u003c/코드 설명\u003e\n\n\u003c응답\u003e\n* 위식도 역류병: 예, 환자에게 오메프라졸 처방함.\n* 장전위: 아니오.\n* 급성비인두염 [감기]: 아니오.\n\u003c/응답\u003e\n\n[작업]:\n예시 응답 형식을 정확히 따르십시오. (예) 판단하기 전에 전체 설명과 (예|아니오) 판단을 입력한 후에 새 줄을 추가하십시오.\n다음 ICD-10 코드 설명을 고려하고 사례 노트에서 관련 언급이 있는지 확인하십시오.\n\n{code_descriptions}\"\"\"\n}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n의료 기록과 코드 설명에 기반한 프롬프트를 지금 만들어 보겠습니다. 프롬프트 및 코딩에서 우리에게 이점은 GPT-3.5 및 Llama 2 모두와 상호 작용하기 위해 동일한 openai 라이브러리를 사용할 수 있다는 것입니다. 단, Llama-2가 deepinfra를 통해 배포되어야 합니다. deepinfra는 LLM에 요청을 보내기 위한 openai 형식도 지원합니다.\n\n```js\ndef construct_prompt_template(case_note, code_descriptions, model_name):\n    \"\"\"\n    주어진 케이스 노트와 ICD-10 코드 설명을 평가하는 프롬프트 템플릿 구성\n\n    Args:\n        case_note (str): 의료 케이스 노트\n        code_descriptions (str): ICD-10 코드 설명을 단일 문자열로 포맷팅\n\n    Returns:\n        str: 언어 모델에 입력으로 사용할 준비된 구조화된 템플릿\n    \"\"\"\n    template = prompt_template_dict[model_name]\n\n    return template.format(note=case_note, code_descriptions=code_descriptions)\n\ndef build_zero_shot_prompt(input_note, descriptions, model_name, system_prompt=\"\"):\n    \"\"\"\n    시스템 및 사용자 역할에 대한 제로샷 분류용 프롬프트 빌드\n\n    Args:\n        input_note (str): 입력 노트 또는 질의\n        descriptions (list of str): ICD-10 코드 설명 리스트\n        system_prompt (str): 선택적 초기 시스템 프롬프트 또는 지시\n\n    Returns:\n        list of dict: 각 메시지의 역할 및 내용을 정의하는 구조화된 사전 목록\n    \"\"\"\n    if model_name == \"meta-llama/Llama-2-70b-chat-hf\":\n        code_descriptions = \"\\n\".join([\"* \" + x for x in descriptions])\n    else:\n        code_descriptions = \"\\n\".join(descriptions)\n\n    input_prompt = construct_prompt_template(input_note, code_descriptions, model_name)\n    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": input_prompt}]\n```\n\n프롬프트를 구성한 후, 이제 LLM을 호출하여 응답을 받겠습니다:\n\n```js\ndef get_response(messages, model_name, temperature=0.0, max_tokens=500):\n    \"\"\"\n    채팅-완성 API를 통해 지정된 모델로부터 응답을 획득\n\n    Args:\n        messages (list of dict): API 입력용 구조화된 메시지 목록\n        model_name (str): 쿼리할 모델의 식별자\n        temperature (float): 응답의 무작위성을 제어하는 값, 0이면 결정론적\n        max_tokens (int): 응답의 토큰 수 제한\n\n    Returns:\n        str: 모델에서의 응답 메시지 내용\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens\n    )\n    return response.choices[0].message.content\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n좋아요, 우리가 출력물을 얻었어요! 이 응답으로부터, 이제 LLM이 추가적인 탐색을 위해 관련있는 노드들과 거부한 노드들을 식별하기 위해 각 코드 설명을 구문 분석합니다. 우리는 출력 응답을 새 줄로 나누고 각 응답을 분할하여 LLM의 각 코드 설명에 대한 예측을 식별합니다.\n\n```js\ndef remove_noisy_prefix(text):\n    # 문자 또는 숫자가 뒤따르고 점과 선택적 공백으로 시작하는 문자열의 제일 앞에 있는 숫자나 문자를 제거합니다.\n    cleaned_text = text.replace(\"* \", \"\").strip()\n    cleaned_text = re.sub(r\"^\\s*\\w+\\.\\s*\", \"\", cleaned_text)\n    return cleaned_text.strip()\n\ndef parse_outputs(output, code_description_map, model_name):\n    \"\"\"\n    모델 출력을 구문 분석하여 주어진 설명 매핑에 따른 ICD-10 코드를 확인합니다.\n\n    Args:\n        output (str): 확인을 포함하는 모델 출력입니다.\n        code_description_map (dict): 설명과 ICD-10 코드의 매핑입니다.\n\n    Returns:\n        list of dict: 확인된 코드 및 해당 설명의 목록입니다.\n    \"\"\"\n    confirmed_codes = []\n    split_outputs = [x for x in output.split(\"\\n\") if x]\n    for item in split_outputs:\n        try:\n            code_description, confirmation = item.split(\":\", 1)\n            if model_name == \"meta-llama/Llama-2-70b-chat-hf\":\n                code_description = remove_noisy_prefix(code_description)\n\n            if confirmation.lower().strip().startswith(\"yes\"):\n                try:\n                    code = code_description_map[code_description]\n                    confirmed_codes.append({\"code\": code, \"description\": code_description})\n                except Exception as e:\n                    print(str(e) + \" Here\")\n                    continue\n        except:\n            continue\n    return confirmed_codes\n```\n\n이제 루프의 나머지를 살펴봅시다. 지금까지 우리는 프롬프트를 구성했고, LLM으로부터 응답을 받았으며, 출력을 구문 분석하여 LLM에 의해 관련이 있다고 판단된 코드를 식별했습니다.\n\n```js\nwhile prompt_count \u003c 50:\n    code_descriptions = {}\n    for x in candidate_codes:\n        description, code = get_name_and_description(x, model_name)\n        code_descriptions[description] = code\n\n    prompt = build_zero_shot_prompt(medical_note, list(code_descriptions.keys()), model_name=model_name)\n    lm_response = get_response(prompt, model_name, temperature=temperature, max_tokens=500)\n    predicted_codes = parse_outputs(lm_response, code_descriptions, model_name=model_name)\n\n    for code in predicted_codes:\n        if cm.is_leaf(code[\"code\"]):\n            assigned_codes.append(code[\"code\"])\n        else:\n            parent_codes.append(code)\n\n    if len(parent_codes) \u003e 0:\n        parent_code = parent_codes.pop(0)\n        candidate_codes = cm.get_children(parent_code[\"code\"])\n    else:\n        break\n\n    prompt_count += 1\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 예측된 코드를 반복하며 각 코드가 \"leaf\" 코드인지 확인합니다. 이는 코드가 유효하고 할당 가능한 ICD 코드임을 보증하는 것입니다. 예측된 코드가 유효하면 LLM이 그 의료 노트에 대한 예측으로 간주합니다. 유효하지 않으면 상위 코드에 추가하여 ICD 트리를 더 탐색하기 위해 자식 노드를 얻습니다. 더 이상 탐색할 상위 코드가 없을 경우 루프를 탈출합니다.\n\n이론적으로 의료 노트 당 LLM 호출 수는 임의로 높을 수 있으며, 알고리즘이 많은 노드를 탐색하는 경우 지연 시간이 증가할 수 있습니다. 저자는 의료 노트 당 최대 50회 프롬프트/LLM 호출로 처리를 종료하는 최대 수를 시행했습니다. 이 한계는 우리가 구현에서도 채택합니다.\n\n## 결과\n\n이제 GPT-3.5와 Llama-2를 LLM으로 사용하여 트리 탐색 알고리즘의 결과를 평가할 수 있습니다. 우리는 알고리즘의 성능을 마이크로-평균 및 매크로-평균 정밀도, 재현율 및 F1 점수를 통해 평가합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![ExploringLLMsforICDCodingPart1_2](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_2.png)\n\n구현 결과는 논문에 보고된 점수와 대략적으로 일치하지만 주목할 만한 차이점이 있습니다.\n\n- 이 구현에서 GPT-3.5의 마이크로 평균 측정 지표는 보고된 값보다 약간 뛰어나지만, 매크로 평균 측정 지표는 보고된 값보다 조금 부족합니다.\n- 마찬가지로 Llama-70B의 마이크로 평균 측정 지표는 보고된 값과 일치하거나 조금 뛰어나지만, 매크로 평균 측정 지표는 보고된 값보다 낮습니다.\n\n앞서 언급했듯이, 이 구현은 몇 가지 미세한 차이점을 가지고 있어 최종 성능에 영향을 미칩니다. 이 구현이 원본 논문과 어떻게 다른지에 대한 보다 자세한 내용은 링크된 저장소를 참조해 주세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n이 방법을 이해하고 구현하는 것은 여러 측면에서 나에게 매우 유익했습니다. 이를 통해 대규모 언어 모델(LLMs)의 강점과 약점에 대해 보다 세밀하게 이해할 수 있었고 임상 코딩 사례에서 그것을 구현할 수 있었습니다. 구체적으로, 코드에 관련된 중요한 정보에 동적으로 접근할 수 있는 경우 LLMs는 임상 문맥을 효과적으로 이해하고 관련 코드를 정확하게 식별할 수 있다는 것이 분명해졌습니다.\n\nLLMs를 임상 코딩을 위한 대리자로 활용하는 것이 성능을 더욱 향상시킬 수 있는지 탐구하는 것이 흥미로울 것입니다. 생명공학 및 임상 텍스트에 대한 외부 지식 소스가 논문이나 지식 그래프 형태로 풍부하게 제공되는 상황에서 LLM 대리자는 의료 문서를 보다 세밀한 단위로 분석하는 워크플로에 활용될 수 있습니다. 또한 필요한 경우 외부 지식을 참고하여 최종 코드에 도달할 수 있도록 동적으로 도구를 활용할 수도 있습니다.\n\n## 감사의 글\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 방법을 평가하는 데 도움을 준 이 논문의 주 저자 Joseph에게 큰 감사를 표합니다!\n\n- 참고 자료:\n\n[1] https://www.who.int/standards/classifications/classification-of-diseases\n\n[2] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. W. H., Feng, M., Ghassemi, M., … \u0026 Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database Sci. Data, 3(1), 1.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n[3] Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., \u0026 Sontag, D. (2022). 대형 언어 모델은 소수의 적은 데이터로도 임상 정보를 추출합니다. arXiv 사전 인쇄 arXiv:2205.12689.\n\n[4] Zhou, H., Li, M., Xiao, Y., Yang, H., \u0026 Zhang, R. (2023). 임상 관계 추출을 위한 LLM Instruction-Example Adaptive Prompting (LEAP) 프레임워크. medRxiv : 의학과학 사전 인쇄 서버, 2023.12.15.23300059. https://doi.org/10.1101/2023.12.15.23300059\n\n[5] Boyle, J. S., Kascenas, A., Lok, P., Liakata, M., \u0026 O’Neil, A. Q. (2023, 10월). 상업용 대형 언어 모델을 사용한 자동 임상 코딩. NeurIPS 2023에서 Deep Generative Models for Health Workshop 발표.\n\n[6] Huang, C. W., Tsai, S. C., \u0026 Chen, Y. N. (2022). 사전 훈련된 언어 모델로 자동 ICD 코딩하기: PLM-ICD. arXiv 사전 인쇄 arXiv:2207.05289.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMiranda-Escalada, A., Gonzalez-Agirre, A., Armengol-Estapé, J., \u0026 Krallinger, M. (2020). CLEF (Working Notes), 2020에서 CodiEsp Track의 비영어 임상 사례에 대한 주석, 가이드라인 및 솔루션에 대한 개요.\n\nMiranda-Escalada, A., Gonzalez-Agirre, A., \u0026 Krallinger, M. (2020). CodiEsp corpus: ICD10 (CIE10)로 코드화된 골드 표준 스페인어 임상 사례 - eHealth CLEF2020 (1.4) [데이터 세트]. Zenodo. https://doi.org/10.5281/zenodo.3837305 (CC BY 4.0)\n","ogImage":{"url":"/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png"},"coverImage":"/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png","tag":["Tech"],"readingTime":23},{"title":"살아있는 인공지능의 첫 걸음, 바디 인텔리전스","description":"","date":"2024-05-17 19:35","slug":"2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence","content":"\n두 개의 세계가 충돌하고 있어요.\n\n우리는 방금 두 가지 분야에서 로봇 공학의 상당한 도약을 목격했어요. Figure AI의 말하는 로봇과 Google의 일반적인 에이전트 SIMA가 그겁니다.\n\n하지만, 아니요, 이것은 일반적인 인공 지능(AGI)은 아니에요. 일부 사치스럽지만 입증되지 않은 주장이 돌아다니고 로봇들이 우리를 다 죽일 거라고 말하진 않을 거예요.\n\n그러나 두 소식이 이미 자체로 흥미롭지만, 그들 간의 시너지는 내 관점에서 AI 구현 지능의 첫 번째 발걸음이라고 생각해요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# GPT-4의 구현\n\n그러면 Figure AI는 뭔가요?\n\nFigure AI는 로봇 회사로, CEO의 말에 따르면 \"위험하고 원하지 않는 직업이 필요하지 않도록 하여 미래 세대가 더 행복하고 의미 있는 삶을 살도록 허용하는 로봇을 구축하고 있습니다.\"\n\n이미 유사한 웅장한 사명을 가진 회사들을 들어보셨을 것입니다. 그러나 OpenAI, NVIDIA, Jeff Bezos, 그리고 Intel이 시장에 제품이 없는 회사에 대해 26억 달러 평가액에서 6억 7500만 달러 시리즈 투자를 한다면, 그들이 무엇인가를 찾고 있다는 것을 알 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 왜 모두가 이 회사에 대해 이야기하는 걸까요? 이 비디오 때문입니다.\n\n그 링크를 클릭하는 것을 강력히 권장합니다. 간단히 말하자면, 이 비디오를 통해 사람과 상호작용하는 로봇이 여러 작업을 민첩하게 수행한다는 것을 보여줍니다.\n\n흥미로운 점은 Figure AI의 로봇의 핵심에는 GPT-4V, OpenAI의 주요 다중모달 대형 언어 모델인 MLLM이 있다는 것입니다.\n\n다시 말해, Figure AI의 로봇은 '화신 ChatGPT'의 처음으로, 즉 LLMs가 실체화된 작업도 할 수 있는 능력이 생긴 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 대부분의 사람들이 놓칠 수도 있는데, 로봇이 이전 동작에 대한 이유를 설명하면서 사람에게 작업을 수행하도록 요청하는 순간에 주목하여 주길 바랍니다.\n\n이 질문이나 요청이 사소한 것은 아니라는 점을 분명히 이해해야 합니다. 모델이 동시에 여러 작업을 수행할 수 있는 능력을 보여주기 위해 일부러 그랬던 것입니다.\n\n구체적으로 말하자면, 그들은 GPT-4를 세밀하게 조정하여 텍스트와 작업 표현을 출력하도록 만들었는데, 전자는 보코더를 통해 음성으로 디코딩되고, 후자는 액추에이터 동작으로 디코딩되어 몸을 움직이도록 합니다.\n\nFigure AI의 로봇의 기본 메커니즘에 대해서는 많이 알지 못하지만, Deepmind의 RT-2 모델과 같은 예시를 통해, 우리는 이미 LLM을 로봇 동작이나 텍스트를 출력하도록 훈련하는 방법을 증명한 연구자들이 있기에 꽤 좋은 아이디어를 얻을 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png)\n\n그러나 RT-2는 GPT-4의 능력에 가까운 두 LLM인 PALM-E와 PALI-X를 사용하여 훈련된 것을 고려할 때, Figure AI의 로봇 뒤에 있는 LLM이 이 모델이 지금까지 존재한 가장 고급 비전-언어-행동 모델일 수 있다는 주장이 가능합니다.\n\n하지만 현실적으로 생각해 봅시다.\n\n그것은 여러 차례 연습된 것일 수 있는 매우 제한된 데모였습니다. 사실, 로봇이 일반적인 용도의 능력 측면에서 아직 매우 초기 단계에 있다는 것은 거의 확실합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n따라서, 이 로봇들의 여정에서 다음 단계를 상상하기 위해 구글이 방금 공개한 것에서 영감을 받을 수 있습니다.\n\nSIMA, 3D 총론적 에이전트.\n\n하지만 이번에는 Figure AI의 경우와 달리, 에이전트에 대해 훨씬 더 많은 정보를 갖고 있습니다.\n\n# SIMA, 최초의 진정한 총론적 에이전트?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nGoogle의 확장 가능한 인공 지능 멀티월드 에이전트(SIMA) 프로젝트는 어떤 3D 환경에서 임의의 언어 명령을 이해하고 실행할 수 있는 인공 지능(AI) 시스템을 만들고자 합니다.\n\n이 계획은 언어를 지각 및 다양한 가상 세계에서 실제 행동과 결합하여 처리함으로써 일반적인 AI 개발에서 중요한 과제에 대처합니다. 이는 연구 환경 및 상업 비디오 게임을 포함한 다양한 가상 세계를 대상으로 합니다.\n\n간단히 말하면, SIMA는 언어 요청을 받아들이고 이를 3D 환경에서 키보드 및 마우스 조작으로 변환합니다.\n\n그러나 고려해야 할 중요한 요소는 다음과 같습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 에이전트들의 목표는 \"자원을 수집하고 집을 짓는\" 등 사용자가 제시하는 자연어 지침대로 작업을 수행하는 것입니다.\n\n간단히 말해, 모델은 해당 환경에서 상호작용할 때 사람들과 똑같은 입력을 갖고 있으므로 사람들보다 약점이 없다는 것을 의미하며, 이는 기접근 API나 이와 유사한 것에 대한 접근 권한이 없다는 것을 의미합니다.\n\n결과적으로, 요구된 작업을 수행하는 유일한 방법은 해당 작업을 수행하는 데 사람이 수행할 키보드 및 마우스 작업을 예측하는 것입니다.\n\nSIMA 에이전트는 다음 구성 요소로 이루어져 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 마크다운 형식으로 변경한 텍스트입니다.\n\n![이미지](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_1.png)\n\n- 인코더:\n\n  - 텍스트 인코더: 언어 명령을 모델이 해석할 수 있는 임베딩으로 번역합니다.\n  - SPARC 개발을 기반으로 한 이미지 인코더.\n  - 비디오 인코더.\n\n2. Multi-modal Transformer + Transformer XL: 두 개의 트랜스포머 아키텍처로, 전자는 모달 간 교차 어텐션을 수행하고, 후자는 이전 상태를 취해 새로운 상태를 식별합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n3. 정책: 선택된 행동이 600가지 가능한 기술 중 하나로 결정되는 분류 헤드\n\n여기에는 해석할 내용이 많기 때문에 단계별로 진행해 봅시다.\n\n## 세계 처리\n\n대부분의 최신 모델들에서와 마찬가지로, 첫 번째 단계는 입력을 \"인코딩\"하는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n일반적인 용어로 설명하자면, 입력 데이터(텍스트와 비디오)를 가져와 해당 인코더를 사용하여 이러한 데이터 포인트를 벡터 임베딩으로 변환하는 것이 아이디어입니다.\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_2.png)\n\n하지만 왜 이 작업을 하는 것일까요?\n\n이 변환을 수행함으로써 각 요소가 해당 개념의 의미론을 캡처하는 밀집 벡터로 표현됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다시 말해, 의미론적으로 유사한 개념은 유사한 벡터를 갖게 되어 벡터 공간에서 표현되었을 때 더 가까이 위치하게 됩니다:\n\n![Concept Vectors](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_3.png)\n\n또 다른 중요한 이점은 '개념을 벡터로 변환함'으로, 인간이 아주 무의식적으로 수행하는 '우리 세계를 이해하는' 행위를 수학적인 연습(컴퓨터에 이상적)으로 변환하여, 개념이 이제 수치로 표현된다는 것입니다.\n\n특히, 이 모델은 이러한 벡터들 사이의 유사성(그들 사이의 거리)을 계산하여, 어떤 개념이 다른 것들과 유사한지를 알아냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어, 모델은 '개'와 '고양이'가 유사한 속성 (포유류, 길들여진 동물, 네 다리 등)을 공유하는 유사한 개념을 나타낸다는 것을 알 수 있습니다.\n\n분명히 말씀드리자면, AI가 각 숫자를 다루는 방식은 아니며, AI가 중요시하는 것은 벡터 간의 근접성입니다. 만약에 도움이 될 경우, 벡터의 의미를 단순히 벡터 자체로만 생각하지 마십시오 (우리는 AI가 정말 '개'가 무엇인지 아는지 확신할 수 없습니다) 대신, 가장 가까운 이웃들의 합으로 생각하고, 모델에게 다른 것들과 유사하다는 신호를 보내는 것으로 여기면 됩니다 (강아지는 고양이와 유사하며 문과는 다릅니다).\n\n비슷한 이유로 유사성은 데이터 유형이 다른 상황에 집중하는 데 중요한 역할을 합니다.\n\n다양한 데이터 유형에서 나온 벡터로 오는 여러 모달 상황에 집중하는 데 유용합니다.\n\n기본 아이디어는 '개'라는 단어와 '개의 이미지'가 유사한 벡터를 공유하도록 원한다는 것으로, 이것은 모델에게 두 가지가 동일한 개념을 나타낸다는 것을 알려줍니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하지만 텍스트와 이미지는 구조적으로 매우 다른 데이터 유형이기 때문에 서로 다른 인코더가 필요합니다.\n\n이것은 문제입니다. 비슷한 개념에 대해 유사한 임베딩을 생성하도록 보장하며 별도의 인코더를 훈련해야 합니다.\n\n이 문제를 해결하기 위해 SIMA는 SPARC 이미지 인코더를 사용합니다.\n\n매우 최근에 개발된 SPARC 인코더는 대부분의 다른 이미지 인코더와 매우 유사한 방식으로 훈련됩니다(대비 학습을 사용), 그러나 미세 구체적인 세부 사항을 더 잘 캡처할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가장 흔한 이미지 인코더들은 이미지의 지역 세부사항을 제대로 포착하지 못하는 문제가 있습니다.\n\n네, 그들은 이미지가 무엇에 대해인지를 알려줄 수 있지만, 이미지의 전역 의미를 설명하는 데 도움이 되지 않는 중요한 작은 세부사항을 놓치기도 합니다. 그럼에도 불구하고 많은 경우에 중요합니다.\n\nSPARC는 유사한 방법을 제안하지만 매우 흥미로운 요소를 추가합니다.\n\n예를 들어, \"바구니 속 고양이와 개\"를 나타내는 이미지-텍스트 쌍이 있다고 가정해 봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_4.png\" /\u003e\n\n- 먼저, SPARC는 이미지를 패치로 나눕니다.\n- 그런 다음 각 패치에 대해 텍스트 설명 중 하나의 토큰을 할당합니다. 예를 들어, 패치가 개의 일부를 나타내면 \"개\"라는 단어가 할당됩니다.\n- 이 작업은 모든 패치(왼쪽 색상 그리드의 수직 열)에 대해 수행되며, 텍스트 설명의 여러 측면을 다루는 패치의 경우 각각에 가중치가 할당됩니다.\n\n- 특정 개념에 할당된 모든 패치를 가지고 있으면 각 패치가 '개'와 같은 특정 단어에 부여하는 가중치를 그룹화하여 이 그룹화된 임베딩을 실제 단어와 비교합니다. 그러나 여기에는 중요한 점이 있습니다. 이미지 전역에 대해 적용하는 대신, 모든 개념에 대해 이를 다섯 번 적용합니다.\n\n하지만 모든 이 작업을 하는 이유는 무엇일까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지역성. 이미지의 특정 부분이 이제 특정 개념에 할당되었으므로 모델은 이제 두 가지를 알게 됩니다:\n\n- 이미지에서 어떤 개념이 더 많이 표현되는지\n- 이러한 개념이 이미지에서 어디에 위치하는지.\n\n일반인이 이해하기 쉽게 설명하면 SPARC와 다른 이미지 인코더 간의 주요 차이점은 텍스트 설명의 개별 단어를 이미지의 특정 부분에 할당한다는 사실에 있습니다.\n\n이러한 방식으로 이미지의 특정 부분의 그룹화된 임베딩이 '개' 단어 토큰에 크게 편향되어 있다면, 이미지의 해당 영역에는 아마도 개가 포함되어 있을 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSIMA의 중요한 요소입니다. 3D 에이전트는 요청된 작업의 일환으로 상호 작용할 특정 객체를 식별할 수 있어야합니다.\n\n마지막으로, 비디오 인코더에 대해 이야기하면, 모델이 과거 상태를 고려할 수 있도록 포함되어 있습니다. 중요한 것은 비디오 인코더가 시간적 인식을 포함한다는 것인데, 이는 텍스트나 이미지 인코더가 제공할 수 없는 기능입니다.\n\n환경의 현재 상태뿐만 아니라 과거의 환경 상태와 취해진 조치에 따라 다음 조치를 취할지 결정하기 때문에 이는 중요합니다.\n\n## 최적의 정책 선택\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n제공된 정보를 바탕으로 SIMA는 다른 인코더들에 의해 생성된 표현을 사용하여 변환 모델 세트를 활용합니다. 이 모델은 LLM이 단어를 예측하는 대신 작업을 출력하여 에이전트가 실행할 키보드 및 마우스 조작을 지시합니다.\n\n한편, 왜 Gemini, 구글의 MLLM을 사용하는 대신 모델의 주요 '두뇌'로 이상한 Transformers 세트를 사용했는지 궁금할 수도 있습니다.\n\n이유는 아마도 예산 때문인데, 연구자들 자신들이 기술 보고서에서 SIMA의 분명한 다음 단계는 Gemini를 사용하는 것이라고 인정했기 때문입니다.\n\n이것은 매우 흥미로운데, 최고의 '두뇌'를 사용하지 않았음에도 놀라운 결과를 얻었다고 하니, 이제 우리가 곧 살펴볼 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 진정한 일반화자\n\n지금쯤이면 알 수 있겠지만, 목표는 여태까지 한 게임에서도 능숙한 에이전트를 훈련시키는 것이었습니다. 게임을 해본 적이 없는 게임조차도요.\n\n훈련 후 SIMA 에이전트는 다양한 범주로 구분된 600가지 다양한 기본 작업을 수행할 수 있었습니다. 이러한 범주에는 네비게이션, 동물 상호작용, 음식 등이 포함되어 있었습니다:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_5.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSIMA가이러한작업을수행하고있는모습을여기에서확인할수있습니다. 게다가, SIMA는매우유망한결과를얻어 언급할만한가치가있습니다.\n\n우선, 다양한게임에서훈련을받았음에도불구하고, 평균적으로SIMA는 단일게임에서전문화된 에이전트들보다 더우수한성능을보였습니다... 그특정게임안에서:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_6.png)\n\n더욱인상적인점은, 여러가지다른게임에서, 에이전트가영이의업무(non-trivial tasks)를달성했으며, 이중대부분은얼마들이나예시없이(zero-shot tasks)성과를거두었으며, 다시한번전문화된 에이전트들을이겼습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_7.png\" /\u003e\n\n이는 과거에 본 적 없는 환경에 놓여도 모델이 일반적으로 잘 수행되었으며, 때로는 Goat Simulator 3와 같은 경우에는 전문화된 에이전트(해당 게임에서만 훈련된 에이전트)보다 뛰어난 성과를 보였습니다.\n\n그렇다면 이 모든 것이 의미하는 바는 무엇인가요?\n\n간단히 말하자면, 우리는 게임 간의 지식 전이의 구체적인 증거를 관찰하고 있다는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다시 말해, 모델은 일부 게임에서 유용하게 적용할 수 있는 의미 있는 기술을 학습합니다 (예: 키보드로 이동하는 방법을 배우는 것과 같이).\n\n더욱이, 이러한 기술들은 상당히 높은 품질을 갖고 있어서 이 일반화된 에이전트가 많은 게임에서 특화된 에이전트들을 이기는 것을 의미하며, 이는 일반화된 접근 방식이 그들이 다양한 환경에 적용할 우수한 기술을 학습하는 데 도움이 된다는 것을 시사합니다.\n\nFigure AI의 발전과 함께 이러한 결과들이 어떠한 맥락에서 중요성을 얻을 수 있는 경우, 전체적으로 매우 인상적인 결과입니다.\n\n# 로보틱스에게 훌륭한 주였습니다\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAI 로봇 기술은 요즘 정말 빠르게 발전하고 있어요.\n\n- 한 쪽에서, Figure AI는 우리가 점점 더 많은 수동 작업 범위를 알리는 인간형 로봇을 만들어내는 데 탁월한 실력을 보여줍니다.\n- 반면에, SIMA는 3D 환경에서 최초의 일반 적 에이전트를 보여주고 있음을 암시합니다.\n\n하지만 우리가 깨닫는 것은 시너지의 잠재력입니다.\n\n이러한 에이전트들을 실생활 상황으로 가져오는 데는 아직 이르지만, 이 두 분야 사이의 융합은 다음 단계로 자연스러운 발전을 이루고 있습니다; SIMA가 훈련의 장소일 뿐만 아니라, Figure AI 로봇들이 일반적인 에이전트의 구현체 역할을 하기 때문이죠.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그리고 다른 기업들도 구체적 지능에 대한 자신들의 시사를 제시하며 경쟁이 치열해지고 있습니다. 많은 기존 기업들이 인류의 기술이 충분히 준비되어 있다고 느끼고 있어서 다음 큰 도전, 인공지능을 실생활에 적용하는 것에 착수할 준비가 되어있다고 느끼는 것이 분명합니다.\n","ogImage":{"url":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png"},"coverImage":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png","tag":["Tech"],"readingTime":15},{"title":"내일의 기계들","description":"","date":"2024-05-17 19:34","slug":"2024-05-17-MachinesofTomorrow","content":"\n# 내일의 기계: AI 원래부터 초지능 및 넘인간성까지, 어떻게 AI가 우리의 세계를 형성할 것인가\n\n아마존에서 \"내일의 기계\" 구매\n\n아마존이나 굿리드에서 리뷰 작성\n\n우리의 뉴스레터 구독하여 모든 챕터를 받아보세요\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Machines of Tomorrow](/assets/img/2024-05-17-MachinesofTomorrow_0.png)\n\n우리의 세상을 형성할 AI.\n\n모두가 쉽게 접근할 수 있는 쉬운 읽기 스타일로 작성된 \"Machines of Tomorrow\"은 인공 지능의 역사와 설명을 철저하게 소개함으로써 독자들이 실제 전망을 파악하고 최종적으로 이 책의 핵심 개념과 더 깊은 참여 수준을 얻게 합니다. 다가오는 수십 년 동안 사회들이 AI로 무엇을 기대해야 하는지에 대한.\n\nAI가 가까운 미래와 중기에 고용과 교육에 어떤 영향을 미칠까요? AI가 우리 사회, 경제, 정부, 문화 및 지정학에 어떤 영향을 미칠까요? AI 구현의 유토피아적 및 디스토피아적 결과는 무엇이며 누가 영향을 받을 것인가요? 중국과 새로운 AI 무기 경쟁에 대해 실제로 무슨 일이 벌어지고 있을까요? 합성생물학이 정말 인간-사이보그 공존으로 이끌 것이며 결국 인간 진화에 영향을 미칠까요? 이 기술의 불가피한 진전에 대비하려면 리더들에게 지금 어떤 것을 요구해야 할까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nPedro Uria-Recio와 Randy McGraw가 제기하고 대답한 몇 가지 신선하고 논쟁적인 질문들 중 일부입니다. Pedro Uria-Recio는 McKinsey 컨설턴트 출신, Randy McGraw는 AI 부문 최고 책임자 및 Amazon 전 임직원으로 이들은 수년간 이 기술을 전 세계적으로 활용해 왔습니다.\n\n기술자일지라도 주부일지라도 사업가일지라도 학생일지라도 유권자일지라도 정책 입안자일지라도 혹은 단순히 인류의 미래에 대해 궁금해하는 분이라도, \"Machines of Tomorrow\"은 인공지능의 복잡성을 탐험하는 데 필수적인 안내를 제공합니다.\n\n인류와 AI가 얽히는 지점.\n\nAI는 우리의 새로운 마음입니다. 로봇공학은 새로운 신체입니다. 우리는 탄소와 실리콘의 교차로에서 새로운 종이 어떻게 될까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAI가 기하급수적으로 발전하고 있습니다.\n\n일반적인 AI. 안드로이드와 사이보그. 합성 생물학. 양자 컴퓨팅. 마음의 모방. 이 모든 것들이 어떻게 펼쳐질까요?\n\nAI 독재가 위협적입니다.\n\nAI는 진실을 무효화하고, 자유를 재정의하며, 일자리 부족을 불러올 것입니다. 우리는 아직도 AI를 모두의 이익을 위해 형성할 수 있을까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지구정치의 역학화.\n\n초인지능이 숭배받을 것입니다. 중국과 미국은 인공지능에 대한 견해를 놓고 충돌할 것입니다. 정치는 종족 정체성을 중심으로 펼쳐질 것입니다.\n\n인류의 위대한 서사시.\n\n신화에서 큐브릭까지. 아리스토텔레스에서 샘 알트만까지. 레오나르도 다 빈치에서 보스턴 다이내믹스까지. 오늘부터 초인지능까지.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음 장 `` 차례\n\n참고 자료, 용어 해설\n\n\"내일의 기계\"를 아마존에서 구입하세요\n\n아마존이나 구글 리드에서 리뷰 작성하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리의 뉴스레터 구독을 신청하시면 모든 챕터를 받아보실 수 있어요\n\n작성자: Pedro Uria-Recio, Randy McGraw와 함께\n\nmachinesoftomorrow.ai에서 확인해보세요\n\n![이미지](/assets/img/2024-05-17-MachinesofTomorrow_1.png)\n","ogImage":{"url":"/assets/img/2024-05-17-MachinesofTomorrow_0.png"},"coverImage":"/assets/img/2024-05-17-MachinesofTomorrow_0.png","tag":["Tech"],"readingTime":4},{"title":"라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기","description":"","date":"2024-05-17 19:32","slug":"2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi","content":"\n## Phi-2, Mistral, 그리고 LLaVA와 같은 모델을 Raspberry Pi에서 Ollama를 사용하여 로컬에서 실행하기\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png)\n\n자신의 기기에서 큰 언어 모델(LLMs)이나 시각 언어 모델(VLMs)을 실행해 보고 싶은 적이 있었나요? 아마 그렇겠죠. 그러나 모든 것을 처음부터 설정하고 환경을 관리하고 적절한 모델 가중치를 다운로드해야 한다는 생각, 그리고 기기가 해당 모델을 처리할 수 있는지에 대한 미심쩍음 때문에 조금 망설이셨을 겁니다.\n\n그보다 한 발 더 나아가보죠. 크기가 신용카드보다 작은 기기인 Raspberry Pi에서 자체 LLM 또는 VLM을 운영하고 있다고 상상해보세요. 불가능한 것일까요? 전혀 그렇지 않아요. 내가 이 게시물을 작성하고 있으니까 가능한 일이죠.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 가능하다면, 그렇게 해도 괜찮아요. 하지만 왜 그걸 하고 싶으세요?\n\n현재에는 가장자리의 LLM이 매우 비현실적으로 보입니다. 하지만 특정한 이러한 예외적인 사용 사례는 시간이 지나면 성숙해질 것이며, 기기에서 실행되는 올 로컬 생성형 AI 솔루션을 이용해 멋진 가장자리 솔루션이 구축될 것입니다.\n\n이것은 무엇이 가능한지 알아보고자 하는 것입니다. 만약 이것이 컴퓨팅 규모의 극단단에 가능하다면, 라즈베리 파이와 강력한 서버 GPU 사이의 어느 수준에서든 가능할 것입니다.\n\n전통적으로, 가장자리 AI는 컴퓨터 비전과 밀접하게 관련되어 왔습니다. LLMs와 VLMs의 배포를 탐색함으로써 가장자리에 이러한 새로운 면을 추가하면 이 분야에서 떠오르고 있는 흥미로운 측면을 발견할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가장 중요한 것은, 최근에 얻은 Raspberry Pi 5에서 뭔가 재미있는 것을 하고 싶었습니다.\n\n그래서, Raspberry Pi에서 이 모든 것을 어떻게 구현할 수 있을까요? Ollama를 사용해서!\n\n## Ollama가 무엇인가요?\n\nOllama는 처음부터 설정하기 귀찮은 일 없이 개인 컴퓨터에서 로컬 LLM을 실행하는 데 최적의 솔루션이 되어 나타났습니다. 몇 가지 명령어로 모든 것을 문제없이 설정할 수 있습니다. 모든 것이 독립적으로 구성되어 있으며, 몇 가지 기기 및 모델에서의 제 경험에 따르면 훌륭하게 작동합니다. Raspberry Pi에서 실행시켜 놓고 원한다면 다른 응용프로그램 및 기기에서 호출할 수 있는 모델 추론을 위한 REST API까지 노출합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_1.png\" /\u003e\n\n또한, 명령 줄 인터페이스에 대해 걱정하는 사람들을 위해 웅장한 AI UI/UX인 Ollama 웹 UI도 있습니다. 이것은 바로 지금 기본적으로 로컬 ChatGPT 인터페이스입니다, 만약 당신이 원한다면요.\n\n이 두 오픈 소스 소프트웨어는 현재 최상의 로컬 호스팅 LLM 경험을 제공한다고 생각합니다.\n\nOllama 및 Ollama 웹 UI는 LLaVA와 같은 VLM도 지원하며, 이는 엣지 생성적 AI 사용 사례에 대한 더 많은 가능성을 엽니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 기술 요구 사항\n\n다음만 있으면 됩니다:\n\n- Raspberry Pi 5 (속도가 느린 설정을 위해 4 버전 사용 가능) — 7B 모델에 맞게 8GB RAM 모델을 선택하세요.\n- SD 카드 — 최소한 16GB, 크기가 클수록 더 많은 모델을 넣을 수 있습니다. Raspbian Bookworm이나 Ubuntu와 같은 적합한 OS가 이미 로드되어 있어야 합니다.\n- 인터넷 연결\n\n이전에 언급했듯이, Raspberry Pi에서 Ollama를 실행하는 것은 이미 하드웨어 스펙트럼의 극단에 가깝습니다. 기본적으로 Raspberry Pi보다 강력한 장치는 Linux 배포판을 실행하고 유사한 메모리 용량을 가지고 있다면, 이론적으로는 Ollama와 이 포스트에서 논의된 모델을 실행할 수 있어야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 1. Ollama 설치하기\n\nRaspberry Pi에 Ollama를 설치하기 위해서는 리소스를 절약하기 위해 Docker를 사용하지 않을 것입니다.\n\n터미널에서 다음 명령을 실행하세요.\n\n```js\ncurl https://ollama.ai/install.sh | sh\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 명령을 실행한 후 아래 이미지와 유사한 내용을 보게 될 것입니다.\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_2.png)\n\n출력에 따라서 Ollama가 실행 중인지 확인하려면 0.0.0.0:11434로 이동하십시오. 'WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only mode.'라는 메시지가 표시되는 것은 Raspberry Pi를 사용하고 있기 때문에 정상입니다. 그러나 NVIDIA GPU가 있는 것으로 알려진 장치에서 이 지침을 따르고 있다면, 무언가 잘못되었습니다.\n\n문제 또는 업데이트가 있는 경우 Ollama GitHub 리포지토리를 참조하십시오.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 2. 명령 줄을 통한 LLM 실행\n\nOllama 공식 모델 라이브러리에서 Ollama를 사용하여 실행할 수 있는 모델 목록을 살펴보세요. 8GB 라즈베리 파이에서 7B보다 큰 모델은 맞지 않습니다. 마이크로소프트의 2.7B 크기인 Phi-2 LLM을 MIT 라이센스로 사용하겠습니다.\n\n기본 Phi-2 모델을 사용할 것이지만, 여기에서 찾을 수 있는 다른 태그 중에서 필요한 것을 자유롭게 사용해도 됩니다. Phi-2 모델 페이지를 확인하여 상호 작용하는 방법을 살펴보세요.\n\n터미널에서 다음을 실행하세요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n올라마 편리해요\n```\n\n아래 출력물과 유사한 것을 볼 때, 이미 라즈베리 파이에서 LLM이 작동 중임을 확인했습니다! 정말 간단해요.\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_3.png)\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기타 모델인 Mistral, Llama-2 등도 시도해 볼 수 있어요. 다만 모델 가중치를 저장할만큼 SD 카드에 충분한 공간이 있어야 해요.\n\n모델이 클수록 출력이 느려질 수 있어요. 예를 들어, Phi-2 2.7B에서는 초당 약 4토큰을 얻을 수 있어요. 하지만 Mistral 7B를 사용하면 세대 속도가 초당 약 2토큰으로 느려집니다. 토큰은 대략 한 단어와 동일한 양이에요.\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_5.png)\n\n이제 Raspberry Pi에서 LLMs가 실행되고 있지만 아직 끝나지 않았어요. 터미널이 모두에게 익숙한 것은 아니에요. Ollama 웹 UI도 실행해 보겠어요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3. Ollama 웹 UI 설치 및 실행\n\n공식 Ollama 웹 UI GitHub 저장소의 지침을 따라 Docker 없이 설치할 것입니다. 최소한 Node.js 버전이 `= 20.10 이 되어야 한다고 권장하므로, 해당 권장사항을 따를 것입니다. 또한 Python은 적어도 3.11 이어야 한다고 권장하나, Raspbian OS에는 이미 그 버전이 설치되어 있습니다.\n\n먼저 Node.js를 설치해야 합니다. 터미널에서 다음을 실행하세요.\n\n```js\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - \u0026\u0026\\\nsudo apt-get install -y nodejs\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 향후 독자들을 위해 20.x를 더 적합한 버전으로 변경하려면 변경하십시오.\n\n그런 다음 아래 코드 블록을 실행하십시오.\n\n```js\ngit clone https://github.com/ollama-webui/ollama-webui.git\ncd ollama-webui/\n\n# 필요한 .env 파일 복사\ncp -RPp example.env .env\n\n# Node를 사용하여 프론트엔드 빌드\nnpm i\nnpm run build\n\n# 백엔드와 함께 프론트엔드 제공\ncd ./backend\npip install -r requirements.txt -- break-system-packages\nsh start.sh\n```\n\n이는 GitHub에서 제공된 것을 약간 수정한 것입니다. 가상 환경을 사용하거나 --break-system-packages 플래그를 사용하는 등의 최선의 방법을 따르지 않고 간단하게하고자 하였으니 유의하시기 바랍니다. uvicorn을 찾을 수 없다는 오류가 발생하면 터미널 세션을 재시작하십시오.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모든 것이 제대로 진행되면 라즈베리 파이의 8080 포트를 통해 http://0.0.0.0:8080로 Ollama 웹 UI에 액세스할 수 있거나, 동일한 네트워크에서 다른 장치를 통해 접속하고 있다면 http://라즈베리 파이의 로컬 주소:8080/을 통해 접속할 수 있습니다.\n\n![이미지 설명](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_6.png)\n\n계정을 생성하고 로그인한 후, 아래 이미지와 유사한 화면이 보여야 합니다.\n\n![이미지 설명](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_7.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델 가중치를 이전에 다운로드했다면, 아래와 같이 드롭다운 메뉴에 표시될 것입니다. 그렇지 않은 경우 설정으로 이동하여 모델을 다운로드할 수 있습니다.\n\n![이미지 1](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_8.png)\n\n![이미지 2](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_9.png)\n\n전체적으로 사용자 인터페이스는 매우 깔끔하고 직관적이므로 설명할 것이 별로 없어요. 정말로 아주 훌륭하게 구현된 오픈 소스 프로젝트입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_10.png)\n\n## 4. Ollama 웹 UI를 통해 VLM 실행하기\n\n이 글을 시작할 때 언급했듯이, VLM도 실행할 수 있습니다. LLaVA를 실행해보겠습니다. 이것은 Ollama에서도 지원되는 인기 있는 오픈 소스 VLM입니다. 실행하려면 인터페이스를 통해 'llava'를 가져와서 가중치를 다운로드하면 됩니다.\n\n안타깝게도, LLM과는 달리 Raspberry Pi에서 이미지를 해석하는 설정에 꽤 많은 시간이 걸립니다. 아래 예시는 처리하는 데 약 6분 정도 걸렸습니다. 대부분의 시간은 아마 이미지 측면이 아직 제대로 최적화되지 않았기 때문인데, 이는 앞으로 확실히 변경될 것입니다. 토큰 생성 속도는 초당 약 2토큰입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_11.png)\n\n## 모든 것을 감싸며\n\n지금까지 이 기사의 목표를 거의 달성한 상태입니다. 요약하자면, 우리는 Ollama와 Ollama Web UI를 사용하여 Raspberry Pi에서 Phi-2, Mistral, LLaVA와 같은 LLMs와 VLMs를 실행하는 방법을 성공적으로 이끌어냈습니다.\n\nRaspberry Pi나 다른 작고 가장한 장치에서 실행되는 로컬 호스팅 LLMs에 대한 다양한 사용 사례들을 상상할 수 있습니다. 특히, 4토큰/초가 Phi-2 크기의 모델을 위해 일부 사용 사례에 대해 스트리밍 속도로는 합당해 보입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n'작은' LLM 및 VLM의 분야는 얼마 전에 다소 모순적으로 이름이 지어졌지만 '큰' 지칭을 받는 활발한 연구 분야입니다. 최근에는 꽤 많은 모델이 출시되었습니다. 이 새로운 트렌드가 계속되어 더 효율적이고 간결한 모델들이 계속 출시되기를 바랍니다! 다가오는 몇 달 동안 살펴봐야 할 분야입니다.\n\n면책 성명: 저는 Ollama나 Ollama Web UI와 어떠한 관련도 없습니다. 모든 의견과 견해는 제 개인적인 것이며 어떤 조직도 대표하지 않습니다.\n","ogImage":{"url":"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png"},"coverImage":"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png","tag":["Tech"],"readingTime":11},{"title":"라즈베리 파이를 원격으로 접속하는 방법 Tailscale을 활용한 포괄적인 가이드","description":"","date":"2024-05-17 19:30","slug":"2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale","content":"\n\u003cimg src=\"/assets/img/2024-05-17-원격으로RaspberryPi에액세스하는포괄적인가이드Tailscale을사용하여\" /\u003e\n\n현재 연결된 세상에서 장치에 원격으로 액세스하는 능력은 놀라운 융통성과 유용성을 제공합니다. 라즈베리 파이 서버를 관리하거나 파일에 액세스하거나 어디에서든 IoT 장치를 제어하려는 경우 안전한 네트워크 액세스는 중요합니다. 이 안내서는 와이어가드(WireGuard) 암호화를 활용하여 안전한 네트워크를 만드는 강력하고 안전한 서비스인 Tailscale을 사용하는 데 초점을 맞춥니다. 우리는 Raspberry Pi에 Tailscale을 설정하여 가정 네트워크 외부에서 액세스하는 단계를 안내할 것입니다.\n\n# 원격 액세스 소개\n\n원격 액세스를 통해 인터넷을 통해 다른 위치에서 Raspberry Pi에 연결할 수 있습니다. 이겢은 서버를 관리하거나 업데이트를 실행하거나 집을 벗어난 상태에서 미디어 파일에 액세스하는 데 특히 유용할 수 있습니다. 그러나 포트 전달과 같은 전통적인 방법은 네트워크를 보안 위험에 노출시킬 수 있습니다. Tailscale은 안전한 VPN을 만들어 더 안전하고 간단한 대안을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 필요한 준비물\n\n- 인터넷에 연결된 라즈베리 파이 헤드리스 시스템\n- Tailscale 계정: 개인용으로 무료로 제공됩니다.\n- SSH 접근: 라즈베리 파이에 소프트웨어를 설치하고 구성하기 위해 필요합니다.\n\n# 단계 1: 라즈베리 파이에 Tailscale 설정하기\n\n라즈베리 파이에 연결하세요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 아래 명령어를 사용하여 Raspberry Pi에 SSH로 연결하세요:\n\n```js\nssh pi@\u003cIP-ADDRESS\u003e\n```\n\n`\u003cIP-ADDRESS\u003e` 자리에 Raspberry Pi의 실제 IP 주소를 입력해주세요.\n\nTailscale 설치:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n라즈베리 파이에 Tailscale 패키지 소스를 추가하세요:\n\n```js\ncurl -fsSL https://pkgs.tailscale.com/stable/raspbian/buster.gpg | sudo apt-key add -\ncurl -fsSL https://pkgs.tailscale.com/stable/raspbian/buster.list | sudo tee /etc/apt/sources.list.d/tailscale.list\n```\n\n패키지 목록을 업데이트하고 Tailscale을 설치하세요:\n\n```js\nsudo apt update\nsudo apt install tailscale\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n시작 및 Tailscale 인증:\n\n- Raspberry Pi에서 Tailscale 서비스를 시작합니다:\n\n```js\nsudo tailscale up\n```\n\n터미널에서 제공된 인증 지침을 따르세요. 이는 URL을 방문하고 Tailscale 계정에 로그인하여 장치를 인증하는 과정을 포함합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 단계 2: 안전한 액세스를 위한 Tailscale 설정\n\nRaspberry Pi의 Tailscale IP 확인:\n\n인증이 완료되면 Raspberry Pi에 Tailscale IP 주소가 할당됩니다:\n\n```js\ntailscale ip -4\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n접근을 위한 Raspberry Pi 원격 액세스입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n클라이언트 장치에 Tailscale 설치하기:\n\n- 원격 액세스에 사용할 장치(예: 노트북, 스마트폰)에 Tailscale을 다운로드하고 설치하세요. Tailscale 앱은 Windows, macOS, Linux, iOS 및 Android용으로 제공됩니다.\n\n클라이언트 장치 인증하기:\n\n- 라즈베리 파이 설정과 유사하게 클라이언트 장치에서 Tailscale을 시작하고 Tailscale 계정을 통해 인증하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nRaspberry Pi에 SSH로 연결하기:\n\n- 두 장치가 Tailscale 네트워크에 연결되면, Raspberry Pi로 SSH를 사용할 수 있습니다. Raspberry Pi의 Tailscale IP를 사용하여 다음과 같이 명령을 입력하세요:\n\n```js\nssh pi@\u003cTAILSCALE-IP\u003e\n```\n\n위 명령에서 `TAILSCALE-IP`를 이전에 메모한 Tailscale IP 주소로 대체해주세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 단계 4: Tailscale를 사용하여 기기 관리하기\n\n- 연결된 기기 모니터링하기:\n\n- Tailscale 관리자 콘솔을 사용하여 모든 연결된 기기를 확인하고 권한을 관리할 수 있습니다.\n\n- 서브넷 설정하기 (옵션):\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 만약 라즈베리 파이의 모든 트래픽을 Tailscale 네트워크를 통해 라우팅하고 싶다면 서브넷 라우트를 설정할 수 있습니다. 이것은 Tailscale 네트워크의 다른 기기들이 액세스해야 하는 Pi 상의 서비스가 있는 경우 유용합니다.\n\n# 단계 5: 보안 강화\n\n- 정기적인 업데이트:\n\n- 라즈베리 파이와 모든 기기를 최신 Tailscale 및 OS 업데이트로 업데이트하여 보안 패치가 적용되도록 유지하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 강력한 인증 사용하기:\n\n- 라즈베리 파이와 Tailscale 계정에 강력하고 고유한 암호를 사용하세요. 추가적인 보안을 위해 이중 인증 (2FA)을 활성화하는 것을 고려해보세요.\n\n# 결론\n\n이제 라즈베리 파이에 Tailscale을 성공적으로 설정하여 전 세계 어디에서나 안전하게 액세스할 수 있습니다. Tailscale의 사용 편의성과 견고한 보안 기능은 가정 기기에 원격 액세스하기 위한 탁월한 선택지로 제공됩니다. 개인 프로젝트 또는 복잡한 IoT 시스템을 관리하든, Tailscale은 연결성과 제어를 유지하기 위한 확장 가능하고 안전한 방법을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 안내서는 원격 액세스를 위해 Tailscale을 설정하고 사용하는 방법에 대한 종합적인 소개를 제공합니다. 그러나 가능성은 여기서 끝나지 않습니다. Tailscale을 사용하면 네트워크를 확장하고 더 많은 기기를 추가하며 설정을 향상시키기 위해 자동화 스크립트를 통합할 수 있습니다. 원격 기기 관리의 세계가 여러분의 손끝에 있고, Tailscale은 이를 접근 가능하고 안전하게 만들어줍니다.\n\n## 리소스:\n","ogImage":{"url":"/assets/img/2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale_0.png"},"coverImage":"/assets/img/2024-05-17-AccessingYourRaspberryPiRemotelyAComprehensiveGuideUsingTailscale_0.png","tag":["Tech"],"readingTime":7},{"title":"수제 가슴 냉동고 아이스 배스를 만들고 싶으세요","description":"","date":"2024-05-17 19:29","slug":"2024-05-17-SoyouwanttobuildaDIYchestfreezericebath","content":"\n\u003cimg src=\"/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png\" /\u003e\n\n안녕, 미친놈! 너무너무 미쳤구나! 이런 걸 하고 싶어하는 너, 진짜 미치기 일보 직전이구나. 하지만 사실 나도 꽤 미친 놈이야. 만나서 반가워. 얼음물 가득한 냉동고 안으로 들어갈 얘기 좀 나눠볼까?\n\n단계 1: 몇 가지 꼭 해봐야지…진짜요.\n\n야, 차가운 목욕을 하는 열 두 명한테 물어보면, 다들 자신만의 \"완벽한\" 사이즈의 냉동고를 추천해주겠지. 너도 그것이 400 리터이어야 한다. 너도 14평방 피트라면 안 되잖아... 만약 이걸 하려면, 판매점에 가서 확인해보라고. 들어가 봐. 시체도 들어갈 수 있는지 묻는 농담을 해봐. 체포당하고. 그걸 어떻게 설명할 건지 볼까...\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하지만 진지하게 생각해보세요. 들어오세요. 편안히 앉으세요. 머리를 넣을지 고려해보세요. 움직일지 결정할지 또는 그냥 해버릴지 고려해보세요. 내가 산 식품 냉동고는 너무 작아. 작동은 되나요? 그렇죠... 근데 편안해요? 아니에요.\n\n단계 2: 확보하기\n\n이것은 가장 쉬운 단계에요. 두 가지 방법으로 할 수 있어요. 먼저 가게에 가서 사거나 Facebook Marketplace, Craigslist 또는 어디에서 중고로 구해오세요. 원하는 종류, 원하는 크기를 찾아서 이루어내세요.\n\n집에 가져와서 바닥에 어떤 종류의 패딩을 놓고 (제가 아이들용 폼 패딩을 사용했어요) 바퀴를 제거하세요. 무거운 것을 분산시키고, 물과 몸이 들어가면 더 힘들게 일해야 해요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n단계 3: 갈아내기\n\n이제 힘들어지네요. 자신을 위해 Ryobi 샌더를 구입했어요. 하지만 모든 벽을 갈아내보세요. 프라이머와 폰드 실드가 잘 부착되려면 표면이 거칠어야 해요. 또한 유해 물질을 마시지 않도록 마스크를 착용하세요.\n\n단계 4: JB 워터 웰드로 고쳐 보세요\n\n마린 실리콘이나 방수용 화이트 제품을 사용해도 좋지만, 작동하려면 오래 지속되길 원한다면 JB 워터 웰드를 사용하세요. 마침표. 이게 전체 작업에서 가장 어려운 부분이지만, 실제로 작동하기 때문에 가치가 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n파란색 나일론 장갑 몇 개를 구입하세요. 100개가 들어있는 팩팅이 필요할 거에요. 그리고 욕조 가장자리를 포함하여 욕조 모서리 전체에 충분한 양의 JBWW를 밀어 넣으세요. 위의 사진을 참고하여 실제로 채워졌는지 확인해주세요. JBWW가 장갑에 달라 붙고 잘 작동하지 않는 것 같으면 새로운 장갑을 착용해주세요. 제 말 믿어요, 나중에 저에게 감사할 거예요.\n\n그리고 냉동고에 배수구가 있나요? 그것도 채워주세요. 믿어요. 처음부터 누수 방지가 되도록 하고 싶으실 거예요.\n\n모두 채워지면 며칠 동안 그대로 둬주세요.\n\n단계 5: 바탕 도료하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그 다음으로, 그 야수에 맞추어 스프레이 페인트를 뿌려보세요! 러스톨리움 제품을 사용하지 마세요. 아마도 그 제품은 실패할 거에요. 덧붙여 말하면, 다음 단계가 그것에 잘 안붙을 거에요. 두 번 읽어 보세요.\n\n저는 내 것에서 이 다음 단계를 잘못했어요, 그러니 사진이 아니라 글에 주의하세요.\n\n욕조 안쪽 전체 및 욕조 외부 입술에 이르는 길 전체에 프라이머를 바르세요. 저는 내 것에서 내부 가장자리까지만 프라이먹를 바른 거에요.\n\nStep 6: 파운드 실드를 바르세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로, 연못 실드가 필요합니다. 매일 몸을 담그게 될 이 물건에 아끼지 말고 투자해야 합니다. 이 물건은 유기물에 안전해야 하며, 즉, 물고기와 사람에게 안전해야 합니다. 방수가 가능한 무언가를 찾아서 대충 봉인하려고 하지 마세요. 건강을 위해서 하는 일인 만큼, 싼 길로 가려다 몸을 해치지 마세요.\n\n페인팅을 하는데 제안할 만한 점은, 붓을 사용하고 작은 구역에서 혼합하는 것입니다. 즉, 연못 실드 상자 안의 두 캔을 함께 섞지 말고, 한 번에 모두 칠하는 것을 피하세요. 화학 혼합물을 만들고, 한꺼번에 섞으면 혼합용 통에 큰 덩어리가 생길 수 있어요 (믿어요).\n\n또한 빠른 속도로 진행하세요… 먼저 1/3 정도 혼합하고, 제대로 섞이면 되도록 빨리 칠하세요. 그런 다음 두 번째로 혼합하고, 되도록 빨리 칠하세요. 마지막으로 세 번째를 혼합하고, 되도록 빨리 칠하세요. 이 물건은 빨리 굳어가기 때문에 통 안이 아니라 욕조 위에 잘 덧칠되도록 해야 합니다.\n\n저는 제 것에 알코올을 혼합하지 않았지만, 여러분이 옳다고 생각하는 대로 하세요. 저는 제 것에 한 층만 발랐는데, 정말 좋은 성과를 내고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n7단계: 기다려주세요\n\n48시간 후에 시작해도 괜찮다고 하지만, 더 안전하게 한 주 정도 기다려서 확실히 고정되게 해주세요.\n\n8단계: 채워주세요\n\n물을 반 정도 채워주세요. 그리고 기다리세요. 어디서든 누수가 있나요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아니요?\n\n괜찮아요, 잠시만 기다려주세요.\n\n![Step 9: Inkbird time](/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n물을 부어 둔 채로 하루 정도 안 눌렀나요?\n\n축하해요!\n\n켜고 시작해봐요. 인크버드를 전원에 연결하고, 가슴 냉동고를 인크버드에 연결하고 설정값을 입력해주세요. 제가 설치할 때 사용한 동영상이에요.\n\n저는 추운 것을 원하지만 얼지 않게 하려고 3도와 좌우 2도를 선택했어요. 얼릴까 봐 걱정돼요. 그러니까 냉동고가 부서지거나 파손되거나 접착제가 떨어지지 않도록 하는 게 중요해요. 오랫동안 사용하고 싶어서 말이죠. 하지만 어떻게 할지는 당신맘대로에요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 내가 링크하고 있는 Inkbird 제품은 방수 센서가 장착된 업그레이드된 버전인 것으로 보입니다. 예전 버전을 구입한다면 절대 방수 처리가 되어있지 않기 때문에 꼭 주위에 JBWW를 사용해 주세요.\n\n단계 10: 몇 년 동안 계속해서 하루종일 얼음 목욕을 즐기세요!\n\n당신의 결과물도 보여주세요! 이번이 처음이라 완벽히 한 것은 아니라고 생각합니다. 몇 가지를 다르게 했으면 좋았을 것 같지만, 이 가이드를 따라하면 이루고 싶으시다면 큰 도움이 될 것입니다.\n\nwww.coldfeat.com에서 제 소식을 만나요! 당신과 연결하고 싶어요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n곧 시작하겠습니다! 혹시 어떤 특정 언어로 프로그래밍을 하시나요? 저에게 필요한 계속되는 안내를 해주세요. 😊✨\n","ogImage":{"url":"/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png"},"coverImage":"/assets/img/2024-05-17-SoyouwanttobuildaDIYchestfreezericebath_0.png","tag":["Tech"],"readingTime":6},{"title":"자초에서 시작하는 자아 찾기","description":"","date":"2024-05-17 19:27","slug":"2024-05-17-KnowYourselfSewYourself","content":"\n## DIY: 니가 스소!\n\n![이미지](/assets/img/2024-05-17-KnowYourselfSewYourself_0.png)\n\n## 얽힌 자아의 깨달음\n\n몇 년 전의 한 업무 회의에서, 운영자들은 두 개의 이분법적 선택을 대표하는 슬라이드를 보여주었습니다. 참가자들에게 두 옵션 중 어떤 것을 선택할지 기반으로 방의 한쪽으로 이동하라고 요청했습니다. 정렬된 후, 우리는 반대편 사람을 찾아가 우리의 입장을 논의해야 합니다. 그들은 젓가락을 사용하는지 여부, 화장지를 위로 넣는지 아래로 넣는지, 제로 인박스를 유지하거나 357개의 읽지 않은 이메일이 있는지와 같은 재미있는 주제들이었습니다. 한 이미지에서는 헤드폰 코드를 꼬아 정리하여 정돈하거나 전선이 얽힌 가방에 넣는 것 중 하나를 선택하라고 했습니다. 나는 후자를 선택했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저는 \"다른 편\"에서 온 친구이자 동료와 대화를 나눌 때, 그녀가 헤드폰 코드를 꼬리 말아 정리한 후에 정리하는 것이 \"미래의 나에게 선물\"이 된다고 언급했습니다. 그 말을 듣고 나도 한번 시도해보기로 결심했지만, 겪은 고충은 결국 저는 엉킨 사람이고 그렇게 되어야 한다는 것을 더 확신시켰습니다. 또한 왜 그런지를 이해하는 데 도움이 되었습니다: 헤드폰을 깔끔하게 정리하는 것이 어렵고, 해제하는 것은 그렇지 않다는 것입니다. 한 전략은 나에게 더 많은 노력을 요구하며, 그것은 제 친구에게는 더 많은 노력을 요하는 전략이 아닙니다.\n\n저는 저번 주말, 아무런 준비 없이 내게 플리스 넥 워머를 직접 만들기로 갑자기 결심한 것을 기억했습니다.\n\n## 혼돈 가운데 창작하기\n\n저는 머리가 짧아 목이 차가워집니다. 겨울 초반에 이스티에서 넥 워머를 둘러보았지만, 스스로를 위해 무언가를 만들고 돈을 덜 쓸 수 있다고 결심했지만, 결심한 대로 행동하지 않았습니다. 당신은 그냥 스카프를 착용하면 되지 않을까 생각할 수 있겠지만(제가 많이 가지고 있습니다), 긴 끝 부분이 늘어져서 짜증이 나고, 저는 더 꼭 맞는 것을 원했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n뉴 잉글랜드에서 토요일에 발생한 얼음 폭풍으로 나는 드디어 목도리 꿈을 이룰 수 있는 완벽한 조건을 얻었어: 목이 춥고 할 일이 없었거든. 지난 해 어머니가 선물해준 스크랩 후리스 가방을 뒤져보니 충분히 크다고 생각되는 조각들을 찾을 수 있었어. 머리와 목 주위에 걸쳐보고 확인했더니 충분한 크기였어.\n\n나의 어머니는 패치워크를 하는데 정말 능숙해. 그녀는 패치워크를 선호하고 사랑하는 일로 꼽는다. 그러나 그녀는 천으로 무엇이든 만들 수 있어. 그녀는 그에 맞는 기술과 인내심을 가지고 있어. 정확하고 사려 깊고 성실히, 약간의 철저함, 그녀의 모서리는 깔끔하고, 솔기는 모조리 균일해. 그녀는 나에게도 이런 방식으로 하라고 가르쳐주었어 (그러려했지만 그렇게 되지 않았어).\n\n난 자르고 재는 것을 싫어해, 그렇게 늘 그렇었어. 어머니는 준비 단계가 마지막 작품에 반드시 영향을 미치게 된다는 이야기를 했었어 - 아이템이 맞지 않을 수도 있고, 천이 뭉치거나 간극이 생길 수도 있고, 재료를 제대로 준비하지 않으면 다른 문제가 발생할 수 있다고. 하지만 나는 뒤엉킨 헤드폰 소유자야. 나에게는 목도리 프로젝트에 두 가지 노력 방식이 있었어: 지루하고 지루하고 짜증나게 하는 일, 측정을 하고 직선으로 잘라내고 깔끔하게 모든 것을 핀으로 고정하는 일; 또는 생동감 있고 놀랍고 순조로움으로 문제를 해결하는 일. 나는 후자를 선택했어.\n\n그리고 나는 근사하게도 자주 몸 어디에 \"혼돈\"이라는 단어의 타투를 하고 싶어했었다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-17-KnowYourselfSewYourself_1.png)\n\n## 내 목워머\n\n난 대략 한 절반 정도의 주아로 불쾌한 작업을 시작했어. 그래서 시작할 때 절단 매트를 꺼내는 노력을 기울였지. 내 어렸을 적에 엄마가 나에게 회전 절단 날을 무서워하게 했어. 그래서 손가락을 봉투에 담아 응급실로 가야 할까 봐 가능한 경우에는 피하려 해. 그러나 이 경우에는 그 위험을 감수해야 했어.\n\n이전에 선택한 피슬로 머리와 목을 감싸 둘 때, 예쁜 보라색 스크랩 피슬의 한 부분이 딱 맞게 긴 것을 깨달았어. 그래서 그 영역에서 길쭉한 직사각형을 잘라내려 노력했어. 길이를 위해 기존 끝점을 사용하고 균일한 모양을 만들기 위해 노력했지. 초기 직사각형의 높이에 대해 추측을 했어. 나중에 보라색 가장자리를 안감 플리스 주위로 감을 것이기 때문에 약간 짧게 나올 거라는 것을 알고 있었기 때문이지 (“약간”에 구체적인 측정치는 없어).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n작업을 마치고 나면, 안감 플리스에서 같은 길이의 조금 더 짧은 직사각형을 자르았어요. 이 작업 중에는 어떤 것의 폭이나 길이를 측정한 적이 없어서, 아직도 각 부분의 크기가 어떻게 되는지 잘 몰라요. 이 시점에서 나는 보라색 플리스 위에 도트무늬 플리스를 올려 위아래에 약간의 보라색 부분이 드러나도록 하고, 그런 다음 이 보라색 가장자리 폭을 접어서 고정했어요.\n\n이때, 어머니는 봤던 것을 먼저 입어보라고 했겠지만, 나는 그렇게 하지 않았어요. 혼돈스러운 수공예 정신으로, 나는 소잉 머신을 꺼내서 붐비는 식탁 위에 놓았어요. 정리되지 않은 종이들과 천 냅킨들을 한쪽으로 밀어내고 두툼한 피치도를 떼었어요. 제가 만든 직사각형의 길이를 따라 접힌 가장자리를 따라 솔직히 소었습니다. 나는 내 솔링 허용치에 특별히 신경쓰지 않았지만, 가장자리에서 멀리 떨어진 위치에 실을 유지하려고 했어요. 편하게 뽀 그나 목에 닿지 않게 마감부를 둘 수 있도록, 그리고 직선으로 유지하려고 노력했어요.\n\n다음으로, 직사각형의 끝을 겹쳐 튜브 형태로 만들고, 이를 바깥쪽을 안쪽으로 향하도록 함께 핀으로 고정했어요. 그런 다음 다시 솔잎습니다. Voilà! 그게 전부예요.\n\n![이미지](/assets/img/2024-05-17-KnowYourselfSewYourself_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n목워머를 착용해 보았을 때 부드럽고 따뜻하며 딱 맞는 직경이었지만, 원하는 것보다 조금 더 높았습니다. 당겨서 쓰면 코와 볼을 가릴 수 있어서 춥고 날씨가 추운 날에 나가기에 좋을 수도 있지만, 제가 상상했던 것과는 좀 달랐습니다. 위쪽을 몇 인치 자르고 가장자리를 마무리하는 것은 너무 어렵지 않을 것 같았지만, 성급하지 않기로 결정했습니다. 잠시 입고 다녀보고 느낌을 살펴볼 거에요.\n\n## 성찰\n\n많은 패스/불합격 과목에서 합격 성적은 대략 70% 달성하는 것과 동등하다고 합니다. 이 숫자는 내 머리 속에서 자주 나옵니다. 영국 육아 저자 사라 오크웰-스미스는 부모들이 \"시간당 70% 정도 정확하게 맞추려고 애쓰고 다른 30%에 대해 너무 걱정하지 말아야 한다\"고 말합니다. 이 말은 제 아이가 어렸을 때 저를 안심시켰습니다. 제가 평생 완벽주의자였는지 확신하지는 못하지만, 학교에서 완벽한 성적을 받는 것을 좋아했고 딱 적합한 성적으로는 실망스러워했을 것입니다. 요즘에는 모든 일(직업, 부모님 역할 등)에서 일상 목표로 70%의 기준을 갖고 있습니다.\n\n내 목워머는 쉽게 합격 성적을 받을만한 것 같아요. 만약 더 계획적이고 성실하며 집중력이 있다면 A+ 프로젝트가 되었을지도 모르겠어요. 그렇지만 아마 그렇게 마무리하지 못했을 거에요. 막 발로 진행해서 공예하는 과정에서 문제를 마주할 것으로 예상했지만, 그 정도는 아니었습니다. 비교적 쉽고 스트레스가 없었어요. 이 모든 것은 제 기대를 능가했다는 것을 의미합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n나는 어머니에게 지난 해 주머니에 대해 문자를 보냈고 내가 어떻게 만들었는지 얘기했을 때, 다음 번에 조언을 해줄 수도 있거나 적어도 나의 무법함에 잠시 웃음을 참지 않을까 싶었어요. 그러나 대신, 그녀는 내 노력에 감격했고, “바로 뛰어든 게 좋네. 시도해보기 전까지는 알 수 없으니까, 그리고 플리스는 너그러워.” 라고 써 보내주셨어요. 나는 그녀가 다르게 했을 것이라는 것을 알고 있어요 (궁극적으로 더 나은 결과를 얻을 거라는 것을 알면서도), 그리고 그녀도 내가 한 방식대로 해야 했다는 걸 알고 있어요.\n\n유니크한 창작 방법을 나누는 DIY 다이어리를 운영해 주시는\nAmanda Laughtland님에게\n항상 감사드립니다.\n","ogImage":{"url":"/assets/img/2024-05-17-KnowYourselfSewYourself_0.png"},"coverImage":"/assets/img/2024-05-17-KnowYourselfSewYourself_0.png","tag":["Tech"],"readingTime":6}],"page":"105","totalPageCount":110,"totalPageGroupCount":6,"lastPageGroup":10,"currentPageGroup":5},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"105"},"buildId":"-dPCbnM2yhdKNgXe92VJV","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>