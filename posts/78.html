<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/78" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/78" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/_next/static/chunks/463-0429087d4c0b0335.js" defer=""></script><script src="/_next/static/chunks/873-8ce515d2b46d0f43.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-cd321dee6458c228.js" defer=""></script><script src="/_next/static/wOkGEDZCvEs3S_XaNsdwr/_buildManifest.js" defer=""></script><script src="/_next/static/wOkGEDZCvEs3S_XaNsdwr/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="한 사람이 인터넷의 보안을 구했다" href="/post/2024-05-20-OnePersonSavedTheSecurityoftheInternet"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="한 사람이 인터넷의 보안을 구했다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="한 사람이 인터넷의 보안을 구했다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">한 사람이 인터넷의 보안을 구했다</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="10 분 만에 Google Dork를 사용하여 NASA를 해킹한 방법" href="/post/2024-05-20-HowIHackedNASAUsingGoogleDorkinJust10Minutes"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="10 분 만에 Google Dork를 사용하여 NASA를 해킹한 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-HowIHackedNASAUsingGoogleDorkinJust10Minutes_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="10 분 만에 Google Dork를 사용하여 NASA를 해킹한 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">10 분 만에 Google Dork를 사용하여 NASA를 해킹한 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="AI Boogeyman 공포를 들추고 현실을 받아들이다" href="/post/2024-05-20-AIBoogeymanexposingthefearsembracingthereality"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="AI Boogeyman 공포를 들추고 현실을 받아들이다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-AIBoogeymanexposingthefearsembracingthereality_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="AI Boogeyman 공포를 들추고 현실을 받아들이다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">AI Boogeyman 공포를 들추고 현실을 받아들이다</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="GNSS 안내 Cipr 스펙" href="/post/2024-05-20-GNSSGuideCiprSpecie"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="GNSS 안내 Cipr 스펙" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-GNSSGuideCiprSpecie_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="GNSS 안내 Cipr 스펙" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">GNSS 안내 Cipr 스펙</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="중간여행에서 화제를 모으는 아트워크 만드는 방법 - 6가지 예시" href="/post/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="중간여행에서 화제를 모으는 아트워크 만드는 방법 - 6가지 예시" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="중간여행에서 화제를 모으는 아트워크 만드는 방법 - 6가지 예시" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">중간여행에서 화제를 모으는 아트워크 만드는 방법 - 6가지 예시</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="놀라운, 끊임없이 변화하는 상호작용하는 AI 예술 전시회 HyperCinema가 뉴질랜드에서 공개되었습니다" href="/post/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="놀라운, 끊임없이 변화하는 상호작용하는 AI 예술 전시회 HyperCinema가 뉴질랜드에서 공개되었습니다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="놀라운, 끊임없이 변화하는 상호작용하는 AI 예술 전시회 HyperCinema가 뉴질랜드에서 공개되었습니다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">놀라운, 끊임없이 변화하는 상호작용하는 AI 예술 전시회 HyperCinema가 뉴질랜드에서 공개되었습니다</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="스타일러 AI 얼굴 킷" href="/post/2024-05-20-StylarAIFaceKit"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스타일러 AI 얼굴 킷" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-StylarAIFaceKit_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스타일러 AI 얼굴 킷" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">스타일러 AI 얼굴 킷</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="고급 RAG 08 Self-RAG" href="/post/2024-05-20-AdvancedRAG08Self-RAG"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="고급 RAG 08 Self-RAG" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="고급 RAG 08 Self-RAG" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">고급 RAG 08 Self-RAG</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="OpenAI API를 통해 GPT-4o에 접속하기" href="/post/2024-05-20-AccessingGPT-4oviaOpenAIAPI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="OpenAI API를 통해 GPT-4o에 접속하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-AccessingGPT-4oviaOpenAIAPI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="OpenAI API를 통해 GPT-4o에 접속하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">OpenAI API를 통해 GPT-4o에 접속하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">11<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="BERT 코드와 함께하는 완벽 가이드" href="/post/2024-05-20-ACompleteGuidetoBERTwithCode"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="BERT 코드와 함께하는 완벽 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="BERT 코드와 함께하는 완벽 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">BERT 코드와 함께하는 완벽 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">54<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link posts_-active__YVJEi" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"한 사람이 인터넷의 보안을 구했다","description":"","date":"2024-05-20 21:23","slug":"2024-05-20-OnePersonSavedTheSecurityoftheInternet","content":"\n![Image](/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_0.png)\n\n태양 피해 공격은 위협 요소가 소프트웨어 공급망 공격으로 이동하고, 소프트웨어의 전달 과정에서 취약한 활동을 침투하는 것을 보여주었습니다. 이러한 위협 요소들은 종종 고급 공격 도구를 만들기 위해 시간, 자금 및 전문성을 갖고 있으며, 그들의 공격 방법은 소프트웨어 패치와 시스템 업그레이드를 통해 감지되지 않을 수 있습니다.\n\n그래서 누군가 (아니면, 더 정확히는 일부 국가/법 집행 기관)가 인터넷에 백도어를 설치하려고 했으며, 방송 매체는 거의 전혀 이야기에 언급하지 않았습니다. 놀랍게도, 사이버 보안 정보 기관 중 많은 기관들은 기본적으로 조언만 반복하는 것 외에는 조용했습니다. 루머를 퍼뜨리고 싶지 않지만... [거기로 가면 안 되겠죠, \"그들\"이 감시 중일 수 있으니까].\n\n## 안드레스 프로인트\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n사회공학의 이야기이자 APT(Advanced Persistent Threat)의 가장 좋은 예시 중 하나로, 뛰어난 기술력을 자랑하는 사례입니다. 그리고 마치 Scooby Doo에서 말했던 것처럼, \"그 성가신 개발자가 방해 안 한다면 우리는 성공했을 거야\" 그랬죠. 그 성가신 개발자는 Microsoft의 소프트웨어 엔지니어인 38세의 Andres Freund입니다. 그의 주요 업무는 PostgreSQL 소프트웨어를 개발하는 것입니다.\n\nAndres는 이를 통해 어떤 소프트웨어 도구가 왜 느린지에 대해 몇 가지 테스트를 진행했고, Linux 운영 체제에서 서드파티 라이브러리 내에 백도어를 발견했는데, 이는 SSH 서버 응용 프로그램(OpenSSH)과 관련이 있었습니다. 이 응용 프로그램은 클라우드 시스템에 로그인하거나 개인 GitHub 저장소에 인증하는 데 사용됩니다.\n\n## Jia Tan\n\n문제는 LZMA라는 일반적으로 사용되는 압축 방법과 관련이 있으며 XZ 프로젝트에서 제공됩니다. 성공적으로 공격자가 백도어를 설치하여 원격으로 제어할 수 있는 시스템에 침입할 수 있었습니다. 이제 취약점 등급이 10으로 지정된 CVE–2024–3094로 정의되었습니다. 악의적인 코드는 이제 \"Jia Tan\"이라는 위협 요소에게로 연결되었는데, 그의 GitHub 계정은 2021년부터 거슬러 올라가며, 2022년에 처음 XZ 프로젝트에 참여했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그것은 악의를 품은 천재가 인터넷의 핵심 부분에 침투하여 후문을 삽입하는 할리우드 시나리오 중 하나 같은 느낌이었죠. 그 후 그들은 누구도 감지되지 않고 모두의 비밀 통신을 청취할 수 있습니다. 하지만 이것은 공상 과학이 아니에요. 실제로 \"Jia Tan\"이라는 사람이 SSH에 후문을 만들었고 거의 미궁에 빠져들 뻔했습니다. 그래서 하트블리드 이후 가장 심각한 취약점인 XZ 후문을 조사해봅시다.\n\nXZ 취약점은 CVSS 값이 10으로 평가된 심각한 결함입니다. 이것은 가능한 최고 수준이며 관련 시스템에 즉시 패치되어야 합니다. 전반적으로 XZ 라이브러리의 관련 후문은 Jia Tan(별명 JiaT75)이 심어놓은 것으로 보입니다. 그들은 자신의 재능을 선보이며 XZ GitHub의 관리자 권한을 얻었습니다.\n\nJia Tan은 이 라이브러리에 후문을 삽입한 사람 또는 그룹입니다. 그들은 코드에 대한 사회 공학 공격에 참여했습니다. 이 공격은 수년 동안 진행된 것으로 보입니다. 많은 사람들이 이 이메일이 인공지능에 의해 생성된 것처럼 보인다고 관찰했고, 아마도 국가 주체의 활동을 의미하는 표시입니다. 목표는 Linux 서버이며 취약점은 마이크로소프트 직원 (PostgreSQL 개발자인 Andres Freund)에 의해 발견되었습니다. [여기]에서 확인하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![One Person Saved The Security of the Internet](/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_2.png)\n\n## 타임라인\n\n2005년부터 2008년까지, Lasse Collins와 다른 사람들은 .xz 파일 형식을 만들었으며, 이는 LZMA 압축을 사용합니다. 이는 많은 Linux 커널의 핵심 부분이 되었습니다. 2021년과 2022년에는 Jia가 xz-devel 메일링 리스트에 여러 패치를 게시합니다. “Jigar Kumar”라는 사용자가 메일링 리스트에 여러 게시물을 작성하여 Jia의 패치가 적용되지 않은 이유를 묻습니다. Lasse는 2022년 6월에 다음과 같이 응답했습니다:\n\nJia의 최초의 중요한 업데이트 커밋은 이 게시물 이후에 발생했으며, Jia가 저자로 정의되었습니다. 그럼에도 불구하고 Jigar는 여전히 느린 업데이트를 불평했습니다. Dennis Ens를 비롯한 다른 사람들도 Lasse에게 압박을 가하였습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 시점에서 Lasse는 Jia가 프로젝트에서 더 큰 역할을 맡아야 한다고 보고했으며, Jigar는 계속해서 Jia가 더 큰 역할을 맡아야 한다고 주장했습니다. 이후 Lasse는 Jia에게 저장소 관리자의 역할을 맡길 것을 허락했습니다. Jigar나 Dennis는 인터넷 상에서 찾을 수 없으며, Jia가 액세스를 얻도록 노력한 가짜 ID일 가능성이 높습니다. 그러나 2022년 9월에는 Jia가 5.4.0 릴리스를 개괄하고, 2022년 11월까지 Lasse가 README에 프로젝트 관리자로 Lasse Collin과 Jia Tan이 있다고 기술했습니다.\n\nGitHub 커밋에서 Jia의 첫 흔적은 2024년 1월에 나타났으며, 버전 5.4.1을 릴리스했습니다. 2024년 2월에는 백도어 코드를 테스트 파일에 숨겼으며, xz-5.6.0.tar.gz 배포로 버전 5.6.0을 배포했습니다. 백도어가 숨겨진 파일인 build-to-host.m4은 GitHub 저장소에 나타나지 않았습니다. 2024년 3월까지 Jia는 새로운 백도어를 버전 5.6.1에 만들어 배포했습니다. 2024년 3월 28일, Andres Freund가 백도어를 발견하고, RedHat은 CVE-2024–3094를 할당했습니다.\n\n2024년 2월과 3월 사이, Jia는 5.6.0과 5.6.1 두 버전을 커밋했는데, 이는 백도어 코드를 포함하고 있었습니다. 그리고 이후에 Ubuntu, Red Hat, Debian의 개발자들에게 새 버전을 시스템에 통합하도록 요청했습니다. 통합한 사람들은 [여기]에 있습니다:\n\n![image](/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 완벽한 10점 평가\n\n우리의 디지털 세계의 대부분은 마이크로소프트 윈도우가 아니라 리눅스로 이루어져 있습니다. 리눅스는 대부분의 서버를 구동하고 많은 스마트 기기에 내장되어 있습니다. 이제 XZ utils의 새로운 배키도어가 최고 리스크 등급으로 평가되었습니다.\n\n![이미지](/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_4.png)\n\n배키도어는 xz의 tarballs(버전 5.6.0부터)에서 발견되었으며, 감지를 피하기 위해 은폐되어 있었습니다. 이는 liblzma 라이브러리 내의 코드를 수정하고 데이터를 가로채고 수정하는 데 사용됩니다. 하나 주목할 만한 발견은 OpenSSH 데몬이 해당 배키도어의 영향을 받았지만, 직접적으로 liblzma에 연결되어 있지 않았다는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nxz 코드는 XZ 파일을 생성하고 데이터 스트림과 블록을 통합하며, 읽을 때 빠른 무작위 액세스를 지원합니다. 이는 lzma 모듈 내에서 지원되지 않으며, 무작위 액세스 쿼리에서 이전 블록을 모두 읽어야 합니다. 여기서 예제가 있습니다:\n\n```js\n\u003e\u003e\u003e with xz.open('example.xz') as fin:\n...     fin.read(18)\n...     fin.stream_boundaries  # 2 스트림\n...     fin.block_boundaries   # 첫 번째 스트림에 4개의 블록, 두 번째 스트림에 2개의 블록\n...     fin.seek(1000)\n...     fin.read(31)\n...\nb'Hello, world! \\xf0\\x9f\\x91\\x8b'\n[0, 2000]\n[0, 500, 1000, 1500, 2000, 3000]\n1000\nb'\\xe2\\x9c\\xa8 Random access is fast! \\xf0\\x9f\\x9a\\x80'\n```\n\nxz와 관련된 많은 GitHub 저장소들이 오프라인이거나 비활성화되었습니다: [여기](https://github.com/).\n\n이제 table 태그를 Markdown 형식으로 바꿨습니다. 페이지를 다시 열어보시면 변경된 내용을 확인할 수 있습니다!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nThe seriousness of the vulnerability cannot be emphasized enough. Anthony Weems successfully reverse engineered the backdoor and developed a proof-of-concept exploit for Remote Code Execution (RCE) [here]:\n\n![Proof of Concept](/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_6.png)\n\nUnder certain conditions, an external attacker could bypass sshd authentication and gain full system access. This exploit involves replacing the RSA_public_decrypt function in OpenSSH with a third-party integration. Although OpenSSH does not typically use liblzma, the malicious code enables its integration.\n\n# 결론\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 GitHub를 운영하고 다른 사람들이 참여하도록 하려면, 비디오 링크를 통해 직접 상대방과 대화하고 그들의 신원을 확인하세요. APT에서는 자주 시간, 자원 및 전문지식을 가진 위협 요소들을 볼 수 있으며, 그들은 쉽게 어떤 시스템이든 침투할 수 있습니다. AI의 급부상으로, 이 모든 것이 몇 단계 높아지고 영상 및 오디오 사용이 공격의 일부로서 활용될 것을 볼 것입니다.\n","ogImage":{"url":"/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_0.png"},"coverImage":"/assets/img/2024-05-20-OnePersonSavedTheSecurityoftheInternet_0.png","tag":["Tech"],"readingTime":8},{"title":"10 분 만에 Google Dork를 사용하여 NASA를 해킹한 방법","description":"","date":"2024-05-20 21:22","slug":"2024-05-20-HowIHackedNASAUsingGoogleDorkinJust10Minutes","content":"\n안녕, 보안 애호가 여러분!👋\n\n난 고리쉬야, 버그 사냥 세계의 초보자야. 그리고 이제 뭘 알아? 최근 NASA 웹사이트에서 개인 식별 정보 노출 취약점을 발견했어. 이 글은 내 여정을 공유하고 여러분 중 일부에게 도움이 되기를 희망하는 내 작은 노력이야. 그래서, 준비되고 몇몇 웃음 (아마도 약간의 당황)을 준비해보고 나의 첫 중요한 발견까지 어떻게 했는지 알려줄게.\n\n겸손한 시작들✨\n\n자, 솔직하게 말하자면. 신참으로서 버그 사냥에 뛰어들 때는 기어가기도 전에 수영을 배우려는 것 같은 느낌일 수 있어. 그렇지만 당신이 프로처럼 느끼게 해주는 하나의 도구가 있어: Google Dorking. 미숙한 사람들을 위한, Google Dorking은 구글 검색을 스테로이드를 맞은 것과 같아 - 몇 가지 똑똑한 검색 쿼리로 민감한 정보의 보물을 발견할 수 있어.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n초기 탐사🚀\n\n이 분야에 새로운 입문자로서, NASA를 해킹하는 꿈이 조금 과대한 것 같다는 것을 알고 있었지만, 누가 크게 꿈을 꾸지 않겠어요? 버그 사냥은 보통 많은 인내와 튼튼한 기초 지식이 필요한데, 특히 NASA와 같이 거대한 기관을 대상으로 할 때는 더 그렇죠. 그래도 낮은 수준의 버그 사냥 팁 몇 가지를 갖고 미루어 본 결과, 직접 뛰어들어 보기로 결심했습니다. 똑똑한 선택이었나요? 아마 그렇지 않았을 겁니다. 즐거웠나요? 절대로요.\n\nGoogle Dorking을 이용해 NASA 정찰 임무를 시작했습니다. Google Dorking이란 해킹의 저격수 소총과 같은 것으로 생각하면 되요 - 다재다능하고 놀랍도록 강력한 도구입니다.\n\n여기에 제가 사용한 초기 도크가 있어요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nintitle:\"index of /\" site:nasa.gov\n\n이 쿼리는 NASA의 도메인에서 디렉토리 목록을 찾는 데 사용됩니다. 놀랍게도, 100개 이상의 디렉토리 목록을 발견했습니다. 탐험할 잠재적 취약성이 많이 남아 있네요!\n\n검색 범위 축소하기🧐\n\n너무 많은 디렉토리가 있어서 소음을 걸러내는 방법이 필요했어요. 특정 키워드에 집중하기로 결정했습니다. 먼저 데이터베이스를 찾아보는 것으로 시작해보았습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지난 NASA의 숨겨진 데이터베이스를 발견하는 꿈은 실패했습니다 - 아무것도 나오지 않았죠. 다음으로, 어드민 페이지를 찾아보려고 시도해 보았습니다.\n\nintitle:\"index of /\" \"admin\" site:nasa.gov\n\n여전히 운이 없었어요. 이 시점에서 저는 우주가 저를 조소하고 있다고 생각하기 시작했습니다. 그런데 그때 갑자기 뇌피셜이 번졌어요: PII (개인 식별 정보). 방법을 바꿔서 연락처를 찾아보았습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nintitle:\"index of /\" \"contact\" site:nasa.gov\n\n## 대박!🤯\n\n그곳에 있었지요 - nasa의 서브도메인에 contacts.asc라는 파일이 있었습니다. 제 마음은 두근두근하였고, 파일을 클릭하여 내 시스템으로 다운로드 받았습니다. 그 안에는 황금같은 민감한 정보가 포함된 120명 이상의 화성 패스파인더 미션 관련 직원들의 이름, 이메일, 전화번호, 팩스번호, 주소 등이 있었습니다.\n\n\u003cimg src=\"/assets/img/2024-05-20-HowIHackedNASAUsingGoogleDorkinJust10Minutes_0.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n버그 신고\n\n나는 시간을 낭비하지 않고 문제를 Bugcrowd를 통해 신고했습니다. 4시간이라는 고통받는 대기 후, 응답을 받았어요: 문제가 재현되었고 P3 심각도로 처리되었습니다. 다음 날 새벽 1시쯤에 내가 기대했던 확인을 받았어요 - 내 문제가 검증되었고, NASA로부터 첫 번째 명예의 전당 언급을 획득했습니다.\n\n얻은 교훈\n\n여기 내 모험으로부터 얻은 몇 가지 교훈이 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n1. 끈기가 있으면 결실을 맺습니다: 처음 실패에 실망하지 마세요. 다양한 키워드와 방법으로 실험을 계속해보세요.\n\n2. 구글 도킹은 강력합니다: 초심자들에게 특히 효과적인 재정 방법이지만 과소평가되고 있습니다.\n\n3. 책임있게 보고하세요: 무언가를 발견하면 적절한 경로를 통해 보고하세요. 윤리적 해킹은 인터넷을 보다 안전한 곳으로 만들기 위한 것입니다.\n\n종합적으로\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n버그를 찾는 것은 나와 같은 초보자에게도 흥미로운 경험이 될 수 있어요. 만약 나가다가 NASA에서 중요한 버그를 우연히 발견할 수 있다면, 당신도 할 수 있어요. 그러니 나가서 탐험해보고, 버그들이 당신과 함께하기를 바라요!\n\n모두 행복한 해킹 되세요!\n\n의견이나 경험을 공유하셔도 좋아요. 함께 배우고 웃을 수 있기를 기대해요!\n\n제 LinkedIn과 연락할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n해당 글은 순수히 교육 목적으로 작성되었습니다. 항상 윤리적 가이드라인을 준수하고 취약점을 책임있게 보고하십시오.\n\n이 여정을 저만큼 즐겁게 보내셨길 바랍니다. 다음에 또 만나요. 호기심을 가지고 지속적으로 노력해 주세요!\n","ogImage":{"url":"/assets/img/2024-05-20-HowIHackedNASAUsingGoogleDorkinJust10Minutes_0.png"},"coverImage":"/assets/img/2024-05-20-HowIHackedNASAUsingGoogleDorkinJust10Minutes_0.png","tag":["Tech"],"readingTime":5},{"title":"AI Boogeyman 공포를 들추고 현실을 받아들이다","description":"","date":"2024-05-20 21:19","slug":"2024-05-20-AIBoogeymanexposingthefearsembracingthereality","content":"\n![AI Boogeyman](/assets/img/2024-05-20-AIBoogeymanexposingthefearsembracingthereality_0.png)\n\n작년 겨울, 나는 정보 디자인에 관한 세 번째 학년 대학 과목을 강의할 기회를 가졌어. 캐나다에서는 이러한 수업에 학생들의 다양한 조합을 찾을 수 있다는 게 꽤 보편적이야. 이 중엔 세 번 혹은 네 번 길 수 있는 프로그램(학사 및 학사 with Honours)을 마치는 마지막 해에 있는 학생들도 있고, 컴퓨터 과학, 수학, 심리학, 언어 등의 다양한 학부 출신 학생들도 있어. 특히 학제간 프로그램을 수료하고 있는 학생들이 늘어나는 추세인데, 이는 현재 학계에서 눈에 띄는 트렌드야.\n\n과정을 진행하면서 우리가 마침내 마지막 해 중반에 다다른 순간, 학생들 사이의 대화는 자연스럽게 졸업, 취업 기회, 그리고 점점 더 중요해지는 주제인 Chat GPT 생성적 AI와 그 확장 가능성에 대해 이야기하는 방향으로 향했어.\n\n이들 중 상당수는 디자인과 커뮤니케이션 분야에서의 커리어를 준비하고 있었는데, 기술 문서 작성과 사용자 경험(UX)과 같은 분야에 특히 관심이 많았어. 이러한 흥미진진한 토론 속에서 나는 밑바닥에 숨은 불안감에 민감해졌어. 그것은 'AI 보게이먼을 드러내는 것, 현실을 받아들이는 것'이라는 구절로 간결하게 표현될 수 있는 두려움이었어.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 깨달음은 해당 주제에 대한 전용 블로그 토론이 필요하다는 점을 강조했습니다.\n\n게다가, 최근 럭비 월드컵 중에 우리가 팔로우하는 매우 인기 있는 럭비 피드에서 이 감정이 되풀이되었습니다. 네! 우리는 큰 럭비 팬이에요 - 나의 두 아들도 토론토 대학에서 하던 파트너와 서레이, 영국 출신의 지금은 은퇴한 외과 의사 아버지처럼.\n\n홍보 웹사이트에 대한 코멘터리는 새로운 AI를 활용한 비디오 'Rugby Titans'에 대한 논란을 일으켰고, 그에 대한 의문을 품는 이들 또는 소셜 미디어 애호가들의 \"해터\"라 불리는 사람들도 늘었다. 일부 코멘터는 AI를 활용하는 것이 스포츠를 홍보하는데 해로운 방향으로 흘러간다고 제안했는데, 특히 생성된 인물들이 모두 하얀 피부인데 스포츠계의 다양성으로 유명한 종목인데도 그렇다는 점을 고려해보라고 했습니다. 한 코멘터는 더 다양한 언어를 사용하며 이를 만든 사람을 \"키보드와 함께 지하실에 있는 사람\"으로 언급했고, 이것이 전통적인 창의성이 부여되지 않았다고 말했습니다. 다른 코멘트는 이렇습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또 하나의 경우:\n\n그리고 다른 하나의 경우:\n\n저는 예측하는 바로는 인공지능이 사회의 집단 의식에 더 깊이 인준되고 그 영향이 더욱 두드러지게 나타나자 반대하는 의견의 홍수가 될 것임을 예견하여 이러한 맥락과 코멘트를 언급하고 있습니다. 디지털 플랫폼과 직접적인 대화에서 이뤄진 대화들은 하나로 묶인 공통된 우려를 강조합니다: 인간들을 보다 포괄할 수 있게 하는 AI에 관한 솔직하고 쉽게 접근할 수 있는 대화가 절박합니다. 이 공간을 지켜봐주세요!\n\n인공지능은 실제로 추상적인 기술 논쟁의 영역을 넘어 우리 일상속으로 그 존재를 느끼게 하고 있습니다. 기술로서, AI는 로봇이나 미래의 자동차의 영역을 훨씬 뛰어넘어 우리 집, 직장, 심지어 포켓에 든 장치들까지 침투하는 현실입니다. 그렇다면 그것이 일상인들에게 불길하게 그림자 속에 숨은 악랄한 힘으로 자초했던 이야기를 먹었던 평범한 사람들에게 미치는 영향은 무엇일까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 인공지능의 실체를 분석해보기\n\n인공지능에 대해 첫 시도를 해보겠습니다. 이 때 '밀과 겨울 강아지'를 구분해보죠. AI가 무엇인지와 무엇이 아닌지를 명확하게 구분합니다. 'AI'라는 용어 자체가 미디어 헤드라인과 기술적인 대화에 자주 언급되어 모든 것을 아우르는 말로 쓰이면서 종종 특정한 의미를 잃어버릴 때가 많습니다. AI 또는 인공지능은 핵심적으로 학습과 문제 해결과 같은 인지 기능을 모방하기 위해 프로그래밍된 기계를 가리킵니다.\n\n이 기본적인 이해는 다른 AI 용어 주변의 흐릿함을 걷어내고 흔히 퍼져있는 오해를 해소하는 데 도움이 됩니다. 예를 들어, AI는 세계 지배를 꾀하는 감각 있는 로봇을 의미하지 않습니다. 대신, 우리가 익숙해진 일상 기술에 구현되어 있습니다.\n\nNetflix나 Prime과 같은 스트리밍 서비스에서 다음에 뭘 보아야 할지 제안하는 추천 엔진부터, 우리의 스마트 홈을 관리하는 가상 어시스턴트에 이르기까지, AI는 우리의 일상 기술 상호작용에서 많은 편리함과 개인화된 경험을 주도하는 조용한 엔진입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 스마트 홈\n\n우리가 스마트 홈의 세계를 향해 시선을 돌릴 때, AI는 침입자보다는 더 많은 사람들에게는 보이지 않지만 없어서는 안 될 필수적인 집 동료로 떠오릅니다. 집 자동화에 통합되는 것은 미묘하면서도 깊은 영향을 미칩니다. 스마트 온도 조절기가 우리의 일상 루틴을 학습하여 최적의 편안함과 효율성을 위해 온도를 조절하거나, 애완 동물과 침입자를 구분하여 실제 위협을 우리에게 알려주는 보안 시스템 등을 생각해보십시오.\n\n그러나 편의성이 크면서도 개인 정보 보호에 대한 문제가 발생합니다. 솔직한 검토가 필요한 교환입니다.\n\nAI가 우리의 집을 성실히 관리하는 동시에 많은 양의 데이터를 수집합니다. 우리 집에서의 AI 역할에 대한 진실은 개인화된 자동화의 럭셔리를 위해 우리의 일부 개인 정보를 거래한다는 점입니다. 중요한 것은 정보 동의와 개인 정보를 존중하고 보호하는 견고한 보안 조치에 있습니다. AI 집 동료의 범위를 이해하고 설정하여 우리가 넘지 않는 한 우리를 봉사하며, 편리함과 기밀 보장을 유지하면서 스마트 홈을 안전하게 유지할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 직장에서의 AI\n\nAI의 직장 통합은 종종 자동화의 무몰이로 인한 인간 직업의 폐기를 걱정하는 사람들로부터 넓은 범위의 두려움으로 묘사됩니다. 그러나 이러한 관점은 완전하지 않을 뿐만 아니라 대부분 오해되었습니다. 우리가 변화하는 직업 환경을 탐색하는 동안 AI의 역할은 대량 대체가 아니라 변모라는 점을 인식하는 것이 중요합니다. 네, 일부 반복적인 작업이 자동화되고 있지만, 이는 결과적으로 AI가 수행할 수 없는 인간 근로자들이 더 복잡하고 창의적이며 의미 있는 역할에 참여할 수 있는 기회를 창출하고 있습니다.\n\nAI가 육체적인 일을 대신 하면서 직원들은 인간의 창의력과 감성이 교체할 수 없는 영역에 집중할 수 있게 됩니다. 이러한 변화는 직업 인력을 차감시키는 것이 아니라 인간의 손길과 전문성이 중요한 영역으로 전환되도록 유도하는 것입니다. 이 관점에서 AI는 경쟁자가 아니라 협력자로 볼 수 있습니다. 단순히 대체물로 작용하는 것이 아니라 인간 능력을 보완하고 향상시키는 디지털 동맹국으로, 사람에게 치환 가능성이 있는 것이 아니라요. 예로서, AI가 도와주는 의료 지침을 들고 의사가 환자와 더 많은 개인적인 시간을 보낼 수 있는 상상을 해보세요.\n\n# 주머니 속의 AI와 웨어러블 기기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리의 스마트폰과 웨어러블 기기는 지금 우리와 AI 기술 간의 가장 개인적인 접촉점이 되었어요. 하지만 이 관계는 널리 보급되어 있어서 종종 간과되곤 해요. 이러한 기기들에서 AI의 역할을 분석하면 우리의 일상 활동을 향상시키는 여러 기능이 나타나요: 우리 타자 습관을 학습하여 의사 소통을 간소화하는 예측 텍스트, 우리 명령에 대답할 준비가 된 가상 비서, 우리의 생체 신호를 주의 깊게 추적하고 더 건강한 생활을 추천해주는 건강 모니터링 애플리케이션 등이 있어요.\n\nAI를 적용한 우리의 기기는 우리에게 맞춤형 경험을 제공하며 우리의 선호도와 행동을 학습하여 우리에게 더 나은 서비스를 제공해요. 집으로 가는 가장 빠른 길을 제안하거나, 재생 목록에 추가할 최신 노래를 추천하거나, 몇 시간동안 활동하지 않은 후에 일어나 스트레칭을 하는 것을 상기시켜줍니다. 하지만 이러한 맞춤화는 방대한 양의 개인 데이터를 분석함으로써 가능해지며, 디지털 시대의 개인정보 보호와 보안에 관한 지속적인 대화와 직면하게 됩니다. 우리가 우리 주머니 속의 AI의 편리함을 받아들일 때, 이러한 기술들과 의식적으로 관여하여 그들이 어떻게 작동하는지 이해하고 우리에게 요구하는 것을 이해하는 것이 중요해요.\n\n# AI를 향한 더 큰 선순환—건강, 교육 등 넘어서\n\nAI가 더 큰 선순환에 기여할 수 있는 가능성은 건강 및 교육 분야와 같은 분야에서 가장 선명하게 볼 수 있어요. 여기서 혁신뿐만 아니라 웰빙과 사회 발전에 미치는 심오한 영향에 대해 이야기할 수 있죠. 예를 들어, 의료 분야에서 AI는 암과 같은 질병을 더 정확하게 그리고 초기 단계에서 감지할 수 있는 고급 진단 도구로 분야를 혁신하고 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n교육에서, 인공지능이 제작한 맞춤형 학습 경험은 학생들의 수준에 맞추어 제공되어 학습 속도에 맞게 적응하며 발전하는 요구 사항에 맞게 콘텐츠를 맞춤화하여 포괄적이고 효과적인 교육 환경을 조성합니다. 그러나 이러한 민감한 영역에서 인공지능을 적용하는 것은 상당한 제약과 윤리적 고려 사항이 수반됩니다. 우려는 AI의 효능뿐만 아니라 그 권장 사항 및 결정의 함의에 대한 것입니다.\n\n의료 분야에서는 AI에 의존해야 하지만 이것이 만능이 아니며 의료 전문가의 대체 불가능한 판단을 소중히 여기는 시스템에 통합되어야 합니다. 마찬가지로 교육 분야에서는 AI가 맞춤형 학습 경로를 제공할 수 있지만 비판적 사고와 감정 발달을 육성하는 데 필수적인 인간적 접촉을 대체해서는 안 됩니다. 윤리적으로, 이러한 분야에서 AI 사용은 개인 정보 보호, 동의 및 공정한 대우 권리를 존중하는 균형 잡힌 접근이 필요하며 AI는 인간의 전문 지식과 윤리적 판단을 대체하는 것이 아니라 향상을 위한 도구로 작용함을 보장해야 합니다.\n\n# AI와 창의성 — 새로운 예술적 팔레트?\n\n인공지능의 창조적 영역으로의 침투는 예술가의 팔레트가 기술에 의해 끝없이 확장되는 새 시대를 예고합니다. 인공지능은 더 이상 음악, 미술, 글쓰기 영역에서 잠잠한 관전자가 아니라 활발히 참여하는 주체가 되었습니다. 알고리즘은 심포니를 작곡하고 멋진 시각적 작품을 생성할 수 있고, 인간 감정과 공감대를 형성하는 서사를 창작할 수도 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 이 급속히 발전하는 능력 속에서는 인공지능 지원 창작과 원본성의 성역 사이의 경계에 대한 숙고가 필요합니다. 논쟁은 저작권과 창의성의 본질에 관한 질문을 중심으로 전개되고 있습니다: 기계가 창의적일 수 있을까요, 아니면 인간의 창의력을 단지 보완할 뿐인가요? 이 새로운 세상에서 예술적 과정이 재검토되고, 원작의 정의가 진화 중에 있음을 확인할 수 있습니다 — 최근 'Drake'와 'The Weeknd'의 목소리를 모방한 인기를 끌었던 인공지능 히트곡 \"Heart on My Sleeve\"의 저작권 문제만 언급해도 충분합니다. 이러한 사건을 포함하여 이와 유사한 다른 사례들은 우리에게 AI와의 권리 사이의 상호작용을 어떻게 이해해야 하는지 강요합니다. AI가 음악가에게 화음 진전을 제안하거나 작가에게 서술 가능성을 제공할 수 있다 한들, 인간 예술가는 이러한 제안을 의미와 맥락과 함께 부여함으로써 창의적 과정을 소유합니다. 이 상호협력은 디지턼 시대에 창의성을 다시 생각하도록 우리에게 도전을 던지며, 다음을 인식하게 됩니다:\n\n# 인공지능 편향 이해: 코드 속의 불완전함\n\n인공지능 편향 문제는 코드 내에서 발생할 수 있는 불완전함이라는 사실을 드러냅니다 — 즉, 우리 자신의 인종차별과 같은 인간의 편견을 반영하는 결함입니다. AI 시스템이 데이터로부터 학습함에 따라, 해당 데이터에 존재하는 편향이 그대로 전달될 수 있습니다. 인종, 성별, 경제적 지위 또는 기타 속성과 관련된 것일 수도 있습니다. 이는 편향된 AI가 이러한 편견을 확실하게 하고 기존의 불평등을 증폭시킬 수 있다는 중대한 영향을 가집니다, 취업 심사 과정부터 대출 승인까지 다양하게 이어집니다.\n\n개발자들은 이제 더 다양한 데이터셋을 사용하여 AI를 학습시키고, 편향을 식별하고 보정할 수 있는 알고리즘을 설계하고 있습니다. 게다가, AI의 윤리적 배치를 안내하기 위해 윤리학자와 사회학자가 포함된 다학제적 팀에 대한 강조가 높아지고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n투명성 또한 매우 중요합니다. AI 시스템이 결정에 도달한 방식과 그 이유를 이해하는 것은 그 속에 내재된 편견을 식별하는 데 도움이 될 수 있습니다. 이러한 노력은 국제 표준과 규제의 지속적인 발전과 결합하여 AI 시스템이 공정하게 작동하도록 보장하기 위한 것입니다. 목표는 더 똑똑한 AI를 구축하는 것뿐만 아니라, 우리가 준수해야 할 최고의 윤리 기준을 반영한 시스템을 형성함으로써 우리 삶의 기술에 대한 신뢰와 진실성을 육성하는 것입니다.\n\n# 미래 전망\n\n미래로의 시선을 향하면 우리는 AI가 일상생활의 일부에 그치지 않고 일상적인 존재의 일부로 긴장되는 지평선을 보게 됩니다. 주목해야 할 다가오는 AI 기술들은 교통을 재정의할 수 있는 자율 주행 차량부터 우리의 요구를 예측하고 우리의 일정을 거의 완벽하게 관리할 수 있는 AI 주도 개인 비서에 이르기까지 다양합니다.\n\n실시간으로 교통 흐름, 에너지 소비, 비상 서비스를 최적화하는 AI가 스마트 시티를 상상해보세요. 건강관리 분야에서 AI가 세계 건강 결과 데이터베이스를 기반으로 맞춤형 치료법을 구상하는 경우를 상상해보세요. 교육 분야에서 AI의 잠재력을 고려해보세요. AI는 학습을 맞춤화할 뿐만 아니라 각 학생의 독특한 강점을 식별하고 발전시키기도 합니다. 이러한 것들은 멀리서 꿈꾸는 상황이 아닌, 당신도 닿을 수 있는 범위 내의 발전이며, 흔한 것을 특별한 것으로 변화시키기 위해 준비가 마련되어 있는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n미래에 인공지능이 보편화될 준비를 하는 데에는 적응적인 마음가짐과 선제적인 조치가 필요합니다. 교육 시스템은 개인들이 인공지능과 협업할 수 있는 기술을 갖추도록 진화해야 하며, 인공지능이 어떻게 작동하며 어떻게 긍정적 결과로 이끌 수 있는지에 대해 이해할 수 있도록 지원해야 합니다.\n\n게다가, 인공지능의 혜택에 공평하게 접근할 수 있도록 집중된 노력이 필요하며, 새로운 디지털 격차의 발생을 예방해야 합니다. 이러한 미래를 받아들이는 것은 인공지능이 효율성을 향상시킬 뿐만 아니라 전례 없는 방식으로 인간 경험을 풍부하게 할 수 있는 잠재력이 있다는 것을 이해하는 것을 의미합니다.\n\n# 실용적인 팁: 인간성을 잃지 않고 인공지능 받아들이기\n\n자칭 '테크 휴머니스트'인 저는 다른 사람들의 불안과 많은 이들이 겪고 있는 공포에 매우 민감하며, 그것이 동기부여와 계획에 영향을 미친다는 점을 잘 알고 있습니다. 알고리즘에 의해 점점 더 조정되는 세상에서, 인공지능을 받아들일 때 우리의 인간성을 단호하게 지키는 것은 동시에 도전과 기회가 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAI를 활용하여 작업 자동화를 수행하면 배우기, 창의성을 즐기거나 단순히 휴식을 취하고 가족과 연결하는 데 투자할 수 있는 시간을 되찾을 수 있습니다. AI는 우리의 일정을 관리하고 건강 목표를 추적하는 데 도움을 주며, 새로운 언어를 배우는 데도 도움을 줌으로써 우리의 성장과 발전 능력을 높일 수 있습니다. 그러나 AI와 우리의 삶을 뒤섞으면서 개인 정보 보호와 주체성을 유지하는 것이 중요해집니다. 이를 보호하기 위한 전략에는 AI 시스템 작동 방식 및 수집하는 데이터의 성격에 대해 정기적으로 정보를 습득하고, 장치 및 애플리케이션의 개인 정보 보호 설정을 정기적으로 검토하고 기술 기업으로부터 개인 데이터 사용에 대한 투명성과 통제를 지원하고 요구하는 것이 포함됩니다.\n\nAI가 우리의 삶을 어디와 어떻게 도와야 하는지를 선택적으로 결정하는 것도 매우 중요합니다. 편리함이 개인 정보 보호의 비용으로 이어지지 않도록 보장하기 위해 경계를 설정하는 것이 필요합니다. AI의 사용을 의식적으로 오프라인 순간에 허용하는 사려 깊은 약속을 통해 본질을 유지하는 데 도움이 됩니다. 기술이 우리를 섬기도록 하는지, 그 반대인지를 확인하는 것입니다. AI 통합에 대해 정보를 제공하고 신중한 접근을 촉진함으로써 AI의 잠재력을 활용하여 인간의 본질을 희생하지 않고 우리의 능력을 향상시킬 수 있습니다.\n\n내 소개: 안녕하세요, 제 이름은 Kem-Laurin이고, Human Tech Futures의 공동 창업 팀 중 한 명입니다. 현재 워터루 대학교에서 박사 학위를 취득 중입니다. 제 연구는 시민들의 데이터가 수집되고 활용되는 현대 사례 연구(사법)를 통해 정체성 구축에 관한 것입니다. 데이터가 불법적으로 수집되었는지 의도적으로 공유되었는지에 따라 결과적인 알고리즘 구성은 정보를 통해 정량화된 사용자들에게 엄청난 권력을 행사합니다. 제 연구 목표는 (1) 비판적 이해를 개발하여 (2) 제 공학 설계 시스템에서 사용할 구체적 휴리스틱 원칙을 생성하는 데 도움이 되는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저는 주요 HCD 전략가 및 디자인 사고 코치로 활동하고 있습니다. 이전에는 블랙베리 Autodesk의 대규모 디자인 및 연구팀을 이끌었으며 독일의 시멘스에서도 일했습니다. 여가 시간에는 열대 분위기의 다층 정원을 만들면서 135파운드짜리 GSD가 지켜보고 있습니다. 오늘은 세계가 현재의 모습으로 변하기 전 25개 이상의 나라를 여행한 기회를 가졌다는 점이 축복이라고 느낍니다. 다양한 인간 경험을 쌓을 수 있었습니다. 정기적으로 세계를 더 나은 곳으로 만들고자 하는 비판적 사고 능력을 갖춘 HCD 연구자들을 위한 적극적인 멘토로 활동하며 학생들에게 기여하고 있습니다.\n\n![이미지](/assets/img/2024-05-20-AIBoogeymanexposingthefearsembracingthereality_2.png)\n","ogImage":{"url":"/assets/img/2024-05-20-AIBoogeymanexposingthefearsembracingthereality_0.png"},"coverImage":"/assets/img/2024-05-20-AIBoogeymanexposingthefearsembracingthereality_0.png","tag":["Tech"],"readingTime":12},{"title":"GNSS 안내 Cipr 스펙","description":"","date":"2024-05-20 21:18","slug":"2024-05-20-GNSSGuideCiprSpecie","content":"\nMGXS의 GNSS 수집에 대한 전체 가이드를 보려면 'The Guide to GNSS'를 참조하세요. 이 문서는 Cipr 종 및 관련 아종에 대한 개요입니다. Cipr GNSS는 과학 소설에 묘사된 안드로이드 존재와 가장 비슷하다고 관찰되었습니다.\n\n# 종 이름: Cipr\n\n- GNSS 수: 93 (아종 = 없음); 아종 포함 총 601마리\n- 설명: Cipr GNSS는 단색 강조가 있는 가느다란 금속 존재로, 흰색 눈을 위한 Chrom 정렬이 있습니다. 대칭적 또는 비대칭적일 수 있습니다.\n- 아종: Cipr AX, Cipr Bess, 그리고 Cipr Caos (아래 참조)\n\n# 아종: Cipr AX\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- Count: 156\n- Description: Cipr AX GNSS features sharp shapes with white milky glass and thin eyes of varying colors, accented by iron or gold and black or white parts. They can be either symmetrical or asymmetrical.\n\n## Subspecies: Cipr Bess\n\n- Count: 183\n- Description: Symmetrical and asymmetrical. Round shapes. White milky glass, thin eyes that can vary color/alignment. May contain iron or gold parts with color gradient accents, and black or white parts. They can be either symmetrical or asymmetrical.\n\n## Subspecies: Cipr Caos\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 카운트: 169\n- 설명: Cipr Caos GNSS는 두꺼운 원형의 대칭적 또는 비대칭적인 형태로, 흰 우유껌색 유리, 다양한 색상의 얇은 눈, 그라데이션 악센트가 있는 철 또는 금 소재, 검은색 또는 흰색 부분이 특징입니다.\n\nMGXS Studio의 새롭게 브랜드된 웹사이트 https://mgxs.co/를 방문해보세요. OpenSea에서 GNSS 아트 컬렉션을 탐험하고, MGXS Studio가 창조하고 있는 세계에 참여하기 위해 Discord의 활기찬 커뮤니티 토론에 참여해보세요.\n\nMad is the way.\n","ogImage":{"url":"/assets/img/2024-05-20-GNSSGuideCiprSpecie_0.png"},"coverImage":"/assets/img/2024-05-20-GNSSGuideCiprSpecie_0.png","tag":["Tech"],"readingTime":2},{"title":"중간여행에서 화제를 모으는 아트워크 만드는 방법 - 6가지 예시","description":"","date":"2024-05-20 21:18","slug":"2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples","content":"\n# 1. 리얼과 가짜를 섞어보세요\n\n![이미지](/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_0.png)\n\n# 2. 실제 세계 물건과 유명 지형을 다른 장소에 배치해보세요\n\n![이미지](/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 3. 사진 조작\n\n![image](/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_2.png)\n\n# 4. 터무니없음\n\n![image](/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 5. 기분이 좋지 않아\n\n![이미지](/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_4.png)\n\n# 6. 파스텔 톤 추가\n\n![이미지](/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_5.png)\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoMakeEyeCatchingArtworkinMidjourney6Examples_0.png","tag":["Tech"],"readingTime":2},{"title":"놀라운, 끊임없이 변화하는 상호작용하는 AI 예술 전시회 HyperCinema가 뉴질랜드에서 공개되었습니다","description":"","date":"2024-05-20 21:15","slug":"2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand","content":"\n저는 HyperCinema의 CTO 인 Gareth Hordyk와 이야기할 기회를 가졌어요. 그는 세계에서 첫 번째로 소개된 \"관객을 영화적 서사의 중심에 위치시키는 라이브 AI 체험\"이라고 소개되는 HyperCinema의 CTO입니다. 우리가 이야기를 시작하자마자, 전문가뿐만 아니라 열정적인 이야기꾼이자 AI에 무언가를 기여하고 있는 사람과 함께 있는 것을 깨달았어요.\n\nGareth는 이전에 Pinewood Studios와 Jim Henson의 Creature Shop에서 로봇 시스템을 만들었던 경험이 있어요. AI 기술을 완전히 받아들인 창의적인 사고와 대화를 나누는 것은 흥미로워요. (우리 둘 다 그냥 \"도구\"로 부르는 일반적인 용어에 만족하지 않고 \"협력자\"로 정의하는 것에 동의했어요.) 산업에서는 AI가 여전히 의심을 받고 있고 대중도 불확실해하고 있는 상황에서, Gareth가 AI 예술에 대한 첫 경험을 하는 방문객이 많다는 점을 언급하며, 좋은 첫 경험이 되도록 하고 싶다고 말했어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이미지 목록\n\n웃음. 기쁨. 그것이 성공을 측정하는 방법입니다. 가레스는 때때로 다른 층에서 작업 중일 때 관중들의 기쁨과 놀람의 구호를 듣습니다. 또한, AI 세대의 방향을 결정할 때 팀이 따르는 원칙이기도 합니다. 초기에 그들은 HyperCinema가 참가자들을 안내하는 AI 다중우주의 여정에서 자신들의 버전을 만나게 되는 곳에서 유머, 경이, 그리고 낙관주의의 길로 나아가야 한다는 것을 깨달았습니다.\n\n하지만 저는 스스로를 앞서가고 있습니다! (아마도 Hypercinema의 파편화된 시간들이 전염되었나 봐요). 다중우주? HyperCinema는 무엇인가요?\n\n## HyperCinema: 탄생 이야기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n뉴질랜드 오클랜드를 기반으로 한 HyperCinema는 극장의 거물인 Miles Gregory 박사와 Gladeye 팀이 만들어낸 작품입니다.\n\nMiles는 뉴질랜드와 호주에서 Pop-up Globe를 개척해 크게 화제가 되었습니다. 그 프로젝트는 1614년 실제 셰익스피어 극장의 환상적인 세계들을 아오테아로아(뉴질랜드의 현대 몽골어 이름인 아오테아로아는 국제 독자들을 위한 설명입니다)에 가져다 주었습니다.\n\n한편, Gladeye는 2023년 롤링 스톤에서 발표한 디지털 스토리텔링 프로젝트로 우수한 대화형 미디어에 노미네이트된 크리에이티브 디지털 에이전시입니다. Gladeye의 CEO인 Tarver Graham은 HyperCinema의 공동 창립자이자 공동 크리에이티브 디렉터입니다. Gareth Hordyk는 HyperCinema의 CTO이자 Gladeye의 오퍼레이션 매니저 및 제품 컨설턴트입니다. 이 창의적인 인재들이 함께 작업하며 극적이고 디지털적인 스토리텔링 분야에서의 강점을 살려 HyperCinema 관객들에게 다른 어느 극적 상호작용 경험과도 비슷하지 않은 극장적 경험을 선사할 것입니다.\n\n## AI, 예술, 그리고 멀티버스의 교차점\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하지만 HyperCinema는 무엇인가요? HyperCinema는 AI가 생성한 하이퍼 개인화된 내러티브를 통해 짧은 영화, 이미지, 텍스트 및 오디오를 통해 탐구할 수 있는 대화형 경험입니다.\n\n![이미지](/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_1.png)\n\n이는 선택-당신-모험의 부분과 Night Gallery의 친절한 에피소드의 일부입니다. 여기서 모든 초상화는 자신의 것입니다(다소 역사적인 분위기의 Twilight Zone에서 온 우주적인 1920년대 벨보이들의 것일 수도 있습니다). 이곳은 일어나지 않은 삶을 전시하는 박물관입니다.\n\n![이미지](/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## AI 예술로 초괴도 가속\n\n현재 하이퍼시네마의 시즌은 \"멀티버스에 들어가다\"입니다. Gareth은 다른 주제도 볼 것이라고 암시했습니다; 할로윈은 너무 빨리 지나갔지만, 크리스마스 버스가 곧열날 것일까요? Gareth는 AI 공간에서 당신은 흥분을 느끼며 창조해야 하며, 돌아보지 말고 먼저 내보내야 한다고 관찰했습니다 (그것이 세계 최초의 라이브 AI 경험을 성취한 방식입니다).\n\n이 접근 방식은 저도 강력히 동의하는 부분입니다; AI 예술의 변덕스러운 세계에서는 선발이 되어야 하고, 훌륭한 것을 만들어야 하며, 진보해야 합니다. 아마도 각 맞춤형 작품들은 데이터 보호를 위해 각 맞춤 세션 사이에 삭제되기 때문에, 과거에는 이 정도로 전체적인 아트 컬렉션을 삭제하고 재설정한 갤러리나 그와 같은 특별한 매력이 있는 곳이 있었을까요? 이것은 일시적이지만 일생일대입니다. 예전에는 명화를 주문받기 위해 부유하고 유명하거나 귀족이어야 했지만, 이제 자아반영의 전시회를 거쳐갈 수 있지만, 그 뒤로는 사라집니다.\n\n이러한 빠르게 생성된 AI 걸작 예술품은 당신이 주인공인 것을 보는 다른 이는 없습니다 (여러분의 \"사이드킥\"을 제외하고, 곧 초능력 슈퍼히어로 원형에 대해 이야기할 것입니다). 이 독점성이 자신 중심적인지 아니면 겸손한지는 결정할 수 없습니다 - 당신에 관한 일시적인 갤러리, \"명성은 금방 사라진다\"는 것을 상기시키는 것인가요. 최소한, 당신이 이 짧은 경험을 상품화하려고 결정한다면 기념품 가게가 있습니다! (사진을 찍을 수는 있지만, Eco가 의심 없이 이야기하듯이, 기념품은 초현실 경험의 극치입니다).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 왜 멀티버스인가요?\n\n더 넓은 의미에서 \"멀티버스\"의 주제는 인공지능이 빠르게 무한한 변형을 반복하는 능력을 축하합니다(\"비나다는 다양성 속의 무한한 조합\"이 떠오릅니다). 모든 AI 예술가가 아는 것처럼, 이미지를 계속 생성하는 것은 중독적입니다. 각각은 다른 가정을 엿볼 수 있게 해줍니다.하지만 종종 설득력 있는 '실제' 장면이다. 지금까지 전 세계에서 150 억 개 이상의 AI 이미지가 생성되었습니다. 일찍부터 사진작가들이 이 기록적인 순간에 이르기까지는 약 150년이 걸렸습니다, 대략 1975년경에.\n\n![이미지](/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_3.png)\n\nHyperCinema는 우리의 현재 모든 정체성, 현실에 관한 논의와 너자신을 중심으로 한 예술적 경험에 모두 포장되어 있습니다. 그러나 여기에... 너가 포함되어 있지 않나요? AI 셀카가 기술적으로 발달한 포스트모던 사회에서 초현실주의의 정점일까요, 사실과 허구 사이의 경계를 흐리게 하는 걸까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_4.png\" /\u003e\n\n저는 모르겠어요, 그러나 이건 한 번 해봐야 할 사진 기회에요 (불가능한 AI 초상화로 예술 갤러리에서 셀카를 찍으면 장 보드리야르의 머리를 놓겠지). 다중 우주 테마는 시대정신과 일치하지만, AI의 창의력과 다양한 변수에 대한 재미난 실험이기도 해요.\n\n## 초현실 변화\n\nHyperCinema는 세 가지 다른 단계에서 진행돼요. 첫 번째는 트립티한 몰입형 프로젝션 공간으로, 참가자들은 자신을 이세계적인 상황에서 4미터 높이에 투사된 모습을 볼 수 있어요. 두 번째는 맞춤형 AI 예술 갤러리이고, 세 번째는 참가자가 자신의 병렬적인 삶을 자세히 소개하는 짧은 전기영화를 감상하는 극장이에요. 주된 참가자는 이 만남 동안 계속해서 “영웅(Hero)”으로 칭해요; 그러나 사진을 찍는 것을 덜 좋아하는 친구인 “조수(Sidekick)”와 함께 이 경험을 공유할 수도 있어요. 다른 참가자들을 마주칠 수도 있지만, 그들 또한 자신만의 다중 우주 여행을 통과하는 지나가는 여행자들이에요. 하지만 다시 한 번, 내가 다시 앞서 나가버렸네요 (계속 변화하는 시간축을 탓해야겠어요!).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n평범한 세계를 벗어나는 데 물리적인 표식이 없는 영웅의 여정은 무엇일까요? HyperCinema에는 신비로운 큐브가 있습니다. 이것에 대해 간단하고 감동적인 느낌이 있습니다. AI와 Marvel은 Jarvis AI부터 Secret Invasion까지 계속해서 등장하고 있죠. HyperCinema가 최고의 영화 시리즈의 트로프를 의도적으로 따르고 있는 걸까요? \"멀티버스\", \"영웅\", \"조수\" 및 \"우주 큐브\"라고 하면요? 이 중에서 심지어 참가자가 우주 비행사로 변신한 후 갓 태어난 아기가 되는 멀티버스 시퀀스도 있습니다. 이는 마치 '엔드게임' 속 한 장면을 떠올리게 합니다.\n\n가레스는 아는 듯이 웃으면서 HyperCinema의 가능성 있는 미래 버전에서 라이선스를 획득한 캐릭터를 중심으로 주제를 구성할 수도 있다고 암시합니다. 제 정기 독자들이 알 것처럼, AI 중심의 맞춤형 엔터테인먼트가 Disney/Marvel과 같은 거대 기업이 AI의 도전에 직면하는 방향이 될 것이라고 작성해 왔습니다. HyperCinema는 그 일부를 맛 볼 수 있게 해줍니다.\n\n## 다시 돌아와서\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모험의 부름을 받은 영웅들은 큐브를 콘솔에 넣고 성격 질문을 응답합니다. 그들의 디지털 프로필 결과는 이 큐브에 저장된다고 하는데, 나는 그게 인공지능 초보자들을 위한 눈꺼풀질에 불과한 것 같다고 의심합니다. 그것은 그들에게 안심감을 주는 물리적인 \"토템\"을 제공하는 것입니다.\n\n나는 이 아이디어를 좋아합니다. 이는 촉감과 인공지능 사이를 흐린다는 점 뿐만 아니라, 그들의 고유한 식별자가 큐브로 다운로드되는 느낌, 그것이 경험 동안 영웅들이 설치물을 활성화하는 데 사용되는 것입니다. 이것은 훌륭한 손길이며, 비록 사기책일지라도 말입니다.\n\n질문서는 이야기를 이끌어가기 위해 사용되며, 당신이 좋아하는 채소와 가지고 싶은 초능력 같은 해밍웨이 질문부터 시작하여 인류에게 가장 큰 위협이 무엇이라고 생각하는 등 더 깊은 존재주의적 질문까지 이어집니다. \"딥 페이크\"라고 대답하는 것이 알고리즘을 자화상의 토속으로 빠지게 할 수 있는 흥미진진한 삽을 상상할 수밖에 없다는 거죠!\n\n성격 프로필 외에도 15장의 사진이 찍힙니다. 인공지능 전문가는 이것을 인공지능을 훈련하기 위한 소규모 데이터 세트에 필요한 대략적인 숫자로 인식할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n약 15분 후에, 개인화 과정이 완료됩니다. 히어로의 시각적 여정은 앞서 말한 미지의 미래(Dreamscapes of Untold Futures)에서 시작하며(크기가 커보이는 프로젝션), 이어서 이상한 즐거움의 살롱(Salon of Uncanny Delights)(갤러리)로 이어지고, 마지막으로 불가능한 역사의 영화관(Cinema of Impossible Histories)(시네마)로 이어집니다.\n\n# 당신의 갤러리\n\n![이미지](/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_5.png)\n\n미술 갤러리에서 당신의 큐브가 녹색으로 빛나며, 당신의 얼굴로 변하는 프레임된 작품의 집단을 변경할 수 있습니다. Gareth는 참가자들이 주변을 돌아다니면서 모든 초상화를 활성화하고자 하거나 - 가능한 많이 - 자신을 반영하고 개인 갤러리를 갖도록 하는 것을 설명합니다(여러 히어로들과 그들의 사이드킥들이 동시에 경험을 즐길 수 있음을 언급할 가치가 있습니다; 사용자의 개인적인 움직임을 통해 사용자 정의 요소가 공간을 통해 활성화됩니다).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한, 각 이미지에는 대체로 어떻게 그 지점에 이르렀는지에 대한 AI 텍스트 생성 설명이 포함되어 있습니다. Gareth는 이러한 안내판이 의외로 인기가 많다고 말했는데, 사람들이 멀티버스 이미지만큼이나 이 배경 이야기의 사진을 찍는다고 합니다.\n\n## 자신만의 \"빙 존 말코비치\" 순간\n\n![이미지](/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_6.png)\n\n다음으로, 영웅은 불가사의한 역사의 영화관에 들어갑니다. 여기서 그들은 멀티버스 콜라주가 특징인 꿈틀거리는 짧은 영화를 보게 됩니다. AI 보이스 오버로 이뤄진 AI 생성 텍스트에서 내려오는 설명으로 이야기됩니다. 이 시퀀스는 거의 예고편의 연속이며, 대체된 삶에 대한 매혹적인 엿보기를 제공합니다. 아마도 가족 식사하는 가족 구성원 모두가 되어 있는 상황 같은 것도 포함할 수 있습니다 (백 투 더 퓨처 파트 II의 장면처럼, 마이클 J. 폭스가 모든 역할을 맡은 경우):\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 당신은 생물학 영화의 어떤 모음을 얻을지 정확히 알 수 없습니다: 장면은 맞춤형이고 확률적이기 때문에 두 번 다를 수 없습니다. Gareth는 HyperCinema의 재생 가능성에 대해 이야기하며, 반복 방문자들이 여정을 다시 경험하기 위해 얼마나 자주 방문하는지에 대한 놀라운 점 중 하나였다고 말합니다. 이것은 신비로운 모순입니다. 왜냐하면 이것은 \"한 강에 두 번째 발을 딛을 수 없다\"는 고대 헤라클리테스의 개념에 공명합니다. 창조의 각 순간은 독특하며, 소프트웨어가 적응합니다. 이 작업은 지속적인 변화 중에 있습니다.\n\nGareth가 멀티미디어 구성 요소를 설명하는 동안, 나는 너무 많은 가능성을 보기 때문에 방해하기 싫어하며, HyperCinema를 탈출 방과 같은 서사가 풍부한 엔터테인먼트로 확장할 생각이 있느냐고 물었습니다. 아직은 아니라고 Gareth가 빛을 발하며 말하자, 나는 그들의 멀티버스에서 파급되는 새로운 방향을 일으켰는지 궁금해합니다.\n\n# 마술 뒤의 마술: HyperCinema가 작동하는 방식\n\n내 독자들이 HyperCinema를 구동하는 핵심 AI 기술과 맞춤형 시네마틱 경험을 만들 때 고려된 메소드(또는 직면한 도전)에 대해 더 알고 싶을 것이다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3차원 인격\n\n먼저, AI 애호가로서, 나는 이 육면체에 대해 뭔가를 발견했어. 상상력에 미치는 미신적인 힘이 그들에게 얼마나 큰 영향을 주는지를 사랑해 — 당신이 자신의 본성을 다운로드하고 물리적으로 소유할 수 있다는 이 아이디어, 마치 비트코인을 위한 하드웨어 고안 보안 지갑처럼 — 그러나 IT Crowd 에피소드에서 Moss가 인터넷이 검은 상자에 빨간 불빛이 들어있다고 여하면서 젠을 속이는 모습을 떠올려보곤 해.\n\n육면체가 정말 \"인격 프로필을 담고 있는가\" 묻자 Gareth는 참가자의 위치를 따라간다는 것을 확인했어. 하지만 여전히, 영웅들은 그것을 이상하고 초자연적인 힘으로 꽉 쥐고 있어. Gareth는 참가자들에게 처음으로 육면체를 건네주면 참가자들이 셀카를 찍기 시작한다고 언급했어. 이 모든 것은 쇼의 일부이고, 나는 공연적이고 연극적인 성분이 존재한다는 것을 떠올리게 돼.\n\n육면체는 세대에게 기능적인가? 아마 아냐. 경험에 필수불가결한가? 절대적으로 그렇지. 나처럼 종굥을 구매할 때, 그 문을 들어서면서 믿음의 중단에 동의해야만 해. 우리가 물어봐야 할 유일한 질문은 앨리스가 원덜랜드에서 스스로에게 묻는 질문과 동일하다 (그게 좋은 주제가 될 것 같아?): \"나는 세상에서 누구인가? 음, 그것이 큰 수수께끼인걸.\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_7.png\" /\u003e\n\n## 사진 (불)완전한 맞춤화\n\n전체 경험 뒤에 숨어있는 워크플로에 대해 말하자면, 프로세스의 특허 출원 파트가 있지만, OpenAI에 의해 제공된 파워로 구동됩니다. Gareth는 그 마법이 어떻게 작동하는지에 대해 이해하기 어렵다는 이유로 비밀스럽게 하고 있지만, 각 방문자를 위한 Stable Diffusion의 맞춤형 모델이 포함되어 빠르게 15개의 이미지에 대해 훈련됩니다. 엔드포인트로 이어지는 2000개 이상의 레시피를 책임지는 프롬프트 엔지니어팀이 있으며, 결과물이 표시되기 전에 일부 인간에 의한 결과의 선별도 수행합니다. 결국, 완전히 자율적인 정렬 방향으로 진화할 것으로 예상됩니다. 그러나 가끔 발생하는 장애물이나 환각은 특히 유머러스하거나 '멀티버스'로 설명될 수 있다면 그대로 남겨둡니다.\n\n예를 들어, Gareth는 어떻게 푸른 머리의 주부들이 하루 중 무작정 등장했는지 설명합니다. 모두가 1960년대 AI 마지 심슨 변신을 한 것입니다. 여기서 멀티버스 주제가 AI의 즉흥적 행동과 완벽하게 어울리는 지점 하나가 되어 AI 의사 결정 프로세스의 불투명성 및 비예측성의 매력적인 측면을 강조합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 그것을 Everything Everywhere All at Once와 비교해보며, 핫도그 우주가 인공지능 예술의 실수를 조명하는 데 얼마나 도움이 되었는지 알아보았어.\n\n내 독자들을 위해 더 얻어낸 귀한 정보가 하나 더 있는데(나는 스마우그의 보물 중 하나인 빌보처럼 느껴): 촬영된 15장의 사진은 지난 다섯 년간의 전체 가짜 소셜 미디어 갤러리를 씨앗으로 사용합니다. 암벽등방법, 바닷가 등; 보통 일상생활 — 그리고 이것은 Multiverse를 보기 전에 사용되는 AI 교육 말뭉치로 사용됩니다. 명백히 실제 소셜 미디어 계정은 개인 정보 보호 및 표준화 이유로 사용되지 않습니다. 실제로 나는 이 과정을 단순화하는 데 사용되는 일상적인 사진첩들을 보는 것에 대해 더 흥미를 느끼며, 우리 모두가 코로나19 동안 중요한 사건을 놓치면서 그것들에서 의미를 찾을 것으로 의심합니다.\n\n## Multiverse로부터의 예술적 조언\n\n나는 비상한 AI 예술가들을 위한 조언을 Gareth에게 물어보기도 했어. 새로 나온 이 매체의 한가지 중요성 중 하나인 빠르게 예술을 창작하고 작품을 알리는 것에 대한 조언과 함께 Gareth는 커뮤니티 자원과 튜토리얼을 사용하고 온라인에서 트렌드인 \"완벽한\" 작품과 비교함으로써 좌절하지 말아야 한다는 중요성을 강조했어.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n“창의적인 여정의 일부로서 오류를 받아들이세요,” 그는 조언합니다. 이는 창의성을 촉진하지만 또한 AI의 학습 패턴과 출력물에 신속하게 적응하는 필요성을 강조합니다. 그는 희망하는 예술가들이 불확실성과 종종 예측할 수 없는 AI의 본질에 안락감을 느끼는 것이 중요하다고 강조하기도 합니다.\n\n## 더 많은 세계, 더 많은 선택\n\n나에게 남은 마지막 인상은 낙관주의적인 한가지입니다; AI 예술에 새로운 스타일을 불어 넣었으며 (국제 독자들에게, 여행을 원한다면 오크랜드를 방문하는 가치가 있다), 세계로 펼칠 것이라고 확신합니다. 하이퍼시네마는 NZ에서만 시작할 수 있는 여정입니다. 제 마지막 질문은 가레스가 참여자들이 어떤 경험이나 메시지를 갖고 떠나길 희망하는지 묻는 것입니다. 예상 적으로 AI 기술을 예술적인 것으로 인식할 때 사람들이 더 편안해지도록 하는 데에 대해 어떤 언급이 있을 것으로 기대합니다.\n\n저는 동의하지 않을 수 없습니다. 이는 신기술로 인한 사회적 경험뿐만 아니라 AI의 경이로움에 관한 것이기도 합니다. 기술이 엄청난 속도로 발전하더라도, 인간의 손길, 뉴질랜드 특유의 창의성과 따뜻함이 진정으로 이러한 ‘진정한 가공적인’ 경험을 잊을 수 없게 만드는 것임을 상기시켜줍니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n호기심 많은 분들을 위해 상호작용하는 라이브 AI 아트를 찾거나 자신과 세상을 새롭게 바라보는 유니크한 시선을 찾고 있다면, HyperCinema는 이제 뉴질랜드 오크랜드 퀸스트리트 131번지에서 12월 23일까지 연장 기간 동안 열려 있습니다.\n\n# Jim the AI Whisperer는 누구인가요?\n\nJim the AI Whisperer는 오리지널하고 매력적인 콘텐츠 작성 방법과 AI 생성기를 사용하여 멋진 시각물을 만드는 방법에 대한 개인 코칭을 제공합니다. 더 알고 싶다면 언제든지 저에게 연락하십시오.\n\n또한 저는 팟캐스트, 인터뷰, AI 프롬프트 세밀 조정, 기업을 위한 프롬프트 라이브러리 및 전문 AI 아트 작업도 가능합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Jim the AI Whisperer이 추천하는 관련 기사를 즐겨보실 수 있을 거에요:\n","ogImage":{"url":"/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_0.png"},"coverImage":"/assets/img/2024-05-20-Mind-blowingever-changingInteractiveAIartexhibitionHyperCinemagoesliveinNewZealand_0.png","tag":["Tech"],"readingTime":15},{"title":"스타일러 AI 얼굴 킷","description":"","date":"2024-05-20 21:13","slug":"2024-05-20-StylarAIFaceKit","content":"\nAI는 인간 얼굴을 만드는 데 점점 능숙해지고 있어요. 하지만 복잡한 상황에서는 가끔 실수가 발생할 수 있어요. 대부분의 어플리케이션들에서 그런 경우에는 프롬프트를 다시 실행하는 것뿐이에요. 그런데 그렇게 하면 전혀 예상치 못한 이미지가 생성되기도 해요.\n\n그래서 Stylar AI가 전체적인 얼굴 키트를 만들어 줬다는 것이 좋은 소식이에요 — 세 가지 다른 도구들이 있어요. 이 도구들을 사용하면 문제를 해결하고 얼굴을 다룰 수 있답니다.\n\n실제 사진이나 Stylar에서 생성된 이미지, 다른 AI 어플리케이션에서 가져온 이미지를 사용해서 문제를 해결할 수 있는 Stylar의 얼굴 키트를 활용할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n**킷에는 세 가지 도구가 포함되어 있습니다: Face Repair(얼굴 수리), Face Swap(얼굴 교체) 및 Face Match(얼굴 일치).**\n\n# Face Repair Tool(얼굴 수리 도구)\n\n![이미지](/assets/img/2024-05-20-StylarAIFaceKit_0.png)\n\n**이름이 말해주는 대로, 이 도구는 얼굴의 렌더링 아티팩트와 문제를 수리합니다. 이 도구에는 자체적인 멋진 창이 있으며, 여는 순간, Stylar AI가 얼굴 영역을 선택하여 수정할 수 있도록 선택 브러시를 활성화합니다.**\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 기능에는 Styal에게 이 얼굴을 어떻게 만들고 싶은지 말할 수 있는 별도의 프롬프트 창과 \"원본 얼굴 보존\" 슬라이더가 있습니다.\n\n그 사용법은 명백하고 쉽습니다.\n\n저는 스팀펑크 소녀를 렌더링해보기로 결심했습니다. 원본 이미지에는 좋은 아이디어가 있었지만 품질은 다소 낮았으며, 얼굴은 선글라스에 가려져 있었고, 어떤 이유로 안경 아래에 안경이 있었습니다.\n\n어떤 얼굴을 표시하는 데 한순간이 걸린 후에 약간의 처리 시간이 지난 후에 정말 좋은 결과물을 제시받았습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-20-StylarAIFaceKit_1.png)\n\n이 도구는 사용하기 쉽습니다. 그 말대로 작동하며 굉장히 편리합니다. 이미지 렌더링 및 혼합 아티팩트가 있는 이미지들에 자주 사용하며 사랑합니다.\n\n\"원본 얼굴 보존\" 슬라이더도 유용합니다. 특정 얼굴 특징들을 보존하는 것이 중요할 때는 슬라이더를 높이면 되고, 그렇지 않은 경우에는 작은 값을 사용하면 더 나은, 더 아름다운 결과물을 얻을 수 있습니다.\n\n# 얼굴 스왑 도구\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Stylar AI Face Swap Tool](/assets/img/2024-05-20-StylarAIFaceKit_2.png)\n\n얼굴 스왑 도구를 클릭하면 완전히 미니멀한 창이 표시됩니다. 여기에서 새 얼굴을 업로드할 수 있어요. 그것이 전부에요; Stylar AI가 나머지를 처리할 거예요.\n\n얼굴이 이미지 전체를 지배하는 경우, 환상적인 결과를 기대하지 마세요.\n\n하지만 이미지에서 얼굴이 이미지 크기의 약 30-40% 이하인 경우에는 잘 작동할 거예요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n사실, 대부분의 테스트에서 얼굴 교체 도구가 광고대로 작동하여 인상적인 결과물을 제공했습니다.\n\n그리고 컨트롤이나 옵션이 없기 때문에 배울 것이 없어요 — 그냥 사용하면 됩니다.\n\n아래는 얼굴 교체의 예시입니다. 나는 이 이미지를 물감이 튀는 것과 함께 만들었고, 내가 좋아했지만 원본 이미지의 얼굴이 너무 일러스트와 같았어요. 얼굴 교체 후 결과물은 정말 좋아 보입니다.\n\n![Face Swap Example](/assets/img/2024-05-20-StylarAIFaceKit_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이렇게 또 다른 예시를 보여드릴게요.\n\n![image](/assets/img/2024-05-20-StylarAIFaceKit_4.png)\n\n그런데 이 도구가 얼굴을 교환하는 데 탁월하게 처리했지만, 이 예시는 이 도구의 조금 특이한 점을 보여줍니다: 때때로 머리 크기를 변경하는 경향이 있어요.\n\n# 얼굴 매치 도구\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![StylarAIFaceKit](/assets/img/2024-05-20-StylarAIFaceKit_5.png)\n\n이 도구는 이미지 간 변환 도구 안에 토글로 위치하고 있습니다. 다른 스타일을 \"영감\" 이미지에 적용할 때 원하는 경우 원본 얼굴을 유지하는 데 목적이 있습니다.\n\n이 도구의 효과에 대해 굉장히 놀랐고 감명을 받았습니다. 활성화되어 있으면 인식 가능한 얼굴을 잘 보존합니다.\n\n이미지 간 변환 도구 안에는 스타일 영향력과 원본 이미지 구조의 강도를 조절하는 두 개의 슬라이더도 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그것은 상당히 좋은 제어를 제공하며 새로운 스타일과 원본 이미지 사이의 균형을 찾을 수 있는 가능성을 제공합니다.\n\n물론, 스타일에 따라 결과가 다를 수 있습니다. 과장된 만화 스타일이나 제작한 독특한 스타일과 같은 극단적인 스타일은 얼굴을 덜 알아볼 수 있게 만들지만, 그것은 예상된 바입니다.\n\n그럼에도 불구하고 대부분의 경우에는 아래 이미지들이 보여주는 대로 아주 잘 작동합니다.\n\n![StylarAIFaceKit_6](/assets/img/2024-05-20-StylarAIFaceKit_6.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 내 얼굴을 어딘가에 테스트하지 않았다면 테스트는 완전하지 않았을 것입니다. 그래서 Stylar의 인공지능 스타일 중 하나를 사용하여 바이킹 이미지를 만들고 나의 얼굴로 바이킹 얼굴을 바꿨어요.\n\n여기 두 이미지가 있어요; 잘 생긴 사람은 나에요.\n\n![내 얼굴 이미지](/assets/img/2024-05-20-StylarAIFaceKit_7.png)\n\nStylar AI에는 얼굴을 다루는 데 도움이 되는 Enhance 도구가 하나 더 있어요. 이 도구는 특별히 얼굴을 다루기 위해 설계된 것은 아니지만 이미지를 미세하게 조정하는 데(얼굴 포함) 도움이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기 있어요. Stylar AI에는 이미지에서 얼굴을 수정, 교체 및 향상하는 멋진, 유용하고 실용적인 도구 세트가 있습니다.\n\n이 도구들은 블로그 포스트 및 마케팅 캠페인에서 브랜드를 대표할 특정 얼굴을 원할 때 유용할 수 있습니다.\n\n이 도구들은 이미지를 복원하고 수정하는 데 도움이 됩니다.\n\n그리고 실제로, 당신은 어디든지 거의 모든 스타일로 자신의 얼굴을 넣을 수 있고, 그것은 재미있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 Stylar로 실험을 하려면 예제 얼굴이나 재미있는 스타일 이미지가 필요하다면 무료로 얻을 수 있어요.\n\nAivaras Grauzinis\n","ogImage":{"url":"/assets/img/2024-05-20-StylarAIFaceKit_0.png"},"coverImage":"/assets/img/2024-05-20-StylarAIFaceKit_0.png","tag":["Tech"],"readingTime":7},{"title":"고급 RAG 08 Self-RAG","description":"","date":"2024-05-20 21:10","slug":"2024-05-20-AdvancedRAG08Self-RAG","content":"\n이 기사는 흔한 시나리오로 시작됩니다: 공개 시험을 보는 경우입니다. 일반적으로 두 가지 전략을 사용합니다:\n\n- 방법 1: 익숙한 주제에 대해서는 빠르게 답변하고, 익숙하지 않은 주제에 대해서는 참고서를 열어서 확인하고, 관련 부분을 빠르게 찾아내어 정리하고 요약한 다음, 시험지에 답변합니다.\n- 방법 2: 모든 주제에 대해 책을 참고합니다. 적절한 부분을 찾아내고, 정리하고 요약한 다음, 시험지에 답변합니다.\n\n분명히 방법 1이 선호되는 방법입니다. 방법 2는 시간이 소비될 수 있고, 관련성 없는 정보나 잘못된 정보가 들어올 수 있어 혼란과 실수를 야기할 수 있습니다. 심지어 처음에 이해한 부분에서도 발생할 수 있습니다.\n\n하지만, 방법 2는 고전적인 RAG 프로세스를 보여주며, 방법 1은 자체 RAG 프로세스를 대표합니다. 이에 대해 이 기사에서 더 자세히 다룰 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 개요\n\n그림 1은 RAG 및 Self-RAG의 주요 프로세스를 비교한 것을 보여줍니다:\n\n![그림](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png)\n\nSelf-RAG는 세 단계로 구성되어 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 필요한 경우 검색: 모델이 검색을 요구하는 경우, 예를 들어 \"미국 주가 이름을 어떻게 얻었습니까?\" (그림 1의 오른쪽 상단)와 같은 쿼리가 있을 때, 모델의 출력에는 [검색] 토큰이 포함됩니다. 이는 쿼리와 관련된 내용을 검색해야 함을 나타냅니다. 반면에 \"최고의 여름 휴가에 대해 에세이를 쓰세요\" (그림 1의 오른쪽 아래)와 같이 물어볼 때, 모델은 검색 없이 직접 답변을 생성하도록 선택합니다.\n- 병렬 생성: 모델은 프롬프트와 검색된 콘텐츠를 모두 사용하여 출력을 생성합니다. 이 과정에서 세 가지 유형의 반영 토큰이 검색된 콘텐츠의 관련성을 나타냅니다.\n- 평가 및 선택: 단계 2에서 생성된 콘텐츠가 평가되고, 최상의 세그먼트가 출력으로 선택됩니다.\n\n상기 모델은 특별히 훈련된 모델이라는 것을 유의하십시오. 이 모델의 훈련 과정은 이 기사의 후반부에서 논의될 것입니다.\n\n# 반영 토큰\n\nSelf-RAG 프레임워크의 RAG와 비교했을 때, Self-RAG 프레임워크의 차이는 생성 중 더 정확한 제어를 위해 반영 토큰을 사용한다는 것입니다. 그림 2에서 보여집니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_1.png\" /\u003e\n\n본질적으로, self-RAG는 네 가지 명확한 판단을 내립니다:\n\n- [Retrieve]: 리소스 R로부터 정보를 검색할지를 결정하는 의사결정 과정.\n- [IsREL]: 주어진 데이터 d가 문제 x를 해결하는 데 필요한 정보를 포함하고 있는지를 결정하는 관련성 확인.\n- [IsSUP]: 제공된 응답 y의 내용이 데이터 d로부터 지원되는지를 확인하는 검증 과정.\n- [IsUSE]: 문제 x에 대한 응답 y의 유용성을 평가하는 평가 과정. 결과는 1에서 5까지의 점수로, 5는 가장 높은 유용성을 나타냅니다.\n\nRAG에서 검색은 상태에 관계없이 항상 처음에 수행되는 고정된 과정입니다. 반면 self-RAG는 반사 토큰을 도입하여 LLM을 더 적응적이고 지능적으로 만듭니다. LLM이 텍스트를 생성하다가 불확실성이 발생하는 부분에 도달하면 반사 토큰에서 일시 정지하여 신속하고 정확한 검색을 수행한 후 새로 습득한 정보를 사용하여 생성을 재개합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 코드 설명\n\nself-RAG 프로세스를 직관적으로 이해하기 위해 먼저 코드를 살펴보고 모델의 훈련 과정을 설명하겠습니다.\n\nself-RAG는 오픈 소스이며, Langchain과 LlamaIndex에는 각각의 구현이 있습니다. 우리는 설명을 위해 LlamaIndex의 구현을 참조할 것입니다.\n\n## 환경 설정\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n먼저, 환경을 설정하세요.\n\n```js\n(base) Florian@instance-1:~$ conda create -n llamaindex python=3.11\n\n(base) Florian@instance-1:~$ conda activate llamaindex\n\n\n(llamaindex) Florian@instance-1:~$ pip install llama-index\n\n(llamaindex) Florian@instance-1:~$ pip install huggingface-hub\n\n(llamaindex) Florian@instance-1:~$ huggingface-cli login\n```\n\n설치 후, LlamaIndex의 대응 버전은 다음과 같습니다:\n\n```js\nllama-index                             0.10.20\n\nllama-index-core                        0.10.20.post2\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"여러분의 오픈AI API 키\"\n\nfrom llama_index.core import Document, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom pathlib import Path\n\n\n# 옵션: SelfRAGPack 다운로드\n# 첫 실행 시 SelfRAGPack을 다운로드해야 합니다.\n# 다음 실행부터는 이 부분을 주석 처리할 수 있습니다.\nfrom llama_index.core.llama_pack import download_llama_pack\ndownload_llama_pack(\n    \"SelfRAGPack\",\n    \"./self_rag_pack\")\n\nfrom llama_index.packs.self_rag import SelfRAGQueryEngine\n\n# 이전에 다운로드하고 저장한 Llama2 모델이 있는 디렉토리.\ndownload_dir = \"여러분의 다운로드 모델 디렉토리\"\n\n# 테스트 문서 생성\ndocuments = [\n    Document(\n        text=\"남극 얼음 위를 '웨들'이라고 불리는 물개 떼가 지나다녔다. 그들의 턱시도 같은 깃털은 눈 위에서 돋보였다.\"\n    ),\n    Document(\n        text=\"펭귄 중 가장 키가 큰 황제펭귄은 다른 어떤 새보다도 더 깊이 다이빙을 할 수 있어서 500m 이상의 심해까지 다이빙을 합니다.\"\n    ),\n    Document(\n        text=\"펭귄들의 흑백색깔은 위험 방어라는 화장법의 한 종류인 카운터셰이딩입니다. 위에서 보면 펭귄의 검은 등은 바다 심지와 어우러지고, 아래에서는 펭귄의 흰 배는 밝은 표면과 어우러집니다.\"\n    ),\n    Document(\n        text=\"수직 자세이지만, 펭귄은 날지 못하는 조류입니다. 그들의 날개는 지느러미로 진화했기 때문에 수중에서 전문 수영가입니다.\"\n    ),\n    Document(\n        text=\"가장 빠른 펭귄 종류인 젠투 펭귄은 시속 36킬로미터까지 수영할 수 있으며, 수중을 순찰하는 동안 지느러미와 윤곽을 이용해 물을 가르는 식으로 전진합니다.\"\n    ),\n    Document(\n        text=\"펭귄은 집단생활을 하는 조류입니다. 많은 종들이 번식을 위해 수만 마리까지 이를 결성합니다.\"\n    ),\n    Document(\n        text=\"펭귄은 놀랍게도 귀가 우수하며 지저분한 떼 속에서 배우량과 새끼를 식별하는 데 명확한 호출을 의존합니다.\"\n    ),\n    Document(\n        text=\"가장 작은 펭귄 종인 리틀 블루 펭귄은 약 40cm 높이로, 남부 호주와 뉴질랜드 해안가에서 발견됩니다.\"\n    ),\n    Document(\n        text=\"번식 기간 중, 수컷 황제펭귄은 한없이 지속되는 남극 겨울을 버텨내며 몇 달간 급식없이 알을 부화시키는 반면, 암컷은 바다에서 사냥을 합니다.\"\n    ),\n    Document(\n        text=\"펭귄은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 그리고 크릴로 이루어져 있으며 이를 수중 다이빙을 통해 잡습니다.\"\n    ),\n]\n\nindex = VectorStoreIndex.from_documents(documents)\n\n# 간단한 리트리버 설정\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=10,\n)\n\n\nmodel_path = Path(download_dir) / \"selfrag_llama2_7b.q4_k_m.gguf\"\nquery_engine = SelfRAGQueryEngine(str(model_path), retriever, verbose=True)\n\n# 리트리벌 예시\nresponse = query_engine.query(\"어떤 장르인가요?\")\n\n# 리트리벌 예시\nresponse = query_engine.query(\"가장 작은 펭귄의 키는 얼마인가요?\")\n```\n\n위의 테스트 코드는 다음 결과를 생성했습니다(대부분의 llama_cpp 디버깅 정보가 제거되었습니다):\n\n```js\nModel metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: None\n\nllama_print_timings:        load time =    4887.53 ms\nllama_print_timings:      sample time =      11.29 ms /    22 runs   (    0.51 ms per token,  1947.76 tokens per second)\nllama_print_timings: prompt eval time =    4887.46 ms /    24 tokens (  203.64 ms per token,     4.91 tokens per second)\nllama_print_timings:        eval time =    5883.27 ms /    21 runs   (  280.16 ms per token,     3.57 tokens per second)\nllama_print_timings:       total time =   10901.84 ms /    45 tokens\n최종 답변: '오만과 편견'은 제인 오스틴의 로맨스 소설입니다.\nllama_print_timings:        load time =    4887.53 ms\nllama_print_timings:      sample time =      11.74 ms /    20 runs   (    0.59 ms per token,  1703.29 tokens per second)\nllama_print_timings: prompt eval time =    7473.66 ms /    37 tokens (  201.99 ms per token,     4.95 tokens per second)\nllama_print_timings:        eval time =    5414.34 ms /    19 runs   (  284.96 ms per token,     3.51 tokens per second)\nllama_print_timings:       total time =   13076.88 ms /    56 tokens\n입력: ### 지시사항:\n가장 작은 펭귄은 얼마나 키가 큰가요?\n\n### 응답:\n[검색]\u003c문단\u003e펭귄들은 다양한 해산물을 섭취합니다. 그들의 식단은 주로 생선, 오징어, 크릴로 구성되어 있으며 이를 다이빙으로 잡습니다.\"\u003c/문단\u003e\n예측: [관련]가장 작은 펭귄 종류의 키는 종에 따라 달라질 수 있습니다.[지원되지 않음 / 모순][유틸리티:5]\n점수: 1.4213598342974367\n10/10 단락 완료\n\n평가 종료\n최상의 답변 선정: [관련]가\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n테스트 코드를 이해하는 핵심은 SelfRAGQueryEngine 클래스의 구현에 있습니다. 이제 이 클래스를 자세히 살펴보겠습니다.\n\n## 클래스 SelfRAGQueryEngine\n\n먼저 생성자입니다. 주로 llama_cpp를 사용하여 Llama2-7B 모델을 로드하기 위해 사용됩니다.\n\n```python\nclass SelfRAGQueryEngine(CustomQueryEngine):\n    \"\"\"간단한 Self RAG 쿼리 엔진.\"\"\"\n\n    llm: Any = Field(default=None, description=\"llm\")\n    retriever: BaseRetriever = Field(default=None, description=\"retriever\")\n    generate_kwargs: Dict = Field(default=None, description=\"llm generation arguments\")\n    verbose: bool = Field(default=True, description=\"Verbose.\")\n\n    def __init__(\n        self,\n        model_path: str,\n        retriever: BaseRetriever,\n        verbose: bool = False,\n        model_kwargs: Dict = None,\n        generate_kwargs: Dict = None,\n        **kwargs: Any,\n    ) -\u003e None:\n        \"\"\"매개변수 초기화.\"\"\"\n        super().__init__(verbose=verbose, **kwargs)\n        model_kwargs = model_kwargs or _MODEL_KWARGS\n        self.generate_kwargs = generate_kwargs or _GENERATE_KWARGS\n        try:\n            from llama_cpp import Llama\n        except ImportError:\n            raise ImportError(_IMPORT_ERROR_MSG)\n        self.llm = Llama(model_path=model_path, verbose=verbose, **model_kwargs)\n        self.retriever = retriever\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그 다음으로 쿼리 기능에 대해 설명하겠습니다. 주요 프로세스는 아래 그림 3에 표시되어 있습니다:\n\n![Image](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_2.png)\n\n이해를 돕기 위해 주요 부분에는 주석이 달려 있습니다.\n\n```python\n    def custom_query(self, query_str: str) -\u003e Response:\n        \"\"\"커스텀 쿼리 실행.\"\"\"\n        # Llama2 모델을 사용하여 응답을 가져옵니다.\n        response = self.llm(prompt=_format_prompt(query_str), **_GENERATE_KWARGS)\n        answer = response[\"choices\"][0][\"text\"]\n        source_nodes = []\n\n        # 검색이 필요한지 여부를 결정합니다.\n        if \"[Retrieval]\" in answer:\n            if self.verbose:\n                print_text(\"검색이 필요합니다\\n\", color=\"blue\")\n            # 그림 1의 단계 1, 필요한대로 검색합니다.\n            documents = self.retriever.retrieve(query_str)\n            if self.verbose:\n                print_text(f\"받은 문서: {len(documents)}\\n\", color=\"blue\")\n            paragraphs = [\n                _format_prompt(query_str, document.node.text) for document in documents\n            ]\n\n            if self.verbose:\n                print_text(\"평가 시작\\n\", color=\"blue\")\n\n            # 그림 1의 단계 2 및 3, 병렬로 생성하고 평가합니다\n            # (코드에서 병렬화를 구현하지는 않음)\n            critic_output = self._run_critic(paragraphs)\n\n            paragraphs_final_score = critic_output.paragraphs_final_score\n            llm_response_per_paragraph = critic_output.llm_response_per_paragraph\n            source_nodes = critic_output.source_nodes\n\n            if self.verbose:\n                print_text(\"평가 종료\\n\", color=\"blue\")\n\n            # 가장 높은 점수를 받은 답변을 선택하고 반환합니다.\n            best_paragraph_id = max(\n                paragraphs_final_score, key=paragraphs_final_score.get\n            )\n            answer = llm_response_per_paragraph[best_paragraph_id]\n            if self.verbose:\n                print_text(f\"최적 답변 선택: {answer}\\n\", color=\"blue\")\n\n        answer = _postprocess_answer(answer)\n        if self.verbose:\n            print_text(f\"최종 답변: {answer}\\n\", color=\"green\")\n        return Response(response=str(answer), source_nodes=source_nodes)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 코드에서 우리는 그림 1의 모든 세 단계가 표현된 것을 확인할 수 있습니다. 그러나 LlamaIndex의 코드는 병렬 처리를 구현하지 않았습니다. 더 자세한 정보는 관심 있는 독자들이 self.\\_run_critic 함수를 살펴볼 수 있습니다. 해당 함수는 다양한 반사 토큰에 해당하는 점수를 처리합니다.\n\n# Llama2-7B 모델 훈련 방법\n\n이전에 여러 번 Llama2-7B 모델을 사용해왔으니, 이제 어떻게 얻을 지 알아봅시다.\n\n## 훈련 목표\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n훈련 과정에서는 평가 모델 C와 생성 모델 M 두 가지 모델이 필요합니다. 평가 모델 C는 모델 M이 필요로 하는 감독 데이터를 생성합니다.\n\n그러나 추론 과정에서는 모델 M만 사용되며 모델 C는 필요하지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 비평가 모델 C\n\n비평가 모델은 반사 토큰을 생성하는 데 훈련됩니다. 이 모델을 사용하는 목적은 작업 출력 오프라인에 반사 토큰을 삽입하여 훈련 말뭉치를 업데이트하는 것입니다.\n\n각 세그먼트의 반사 토큰을 수동으로 주석 달기는 비용이 많이 듭니다. Self-RAG는 GPT-4를 활용하여 각 반사 토큰에 대해 고유한 지침을 할당하여 서로 다른 정의, 입력 및 출력을 가지고 있기 때문에 효율적으로 데이터 주석 작업을 완료합니다. 예를 들어, [검색] 토큰의 지시는 GPT-4가 외부 문서를 통합하는 것이 결과를 향상시킬지를 평가하도록 요청합니다.\n\n훈련 데이터 D_critic를 얻으면 표준 조건부 언어 모델을 기반으로 훈련 목표를 구성할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_3.png)\n\n비평가 모델 C는 어떤 언어 모델로도 초기화할 수 있습니다. 예를 들어 생성자와 동일한 모델로 초기화할 수 있습니다. 예를 들면 Llama2-7B와 같은 모델을 사용할 수 있습니다.\n\n## 생성자 모델 M\n\nFigure 4는 훈련 데이터를 수집하는 구체적인 과정을 보여줍니다. 입력-출력 쌍 (x, y)가 주어지면 self-RAG는 검색 및 비평가 모델을 사용하여 원래의 출력 y를 확장하고 지도 데이터를 생성합니다. y의 각 세그먼트 yt에 대해:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_4.png\" /\u003e\n\nFigure 4의 모든 조건 판단은 비평가 모델 C를 통해 실행됩니다. 획득한 훈련 데이터는 Figure 5에 나타나 있습니다:\n\n\u003cimg src=\"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_5.png\" /\u003e\n\n훈련 데이터 D_gen을 획득한 후, 다음 토큰 예측 표준 목적 함수를 다음과 같이 구성할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-20-AdvancedRAG08Self-RAG_6.png)\n\nM 생성기는 결과뿐만 아니라 반영 토큰도 예측해야 합니다.\n\n# self-RAG에 대한 나의 인사이트와 생각\n\n일반적으로 self-RAG는 RAG 프로세스를 강화하는 새로운 관점을 제공합니다. 그러나 더 복잡한 훈련 과정이 필요하며 생성 단계 중에 여러 레이블 생성과 판단이 필요하기 때문에 추론 비용이 증가하기 때문에 실시간 성능이 필요한 프로젝트에는 중요한 영향을 줄 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한, 이 프레임워크 내에서 최적화할 여지가 많이 있습니다. 더 많은 토론과 혁신을 일으키기 위해 몇 가지 포인트를 공유하겠습니다:\n\n- 반영 토큰을 최적화하는 방법. Self-RAG는 네 가지 반영 토큰을 설계했습니다. [검색] 토큰 외에도 세 가지([IsREL], [IsSUP], [IsUSE])는 특정 유사성이 있습니다. 더 적은 반영 토큰을 사용하거나 다른 의미를 나타내는 반영 토큰을 고려하는 것이 타당한 방향일 수 있습니다.\n- 비평가 모델이 LLM을 사용하는 이유는 무엇인가요? 제 생각에는 [IsUSE]와 같은 토큰이 공통 지식에 많이 의존하기 때문일 수 있습니다. 질의에 대한 답변의 유용성을 판단하는 것은 더 작은 모델이 수행할 수도 있습니다. 그러나 이러한 모델은 일반적인 지식을 부족하게 습득하며 종래의 특정 교육 자료만을 학습합니다. 따라서 비평가 모델로 LLM을 사용하는 것이 합리적일 수 있습니다.\n- 비평가 모델 크기 선택. Self-RAG는 7B 및 13B 모델로 테스트되어 우수한 결과를 얻었습니다. 그러나 만약 더 작은 LLM인 3B로 전환하면 어떤 차이를 관찰할 수 있을까요? 마찬가지로, 더 큰 LLM인 33B로 전환했을 때 얼마나 개선을 기대할 수 있을까요?\n- 인간 피드백을 통한 강화학습(RLHF)을 사용하지 않는 이유는 무엇인가요? 논문에서는 작업 예제를 통해 대상 언어 모델을 학습하는 것을 제안합니다. 이 예제는 비평가 모델에서 오프라인으로 반영 토큰이 추가된 것입니다. 이로 인해 RLHF 대비 훨씬 낮은 교육 비용이 발생합니다. 또한, self-RAG의 반영 토큰은 추론 중 생성을 제어할 수 있게 만들어주며 RLHF는 훈련 중 인간의 선호도 조정에 초점을 두고 있습니다. 그러나 논문에는 RLHF와 관련된 비교 실험 내용이 포함되어 있지 않습니다.\n\n# 결론\n\n본문은 직관적인 예시로 시작하여 Self-RAG의 기본적인 과정을 소개하고 코드 설명을 보완하는 내용을 담고 있습니다. 또한 제 생각과 통찰을 공유하였습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nRAG 기술에 관심이 있다면, 내 다른 기사들도 살펴보세요.\n\n또한, 최신 AI 관련 콘텐츠는 내 뉴스레터에서 찾을 수 있어요.\n\n마지막으로, 어떠한 오류나 누락이 있거나 궁금한 사항이 있으시면 댓글 섹션에서 자유롭게 토론해 주세요.\n","ogImage":{"url":"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png"},"coverImage":"/assets/img/2024-05-20-AdvancedRAG08Self-RAG_0.png","tag":["Tech"],"readingTime":17},{"title":"OpenAI API를 통해 GPT-4o에 접속하기","description":"","date":"2024-05-20 21:08","slug":"2024-05-20-AccessingGPT-4oviaOpenAIAPI","content":"\n![image](/assets/img/2024-05-20-AccessingGPT-4oviaOpenAIAPI_0.png)\n\n# 소개\n\nOpenAI가 최근에 발표한 GPT-4o는 텍스트, 이미지, 비디오 및 오디오 분석에 강력한 능력을 갖춘 첫 번째 멀티 모달 모델입니다. 이는 생성적 AI 모델의 응용 프로그램을 크게 확장시켰습니다. 이 블로그에서는 현재 텍스트 및 이미지 입력을 지원하는 API를 통해 이 모델을 사용하는 방법을 보여 드리려고 합니다. 모든 기능이 아직 제공되지 않았지만, OpenAI가 곧 출시할 예정입니다.\n\n# 특징\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 일반 텍스트 생성\n- JSON 모드의 텍스트 생성\n- 이미지 이해\n- 함수 호출\n\n초기 설정\n\nOpenAI 시크릿 키로 라이브러리를 설치하고 가져온 후, 환경 변수를 설정해주세요.\n\n```js\npip install --upgrade openai --quiet\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n첫 번째 단계는 OpenAI 클라이언트를 설정하는 것입니다. 이를 위해 먼저 시크릿 키로 환경 변수를 만들어야 합니다. OPENAI_KEY=xyz와 같이 OpenAI 시크릿 키가 저장된 .env 파일을 만들어주세요.\n\n작업이 완료되면 dotenv를 사용하여 키에 액세스할 수 있습니다.\n\n```python\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n## API 키 및 모델 이름 설정\nMODEL=\"gpt-4o\"\n\napi_key = os.getenv('OPENAI_KEY')\nclient = OpenAI(api_key=api_key)\n```\n\n일반 텍스트 생성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ncompletion = client.chat.completions.create(\n  model=MODEL,\n  messages=[\n    {\"role\": \"system\", \"content\": \"당신은 도움이 되는 조수입니다. 수학 숙제를 돕습니다!\"}, # \u003c-- 모델에 맥락을 제공하는 시스템 메시지입니다\n    {\"role\": \"user\", \"content\": \"안녕하세요! 2+2를 해결할 수 있나요?\"}  # \u003c-- 모델이 응답을 생성할 사용자 메시지입니다\n  ]\n)\n\nprint(\"조수: \" + completion.choices[0].message.content)\n```\n\n출력:\n\n물론이죠! 2 + 2 = 4. 다른 도움이 필요하시면 언제든지 물어보세요!\n\nJson 모드에서 텍스트 생성하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ncompletion = client.chat.completions.create(\n  (model = MODEL),\n  (response_format = { type: \"json_object\" }),\n  (messages = [\n    {\n      role: \"system\",\n      content: \"You are a trainer who always responds in JSON\",\n    },\n    { role: \"user\", content: \"Create a weekly workout routine for me\" },\n  ])\n);\n\njson.loads(completion.choices[0].message.content);\n```\n\n출력:\n\n```bash\n'‘workoutRoutine’: '‘week’: 1, ‘days’: '‘Monday’: '‘muscleGroup’: ‘Chest and Triceps’, ‘exercises’: ['‘name’: ‘Bench Press’, ‘sets’: 4, ‘reps’: 12', '‘name’: ‘Incline Dumbbell Press’, ‘sets’: 4, ‘reps’: 12', '‘name’: ‘Tricep Dips’, ‘sets’: 3, ‘reps’: 15', '‘name’: ‘Tricep Pushdown’, ‘sets’: 3, ‘reps’: 15']', ‘Tuesday’: '‘muscleGroup’: ‘Back and Biceps’, ‘exercises’: ['‘name’: ‘Pull-Ups’, ‘sets’: 4, ‘reps’: 10', '‘name’: ‘Deadlifts’, ‘sets’: 4, ‘reps’: 12', '‘name’: ‘Barbell Rows’, ‘sets’: 4, ‘reps’: 12', '‘name’: ‘Bicep Curls’, ‘sets’: 3, ‘reps’: 15']',\n```\n\n- 이미지 이해\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n로컬 이미지 사용하기\n\n```js\nfrom IPython.display import Image, display, Audio, Markdown\nimport base64\n\nIMAGE_PATH = \"triangle.png\"\n\n# 컨텍스트를 위한 이미지 미리보기\ndisplay(Image(IMAGE_PATH))\n```\n\n\u003cimg src=\"/assets/img/2024-05-20-AccessingGPT-4oviaOpenAIAPI_1.png\" /\u003e\n\n```js\n# 이미지 파일 열고 base64 문자열로 인코딩하기\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nbase64_image = encode_image(IMAGE_PATH)\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"당신은 Markdown으로 응답하는 유용한 도우미입니다. 내 수학 숙제를 도와주세요!\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"삼각형의 면적은 얼마인가요?\"},\n            {\"type\": \"image_url\", \"image_url\": {\n                \"url\": f\"data:image/png;base64,{base64_image}\"}\n            }\n        ]}\n    ],\n    temperature=0.0,\n)\n\nprint(response.choices[0].message.content)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```bash\n삼각형의 면적을 찾기 위해 직갛각 삼각형의 면적을 구하는 공식을 사용할 수 있습니다: \\[ \\text{면적} = \\frac{1}{2} \\times \\text{밑변} \\times \\text{높이} \\] 이 삼각형에서 밑변은 20 cm이고 높이는 15 cm입니다. \\[ \\text{면적} = \\frac{1}{2} \\times 20 \\, \\text{cm} \\times 15 \\, \\text{cm} \\] \\[ \\text{면적} = \\frac{1}{2} \\times 300 \\, \\text{cm}² \\] \\[ \\text{면적} = 150 \\, \\text{cm}² \\] 따라서, 삼각형의 면적은 \\( 150 \\, \\text{cm}² \\)입니다.\n```\n\nURL을 사용하는 예시\n\n```js\nresponse = client.chat.completions.create(\n  (model = MODEL),\n  (messages = [\n    { role: \"system\", content: \"마크다운으로 응답하는 유용한 도우미입니다.\" },\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: \"이 이미지에서 무엇을 보고 무슨 감정이 표현되었는지 설명해주세요.\",\n        },\n        {\n          type: \"image_url\",\n          image_url: {\n            url: \"https://pbs.twimg.com/media/GNeb4-Ua8AAuaKp?format=png\u0026name=small\",\n          },\n        },\n      ],\n    },\n  ]),\n  (temperature = 0.0)\n);\n\nprint(response.choices[0].message.content);\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이미지:\n\n![이미지](/assets/img/2024-05-20-AccessingGPT-4oviaOpenAIAPI_2.png)\n\n결과:\n\n이 이미지는 웃는 사람을 보여줍니다. 전달되는 감정은 행복이나 만족으로 보입니다. 웃음은 긍정적이고 즐거운 기분을 시사합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기능 호출\n\n```js\n# NBA 게임 점수를 가져 오기 위한 Mock 함수\ndef get_nba_game_score(team):\n    print('get_nba_game_score가 호출되었습니다.')\n    \"\"\"주어진 팀에 대한 NBA 게임의 현재 점수를 가져옵니다.\"\"\"\n    if \"lakers\" in team.lower():\n        return json.dumps({\"team\": \"Lakers\", \"score\": \"102\", \"opponent\": \"Warriors\", \"opponent_score\": \"98\"})\n    elif \"bulls\" in team.lower():\n        return json.dumps({\"team\": \"Bulls\", \"score\": \"89\", \"opponent\": \"Celtics\", \"opponent_score\": \"95\"})\n    else:\n        return json.dumps({\"team\": team, \"score\": \"N/A\", \"opponent\": \"N/A\", \"opponent_score\": \"N/A\"})\n```\n\n필요한 경우 도구를 통해 함수 호출:\n\n```js\ndef function_calling():\n    # 단계 1: 사용자 메시지로 대화를 초기화합니다\n    messages = [{\"role\": \"user\", \"content\": \"레이커스 게임 점수가 어떻게 되나요?\"}]\n\n    # 모델이 사용할 수있는 도구(함수) 정의\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_nba_game_score\",\n                \"description\": \"주어진 팀의 NBA 게임의 현재 점수를 가져옵니다.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"team\": {\n                            \"type\": \"string\",\n                            \"description\": \"NBA 팀의 이름, 예: 레이커스, 불스\",\n                        },\n                    },\n                    \"required\": [\"team\"],\n                },\n            },\n        }\n    ]\n\n    # 단계 2: 대화 컨텍스트와 사용 가능한 도구를 모델에게 전송합니다\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",  # auto가 기본값입니다. 명시적으로 지정해줍니다.\n    )\n\n    # 모델의 응답을 추출합니다.\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls  # 모델이 도구를 호출하도록 요청하는지 확인합니다\n\n    # 단계 3: 모델이 요청한 도구 호출이 있는지 확인합니다\n    if tool_calls:\n        # 사용 가능한 함수 정의\n        available_functions = {\n            \"get_nba_game_score\": get_nba_game_score,\n        }  # 이 예제에서는 함수가 한 개뿐이지만 확장할 수 있습니다\n\n        # 모델의 응답을 대화 기록에 추가합니다\n        messages.append(response_message)\n\n        # 단계 4: 모델에서 요청된 함수를 호출합니다\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(tool_call.function.arguments)\n\n            print(f\"도구 호출: {tool_call}\")\n\n            # 추출 된 인수로 함수를 호출합니다\n            function_response = function_to_call(\n                team=function_args.get(\"team\"),\n            )\n\n            # 함수 응답을 대화 기록에 추가합니다\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": function_response,\n                }\n            )\n\n        # 단계 5: 업데이트된 기록을 사용하여 대화를 계속합니다\n        second_response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages,\n        )  # 함수 응답을 확인할 수 있는 모델의 새로운 응답을 받습니다\n\n        return second_response\n\n# 대화를 실행하고 결과를 인쇄합니다\nresponse = function_calling()\nprint(response.choices[0].message.content)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n출력:\n\n도구 호출: ChatCompletionMessageToolCall(id='call_k2lcfdlVAcQ8PTUL1uwu3fYz', function=Function(arguments=' \"team\": \"Lakers\" ', name='get_nba_game_score'), type='function') get_nba_game_score 호출됨\n\n현재 레이커스 경기 점수는 레이커스 102, 워리어스 98 입니다.\n\n마무리\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 전체 기사를 읽으셔서 GPT-4o 모델을 활용해 텍스트 생성, JSON 모드, 이미지 이해, 그리고 함수 호출을 OpenAI API를 통해 사용할 준비가 되셨습니다. API에 오디오 및 비디오 지원이 추가되면 다시 블로그를 쓸 계획입니다. 그 때까지 계속 탐험하고 배우세요!\n","ogImage":{"url":"/assets/img/2024-05-20-AccessingGPT-4oviaOpenAIAPI_0.png"},"coverImage":"/assets/img/2024-05-20-AccessingGPT-4oviaOpenAIAPI_0.png","tag":["Tech"],"readingTime":11},{"title":"BERT 코드와 함께하는 완벽 가이드","description":"","date":"2024-05-20 20:59","slug":"2024-05-20-ACompleteGuidetoBERTwithCode","content":"\n## 역사, 아키텍처, 사전 훈련 및 미세 조정\n\n\"LLMs from Scratch\" 시리즈의 제4부 - 대형 언어 모델을 이해하고 구축하는 완벽한 가이드입니다. 이러한 모델이 어떻게 작동하는지 알아보고 싶다면 아래 내용을 읽어보시기를 권장합니다:\n\n- Prologue: LLMs와 Transformers의 간단한 역사\n- 제1부: 토큰화 - 전체 가이드\n- 제2부: Python에서 처음부터 word2vec으로 단어 임베딩\n- 제3부: Self-Attention으로 Transformer 임베딩 생성\n- 제4부: 코드와 함께 BERT의 완전 가이드 - 역사, 아키텍처, 사전 훈련 및 미세 조정\n\n# 소개\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n버트(Bidirectional Encoder Representations from Transformers)는 구글 AI Language에서 개발한 대규모 언어 모델(Large Language Model, LLM)로, 자연어 처리(Natural Language Processing, NLP) 분야에서 중요한 발전을 이루고 있습니다. 최근 몇 년간 많은 모델이 버트에 영감을 받아 발전하거나 직접적인 개선을 하였는데, RoBERTa, ALBERT, DistilBERT 등이 대표적입니다. 최초의 버트 모델은 OpenAI의 Generative Pre-trained Transformer (GPT) 이후 빠르게 공개되었으며, 둘 다 그 전년에 제안된 Transformer 아키텍처에 기반을 두었습니다. GPT는 자연어 생성(Natural Language Generation, NLG)에 초점을 맞추었지만, 버트는 자연어 이해(Natural Language Understanding, NLU)에 우선순위를 두었습니다. 이 두 개발은 NLP의 지형을 재편하며 기계 학습의 진전에 주목할 만한 이정표로 자리 잡았습니다.\n\n다음 글은 버트의 역사를 살펴보고, 창조 당시의 환경을 자세히 소개할 것입니다. 이를 통해 논문 저자들이 한 아키텍처적 결정 뿐만 아니라 산업 및 취미용 응용 프로그램에서 버트를 훈련하고 세밀 조정하는 방법을 이해할 수 있는 완전한 그림을 제공할 것입니다. 우리는 다이어그램으로 아키텍처를 자세히 살펴보고, 감정 분석 작업을 위해 버트를 세밀 조정하는 코드를 처음부터 작성해볼 것입니다.\n\n# 목차\n\n1 — 버트의 역사 및 주요 기능\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2 - 아키텍처 및 사전 훈련 목표\n\n3 - 감정 분석을 위한 BERT 파인 튜닝\n\n4 - 결론\n\n5 - 더 많은 독해\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 1 — BERT의 역사 및 주요 기능\n\nBERT 모델은 다음 네 가지 주요 기능에 의해 정의될 수 있습니다:\n\n- 인코더 전용 구조\n- 사전 훈련 접근 방식\n- 모델 미세 조정\n- 양방향 문맥 활용\n\n이러한 기능 각각은 논문의 저자들이 만든 설계 선택사항이며, 이 모델이 생성된 시기를 고려하여 이해할 수 있습니다. 다음 섹션에서는 이러한 기능 각각을 살펴보고, 이러한 기능이 BERT의 동시대인 Transformer와 GPT에서 영감을 받았거나 그들을 개선하기 위한 것임을 보여줄 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 1.1 — 인코더 전용 아키텍처\n\n2017년 트랜스포머의 등장으로 혁신적인 디자인을 바탕으로 한 새로운 모델을 생산하기 위한 레이스가 시작되었습니다. OpenAI는 2018년 6월 첫 번째로 GPT를 만들어내 역량 있는 NLG를 자랑하며 나중에는 ChatGPT를 구동하는 모델을 발표했습니다. 구글은 이에 4개월 후인 BERT를 공개하여 NLU를 위해 설계된 인코더 전용 모델을 선보였습니다. 이 두 아키텍처 모두 매우 뛰어난 모델을 생산할 수 있지만 수행할 수 있는 작업들은 약간 다를 수 있습니다. 각 아키텍처에 대한 개요는 아래에서 제공됩니다.\n\n디코더 전용 모델:\n\n- 목표: 입력 시퀀스에 대한 새로운 출력 시퀀스 예측\n- 개요: 트랜스포머의 디코더 블록은 인코더에 제공된 입력을 바탕으로 출력 시퀀스를 생성하는 역할을 합니다. 디코더 전용 모델은 인코더 블록을 완전히 생략하고 여러 디코더를 단일 모델에 쌓아 올려 생성됩니다. 이러한 모델은 입력으로 프롬프트를 받아들이고, 다음 가장 확률이 높은 단어를 하나씩 예측함으로써 응답을 생성하는 작업으로 알려진 Next Token Prediction (NTP)이라는 작업을 수행합니다. 그 결과, 디코더 전용 모델은 대화형 챗봇, 기계 번역 및 코드 생성과 같은 NLG 작업에서 뛰어난 성능을 발휘합니다. 이러한 종류의 모델은 ChatGPT에서 구동되는 디코더 전용 모델 (GPT-3.5 및 GPT-4)의 광범위한 사용으로 인해 일반 대중에게 가장 익숙할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n인코더 전용 모델:\n\n- 목표: 입력 시퀀스 내의 단어에 대한 예측 수행\n- 개요: Transformer의 인코더 블록은 입력 시퀀스를 수용하고 각 단어(또는 좀 더 구체적으로, 각 토큰)에 대한 풍부한 숫자 벡터 표현을 생성하는 역할을 합니다. 인코더 전용 모델은 디코더를 생략하고 여러 Transformer 인코더를 쌓아 하나의 모델을 생성합니다. 이러한 모델은 프롬프트를 수용하지 않고, 예측을 수행할 입력 시퀀스(예: 시퀀스 내의 빠진 단어를 예측)를 받습니다. 인코더 전용 모델은 새로운 단어 생성을 위해 디코더를 사용하지 않기 때문에 GPT와 같이 대화형 챗봇 애플리케이션에 사용되지 않습니다. 대신, 인코더 전용 모델은 대부분 NLU 작업인 Named Entity Recognition (NER) 및 감성 분석에 주로 사용됩니다. 인코더 블록에서 생성된 풍부한 벡터 표현은 BERT가 입력 텍스트를 심층적으로 이해하는 데 기여합니다. BERT 저자들은 이 구조적 선택이 BERT의 성능을 향상시킬 것이라고 주장했으며, 특히 디코더 전용 구조는 GPT와 비교하여 BERT의 성능을 향상시킬 것이라고 기술했습니다.\n\nTransformer, GPT 및 BERT를 위한 아키텍처 다이어그램:\n\n지금까지 논의한 세 모델에 대한 아키텍처 다이어그램이 아래에 나와 있습니다. 이는 원본 Transformer 논문 \"Attention is All You Need\" [2]의 아키텍처 다이어그램을 적응하여 작성되었습니다. 모델의 인코더 또는 디코더 블록 수는 N으로 표시됩니다. 원래 Transformer에서 인코더와 디코더 각각은 쌓인 6개의 인코더 및 디코더 블록으로 이루어져 있기 때문에, N은 각각 6입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_0.png)\n\n## 1.2 — Pre-training Approach\n\nGPT은 BERT의 개발에 여러 가지 방식으로 영향을 미쳤습니다. 모델이 첫 번째 디코더 전용 변형 모델이었을 뿐만 아니라 GPT는 또한 모델 사전 훈련을 인기 있게 만들었습니다. 사전 훈련은 언어의 넓은 이해를 얻기 위해 단일 대형 모델을 훈련하는 것을 포함하며 이는 단어 사용 및 문법적 패턴과 같은 측면을 아우릅니다. 그 결과, 작업에 중립적인 기본 모델을 생성합니다. 위 다이어그램에서, 기본 모델은 선형 레이어 아래의 구성 요소들로 이루어져 있습니다 (보라색으로 표시됨). 훈련된 후, 이 기본 모델의 복사본은 특정 작업을 해결하기 위해 미세 조정될 수 있습니다. 미세 조정은 선형 레이어만 훈련하는 것을 의미합니다: 작은 피드포워드 신경망으로 종종 분류 헤드 또는 헤드라고 불립니다. 모델의 나머지 부분(즉, 기초 부분)에 있는 가중치와 바이어스는 변경되지 않거나 고정됩니다.\n\n아날로지:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n간략한 비유를 만들어보겠습니다. 감성 분석 작업을 생각해보세요. 여기서 목표는 표현된 감정에 기반하여 텍스트를 긍정 또는 부정으로 분류하는 것입니다. 예를 들어, 어떤 영화 리뷰에서 \"이 영화를 사랑했다\"라는 텍스트는 긍정으로 분류되고, \"이 영화를 싫어했다\"라는 텍스트는 부정으로 분류될 것입니다. 전통적인 언어 모델링 접근 방식에서는 이 작업에 특화된 새로운 아키텍처를 일반적으로 처음부터 학습할 것입니다. 이것은 영화 리뷰를 보여주는 것으로 영어를 처음부터 가르치는 것과 같다고 생각할 수 있습니다. 물론, 이것은 느리고 비용이 많이 들며 많은 학습 예제가 필요합니다. 게다가, 그 결과로 얻는 분류기는 여전히 이 한 가지 작업에만 능숙할 것입니다. 대조적으로, 사전 훈련 접근 방식에서는 범용 모델을 가져와서 이를 감성 분석 작업에 대해 미세 조정합니다. 이것은 이미 영어에 능숙한 사람에게 현재 작업에 익숙해지도록 모자란 수의 영화 리뷰를 보여주는 것으로 생각할 수 있습니다. 아마도 두 번째 접근 방식이 훨씬 더 효율적하다는 것이 직관적일 것입니다.\n\n**사전 훈련에 대한 이전 시도:**\n\n사전 훈련 개념은 OpenAI에 의해 발명된 것이 아니며, 그 이전 몇 년 동안 다른 연구자들에의해 탐구되어왔습니다. 한 가지 주목할만한 예는 Allen Institute의 연구자들이 개발한 ELMo 모델(Embeddings from Language Models)입니다. 이전 시도에도 불구하고, OpenAI의 큰 논문에서처럼 다른 연구자들은 사전 훈련의 효과를 명백하게 입증하지 못했습니다. 그들 자신의 말에 따르면, 팀은 이 발견으로 사전 훈련 패러다임을 앞으로의 언어 모델링에서 우세한 접근 방식으로 확립시켰습니다. 이 추세와 일치하도록, BERT 저자들도 사전 훈련 접근 방식을 완전히 채택했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 1.3 — 모델 파인튜닝\n\n파인튜닝의 장점:\n\n파인튜닝은 오늘날 흔한 일로, 이러한 접근 방식이 주목받기 전 얼마나 최신인지 쉽게 간과하기 쉽습니다. 2018년 이전까지는 각기 다른 NLP 작업을 위해 새로운 모델 아키텍처를 도입하는 것이 일반적했습니다. 훈련을 사전에 시작함으로써 신규 모델 개발에 필요한 훈련 시간과 컴퓨팅 비용이 급격히 감소했을 뿐만 아니라 필요한 훈련 데이터의 양도 줄어들었습니다. 언어 모델을 처음부터 완전히 재설계하고 재훈련하는 대신 제네릭 모델인 GPT를 소량의 작업별 데이터로 파인튜닝함으로써 해당 시간을 분수로 줄일 수 있습니다.\n\n작업에 따라서 분류 헤드를 수정하여 다른 수의 출력 뉴런을 포함시킬 수 있습니다. 이는 감정 분석과 같은 분류 작업에 유용합니다. 예를 들어 BERT 모델의 원하는 출력이 리뷰가 긍정인지 부정인지 예측하는 것이라면, 헤드를 두 개의 출력 뉴런이 있는 형태로 변경할 수 있습니다. 각각의 활성화는 리뷰가 긍정인지 부정인지일 확률을 나타냅니다. 10 클래스로 구성된 다중 분류 작업의 경우, 헤드를 10개의 뉴런을 가진 출력 레이어로 변경할 수 있습니다. 이렇게 함으로써 BERT를 더 다양하게 활용할 수 있어 여러 하류 작업에 기본 모델이 사용될 수 있습니다.\n\nBERT의 파인튜닝:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nBERT은 GPT의 발자취를 따르며 사전 훈련/미세 조정 접근 방식을 채택했습니다. 구글은 BERT의 두 가지 버전을 출시했는데, Base와 Large로 하드웨어 제약에 따라 모델 크기를 유저에게 유연하게 제공했습니다. 양쪽 모델 모두 많은 TPUs(텐서 처리 장치)에서 약 4일이 걸렸다고 하는데, BERT Base는 16개의 TPUs에서 훈련되었고 BERT Large는 64개의 TPUs에서 훈련되었습니다. 대부분의 연구자, 취미로 연구하는 사람들 및 산업 실무자들에게는 이 수준의 훈련이 현실적이지 않을 수 있습니다. 그래서 어떤 특정 작업에 기초 모델을 몇 시간 동안 미세 조정하는 아이디어는 훨씬 더 매력적인 대안입니다. 원래 BERT 아키텍처는 다양한 작업과 데이터셋에 걸쳐 수천 번의 미세 조정 이터레이션을 거쳤는데, 이 중 많은 것들이 Hugging Face와 같은 플랫폼에서 공개적으로 다운로드할 수 있습니다.\n\n## 1.4 — 양방향 컨텍스트 사용\n\n언어 모델로, BERT는 이전 단어가 관찰되었을 때 특정 단어가 관측될 확률을 예측합니다. 이 기본적인 측면은 아키텍처와 의도된 작업에 관계없이 모든 언어 모델이 공유하는 것입니다. 그러나 모델이 이러한 확률을 활용하는 것이 모델의 특정 작업에 적합한 행동을 부여합니다. 예를 들어 GPT는 시퀀스에서 다음으로 가장 가능성이 높은 단어를 예측하도록 훈련됩니다. 즉, 모델은 이전 단어가 관찰되었을 때 다음 단어를 예측합니다. 다른 모델들은 감정 분석에 훈련될 수 있고, 긍정적 또는 부정적과 같은 텍스트 라벨을 사용하여 입력 시퀀스의 감정을 예측할 수 있습니다. 텍스트에 대한 어떠한 의미 있는 예측을 하려면 주변 컨텍스트가 이해되어야 합니다. 특히 NLU 작업에서는 BERT가 이러한 이해를 보장하여 양방향성이라는 핵심 특성 중 하나를 통해 우수한 이해를 제공합니다.\n\n양방향성은 아마도 BERT의 가장 중요한 특징인데, NLU 작업에서 BERT의 높은 성능의 중요한 이유이며, 또한 모델의 인코더 전용 아키텍처의 주요 동기입니다. Transformer 인코더의 self-attention 메커니즘은 양방향 컨텍스트를 계산하지만, 이와 같은 것을 할 수 없는 디코더는 단방향 컨텍스트를 생산합니다. BERT 저자들은 GPT의 이 양방향성 부족으로 인해 BERT만큼의 언어 표현 깊이를 달성하지 못한다고 주장했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n양방향성 정의하기:\n\n그렇다면 \"양방향성\"이라는 맥락이 정확히 무엇을 의미할까요? 여기서 양방향성은 입력 시퀀스의 각 단어가 앞뒤 단어(좌측 맥락과 우측 맥락이라고 함)로부터 맥락을 확보할 수 있다는 것을 의미합니다. 기술적인 용어로는 어텐션 메커니즘이 각 단어에 대해 이전 및 이후의 토큰에 주의를 기울일 수 있다고 말합니다. 이를 분해해보면 BERT는 입력 시퀀스 내 단어에 대해서만 예측을 하고 GPT처럼 새로운 시퀀스를 생성하지는 않습니다. 따라서 BERT가 입력 시퀀스 내의 단어를 예측할 때, 모든 주변 단어에서 문맥적 단서를 접목할 수 있습니다. 이는 양방향으로 문맥을 제공하므로 BERT가 보다 정보를 기반으로 한 예측을 할 수 있도록 도와줍니다.\n\n이를 GPT와 같은 디코더 전용 모델과 대조해보면, 여기서 목표는 출력 시퀀스를 생성하기 위해 한 번에 새로운 단어를 예측하는 것입니다. 각 예측된 단어는 이전 단어가 제공한 맥락(좌측 맥락)만 활용할 수 있으며, 그 이후의 단어(우측 맥락)는 아직 생성되지 않았습니다. 그러므로 이러한 모델을 단방향으로 지칭합니다.\n\n![이미지](\"/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_1.png\")\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이미지 설명:\n\n위의 이미지는 양방향 컨텍스트를 사용하는 전형적인 BERT 작업과 단방향 컨텍스트를 사용하는 전형적인 GPT 작업의 예시를 보여줍니다. BERT에서는 이 [MASK]로 표시된 가리킨 가림막된 단어를 예측하는 것이 작업입니다. 이 단어는 왼쪽과 오른쪽에 단어가 있기 때문에 양쪽의 단어를 사용하여 컨텍스트를 제공할 수 있습니다. 만약 당신이 인간으로써 이 문장을 왼쪽 또는 오른쪽 컨텍스트만 가지고 읽는다면, 가리킨 가림막된 단어를 본인이 예측하기 어려울 것입니다. 그러나 양방향 컨텍스트를 사용하면 가림막된 단어가 'fishing'이라는 것을 추측하는 것이 훨씬 더 가능해집니다.\n\nGPT의 경우, 목표는 전통적인 NTP 작업을 수행하는 것입니다. 이 경우, 목적은 입력 시퀀스 및 이미 생성된 단어를 기반으로 새로운 시퀀스를 생성하는 것입니다. 입력 시퀀스가 모델에게 시를 쓰라는 지시를 준 상황에서 이미 생성된 단어가 \"Upon a\" 인 것을 고려한다면, 다음 단어는 \"river\"가 될 것으로 예상해볼 수 있습니다. 여러 후보 단어가 있을 때 GPT(언어 모델로서)는 어휘 중 각 단어가 다음에 나타날 가능성을 계산하고 훈련 데이터를 기반으로 가장 가능성이 높은 단어 중 하나를 선택합니다.\n\n## 1.5 — BERT의 한계\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n양방향 모델로서 BERT는 두 가지 주요 단점을 가지고 있습니다:\n\n1. 훈련 시간이 증가합니다:\n\n트랜스포머 기반 모델의 양방향성은 당시 주류였던 좌측에서 우측으로의 문맥 모델에 대한 직접적인 개선으로 제안되었습니다. GPT는 입력 시퀀스에 대한 문맥 정보를 일방적으로만 얻을 수 있기 때문에 단어 사이의 인과 관계에 대해 완전히 파악하지 못했습니다. 그러나 양방향 모델은 단어 간의 인과 관계를 보다 포괄적으로 이해할 수 있으므로 NLU 작업에서 보다 나은 결과를 볼 수 있습니다. 과거에 양방향 모델이 탐구되었지만, 늦은 1990년대의 양방향 RNN과 같은 한계가 있었습니다. 일반적으로 이러한 모델들은 훈련을 위해 더 많은 계산 리소스를 요구하기 때문에 동일한 계산 성능을 달성하기 위해서는 더 큰 단방향 모델을 훈련해야 합니다.\n\n2. 언어 생성 작업에서 성능이 저하됩니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nBERT는 NLU 작업을 해결하기 위해 특별히 설계되었으며, 디코더 및 새로운 시퀀스를 생성하는 능력을 포기하고 인코더 및 입력 시퀀스에 대한 풍부한 이해를 개발하는 것으로 대체했습니다. 결과적으로 BERT는 NER, 감정 분석 등과 같은 NLP 작업 부분에 최적화되어 있습니다. 특히, BERT는 프롬프트를 수용하지 않고 대신 입력 시퀀스를 처리하여 예측을 작성합니다. BERT는 기술적으로 새로운 출력 시퀀스를 생성할 수 있지만, LLM(언어 모델)의 설계적 차이를 인지하고 중대하게 생각해야 합니다. ChatGPT 시대 이후의 LLM들을 생각할 때와 BERT 설계의 현실과의 차이를 인지하는 것이 중요합니다.\n\n## 2 — 아키텍처 및 사전 훈련 목표\n\n### 2.1 — BERT의 사전 훈련 목표 개요\n\n양방향 모델을 훈련시키려면 좌/우 컨텍스트가 모두 예측에 활용되는 작업이 필요합니다. 따라서 저자들은 BERT의 언어 이해를 강화하기 위해 주의 깊게 2가지 사전 훈련 목표를 구성했습니다. 이것들은 Masked Language Model(MLM) 작업과 Next Sentence Prediction(NSP) 작업이었습니다. 각각의 훈련 데이터는 당시 사용 가능한 모든 영어 위키피디아 기사 (25억 단어)와 BookCorpus 데이터셋의 추가 11,038권의 책 (8억 단어)로 구성되었습니다. 초기 데이터는 구체적인 작업에 따라 전처리되어야 했지만, 아래에서 설명한 대로 수행되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 2.2 — 마스크된 언어 모델링 (MLM)\n\nMLM 개요:\n\n마스크된 언어 모델링 작업은 양방향 모델을 훈련해야 하는 필요를 직접적으로 해결하기 위해 만들어졌습니다. 이를 위해 모델은 입력 시퀀스의 좌측 문맥과 우측 문맥을 모두 사용하여 예측을 수행하도록 훈련되어야 합니다. 이는 훈련 데이터에서 15%의 단어를 무작위로 마스크하고 BERT를 사용하여 누락된 단어를 예측하도록 훈련함으로써 달성됩니다. 입력 시퀀스에서 마스크된 단어는 [MASK] 토큰으로 대체됩니다. 예를 들어, 책 코퍼스에서 발견된 원시 훈련 데이터 중에 A man was fishing on the river 라는 문장이 있다고 가정해보겠습니다. MLM 작업에 대한 훈련 데이터로 원시 텍스트를 변환할 때, 단어 fishing이 무작위로 마스크되어 [MASK] 토큰으로 대체될 수 있으며, 이는 훈련 입력 A man was [MASK] on the river with target fishing을 제공합니다. 따라서 BERT의 목표는 단일 누락된 단어 fishing을 예측하는 것이며, 누락된 단어가 채워진 입력 시퀀스를 재생산하는 것이 아닙니다. 마스킹 프로세스는 MLM 작업의 훈련 데이터를 작성할 때 모든 가능한 입력 시퀀스(예: 문장)에 반복적으로 적용될 수 있습니다. 이 작업은 이전에 언어학 문헌에서 존재했었으며 Cloze 작업으로 알려져 있습니다.[8] 그러나 기계 학습 맥락에서는 BERT의 인기로 인해 MLM으로 흔히 언급됩니다.\n\n미세 조정과 사전 훈련 간 불일치 완화 방법:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저자들은 그러나 [MASK] 토큰은 훈련 데이터에만 나타나고 라이브 데이터(추론 시간)에는 나타나지 않기 때문에 사전 훈련과 세부 튜닝 간에 불일치가 있을 것이라고 지적했습니다. 이를 완화하기 위해 모든 마스킹된 단어가 [MASK] 토큰으로 대체되는 것은 아닙니다. 저자들은 대신 다음과 같이 설명합니다:\n\n예측 단어와 목표 단어 사이의 오차 계산:\n\nBERT는 BERT Base 및 BERT Large 모두 최대 512개의 토큰을 입력으로 받습니다. 시퀀스에서 최대 토큰 수보다 적은 경우, [PAD] 토큰을 사용하여 패딩이 추가되어 최대 512개에 도달합니다. 출력 토큰 수도 입력 토큰 수와 정확히 일치합니다. 입력 시퀀스의 i 위치에 마스크 토큰이 있는 경우, BERT의 예측은 출력 시퀀스의 i 위치에 있을 것입니다. 훈련 목적으로 다른 모든 토큰은 무시되므로 모델의 가중치 및 편향에 대한 업데이트는 입력 시퀀스의 i 위치에 있는 예측 토큰과 목표 토큰 간의 오차를 기반으로 계산됩니다. 이 오차는 Cross Entropy Loss(음의 로그 우도) 함수를 사용하여 계산됩니다. 이러한 내용은 나중에 보게 될 것입니다.\n\n## 2.3 — 다음 문장 예측 (NSP)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n개요:\n\nBERT의 사전 학습 작업 중 두 번째는 Next Sentence Prediction입니다. 이 작업은 하나의 세그먼트(일반적으로 문장)가 다른 세그먼트를 논리적으로 잇는지 분류하는 것을 목표로 합니다. NSP를 사전 학습 작업으로 선택한 이유는 MLM을 보완하고 BERT의 NLU 능력을 향상시키기 위한 것이며, 저자들은 다음과 같이 설명합니다:\n\nNSP에 대해 사전 학습함으로써, BERT는 채의 흐름에 대한 이해를 발전시킬 수 있으며, 이는 많은 NLU 문제에 유용합니다. 예를 들어 다음과 같은 것들이 있습니다:\n\n- 내용을 바꿔 말한 문장 쌍\n- 추론을 위한 가설-전제 쌍\n- 질문 응답에서 질문-통과 문장 쌍\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nBERT에서의 NSP 구현:\n\nNSP의 입력은 첫 번째와 두 번째 세그먼트(표시된 A 및 B)로 구성되며 두 번째 [SEP] 토큰이 있는 [SEP] 토큰으로 구분되고 끝에 두 번째 [SEP] 토큰이 있습니다. BERT는 실제로 NSP를 수행하든 아니든 입력 시퀀스 당 최소한 하나의 [SEP] 토큰을 예상합니다. 이 토큰은 시퀀스의 끝을 나타내며, MLM 작업에 대한 입력의 끝에 이러한 토큰 중 하나가 추가됩니다. 또한, NSP가 포함되지 않은 다른 모든 작업들에 대해서도 동일한 처리가 이루어집니다. NSP는 분류 문제를 형성하며, 출력은 A 세그먼트가 B 세그먼트를 논리적으로 따르는 경우 IsNext에 해당하고, 그렇지 않은 경우 NotNext에 해당합니다. 교육 데이터는 단어 조각(WordPiece) 토크나이저가 50%의 경우에는 다음 문장과 함께 문장을 선택하고, 나머지 50%의 경우에는 무작위 문장을 선택함으로써 어떤 단일 언어 말뭉치에서 쉽게 생성할 수 있습니다.\n\nBERT의 입력 임베딩:\n\nBERT의 입력 임베딩 과정은 위치 인코딩, 세그먼트 임베딩 및 토큰 임베딩 세 단계로 구성됩니다(아래 다이어그램 참조).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위치 인코딩:\n\nTransformer 모델과 마찬가지로 각 토큰의 임베딩에 위치 정보가 주입됩니다. 그러나 Transformer와 달리 BERT의 위치 인코딩은 고정되어 있고 함수에 의해 생성되지 않습니다. 이는 BERT가 BERT 베이스와 BERT 라지의 입력 시퀀스에서 512개의 토큰으로 제한된다는 것을 의미합니다.\n\n세그먼트 임베딩:\n\n각 토큰이 속한 세그먼트를 인코딩하는 벡터도 추가됩니다. MLM 사전 훈련 작업 또는 다른 NSP 작업(단일 [SEP] 토큰만 있는 경우)의 경우 입력의 모든 토큰은 세그먼트 A에 속하는 것으로 간주됩니다. NSP 작업의 경우, 두 번째 [SEP] 이후의 모든 토큰은 세그먼트 B로 표시됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n토큰 임베딩:\n\n원래의 Transformer와 마찬가지로 각 토큰에 대해 학습된 임베딩은 위치 및 세그먼트 벡터에 추가되어 BERT에서 자기 주의 메커니즘에 전달되는 최종 임베딩을 만들어 내며 문맥 정보를 추가합니다.\n\n![이미지](/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_2.png)\n\n## 2.5 — 특별 토큰\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 이미지에서 볼 수 있었듯이 입력 시퀀스에는 [CLS] (분류) 토큰이 앞에 추가되었습니다. 이 토큰은 전체 입력 시퀀스의 의미론적 의미를 요약하고, BERT가 분류 작업을 수행하는 데 도움이 됩니다. 예를 들어, 감성 분석 작업에서 최종 층의 [CLS] 토큰은 입력 시퀀스의 감정이 긍정적인지 부정적인지 예측 추출하기 위해 분석될 수 있습니다. [CLS] 및 [PAD] 등은 BERT의 특수 토큰 예시입니다. 여기서 BERT의 특수 토큰은 총 다섯 개 있습니다. 아래에 요약을 제공합니다:\n\n- [PAD] (토큰 ID: 0) — 512개 토큰으로 구성된 입력 시퀀스의 총 수를 맞추기 위해 사용되는 패딩 토큰.\n- [UNK] (토큰 ID: 100) — BERT 어휘에 없는 토큰을 나타내기 위해 사용되는 알 수 없는 토큰.\n- [CLS] (토큰 ID: 101) — 분류 토큰으로 기대되는 것은 각 시퀀스의 시작 부분에 나타냅니다. 이러한 토큰은 분류 작업을 위한 클래스 정보를 캡슐화하며, 종합적인 시퀀스 표현으로 생각할 수 있습니다.\n- [SEP] (토큰 ID: 102) — 단일 입력 시퀀스의 두 세그먼트를 구분하는 데 사용되는 구분자 토큰 (예: Next Sentence Prediction). 적어도 입력 시퀀스 당 하나의 [SEP] 토큰이 필요하며, 최대 두 개까지 사용 가능합니다.\n- [MASK] (토큰 ID: 103) — 마스크 토큰은 BERT를 마스킹된 언어 모델링 작업으로 훈련하거나 마스크된 시퀀스에 대한 추론을 수행하는 데 사용됩니다.\n\n## 2.4 — BERT Base와 BERT Large의 아키텍처 비교\n\nBERT Base와 BERT Large는 아키텍처적으로 매우 유사합니다. 두 모델 모두 WordPiece 토크나이저를 사용하여(따라서 앞에서 설명한 동일한 특수 토큰을 사용) 최대 시퀀스 길이가 512 토큰입니다. BERT의 어휘 크기는 30,522이며, 그 중 약 1,000 개의 토큰은 \"사용되지 않은\" 상태를 유지합니다. 사용되지 않은 토큰은 사용자가 전체 토크나이저를 다시 훈련하지 않고 사용자 지정 토큰을 추가할 수 있도록 고의로 비워둡니다. 의료 및 법률 용어와 같은 도메인별 어휘와 함께 작업할 때 유용합니다. BERT Base와 BERT Large는 원래 Transformer의 embedding 차원 (d_model)보다 높은 수를 갖습니다. 이는 모델의 어휘에 대해 학습된 벡터 표현의 크기에 해당합니다. BERT Base의 d_model은 768이며, BERT Large의 d_model은 1024입니다(원래 Transformer의 512를 두 배 증가시킨 값).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n두 모델은 주로 네 가지 범주에서 차이가 있습니다:\n\n- 인코더 블록의 수, N: 서로 쌓인 인코더 블록의 수입니다.\n- 인코더 블록 당 어텐션 헤드 수: 어텐션 헤드는 입력 시퀀스의 문맥 벡터 임베딩을 계산합니다. BERT는 멀티헤드 어텐션을 사용하므로, 이 값은 인코더 레이어 당 헤드 수를 나타냅니다.\n- 피드포워드 네트워크의 은닉층 크기: 선형 레이어는 고정된 뉴런 수(예: BERT Base의 경우 3072)를 가진 은닉층으로 구성되며, 다양한 크기의 출력 레이어로 이어집니다. 출력 레이어의 크기는 작업에 따라 다릅니다. 예를 들어, 이진 분류 문제는 단지 두 개의 출력 뉴런이 필요하고, 10개 클래스를 가진 다중 클래스 분류 문제는 10개의 뉴런이 필요합니다.\n- 총 매개변수: 모델 내 가중치와 바이어스의 총 수입니다. 당시 수억 개의 모델이 매우 큰 것으로 여겨졌지만, 오늘날 기준에서는 이 값들이 상대적으로 작습니다.\n\n이러한 범주별 BERT Base와 BERT Large간의 비교는 아래 이미지에서 확인할 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 3 — 감정 분석을 위한 BERT Feine-Tuning\n\n이 섹션에서는 파이썬을 사용하여 BERT를 세밀 조정하는 실제 예제를 다룹니다. 코드는 작업에 무관한 세밀 조정 파이프라인 형태로, 파이썬 클래스에 구현되어 있습니다. 그런 다음 이 클래스의 객체를 생성하고 이를 사용하여 감정 분석 작업에 대해 BERT 모델을 세밀 조정할 것입니다. 이 클래스는 질문 응답, 개체 인식 등 다른 작업에 대해 BERT를 세밀 조정하는 데 재사용할 수 있습니다. 섹션 3.1에서 3.5는 세밀 조정 과정을 설명하며, 섹션 3.6에서는 전체 파이프라인을 보여줍니다.\n\n## 3.1 — 세밀 조정 데이터 집합 로드 및 전처리\n\n세밀 조정의 첫 번째 단계는 특정 작업에 적합한 데이터 집합을 선택하는 것입니다. 이 예제에서는 스탠퍼드 대학이 제공하는 감정 분석 데이터 세트를 사용할 것입니다. 이 데이터 세트에는 인터넷 영화 데이터베이스 (IMDb)에서 가져온 5만 개의 온라인 영화 리뷰가 포함되어 있으며, 각 리뷰는 긍정적 또는 부정적으로 레이블이 지정되어 있습니다. 스탠퍼드 대학 웹사이트에서 데이터 세트를 직접 다운로드할 수도 있고, Kaggle에서 노트북을 만들어 작업 결과를 다른 사람들과 비교할 수도 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nimport pandas as pd\n\ndf = pd.read_csv('IMDB Dataset.csv')\ndf.head()\n```\n\n![image](/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_4.png)\n\n이제 이전의 NLP 모델과 달리 BERT와 같은 Transformer 기반 모델은 최소한의 전처리가 필요합니다. 불용어(stop words) 및 구두점을 제거하는 단계는 경우에 따라 오히려 역효과를 낼 수 있습니다. 이러한 요소들은 BERT가 입력 문장을 이해하기 위한 중요한 맥락을 제공하기 때문입니다. 그럼에도 불구하고 텍스트를 검사하여 형식 문제나 원치 않는 문자가 있는지 확인하는 것이 여전히 중요합니다. 전반적으로 IMDb 데이터셋은 꽤 깨끗합니다. 그러나 스크래핑 프로세스에서 남은 몇 가지 흔적, 예를 들면 HTML 태그(`\u003cbr /\u003e`)나 불필요한 공백과 같은 것들을 제거해야 합니다.\n\n```js\n# 줄바꿈 태그(\u003cbr /\u003e) 제거\ndf['review_cleaned'] = df['review'].apply(lambda x: x.replace('\u003cbr /\u003e', ''))\n\n# 불필요한 공백 제거\ndf['review_cleaned'] = df['review_cleaned'].replace('\\s+', ' ', regex=True)\n\n# 첫 번째 리뷰의 72자를 비교하여 클리닝 전/후 출력\nprint('클리닝 전:')\nprint(df.iloc[1]['review'][0:72])\n\nprint('\\n클리닝 후:')\nprint(df.iloc[1]['review_cleaned'][0:72])\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nBefore cleaning:\nA wonderful little production. \u003cbr /\u003e\u003cbr /\u003eThe filming technique is very\n\nAfter cleaning:\nA wonderful little production. The filming technique is very unassuming-\n```\n\n감정 인코딩:\n\n전처리의 마지막 단계는 각 리뷰의 감정을 부정인 경우 0 또는 긍정인 경우 1로 인코딩하는 것입니다. 이러한 레이블은 나중에 미세 조정 프로세스에서 분류 헤드를 훈련하는 데 사용될 것입니다.\n\n```js\ndf['sentiment_encoded'] = df['sentiment'].\\\n    apply(lambda x: 0 if x == 'negative' else 1)\ndf.head()\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_5.png\" /\u003e\n\n## 3.2 — 데이터 파인튜닝에 토큰화 적용하기\n\n전처리된 이후, 데이터를 파인튜닝할 수 있도록 토큰화가 진행됩니다. 이 과정은 리뷰 텍스트를 개별 토큰으로 분리하고, [CLS] 및 [SEP] 특수 토큰을 추가하며, 패딩을 처리합니다. 모델에 적합한 토크나이저를 선택하는 것이 중요합니다. 다양한 언어 모델은 서로 다른 토큰화 단계가 필요하기 때문입니다 (예: GPT는 [CLS] 및 [SEP] 토큰을 요구하지 않습니다). 본 가이드에서는 Hugging Face transformers 라이브러리의 BertTokenizer 클래스를 사용할 것입니다. 이 클래스는 BERT 기반 모델과 함께 사용하도록 설계되었습니다. 토큰화가 작동하는 방식에 대한 더 자세한 설명은 본 시리즈의 Part 1을 참조하세요.\n\ntransformers 라이브러리의 Tokenizer 클래스들은 from_pretrained 메서드를 사용하여 사전 훈련된 토크나이저 모델을 간단히 생성할 수 있는 방법을 제공합니다. 이 기능을 사용하려면: 토크나이저 클래스를 가져와서 인스턴스화하고, from_pretrained 메서드를 호출하고, Hugging Face 모델 저장소에 호스팅된 토크나이저 모델 이름을 나타내는 문자열을 전달하면 됩니다. 또는 토크나이저가 요구하는 어휘 파일이 포함된 디렉토리 경로를 전달할 수도 있습니다. 이 예시에서는 모델 저장소에서 사전 훈련된 토크나이저를 사용할 것입니다. BERT를 다룰 때 주요한 옵션은 네 가지가 있으며, 각각 구글의 사전 훈련 토크나이저 어휘를 사용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- bert-base-uncased — BERT의 작은 버전에 대한 어휘 사전으로 대소문자를 구분하지 않습니다 (예: 토큰 Cat과 cat은 동일하게 처리됩니다)\n- bert-base-cased — BERT의 작은 버전에 대한 어휘 사전으로 대소문자를 구분합니다 (예: 토큰 Cat과 cat은 동일하게 처리되지 않습니다)\n- bert-large-uncased — BERT의 큰 버전에 대한 어휘 사전으로 대소문자를 구분하지 않습니다 (예: 토큰 Cat과 cat은 동일하게 처리됩니다)\n- bert-large-cased — BERT의 큰 버전에 대한 어휘 사전으로 대소문자를 구분합니다 (예: 토큰 Cat과 cat은 동일하게 처리되지 않습니다)\n\nBERT Base와 BERT Large는 동일한 어휘 사전을 사용하므로 bert-base-uncased와 bert-large-uncased 사이에는 차이가 없으며, bert-base-cased와 bert-large-cased 사이에도 차이가 없습니다. 다른 모델의 경우에는 이와 같지 않을 수 있으므로 확실하지 않을 경우 동일한 토크나이저와 모델 크기를 사용하는 것이 가장 좋습니다.\n\n대소문자 사용 여부 선택:\n\n대소문자 사용 여부를 선택하는 것은 데이터셋의 특성에 따라 달라집니다. IMDb 데이터셋은 인터넷 사용자가 작성한 텍스트를 포함하고 있으며 대문자 사용에 일관성이 없는 경우가 있을 수 있습니다. 예를 들어 일부 사용자는 예상대로 대문자를 생략하거나 강조를 위해 대문자를 사용할 수 있습니다. 이러한 이유로 대소문자를 무시하고 bert-base-uncased 토크나이저 모델을 사용하기로 결정했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다른 상황에서는 케이스를 고려하여 성능상의 이점을 볼 수 있습니다. 여기 예제 중 하나는 Named Entity Recognition 작업에서, 즉 사람, 조직, 위치 등을 텍스트에서 식별하는 것이 목표인 경우입니다. 이 경우 대문자의 존재는 단어가 누군가의 이름인지 아니면 장소인지를 식별하는 데 매우 도움이 될 수 있어서, 이 상황에서는 bert-base-cased를 선택하는 것이 더 적절할 수 있습니다.\n\nMarkdown 형식으로 표를 변경합니다.\n\n```js\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nprint(tokenizer)\n```\n\n```js\nBertTokenizer(\n  (name_or_path = \"bert-base-uncased\"),\n  (vocab_size = 30522),\n  (model_max_length = 512),\n  (is_fast = False),\n  (padding_side = \"right\"),\n  (truncation_side = \"right\"),\n  (special_tokens = {\n    unk_token: \"[UNK]\",\n    sep_token: \"[SEP]\",\n    pad_token: \"[PAD]\",\n    cls_token: \"[CLS]\",\n    mask_token: \"[MASK]\",\n  }),\n  (clean_up_tokenization_spaces = True)\n),\n  (added_tokens_decoder = {\n    0: AddedToken(\n      \"[PAD]\",\n      (rstrip = False),\n      (lstrip = False),\n      (single_word = False),\n      (normalized = False),\n      (special = True)\n    ),\n\n    100: AddedToken(\n      \"[UNK]\",\n      (rstrip = False),\n      (lstrip = False),\n      (single_word = False),\n      (normalized = False),\n      (special = True)\n    ),\n\n    101: AddedToken(\n      \"[CLS]\",\n      (rstrip = False),\n      (lstrip = False),\n      (single_word = False),\n      (normalized = False),\n      (special = True)\n    ),\n\n    102: AddedToken(\n      \"[SEP]\",\n      (rstrip = False),\n      (lstrip = False),\n      (single_word = False),\n      (normalized = False),\n      (special = True)\n    ),\n\n    103: AddedToken(\n      \"[MASK]\",\n      (rstrip = False),\n      (lstrip = False),\n      (single_word = False),\n      (normalized = False),\n      (special = True)\n    ),\n  });\n```\n\n인코딩 프로세스: 텍스트를 토큰으로 변환하여 토큰 ID로 변환하기.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로, 토크나이저를 사용하여 정제된 파인 튜닝 데이터를 인코딩할 수 있어요. 이 과정은 각 리뷰를 토큰 ID들의 텐서로 변환할 거예요. 예를 들어, 리뷰인 'I liked this movie'는 다음 단계를 통해 인코딩되어요:\n\n1. 리뷰를 소문자로 변환하기 (우리가 'bert-base-uncased'를 사용하고 있으니)\n\n2. 리뷰를 'bert-base-uncased' 어휘에 따라 개별 토큰으로 분리하기: [`i`, `liked`, `this`, `movie`]\n\n3. BERT가 기대하는 특수 토큰을 추가하기: [`[CLS]`, `i`, `liked`, `this`, `movie`, `[SEP]`]\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n3. 병합된 토큰을 bert-base-uncased 어휘에 따라 해당 토큰 ID로 변환합니다. (예: [CLS] - 101, i - 1045 등)\n\nBertTokenizer 클래스의 encode 메서드는 위 과정을 사용하여 텍스트를 인코딩하고, PyTorch 텐서, Tensorflow 텐서 또는 NumPy 배열로 된 토큰 ID의 텐서를 반환할 수 있습니다. 반환 텐서의 데이터 유형은 return_tensors 인자를 사용하여 지정할 수 있으며, 각각 pt, tf, np 값을 취합니다.\n\n```js\n# 샘플 입력 문장을 인코딩합니다\nsample_sentence = 'I liked this movie'\ntoken_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\nprint(f'Token IDs: {token_ids}')\n\n# 특수 토큰이 추가된 토큰 ID를 토큰으로 다시 변환하여 확인합니다\ntokens = tokenizer.convert_ids_to_tokens(token_ids)\nprint(f'Tokens   : {tokens}')\n```\n\n```js\nToken IDs: [ 101 1045 4669 2023 3185  102]\nTokens   : ['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nTruncation and Padding:\n\nBERT Base 및 BERT Large는 정확히 512토큰의 입력 시퀀스를 처리하기 위해 설계되었습니다. 그러나 입력 시퀀스가 이 제한에 맞지 않는 경우 어떻게 해야 할까요? 그 답은 Truncation과 Padding입니다! Truncation은 특정 길이 이상의 어떠한 토큰도 간단히 제거하여 토큰 수를 줄입니다. encode 메서드에서 truncation을 True로 설정하고 모든 인코딩된 시퀀스에 길이 제한을 부여할 max_length 인수를 지정할 수 있습니다. 이 데이터 세트의 여러 항목은 512토큰 제한을 초과하므로 여기서 max_length 매개변수가 모든 리뷰에서 가능한 가장 많은 텍스트를 추출하기 위해 512로 설정되었습니다. 리뷰가 512토큰을 초과하는 경우가 없다면 max_length 매개변수를 설정하지 않고 남기면 모델의 최대 길이로 기본 설정됩니다. 또는 Feine-Tuning 중에 학습 시간을 줄이기 위해 512보다 작은 최대 길이를 여전히 강제로 지정할 수 있지만 모델 성능의 손실이 발생합니다. (대부분이 그렇지만) 512토큰보다 짧은 리뷰의 경우 패딩 토큰이 추가되어 인코딩된 리뷰가 512토큰으로 확장됩니다. 이렇게 설정하는 방법에 대해서는 패딩 매개변수를 max_length로 설정하면 됩니다. encode 매서드에 대한 자세한 내용은 Hugging Face 문서를 참조하십시오 [10].\n\n```js\nreview = df[\"review_cleaned\"].iloc[0];\n\ntoken_ids = tokenizer.encode(\n  review,\n  (max_length = 512),\n  (padding = \"max_length\"),\n  (truncation = True),\n  (return_tensors = \"pt\")\n);\n\nprint(token_ids);\n```\n\n```js\ntensor([\n  [\n    101,\n    2028,\n    1997,\n    1996,\n    2060,\n    15814,\n    2038,\n    3855,\n    2008,\n    2044,\n    3666,\n    2074,\n    1015,\n    11472,\n    2792,\n    2017,\n    1005,\n    2222,\n    2022,\n    13322,\n\n    ...0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n    0,\n  ],\n]);\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAttention Mask을 encode_plus와 함께 사용하기:\n\n위의 예제는 데이터셋에서 첫 번째 리뷰의 인코딩을 보여줍니다. 이 리뷰에는 119개의 패딩 토큰이 포함되어 있습니다. 현재 상태로 fine-tuning에 사용된다면, BERT는 패딩 토큰에 주의를 기울일 수 있어 성능이 떨어질 수 있습니다. 이를 해결하기 위해 입력에서 특정 토큰(이 경우 패딩 토큰)을 무시하도록 BERT에 지시하는 attention mask를 적용할 수 있습니다. 위의 코드를 수정하여 encode_plus 메소드를 사용하면 이러한 attention mask를 생성할 수 있습니다. encode_plus 메소드는 표준 encode 메소드 대신 사용되며 Batch Encoder(허깅페이스의 용어)라고 불리는 딕셔너리가 반환됩니다. 이 딕셔너리는 다음과 같은 key를 포함합니다:\n\n- input_ids — 표준 encode 메소드로부터 반환된 동일한 토큰 ID\n- token_type_ids — 문장 A (id = 0)와 문장 B (id = 1)를 구분하는 데 사용되는 세그먼트 ID(예: Next Sentence Prediction과 같은 문장 쌍 작업)\n- attention_mask — 특정 토큰이 attention 과정에서 무시되어야 하는지 (0을 의미) 아니면 무시되어서는 안 되는지 (1을 의미) 나타내는 0과 1의 리스트\n\n```js\nreview = df[\"review_cleaned\"].iloc[0];\n\nbatch_encoder = tokenizer.encode_plus(\n  review,\n  (max_length = 512),\n  (padding = \"max_length\"),\n  (truncation = True),\n  (return_tensors = \"pt\")\n);\n\nprint(\"Batch encoder keys:\");\nprint(batch_encoder.keys());\n\nprint(\"\\nAttention mask:\");\nprint(batch_encoder[\"attention_mask\"]);\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n벡터 인코더 키:\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\n주의 마스크:\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n\n                                      ...\n\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])\n\n모든 리뷰 인코딩:\n\n토큰화 단계의 마지막 단계는 데이터 세트의 모든 리뷰를 인코딩하고 토큰 ID 및 해당하는 주의 마스크를 텐서로 저장하는 것입니다.\n\n```python\nimport torch\n\n토큰 ID = []\n주의 마스크 = []\n\n# 각 리뷰 인코딩\nfor review in df['review_cleaned']:\n    batch_encoder = tokenizer.encode_plus(\n        review,\n        max_length = 512,\n        padding = 'max_length',\n        truncation = True,\n        return_tensors = 'pt')\n\n    token_ids.append(batch_encoder['input_ids'])\n    attention_masks.append(batch_encoder['attention_mask'])\n\n# 토큰 ID 및 주의 마스크 목록을 PyTorch 텐서로 변환\n토큰 ID = torch.cat(토큰 ID, dim=0)\n주의 마스크 = torch.cat(주의 마스크, dim=0)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3.3 — 트레인 및 검증 데이터로더 생성하기\n\n이제 각 리뷰가 인코딩되었으므로 데이터를 훈련 세트와 검증 세트로 분할할 수 있습니다. 검증 세트는 피니튜닝 프로세스의 효과를 평가하는 데 사용될 것이며, 프로세스 중에 성능을 지속적으로 모니터링할 수 있도록 합니다. 에폭이 진행됨에 따라 손실이 감소하고 모델 정확도가 증가할 것으로 예상됩니다. 에폭이란 트레인 데이터를 전부 한 번 통과하는 것을 의미합니다. BERT 저자들은 플레튜닝을 위해 2~4 에폭을 권장하며, 이는 분류 헤더가 모든 리뷰를 2~4번 보게 됨을 의미합니다.\n\n데이터를 분할하기 위해, SciKit-Learn의 model_selection 패키지에서 제공하는 train_test_split 함수를 사용할 수 있습니다. 이 함수는 분할할 데이터셋, 테스트 세트(또는 우리의 경우에는 검증 세트)로 할당할 아이템의 비율, 그리고 데이터를 무작위로 섞을지에 대한 옵션 인수를 필요로 합니다. 재현성을 위해 무작위로 섞는 매개변수를 False로 설정할 것입니다. 테스트 크기에는 0.1이라는 작은 값(10%에 해당)을 선택할 것입니다. 모델을 평가하고 수행을 정확히 파악하기 위해 충분한 데이터를 사용하는 것과 모델을 훈련하고 성능을 향상시키는 데 충분한 데이터를 유지하는 것 사이에 균형을 유지하는 것이 중요합니다. 따라서 0.1과 같은 작은 값이 종종 선호됩니다. 토큰 ID, 어텐션 마스크, 라벨을 분리한 후, 훈련 및 검증 텐서를 PyTorch TensorDatasets에 함께 그룹화할 수 있습니다. 그런 다음 이 TensorDatasets를 배치로 분할하여 훈련 및 검증용 PyTorch DataLoader 클래스를 생성할 수 있습니다. BERT 논문에서는 16 또는 32의 배치 크기를 권장합니다(즉, 모델에 16개의 리뷰와 해당 감정 라벨을 제시하고 분류 헤더에서 가중치와 바이어스를 다시 계산하기 전에). DataLoader를 사용하면 병렬 처리를 활용하여 피니튜닝 프로세스 중에 데이터를 효율적으로 모델로 로드할 수 있으며, 다중 CPU 코어를 활용할 수 있습니다.\n\n```js\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\nval_size = 0.1\n\n# 토큰 ID 분할\ntrain_ids, val_ids = train_test_split(\n                        token_ids,\n                        test_size=val_size,\n                        shuffle=False)\n\n# 어텐션 마스크 분할\ntrain_masks, val_masks = train_test_split(\n                            attention_masks,\n                            test_size=val_size,\n                            shuffle=False)\n\n# 라벨 분할\nlabels = torch.tensor(df['sentiment_encoded'].values)\ntrain_labels, val_labels = train_test_split(\n                                labels,\n                                test_size=val_size,\n                                shuffle=False)\n\n# 데이터로더 생성\ntrain_data = TensorDataset(train_ids, train_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\nval_data = TensorDataset(val_ids, val_masks, val_labels)\nval_dataloader = DataLoader(val_data, batch_size=16)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3.4 — BERT 모델 인스턴스화\n\n다음 단계는 미세 조정할 사전 훈련된 BERT 모델을 로드하는 것입니다. 우리는 이전과 마찬가지로 Hugging Face 모델 저장소에서 모델을 가져올 수 있습니다. Hugging Face에는 이미 분류 헤드가 연결된 여러 버전의 BERT가 있어 이 프로세스가 매우 편리합니다. 미리 구성된 분류 헤드를 가진 일부 모델의 예는 다음과 같습니다:\n\n- BertForMaskedLM\n- BertForNextSentencePrediction\n- BertForSequenceClassification\n- BertForMultipleChoice\n- BertForTokenClassification\n- BertForQuestionAnswering\n\n물론, PyTorch 또는 Tensorflow에서 headless BERT 모델을 가져와 직접 분류 헤드를 만들 수도 있습니다. 그러나 우리의 경우에는 이미 필요한 선형 레이어를 포함하는 BertForSequenceClassification 모델을 가져와 사용할 수 있습니다. 이 선형 레이어는 무작위 가중치와 바이어스로 초기화되며, 미세 조정 중에 훈련될 것입니다. BERT Base는 768개의 임베딩 차원을 사용하므로, 숨겨진 레이어에는 모델의 최종 인코더 블록에 연결된 768개의 뉴런이 포함되어 있습니다. 출력 뉴런의 수는 num_labels 인수에 의해 결정되며, 고유한 감정 레이블의 수와 일치합니다. IMDb 데이터셋은 긍정과 부정만 포함하므로 num_labels 인수가 2로 설정됩니다. 중립 또는 혼합과 같은 레이블을 포함하여 보다 복잡한 감정 분석을 수행하려면 num_labels 값을 쉽게 조절할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ntransformers 패키지에서 BertForSequenceClassification을 가져옵니다.\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2)\n```\n\n## 3.5 — Optimizer, Loss Function, 및 Scheduler 인스턴스화\n\nOptimizer:\n\n분류 head가 학습 데이터의 일괄 처리를 만나면, 선형 레이어의 가중치와 바이어스를 업데이트하여 그 입력에 대한 모델 성능을 향상시킵니다. 여러 배치와 여러 epoch 동안, 이러한 가중치와 바이어스가 최적값으로 수렴하도록 하는 것이 목표입니다. 각 가중치와 바이어스에 필요한 변경 사항을 계산하기 위해 옵티마이저가 필요하며, PyTorch의 `optim` 패키지에서 가져올 수 있습니다. Hugging Face는 자신들의 예제에서 AdamW 옵티마이저를 사용하므로, 여기서도 이 옵티마이저를 사용할 것입니다 [13].\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n손실 함수:\n\n옵티마이저는 분류 헤드의 가중치 및 편향에 대한 변경이 손실 함수에 어떤 영향을 미칠지 결정함으로써 작동합니다. 이 손실 함수라 불리는 점수 함수에 대한 손실은 PyTorch의 nn 패키지에서 쉽게 가져올 수 있습니다. 언어 모델은 일반적으로 교차 엔트로피 손실 함수(음의 로그 우도 함수라고도 함)를 사용하며, 따라서 여기에서는 이 손실 함수를 사용할 것입니다.\n\n스케줄러:\n\n학습 속도라 불리는 매개변수는 분류 헤드의 가중치와 편향에 대한 변경의 크기를 결정하는 데 사용됩니다. 초기 배치와 에포크에서는 무작위로 초기화된 매개변수가 상당한 조정이 필요할 수 있으므로 큰 변경이 유용할 수 있습니다. 그러나 학습이 진행됨에 따라 가중치와 편향이 향상되면서 큰 변경이 역효과적일 수 있습니다. 스케줄러는 학습 과정이 계속되는 동안 학습 속도를 점차 감소시켜 각 가중치 및 편향에 대한 각 최적화 단계에서 발생하는 변경 크기를 줄이도록 설계되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom transformers import get_linear_schedule_with_warmup\n\nEPOCHS = 2\n\n# Optimizer\noptimizer = AdamW(model.parameters())\n\n# Loss function\nloss_function = nn.CrossEntropyLoss()\n\n# Scheduler\nnum_training_steps = EPOCHS * len(train_dataloader)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps)\n```\n\n## 3.6 — Fine-Tuning Loop\n\nCUDA를 사용하여 GPU 활용:\n\nNVIDIA에서 만든 CUDA(Compute Unified Device Architecture)는 과학 및 공학 분야의 응용 프로그램 성능을 향상시키기 위한 컴퓨팅 플랫폼입니다 [14]. PyTorch의 cuda 패키지를 사용하면 Python에서 CUDA 플랫폼을 활용하여 머신 러닝 모델을 훈련할 때 GPU를 사용할 수 있습니다. torch.cuda.is_available 명령을 사용하여 GPU의 가용성을 확인할 수 있습니다. GPU가 없는 경우 코드는 가속 계산을 위해 그래픽 처리 장치 (GPU)를 사용할 수 없도록 기본 설정됩니다. 이후의 코드 스니펫에서는 PyTorch Tensor.to 메서드를 사용하여 텐서(모델 가중치 및 편향 등이 포함됨)를 더 빠른 계산을 위해 GPU로 이동합니다. 장치가 cpu로 설정된 경우 텐서가 이동되지 않고 코드에 영향을 미치지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# GPU 사용 가능 여부 확인하여 빠른 학습 시간을 위한 준비\n\nif torch.cuda.is_available():\ndevice = torch.device('cuda:0')\nelse:\ndevice = torch.device('cpu')\n\n학습 프로세스는 두 개의 for 루프를 통해 이루어집니다: 각 epoch마다 프로세스를 반복하는 외부 루프(모델이 모든 학습 데이터를 여러 번 보게하는 역할)와 각 배치마다 손실 계산 및 최적화 단계를 반복하는 내부 루프가 있습니다. 학습 루프를 설명하기 위해 아래 단계를 고려해 보세요. 학습 루프의 코드는 Chris McCormick과 Nick Ryan의 훌륭한 블로그 글[15]에서 적용되었으며 매우 추천합니다.\n\n각 epoch에 대해:\n\n1. 모델을 train 모드로 변경합니다. 모델 객체의 train 메서드를 사용하여 모델이 평가 모드일 때와는 다르게 작동하도록 합니다. 특히 batchnorm과 dropout 레이어와 함께 작업할 때 유용합니다. 이전에 BertForSequenceClassification 클래스의 소스 코드를 살펴봤다면, 분류 헤드에 실제로 dropout 레이어가 포함되어 있는 것을 보았을 겁니다. 따라서 fine-tuning 시에 training 및 evaluation 모드를 올바르게 구분해야 합니다. 이러한 종류의 레이어는 학습 중에만 활성화되어야 하며 추론 중에는 활성화되지 않아야 합니다. 따라서 학습과 추론을 위해 서로 다른 모드로 전환할 수 있는 기능은 유용한 기능입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2. 에포크 시작 시 훈련 손실을 0으로 설정하세요. 이것은 이후 에포크에서 모델의 훈련 데이터 손실을 추적하는 데 사용됩니다. 훈련이 성공적이라면 각 에포크마다 손실이 감소해야 합니다.\n\n각 배치에 대해:\n\nBERT 저자들의 권장에 따라, 각 에포크의 훈련 데이터를 배치로 나누세요. 각 배치마다 훈련 프로세스를 반복하세요.\n\n3. 가능한 경우 토큰 ID, 어텐션 마스크 및 레이블을 GPU로 이동하여 처리 속도를 높이세요. 그렇지 않으면 이러한 데이터는 CPU에 유지됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n4. 이 루프의 이전 반복에서 계산된 그래디언트를 재설정하기 위해 zero_grad 메서드를 호출하세요. PyTorch에서 기본 동작이 아닌 이유가 명확하지 않을 수 있지만, 이는 Recurrent Neural Networks와 같은 모델이 반복 사이에 그래디언트를 재설정해선 안 되는 이유로 제안됩니다.\n\n5. 배치를 모델에 전달하여 로짓(현재의 분류기 가중치와 편향에 기반한 예측)과 손실을 계산하세요.\n\n6. 에폭별 총 손실을 증가시키세요. 모델에서 손실이 PyTorch 텐서로 반환되므로 `item` 메서드를 사용하여 부동 소수점 값을 추출하세요.\n\n7. 모델에 역전파를 수행하고 분류기 헤드를 통해 손실을 전파하세요. 이를 통해 모델은 배치에 대한 성능을 향상시키기 위해 가중치와 편향을 조정해야 하는지를 결정할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n8. 모델이 폭발하는 그래디언트 문제를 겪지 않도록 그래디언트를 1.0보다 크게 만들지 마세요.\n\n9. 역전파에 따라 오차 표면 방향으로 옵티마이저를 호출하여 한 단계씩 진행하세요.\n\n각 배치 훈련 후:\n\n10. 에포크에서의 평균 손실과 소요 시간을 계산하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nfor epoch in range(0, EPOCHS):\n\n    model.train()\n    training_loss = 0\n\n    for batch in train_dataloader:\n\n        batch_token_ids = batch[0].to(device)\n        batch_attention_mask = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        loss, logits = model(\n            batch_token_ids,\n            token_type_ids = None,\n            attention_mask=batch_attention_mask,\n            labels=batch_labels,\n            return_dict=False)\n\n        training_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    average_train_loss = training_loss / len(train_dataloader)\n\n외부 루프 내에서 검증 단계가 수행되므로 각 epoch마다 평균 검증 손실을 계산합니다. epoch 숫자가 증가함에 따라 검증 손실이 감소하고 분류기 정확도가 증가할 것으로 기대됩니다. 검증 프로세스 단계는 아래에 설명되어 있습니다.\n\n에포크의 검증 단계:\n\n11. evaluation 메서드를 사용하여 모델을 평가 모드로 전환합니다. 이렇게 하면 드롭아웃 레이어가 비활성화됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n12. 검증 손실을 0으로 설정하세요. 이 값은 후속 에포크에서 모델의 검증 데이터에 대한 손실을 추적하는 데 사용됩니다. 학습이 성공적이었다면 손실은 각 에포크마다 감소해야 합니다.\n\n13. 검증 데이터를 배치로 나누세요.\n\n각 배치에 대해:\n\n14. 사용 가능한 경우 토큰 ID, 어텐션 마스크 및 레이블을 GPU로 이동하여 처리 속도를 높이세요. 그렇지 않으면 이러한 값들은 CPU에 유지됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n15. 여기서는 최적화 단계를 수행하지 않고 추론만 할 것이기 때문에 모델이 그라디언트를 계산하지 않도록 no_grad 메서드를 호출하세요.\n\n16. 배치를 모델에 전달하여 로짓(현재 분류기의 가중치와 편향을 기반으로 한 예측)와 손실을 계산하세요.\n\n17. 모델에서 로짓과 레이블을 추출하여 CPU로 이동하세요 (이미 CPU에 있지 않은 경우).\n\n18. 손실을 증가시키고 검증 데이터로더의 실제 레이블에 기반하여 정확도를 계산하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n19. 손실 및 정확도의 평균을 계산하세요.\n\n```js\n    model.eval()\n    val_loss = 0\n    val_accuracy = 0\n\n    for batch in val_dataloader:\n\n        batch_token_ids = batch[0].to(device)\n        batch_attention_mask = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            (loss, logits) = model(\n                batch_token_ids,\n                attention_mask = batch_attention_mask,\n                labels = batch_labels,\n                token_type_ids = None,\n                return_dict=False)\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = batch_labels.to('cpu').numpy()\n        val_loss += loss.item()\n        val_accuracy += calculate_accuracy(logits, label_ids)\n\n    average_val_accuracy = val_accuracy / len(val_dataloader)\n```\n\n위의 코드 스니펫의 끝에서 calculate_accuracy 함수를 사용하고 있지만 아직 정의하지 않았으니, 이제 정의해 보겠습니다. 모델의 검증 세트에서의 정확도는 옳은 예측의 비율로 주어집니다. 따라서 모델에 의해 생성된 로짓 값, 즉 변수 logits에 저장된 값을 사용할 수 있고, 이를 NumPy의 argmax 함수를 이용할 수 있습니다. argmax 함수는 배열에서 가장 큰 요소의 인덱스를 반환합니다. 텍스트 I liked this movie에 대한 로짓이 [0.08, 0.92]인 경우, 0.08은 텍스트가 부정일 확률을 나타내고 0.92는 텍스트가 긍정일 확률을 나타내므로 argmax 함수는 인덱스 1을 반환할 것입니다. 모델은 텍스트가 부정보다 긍정일 가능성이 더 높다고 판단합니다. 그런 다음 해당 레이블을 Section 3.3(19번째 줄)에서 이미 인코딩한 labels 텐서와 비교하면 됩니다. 로짓 변수에는 배치(총 16개)의 모든 리뷰에 대한 긍정 및 부정 확률 값이 포함되므로 모델의 정확도는 최대 16개의 옳은 예측 중 계산됩니다. 위의 코드는 val_accuracy 변수가 각 정확도 점수를 기록하고, 검증을 마친 후에 모델의 검증 데이터에 대한 평균 정확도를 결정하기 위해 나누는 것을 보여줍니다.\n\n```js\ndef calculate_accuracy(preds, labels):\n    \"\"\" 모델 예측과 실제 레이블의 정확도를 계산합니다.\n\n    매개변수:\n        preds (np.array): 모델의 예측된 레이블\n        labels (np.array): 실제 레이블\n\n    반환값:\n        정확도 (float): 올바른 예측의 백분율로 정확도가 반환됩니다.\n    \"\"\"\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n    return accuracy\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3.7 — 완벽한 파인튜닝 파이프라인\n\n그렇게 해서 우리는 파인튜닝의 설명을 마쳤습니다! 아래 코드는 위의 모든 것을 하나의 재사용 가능한 클래스로 가져와서 BERT를 사용하는 모든 NLP 작업에 사용할 수 있습니다. 데이터 전처리 단계는 작업에 따라 다르기 때문에 이는 파인튜닝 클래스 밖으로 뺐습니다.\n\nIMDb 데이터셋을 이용한 감정 분석을 위한 데이터 전처리 함수:\n\n```python\ndef preprocess_dataset(path):\n    \"\"\" 불필요한 문자를 제거하고 감정 레이블을 인코딩합니다.\n\n    필요한 전처리 유형은 데이터셋에 따라 변경됩니다. IMDb 데이터셋의 경우, 리뷰 텍스트에는 스크래핑 과정에서 남아 있는 HTML 줄 바꿈 태그 (\u003cbr/\u003e)와 일부 불필요한 공백이 있습니다. 이를 제거합니다. 마지막으로, \"부정\"에 대한 감정 값을 0으로, \"긍정\"에 대한 감정 값을 1로 인코딩합니다. 이 메서드는 데이터셋 파일에 \"review\" 및 \"sentiment\" 헤더가 포함되어 있다고 가정합니다.\n\n    매개변수:\n        path (str): 감정 분석 데이터셋을 포함하는 데이터셋 파일의 경로입니다. 파일 구조는 다음과 같아야 합니다:\n            리뷰 텍스트를 담은 \"review\" 열과, ground truth 레이블을 담은 \"sentiment\" 열이 하나씩 있는 한 열인 파일입니다.\n            레이블 옵션은 \"부정\"과 \"긍정\"이어야 합니다.\n\n    반환:\n        df_dataset (pd.DataFrame): self.dataset 경로에서 로드한 원시 데이터가 있는 DataFrame입니다. \"review\"와 \"sentiment\" 열 외에도 다음이 포함됩니다:\n            - review_cleaned: \"review\" 열의 사본이고 HTML 줄 바꿈 태그와 불필요한 공백이 제거된 컬럼\n            - sentiment_encoded: \"sentiment\" 열의 사본이며 \"부정\" 값을 0으로 매핑하고 \"긍정\" 값을 1로 매핑한 컬럼\n\n\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n작업에 중립적인 파인튜닝 파이프라인 클래스:\n\n\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import (\n    BertForSequenceClassification,\n    BertTokenizer,\n    get_linear_schedule_with_warmup)\n\n\nclass FineTuningPipeline:\n\n    def __init__(\n            self,\n            dataset,\n            tokenizer,\n            model,\n            optimizer,\n            loss_function = nn.CrossEntropyLoss(),\n            val_size = 0.1,\n            epochs = 4,\n            seed = 42):\n\n        self.df_dataset = dataset\n        self.tokenizer = tokenizer\n        self.model = model\n        self.optimizer = optimizer\n        self.loss_function = loss_function\n        self.val_size = val_size\n        self.epochs = epochs\n        self.seed = seed\n\n        # Check if GPU is available for faster training time\n        if torch.cuda.is_available():\n            self.device = torch.device('cuda:0')\n        else:\n            self.device = torch.device('cpu')\n\n        # Perform fine-tuning\n        self.model.to(self.device)\n        self.set_seeds()\n        self.token_ids, self.attention_masks = self.tokenize_dataset()\n        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n        self.scheduler = self.create_scheduler()\n        self.fine_tune()\n\n    def tokenize(self, text):\n        \"\"\" Tokenize input text and return the token IDs and attention mask.\n\n        Tokenize an input string, setting a maximum length of 512 tokens.\n        Sequences with more than 512 tokens will be truncated to this limit,\n        and sequences with less than 512 tokens will be supplemented with [PAD]\n        tokens to bring them up to this limit. The datatype of the returned\n        tensors will be the PyTorch tensor format. These return values are\n        tensors of size 1 x max_length where max_length is the maximum number\n        of tokens per input sequence (512 for BERT).\n\n...\n\n\n감정 분석을 위한 클래스 사용 예 (IMDb 데이터셋):\n\n# 매개변수 초기화\ndataset = preprocess_dataset('IMDB Dataset Very Small.csv')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\nnum_labels=2)\noptimizer = AdamW(model.parameters())\n\n# 클래스를 사용하여 모델을 파인튜닝\nfine_tuned_model = FineTuningPipeline(\n    dataset=dataset,\n    tokenizer=tokenizer,\n    model=model,\n    optimizer=optimizer,\n    val_size=0.1,\n    epochs=2,\n    seed=42\n)\n\n# 유효성 검사 데이터셋을 사용하여 일부 예측 수행\nmodel.predict(model.val_dataloader)\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 4 — 결론\n\n이 글에서는 BERT의 여러 측면을 탐색했습니다. BERT의 창시 시점의 배경, 모델 아키텍처의 자세한 분석 및 감성 분석을 사용하여 시업 무관한 미세 조정 파이프라인 작성을 포함했습니다. BERT는 가장 초기의 LLM 중 하나임에도 불구하고, 오늘날에도 여전히 중요하며 연구 및 산업 분야에서 응용 프로그램을 발전시키고 있습니다. BERT를 이해하고 NLP 분야에 미치는 영향을 이해하면 최신 고품질 모델을 다루는 데 튼실한 기반을 다질 수 있습니다. 미세 조정 및 사전 훈련이 LLM의 지배적 패러다임으로 유지되고 있으므로, 이 글이 여러분의 프로젝트에 적용해 가며 가치 있는 통찰을 제공했기를 바랍니다!\n\n# 5 — 추가 자료\n\n[1] J. Devlin, M. Chang, K. Lee, and K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019), North American Chapter of the Association for Computational Linguistics\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is All You Need (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)\n\n[3] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, Deep contextualized word representations (2018), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\n\n[4] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever (2018), Improving Language Understanding by Generative Pre-Training,\n\n[5] Hugging Face, Fine-Tuned BERT Models (2024), HuggingFace.co\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 논문 및 참고 자료 목록\n\n- M. Schuster 및 K. K. Paliwal, Bidirectional recurrent neural networks (1997), IEEE Signal Processing 트랜잭션 45\n\n- Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba 및 S. Fidler, Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books (2015), 2015 IEEE International Conference on Computer Vision (ICCV)\n\n- L. W. Taylor, “Cloze Procedure”: A New Tool for Measuring Readability (1953), Journalism Quarterly, 30(4), 415–433.\n\n- Hugging Face, Pre-trained Tokenizers (2024) HuggingFace.co\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n[10] Hugging Face, Pre-trained Tokenizer Encode Method (2024) HuggingFace.co\n\n[11] T. Vo, PyTorch DataLoader: Features, Benefits, and How to Use it (2023) SaturnCloud.io\n\n[12] Hugging Face, Modelling BERT (2024) GitHub.com\n\n[13] Hugging Face, Run Glue, GitHub.com\n\n\u003c!-- ui-station 사각형 --\u003e\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n[14] NVIDIA, CUDA Zone (2024), Developer.NVIDIA.com\n\n[15] C. McCormick and N. Ryan, BERT Fine-tuning (2019), McCormickML.com\n```\n","ogImage":{"url":"/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_0.png"},"coverImage":"/assets/img/2024-05-20-ACompleteGuidetoBERTwithCode_0.png","tag":["Tech"],"readingTime":54}],"page":"78","totalPageCount":119,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"78"},"buildId":"wOkGEDZCvEs3S_XaNsdwr","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>