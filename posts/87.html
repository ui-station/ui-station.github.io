<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/87" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/87" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_buildManifest.js" defer=""></script><script src="/_next/static/-dPCbnM2yhdKNgXe92VJV/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="탈취 가능한 사이버 보안 취약점 약 1,000여 개가 MITRE, NIST에서 놓칠 수도 있지만 중국 또는 러시아는 놓치지 않았을 수도 있어요" href="/post/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="탈취 가능한 사이버 보안 취약점 약 1,000여 개가 MITRE, NIST에서 놓칠 수도 있지만 중국 또는 러시아는 놓치지 않았을 수도 있어요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="탈취 가능한 사이버 보안 취약점 약 1,000여 개가 MITRE, NIST에서 놓칠 수도 있지만 중국 또는 러시아는 놓치지 않았을 수도 있어요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">탈취 가능한 사이버 보안 취약점 약 1,000여 개가 MITRE, NIST에서 놓칠 수도 있지만 중국 또는 러시아는 놓치지 않았을 수도 있어요</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="중간 과정 프롬프트를 A부터 Z까지 최상위 사진 용어로 변환하기" href="/post/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="중간 과정 프롬프트를 A부터 Z까지 최상위 사진 용어로 변환하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="중간 과정 프롬프트를 A부터 Z까지 최상위 사진 용어로 변환하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">중간 과정 프롬프트를 A부터 Z까지 최상위 사진 용어로 변환하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">32<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인공지능 회사들에 속지 않기 위해 예술가들이 작품에 함정을 설치하고 있어요" href="/post/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인공지능 회사들에 속지 않기 위해 예술가들이 작품에 함정을 설치하고 있어요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인공지능 회사들에 속지 않기 위해 예술가들이 작품에 함정을 설치하고 있어요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">인공지능 회사들에 속지 않기 위해 예술가들이 작품에 함정을 설치하고 있어요</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="안정적인 확산에 대한 설명" href="/post/2024-05-18-StableDiffusionExplained"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="안정적인 확산에 대한 설명" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-StableDiffusionExplained_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="안정적인 확산에 대한 설명" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">안정적인 확산에 대한 설명</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인간의 시선 AI가 창의력에 미치는 영향 탐구" href="/post/2024-05-18-TheHumanLensExploringAIsImpactonCreativity"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간의 시선 AI가 창의력에 미치는 영향 탐구" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간의 시선 AI가 창의력에 미치는 영향 탐구" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">인간의 시선 AI가 창의력에 미치는 영향 탐구</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법" href="/post/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드" href="/post/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기" href="/post/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">19<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기" href="/post/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기" href="/post/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">39<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/81">81</a><a class="link" href="/posts/82">82</a><a class="link" href="/posts/83">83</a><a class="link" href="/posts/84">84</a><a class="link" href="/posts/85">85</a><a class="link" href="/posts/86">86</a><a class="link posts_-active__YVJEi" href="/posts/87">87</a><a class="link" href="/posts/88">88</a><a class="link" href="/posts/89">89</a><a class="link" href="/posts/90">90</a><a class="link" href="/posts/91">91</a><a class="link" href="/posts/92">92</a><a class="link" href="/posts/93">93</a><a class="link" href="/posts/94">94</a><a class="link" href="/posts/95">95</a><a class="link" href="/posts/96">96</a><a class="link" href="/posts/97">97</a><a class="link" href="/posts/98">98</a><a class="link" href="/posts/99">99</a><a class="link" href="/posts/100">100</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"탈취 가능한 사이버 보안 취약점 약 1,000여 개가 MITRE, NIST에서 놓칠 수도 있지만 중국 또는 러시아는 놓치지 않았을 수도 있어요","description":"","date":"2024-05-18 20:56","slug":"2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt","content":"\n# 우리는 누구인가요?\n\nA.R.P. Syndicate은 고객이 대상, 취약점 및 위협에 대한 정보를 집중 및 탐색하는 데 도움을 주는 글로벌 사이버보안 인텔리전스 및 연구 회사입니다.\n\n![](/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png)\n\n# Exploit Observer의 VEDAS란 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nVEDAS는 취약점 및 악용 데이터 집계 시스템의 약자입니다. Exploit Observer의 기술이며, 우수한 취약성 + 악용 크롤링 및 상관 기능으로 유명합니다.\n\n이 인텔리전스는 현재 초기 단계이며 실험 단계이기 때문에 거짓 긍정으로 조작될 수 있습니다. 그러나 모든 AI 시스템과 마찬가지로 시간이 지남에 따라 진화하고 있으며, 우리는 그 미래에 대해 매우 희망적입니다.\n\n### 왜 거짓 긍정을 출력하는 시스템을 믿어야 할까요?\n\n우리의 주장은 소중하게만 믿어지면 안 됩니다. 이와 관련하여, 어떤 자동 보안 시스템의 재앙적인 실패는 수백 개의 거짓 긍정 (1형 오류)이 아닌 몇 개의 거짓 부정 (2형 오류) 때문에 발생합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n사이버 보안 주변의 자동화 시스템 대부분은 잘못된 긍정 사례를 제거하는 데 너무 집중하고 있습니다. 이 행동은 결과적으로 잘못된 부정 사례의 수가 증가하게 됩니다. 우리의 목표는 항상 잘못된 긍정 사례보다 잘못된 부정 사례를 제거하는 것입니다.\n\n항상 독립적인 연구자들이 저희의 작업을 테스트, 확인하고 비평하도록 권장합니다. 어떠한 불일치 사항이라도 https://github.com/ARPSyndicate/puncia/issues에서 보고할 수 있습니다.\n\n# MITRE, NIST \u0026 CNA 파트너들이 놓칠 수도 있는 취약점을 찾아보세요\n\n취약점 목록은 Exploit Observer의 API를 통해 접근할 수 있습니다 —\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- CVE가 없는 BDU: https://api.exploit.observer/russia/noncve\n- CVE가 없는 CNVD / CNNVD: https://api.exploit.observer/china/noncve\n\n해당 VEDAS 클러스터의 URL 목록을 반환합니다. VEDAS 클러스터는 CVE 식별자와는 달리 단일 또는 일정한 취약성에 매핑되지 않는 VEDAS 식별자로 표시됩니다. 이 클러스터는 자체 조절되며 더 많은 데이터가 집계될 때 매우 발전할 가능성이 높습니다.\n\n# CVE가 무엇이든 놓치지 않는 방법은 무엇인가요?\n\nCVE 생태계에는 가치 있는 정보를 생성하는 370개 이상의 CNA 파트너가 하루 24시간 열심히 입력을 생성합니다. CVE 데이터베이스에서는 매일 많은 변화가 일어나고 시간이 지날수록 더욱 개선됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그럼에도 불구하고, 그것은 우리에게도 여전히 수수께끼인데, 실제 영향이 있는 많은 취약점들이 공개적으로 접근 가능하지만 CVE가 할당되지 않은 것이 많습니다.\n\n# 중국과 러시아는 훨씬 놓쳤습니다. 누구나 그들보다 더 우려해야 하는 이유가 있을까요?\n\nCVE 생태계는 실제로 개선되고 있습니다. 하지만 아직 최상이 아닙니다! 우리는 중국과 러시아가 아무것도 놓치지 않는다고 믿습니다. CVE가 놓친 것들을 커버하기 위해 더 많은 보호를 제공하기 위해서 요구됩니다.\n\n게다가, CNITSEC \u0026 FSTEC가 좁은 초점을 가지고 있음에도 불구하고, 그들은 국내 및 전 세계 취약성 생태계에 대한 이해력, 통제 및 커버리지가 외부 세계에 공개하는 만큼 크게 가지고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 글로벌 보안 팀들은 여전히 CNVD/CNNVD/BDU보다 CVE에 더 의존하고 있습니다. 따라서, 각 능력 있는 보안 팀들에게는 우려스러운 문제이며, 이 문제를 해결하기 위해 상업적 및 무료 취약점 및 공격 데이터베이스를 최대한 통합할 필요가 있습니다.\n\n# Exploit Observer가 이러한 문제 해결에 어떻게 도움이 될까요?\n\nExploit Observer는 인터넷에서의 취약점/공격 데이터를 집계하고 해석합니다. 결과적으로, 이는 세계 최대의 공격 및 취약점 인텔리전스 데이터베이스로 발전하였으며 모든 사용자에게 무료로 제공됩니다.\n\n![이미지](/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n어떤 보안팀도 이것을 사용하여 새로운 취약성과 악용을 모니터링할 뿐만 아니라 패치 및 악용을 우선순위로 정하는 데 쉽게 활용할 수 있습니다.\n\n사실, 우리의 평가 알고리즘은 CVSS, EPSS 및 Vulners AI Score와 같은 표준과 완전히 독립적입니다. 우리의 기준은 심각도를 고려하지 않습니다. 고려하는 유일한 요소는 \"최소한의 지식으로 상대방이 특정 취약성을 얼마나 쉽게 악용할 수 있는가?\" 를 양적으로 표현하려고 하는 것입니다.\n\n우리의 모든 작업은 현재 굉장히 실험적이며 맹목적으로 의존해서는 안 됩니다!\n\n더 궁금한 점이 있다면 언제든지 문의해 주세요.\n","ogImage":{"url":"/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png"},"coverImage":"/assets/img/2024-05-18-Around1000exploitablecybersecurityvulnerabilitiesthatMITRENISTmighthavemissedbutChinaorRussiadidnt_0.png","tag":["Tech"],"readingTime":5},{"title":"중간 과정 프롬프트를 A부터 Z까지 최상위 사진 용어로 변환하기","description":"","date":"2024-05-18 20:52","slug":"2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ","content":"\n사진 효과와 기술에 대한 A-Z 안내서에 오신 것을 환영합니다. 이것은 미드저니 작품을 향상시키기 위해 디자인된 가이드입니다. \"추상\"부터 \"줌 버스트\"까지, 이 기사는 다양한 시각적 표현의 팔레트에 대해 심층적으로 다룹니다.\n\n경험 많은 사진 작가이든 호기심 많은 초보자이든, 이 용어들은 여러분의 프롬프트를 풍부하게 만들어 주며 창의력을 넓히게 도와줄 것입니다. 이 변형적 기술을 함께 탐험하며, 디지털 예술 작품에서 새로운 차원을 열어보세요. 이미지에 혁신과 풍미를 부여할 준비가 되셨나요?\n\n# 추상\n\n추상 사진은 전통적인 현실주의와는 다르게 색상, 모양 및 패턴에 집중하여 종종 현실 세계의 대상과 거의 비슷하지 않은 이미지를 만드는 흥미로운 작품을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n형태, 선, 질감과 같은 주요 구성 요소를 강조하여 주제를 전형적인 맥락에서 추상적으로 처리하고 관행적인 지각에 도전하는 방식으로 제시합니다.\n\n극단적인 근접 촬영, 선택적 초점, 동적 흐림, 그리고 빛과 그림자의 상호 작용과 같은 기술이 이러한 효과를 달성하는 데 활용됩니다. 이러한 결과물은 그림이나 그래픽 아트와 유사하며 주관적 해석을 초대하며 종종 심금을 울리거나 사진 주제의 문자 그대로의 외관을 초월하는 깊은 감정이나 아이디어를 떠올리도록 합니다.\n\n추상 사진은 우리에게 예상치 못한 것을 표현하도록 장려하며, 질감 구조, 시각적 관점, 척도, 셔터 속도를 활용하여 생각을 자극하는 추상적 예술 작품을 만들게 합니다.\n\n[이미지 첨부](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_1.png)\n![image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_2.png)\n![image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_3.png)\n\n# Bokeh\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n볼케는 이미지의 초점을 맞추지 않은 부분에서 생산된 흐릿한 미학적인 효과로 유명한 사진 효과입니다. \"흐릿함\"이나 \"안개\"를 의미하는 일본어 단어에서 비롯되었으며, 보케는 일반적으로 사진 속에서 꿈틀거리는 분위기를 만듭니다.\n\n보케의 품질은 주로 카메라 렌즈 설계와 조리개 모양에 의해 결정됩니다. 둥근 조리개 블레이드를 가진 렌즈는 초점을 맞추지 않은 영역에서 더 매력적이고 부드럽고 둥근 흐림을 만들어냅니다.\n\n보케는 주로 초점 깊이가 얇은 초상화에서 인기가 많습니다. 이는 주제물을 부드럽게 흐린 배경에 두드러지게 만들어서 주제물을 두드러지게 하는 효과를 더해줍니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_4.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![This is an image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_5.png)\n\n![This is an image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_6.png)\n\n![This is an image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_7.png)\n\n# Contre jour\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nContre jour, 또는 역광은 카메라를 빛 원본 쪽으로 향하게 함으로써 대상을 그림자에 둔 채 실루엣 효과를 만들어내는 사진 촬영 기술입니다. 이 기법은 형태와 선을 강조하며 장면에 드라마나 신비로움을 불어넣어 빛과 어둠 사이의 높은 대비를 만들어냅니다.\n\n이 기술은 또한 대상 주변에서 헤일로 또는 테두리 조명 효과를 생성합니다. 초과 노출을 방지하기 위해 정확한 노출 제어가 필요하기 때문에 도전적이지만, 역광을 효과적으로 사용하면 화려하고 예술적인 결과물을 얻을 수 있습니다.\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_8.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_9.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_10.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_11.png)\n\n# Dutch Angle\n\n더치 앵글(Dutch Angle)은 카메라를 일부러 한 쪽으로 기울여 수평 선이 경사진 상태가 되도록 하는 기술입니다. 이 접근 방식은 혼란, 불안, 또는 역동성을 일으키며, 이는 스릴러와 호러와 같은 장르에서 심리적 긴장을 전달하는 데 좋아지게 만듭니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n보는 사람의 균형과 정상적인 시각을 단연 방해하여, 네덜란드 기욤은 객관적이거나 변형된 현실을 제공하여 종종 심리적 불안을 표현하거나 이미지에 독특한 스타일적 매력을 더합니다.\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_12.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_13.png)\n\n![image3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_14.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_15.png)\n\n# Edge Darkening\n\nEdge darkening, 또는 바이네팅(vignetting),는 이미지의 모서리가 중앙보다 어두워 보이는 효과입니다. 이는 렌즈의 특성으로 자연스럽게 발생할 수 있으며 종종 엣지에서 센서나 필름에 도달하는 빛이 적어지는 결과를 초래합니다.\n\n이 효과는 사진의 중요한 요소에 관전자의 주의를 끌기 위해 활용되며 자연스러운 프레임을 제공하고 이미지에 깊이나 드라마를 추가합니다. 때때로 결함으로 간주되기도 하지만, 의도적으로 적용할 때 바이네팅은 사진의 예술적 영향을 크게 향상시킬 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_16.png)\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_17.png)\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_18.png)\n![Image 4](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_19.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# Fisheye\n\n피시아이 효과는 특수 초광각 렌즈를 사용하여 달성되며, 화려하고 확장된 스타일의 사진을 제공합니다. 비주얼 왜곡이 심각하게 나타나는 피시아이 렌즈는 최대 180도까지의 매우 넓은 시야를 포괄하며 광활한 공간을 한 장면으로 압축합니다. 이로 인해 외곽에 곡선 형태의 선이 나오며 중앙의 물체가 확대되거나 강조될 수 있습니다. 피시아이 사진은 재미있고 초현실적인 요소로 인해 경치, 스포츠 및 창의적인 초상화에 대한 독특한 시각을 제공합니다.\n\n![이미지 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_20.png)\n\n![이미지 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_21.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_22.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_23.png)\n\n# Grain\n\n사진에서의 \"grain\"은 이미지에 시각적 질감이나 얼룩을 부여하는 효과로, 고속 필름에서 볼 수 있는 곡물 모양의 질감을 상기시킵니다. 이 질감은 필름 유화액에서 금속 은 입자의 분포로부터 발생하며, 높은 ISO 필름의 큰 입자가 더욱 뚜렷한 곡물을 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n디지털 사진에서는 센서가 일반적으로 노이즈를 생성하지 않지만, 어두운 조건이나 높은 ISO 설정에서 비슷한 효과가 발생할 수 있습니다. 이는 이미지에 촉감 품질을 더해 줍니다.\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_24.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_25.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_26.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_27.png)\n\n# High Key\n\nHigh key photography is distinguished by its light, airy feel, with an emphasis on bright tones and minimal dark shadows. Achieved through careful lighting, exposure settings, and post-processing, this style features soft lighting and high exposures to wash out midtones and shadows, creating images dominated by whites and light colors.\n\nHigh key photography is often associated with conveying a positive, cheerful mood and is widely used in fashion, beauty, and portrait photography for its clean, minimalist aesthetic.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_28.png)\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_29.png)\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_30.png)\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_31.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 적외선\n\n적외선 사진술은 적외선 필터나 개조된 카메라와 같은 전문 장비를 사용하여 맨눈으로는 보이지 않는 적외선 스펙트럼의 빛을 캡처합니다.\n\n이 기술은 독특하고 종종 초현실적인 이미지를 만들어냅니다. 살아있는 녹색 식물은 강한 적외선 광선 반사로 인해 밝은 흰색 또는 분홍색으로 나타나며, 하늘은 어두워져 구름과 식물과의 대비를 향상시킵니다.\n\n이는 풍경에 초월적이고 거의 초자연적인 퀄리티를 부여하여 익숙한 장면을 신비롭고 신탁스럽게 만들어냅니다. 처음에는 과학 및 군사 응용 분야로 개발되었지만, 적외선 사진술은 독특한 시각적 매력 때문에 예술적인 분야에서도 자리를 잡았습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_32.png)\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_33.png)\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_34.png)\n\n![Transform your Mid-journey prompts with top photography terms from A to Z](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_35.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 자누스 효과\n\n로마 신 자누스를 영감으로 한 자누스 효과는 두 얼굴을 가진 신의 모습으로 유명한데, 사진에서는 하나의 이미지 내에서 두 가지 대조되는 요소를 캡처하거나 대표합니다.\n\n이 기술은 종종 오래된 것과 새로운 것, 밝음 대 어둠, 또는 성장과 부패와 같은 주제를 탐구하며, 대상의 이중적 성격을 강조하고 시청자들에게 주변 환경의 복잡성을 숙고하도록 장려합니다.\n\n![이미지](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_36.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_37.png)\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_38.png)\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_39.png)\n\n# 키를리안 효과\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n키를리안 효과는 1939년에 그를 발견한 세물론 키를리안의 이름에서 따왔어요. 이것은 사진 과정을 포함하는데, 이는 물체의 에너지 필드 또는 \"오라\"를 포착한다고 알려져 있어요. 특히 잎이나 손과 같은 유기물 아이템에 관한 거든요.\n\n이 기술은 고전압 원원에 연결된 사진판을 사용하는데요; 코로나 방전을 통해 형성된 결과 이미지는 물체의 에너제틱한 특성을 보여준다고 합니다.\n\n![사진](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_40.png)\n\n![사진](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_41.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_42](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_42.png)\n\n![TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_43](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_43.png)\n\n# 롱 익스포저\n\n롱 익스포저 사진술은 카메라의 셔터가 일반적으로 열린 시간보다 길게 열려 빛이 더 많이 들어오도록 한 기술로, 단일 이미지 내에서 시간의 흐름을 캡처하는 것을 가능하게 합니다. 이 방법은 밤 풍경, 광선 뒤끝, 물의 흐름, 천체 사건과 같이 독특하고 종종 초현실적인 시각 효과를 만들어냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n정적 및 동적 요소가 혼재된 시나리오에서는 롱 노출이 부드러운 빛 흔적이나 물 흐름을 포착하는 동시에 정지된 물체의 선명함을 유지할 수 있습니다. 보통 삼각대와 원격 셔터 릴리스를 사용하여 안정성을 보장하는 롱 노출 사진술은 사진에서 시간을 시각적 요소로 탐구할 수 있도록 해줍니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_44.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_45.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_46.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\\[![이미지](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_47.png)\\]\n\n# 멀티소닉 효과\n\n멀티소닉 사진 촬영은 연속해서 빠르게 번갈아가며 발광하는 플래시 펄스를 사용하여 한 장의 복합 이미지 내에서 여러 각도에서 움직임을 얼립니다. 이 기술은 다양한 움직임 단계를 캡처하여 동적인 장면의 독특한 시각과 포괄적인 관점을 제공합니다.\n\n\\[![이미지](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_48.png)\\]\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_49.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_50.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_51.png\" /\u003e\n\n# Night Vision\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n밤 시야 사진술은 특징적인 녹색 단색으로 특징 지어지는 야간 시야 장비로 보는 효과를 모방하여 저조명 조건에서 시계바늘을 뚜렷하게 함으로써 시야를 높이는 기술입니다. 이 기술은 종종 대기 효과에 사용되거나 야간 활동 또는 감시와 관련된 주제적 문맥에서 사용됩니다.\n\n![Night vision photography](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_52.png)\n\n![Night vision photography](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_53.png)\n\n![Night vision photography](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_54.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_55.png)\n\n# Overlay\n\n사진 및 디지털 이미징에서의 오버레이 기술은 한 이미지를 다른 이미지 위에 쌓아 두어 두 이미지의 요소를 합쳐 하나의 복합 이미지를 형성하는 것을 의미합니다. 이 방법은 서로 다른 시각적 구성 요소를 창의적이고 예술적으로 혼합하여 사진 작품에서 현실과 상상력을 융합하는 것을 가능하게 합니다.\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_56.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_57.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_58.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_59.png)\n\n# Pinhole Effect\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n핀홀 효과는 핀홀 카메라와 관련이 있으며 망원현 대신 작은 구멍을 통해 이미지를 캡처하여 모든 장면 요소가 동등하게 초점을 맞추는 무한한 깊이의 영역으로 이어집니다.\n\n이 효과는 핀홀 가장자리에서 발생하는 빛의 굴절로 인해 독특한 부드러움을 가진 이미지를 만들어 냅니다. 핀홀 사진술은 긴 노출 시간과 독특한 시각적 스타일로 알려져 있으며, 렌즈 기반 사진술에서 예상되는 선명함과 대비를 제공합니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_60.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_61.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_62.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_63.png)\n\n# 퀘이사르 버스트\n\n퀘이사르 버스트는 사후 처리에서 방사형, 오염된, 별 모양의 필터 효과를 추가하여 은구멍 중심에 있는 거대한 블랙홀인 퀘이사르의 광도가 높고 동적인 모습을 시뮬레이션합니다. 이 효과는 사진에 화려하고 우주적인 퀄리티를 부여하여 우주와 공상과학 주제를 향상하는 데 이상적입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_64.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_65.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_66.png)\n\n![Image 4](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_67.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 롤링 셔터 효과\n\n롤링 셔터 효과는 이미지 센서 라인의 순차적 노출로 인한 왜곡으로, 빠른 이동 중 발생하는 기울어짐이나 스며림과 같은 변형을 초래합니다. 이 현상은 전체 이미지를 동시에 캡처하는 글로벌 셔터와 대조적이며, 빠르게 움직이는 상황이나 카메라 자체가 이미지를 캡처하는 동안 빠르게 이동할 때 특히 두드러집니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_68.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_69.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_70.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_71.png)\n\n# 셀렉티브 컬러링\n\n셀렉티브 컬러링은 주로 흑백 이미지 내에서 특정 주제물을 컬러로 분리하여 시선을 집중시키고 사진의 특정 부분을 강조하는 기술입니다. 이 기술은 주요 요소에 빛을 발해 이야기의 영향력을 향상시키며, 이미지의 컬러부분과 흑백 부분 간의 획기적인 대조를 만들어냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_72.png)\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_73.png)\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_74.png)\n\n![Image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_75.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 텍스처 오버레이\n\n텍스처 오버레이는 디지털 텍스처를 사진 위에 겹쳐서 모양을 변경하거나 향상시키는 작업입니다. 사진 작가들은 예쁜 또는 현실적인 텍스처(종이, 천, 녹슨 것 등)를 편집 소프트웨어를 사용해 이미지에 도입하여, 그것들을 혼합하여 사진 속에 심도나 테마적 공감을 만들어냅니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_76.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_77.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_78.png)\n\n# Unicolor\n\n“Unicolor” refers to the use of a single color in an image, often employed to create a uniform or thematic visual impact. When applied correctly, unicolor can evoke specific moods or artistic expressions, enhancing the conceptual unity of the image.\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_79.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_80.png)\n\n![image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_81.png)\n\n![image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_82.png)\n\n![image 4](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_83.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_84.png)\n\n![image2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_85.png)\n\n![image3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_86.png)\n\n# Voronoi Map\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n보로노이 맵은 이미지를 일부 지점과의 근접성에 기초하여 셀들의 모자이크로 분해합니다. 각 셀은 원본 사진의 일부를 포함하며, 이 효과는 조각난 느낌과 패턴이 있는 외관을 만들어내어 유리창 같은 예술 작품을 연상시키며 촬영된 장면에 대한 색다른 관점을 제공합니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_87.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_88.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_89.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Wet Plate Look](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_90.png)\n\nThe wet plate look replicates the style of early 19th-century wet plate photography, where a glass or metal plate coated with a light-sensitive emulsion was exposed while still wet. This method produced images with a distinctive soft focus and subtle imperfections like light leaks, offering a nostalgic and evocative quality reminiscent of historical photographs.\n\n![Wet Plate Look](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_91.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_92.png)\n\n![Image 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_93.png)\n\n![Image 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_94.png)\n\n# Xylography Effect\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nXylography, 또는 목판 인쇄는 목재 표면에 디자인을 새기고 잉크를 바르며 종이나 천에 옮기기 위해 누르는 것을 포함합니다. 이 기술은 굵은 선과 질감 있는 외관으로 특징지어지며 전통 인쇄 기술의 본질을 담아 독특한 예술적 효과를 낳습니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_95.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_96.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_97.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 요코쵸 효과\n\n요코쵸 효과는 간단한 구조와 비네팅, 흐린 가장자리, 빛 누출과 같은 독특한 이미지 특성으로 알려진 빈티지 일본 장난감 카메라의 미학을 흉내냅니다. 이 효과는 꿈풍경적인 시각적 스타일을 만들어내며 종종 향수와 예술적 불완전함의 느낌을 담고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지 1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_100.png)\n\n![이미지 2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_101.png)\n\n![이미지 3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_102.png)\n\n# 줌 터보 효과\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n줌 버스트 효과는 조리개 렌즈의 초점 길이를 조정하여 얻어지며, 사진 속에서 움직임을 느끼게 합니다. 이 기술은 정지된 요소들이 프레임의 중심쪽으로 빠르게 다가오거나 멀어지는 것처럼 보이게 하며, 촬영된 장면에 에너지와 확장된 느낌을 더합니다.\n\n![이미지1](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_103.png)\n\n![이미지2](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_104.png)\n\n![이미지3](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_105.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_106.png)\n\n# 결론: 당신의 창의적인 나침반\n\n우리가 사진 효과의 A부터 Z까지의 여정을 마무리하는 동안, 각 기술이 당신을 자극하여 당신의 Midjourney 프롬프트의 경계를 넓히도록 합시다. 강렬한 \"줌 버스트\"에서 신비로운 \"적외선\"까지 모든 것을 실험하여 이미지의 감정과 영향력을 깊게 하세요.\n\n기억하세요, 사진은 시각만큼이나 시각적인 것입니다. 이 도구들을 사용하여 보통 것을 환상적인 것으로 바꾸고, 창의적인 풍경의 끝없는 가능성을 탐험하시기 바랍니다. 계속해서 창작하고, 꿈꾸고, 당신의 예술을 빛나게 해보세요!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n--- by 公众号: 小祸碎碎念\n\n💡Want a deeper dive? My Midjourney collection awaits you.\n\n## Loved the article?\n\nIf yes, then:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 댓글 남기기\n- 업데이트 팔로우하기\n- 무료 이메일 알림\n","ogImage":{"url":"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png"},"coverImage":"/assets/img/2024-05-18-TransformyourMidjourneypromptswithtopphotographytermsfromAtoZ_0.png","tag":["Tech"],"readingTime":32},{"title":"인공지능 회사들에 속지 않기 위해 예술가들이 작품에 함정을 설치하고 있어요","description":"","date":"2024-05-18 20:46","slug":"2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork","content":"\n![이미지](/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png)\n\n포스트된 Dall-E의 출현 이후에 Midjourney나 Stable Diffusion과 같은 다른 생성 이미지 처리 알고리즘들이 나오기까지 오랜 시간이 걸리지는 않았습니다. 그러나 큰 문제가 드러났습니다: 이러한 알고리즘을 만든 기업들이 이미지에 설명을 달고 라벨을 붙인 거대한 이미지 컬렉션을 축적하고, 그러고는 이를 바탕으로 알고리즘을 훈련시켰다는 것입니다.\n\n이러한 거대한 이미지 컬렉션은 어디서 얻었을까요? 대부분은 이미지 저장소 등의 웹 사이트를 스크랩핑함으로써 입수했습니다. Getty Images가 Stable Diffusion에 대한 소송을 제기함으로써 그들의 이미지 출처가 명백하게 드러났습니다. 많은 경우, 알고리즘은 그것을 이미지의 다른 부분으로 해석하여 그들의 워터마크의 왜곡 버전이 포함된 이미지를 생성하기도 했습니다.\n\n법적 문제는 분명합니다: 웹 상에 공개된 정보는 스크랩핑 대상이 될 수 있다는 것을 수년간 말해 왔습니다. 어떤 목적으로든 웹 페이지에 가서 그 내용을 모두 복사하는 권리를 달성하는 다양한 법적 선례들이 있습니다. 복잡성 때문에 해당 사례가 몇 년간 계속되어 대법원에 이르기도 하겠지만, 이 중요한 사안에 대해 저작물이 알고리즘 훈련에 사용된 작가들은 자신들의 작품이 쉽게 모방될 수 있다는 것을 보거나 누군가가 그들의 스타일을 시뮬레이션하여 새로운 이미지를 만들어낼 수 있다는 것을 보게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n법원이 도움을 많이 제공하지 않을 것이라는 점을 인정하면, 일부 예술가들은 자신들의 작품을 보호하기 위해 보비 트랩을 설치하고 있습니다. 이들은 소프트웨어로 처리된 이미지를 생성하여 알고리즘이 혼란스럽게 만드는 무시무시한 변경을 가하고 있습니다. 이 과정은 사람들의 얼굴을 사진이나 동영상에서 무시무시하게 수정해서 얼굴 인식 알고리즘을 방지하는 방식과 동일합니다. Atropa belladonna 식물(환각을 일으키는 식물)을 기리며 명명된 Nightshade라는 알고리즘은 수정된 사진을 게시할 수 있게 허용하며, 이러한 수정된 사진은 알고리즘이 실제 내용과 다른 설명을 생성하도록 합니다. 이 때문에 알고리즘이 혼란스러워져 결과물로 요청된 것과 다른 이미지를 제공합니다.\n\n결과적으로, 이는 아직도 기능을 수행하는 이미지를 “독”으로 오염시킨 것과 같습니다. 여전히 그림을 보고 예쁘게 고르고 예술가들이 설정한 조건에 따라 선택할 수 있지만, 알고리즘에 흡입되면 “환각”을 유발합니다. “독”이 가미된 이미지가 많을수록 알고리즘은 더 예측할 수 없어지며, 기업들은 알고리즘 교육에 사용하는 콘텐츠를 모니터링하기 위한 메커니즘을 구축하여 비용이 크게 증가합니다.\n\n이것은 이러한 유형의 도구를 만드는 기업에게 경고의 신호이며, 그들이 경고받은 문제들 중 많은 것을 설명합니다: 알고리즘을 쓰레기로 먹이면 쓰레기가 생성됩니다. 많은 경우, 투자자들에게 정당화하기 위해 결과를 너무 빨리 제공해야하는 기업들이 있는데, 이 때문에 어중간한 정보를 사용하게 되어 어떤 교육의 기초가 되어서는 안 되는 상황이 됩니다. 따라서 그들의 알고리즘은 덜 신뢰할 수 있게 됩니다. 기본적으로, “쓰레기를 입력하면 쓰레기를 출력한다.” 대부분의 교육 과정에서 마찬가지로 서둘러서 움직이는 것은 좋은 생각이 아닙니다.\n\n실무적으로, 예술가들은 자신들의 작품에 대해 마음대로 조작할 수 있습니다. 이전까지 회사가 아카이브 전체를 긁어 모든 내용으로 알고리즘을 교육시킬 수 있는 방법을 방해할 수 없다고 믿어져 왔던 것과 마찬가지로 말입니다. 아무것도 절대적으로 확정되어 있는 게 없으며, 일부 예술가들이 보여준 것처럼 특히 저작권을 관리하는 사람들이라면, 이미지가 알고리즘 교육용으로 사용될 때 적절한 보상을 받을 수 있도록 어떠한 협상이 이뤄져야 할 것으로 보입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 공간을 주시해주세요.\n\n(스페인어로는 여기에)\n","ogImage":{"url":"/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png"},"coverImage":"/assets/img/2024-05-18-TiredofbeingrippedofbyAIcompaniesartistsareboobytrappingtheirwork_0.png","tag":["Tech"],"readingTime":3},{"title":"안정적인 확산에 대한 설명","description":"","date":"2024-05-18 20:45","slug":"2024-05-18-StableDiffusionExplained","content":"\n## Stable 확산이 어떻게 작동합니까? 텍스트에서 이미지 생성 기술 설명\n\n![이미지](/assets/img/2024-05-18-StableDiffusionExplained_0.png)\n\n크기가 큰 텍스트에서 이미지 모델은 텍스트 프롬프트로부터 이미지의 고품질 합성을 가능케하여 높은 성공을 거뒀습니다. 확산 모델은 텍스트에서 이미지 생성 작업에 적용되어 최첨단 이미지 생성 결과를 얻는 데 도움이 될 수 있습니다.\n\nStable 확산 모델은 이미지 생성을 위해 최첨단 결과를 달성했습니다. Stable 확산은 특정 유형의 확산 모델인 Latent 확산 모델에 기초합니다. 이 모델은 CompVis, LMU 및 RunwayML의 연구원 및 엔지니어들에 의해 제안된 'Latent 확산 모델을 사용한 고해상도 이미지 합성'에 기반합니다. 이 모델은 LAION-5B 데이터베이스의 하위 집합에서 512x512 이미지로 초기에 교육되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이는 주로 CLIP와 같은 사전 훈련된 언어 모델을 사용하여 텍스트 입력을 잠재 벡터로 인코딩함으로써 달성됩니다. 확산 모델은 텍스트로부터 이미지 데이터를 생성하는 데 최첨단 결과를 달성할 수 있습니다. 그러나 잡음 제거 프로세스는 고해상도 이미지를 생성할 때 매우 느리며 많은 메모리를 소비합니다. 따라서 이러한 모델을 훈련하고 추론에 사용하는 것이 어려운 과제입니다.\n\n이에 따라 잠재 확산은 실제 픽셀 공간 대신 낮은 차원의 잠재 공간에서 확산 프로세스를 적용함으로써 메모리와 계산 시간을 줄일 수 있습니다. 잠재 확산에서 모델은 이미지의 잠재(압축) 표현을 생성하도록 훈련됩니다.\n\n확산 모델의 훈련\n\nStable Diffusion은 수십억 장의 이미지로 훈련된 대규모 텍스트에서 이미지로 확산 모델입니다. 이미지 확산 모델은 이미지를 잡음 제거하여 출력 이미지를 생성하는 방법을 배웁니다. Stable Diffusion은 훈련 데이터에서 인코딩된 잠재 이미지를 입력으로 사용합니다. 또한 주어진 이미지 z0에 대해, 확산 알고리즘은 이미지에 점진적으로 잡음을 추가하여 소음이 섞인 이미지 zt를 생성합니다. 여기서 t는 잡음이 추가된 횟수를 나타냅니다. t가 충분히 크면 이미지는 순수한 잡음에 가까워집니다. 시간 단계 t, 텍스트 프롬프트, 이미지 확산 알고리즘 등의 입력 집합이 주어진 경우, 확산 알고리즘은 노이즈를 예측하는 네트워크를 학습합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n잠재 확산에는 주로 세 가지 주요 구성요소가 있습니다:\n\n- 자동 인코더 (VAE).\n- U-Net.\n- 텍스트 인코더, 예를 들어 CLIP의 텍스트 인코더.\n\n1. 자동 인코더 (VAE)\n\nVAE 모델은 인코더와 디코더 두 부분으로 구성됩니다. 잠재 확산 학습 중에 인코더는 512*512*3 이미지를 순방향 확산 과정을 위한 사이즈가 64*64*4인 낮은 차원의 잠재 표현으로 변환합니다. 우리는 이미지의 이러한 작은 인코딩된 버전을 잠재라고 부릅니다. 학습 각 단계에서 이러한 잠재에 더 많은 잡음을 적용합니다. 이미지의 인코딩된 잠재 표현은 U-Net 모델의 입력으로 작용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기서는 (3, 512, 512) 모양의 이미지를 (4, 64, 64) 모양의 잠재 이미지로 변환하여 메모리를 48배 더 적게 사용합니다. 이는 픽셀 공간 확산 모델과 비교했을 때 메모리 및 계산 요구 사항을 줄여줍니다. 따라서 16GB Colab GPU에서도 512 × 512 이미지를 매우 빠르게 생성할 수 있습니다.\n\n디코더는 잠재 표현을 이미지로 다시 변환합니다. 역확산 과정에서 생성된 노이즈 제거된 잠재 이미지를 VAE 디코더를 사용하여 이미지로 변환합니다.\n\n추론 중에는 노이즈가 제거된 이미지를 실제 이미지로 변환하기 위해 VAE 디코더만 필요합니다.\n\n```python\nfrom torchvision import transforms as tfms\nfrom diffusers import AutoencoderKL\n\n# Decode the latent representation into image space using the autoencoder model.\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Move to the GPU\nvae = vae.to(torch_device)\n\n# Convert PIL image to latents\n\ndef pil_to_latent(input_im):\n    # Single image -\u003e single latent in a batch (size: 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2. UNet\n\nU-Net은 노이즈가 있는 잠재 변수의 더 많은 이미지 표현을 예측합니다. 여기서 노이즈가 있는 잠재 변수가 Unet의 입력으로 작용하며 UNet의 출력은 잠재 변수의 노이즈입니다. 이를 사용하여 우리는 노이즈를 노이즈가 있는 잠재 변수에서 뺌으로써 실제 잠재 변수를 얻을 수 있습니다.\n\nUnet은 노이즈가 있는 잠재 변수(x)를 입력으로 사용하고 노이즈를 예측합니다. 우리는 또한 타임스텝(t)과 텍스트 임베딩을 가이드로 사용하는 조건부 모델을 사용합니다.\n\n따라서, 모델은 다음과 같습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nfrom diffusers import UNet2DConditionModel\n\n# \"CompVis/stable-diffusion-v1-4\" 모델을 사용하여 UNet 모델을 불러옵니다.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n# GPU로 모델을 이동합니다.\nunet = unet.to(torch_device)\n# 노이즈 예측\nnoise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n```\n\n해당 모델은 기본적으로 UNet 모델을 사용하며, 인코더(12블록), 가운데 블록 및 스킵 연결 디코더(12블록)로 구성되어 있습니다. 이 25개 블록 중 8개 블록은 다운샘플링 또는 업샘플링 컨볼루션 레이어이고, 17개 블록은 각각 네 개의 ResNet 레이어와 두 개의 Vision Transformer(ViT)를 포함하는 주요 블록입니다. 여기서 인코더는 이미지 표현을 낮은 해상도 이미지 표현으로 압축하고, 디코더는 낮은 해상도 이미지 표현을 원래의 더 높은 해상도 이미지 표현으로 해석합니다. 이로써 노이즈가 적은 이미지를 생성합니다.\n\n3. 텍스트-인코더\n\n텍스트-인코더는 입력 프롬프트를 임베딩 공간으로 변환하여 U-Net에 입력으로 제공합니다. Unet을 노이즈 제거 프로세스에 학습시킬 때 노이즈가 많은 latents를 안내하는 역할을 합니다. 텍스트-인코더는 일반적으로 간단한 트랜스포머 기반 인코더로, 입력 토큰 시퀀스를 잠재적인 텍스트-임베딩 시퀀스로 매핑합니다. Stable Diffusion은 새로운 텍스트 인코더를 학습하지 않고 이미 학습된 텍스트 인코더인 CLIP을 사용합니다. 텍스트 인코더는 입력 텍스트에 해당하는 임베딩을 생성합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n토큰화\n\n```js\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# 토크나이저 및 텍스트 인코더를로드하여 텍스트를 토큰화하고 인코딩합니다.\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# GPU로 이동\ntext_encoder = text_encoder.to(torch_device)\n\nprompt = 'An astronaut riding a horse'\n# 텍스트를 토큰 시퀀스로 변환합니다.\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n```\n\nEmbedding 결과\n\n```js\n# 토큰에서 출력 임베딩 가져오기\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nprint('모양:', output_embeddings.shape)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다 모아보면, 모델은 추론 과정 중에 다음과 같이 작동합니다:\n\n![image](/assets/img/2024-05-18-StableDiffusionExplained_1.png)\n\n스케줄러\n\n위에서 언급된 3가지 외에도 이미지에 노이즈를 추가하고 모델을 사용하여 노이즈를 예측하는 데 사용되는 스케줄러가 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndiffusers에서 LMSDiscreteScheduler를 가져와주세요\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n```\n\n위 코드는 모델을 훈련하는 데 사용되는 스케줄러를 설정합니다. 작은 단계 수에 대해 스케줄러를 설정하려면 다음과 같이 스케줄러를 설정하세요:\n\n```js\n# 샘플링 단계 수를 설정하세요:\nscheduler.set_timesteps(15)\n```\n\nStable Diffusion과 같은 Latent Diffusion Model은 다양한 창의적인 응용 프로그램을 가능하게 합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 텍스트에서 이미지로 변환\n- 이미지에서 이미지 생성 - 시작점을 기반으로 새 이미지를 생성하거나 수정\n- 이미지 업스케일링 - 작은 이미지를 큰 이미지로 확대\n- 인페인팅 - 이미지의 특정 영역을 마스킹하여 해당 영역에 새로운 디테일을 생성하는 것\n\n잠재 확산 모델은 훈련 비용과 추론을 줄여주어 대량의 고해상도 이미지 합성을 대중화 할 수 있는 잠재력이 있습니다.\n\n다음 블로그에서는 새로운 개념이나 작업을 학습하는데 안정적인 확산을 세밀하게 조정하는 텍스트 역전법에 대해 이야기할 예정입니다.\n\n참고:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 롬바흐, R., 블랫만, A., 로렌츠, D., 에셀, P., \u0026 오머, B. (2022). 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전과 패턴 인식 컨퍼런스 논문집 (pp. 10684–10695).\n- 장, L., \u0026 아그라와라, M. (2023). 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄 arXiv:2302.05543.\n- [Hugging Face Diffusers 문서](https://huggingface.co/docs/diffusers/index)\n","ogImage":{"url":"/assets/img/2024-05-18-StableDiffusionExplained_0.png"},"coverImage":"/assets/img/2024-05-18-StableDiffusionExplained_0.png","tag":["Tech"],"readingTime":9},{"title":"인간의 시선 AI가 창의력에 미치는 영향 탐구","description":"","date":"2024-05-18 20:44","slug":"2024-05-18-TheHumanLensExploringAIsImpactonCreativity","content":"\n\u003cimg src=\"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png\" /\u003e\n\n“블랭크 슬레이트”에서 스티븐 핑커는 현대 미술을 비판하며 생성적 AI 예술의 폭발과 공감합니다. 현대 미술과 AI 창작물은 우리가 본성적으로 아름다운 것에서 멀어지는 것으로 보입니다. 핑커는 이러한 정체성에 대해 이렇게 얘기합니다: “우리는 아름다움, 감정, 그리고 기술에서 멀어지고 있다.” 어떤 작품들은 특정 대상을 위해 설계되어 완전히 이해하려면 광범위한 맥락이 필요합니다.\n\n알고리즘의 결과가 '예술'로 불릴 때, 이는 예술가에게 모욕이 됩니까? AI는 확실히 기존의 정의에 도전을 제기합니다. 그래도, 이러한 정의를 정제하려는 노력은 새로운 것은 아닙니다. 작가들은 작품을 더 나아가도록 자극하는 이 존재에 직면하며, 일부는 익숙한 것을 버리려는 저항을 합니다. 이러한 변화는 우리에게 뻔한 서사와 기술적 공식을 넘어서도록 이끕니다. 이는 이미 확립된 구조에 익숙한 사람들에게는 불편한 일입니다.\n\n# 모두에게 붓을 전달\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n생성적 AI는 예술 창작을 민주화하여, 현대 예술에서 핑커가 비판한 엘리트주의를 흔드는 역할을 합니다. 이제 전통적인 재능의 부재로 인해 창의적인 영혼이 억압받을 필요가 없습니다. 스킬이 항상 신체 민첩성에 결합되지 않는다는 점에 주목한 피커는 이를 예상한 것입니다. 이것이 바로 AI 예술이 구현하는 것입니다.\n\n# 새로운 예술적 도전\n\n현대 예술과 AI 예술은 소수만이 다스릴 수 있는 언어처럼 느껴질 수 있습니다. 사상을 유발하고, 더 깊은 수준의 대화를 유발할 수 있는 작품에 대해 끌리는 것이 있습니다. 그러나 기계가 이러한 대화를 예술을 통해 제공할 수 있을까요? AI 생성은 이를 요구하며, 동시에 핑커가 일부 예술적 영역에서 지적한 속세적인 태도를 강조합니다.\n\n# 디지털 시대의 예술 재정의\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n핑커는 특정 예술 형식의 사라짐을 애도하지만, AI는 예술 표현의 완전히 새로운 시대를 열어줍니다. 이 발전은 우리에게 예술을 기술과 아방가르드 아이디어를 넘어서 바라보게 만듭니다. 창의적 영감의 원천이 중요할까요? 기계가 '아티스트'가 될 수 있을까요, 아니면 인간의 지시의 연장으로 존재할까요? 핑커는 마음을 진화의 산물로 보고, AI 아트는 자연적이고 인공적인 창의성을 결합하여 예술이 무엇을 '예술'로 만드는지에 대한 신념적 질문을 던집니다.\n\n# 브러시가 스스로 들고 있는 경우\n\n알고리즘에 의해 휘둘리는 '브러시'로 인해 '인간의 손길'이 어떤 역할을 하는 걸까요? 모든 예술가는 자신의 선택한 매체에서 이 질문에 답하려 노력합니다. AI와 상호작용하는 모든 사람이 '아티스트'가 되는 것은 사실이 아니며, 같은 이치는 캔버스, 악기 또는 기타 전통 도구를 사용하는 사람들에게도 동일합니다.\n\n# 보이지 않는 것을 느낄 수 있을 때\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n핑커는 현대 예술이 감정적 연결이 부족할 수 있다고 주장하지만, AI가 이 논쟁의 불을 지피고 있다고 합니다. 기계는 우리를 움직이는 작품을 만들어내지만, 인간의 영혼이 그것이 발생하는 시점을 결정합니다. 동굴 벽화부터 아티스트들은 형식과 내용을 동기부여하고 깊은 감정을 일으키기 위해 노력했습니다. 지금은 우리를 강요하여 감정의 근원을 재정의하도록 이끕니다. 예술이 본질적으로 공유된 경험인 경우, 인간이 만든 것인지 기계가 만든 것인지 상관이 있을까요? 예술 경험은 관객 안에서 독특하게 이루어집니다.\n\n# 무한한 캔버스의 세계\n\n문화적 참조를 끊임없이 섞을 수 있는 생성적 AI는 예술적 일률성에 대한 핑커의 우려에 도전하며, 그가 예술적 관문을 비판하는 것에 울린다고 합니다. 내재적인 제약이 적어져서 전례 없이 다양한 표현을 허용합니다. 이는 문화적 도용부터 잠재적 편견까지 자체적인 복잡성을 야기하나, 일부 예술계의 지각되는 엘리트주의에 대응합니다.\n\n# 새로운 윤리적 풍경을 탐색하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAI 예술은 피커가 자주 탐구하는 사회적 갈등을 반영하여 윤리적 문제의 폭풍우를 드러냅니다. 저작권부터 진정성에 대한 우려, 그리고 '영감' 자체의 본질까지, 생성적 AI는 우리에게 현존하는 도덕적 틀을 재고하도록 강요합니다.\n\n생성적 AI는 아름다운 이미지와 윤리적 고민의 폭풍우를 만들어내지만, 피커의 작품 전반에 걸친 통찰은 이 분야에서 놀랍게도 여전히 유의미합니다. 기술과 예술의 본질을 재검토하고 인간의 의도 이상의 감정적 창조력을 사색함으로써, AI 예술은 잠재력과 도발을 제공합니다.\n\n## 이 이야기가 마음에 드셨다면, 다음도 좋아하실 수 있습니다:\n","ogImage":{"url":"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png"},"coverImage":"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png","tag":["Tech"],"readingTime":4},{"title":"스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법","description":"","date":"2024-05-18 20:43","slug":"2024-05-18-HowStylarAIcanmakeyourdoodlesintoart","content":"\n저는 디자이너와 예술가로 훈련을 받았고, 그림 그리는 것을 좋아했어요. 하지만 안타깝게도 그 시절은 지나갔어요. 신경병증 때문에 손을 제어하는 게 어렵게 되었거든요. 그래서 생성 모델 인공지능이 도움을 준 거예요.\n\n어떤 식으로든지 인공지능이 제게 예술적인 비전을 표현하는 데 도움이 돼요. 제 손 대신 단어로요.\n\nStylar AI를 시도해본 적이 있을 때, 나 자신이 적어도 일부분은 다시 창작에 돌아갈 수 있는 잠재력이 있다는 것에 기쁨을 느꼈어요.\n\nStylar AI는 포토샵과 비슷한 레이어, 배경 및 객체 제거 도구, 얼굴 교체 및 수정 도구, 생성 채우기와 확장 기능 등 독특한 기능이 있는 강력한 도구에요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n한 가지 기능 중 하나는 이미지 대 이미지 생성입니다. 다른 생성적 AI 도구에도 있지만, Stylar AI 버전은 가장 간단한 입력과 작업할 수 있는 놀라운 능력이 있어서 뛰어난 고품질 출력물을 생성할 수 있습니다.\n\nStylar AI 홈페이지에는 간단한 손으로 그린 도형이 어떻게 인상적인 로고로 만들어지는지 보여주는 비디오가 심지어 있는데요.\n\n그것이 저에게 영감을 주어 이미지 대 이미지 도구를 사용해 몇 개의 선으로 그린 그림들을 시도해 보았습니다. 첫 번째 그림은 아르누보 양식의 여성 그림이었습니다.\n\n이 결과물들이 정말 마음에 들었고, 놀라울 정도로 Stylar가 선택한 스타일을 적용하는 데 전혀 문제가 없었고 단지 흑백 선으로 이미지를 안내하여 구성했을 뿐이었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음 시도는 해변 오두막을 그린 것이었습니다. 결과는 다시 한 번 인상적이었습니다.\n\n나는 비교적 간단한 입력 이미지가 탁월한 결과를 만들 수 있는 것을 확신했습니다. 그래서 다음 시도는 꽃무늬의 추상 만달라 스타일 그림이었습니다.\n\n다양한 스타일이 이 간단한 입력 이미지를 수정하는 방법이 놀라울 정도입니다. 이것은 추상 벽 장식품, 스크린 세이버, 배경 및 기타 이미지 생성용 새로운 스타일을 만드는 무한한 기회를 제공합니다.\n\n가장 최근 예시는 나무 그림에 대한 간단한 예술적인 그림입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다시, 훌륭한 결과네요. 이 중에 있는 그림들을 벽에 걸어두는 것이 수치스럽지 않을 거에요.\n\n그리고 여기가 관건이에요: 이 그림 중 일부는 프롬프트만 있으면 만들기가 굉장히 어려울 거에요.\n\n좋아요, 지금까지 사용한 것은 여전히 숙련된 손에 의해 만들어진 정적 이미지였어요. 그래서 다음 실험은 심지어 더 간단한 선들을 기반으로 한 것이었어요. 이는 큰 예술 능력이나 훈련이 필요하지 않았답니다.\n\n내 자신이 빠져들고 있었어요. 간단한 그림을 계속 만들고, 스타일러 AI에 입력한 뒤 멋진 이미지들의 산을 만들어내고 있었어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n네, 손이 잘못됐더라도 다시 만들 수 있었어요. 간단한 선 그림은 정확도나 통제력이 많이 필요하지 않아요. 제 시각과 아이디어의 간단한 낙서가 작동했어요.\n\n당신에게도 도움이 될 거예요. 놀라운 건 이 간단한 스케치가 콘텐츠와 구성을 이끌어 줄 수 있다는 거예요. 스타일은 모든 것을 다 다루죠.\n\n이 그림들을 만들기 위해 프롬프트 창에 단어를 입력할 필요가 없었어요. 그냥 스케치와 선택한 스타일뿐이었어요.\n\n그리고 만약 마음에 드는 스타일이나 원하는 스타일을 찾지 못한다면, Stylar AI에서 손쉽게 여러분만의 스타일을 만들 수 있어요. 이미지를 업로드하기만 하면, 다음에 원할 때 적용하기 쉽도록 여러분의 사용자 정의 스타일 사이에 저장돼요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 기사의 표지 그림은 스케치와 이미지 대 이미지 도구를 사용해서 만들어졌어요. 이 기술은 아름다운 사실적인 이미지, 일러스트레이션, 스티커, 티셔츠 아트, 그리고 떠오르는 다른 모든 것들을 만들어낼 수 있어요.\n\nStylar AI는 다른 방법으로나 어플로는 어렵게 만들 수 없는 이미지를 만들어 낼 수 있어요.\n\nAivaras Grauzinis\n","ogImage":{"url":"/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png"},"coverImage":"/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png","tag":["Tech"],"readingTime":4},{"title":"The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드","description":"","date":"2024-05-18 20:40","slug":"2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences","content":"\n![NLP vs LLM](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png)\n\nNLP 및 LLM 기술은 대규모로 사람 언어를 분석하고 생성하는 데 중요합니다. 그들의 증가하는 보급으로, LLM 대 NLP를 구별하는 것은 점점 더 중요해지고 있습니다.\n\nNLP는 인간 언어를 이해, 조작 및 생성하기 위한 일련의 알고리즘을 포함합니다. 1950년대에 처음으로 등장한 이후, NLP는 텍스트 관계를 분석하는 데 진화했습니다. 이는 품사 태깅, 명명된 개체 인식 및 감성 분석 방법을 사용합니다.\n\nOpenAI의 ChatGPT가 보여주는 것처럼, LLM은 깊은 학습을 활용하여 방대한 텍스트 세트로 학습합니다. 인간과 유사한 텍스트를 모방할 수 있지만, 언어의 뉘앙스를 이해하는 것은 제한됩니다. NLP가 언어 분석에 중점을 둔 반면, LLM은 주로 텍스트를 생성합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n제안서를 소개해 드릴게요. NLP와 LLMs의 간결하면서 포괄적인 비교를 제공합니다. 이 두 기술의 복잡성을 탐구하고, 다양한 응용 분야를 알아보며, 도전 과제를 살펴볼 것입니다.\n\n# NLP의 독특한 특징 탐구\n\nNLP는 기계가 인간 언어를 의미 있는 방식으로 이해하고 상호 작용하는 데 도움을 줍니다. 스펠 체크, 자동 교정부터 챗봇과 음성 비서에 이르기까지 다양한 응용 분야에 사용될 수 있습니다.\n\nNLP는 인간 언어 생성을 가능하게 하는 알고리즘을 만드는 것입니다. 이것은 디지털 시스템과 인간 간의 소통 간극을 줄입니다. 이 기술은 산업 전반에 걸쳐 증진된 데이터 분석과 통찰력을 위한 길을 엽니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_1.png\" /\u003e\n\n## NLP에서 꼭 알아야 할 기술: 파싱부터 자연어 생성까지\n\n자연어 처리는 컴퓨터가 인간의 언어를 생성할 수 있도록 다양한 프로세스에 의존합니다:\n\n- 파싱. 이 기술은 문장을 문법적 요소로 분해합니다. 기계에게 언어 구조를 간소화해 주며 품사, 문장 구분 및 구문적 연결을 인식하는 데 도움이 됩니다.\n- 의미 분석. 단순한 단어 식별을 넘어 단어의 의미와 관계를 파악하는 과정입니다. 텍스트의 맥락, 관용구 및 유머를 해석하는 데 중요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_2.png)\n\n- 음성 인식. 구어를 쓰인 텍스트로 변환하여 음성을 읽을 수 있는 형식으로 전환하게 됩니다.\n- 자연어 생성. 음성 인식과는 반대로, NLG는 컴퓨터 데이터에 기반하여 인간의 글쓰기를 모방한 텍스트를 제공합니다. 보고서 작성, 요약, 메시지 작성 등을 포함한 응용분야가 있습니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_3.png)\n\n- 감성 분석. 소셜 미디어 모니터링과 브랜드 평판 관리에 자주 사용됩니다. 글의 감정 톤을 평가하고 고객 피드백 및 시장 동향을 분석합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![NLP vs LLMA: Comprehensive Guide to Understanding Key Differences 4](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_4.png)\n\n- 기계 번역. 한 언어에서 다른 언어로 텍스트나 음성을 변환하는 기능을 제공합니다.\n- 명명된 엔터티 인식. 텍스트에서 중요한 정보를 감지하고 분류합니다. 예를 들어 개인, 장소, 조직의 이름 등을 인식합니다.\n\n![NLP vs LLMA: Comprehensive Guide to Understanding Key Differences 5](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_5.png)\n\n- 텍스트 분류 및 분류. 텍스트에 레이블을 할당하여 방대한 데이터 양을 효율적으로 정리하고 관리할 수 있습니다. 이는 문서, 이메일, 그리고 온라인 콘텐츠를 구성하는 데 유용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## NLP Applications: Enhancing Communication and Analysis\n\n![NLP Applications](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_6.png)\n\nNLP의 응용 프로그램은 광범위하며 다양한 섹터에 영향을 미칩니다:\n\n- 텍스트 분석. 대규모 텍스트 데이터 세트를 분석하여 중요한 통찰을 얻습니다. 시장 조사 및 소셜 미디어 감시에 유용할 수 있습니다.\n- 음성 인식. 음성으로 된 지시를 이해하고 실행하는 데 사용되는 음성 활성화 장치 및 응용 프로그램을 구동합니다. 이 기술은 가상 어시스턴트 및 필기 도구의 기반을 제공합니다.\n- 감성 분석. 감정적인 맥락을 분석합니다. 대중 의견을 모니터링하고 시장 조사를 수행하는 데 중요합니다.\n- 기계 번역. 언어 장벽을 깨뜨려 텍스트나 음성을 번역하여 국제적인 의사 소통을 촉진합니다.\n- 콘텐츠 추천. NLP를 사용하여 사용자 선호도와 콘텐츠 특성에 기반한 콘텐츠 제안을 맞춤화합니다. 또한 온라인 스트리밍 플랫폼과 온라인 쇼핑에서 경험을 향상시킵니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## NLP에서의 과제: 제한 사항을 극복하며\n\n진전은 있지만 NLP는 여러 가지 장애물을 극복해야 합니다. 이러한 문제를 해결한다면 NLP의 정확성과 기술 통합을 높일 수 있습니다:\n\n- 문맥적 이해. 언어의 미묘한 차이(비꼬거나 관용적인 표현)를 이해하는 것은 여전히 어려움을 겪고, 오해를 일으킬 수 있습니다.\n- 언어 다양성. 문법과 구문이 각각 다른 여러 언어와 방언의 수가 많은 것은 상당한 난관으로 작용합니다.\n- 언어의 모호성. 인간의 언어적 모호성은 NLP 시스템의 해석을 복잡하게 만들 수 있습니다.\n- 데이터 품질과 이용 가능성. NLP 시스템의 성능은 훈련 데이터의 품질과 양에 좌우됩니다. 이 데이터의 편향은 왜곡된 결과를 초래할 수 있습니다.\n- 계산 리소스. 고급 앱에 대한 상당한 계산 능력 수요는 그 개발과 배포를 제한합니다.\n- 실시간 처리. 동시 번역 및 고객 서비스와 같은 응용에 대한 실시간 처리는 기술적인 도전을 제시합니다.\n\n# 대형 언어 모델의 능력 탐색\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n대형 언어 모델은 언어 작업에 포괄적인 접근 방식을 제공합니다. 기존의 자연어 처리 시스템 이상으로 유창성과 적응성을 나타냅니다. LLM은 생성적 AI를 위해 정교한 기술 스택을 활용하여:\n\n- 일관성 있고 맥락에 적합한 텍스트 생성\n- 의미 있는 대화를 진행\n- 질문에 대한 답변 제공\n- 인간의 글쓰기와 유사한 콘텐츠 생성\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_7.png)\n\n## LLM의 차별화된 특징\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLLMs는 그들을 돋보이게 하는 몇 가지 핵심 속성으로 특징 지어집니다:\n\n- 광범위한 훈련 데이터. LLMs는 다양한 텍스트 소스에서의 방대한 데이터 세트로 훈련됩니다. 이 접근 방식은 다양한 언어 스타일과 형식을 생성할 수 있게 합니다.\n- 적응성. 언어 모델은 특정 작업마다 특별한 훈련이 필요 없이 여러 가지 언어 작업에 대응할 수 있습니다. LLMs는 자동 콘텐츠 생성과 고급 챗봇 기능에 매우 유연하게 대처할 수 있습니다.\n- 문맥적 이해. LLMs는 문맥과 관련된 텍스트를 생성하여 텍스트 단락 사이에 일관성을 유지합니다.\n- 지속적 학습. LLMs는 새로운 데이터에 노출되면 언어 능력을 개선하고 확장할 수 있습니다. 그들은 계속해서 신조어와 용어에 적응하고 있습니다.\n\n## LLMs 뒤의 핵심 기술들\n\n대형 언어 모델의 효과성은 그들의 기술적 기반이에 근간을 두고 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 딥 러닝. LLMs는 여러 층의 신경망을 사용하여 자율적으로 학습하고 결정을 내릴 수 있습니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_8.png)\n\n- 트랜스포머 아키텍처. 이러한 모델은 순차적 데이터를 처리하기 위해 설계되어 있으며, 문장에서 다음 단어의 정확한 예측을 가능하게 합니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_9.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 자기주의 메커니즘. LLMs는 각 단어의 중요성을 평가함으로써 관련성 높은 응답을 생성할 수 있습니다.\n- 확장성. 점진적으로 큰 데이터셋으로 LLMs를 학습시킴으로써 그들의 능력을 향상시킬 수 있습니다.\n\n## LLMs의 실용적인 응용\n\nLLMs는 다음과 같은 다양한 분야에서 응용됩니다:\n\n- 콘텐츠 생성. 기사와 보고서 작성부터 시를 창작하는 데 이르기까지.\n- 고객 서비스. 챗봇을 통해 효율적이고 정확한 자동응답을 제공합니다. 예를 들어, ChatGPT 플러그인 개발은 서비스 중심 분야에서 사용자 경험을 향상시킬 수 있습니다.\n- 언어 번역. 언어적 미묘함에 대한 심층적 이해로 LLMs는 전 세계적인 의사소통을 용이하게 할 수 있습니다.\n- 교육 도구. 개인 맞춤형 학습 자료 생성, 숙제 채점, 방대한 텍스트 요약 등을 보조합니다.\n- 의료 분야. 환자 상호작용, 정보 관리, 의료 문서 분석을 지원합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## LLMs의 도전과 윤리적 고려 사항\n\n고급 기능을 가진 LLMs는 신중한 고려가 필요한 제한과 윤리적 딜레마에 직면하고 있습니다:\n\n- 편견과 공정성. LLMs는 기존 데이터에서 학습하기 때문에 공정성과 대표성에 대한 우려가 제기됩니다.\n- 정확성과 신뢰성. 출력물은 때로는 사실적인 정확성보다는 데이터 패턴을 반영할 수 있습니다. 이로 인해 부정확성이나 비논리적인 응답이 발생할 수 있습니다.\n- 진정한 이해 부족. LLMs는 이해를 시뮬레이션하지만 진정한 이해가 부족합니다. 이는 복잡한 상황에서의 오류나 부적절한 출력물로 이어질 수 있습니다.\n- 데이터 프라이버시. 잠재적으로 민감한 데이터를 처리하는 것은 엄격한 데이터 거버넌스의 중요성을 강조합니다.\n- 에너지 소비. NLP처럼, 필요한 중요한 계산 리소스는 환경 및 자원 할당에 관한 우려를 제기합니다.\n\n# Comparative Analysis: NLP vs LLM\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nNLP과 LLM은 언어를 통해 인간-컴퓨터 상호작용을 향상시키는 데 중요한 역할을 합니다. 둘은 공통 목표를 공유하지만 방법론, 능력 및 적용 영역에서 몇 가지 차이가 있습니다. NLP와 LLM의 성능, 확장성, 정확성 및 다양한 분야에서의 유용성에 초점을 맞춰보겠습니다.\n\n## 성능 메트릭\n\nNLP: 구문 분석 및 entity 인식과 같은 전문 작업에서 높은 정확도를 보여줍니다.\n\nLLM: 인간과 비슷한 텍스트 생성 및 다양한 언어 작업을 처리하는 데 뛰어납니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 확장성 및 효율성\n\nNLP: 낮은 계산 요구 사항으로 구체적인 작업을 실행하는 데 더 효율적입니다.\n\nLLM: 다양한 작업을 수행하는 데 매우 확장 가능하며, 더 많은 계산 리소스를 필요로하지만 능숙합니다.\n\n## 정확성 및 신뢰성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n**NLP**: 전문 분야에서 높은 정확도와 신뢰성을 보입니다. 문맥의 풍부한 이해를 요구하는 작업에서는 도전을 겪을 수 있습니다.\n\n**LLM**: 일관된 언어 출력을 생성하는 데 신뢰성을 보입니다. 그러나 훈련 데이터에 영향을 받아 부정확하거나 편향된 내용을 생성할 수도 있습니다.\n\n## 건강 관리에서의 활용성\n\n**NLP**: 의료 기록 처리, 관련 환자 정보 추출 및 예측 진단을 가능하게 하는 데 활용됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLLM: 환자 상호 작용을 용이하게 하고 정보를 전파하며 일반 의학적 조언을 제공함.\n\n## 금융 분야에서의 유용성\n\nNLP: 감성 분석, 위험 평가, 그리고 고객 서비스 향상에 활용됨. 특히 은행 분야에서 생성 형태 AI를 통한 금융언어 처리에 능하다.\n\nLLM: 금융 보고서 작성, 시장 분석 수행, 그리고 고객 서비스 상호작용 자동화에 유용함.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 전자 상거래에서의 사용성\n\nNLP: 챗봇을 통한 고객 경험 향상, 맞춤 추천 및 고객 피드백 분석을 통해 결과를 개선합니다.\n\nLLM: 콘텐츠 생성, 대규모 고객 상호 작용 관리 및 디지털 마케팅의 측면을 자동화하는 데 도움이 됩니다.\n\n# NLP와 LLM 통합을 통한 AI 강화\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nNLP와 LLM의 통합은 고급 언어 처리 시스템 개발에서 중대한 발전을 이룬 것입니다. 이 협력은 NLP의 정확한 능력과 LLM의 광범위한 문맥 지식을 결합합니다. 이는 업계 전반에서 AI 응용 프로그램의 효율성과 효과성을 크게 향상시킬 수 있습니다.\n\n## NLP와 대형 언어 모델 통합의 상호 혜택\n\nNLP를 LLM 기술과 통합하는 것은 여러 가지 핵심 이점을 제공합니다:\n\n- 향상된 정확도와 문맥 이해. NLP의 특정 처리 능력을 LLM의 광범위한 문맥 이해와 결합함으로써 언어 작업을 실행할 때 정확도와 관련성을 높일 수 있습니다.\n- 자원 최적화. NLP의 특정 작업 처리 효율성이 LLM의 자원 집약적인 성격을 보완합니다. 이는 확장 가능한 솔루션과 컴퓨팅 자원의 더 나은 할당을 이끌어냅니다.\n- 증가하는 유연성과 적응성. 이러한 기술의 결합은 AI 응용 프로그램의 유연성과 적응성을 향상시킵니다. 그들은 진화하는 요구 사항에 더 민첩하게 대응할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 현실 세계 통합 성공 사례\n\nNLP와 LLM의 협력 잠재력은 다양한 성공적인 응용 프로그램을 통해 입증되었습니다. 이 협력이 AI 응용 프로그램을 혁신화하는 방식을 살펴보겠습니다:\n\n- 의료 분야. IBM 왓슨은 NLP와 LLM을 사용하여 방대한 의료 데이터를 해석합니다. NLP가 구체적 정보 추출에서의 정확성을 발휘하면서 LLM은 보다 넓은 맥락을 이해하는 능력을 결합합니다. 회사는 통찰력 있는 진단 및 치료 권장을 위해 이를 활용합니다.\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_10.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 금융분야. Bloomberg과 존스홉킨스 대학의 협력 노력을 통해 탄생한 BloombergGPT. 이 모델은 다양한 금융 업무에서 뛰어난 성과를 내기 위해 방대한 데이터셋으로 훈련되었습니다. 이 모델은 연구 확장, 정보 추출, 의사결정 조율, 편향 식별 및 리스크 관리에 도움을 줍니다.\n- 전자상거래분야. Amazon Comprehend는 이 통합을 활용하여 고객 상호작용, 리뷰 및 지원 문의를 분석합니다. 이를 통해 기업은 고객 행위와 선호도를 보다 깊게 이해할 수 있습니다. 이는 제품 검색, 추천, 고객 지원 및 전반적인 만족도 향상으로 이어질 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_11.png)\n\n## NLP 및 LLM 협력의 미래 예측\n\nNLP와 대규모 언어 모델의 지속적인 통합은 새로운 능력과 응용 프로그램을 개방할 것으로 예상됩니다. 확실히 이는 AI 기술과 상호작용하는 방식에 영향을 미칠 것입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- AI 어시스턴트 업그레이드. 미래 AI 어시스턴트는 복잡한 인간 상호작용에 대한 높은 이해도와 반응성을 보일 것으로 예상됩니다. 이는 NLP와 LLM의 통합 덕분에 실현될 것입니다.\n- 자동 컨텐츠 생성 혁신. NLP의 언어 규칙과 LLM의 창의적 능력을 결합하면 더 정교한 컨텐츠 생성 도구가 제공될 것입니다.\n- 로봇의 언어 이해 업그레이드. 이러한 시너지는 로봇의 언어 처리 능력을 크게 향상시킬 수 있습니다. 이는 보다 자연스럽고 효과적인 인간-로봇 상호작용을 이끌어낼 수 있을 것입니다.\n\n# 결론\n\nNLP 대 LLMs는 각각 인간 언어 처리에 대한 독특한 접근 방식을 가지고 있습니다 — NLP는 구체적인 알고리즘 모델링에 초점을 맞추고 LLMs는 대규모 사전 훈련을 통해 포괄적인 능력을 제공합니다 — 그러나 그들은 서로를 잘 보와합니다. 이들의 통합은 더 풍부한 AI 상호작용, 심층적인 산업 통합, 지속적인 AI 윤리 및 기술 발전을 약속합니다. 이러한 기술들의 책임있는 개발과 적용은 매우 중요합니다.\n\n우리가 미래를 바라보며, LLM과 NLP의 교차점은 새로운 AI 기반 솔루션의 시대를 열 것으로 기대됩니다. NLP와 LLM의 잠재력을 탐색하고자 하는 기관들을 위해, Softermii는 이러한 기술을 효과적으로 활용하기 위한 전문 지식과 지원을 제공합니다. 당사 팀에 연락하시어 혁신적이고 윤리적인 AI 애플리케이션을 위한 길을 열어보세요.\n","ogImage":{"url":"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png"},"coverImage":"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png","tag":["Tech"],"readingTime":14},{"title":"클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기","description":"","date":"2024-05-18 20:37","slug":"2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost","content":"\n## Anthropic이 Karpathy의 비디오 요약 도전에 대한 해결책 재현\n\n이 문서를 작성하는데 출발점은 Andrej Karpathy가 LLM 토큰화에 관한 2시간 13분 비디오 강의를 게시한 직후에 X에서 게시한 글입니다. 이 강의를 책 장/chapter 또는 블로그 글로 자동으로 변환하는 작업에 대한 해결책이 필요한 도전을 받았습니다.\n\n![Image](/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png)\n\n그 후에 Anthropic의 Emmanuel Ameisen과 동료들이 특히 Anthropic의 최신 모델인 Claude 3을 통해 이 작업을 수행할 것을 제안하는 것으로 보이는 해결책이 게시되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_1.png)\n\n사소한 문제와 일관성 부족이 있었지만, 이 방법은 꽤 효율적인 것으로 보였습니다. 결과적으로 생긴 블로그 포스트는 원본 비디오에서 다룬 대부분의 요소와 관련 스크린샷 및 코드 예제를 포함했습니다.\n\n이 작업을 재현하는 데 얼마나 쉽고 비싼지 궁금해졌습니다. 결과적으로, 처음에 기대했던 것보다 더 복잡한 과정이었습니다. 프롬프트는 공유되었으나 코드는 그렇지 않았습니다.\n\n본 문서는 내 구현 방식을 공유하고, 각 단계를 자세히 설명하며, 주요 어려움에 대해 논의합니다. 코드 및 데이터는 이 Github 저장소에서 확인할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n요약:\n\n- 비디오를 블로그 포스트/책 챕터로 변환하는 것은 대형 멀티모달 모델(LMMs)의 또 다른 매력적인 활용 사례이며, 비디오 콘텐츠를 텍스트 형식으로 제공하여 읽기 쉽고 빠르게 살펴보고 검색할 수 있게 만듭니다.\n- 그러나 LMMs를 기반으로 한 텍스트 변환은 다양한 부정확성과 불일치가 포함될 수 있어 철저한 검토와 교정이 필요합니다. 다른 어려움은 결과의 재현 불가능성과 효과적인 프롬프트 식별에 관려된 것입니다.\n- Claude 3 Opus와 같은 LMM을 활용하여 비디오를 텍스트 형식으로 변환하는 것은 저렴하지 않습니다. 본 문서에서 제시된 솔루션은 이 블로그 포스트로 이 비디오를 변환하는 데 약 4달러의 비용을 소요했습니다.\n\n# 워크플로 개요 및 기술적 제약사항\n\nClaude 3 Opus는 Anthropic에서 제공하는 최신이자 가장 성능이 뛰어난 대형 멀티모달 모델(LMM)입니다. 이 모델은 3월 4일에 발표되었으며, claude.ai 웹 인터페이스 또는 API를 통해 액세스할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델은 최대 200K 토큰의 텍스트 또는 이미지를 입력으로 받아들일 수 있고, 최대 4K 토큰의 텍스트를 출력할 수 있습니다. 이것이 정확히 무엇을 의미하는지 조금 더 구체적으로 분석해 보겠습니다:\n\n- 출력의 4K 토큰: 한 토큰이 대략 3/4 단어라는 경험 법칙을 고려하면, 4K 토큰은 대략 3K 단어에 해당합니다. 페이지 당 대략 500단어를 가정한다면, 클로드는 최대 6페이지의 텍스트를 출력할 수 있습니다.\n- 입력의 200K 토큰: 동일한 통계를 따르면, 이는 15만 단어(약 300 페이지)에 해당합니다. 초당 대략 2~3단어의 발화 속도를 전제하면, 약 20시간의 오디오 트랜스크립트를 소화할 수 있으며, 이는 상당히 많은 양입니다. 반면, 1280\\*720 픽셀(비디오 HD) 해상도의 이미지를 인코딩하는 데에는 약 1.25K 토큰이 필요합니다. 따라서 한 번에 이론상으로는 150여 장의 이미지를 입력으로 제공할 수 있습니다. 실제로는, 토큰 사용량과는 무관하게, 현재 Anthropi API는 입력 이미지 수를 20장으로 제한하고 있음을 참고해야 합니다.\n\n따라서, 주요 제약 사항은 입력으로 제공할 수 있는 이미지의 제한된 수와 모델이 생성할 수 있는 페이지 수의 제한에 있습니다. 해결책은 비디오를 챕터로 분할하여, 각각이 LMM에 의해 별도로 처리되게 하는 것입니다. 결과물은 그 후에 결합되어 최종 문서를 생성합니다.\n\n아래 다이어그램은 워크플로우의 주요 단계를 요약하고 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAmeisen \u0026 Co는 YouTube 비디오 설명에 제시된 장을 기준으로 비디오를 분할했습니다(총 24장). 다른 전략으로는 LLM과 같은 주제 분할 도구를 활용하여 대본을 주요 부분으로 분할하는 방법이 있습니다. 몇 분 간격의 장을 목표로 설정하는 것이 좋으며, 이를 통해 명령과 대본에 함께 들어갈 10~20개의 스크린샷을 포함할 수 있습니다.\n\n마지막으로 처리 비용을 예상해 봅시다. Claude 3 Opus의 토큰 사용 비용은 입력 토큰당 15달러이며, 출력 토큰당 75달러입니다.\n\n5분 간격의 장을 가정하면, 2시간짜리 비디오는 24개의 장을 제공하는데, 각 장은 평균적으로 다음을 필요로 합니다:\n\n- 13,000개의 입력 토큰(텍스트 토큰 1,000개 및 1.2K 토큰/이미지의 10개)\n- 1,000개의 출력 토큰(2페이지)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n총 입력 토큰은 약 13*2≈300천 개이며, 출력 토큰은 1천 * 24 = 24천 개입니다. 백만 토큰당 비용을 곱하면 입력 비용이 15*0.3=4.5달러, 출력 비용이 75*0.024=1.8달러가 됩니다.\n\n따라서 2시간 비디오에서 게시물을 생성하는 총 비용은 대략 5에서 10달러 정도입니다. 최적화 전략을 사용하여 어떤 스크린샷을 포함할지 신중하게 선택하고 입력 비용을 줄일 수 있습니다.\n\n# 구현\n\n이제 우리의 구현으로 넘어가 봅시다. 이는 우리의 워크플로우에서 제시한 네 가지 주요 단계를 따릅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 비디오를 다운로드하고 텍스트를 획득합니다.\n- 텍스트와 스크린샷을 정렬한 챕터로 분할합니다.\n- 챕터의 LMM 처리를 수행합니다.\n- LMM 출력을 결합하고 블로그 글을 작성합니다.\n\n명확성을 위해 각 단계에 대해 가장 직관적인 구현 방법을 제시합니다. 동반 노트북에는 때로는 데이터를 처리하는 고급 방법을 사용한 추가 코드가 포함될 수 있습니다.\n\n## 비디오를 다운로드하고 오디오 텍스트를 가져옵니다\n\nYouTube에 동영상이 있는 경우, 먼저 pytube 라이브러리를 사용하여 비디오를 다운로드합니다. 나중에 블로그 글을 생성하기 위해 비디오 프레임이 필요하므로 오디오 스트림만이 아닌 전체 비디오를 다운로드합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nimport pytube\n\n# Andrej Karpathy: GPT Tokenizer 만들기 – https://www.youtube.com/watch?v=zduSFxRajkE\nyoutube_video_id = \"zduSFxRajkE\"\ndef download_youtube_video(video_id, output_path):\n    \"\"\"\n    YouTube 비디오를 다운로드하고 output_path에 저장한 후 비디오 ID를 파일 이름으로 반환합니다.\n    \"\"\"\n    # 비디오 ID로 YouTube 객체 생성\n    youtube = pytube.YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n    # 가장 높은 해상도의 비디오 스트림 가져오기\n    stream = youtube.streams.get_highest_resolution()\n    # 비디오 다운로드\n    video_path = stream.download(output_path=output_path, filename=video_id+\".mp4\")\n    return video_path\n# 330MB 비디오에 대해 약 20초 소요\nvideo_path = download_youtube_video(youtube_video_id, DATA_DIR)\n```\n\n대부분의 Youtube 비디오에는 대본이 이미 제공되어 있습니다. youtube_transcript_api와 같은 라이브러리를 사용하여 Youtube 비디오 ID를 제공함으로써 단순히 대본을 얻을 수 있습니다.\n\n```python\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ntranscript = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n```\n\n2시간 13분의 오디오 스트림 전체가 3422개의 세그먼트로 대본화되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```bash\nlen(transcript)\n3422\n\ntranscript[0:4]\n[{'text': \"hi everyone so in this video I'd like us\",  'start': 0.04,  'duration': 4.04}, {'text': 'to cover the process of tokenization in',  'start': 2.04,  'duration': 4.4}, {'text': 'large language models now you see here',  'start': 4.08,  'duration': 4.2}, {'text': \"that I have a set face and that's\",  'start': 6.44,  'duration': 3.88}]\n```\n\n만약 트랜스크립트를 사용할 수 없다면, 오디오 스트림을 음성 인식 모델을 사용하여 텍스트로 변환해야 합니다. 🤗 Open ASR Leaderboard는 성능이 우수한 모델을 찾을 수 있는 좋은 장소입니다. 동반 노트북에 위스퍼 모델과 효율적인 faster-whisper 구현을 사용하여 트랜스크립트를 가져오는 코드를 제공합니다. 이 프로세스는 Google Colab의 T4에서 약 25분이 걸리며(RTX 4090에서는 12분) 완료됩니다.\n\n# 텍스트와 스크린샷이 정렬된 장이로 나누기\n\n장은 수동으로 식별하거나 YouTube가 제공하는 자동 비디오 장 도구와 같은 도구를 사용하여 식별할 수 있습니다. 예제 비디오의 경우, 비디오 설명에 개요된 24개의 장을 복사하여 Python chapters_list 객체에 저장했습니다. 아래에 설명된 것과 같이요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```json\nchapters_list = [\n{'timestamp': 0, 'topic': 'Tokenization을 이해하기 위한 소개 및 동기 부여'},\n {'timestamp': 262, 'topic': 'GPT-2에서 토큰화를 위해 바이트 수준 인코딩을 소개한 논문 소개'},\n {'timestamp': 933, 'topic': '유니코드, UTF-8 인코딩 및 어휘 크기'}\n...\n]\n```\n\n그런 다음이 단계의 핵심은 챕터의 시작/끝 타임스탬프에 따라 텍스트와 스크린샷을 추출하는 것입니다. 이것은 chop_up_in_chapters 함수에 의해 구현되며 각 챕터마다 챕터의 시작 및 끝 타임스탬프를 식별하고 트랜스크립트에서 해당 텍스트를 추출하여 비디오에서 스크린샷을 추출합니다.\n\n스크린샷 추출 전략은 각 주어진 챕터에 대해 최대 10장의 스크린샷을 균일하게 샘플링하여 추출하되, 스크린샷 간에 최소 한 분이 경과하도록합니다.\n\n추출된 텍스트 및 스크린샷은 별도의 폴더에 저장됩니다(챕터 번호를 이름으로 사용).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\ndef chop_up_in_chapters(chapters_list, video_path, transcript, timestamps_screenshots_list_seconds=None):\n    \"\"\"\n    비디오를 장(chapter)으로 나눕니다. 비디오 장(chapter) 목록을 기준으로 합니다.\n    \"\"\"\n    n_chapters=len(chapters_list)-1\n    print(f\"장 수: {n_chapters}\")\n    # 타임스탬프와 주제에 대해 반복합니다.\n    for current_chapter in range(n_chapters):\n        output_dir=CHAPTERS_DIR+\"/\"+str(current_chapter)\n         # 해당 출력 디렉토리가 없으면 생성합니다.\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        # 현재와 다음 타임스탬프를 가져옵니다.\n        current_chunk_start_time=chapters_list[current_chapter]['timestamp']\n        current_chunk_end_time=chapters_list[current_chapter+1]['timestamp']-1\n        print(f\"장 {current_chapter}; 시작: {current_chunk_start_time}, 끝: {current_chunk_end_time}\")\n        # 현재 장에 대한 텍스트 및 프레임을 추출합니다.\n        get_text_chapter(transcript, current_chunk_start_time, current_chunk_end_time, output_dir)\n\n        if timestamps_screenshots_list_seconds is not None:\n            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir,timestamps_screenshots_list_seconds[current_chapter])\n        else:\n            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir)\n```\n\n# 대규모 다중모달 모델(LLM) 처리\n\n이것은 핵심 단계입니다. 각 장마다 오디오 대본과 선택한 스크린샷이 LMM에 제공되어 이 입력 데이터를 교과서에 포함할 수 있는 출력으로 변환하는 것이 목표입니다.\n\n이 단계의 핵심 요소는 우리가 다음과 같이 설계한 LLM 프롬프트입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nprompt_instructions = \"\"\"\n\u003cinstructions\u003e\n동영상의 이미지를 다른 타임 스탬프로 제공했으며, \u003ctranscript\u003e 안에 오디오 대본도 포함되어 있습니다.\n대본은 인공지능 음성 인식 도구에 의해 생성되었으며 일부 오류/불일치가 있을 수 있습니다.\n귀하의 작업은 대본을 마크다운 블로그 포스트로 변환하는 것입니다.\n이 대본은 소음이 많습니다. 다음 가이드라인을 사용하여 블로그 장을 위한 마크다운 형식으로 다시 작성하십시오:\n- 유효한 마크다운 출력\n- 적절한 곳에 섹션 제목 및 다른 서식 삽입\n- 대사 블록의 일부만 제공되어 주요 주제만 포함하세요. 소개나 결론 단락은 포함하지 마세요. 대사에서 논의된 주요 주제만 포함해주세요.\n- 이미지, 텍스트, 코드, 강조 및 페이지 레이아웃 및 여백을 일반적인 블로그 게시물 또는 교과서와 같이 보이도록 스타일링\n- 말투적인 속성을 제거하십시오\n- 중복 정보가 있는 경우 반복되는 정보는 한 번만 제시해주세요\n- 대화식 내용을 유지하되 대화의 구조를 따를 수 있도록 제목을 포함하세요\n- 각 대본에는 너무 많은 이미지가 포함되어 있으므로 출력에는 가장 중요한 1-2개의 이미지만 포함해주세요\n- 대사와 관련된 일부를 시각화하는 데 도움이 되는 이미지를 선택해주세요\n- 이미지를 선택할 때는 대사에서 설명한 것과 관련된 완전한 코드를 표시하는 이미지를 선호해주세요\n- 이미지가 대본의 일부를 설명하는 데 도움이 될 경우 포함해주세요\n- 이미지를 포함하려면, \u003cimg src=\"xxxxx.jpg\"/\u003e 태그를 삽입하며, 여기서 xxxxx는 이미지 데이터 위에 삽입된 정확한 이미지 타임스탬프로 대체되어야 합니다\n- 불필요한 정보를 추가하지 마세요. 대본이나 이미지에서 언급된 사항만 포함해주세요\n최종 출력물은 교과서에 포함하기 적합해야 합니다.\n\u003c/instructions\u003e\n\"\"\"\n```\n\n우리는 주로 Ameisen의 안내를 재사용했습니다. 다음과 같은 수정을 가했습니다.\n\n- LMM 출력을 결합하기 위해 출력 형식을 HTML에서 마크다운으로 변경하여 더 직관적으로 만들었습니다 (그리고 마크다운 형식은 블로그 게시물에 시각적으로 잘 어울립니다).\n- 출력 형식이 마크다운으로 정의되었음에도 불구하고 시각적인 요소와 작성 스타일 이미지를 제거했습니다. 이들이 더 유용한 정보를 추가하지 않는다는 결론에 이른 후였습니다.\n- 프롬프트에서 일부 서식을 Anthropic의 가이드라인에 더 잘 따르도록 변경했습니다. 특히, 앞부분에 있는 스크린샷을 이동시키고 지침을 XML 태그로 래핑했습니다.\n\n프롬프트는 장의 스크린샷 및 대본 앞에 오고 있습니다. 우리는 JPG 스크린샷을 Anthropic의 비전 API에 적합한 형식으로 변환하기 위해 `get_screenshots_as_messages` 도우미 함수를 정의했습니다. 이 함수는 모든 스크린샷을 반복하여 각각에 대한 두 가지 메시지를 설명합니다: 스크린샷의 타임스탬프를 지정하는 텍스트 메시지와 그것의 base64로 인코딩된 표현을 포함하는 이미지 메시지입니다. 나중에 하이퍼링크가 추가된 최종 문서에서 원본 비디오로 이동할 수 있게 해주는 타임스탬프가 포함된 텍스트 메시지입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndef get_screenshots_as_messages(screenshots):\n  screenshots_as_messages = []\n  for i in range(len(screenshots)):\n    screenshots_as_messages.extend([\n    {\n    \"type\": \"text\",\n    \"text\": f\"The timestamp for the following image is {Path(screenshots[i]).stem}.\"\n    },\n    {\n    \"type\": \"image\",\n    \"source\": {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": base64.b64encode(open(screenshots[i], \"rb\").read()).decode(\"utf-8\"),\n      }\n    }])\n  return screenshots_as_messages\n```\n\n우리는 스크린샷, 대본 및 지침을 모아서 함께 가져오는 또 다른 도우미 함수인 get_prompt_as_messages를 정의했습니다. 이 함수는 추가로 Claude의 출력을 미리 채워 마크다운 제목(\"#\")으로 답변을 시작하도록 만듭니다.\n\n```js\ndef get_prompt_as_messages(chapter_id):\n    folder_path=CHAPTERS_DIR+'/'+str(chapter_id)\n    with open(folder_path+'/transcript.txt', \"r\") as f:\n        transcript = f.read()\n    screenshots=sorted(glob.glob(folder_path+'/*.jpg'))\n\n    screenshots_as_messages=get_screenshots_as_messages(screenshots)\n    prompt_as_messages = [\n        {\n            \"role\": \"user\",\n            \"content\": screenshots_as_messages+\n            [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"\u003ctranscript\u003e\\n{transcript}\\n\u003c/transcript\u003e\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt_instructions\n                }\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"#\"\n                }\n            ]\n        }\n    ]\n    return prompt_as_messages\n```\n\n그게 다야!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모든 챕터는 클로드를 반복적으로 호출하여 처리한 후 결과를 해당 챕터 폴더에 있는 마크다운 파일로 작성할 수 있습니다.\n\n```js\n# 챕터 목록을 반복하여 처리\nfor chapter in range(len(chapters_list)-1):\n  # 현재 챕터에 대한 프롬프트 생성 (스크린샷, 대본 및 지침이 포함된 메시지 목록).\n    prompt_generate_markdown = get_prompt_as_messages(chapter)\n    # 프롬프트를 사용하여 메시지 생성하기\n    message = client.messages.create(\n        model=\"claude-3-opus-20240229\",\n        system=\"당신은 마크다운 블로그 글쓰기 전문가입니다.\",\n        temperature=0,\n        max_tokens=4000,\n        messages=prompt_generate_markdown\n    )\n    # 응답에서 생성된 마크다운 콘텐츠 추출\n    answer = message.content[0].text\n    markdown = \"#\"+answer  # 마크다운 콘텐츠 앞에 헤더 태그 추가\n\n    # 현재 챕터에 해당하는 마크다운 파일 경로 정의\n    markdown_file = CHAPTERS_DIR + '/' + str(chapter) + '/markdown.md'\n    # 생성된 마크다운 콘텐츠를 파일에 작성\n    with open(markdown_file, \"w\") as f:\n        f.write(markdown)\n```\n\n아래는 Anthropic의 사용 로그 중 마지막 일곱 챕터 처리에 대한 부분을 보고, 처리 시간 및 입력 및 출력 토큰 수의 변동을 감지할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_2.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n가장 긴 장은 마지막에서 두 번째 장이었습니다 (여기에서 장을 확인하세요), 1시 51분부터 2시 10분까지 총 17689 토큰을 처리하는 데 거의 1분이 걸렸습니다. 전체적으로 비디오 및 24 장을 처리하는 데 약 10분이 소요되었고, 18만 토큰의 입력 및 1만 5천 토큰의 출력이 사용되었습니다. 이 과정은 약 4달러의 비용이 소요되었습니다.\n\n# LMM 출력을 결합하여 최종 블로그 포스트 생성\n\n워크플로우의 최종 단계는 주로 두 가지 작업으로 구성됩니다. 먼저 다양한 마크다운 출력을 병합합니다. 그 다음으로는 장 제목 및 이미지에 하이퍼링크를 추가합니다. 이를 통해 최종 마크다운 파일을 해당 타임스탬프에서 원본 YouTube 비디오에 연결할 수 있습니다.\n\n```js\nmerged_markdown=\"\"\n\n# 장 폴더를 반복하여 마크다운 파일 병합\nfor chapter in range(len(chapters_list)-1):\n    markdown_file=CHAPTERS_DIR+'/'+str(chapter)+'/markdown.md'\n    with open(markdown_file, \"r\") as f:\n        markdown = f.readlines()\n    # 각 장 제목에 해당하는 비디오의 링크를 추가\n    url_chapter = f\"https://www.youtube.com/watch?v={youtube_video_id}\u0026t={chapters_list[chapter]['timestamp']}s\"\n    markdown[0] = f\"# [{chapter+1}) {markdown[0][2:].strip()}]({url_chapter})\"\n    markdown = '\\n'.join(markdown)\n    merged_markdown+=\"\\n\"+markdown\n# 병합된 마크다운에서 src 속성의 타임스탬프가 포함된 모든 \u003cimg\u003e 태그를 찾아 해당 비디오의 링크를 추가\ntimestamps_screenshots = re.findall(r'\u003cimg src=\"(\\d+)\\.jpg\"/\u003e', merged_markdown)\ntimestamps_screenshots = [timestamp for timestamp in timestamps_screenshots]\n# 각 이미지에 해당하는 타임스탬프에서 올바른 비디오 링크를 추가\nfor timestamp in timestamps_screenshots:\n    video_link = f'\u003ca href=\"https://www.youtube.com/watch?v={youtube_video_id}\u0026t={int(timestamp)}s\"\u003e비디오 링크\u003c/a\u003e'\n    merged_markdown = merged_markdown.replace(f'\u003cimg src=\"{timestamp}.jpg\"/\u003e', f'\u003cimg src=\"{timestamp}.jpg\"/\u003e\\n\\n{video_link}')\n# 병합된 마크다운에서 이미지를 기반으로한 프레임을 추출하고 merge 폴더에 저장\nget_frames_chapter(video_path, None, None, MERGE_DIR, timestamps_screenshots=timestamps_screenshots)\n# 병합된 마크다운을 markdown 블로그포스트.md 파일로 저장\nmarkdown_file=MERGE_DIR+'/blogpost.md'\nwith open(markdown_file, \"w\") as f:\n        f.write(merged_markdown)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n결합된 마크다운 파일은 MERGE_DIR 폴더에 모든 선택한 JPG 스크린샷과 함께 'markdown.md'로 저장됩니다 (최종 출력).\n\n# 토의\n\n결과적으로 포스트는 원래 비디오의 대부분의 내용을 성공적으로 보존하여, Ameisen 및 그의 동료가 설명한 것과 유사한 품질을 달성했습니다. 또한, 의미 있는 스크린샷 및 코드 조각을 정확하게 식별하여 오디오 대본의 이해를 돕습니다. 그러나, 텍스트 변환이 정확하지 않은 부분을 비롯해 일부 결함이 있습니다.\n\n정확하지 않거나 모순된 부분을 해결하기 위해 철저한 편집과 교정이 여전히 필요합니다. (Ameisen의 작업에 발견된 것과 유사한) 문제의 예로는 예를 들어, \"hello world\" 토큰을 2가 아닌 300으로 잘못 세는 오류, \"tokenization\"의 첫 번째 토큰을 잘못 번호 매기는 오류, 공백을 토큰으로 오도록 잘못 인식하는 등이 있습니다 (블로그 포스트의 2장 참조). 이러한 부정확성 외에도, 이 방법론은 효과적인 프롬프트를 만드는 복잡성, 결과의 재현 불가능성 및 LMM 운영에 따른 비용과 같은 다른 어려움을 야기합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하지만 이러한 단점에도 불구하고 비디오를 접근 가능하고 쉽게 탐색할 수 있는 텍스트 블로그 포스트로 변환하는 것은 대형 다중 모달 모델의 가치 있는 응용 프로그램입니다. 특히 경쟁하는 LMM(대형 다중 모달 모델)인 GPT4-V, Gemini Pro Vision 및 오픈 소스 대규모 월드 모델의 비디오/이미지 이해 능력과 비교는 내일의 블로그 포스트 주제가 될 것입니다.\n\n# 유용한 링크\n\n- 동반자 Github 저장소\n- Karpathy의 도전과 Ameisen 및 동료의 저장소\n- 토큰화에 대한 비디오 튜토리얼 : https://www.youtube.com/watch?v=zduSFxRajkE 및 손으로 쓴 튜토리얼 요약 : https://github.com/karpathy/minbpe/blob/master/lecture.md\n- Claude 3 — Vision 문서\n- Misbah Syed의 다른 접근 방식\n\n참고: 별도로 표시되지 않는 한, 모든 이미지는 작성자가 제공한 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 게시물을 즐겼나요? 생각을 공유하거나 박수를 보내거나 LinkedIn에서 저와 연락하세요.\n","ogImage":{"url":"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png"},"coverImage":"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png","tag":["Tech"],"readingTime":19},{"title":"스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기","description":"","date":"2024-05-18 20:36","slug":"2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels","content":"\nSpeechBrain은 음성 처리를 위해 설계된 오픈 소스, 올인원 툴킷입니다. PyTorch를 기반으로 구축되었으며 음성 인식, 화자 식별 및 음성 향상을 포함한 다양한 음성 관련 작업을 위한 포괄적인 도구 모음을 제공합니다. 모듈식이며 유연한 구조로 연구자와 개발자 모두에게 접근하기 쉽도록 설계되어 최신 음성 처리 모델로의 신속한 개발과 실험을 가능하게 합니다.\n\nSpeechBrain은 2021년에 음성 기술 분야를 발전시키고자 열정을 가진 연구자와 엔지니어들에 의해 세상에 소개되었습니다. 이 프로젝트는 오픈 소스의 성격과 견고한 성능으로 빠르게 주목을 받았습니다. 몇 년 동안, SpeechBrain은 전 세계 AI 커뮤니티로부터 다수의 기여를 받아 음성 처리의 최신 기술 발전을 지속적으로 통합해왔습니다.\n\nSpeechBrain은 매우 모듈식으로, 사용자가 자신의 요구에 따라 툴킷을 쉽게 사용자 정의하고 확장할 수 있습니다. 데이터 전처리부터 모델 훈련 및 평가까지 음성 처리 파이프라인의 각 구성 요소는 독립적으로 수정하거나 대체할 수 있습니다. 이 툴킷은 특정 작업에 대해 즉시 사용하거나 미세 조정할 수 있는 다양한 사전 훈련된 모델을 제공합니다. 이러한 모델은 최신 아키텍처에 기반하고 대규모 데이터셋에서 훈련되어 높은 성능을 보장합니다.\n\n# 음성 분리를 위해 SpeechBrain 사용하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nJupyter Notebook에 SpeechBrain을 설치하는 방법부터 시작하겠습니다.\n\n```js\n!pip install speechbrain\n```\n\n그런 다음 필요한 라이브러리를 가져올 것입니다.\n\n```js\nfrom speechbrain.inference.separation import SepformerSeparation as separator\nimport torchaudio\nfrom IPython.display import Audio\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 HuggingFace에서 모델을 다운로드할 것입니다. 우리는 SpeechBrain으로 구현된 sepformer-wham16k-enhancement 모델을 사용할 예정입니다.\n\n```js\nmodel = separator.from_hparams(\n  (source = \"speechbrain/sepformer-wham16k-enhancement\"),\n  (savedir = \"pretrained_models/sepformer-wham16k-enhancement\")\n);\n```\n\n이제 모델을 사용하여 오디오에서 배경 소음을 분리할 준비가 끝났습니다.\n\n```js\naudio_sources = model.separate_file((path = \"original_audio.wav\"));\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델은 텐서 출력을 생성합니다. 우리는 torchaudio를 사용하여 텐서를 오디오 파일로 변환할 수 있습니다.\n\n```python\ntorchaudio.save(\"converted_audio.wav\", audio_sources[:, :, 0], 16000)\n```\n\n# 샘플 오디오 파일로 테스트하기\n\nSpeechBrain의 효과를 테스트하기 위해 Wikimedia에서 제공하는 두 개의 샘플 오디오 파일을 사용할 것입니다: 배경 소음이 많이 섞인 파일과 배경 소음이 적은 파일입니다. 이 오디오를 SpeechBrain을 사용하여 처리하고 결과를 비교할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 샘플 오디오 1: 가벼운 백그라운드 소음\n\n오디오 원본:\n\n이 오디오 파일의 출처 페이지입니다. 우리는 처음 10초를 사용하고 있습니다.\n\n처리된 오디오:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 샘플 오디오 2: 강한 백그라운드 소음\n\n원본 오디오:\n\n이 오디오 파일의 출처 페이지입니다. 처음 10초를 사용하고 있습니다.\n\n처리된 오디오:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 발견 사항\n\nSpeechBrain의 소음 제거 능력을 평가한 결과가 복합적이었습니다. 낮은 배경 소음이 포함된 샘플에 모델을 적용했을 때, 모델은 만족스럽게 작동하여 소음을 효과적으로 줄이고 음성의 선명도를 향상시켰습니다. 이는 SpeechBrain이 적당한 수준의 소음을 다루는 데 강점을 가지고 있으며, 일상적인 오디오 개선 작업에 유용한 도구임을 보여줍니다.\n\n그러나, 고 배경 소음이 포함된 샘플에서는 모델이 어려움을 겪었습니다. 일부 소음을 줄이기는 했지만, 출력물은 오디오 클리핑으로 인해 음성의 이해를 저해하는 문제가 있었습니다. 이는 SpeechBrain의 현재 소음 제거 모델이 극도로 시끄러운 배경 환경에 적합하지 않을 수 있다는 것을 나타냅니다.\n\n# 이상적인 사용 사례\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이러한 결과를 고려해봤을 때, SpeechBrain은 다음과 같은 경우에 적합할 것으로 생각됩니다:\n\n- 팟캐스트 및 보이스오버: 비교적 조용한 환경에서 녹음하는 콘텐츠 크리에이터들에게, SpeechBrain은 오디오를 깔끔하게 정리하여 전문적인 음질을 보장할 수 있습니다.\n- 원격 회의 및 통화: 주변 소음이 적당한 전문적인 환경에서는 SpeechBrain이 발화의 명확성을 향상시킬 수 있어서 참여자들이 방해 없이 서로 이해하기 쉬워집니다.\n- 교육 컨텐츠: 교육자 및 온라인 강좌 제작자들은 SpeechBrain을 사용하여 녹화한 콘텐츠를 깔끔하게 정리함으로써 강의와 발표가 명확하고 이해하기 쉽도록 할 수 있습니다.\n\n위의 테스트와 결과에 대해 어떻게 생각하시는지 아래에 알려주세요.\n\n모두를 위한 오픈 소스 대화형 AI. SpeechBrain. (출처: https://speechbrain.github.io/)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSpeechbrain/Sepformer-WHAM16K-enhancement · Hugging Face. speechbrain/sepformer-wham16k-enhancement · Hugging Face. (n.d.). [https://huggingface.co/speechbrain/sepformer-wham16k-enhancement](https://huggingface.co/speechbrain/sepformer-wham16k-enhancement)\n","ogImage":{"url":"/assets/img/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels_0.png"},"coverImage":"/assets/img/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels_0.png","tag":["Tech"],"readingTime":6},{"title":"자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기","description":"","date":"2024-05-18 20:32","slug":"2024-05-18-UsinglangchainforQuestionAnsweringonOwnData","content":"\n## langchain을 사용하여 자신의 데이터로 대화하는 단계별 안내서\n\n대형 언어 모델은 학습된 주제에 관한 질문에 대답할 수 있습니다. 그러나 그들은 우리의 개인 데이터나 회사의 소유 문서 또는 LLM 학습 이후에 작성된 기사에 관한 질문에 대답할 수 없습니다. 우리 자신의 문서와 대화하고 이러한 문서에서 질문에 대답하는 데 LLM을 사용할 수 있다면 정말 멋질 것입니다. 본 문서는 LangChain: Chat with Your Data라는 강의에서 Andrew Ng 교수와 LangChain 창립자 Harrison Chase로부터 대부분의 내용을 가져왔습니다. 이 글은 langchain에 관한 세 번째 글입니다. 첫 번째 글은 langchain이 LLM 응용 프로그램 개발에 어떻게 사용될 수 있는지에 대해 논의하고 있습니다. 두 번째 글은 LLM 응용 프로그램 개발에 체인과 에이전트를 사용하는 방법에 대해 논의하고 있습니다.\n\nLangChain은 LLM 응용 프로그램을 구축하기 위한 오픈 소스 개발자 프레임워크입니다. 본 글에서는 LangChain의 특정 사용 사례인 LangChain을 사용하여 자신의 데이터와 대화하는 방법에 중점을 둘 것입니다. 이 글에서는 주로 다음 주제를 다룰 것입니다:\n\n- 문서 로딩\n- 문서 분할\n- 벡터 저장소 및 임베딩\n- 검색\n- 질문 응답\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 문서 로딩\n\n검색 증강 생성 (RAG) 프레임워크에서 LLM은 실행 중 일환으로 외부 데이터셋에서 문맥적 문서를 검색합니다. 이는 우리가 특정 문서 (예: PDF, 비디오 등)에 대해 질문하고 싶을 때 유용합니다. 데이터와 대화할 수 있는 애플리케이션을 만들고 싶다면 먼저 데이터를 작업할 수 있는 형식으로 로드해야 합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png)\n\n이를 위해 LangChain의 문서 로더를 사용합니다. 문서 로더는 다양한 형식과 소스에서 데이터에 액세스하고 변환하는 구체적인 사항을 다룹니다. 구조화된 데이터 소스나 구조화되지 않은 데이터 소스에서 로드할 수 있습니다. 예를 들어 웹사이트, 데이터베이스, YouTube, arxiv, Twitter, Hacker News 또는 Figma, Notion과 같은 소유 데이터 소스 또는 Airbyte, Stripe, Airtable과 같은 소스에서 데이터에 액세스하고 로드해야 할 수 있습니다. 이러한 문서는 pdf, html, json, word, PowerPoint과 같은 다양한 데이터 유형이거나 표 형식일 수 있습니다. 문서 로더는 이러한 데이터 소스로부터 데이터를 가져와 내용과 관련 메타데이터로 구성된 표준 문서 객체로 로드합니다. 또한 langchain에는 아래에서 확인할 수 있는 80가지가 넘는 다양한 문서 로더가 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Screenshot](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_1.png)\n\n## PyPDF DataLoader\n\n이제 PyPDF 로더를 사용하여 pdf를 로드할 것입니다. Andrew Ng의 유명한 CS229 강의에서 MachineLearning-Lecture01.pdf를 로드할 것입니다.\n\n```js\nfrom langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n\n#Load the document by calling loader.load()\npages = loader.load()\n\nprint(len(pages))\nprint(pages[0].page_content[0:500])\n\nprint(pages[0].metadata)\n# {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이것은 문서 목록을 불러옵니다. 이 경우에는 PDF에 22개의 서로 다른 페이지가 있습니다. 각 페이지는 문서이며 문서에는 페이지 콘텐츠와 메타데이터가 포함되어 있습니다. 페이지 콘텐츠는 페이지의 내용이며 메타데이터 요소는 각 문서와 관련된 메타데이터가 포함되어 있습니다.\n\n## YouTube DataLoader\n\nLangChain은 YouTube에서 비디오를 불러오는 YoutubeAudioLoader를 제공합니다. 이 loader를 사용하여 비디오나 강의에서 질문을 하거나 할 수 있습니다.\n\n```js\nfrom langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders.parsers import OpenAIWhisperParser\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n\nurl=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\nsave_dir=\"docs/youtube/\"\nloader = GenericLoader(\n    YoutubeAudioLoader([url],save_dir),\n    OpenAIWhisperParser()\n)\ndocs = loader.load()\n\nprint(docs[0].page_content[0:500])\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nYouTubeAudioLoader는 YouTube 링크에서 오디오 파일을 로드하고 OpenAIWhisperParser를 사용합니다. OpenAIWhisperParser는 OpenAI의 음성 대 텍스트 Whisper 모델을 사용하여 YouTube 오디오를 작업할 수 있는 텍스트 형식으로 변환합니다. YouTube URL과 오디오 파일을 저장할 디렉토리를 지정해야 합니다.\n\n## WebBaseLoader\n\nWebBaseLoader는 인터넷에서 URL을 로드하는 데 사용됩니다.\n\n```js\nfrom langchain.document_loaders import WebBaseLoader\n\n# 깃허브 페이지에서 마크다운 파일 사용\nloader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")\n\ndocs = loader.load()\nprint(docs[0].page_content[:500])\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위에 나온 출력물에는 많은 공백이 있으므로 해당 출력물에 후처리를 해야 합니다.\n\n## NotionDirectoryLoader\n\nNotionDirectoryLoader을 사용하여 Notion에서 데이터를 로드합니다. Notion은 개인 및 회사 데이터를 저장하기 위한 인기 있는 툴입니다. Notion Space에서 페이지를 복제하고 페이지를 마크다운/CSV 파일로 내보낼 수 있습니다.\n\n```js\nfrom langchain.document_loaders import NotionDirectoryLoader\nloader = NotionDirectoryLoader(\"docs/Notion_DB\")\ndocs = loader.load()\n\nprint(docs[0].page_content[0:200])\nprint(docs[0].metadata)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금까지 우리는 다양한 소스에서 데이터를로드하고 표준화 된 문서 인터페이스로 가져 오는 방법에 대해 다뤘습니다. 그러나 이러한 문서가 크다면, 작은 청크로 나누어야 할 수도 있습니다. 이것은 검색 확장 생성의 경우, 우리가 우리에게 가장 관련이있는 콘텐츠 조각들을 검색해야하기 때문에 중요합니다.\n\n# 문서 분할\n\n문서 분할은 문서를 작은 청크로 나누는 것이 필요합니다. 문서 분할은 데이터를 표준화 된 문서 형식으로로드 한 후에 일어나지만 벡터 저장소로 이동하기 전에 발생합니다.\n\n![image](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n문서를 작은 조각으로 나누는 것은 중요하며 각 조각 사이의 의미 있는 관계를 유지해야 하는 부분이 까다롭습니다. 예를 들어, Toyota Camry에 대한 2개의 조각이 있다면:\n\n이 경우에 간단히 나누었더니 한 조각에 문장의 일부가, 다른 조각에는 다른 부분이 포함되어 있습니다. 따라서 Camry의 사양에 관한 질문에 대답할 수 없게 될 수 있습니다. 따라서 의미론적으로 관련된 조각으로 나누는 것이 중요합니다.\n\n이제 아래와 같이 RecursiveCharacterTextSplitter와 CharacterTextSplitter를 초기화합니다:\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n\nchunk_size =26\nchunk_overlap = 4\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n입력 텍스트는 지정된 청크 크기 및 정의된 청크 중첩에 따라 분할됩니다. 청크 크기는 청크의 크기를 측정하는 길이 함수입니다. 이는 주로 문자 또는 토큰입니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_3.png)\n\n청크 중첩은 두 청크 사이에 작은 중첩이 있도록 사용되며, 이는 2개의 청크 사이에 어떤 일관성 개념을 가질 수 있도록 합니다. 페이지에 나와 있는 것처럼 Lang Chain에는 다양한 유형의 스플리터가 있습니다:\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLang Chain의 Text Splitters에는 문서 생성 및 문서 분할이라는 2가지 메소드가 있습니다. 둘 다 내부에서 동일한 논리를 가지고 있지만 하나는 텍스트 목록을, 다른 하나는 문서 목록을 입력으로 받습니다. 이러한 텍스트 분할기들은 청크를 나누는 방식(문자 또는 토큰으로)이나 청크의 길이를 측정하는 방식과 같은 다양한 차원에서 다를 수 있습니다. 때로는 문장의 끝을 결정하기 위해 다른 작은 모델을 사용하고 해당 정보를 사용하여 청크를 분할할 수도 있습니다. 메타데이터는 텍스트/문서를 청크로 분할할 때 중요합니다. 모든 청크를 통일된 메타데이터를 유지하면서 새로운 메타데이터 조각을 추가해야 할 수도 있습니다. 때로는 청크의 분할이 문서 유형에 특정할 수 있습니다. 코드에서 분할할 때 나타날 수 있습니다. Python, Ruby, C와 같은 서로 다른 언어에 대해 서로 다른 분리자를 사용하는 언어 텍스트 분할기를 사용합니다.\n\n이제 LangChain의 몇 가지 텍스트 분할기 예제를 살펴보겠습니다.\n\n```js\n# Recursive text Splitter\ntext1 = 'abcdefghijklmnopqrstuvwxyz'\nr_splitter.split_text(text1)\n# Output - ['abcdefghijklmnopqrstuvwxyz']\n\n# Character Text Splitter\ntext2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\nr_splitter.split_text(text2)\n# Output - ['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']\n\n# Recursive text Splitter\ntext3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\nr_splitter.split_text(text3)\n# output - ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n\n# Character Text Splitter\nc_splitter.split_text(text3)\n# output - ['a b c d e f g h i j k l m n o p q r s t u v w x y z']\n\n# Character Text Splitter with separator defined\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separator = ' '\n)\nc_splitter.split_text(text3)\n# Output - ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n```\n\n## RecursiveSplitting\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 실제 예시 몇 가지를 시도해 보겠습니다. 재귀 텍스트 분할기와 문자 텍스트 분할기가 어떻게 다르게 작동하는지 살펴볼 것입니다.\n\n```js\nsome_text = \"\"\"When writing documents, writers will use document structure to group content. \\\nThis can convey to the reader, which idea's are related. For example, closely related ideas \\\nare in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\nParagraphs are often delimited with a carriage return or two carriage returns. \\\nCarriage returns are the \"backslash n\" you see embedded in this string. \\\nSentences have a period at the end, but also, have a space.\\\nand words are separated by space.\"\"\"\n\nlen(some_text) -\u003e 496\n\nc_splitter = CharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separator = ' '\n)\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n```\n\n여기서 CharacterTextSplitter는 공백을 구분자로 사용하며, RecursiveCharacterTextSplitter의 경우 구분자 목록을 전달합니다.\n\n첫 번째 경우에, 다음 출력이 나옵니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```json\n[\n  \"문서를 작성할 때 작성자는 문서 구조를 사용하여 콘텐츠를 그룹화합니다. 이는 독자에게 관련된 아이디어를 전달할 수 있습니다. 예를 들어, 밀접하게 관련된 아이디어는 문장에 있습니다. 비슷한 아이디어들은 단락에 있습니다. 단락은 문서를 형성합니다. \\n\\n 단락은 종종 개행 문자 또는 두 개의 개행 문자로 구분됩니다. 개행 문자는 이 문자열에 포함된 \\\"백슬래시 n\\\"입니다. 문장은 끝에 마침표가 있지만 또한 띄어쓰기를 하고 단어들은 띄어쓰기로 구분됩니다.\"\n]\n```\n\n두 번째 경우에 대한 출력은 다음과 같습니다:\n\n```json\n[\n  \"문서를 작성할 때 작성자는 문서 구조를 사용하여 콘텐츠를 그룹화합니다. 이는 독자에게 관련된 아이디어를 전달할 수 있습니다. 예를 들어, 밀접하게 관련된 아이디어는 문장에 있습니다. 비슷한 아이디어들은 단락에 있습니다. 단락은 문서를 형성합니다.\",\n  \"단락은 종종 개행 문자 또는 두 개의 개행 문자로 구분됩니다. 개행 문자는 이 문자열에 포함된 \\\"백슬래시 n\\\"입니다. 문장은 끝에 마침표가 있지만 띄어쓰기를 하고 단어들은 띄어쓰기로 구분됩니다.\"\n]\n```\n\nRecursiveCharacterTextSplitter의 경우, 구분자 목록으로는 두 번의 개행, 한 개의 개행, 공백, 빈 문자열이 있습니다. 따라서 이는 텍스트를 두 번의 개행으로 분할하고, 그 다음으로 한 개의 개행 뒤에 공백이 따라오는 경우와 마지막으로 문자별로 분할합니다. RecursiveTextSplitter는 이중 개행을 기준으로 텍스트를 분할하므로 텍스트가 두 개의 단락으로 분할됩니다. 첫 번째 단락이 450자보다 짧은 것을 확인할 수 있으며, 이는 두 번의 개행을 기준으로 분할하는 것이 더 나은 분할인 것으로 생각됩니다. 문자 텍스트 분할은 띄어쓰기에 따라 분할되므로, 문장 중간에 이상한 구분이 발생하는 것을 확인할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n자 이제 PDF 파일을 사용한 TextSplitter의 실제 예제를 하나 더 실행해 보겠습니다.\n\n```js\nfrom langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\npages = loader.load()\n\nfrom langchain.text_splitter import CharacterTextSplitter\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\",\n    chunk_size=1000,\n    chunk_overlap=150,\n    length_function=len\n)\n\ndocs = text_splitter.split_documents(pages)\n\nlen(docs) -\u003e 77\nlen(pages) -\u003e 22\n```\n\n여기서는 Python의 기본 길이 함수를 전달한 것입니다.\n\n## 토큰 분할\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금까지 문자 기반으로 텍스트를 분할해 왔습니다. 이제 토큰 수를 기준으로도 분할할 수 있습니다. 이는 LLMs에서 종종 토큰으로 지정된 컨텍스트 창을 가지고 있기 때문에 유용할 수 있습니다. 토큰은 일반적으로 ~4개의 문자로 이루어져 있습니다.\n\n```js\nfrom langchain.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n\ntext1 = \"foo bar bazzyfoo\"\ntext_splitter.split_text(text1)\n# ['foo', ' bar', ' b', 'az', 'zy', 'foo']\n```\n\n```js\ntext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\ndocs = text_splitter.split_documents(pages)\n\ndocs[0]\n# Document(page_content='MachineLearning-Lecture01  \\n', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0})\n\npages[0].metadata\n# {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n```\n\n## 컨텍스트에 따라 분할하기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n체킹의 목적은 공통 컨텍스트를 가진 텍스트를 함께 묶는 것입니다. 텍스트 분할은 종종 문장이나 다른 구분자를 사용하여 관련된 텍스트를 함께 유지하지만 많은 문서(예: Markdown)는 구조(헤더)가 있으므로 분할에 명시적으로 사용할 수 있습니다.\n\n이를 위해 헤더 텍스트 스플리터를 사용하여 헤더 메타데이터를 유지하는 목적으로 마크다운 파일을 분할할 수 있습니다. 헤더나 하위 헤더를 기반으로 마크다운 파일을 나누고 해당 헤더를 메타데이터 필드에 내용으로 추가하여 해당 분할에서 파생된 모든 청크로 전달됩니다.\n\nMarkdown 형식의 표를 다음과 같이 변경해보겠습니다:\n\n```js\nfrom langchain.document_loaders import NotionDirectoryLoader\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter\n```\n\n제목이 있는 문서와 그 후 부제목 (장 1)과 여러 문장이 있는 섹션이 있습니다. 그런 다음 다른 섹션에 부제목 (장 2)과 그곳에 몇 개의 문장이 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nmarkdown_document = \"\"\"# Title\\n\\n \\\n## Chapter 1\\n\\n \\\n안녕하세요, 제임스입니다\\n\\n \\\n안녕하세요, 조입니다\\n\\n \\\n### 섹션\\n\\n \\\n안녕하세요, 랜스입니다\\n\\n \\\n## Chapter 2\\n\\n \\\n안녕하세요, 말리입니다\"\"\"\n\n\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n```\n\n이제 MarkdownHeaderTextSplitter를 정의합니다.\n\n```js\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n  (headers_to_split_on = headers_to_split_on)\n);\nmd_header_splits = markdown_splitter.split_text(markdown_document);\n```\n\n마지막으로, 텍스트 분할 결과를 얻습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nmd_header_splits[0]\n# 문서(page_content='안녕하세요, 제 이름은 짐입니다  \\n안녕하세요, 제 이름은 조입니다', metadata={'Header 1': '제목', 'Header 2': '장 1'})\n\nmd_header_splits[1]\n# 문서(page_content='안녕하세요, 제 이름은 랜스입니다', metadata={'Header 1': '제목', 'Header 2': '장 1', 'Header 3': '섹션'})\n```\n\n의미론적으로 관련있는 청크를 적절한 메타데이터와 함께 얻을 수 있었습니다. 이제 이 데이터 청크를 벡터 저장소로 이동할 것입니다.\n\n# 벡터 저장소와 임베딩\n\n우리는 문서를 작은 청크로 나누었고, 이제 이러한 청크를 색인에 넣어두어 이 문서에 대한 질문에 답변할 때 쉽게 검색할 수 있도록 합니다. 이를 위해 임베딩과 벡터 저장소를 사용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_5.png)\n\n텍스트 분할 이후에는 벡터 저장소와 임베딩이 필요합니다. 문서를 쉽게 접근할 수 있는 형식으로 저장해야 합니다. 임베딩은 텍스트를 가져와 텍스트의 수치적 표현을 만듭니다. 즉, 의미론적으로 유사한 내용을 가진 텍스트는 임베딩 공간에서 유사한 벡터를 가집니다. 따라서 임베딩(벡터)을 비교하고 유사한 텍스트를 찾을 수 있습니다.\n\n전체 파이프라인은 문서로 시작합니다. 이러한 문서를 작은 덩어리로 분할하고 이러한 분할 또는 문서의 임베딩을 만듭니다. 마지막으로, 모든 이러한 임베딩을 벡터 저장소에 저장합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_6.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n벡터 저장소는 나중에 비슷한 벡터를 쉽게 찾을 수 있는 데이터베이스입니다. 질문과 관련 있는 문서를 찾을 때 유용합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_7.png)\n\n따라서 질문에 대한 답변을 얻고 싶을 때, 우리는 질문의 임베딩을 만들고 이를 벡터 저장소 내 모든 다른 벡터들과 비교하여 가장 유사한 n개를 선택합니다. 마지막으로, n개의 가장 유사한 청크를 질문과 함께 LLM에 전달하고 답변을 얻습니다.\n\n이제 우리는 어떻게 문서 세트를 벡터 저장소에 로드하는지 알아봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nfrom langchain.document_loaders import PyPDFLoader\n\n# PDF 파일 로드하기\nloaders = [\n    # 일부러 문서를 중복하여 넣어 지저분한 데이터를 만듭니다.\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n]\ndocs = []\nfor loader in loaders:\n    docs.extend(loader.load())\n```\n\n문서가 로드된 후 RecursiveCharacterTextSplitter를 사용하여 청크를 생성합니다.\n\n```js\n# 텍스트 스플리터 정의\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=150\n)\n\n# 텍스트 스플리터를 사용해 문서를 나눕니다.\nsplits = text_splitter.split_documents(docs)\n```\n\n이제 PDF의 모든 청크에 대한 임베딩을 생성한 다음 벡터 저장소에 저장할 것입니다. 우리는 OpenAI를 사용하여 이러한 임베딩을 만들 것입니다. 우리는 Chroma를 우리의 경우에 벡터 저장소로 사용할 것입니다. Chroma는 가벼우며 메모리에 저장되어 쉽게 시작할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nembedding = OpenAIEmbeddings()\n```\n\n저희는 이 벡터 저장소를 지속적인 디렉토리에 저장하여 나중에 사용할 수 있도록 합니다.\n\n```js\npersist_directory = 'docs/chroma/'\n\n# 벡터 저장소 생성\nvectordb = Chroma.from_documents(\n    documents=splits,\n    embedding=embedding,\n    persist_directory=persist_directory\n)\n\nprint(vectordb._collection.count())\n```\n\n저희는 이전에 생성된 splits, embedding (OpenAI 임베딩 모델), 그리고 지속 디렉토리를 전달하여 벡터 저장소를 생성합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 유사성 검색\n\n유사성 검색 방법을 사용하여 k를 전달하고 반환할 문서의 수를 지정하는 질문을 하겠습니다.\n\n```js\nquestion = \"도움을 요청할 수 있는 이메일이 있나요\"\n\ndocs = vectordb.similarity_search(question,k=3)\n\n# 문서의 길이 확인\nlen(docs)\n\n# 첫 번째 문서의 내용 확인\ndocs[0].page_content\n\n# 나중에 사용하기 위해 데이터베이스 유지\nvectordb.persist()\n```\n\n## 유사성 검색: 극단적인 경우\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기본적인 유사성 검색은 대부분의 결과를 올바르게 가져옵니다. 그러나 유사성 검색이 실패하는 특이한 경우도 있습니다. 이제 다른 쿼리를 수행하여 중복 결과를 확인할 것입니다.\n\n```js\nquestion = \"matlab에 대해 어떻게 말했나요?\"\n\n# k = 5로 유사성 검색\ndocs = vectordb.similarity_search(question,k=5)\n\n# 처음 두 결과 확인\nprint(docs[0])\nprint(docs[1])\n```\n\n여기서 처음 두 결과는 중복 PDF(duplicate MachineLearning-lecture01.pdf)를 로드했기 때문에 동일합니다. 따라서 중복 청크를 받아서 언어 모델로 넘겼습니다. 코드를 실행하면 의미론적 검색이 모든 비슷한 문서를 가져오지만 다양성은 보장하지 않는다는 결론을 내릴 수 있습니다. 다음 섹션에서 어떻게 관련성이 있으면서도 다른 청크를 동시에 가져오는 방법을 다루겠습니다.\n\n다른 쿼리로 유사성 검색에서 다른 실패 사례가 있을 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nquestion = \"세 번째 강의에서 회귀에 대해 무슨 이야기를 했나요?\"\n\ndocs = vectordb.similarity_search(question,k=5)\n\n\n# 유사성 검색 결과의 메타데이터를 출력합니다\nfor doc in docs:\n    print(doc.metadata)\n\nprint(docs[4].page_content)\n```\n\n우리는 검색 결과의 메타데이터, 즉 이 결과가 어떤 강의에서 나왔는지 확인했습니다. 우리는 결과가 세 번째 강의, 두 번째 강의 및 첫 번째 강의에서 나온 것을 볼 수 있습니다. 이 실패의 이유는 우리가 제 3 강의에서만 문서를 원하는 사실이 구조화된 정보 조각이지만, 우리는 임베딩을 기반으로 의미적 조회만을 수행하고 있으며 임베딩은 아마도 '회귀'라는 단어에 더 초점을 맞추고 세 번째 강의에 대한 정보를 캡처하지 못합니다. 따라서 우리는 회귀와 관련이 있는 모든 결과를 받고 있습니다. 이를 확인하기 위해 다섯 번째 문서를 출력하여 실제로 회귀라는 단어가 언급되었는지 확인할 수 있습니다.\n\n# 검색\n\n검색은 검색 보강 생성(RAG) 플로우의 핵심입니다. 문서에서 질문에 대답을 시도할 때 가장 큰 어려움 중 하나는 검색입니다. 질문에 대한 답변이 실패하는 대부분의 경우, 그 원인은 검색에서 실수하는 것입니다. 또한 LangChain에서 자체 쿼리 및 맥락 압축과 같은 고급 검색 메커니즘에 대해 논의할 것입니다. 검색은 쿼리가 입력되고 가장 관련성이 높은 분할을 검색하고자 할 때 쿼리 시간에 중요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![img](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_8.png)\n\n의미 검색이 많은 사용 사례에 대해 꽤 잘 작동했음을 확인했습니다. 그러나 일부 극단적인 상황에서 실패했습니다. 따라서 우리는 검색을 깊이 파고들어 이러한 극단적인 상황을 극복하기 위한 몇 가지 다른 고급 방법을 논의할 것입니다.\n\n- 벡터 스토어에서 데이터 액세스/색인화\n- 기본 의미 유사성\n- 최대 주변 유사성\n- 메타데이터 포함\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 2. LLM 지원 검색\n\n## 3. 문맥적 압축\n\n## 1. 최대 주변 유사성(MMR)\n\nMMR은 검색 결과에서 다양성을 강화하는 중요한 방법입니다. 의미적 검색의 경우, 임베딩 공간에서 쿼리와 가장 유사한 문서를 얻게 되며, 우리는 다양한 정보를 놓칠 수 있습니다. 예를 들어, 쿼리가 \"큰 열매체를 가진 모든 흰색 버섯에 대해 말해주세요\"인 경우, 첫 두 번째로 유사한 결과를 얻어 쿼리와 관련된 정보인 과일체와 모두 흰색에 대한 정보를 얻을 것입니다. 그러나 첫 두 문서와 유사하지 않지만 중요한 정보를 놓칠 수 있습니다. 여기서 MMR은 다양한 문서를 선택하는데 도움을 주어 이 문제를 해결하는 데 도움이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_9.png)\n\nMMR의 아이디어는 먼저 벡터 저장소를 쿼리하고 \"fetch_k\" 가장 유사한 응답을 선택하는 것입니다. 이제 \"fetch_k\" 문서의 작은 집합에서 작업을 수행하여 쿼리에 대한 관련성과 결과물 간의 다양성을 동시에 달성합니다. 마지막으로, 이러한 \"fetch_k\" 응답 중에서 \"k\"개의 가장 다양한 응답을 선택합니다. 처음 2개의 문서의 처음 100자를 출력하면, 위와 같은 유사도 검색을 사용할 경우 동일한 결과를 얻는 것을 발견할 것입니다. 이제 MMR로 검색 쿼리를 실행하고 처음 몇 가지 결과를 확인해 보겠습니다.\n\n```js\ntexts = [\n    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n]\n\nsmalldb = Chroma.from_texts(texts, embedding=embedding)\nquestion = \"Tell me about all-white mushrooms with large fruiting bodies\"\nsmalldb.max_marginal_relevance_search(question, k=2, fetch_k=3)\n```\n\n여기서 우리는 MMR 검색을 사용하여 결과를 다양하게 만들 수 있었습니다. 이제 유사도 검색과 최대 여유성 검색 결과를 비교해 보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 유사도 검색과 MMR 검색 결과를 비교하세요.\n\n```bash\nquestion = \"matlab에 대해 말한 내용은 무엇인가요?\"\ndocs_ss = vectordb.similarity_search(question, k=3)\ndocs_ss[0].page_content[:100]\ndocs_ss[1].page_content[:100]\n\ndocs_mmr = vectordb.max_marginal_relevance_search(question, k=3)\ndocs_mmr[0].page_content[:100]\ndocs_mmr[1].page_content[:100\u003e\n```\n\n유사도 검색에서 처음 2개 문서의 처음 100자가 동일한 것을 확인할 수 있지만, MMR 검색으로는 처음 2개 문서의 처음 100자가 다른 것을 확인할 수 있습니다. 따라서 쿼리 결과에서 다양성을 얻을 수 있습니다.\n\n## 2. 메타데이터\n\n메타데이터는 검색의 특이성을 조정하는 데 사용됩니다. 이전에 \"세 번째 강의에서 회귀에 대해 어떤 내용을 이야기했습니까?\"라는 질문에 대한 답변이 세 번째 강의뿐만 아니라 첫 번째와 두 번째 강의에서도 반환된 것을 발견했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 문제를 해결하기 위해 메타데이터 필터를 지정할 것입니다. 많은 벡터 저장소는 메타데이터에 대한 작업을 지원합니다. 따라서 소스가 세 번째 강의 PDF와 같아야 한다는 정보를 전달할 것입니다. 여기서 메타데이터는 각 포함된 청크에 대한 맥락을 제공합니다.\n\n```js\nquestion = \"세 번째 강의에서 회귀에 대해 얘기한 내용은 무엇인가요?\"\n\ndocs = vectordb.similarity_search(\n    question,\n    k=3,\n    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n)\n\n# 검색된 문서의 메타데이터 출력\nfor d in docs:\n    print(d.metadata)\n```\n\n이제 검색된 문서의 메타데이터를 살펴보면 모든 문서가 세 번째 강의에서 검색되었음을 확인할 수 있습니다.\n\n## 3. 직접 쿼리\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n셀프 쿼리는 쿼리 자체에서 메타데이터를 추론하려는 경우 중요한 도구입니다. 우리는 SelfQueryRetriever를 사용할 수 있습니다. 이는 LLM을 사용하여 다음을 추출합니다.\n\n- 벡터 검색에 사용할 쿼리 문자열\n- 전달할 메타데이터 필터\n\n여기서는 메타데이터를 기준으로 결과를 필터링하기 위해 언어 모델을 사용합니다. 하지만, 이전에 수동으로 필터를 지정할 필요는 없고 대신 메타데이터와 함께 셀프 쿼리 리트리버를 사용할 수 있습니다.\n\n```js\nfrom langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 방법은 우리가 의미론적으로 찾으려는 콘텐츠 이외에도 적용할 메타데이터도 포함하는 쿼리를 가지고 있을 때 사용됩니다.\n\n메타데이터에는 source와 page 두 필드가 있습니다. 이러한 속성 각각에 대해 이름과 유형에 대한 설명을 제공해야 합니다. 이 정보는 언어 모델에서 사용되므로 이 설명을 최대한 상세하게 만들어야 합니다.\n\n```js\nmetadata_field_info = [\n  AttributeInfo(\n    (name = \"source\"),\n    (description =\n      \"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\"),\n    (type = \"string\")\n  ),\n  AttributeInfo(\n    (name = \"page\"),\n    (description = \"The page from the lecture\"),\n    (type = \"integer\")\n  ),\n];\n```\n\n또한 문서 저장소에 실제로 들어 있는 정보에 대해 구체적으로 명시해야 합니다. 여기서 LLM은 메타데이터 필터와 함께 전달해야 하는 쿼리를 추론합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndocument_content_description = \"강의 노트\";\nllm = OpenAI((temperature = 0));\nretriever = SelfQueryRetriever.from_llm(\n  llm,\n  vectordb,\n  document_content_description,\n  metadata_field_info,\n  (verbose = True)\n);\n```\n\n이제 다음 질문으로 검색기를 실행합니다.\n\n```js\nquestion = \"세 번째 강의에서 회귀에 대해 언급한 내용은 무엇인가요?\";\ndocs = retriever.get_relevant_documents(question);\n```\n\n예를 들어, \"1980년에 만들어진 외계인 영화는 어떤 것들이 있나요?\"라는 쿼리를 가질 수 있습니다. 이 쿼리는 2가지 구성 요소를 가지고 있으며, 원본 질문을 메타데이터 필터와 검색 용어로 나누기 위해 언어 모델을 사용할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Using langchain for Question Answering on Own Data](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_10.png)\n\n예를 들어, 이 경우에는 영화 데이터베이스에서 외계인을 검색하고 각 영화의 메타데이터를 1980년도로하는 형태로 필터링합니다. 대부분의 벡터 스토어는 메타데이터 필터를 지원하기 때문에 새로운 데이터베이스나 인덱스가 필요하지 않습니다. 대부분의 벡터 저장소가 메타데이터 필터를 지원하므로, 예를 들어 영화의 년도가 1980년인 경우와 같이 메타데이터를 기반으로 레코드를 쉽게 필터링할 수 있습니다.\n\n## 4. 컨텍스트 압축\n\n압축은 검색된 문서의 품질을 향상시키는 또 다른 방법입니다. 전체 문서를 응용 프로그램을 통해 전달하면 더 많은 비용이 들고 더 나쁜 응답이 될 수 있으므로 검색된 단락의 가장 관련성이 높은 부분만 추출하는 것이 유용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\ndef pretty_print_docs(docs):\n    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n\n\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\n# Wrap our vectorstore\nllm = OpenAI(temperature=0)\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever()\n)\n\nquestion = \"what did they say about matlab?\"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\npretty_print_docs(compressed_docs)\n\n\nThrough compression, all documents are processed using a language model to extract the most relevant segments, which are then passed into a final language model call.\n```\n\n![Using langchain for Question Answering on Own Data](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_11.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이는 랭귀지 모델을 더 많이 호출할 수 있게 되는 대신, 최종 답변을 가장 중요한 내용에만 집중할 수 있도록 도와줍니다. 그래서 이것은 조금은 교환해야 할 부분이 있습니다.\n\n# 질문응답\n\n우리는 검색에서 방금 검색한 문서를 사용하여 질문응답을 하는 방법을 논의했습니다. 이제 이러한 문서와 원래의 질문을 가져와서 랭귀지 모델로 전달하여 모델에게 질문에 답변을 하도록 요청합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_12.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 검색 QA 체인\n\n먼저 벡터 저장소에서 여러 관련 분할이 검색된 후 질문 응답을 어떻게 수행하는지 살펴봅니다. 또한 관련 분할을 LLM 컨텍스트에 맞게 압축해야 할 수도 있습니다. 마지막으로 이러한 분할을 시스템 프롬프트와 인간 질문과 함께 언어 모델로 전송하여 답변을 가져옵니다.\n\n![image](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_13.png)\n\n기본적으로 우리는 모든 청크를 동일한 컨텍스트 창에, 동일한 언어 모델 호출에 넣습니다. 그러나 문서 수가 많을 경우 모두를 동일한 컨텍스트 창에 넣을 수 없을 때 다른 방법도 사용할 수 있습니다. MapReduce, Refine 및 MapRerank는 문서 수가 많을 경우 사용할 수 있는 세 가지 방법입니다. 이제 우리는 이러한 방법을 자세히 살펴보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이전에 유지했던 벡터 데이터베이스를 먼저 로드할 거에요.\n\n```js\n# 이전에 유지했던 벡터 데이터베이스를 로드하고 컬렉션 수를 확인해요\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\npersist_directory = 'docs/chroma/'\nembedding = OpenAIEmbeddings()\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\nprint(vectordb._collection.count())\n```\n\n데이터베이스가 제대로 작동하는지 확인하기 위해 유사성 검색을 먼저 수행할 거에요.\n\n```js\nquestion = \"이 수업의 주요 주제는 무엇인가요?\";\ndocs = vectordb.similarity_search(question, (k = 3));\nlen(docs);\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 우리는 이 질문에 대한 답변을 얻기 위해 RetrievalQA 체인을 사용할 것입니다. 첫 번째로, 언어 모델 (ChatOpenAI 모델)을 초기화합니다. 온도를 영으로 설정합니다. 온도를 영으로 설정하면 모델이 낮은 변이성, 최고의 충실도 및 신뢰할 수 있는 답변을 얻기에 좋습니다.\n\n```js\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(model_name=llm_name, temperature=0)\n```\n\n또한 리트리벌QA 체인이 필요합니다. 이것은 리트리버를 사용하여 지원되는 질문 답변을 수행하는 과정입니다. 언어 모델과 벡터 데이터베이스를 리트리버로 전달하여 생성됩니다.\n\n```js\nfrom langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever()\n)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금, 우리는 묻고 싶은 질문을 가지고 qa_chain을 호출합니다.\n\n```js\n# 질문을 qa_chain에 전달\nquestion = \"이 수업의 주요 주제는 무엇인가요?\"\nresult = qa_chain({\"query\": question})\nresult[\"result\"]\n```\n\n## Prompt로 RetrievalQA 체인 사용하기\n\n이제 좀 더 자세히 이 프로세스를 이해해 봅시다. 먼저 prompt 템플릿을 정의합니다. prompt 템플릿에는 컨텍스트를 사용하는 방법에 대한 지침이 포함되어 있습니다. 또한 컨텍스트 변수를 위한 자리 표시자도 있습니다. 우리는 질문에 대한 답변을 얻기 위해 prompts를 사용할 것입니다. 여기서 prompt는 문서와 질문을 받아서 언어 모델에 전달합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nfrom langchain.prompts import PromptTemplate\n\n# Prompt 템플릿 생성\ntemplate = \"\"\"아래의 문맥을 사용하여 마지막 질문에 대답하세요. 답을 모른다면 그냥 모르는 것으로 말하고 대답을 꾸미지 마세요. 세 문장 이내로 답하세요. 답변은 최대한 간결하게 유지하세요. 답변 끝에 \"질문해 줘서 고마워!\"라고 항상 말씀해 주세요.\n{context}\n질문: {question}\n도움이 되는 답변:\"\"\"\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n\n# 체인 실행\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n)\n```\n\n언어 모델과 벡터 데이터베이스를 사용하여 새로운 검색 QA 체인을 생성했어요.\n\n```js\n# 체인 초기화\n# source 문서를 받으려면 return_source_documents를 True로 설정하세요\n# chain_type을 prompt template으로 정의하세요\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n)\n```\n\n이번에는 새로운 질문을 시도해보고 결과를 확인해 볼 거예요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n질문 = \"확률은 수업 주제인가요?\"\n결과 = qa_chain({\"query\": 질문})\n# 쿼리 결과 확인\n결과[\"result\"]\n# 참고 문서 확인\n결과[\"source_documents\"][0]\n```\n\n지금까지 기본적으로 \"stuff\" 메서드를 사용했습니다. 이 방법은 모든 문서를 최종 프롬프트에 넣습니다. 이는 언어 모델에 한 번의 호출만을 의미합니다. 그러나 문서가 너무 많은 경우, 문서가 컨텍스트 창 안에 맞지 않을 수 있습니다. 이러한 경우, 맵-리듀스, 정제, 그리고 map_rerank와 같은 다른 기술을 사용할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_14.png\" /\u003e\n\n이 기술에서 각 개별 문서는 먼저 언어 모델로 전송되어 원본 답변을 얻은 후, 이러한 답변이 최종 답변으로 구성되며 최종적으로 언어 모델에 대한 호출이 이루어집니다. 이는 더 많은 언어 모델 호출을 필요로 하지만, 임의의 많은 문서에 대해 작동할 수 있는 장점이 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 방법의 두 가지 제한 사항이 있습니다. 첫째, 이전 방법보다 속도가 느리고 둘째, 결과물이 이전 결과물보다 나쁠 수 있습니다. 이는 두 개의 문서에 정보가 분산되어 있을 때 동일한 맥락에서 정보가 나타나지 않을 수 있기 때문입니다.\n\n```js\nqa_chain_mr = RetrievalQA.from_chain_type(\n  llm,\n  (retriever = vectordb.as_retriever()),\n  (chain_type = \"map_reduce\")\n);\nresult = qa_chain_mr({ query: question });\nresult[\"result\"];\n```\n\nRetrievalQA 체인이 하부에서 MapReduceDocumentsChain을 호출할 때 발생합니다. 이는 각 문서에 대해 언어 모델(ChatOpenAI의 경우)에 대해 네 번의 별도 호출을 수행합니다. 이러한 호출의 결과는 최종 체인(StuffedDocumentsChain)에 결합되며, 이는 이러한 응답을 모두 최종 호출에 삽입합니다. StuffedDocumentsChain은 시스템 메시지, 이전 문서의 네 가지 요약 및 사용자 질문을 사용하여 답변을 얻습니다.\n\n```js\nqa_chain_mr = RetrievalQA.from_chain_type(\n  llm,\n  (retriever = vectordb.as_retriever()),\n  (chain_type = \"refine\")\n);\nresult = qa_chain_mr({ query: question });\nresult[\"result\"];\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 우리가 검색을 위해 \"refine\"을 체인 유형으로 사용한다면, RetrievalQA 체인은 RefineDocumentsChain을 호출하며, 이는 LLM 체인에 대한 네 번의 연속적인 호출을 수반합니다. 이 네 번의 호출 각각은 언어 모델로 전송되기 전에 프롬프트를 포함합니다. 이 네 번의 호출에는 이전에 정의된 프롬프트 템플릿에 따른 시스템 메시지가 포함됩니다. 시스템 메시지에는 문맥 정보, 검색한 문서 중 하나, 사용자 질문 및 답변이 포함됩니다. 우리는 다음 언어 모델을 호출합니다. 다음 언어 모델에 보내는 최종 프롬프트는 이전 응답과 새 데이터를 결합하고 추가된 문맥을 포함하여 향상된/정제된 응답을 요청하는 시퀀스입니다. 이 작업은 이전 응답을 새 데이터와 결합하여 정보를 순차적으로 결합함으로써 정보의 지속 전달을 더 많이 유도하는 정제 체인에서 더 나은 답변을 얻을 수 있습니다. 이 과정은 네 번 실행되며 최종 답변에 도달하기 전 모든 문서를 대상으로 실행됩니다. Refine 체인에서는 정보를 순차적으로 결합하여 정보의 지속적 전달이 더 많아져 MapReduce 체인보다 더 나은 답변을 얻을 수 있습니다.\n\n## RetrievalQA 한계\n\nRetrievalQA 체인의 가장 큰 단점 중 하나는 QA 체인이 대화 내력을 보존하지 못하는 것입니다. 이를 확인할 수 있습니다:\n\n```js\n# QA 체인 생성\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever()\n)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 체인에 질문하겠습니다.\n\n```js\nquestion = \"확률은 수업 주제입니까?\";\nresult = qa_chain({ query: question });\nresult[\"result\"];\n```\n\n이제 두 번째 질문을 체인에 하겠습니다.\n\n```js\nquestion = \"왜 해당 선수과목이 필요한가요?\";\nresult = qa_chain({ query: question });\nresult[\"result\"];\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이전 답변과 관련이 없는 체인으로부터 답변을 받을 수 있었습니다. 기본적으로 RetrievalQA 체인은 상태 개념을 가지고 있지 않습니다. 이전 질문이나 이전 답변이 무엇이었는지 기억하지 않습니다. 이전 질문이나 이전 답변을 기억하게 하려면 메모리 개념을 도입해야 합니다. 이전 질문이나 이전 답변을 기억하는 능력은 챗봇의 경우 필요합니다. 챗봇에 후속 질문을 할 수 있거나 이전 답변에 대해 설명을 요청할 수 있기 때문입니다.\n\nLangChain을 사용하여 다양한 문서에서 데이터를로드하는 방법에 대해 토론했습니다. 또한 문서를 청크로 분할하는 방법을 배웠습니다. 그 후에 이러한 청크에 대한 임베딩을 생성하고 벡터 저장소에 저장했습니다. 이 벡터 저장소를 사용하여 의미 검색을 수행했습니다. 의미 검색은 특정 엣지 케이스에서 실패합니다. 그런 다음, 엣지 케이스를 극복하기 위한 다양한 검색 알고리즘을 논의하는 검색을 다루었습니다. 우리는 검색을 LLM과 결합하여 질문 응답에서 사용했는데, 여기서 우리는 검색된 문서와 사용자 질문을 가져와 LLM에 전달하여 우리가 한 질문에 대한 답변을 생성했습니다. 질문 응답의 대화식 측면에 대해서는 논의하지 않았으며, 나중에 데이터 위에 종단간 챗봇을 생성함으로써 그에 대해 나중에 논의할 것입니다.\n","ogImage":{"url":"/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png"},"coverImage":"/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png","tag":["Tech"],"readingTime":39}],"page":"87","totalPageCount":110,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":4},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"87"},"buildId":"-dPCbnM2yhdKNgXe92VJV","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>