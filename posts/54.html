<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/54" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/54" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_buildManifest.js" defer=""></script><script src="/_next/static/YUMR4jSyk_WlOHHc7UfOk/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="오픈에이아이가 스타트업을 압도할 것 같다고 합니다 모델이 더 나아질 때 우리는 무엇을 개발해야 할까요" href="/post/2024-05-23-OpenAIwillsteamrollstartupswhatshouldwebuildasthemodelsgetbetter"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="오픈에이아이가 스타트업을 압도할 것 같다고 합니다 모델이 더 나아질 때 우리는 무엇을 개발해야 할까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-OpenAIwillsteamrollstartupswhatshouldwebuildasthemodelsgetbetter_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="오픈에이아이가 스타트업을 압도할 것 같다고 합니다 모델이 더 나아질 때 우리는 무엇을 개발해야 할까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">오픈에이아이가 스타트업을 압도할 것 같다고 합니다 모델이 더 나아질 때 우리는 무엇을 개발해야 할까요</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="알고 있는 것은 기억하는 것과도 같아요" href="/post/2024-05-23-ToKnowIsAlsotoRemember"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="알고 있는 것은 기억하는 것과도 같아요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="알고 있는 것은 기억하는 것과도 같아요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">알고 있는 것은 기억하는 것과도 같아요</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Ontologies 및 언어 모델을 활용하여 정확한 질문 응답하기" href="/post/2024-05-23-LeveragingOntologiesandLanguageModelsforAccurateQuestionAnswering"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Ontologies 및 언어 모델을 활용하여 정확한 질문 응답하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-LeveragingOntologiesandLanguageModelsforAccurateQuestionAnswering_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Ontologies 및 언어 모델을 활용하여 정확한 질문 응답하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">Ontologies 및 언어 모델을 활용하여 정확한 질문 응답하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="깊은 학습 모델이 GPU에서 더 빨리 실행되는 이유 CUDA 프로그래밍 간단 소개" href="/post/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="깊은 학습 모델이 GPU에서 더 빨리 실행되는 이유 CUDA 프로그래밍 간단 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="깊은 학습 모델이 GPU에서 더 빨리 실행되는 이유 CUDA 프로그래밍 간단 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">깊은 학습 모델이 GPU에서 더 빨리 실행되는 이유 CUDA 프로그래밍 간단 소개</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">23<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="재밌는 주제 프로덕션 환경에서 신뢰할 수 있는 딥러닝 스택 구축하는 방법 이에 대해 알아보도록 하죠" href="/post/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="재밌는 주제 프로덕션 환경에서 신뢰할 수 있는 딥러닝 스택 구축하는 방법 이에 대해 알아보도록 하죠" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="재밌는 주제 프로덕션 환경에서 신뢰할 수 있는 딥러닝 스택 구축하는 방법 이에 대해 알아보도록 하죠" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">재밌는 주제 프로덕션 환경에서 신뢰할 수 있는 딥러닝 스택 구축하는 방법 이에 대해 알아보도록 하죠</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="자기 주의적 문장 임베딩을 사용한 추천 시스템" href="/post/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자기 주의적 문장 임베딩을 사용한 추천 시스템" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자기 주의적 문장 임베딩을 사용한 추천 시스템" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">자기 주의적 문장 임베딩을 사용한 추천 시스템</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="포디즘의 새로운 모습" href="/post/2024-05-23-ThenewfaceofFordism"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="포디즘의 새로운 모습" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-ThenewfaceofFordism_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="포디즘의 새로운 모습" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">포디즘의 새로운 모습</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로봇 공학의 미래를 위한 P2P 통신 로봇 에이전트 007 릴리스" href="/post/2024-05-23-P2PCommunicationfortheFutureofRoboticsRobotAgent007Release"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로봇 공학의 미래를 위한 P2P 통신 로봇 에이전트 007 릴리스" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-P2PCommunicationfortheFutureofRoboticsRobotAgent007Release_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로봇 공학의 미래를 위한 P2P 통신 로봇 에이전트 007 릴리스" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">로봇 공학의 미래를 위한 P2P 통신 로봇 에이전트 007 릴리스</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="M1 M2 Mac 실리콘에 LinuxUbuntu 2004 설치하기" href="/post/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="M1 M2 Mac 실리콘에 LinuxUbuntu 2004 설치하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="M1 M2 Mac 실리콘에 LinuxUbuntu 2004 설치하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">M1 M2 Mac 실리콘에 LinuxUbuntu 2004 설치하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="새로운 냉전 - 인공지능" href="/post/2024-05-23-TheNewColdWarArtificialIntelligence"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="새로운 냉전 - 인공지능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-TheNewColdWarArtificialIntelligence_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="새로운 냉전 - 인공지능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">새로운 냉전 - 인공지능</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">17<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link posts_-active__YVJEi" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"오픈에이아이가 스타트업을 압도할 것 같다고 합니다 모델이 더 나아질 때 우리는 무엇을 개발해야 할까요","description":"","date":"2024-05-23 17:25","slug":"2024-05-23-OpenAIwillsteamrollstartupswhatshouldwebuildasthemodelsgetbetter","content":"\n![이미지](/assets/img/2024-05-23-OpenAIwillsteamrollstartupswhatshouldwebuildasthemodelsgetbetter_0.png)\n\n만약 AI 제공 업체가 다른 회사들이 그들의 모델을 사용하여 단순히 특정 앱을 개발하고, 제공 업체가 영역에 절대 뛰어들지 않는 비즈니스 도메인을 발전시키길 원한다면 좋겠죠. 시장 전체에 온통 존재하기보다 더 똑똑한 전략이겠죠? 결국 공통 플랫폼 전략이죠.\n\n현실적으로, AI 제공 업체는 종합적인 시장 채택을 위해 그들의 길을 질주하며 모든 것을 압도할 것입니다. 기술 플랫폼 및 생태계 발전에는 빠르게 변화되는 패턴이 있습니다; 예를 들어 AWS의 플랫폼 지배력 증가.\n\n하지만 오늘날은 조금 다릅니다. 지능적이고 에이전트 자동화는 이제 비즈니스 및 특정 영역으로 확장되는 복잡한 변수입니다 (인프라 제공 업체로서 하지 않았던 AWS). 즉, 대형 업체들이 모든 것을 일반적으로 해결하기 위해 다른 것을 만들 때 어떻게 무언가를 만들 것인가요? 결국 일반 에이전트는 그런 목적으로 의도된 것이죠.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 그럼 무엇을 만들까요?\n\n오늘 이 질문에 대한 대답은 어려우며 내일의 대답을 예측하는 것도 어렵습니다. 우리는 매 분기마다 새로운 고급 능력의 형태를 보고 있습니다. 지난 주 OpenAI는 AI와의 실시간 대화를 보여주며, 실시간 언어 번역과 같은 다양한 능력을 예시로 들었습니다. 그 다음 날, Duolingo의 주가가 하락했습니다. 그 후, OpenAI는 Google 시트의 자동 분석 및 생성을 시연하며, 적어도 5개의 개발자 중심의 \"데이터 분석 에이전트\" 스타트업을 가려냈으며, 데이터 분석가 자체에게는 잠재적인 경력 단축 시나리오를 시사했습니다.\n\n간단하게 말씀드리면, LLM(Large Language Models)이 직접 상호 작용할 수 없는 복잡한 작업 흐름을 향상시키도록 만들어야 합니다. 여러분은 고급 LLM 사용을 조율하는 데 그치지 않고 비즈니스 프로세스와 기타 시스템과 통합해야 합니다. 더 많은 접착제를 만들수록 AI가 발전함에 따라 스타트업이 더 안전해집니다.\n\n## 예시: AI를 활용한 이력서 생성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n간단한 예시: 이력서 생성... 사실 누구나 오늘 ChatGPT로 이력서 텍스트를 만들 수 있어요. 시장은 이제 많은 이력서 생성기로 가득 차 있어요. 하지만 빠진 부분은 AI를 사용하여 풀 텍스트 뿐만 아니라 완전히 서식이 있는 Word 문서를 생성하는 것입니다 — json으로 채워진 웹 UI나 PDF가 아닌— 사용자가 양식을 작성할 필요가 없는 방식으로. 강력한 AI 인터페이스 + 사용자 참여 제한 + 통합이 열쇠라고 생각해요. CVGist.com은 이를 위한 간단한 예시예요. 사용자는 간단한 요지를 입력하면, CVGist가 ChatGPT와 여러 문서 통합을 사용하여 실제 단어 문서(90개 이상의 이력서 템플릿 서식)를 생성해줘요. 우리가 여기서 만들어낸 간단한 접착제는 단어 문서를 생성하는 것이었죠... 이것은 완전히 간단하지 않았어요. 단어 문서 생성이 우리가 사업 아이디어로 확장하고자 하는 것이죠; 이력서를 넘어, 심지어 우리는 여전히 AI로 단어 처리를 혁신하는 Microsoft의 능력에 능숙해야 해요.\n\n물론, 이것조차 오래가는 것이 아니에요. 우리는 곧 다양한 스타일의 구조화된 문서를 생성해주는 에이전트들이 있을 거에요. 요점은 통합을 구축하려고 하는 것이에요.\n\n## 더 많은 접착제가 열쇠\n\n이력서 예시를 위한 더 발전된 통합 또는 \"접착제\"는 사용자를 대신하여 여러 직업에 지원하는 것을 포함할 수 있어요 (웹 API, 아마도 헤드리스 브라우저 자동화 등을 활용). 그러나 OS 및 브라우저 제공업체들이 에이전트들을 기본적으로 통합할 것이며, 우리는 OpenAI가 언젠가 자체 에이전트 브라우저를 출시하는 것에 놀랄 필요가 없어요. 이러한 시나리오에서 브라우저 기반 에이전트/어시스턴트는 네이티브 브라우저 대화식 경험에서 사용자를 위한 어떤 기본적인 웹 작업(양식 작성 등)을 수행할 만큼 강력하게 될 거에요 (스타트업이 이곳에서 시도해야 하는 좋은 아이디어라고는 생각하지 않아요).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n따라서 통합 기능이 미래에 대비되도록 하는 것은 아이디어 유효성 검증 과정에서 추가적인 단계입니다. 간단한 통합 관점에서 침투하기 어려운 시스템 및 비즈니스 워크플로 프로세스를 찾아 개선해야 합니다. 단순한 UI/API 상호작용을 넘어서 복잡한 워크플로에 집중해야 합니다 (다시 한 번, 공급 업체들은 일반화된 플랫폼 에이전트로 향하고 있기 때문입니다).\n\n그렇다면 알트만에 밀리지 않고 미래에 대비할 수 있는 아이디어가 무엇일까요? 여기에 답변하는 것은 상당히 가치가 있습니다.\n","ogImage":{"url":"/assets/img/2024-05-23-OpenAIwillsteamrollstartupswhatshouldwebuildasthemodelsgetbetter_0.png"},"coverImage":"/assets/img/2024-05-23-OpenAIwillsteamrollstartupswhatshouldwebuildasthemodelsgetbetter_0.png","tag":["Tech"],"readingTime":4},{"title":"알고 있는 것은 기억하는 것과도 같아요","description":"","date":"2024-05-23 17:22","slug":"2024-05-23-ToKnowIsAlsotoRemember","content":"\n![이미지](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png)\n\n한 남자와 한 여자가 임상 연구 센터의 조용한 방 안에서 대화를 나누고 있습니다. 여자는 질문을 하고는 남자가 대답할 때까지 기다리면서 몇 가지 노트를 적습니다. 그냥 보통 대화처럼 보일 수도 있지만, 실제로는 전혀 보통이 아닙니다. 여자의 노트북 안에는 매 페이지마다 써 있는 날짜와 상관없이 남자의 대답이 항상 동일합니다. 대화가 80년대에 발생했더라도, 대답은 10년 이상 전에 일어난 사건을 참조하고 있습니다. Jenni Ogden은 나중에 네오심리학에 영향을 미치면서 그의 진짜 이름인 Henry Molaison으로 더 잘 알려지게 된 환자 H.M.과 대화를 나눈 최초의 연구자 중 한 명이었습니다. 며칠 후, 연구진은 Henry가 27세 때 받았던 뇌 절제술로 인해 새로운 기억을 생성하는 능력을 상실했다고 결론 내렸습니다. Henry의 사례는 뇌 기능과 기억 사이의 연결을 이해하고 단기와 장기 기억이라는 개념을 만들어내는 데 도움이 되었습니다. 이 개념은 기계 학습 분야에서 혁신적인 연구를 위한 토대를 마련했으며, 과학자들과 개발자들이 뇌의 신비한 내부 구조에서 더 나은 예측 모델을 만드는 데 노력하고 있습니다.\n\n# 소개\n\n인공 신경망(ANN)은 우리 뇌에서 작동하는 실제 신경망에서 영감을 받았습니다. 실제로 ANNs는 실제 신경세포가 어떻게 상호 연결되고 위에서 설명한 상황을 설명하는 추상화일 뿐입니다. 개미군 최적화, 차분 진화, 입자 미래 등의 프로세스와 유사하게, ANNs는 실제 과정의 본질을 포착하여 현재 대부분의 AI 솔루션 뒤에 있는 알고리즘을 설계하는 데 사용됩니다. ANNs가 정말로 학습하는지, 그들이 하는 일을 지능이라고 해야 하는지에 대한 논의는 넓고 계속됩니다. 그러나 그들의 다용도성과 성능은 부정할 수 없습니다. 새로운 ANN 구성은 매일 개발되고 있으며 다양한 문제에 성공적으로 적용되고 있습니다. 이러한 변형의 대부분은 여전히 실제 신경망의 행동에서 영감을 받고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nRNN(RNNs)은 일련의 메모리 구성요소를 통합하여 처리하는데, 수년 전에 자연어 처리(NLP)에서 중요한 접근 방식을 나타냅니다. RNNs는 Long-Short Term Memory (LSTM) Networks로 나아가는 길을 열며, NLP 응용 프로그램에서 신경망의 성능을 높였습니다. 이후 LSTM 네트워크는 트랜스포머 모델과 GPT(Generative Pre-trained Transformer)에 의해 대체되었는데, 이것이 ChatGPT의 기초가 되었습니다. 이 기사에서는 LSTM 네트워크가 무엇이며, 그들을 특별하게 만드는 이유에 대해 살펴봅니다.\n\n# RNN의 의미\n\nLSTM 네트워크가 어떻게 작동하는지 이해하기 위해서는 그 목적에 대해 생각해 보는 것이 중요합니다. RNN과 LSTM 네트워크는 비슷한 목표를 따릅니다. 이들은 순차적으로 저장된 데이터를 모델링하고 예측하는 데 사용됩니다. 이는 이 유형의 네트워크가 데이터 시퀀스를 읽고 다음 값이 무엇인지 예측하려고 한다는 것을 의미합니다. 특정 도시의 지난 30일간의 평균 온도를 기록한 로그가 있다고 가정해 봅시다. 그리고 31일차의 온도를 추정하고 싶다면 어떻게 할까요? 한 가지 방법은 온도를 다른 변수에 상관시켜서, 31일에 이러한 변수의 값에 따라 새로운 온도를 추정하는 것입니다. RNN은 몇 일, 예를 들어 30일 이내의 일부 날짜를 고려하여 이전 온도 값을 기반으로 31일의 온도를 예측합니다. 한 마디로, RNN은 시퀀스를 기억하고 다음 값 또는 값 그룹을 제시하려고 노력합니다. 이전에 작성한 기사 중에 RNN이 메모리 전문가와 어떻게 비교되면서 RNN이 어떻게 단계적으로 작동하는지 설명했습니다.\n\n이전 날짜의 온도를 기반으로 새로운 온도를 예측하는 아이디어는 다른 응용 분야로 확장할 수 있습니다. 소개에서 언급된 것처럼, RNN은 NLP 중 첫 접근 방식 중 하나였습니다. 아이디어는 RNN을 텍스트로 학습한 다음, RNN을 사용하여 입력 후 다음에 나오는 단어 또는 단어 그룹을 예측하는 것입니다. 이러한 아이디어는 자동 번역뿐만 아니라 음성 및 필기 인식과 같은 과제에도 적용할 수 있습니다. RNN이 이러한 과제들을 다룰 때 직면하는 문제 중 하나는 죽거나 폭발하는 기울기(vanishing/exploding gradients)로 인한 어려움입니다. 이 문제의 해법은 LSTM 네트워크의 적용입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nFigure 1은 완전히 연결된 인공 신경망(ANNs)과 순환 신경망(RNNs) 간의 주요 구조적 차이를 보여줍니다. 이 간단한 예에서는 (X1,Y1) 및 (X2,Y2)의 값이 Y3의 새로운 값 계산에 사용됩니다. 실제로는 ANN과 RNN이 많은 입력-출력 쌍으로 훈련됩니다. 훈련 과정이 완료되면 네트워크는 새로운 값을 예측하는 데 사용됩니다. 이 프로세스에 익숙하지 않다면, 이 기사의 끝에 유용한 참고 자료를 추가했습니다. 여기에 이 프로세스를 설명한 나의 시도도 곁들였습니다. ANNs와 RNNs 사이의 훈련 및 예측의 일반적인 아이디어는 비슷하지만, 구조적으로 큰 차이가 있습니다. RNN에서는 Y1과 Y2의 값이 X1과 X2 대신 네트워크를 훈련하는 데 사용됨을 주목하십시오. 또한 첫 번째 단위와 두 번째 단위를 연결하는 가중치가 있으며, 이는 이전 단위에서 온 활성화를 나타냅니다. 이 가중치는 RNN의 \"기억\" 구성 요소를 나타냅니다. 더 큰 가중치는 RNN이 이전 값에 더 많은 중요성을 부여함을 의미하고, 더 작은 가중치는 RNN이 과거 값을 잘 기억하지 못한다는 것을 의미합니다. 이것은 신경망 구조의 가중치이므로, 그 값은 프로세스 중에 학습되며, RNN은 재현하려는 순서가 더 많은지 덜 많은지를 결정할 수 있습니다.\n\n![Image](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_1.png)\n\nRNN의 훈련 및 적용 중 중요한 측면은 네트워크가 읽고 훈련 및 예측에 사용하는 값 시퀀스의 길이입니다. 시퀀스 길이가 15라고 가정해 봅시다. 이는 RNN이 15개의 입력 값을 읽은 후 16번째 값을 찾아 훈련한다는 것을 의미합니다(항상 그렇지는 않습니다. 동적 RNN도 있기 때문입니다). 시퀀스 길이로 돌아가보면, 더 긴 시퀀스 길이는 RNN이 최종 출력에 여러 읽기를 통합할 수 있어 유익합니다. 그러나 더 긴 시퀀스 길이는 사라지는/폭주하는 그래디언트를 유발합니다. LSTM 네트워크는 이 문제를 어떻게 극복할까요?\n\n# LSTM 네트워크\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLSTM 네트워크는 어떤 정보를 “기억”하고 어떤 정보를 “잊을지” 결정하도록 훈련됩니다. RNN에서는 이전 유닛의 활성화에만 적용되는 메모리 구성 요소에 대한 가중치가 있습니다. 그러나 LSTM에서는 메모리 구성 요소의 개념이 장기 메모리 구성 요소(셀 상태)와 단기 메모리 구성 요소(은닉 상태)로 대체됩니다. 각 구성 요소는 서로 다른 게이트에 분산된 일련의 편향 및 가중치와 관련이 있습니다. 이는 각 입력이 네트워크에서 얼마나 많은 정보를 유지하고 얼마나 버릴지 결정하는 게이트(또는 단계)를 통과한다는 것을 의미합니다. 이 결정은 훈련 과정 중에 학습된 가중치와 편향 값에 기반합니다.\n\n그림 2는 단일 입력을 읽는 매우 간단한 LSTM 네트워크 스케치를 보여줍니다. 실제 LSTM 네트워크 유닛을 살펴보기 전에이 단순화된 다이어그램을 먼저 분석하겠습니다. RNN을 나타내는 이전 그림과 얼마나 다른지에 주목하세요. 기억할 점 중 첫 번째는 입력 값 외에도 LSTM 네트워크에는 단기 및 장기 메모리 구성 요소가 있다는 것입니다. 이러한 구성 요소는 공식적으로 셀 상태(C)와 숨겨진 상태(h)로 알려져 있습니다. 이 표기법은 나중에 사용되겠지만, 우리는 현재 이전 이름을 사용할 것입니다. 입력 및 메모리 구성 요소는 세 가지 서로 다른 게이트를 통과합니다: 삭제, 입력 및 출력.\n\n- 삭제 게이트는 장기 기억의 얼마나 보관해야 하는지를 결정합니다. 이 게이트는 현재 입력뿐만 아니라 단기 메모리 구성 요소를 고려하여 0과 1 사이의 값을 계산하여 장기 메모리 구성 요소를 곱합니다. 0의 삭제 게이트는 네트워크가 이전 정보를 보존하지 않음을 의미합니다. 그 답은 새로운 입력에만 기초합니다.\n- 입력 게이트는 새 정보가 장기 메모리 구성 요소에 보존되어야 하는 양을 결정합니다. 이 게이트의 출력은 다음 입력에 보존되는 장기 기억 구성 요소에 추가됩니다. 이 구성 요소가 삭제 및 입력 게이트에만 연결된다는 점에 유의하십시오. 이는 각 반복에서 장기 기억이 무엇을 버리고 어떤 정보를 추가해야 하는지에 따라 업데이트된다는 것을 의미합니다.\n- 출력 게이트는 입력 및 단기 메모리 구성 요소를 고려하고 이전 단기 메모리 구성 요소로 저장될 새로운 장기 메모리 구성 요소의 양을 계산합니다. 이는 LSTM 네트워크에서 나온 최종 값이 장기 및 단기 메모리 구성 요소뿐만 아니라 출력 게이트도 고려하여 계산된다는 것을 의미합니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_2.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 LSTM 네트워크 셀의 구조를 파악했으니 여기서 발생하는 계산에 더 가깝게 살펴보겠습니다. Figure 3은 LSTM 네트워크 셀을 더 자세히 보여줍니다. 입력, 단기 기억 구성 요소 및 장기 기억 구성 요소는 각각 x, h 및 C로 표시됩니다. 이들 각각의 글자는 분석 중인 시간 기간에 해당하는 아래 첨자를 가지고 있습니다. t-1의 아래 첨자는 값이 이전 반복에 속한다는 것을 의미합니다. 예를 들어, forget gate는 현재 입력(xt)과 이전 반복의 단기 기억 구성 요소(ht-1)를 고려합니다. 게이트들은 또한 일련의 가중치와 편향을 고려합니다. forget 및 output 게이트에는 각각 3개의 매개변수가 있습니다:\n\n- 편향(bxf, bxo)\n- 입력을 곱하는 가중치(wxf, wxo)\n- 단기 기억 구성 요소를 곱하는 가중치(whf, who)\n\n가중치와 편향은 활성화 함수로 들어가기 전에 입력 값에 곱해지고 더해집니다. 이 예에서 forget 및 output 게이트 모두 시그모이드 활성화 함수를 가지며 그 최종 활성화(af, ao)는 Figure 3 우측에 표시됩니다. 이러한 게이트와 달리, input 게이트는 두 개의 활성화 함수, 시그모이드 및 tanh가 있으며 해당 가중치와 편향을 가집니다. 이는 단일 LSTM 네트워크 단위에 대해 훈련해야 할 매개변수의 총 수가 12임을 의미합니다.\n\nLSTM 네트워크 셀에 대해 이해해야 할 마지막 중요한 측면은 새로운 C 및 h가 어떻게 계산되는지입니다. 이전 장기 기억 구성 요소는 먼저 forget 게이트의 출력과 곱해진 후 입력 게이트의 결과에 추가됩니다. 이는 forget 게이트가 C의 얼마나 다음 반복에 전달하는지를 결정하고, input 게이트가 C의 새 값에 얼마나 추가할지를 결정합니다. 단기 기억 구성 요소 계산을 위해 output 게이트의 결과는 새로운 C의 tanh와 곱해집니다. 이는 output 게이트가 장기 기억 구성 요소를 다음 반복에 얼마나 전달할지 결정합니다. C의 값이 1 이상일 수 있으므로, 값이 -1과 1 사이로 제한되도록 tanh 연산이 적용됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_3.png)\n\n입력이 모든 게이트를 통과하면, 새로운 C와 h가 다음 반복으로 전달되어 새 입력과 상호 작용합니다(그림 4). 이 과정은 시퀀스의 모든 값에 대해 반복되며, 해당 시퀀스의 최종 h에 도달할 때까지 반복됩니다.\n\n![이미지](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_4.png)\n\n# 앞으로 나아가기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLSTM 네트워크는 다른 인공 신경망(ANN)에서 사용되는 방법론과 유사한 방법으로 훈련될 수 있습니다. 문제에 따라 각 순전파 사이클 이후 업데이트되는 손실 함수를 정의합니다. 그런 다음 이 손실 함수를 사용하여 역전파 과정을 통해 가중치와 편향을 업데이트합니다. RNN 및 LSTM 네트워크의 경우, 역전파는 일반적으로 모든 반복 유닛을 통해 가중치와 편향을 누적하는 과정이기 때문에 시간을 거슬러 역전파(backpropagation through time, BPTT)라고합니다. 이는 LSTM 네트워크의 단일 유닛을 읽는 LSTM 네트워크 셀에서 순전파 과정이 어떻게 진행되는지 설명하는 간단한 구현과 순전파, 역전파 과정에 대해 상세히 설명하는 주피터 노트북입니다.\n\n그림 5는 LSTM 네트워크 셀에서 단일 유닌을 읽는 순전파 과정의 예시를 보여줍니다. h의 최종 값이 네트워크 내 모든 게이트를 통해 전달되는 정보를 함께 전달하는 반면, 최종 C는 출력 게이트와 상호작용하지 않는 것에 주목하세요. 이 게이트들 각각이 보존할 정보와 잊을 정보를 규제자로 작용합니다. 입력과 상호작용하는 최적의 방법을 학습하는 완전히 연결된 ANN에서 가중치와 편향이 학습되는 것과 유사하게, LSTM 네트워크에서 매개변수는 보존하거나 버릴 최적의 정보 양을 학습하기 위해 훈련됩니다.\n\n![image](/assets/img/2024-05-23-ToKnowIsAlsotoRemember_5.png)\n\n# 더 많은 유닛?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금까지의 숫자와 예제는 단일 유단 LSTM 네트워크를 보여주었지만, 다른 유형의 네트워크와 마찬가지로 LSTM 네트워크는 여러 개의 유닌을 가질 수 있습니다. 단위 수에 따라 매개 변수 수는 어떻게 변하나요? 두 개의 유닌을 갖는 LSTM 네트워크의 경우, 학습할 매개 변수가 12개가 아닌 32개가 있습니다. 추가된 20개의 매개 변수는 어디에 있을까요? 그러면, 이제 조금 복잡해 질 것입니다. 이를 여러 부분으로 나눠서 살펴보겠습니다.\n\n첫 번째 유닛에는 12개의 매개 변수가 있습니다. 이전에 설명한 것과 같은 매개 변수들입니다: 입력을 곱하는 4개의 가중치, 숨겨진 상태(h)를 곱하는 4개의 가중치, 그리고 각 게이트에 대한 4개의 바이어스입니다. 두 번째 유닌도 12개의 관련된 매개 변수를 가지고 있습니다. 이는 지금까지 총 24개의 매개 변수를 가지게 되었다는 것을 의미합니다.\n\nLSTM 네트워크는 특정 유형의 RNN이므로 각 유닌 사이에 연결이 있을 것입니다. 이는 유닌 1에서 처리된 정보가 유닌 2로 전달된다는 것을 의미합니다. 각각의 연결은 고유의 가중치를 갖습니다. 두 개의 유닌을 갖는 LSTM 네트워크에서, 이전에 언급된 24개의 매개 변수 외에, 유닌 2의 각 게이트와 유닌 1의 각 게이트 사이의 연결 및 유닌 1의 숨겨진 상태와 유닌 2의 게이트 사이의 연결에 해당하는 8개의 매개 변수가 추가로 필요합니다. 그림 7은 유닌 2의 잊기 게이트에서 활성화를 계산하는 방법을 보여줍니다. 이 게이트가 이전 게이트와 이전 숨겨진 상태에 연결되어 있다는 점에 주목하세요. 그림에는 새로운 가중치가 5개만 표시되어 있지만, 실제로는 첫 번째 유닌의 숨겨진 상태가 두 번째 유닌의 각 게이트에 연결되기 때문에 8개의 가중치가 있습니다. n개의 유닌에 대한 매개 변수 수는 12n+4n(n-1) 또는 간소화된 표현으로 8(n+n²/2)입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_7.png\" /\u003e\n\n# Going backwards\n\n인공 신경망(ANNs)에서 흔히 볼 수 있는 바와 같이, 역전파 프로세스는 일반적으로 이해하고 구현하기 가장 어려운 부분입니다. 단일 유닛 LSTM 네트워크에서는 각 역전파가 4개의 편향과 8개의 가중치를 업데이트해야 하며, h(t-1) 및 C(t-1)도 업데이트해야 합니다. h(t-1)가 출력 게이트에 의존하고 현재 C는 다시 입력 및 망각 게이트에 따라 달라짐을 주목해야 합니다. 손실에 대한 편도함수를 계산할 때 이 사항을 고려하는 것이 중요합니다. 이전에 언급한 바와 같이, 이 Jupyter 노트북에는 역전파 프로세스를 포함한 간단한 LSTM 네트워크를 구축하는 데 필요한 모든 방정식이 포함되어 있습니다. 그림 8은 각 편도함수를 계산하는 데 도움이 되는 매개변수 간 의존성을 보여줍니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_8.png\" /\u003e\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 응용 프로그램\n\n다음 섹션에는 LSTM 네트워크의 세 가지 응용 프로그램 예시가 포함되어 있습니다. 예시는 간단한 순서로 제공됩니다. 각 예시에 대한 파이썬 코드를 이 Jupyter 노트북에서 찾을 수 있습니다.\n\n## 연속 함수 모델링을 위한 바닐라 LSTM 네트워크\n\n이것은 처음부터 LSTM 네트워크를 구현하는 매우 간단한 예시입니다. 여기서 배울 중요한 교훈은 LSTM 네트워크에 데이터를 공급하기 전에 데이터를 올바르게 준비하는 중요성입니다. LSTM 네트워크로 모델링하고 싶은 연속 함수가 있다면, 먼저 입력-타겟 데이터의 쌍을 생성해야 합니다 (Figure 1 참조). 이 데이터는 시퀀스 길이에 따라 달라집니다. 예를 들어, 시퀀스 길이가 15인 경우, 각 입력 항목은 15개의 값이 포함되며 16번째 값은 해당 입력의 타겟이 됩니다. 네트워크에 입력하기 전에 데이터를 정규화하는 것도 중요합니다. 이 예시에서는 sin(x) 함수와 함께 웰에서의 석유 생산 행동을 모델링하기 위해 간단한 LSTM 네트워크가 사용됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 워드 예측기를 구축하기 위한 바닐라 LSTM 네트워크\n\n이 예시에서는 이전의 바닐라 LSTM 네트워크가 워드 예측 문제에 적용되었습니다. 짧은 텍스트로 훈련된 후, 모델은 다음에 나올 단어를 예측합니다. 실제로 워드 처리 및 워드 예측 문제는 이 예시처럼 다가가지 않습니다. 그러나 LSTM 네트워크의 가능한 응용에 대한 간단하고 명확한 설명입니다.\n\n## Keras의 LSTM 네트워크를 사용하여 워드 예측기 구축\n\n이 예시는 Keras의 LSTM 네트워크를 사용하여 긴 텍스트로 훈련된 후 다음 단어가 무엇인지 예측하는 더 현실적인 예제입니다. 이 예시에서는 NLP 문제에서 일반적인 추가인 임베딩 레이어를 사용합니다. 임베딩 레이어는 단어의 정수로 인코딩된 표현(인덱스)을 밀집된 벡터로 변환하여 단어 사이의 관계를 더 잘 모델링하는 데 도움이 되는 고정 크기의 벡터로 변환합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n인공 신경망 및 특히 순환 신경망을 사용하여 자연어 처리 문제에 접근하거나 연속 데이터를 모델링하는 것은 최근에 개발된 것이 아닙니다. 이 응용 프로그램은 오랫동안 존재해 왔으며 새로운 기능으로 계속 발전하고 있습니다. LSTM 네트워크가 어떻게 작동하는지 이해하고 그것을 특별한 종류의 RNN으로 만드는 요소를 파악하는 것은 결과와 예상대로 작동하지 않을 수 있는 이유에 대한 통찰력을 제공할 수 있습니다. 본문은 LSTM 네트워크에 대한 포괄적인 설명을 포함하고, 그 응용 예시 세 가지를 제시합니다. 대부분의 현재 NLP 도구 및 솔루션은 다른 네트워크 구조에 의존하지만 LSTM 네트워크 내부 작업에 대한 탄탄한 개념은 머신러닝 분야에서 항상 유익할 것입니다. 이것을 장기 기억 셀에 저장하는 것을 기억하세요! 😉\n\n# 참고문헌\n\n- Ng, Andrew. Machine Learning Specialization.\n- Keras 'Embedding' 레이어는 어떻게 작동합니까? CrossValidated 게시물. 2017\n- Cowan, Nelson (2009). 장기, 단기 및 직업기억 사이의 차이점은 무엇인가? — PMC. Prog Brain Res. 2008;169:323–38. doi: 10.1016/S0079–6123(07)00020–9. PMID: 18394484; PMCID: PMC2657600.\n- Erz, Hendrik (2023). ChatGPT를 생산적으로 사용하는 방법 | Hendrik Erz. hendrik-erz.de, 2023년 2월 14일\n- Adams, Tim (2013). Henry Molaison: 우리가 결코 잊지 않을 기억상실자 | 기억 | The Guardian\n- Dittrich, Luke (2016). 기억할 수 없던 두뇌 — 뉴욕 타임즈\n","ogImage":{"url":"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png"},"coverImage":"/assets/img/2024-05-23-ToKnowIsAlsotoRemember_0.png","tag":["Tech"],"readingTime":13},{"title":"Ontologies 및 언어 모델을 활용하여 정확한 질문 응답하기","description":"","date":"2024-05-23 17:20","slug":"2024-05-23-LeveragingOntologiesandLanguageModelsforAccurateQuestionAnswering","content":"\n금일의 데이터 중심 세계에서는 방대한 양의 정보를 활용하여 정확하게 질문에 답하는 능력이 중요합니다. 대형 언어 모델(LLM)을 활용한 질문 응답(QA) 시스템은 이러한 측면에서 큰 가능성을 보여주고 있습니다.\n\n하지만, 이러한 시스템의 정확도와 신뢰성을 보장하는 것은 여전히 중요한 과제입니다.\n\ndata.world의 최근 연구에 따르면, 온톨로지와 지식 그래프를 활용하면 LLM 기반 QA 시스템의 성능을 크게 향상시킬 수 있음을 입증했습니다.\n\n본 글에서는 온톨로지 기반의 쿼리 유효성 검증과 LLM을 활용한 쿼리 복구를 결합한 혁신적인 접근 방식을 탐구하여, 질문 응답에서 전례없는 수준의 정확성을 달성하는 방법을 살펴보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 배경\n\n질문 응답 시스템은 구조화된 또는 구조화되지 않은 데이터 소스에서 정보를 추출하여 사용자 쿼리에 정확하고 관련성 높은 응답을 제공하는 것을 목표로 합니다. 기존의 QA 시스템은 종종 자연어 쿼리의 복잡성과 모호성에 노출되어 최적의 결과를 얻기 어려웠습니다. 지식 그래프와 온톨로지의 등장은 도메인 지식을 표현하고 추론하는 강력한 프레임워크를 제공하여 보다 정교한 QA 방법을 가능케 하였습니다 [2].\n\n대규모 언어 모델인 GPT-4와 같은 모델은 자연어 처리 분야를 혁신적으로 변화시켰으며, 인간과 유사한 텍스트를 이해하고 생성하는 놀라운 능력을 보여주었습니다. 대형 언어 모델은 SQL 또는 SPARQL 쿼리를 사용하여 구조화된 데이터베이스에서 질문에 답변하는 등 다양한 QA 작업에 적용되었습니다 [3]. 그러나 LLM이 생성한 쿼리의 정확도는 기반이 되는 데이터 스키마와 의미에 대한 명확한 지식 부족으로 제한될 수 있습니다.\n\n# Ontology-based Query Check (OBQC)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-LeveragingOntologiesandLanguageModelsforAccurateQuestionAnswering_0.png\" /\u003e\n\nLLM 기반 QA 시스템의 한계를 해결하기 위해 연구자들은 Ontology 기반 쿼리 체크 (OBQC) 접근 방식을 제안했습니다 [1]. OBQC는 온톨로지에 인코딩된 의미 정보를 활용하여 LLM이 생성한 SPARQL 쿼리의 정확성을 검증합니다. 이 과정은 몇 가지 주요 단계로 이루어집니다:\n\n1. 생성된 SPARQL 쿼리에서 기본 그래프 패턴 (BGP)을 추출하여 쿼리의 그래프 패턴을 나타냅니다 [1].\n2. :쿼리( BGP가 RDF로 바뀐 것을 나타냄)와 :온톨로지(온톨로지 자체를 나타냄)의 두 개의 명명된 그래프를 캡슐화한 결합 그래프를 구성합니다 [1].\n3. SPARQL 쿼리로 구현된 온톨로지 일관성 규칙을 적용하여 :쿼리와 :온톨로지 그래프 사이의 위반을 확인합니다 [1].\n4. 생성된 쿼리에서 구체적인 오류를 식별하고 각 규칙 위반에 대한 사람이 읽을 수 있는 설명을 생성합니다 [1].\n\nLLM이 생성한 쿼리를 온톨로지의 의미와 비교함으로써 OBQC는 도메인이나 범주 클래스 불일치, 호환되지 않는 속성 사용, 정의되지 않은 속성 등 다양한 유형의 오류를 감지할 수 있습니다. 이 유효성 검사 프로세스는 생성된 쿼리가 기저지식 그래프와 일관된지 확인하여 QA 시스템의 전체 정확성을 향상시키는 데 도움을 줍니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# LLM Repair\n\nOBQC는 생성된 쿼리의 오류를 식별하는 데 능숙하지만, 이러한 오류를 수정하는 메커니즘을 제공하지는 않습니다. 여기서 LLM Repair 구성 요소가 필요합니다. LLM Repair는 OBQC에 의해 제공된 오류 설명을 기반으로 쿼리를 반복적으로 다듬고 수정하는 능력을 활용합니다 [1].\n\n수리 과정은 오류 설명과 부정확한 SPARQL 쿼리를 포함하는 프롬프트를 작성하여 시작됩니다. 이 프롬프트는 그런 다음 LLM에 공급되며, LLM은 의도한 의미와 구조를 보존하면서 식별된 문제를 해결하기 위해 쿼리를 다시 작성하려고 시도합니다 [1]. 수정된 쿼리는 그런 다음 OBQC로 반환되어 확인을 받으며, 유효한 쿼리를 얻거나 최대 반복 횟수에 도달할 때까지 반복적인 피드백 루프를 형성합니다 [1].\n\nLLM Repair는 LLM이 자연어 설명을 이해하고 일관된 응답을 생성하는 능력에 활용합니다. 질문이나 온톨로지에 명시적인 액세스가 필요하지 않고 오류 설명을 활용함으로써, 쿼리를 수정하는 데 효과적으로 학습할 수 있습니다 [1].\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n수리 과정이 일정 횟수의 반복 이후 유효한 쿼리를 생성하지 못하면, LLM 수리는 \"알 수 없음\" 또는 \"불확실\" 응답을 반환하여 신뢰할만한 답변을 생성할 수 없음을 표시합니다 [1]. 이 실패 안전 기구는 수리 시도가 실패할 경우 잘못된 또는 오해를 일으킬 수 있는 결과를 시스템이 제공하는 것을 방지합니다.\n\n# 실험 설정 및 결과\n\nOBQC 및 LLM 수리 접근 방식의 효과를 평가하기 위해 연구자들은 Chat with the Data 벤치마크를 사용한 실험을 진행했습니다 [1]. 이 벤치마크에는 기업용 SQL 스키마, 질문-답변 쌍, 그리고 OWL 온톨로지 매핑이 포함되어 있습니다. 테스트된 QA 시스템은 SPARQL 제로샷 프롬프트로 GPT-4였으며, 쿼리는 가상화된 지식 그래프에서 실행되었습니다 [1].\n\n실험 결과는 정확도와 오류 감소에서 상당한 향상을 보여주었습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- OBQC 및 LLM Repair을 사용하여 전체 실행 정확도가 42.88%에서 72.55%로 향상되었습니다 [1].\n- 시스템에서 잘못된 것으로 식별된 알 수 없는 쿼리가 전체 오류율인 19.44% 중 8%를 차지했습니다 [1].\n- 고복잡도 스키마에 관한 질문의 정확도 향상이 특히 유의미했습니다 [1].\n\n추가 분석 결과, OBQC의 도메인 관련 규칙이 수리의 70%를 담당하는 가장 일반적인 규칙임이 밝혀졌습니다 [1]. 이는 온톨로지에서 도메인 지식을 정확하게 모델링하는 것이 QA 성능을 향상시키는 데 중요한 역할을 한다는 것을 시사합니다.\n\n# 현실 세계의 영향과 응용\n\nOBQC 및 LLM Repair의 유망한 결과들은 이미 현실 세계 응용분야에서 찾아보실 수 있습니다. 기업용 데이터 카탈로그 및 발견 솔루션의 선도 업체인 data.world은 이러한 구성 요소를 AI Context Engine에 통합하였습니다 [1]. AI Context Engine은 구조화된 데이터와의 신뢰할 수 있는 대화를 지원하여 고객이 질문을 하고 정확하고 컨텍스트 인식형 답변을 받을 수 있도록 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n몇몇 data.world 고객은 AI Context Engine을 도입하여, 다양한 사용 사례에서 향상된 QA 능력을 활용하고 있습니다 [1]. 이는 실제 환경에서 온톨로지 기반 검증과 LLM 기반 복구를 결합한 가치와 영향을 보여줍니다.\n\n## 미래 방향성과 도전 과제\n\n현재 방법이 높은 성공률을 보여주고 있지만, 미래 연구와 개선을 위한 여러 분야가 아직 남아 있습니다. 하나의 주요 도전 과제는 OBQC를 보다 복잡한 온톨로지 구조에 대응할 수 있도록 확장하는 것입니다. 이는 OWL 어키오름이 포함된 온톨로지 조합(연합, 교집합 또는 다른 논리 연산자)에 대한 처리를 의미합니다 [1]. 이 도전에 대응함으로써 시스템이 쿼리를 풍부하고 표현력 있는 온톨로지에 대해 유효성을 검증하는 능력을 더욱 강화할 수 있을 것입니다.\n\n또 다른 중요한 방향은 해당 접근 방식을 다른 도메인과 데이터 집합으로 일반화하는 것입니다. 현재 실험은 기업용 SQL 스키마와 질문-답변 쌍을 다루는 Chat with the Data 벤치마크에 주로 초점을 맞추었습니다 [1]. 시스템의 성능을 다양한 도메인과 데이터 소스 범위에서 평가함으로써 보다 넓은 적용 가능성과 견고성을 평가할 수 있을 것입니다.\n\n확장성과 계산 비용에 대한 조사도 필요합니다. 온톨로지와 데이터 집합의 크기와 복잡성이 증가함에 따라, OBQC와 LLM Repair 구성 요소의 효율성은 점점 더 중요해집니다 [1]. 최적화된 알고리즘 및 병렬 처리 기술을 개발함으로써 대규모 응용 프로그램에 대한 시스템의 실용성과 반응성을 보장하는 데 도움이 될 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n체계 기반 쿼리 유효성 검사와 LLM(언어 모델) 기반 쿼리 수리의 결합은 질문 응답 분야에서 중요한 발전을 나타냅니다. 오전톨로지에 인코딩된 의미 지식과 LLM의 생성적 능력을 활용하여, 이 방법은 전례없는 수준의 정확도와 신뢰성을 실현합니다.\n\n실험 결과와 현실 세계에서의 채택은 이 기법이 신뢰할 수 있고 맥락에 맞는 구조화된 데이터와의 대화를 가능케 함에 대한 엄청난 잠재력을 입증합니다. 조직이 통찰을 도출하고 데이터 기반 결정을 내리는 데 QA 시스템에 점점 더 의존함에 따라, 정확성의 중요성은 지나치게 강조될 수 없습니다.\n\n그러나 이 방법의 전체 잠재력을 실현하기 위해서는 계속된 연구 및 개발 노력이 필요합니다. 더 복잡한 온톨로지 구조를 처리하고, 다양한 도메인에 적용할 수 있도록 일반화하고, 확장성 문제를 해결하는 것이 미래 작업의 주요 분야입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nOBQC와 LLM Repair의 성공은 의미론, 온톨로지, 그리고 지식 그래프가 정확하고 신뢰할 수 있는 QA 시스템을 구축하는 데 중요한 역할을 한다는 점을 강조합니다. 이러한 기본기술에 투자하고 계속해서 가능한 한 경계를 넓힘으로써, 우리는 자연어 인터페이스의 진정한 잠재력을 발휘하고 사용자들이 필요로 하는 정보에 원활하게 액세스할 수 있도록 돕는 것이 가능합니다.\n\n# 참고문헌\n\n[1] Dean Allemang과 Juan F. Sequeda. 2024. “Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!” 기술 보고서.\n[2] Aidan Hogan 등. 2021. “Knowledge Graphs.” 데이터, 의미론, 그리고 지식에 관한 합성 강의. Morgan \u0026 Claypool Publishers.\n[3] Juan Sequeda, Dean Allemang, 그리고 Brad Jesson. 2023. “A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model’s Accuracy for Question Answering on Enterprise SQL Databases.” arXiv 사전인쇄 arXiv:2406.01688.\n","ogImage":{"url":"/assets/img/2024-05-23-LeveragingOntologiesandLanguageModelsforAccurateQuestionAnswering_0.png"},"coverImage":"/assets/img/2024-05-23-LeveragingOntologiesandLanguageModelsforAccurateQuestionAnswering_0.png","tag":["Tech"],"readingTime":8},{"title":"깊은 학습 모델이 GPU에서 더 빨리 실행되는 이유 CUDA 프로그래밍 간단 소개","description":"","date":"2024-05-23 17:17","slug":"2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming","content":"\n## .to(\"cuda\") 가 무엇을 하는지 이해하고 싶은 분들을 위해\n\n![이미지](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_0.png)\n\n요즘에는 딥 러닝을 이야기할 때 성능을 향상시키기 위해 GPU를 활용한다고 연관 지어지는 것이 매우 일반적입니다.\n\nGPU(그래픽 처리 장치)는 원래 이미지, 2D 및 3D 그래픽의 렌더링을 가속화하기 위해 설계되었습니다. 그러나 다수의 병렬 작업을 수행할 수 있는 능력으로 인해 그 유용성은 그 이상으로 확장되어 딥 러닝과 같은 응용 프로그램에까지 이어집니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n깊은 학습 모델에 GPU를 사용한 것은 2000년대 중후반 경에 시작되었으며 2012년에 AlexNet이 등장하면서 매우 인기를 끌었습니다. AlexNet은 Alex Krizhevsky, Ilya Sutskever 및 Geoffrey Hinton이 디자인한 합성곱 신경망으로, 2012년 ImageNet 대규모 시각 인식 챌린지(ILSVRC)에서 우승했습니다. 이 승리는 깊은 신경망을 통한 이미지 분류의 효과성 및 대형 모델 학습에 GPU 사용을 보여주어 중요한 이정표가 되었습니다.\n\n이후 이 기술적 발전을 통해 깊은 학습 모델에 GPU를 사용하는 것이 점점 인기를 얻으며, PyTorch나 TensorFlow와 같은 프레임워크 개발에 이바지했습니다.\n\n요즘에는 PyTorch에서 데이터를 GPU로 전송하려면 .to(\"cuda\")만 작성하면 학습이 가속화되는 것으로 예상됩니다. 하지만 실제로 어떻게 GPU 컴퓨팅 성능을 활용하는지 알아볼까요?\n\n신경망, CNNs, RNNs 및 트랜스포머와 같은 깊은 학습 아키텍처는 기본적으로 행렬 덧셈, 행렬 곱셈 및 행렬에 함수를 적용하는 수학 연산을 사용하여 구축됩니다. 따라서 이러한 연산을 최적화하는 방법을 찾으면 깊은 학습 모델의 성능을 개선할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러니까, 간단하게 시작해 봅시다. 두 벡터 C = A + B를 추가하고 싶다고 상상해 보세요.\n\n![이미지](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_1.png)\n\n이를 C에서 간단히 구현하는 방법은 다음과 같습니다:\n\n```js\nvoid AddTwoVectors(float A[], float B[], float C[]) {\n    for (int i = 0; i \u003c N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위에서 볼 수 있듯이 컴퓨터는 벡터를 반복해서 각 쌍의 요소를 순차적으로 더해야 합니다. 그러나 이러한 작업은 서로 독립적입니다. i번째 쌍의 요소를 더하는 것은 다른 쌍에 의존하지 않습니다. 그래서 만약 이러한 작업들을 병렬로 실행할 수 있다면 어떨까요?\n\n간단한 접근 방식은 CPU 멀티스레딩을 사용하여 모든 계산을 병렬로 실행하는 것일 것입니다. 그러나 딥러닝 모델에서는 수백만 개 요소를 가진 대규모 벡터를 다루게 됩니다. 일반적인 CPU는 동시에 약 10여 개의 스레드만 처리할 수 있습니다. 이때 GPU가 필요한 것입니다! 현대의 GPU는 수백만 개의 스레드를 동시에 실행할 수 있어 이러한 대규모 벡터에 대한 수학적 연산의 성능을 향상시킵니다.\n\n# GPU 대 CPU 비교\n\nCPU 연산이 GPU보다 단일 작업에서 빠를 수 있지만 GPUs의 장점은 병렬화 능력에 있습니다. 이것의 이유는 그들이 서로 다른 목표로 설계되었기 때문입니다. CPU는 가능한 한 빠르게 연산 순서(스레드)를 실행하는 데 설계되었지만(동시에 약 10여 개의 스레드만 실행할 수 있음) GPU는 수백만 개의 연산을 병렬로 실행하는 데 설계되었습니다(개별 스레드의 속도를 희생하면서).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래의 비디오를 확인해보세요:\n\n예를 들어, CPU가 페라리와 같다고 상상해보세요. GPU는 버스입니다. 한 사람을 옮기는 작업이라면, 페라리(CPU)가 더 나은 선택일 것입니다. 그러나 여러 사람을 이동시키는 경우에는, 페라리(CPU)가 한 번에 더 빠르지만, 버스(GPU)는 한 번에 모두를 옮겨 더 빠르게 목적지에 도착하게 됩니다. CPU는 연속적인 작업을 처리하는 데 뛰어나지만 GPU는 병렬 작업에 적합하게 설계되어 있습니다.\n\n![이미지](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_2.png)\n\n더 높은 병렬 기능을 제공하기 위해 GPU 설계는 데이터 캐싱 및 흐름 제어 보다는 데이터 처리에 더 많은 트랜지스터를 할당합니다. 이는 CPU와는 달리, CPU가 단일 스레드 성능 및 복잡한 명령 실행을 최적화하기 위해 상당 부분의 트랜지스터를 할당하는데 사용하는 것과 대조적입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래 그림은 CPU와 GPU의 칩 자원 분포를 보여줍니다.\n\n![CPU vs GPU Resources](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_3.png)\n\nCPU는 강력한 코어와 더 복잡한 캐시 메모리 아키텍처(이를 위해 상당한 양의 트랜지스터를 할당)를 갖고 있습니다. 이 설계는 순차 작업의 신속한 처리를 가능하게 합니다. 반면, GPU는 고수준의 병렬성을 달성하기 위해 많은 코어를 갖는 것을 우선시합니다.\n\n이러한 기본 개념을 이해했으니, 실전에서 이러한 병렬 처리 능력을 어떻게 활용할 수 있을까요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# CUDA 소개\n\n딥러닝 모델을 실행할 때, 아마도 PyTorch나 TensorFlow와 같은 인기있는 Python 라이브러리를 사용하게 될 것입니다. 그러나 이러한 라이브러리의 핵심이 C/C++ 코드로 구동된다는 것은 잘 알려져 있습니다. 또한 앞에서 언급했듯이, 처리 속도를 높이기 위해 GPU를 사용할 수 있습니다. 여기서 CUDA가 등장합니다! CUDA는 NVIDIA가 개발한 일반 목적의 처리를 위한 플랫폼으로, Compute Unified Architecture의 약자입니다. 그러므로 게임 엔진에서 그래픽 계산을 처리하는 데 DirectX가 사용되는 것과 달리, CUDA는 개발자가 NVIDIA의 GPU 연산 능력을 그래픽 렌더링에만 한정되지 않고 일반 목적의 소프트웨어 응용프로그램에 통합할 수 있도록 합니다.\n\n이를 구현하기 위해 CUDA는 GPU의 가상 명령어 집합과 특정 작업(예: CPU와 GPU 간의 데이터 이동)에 액세스를 제공하는 간단한 C/C++ 기반 인터페이스인 CUDA C/C++을 제공합니다.\n\n더 나아가기 전에, CUDA 프로그래밍 개념과 용어를 몇 가지 이해해 보겠습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 호스트: CPU 및 해당 메모리를 가리킵니다.\n- 장치: GPU 및 해당 메모리를 가리킵니다.\n- 커널: 장치(GPU)에서 실행되는 함수를 가리킵니다.\n\n그래서 CUDA를 사용하여 작성된 기본 코드에서 프로그램은 호스트(CPU)에서 실행되며, 데이터를 장치(GPU)에 전송한 후 장치(GPU)에서 실행될 커널(함수)을 시작합니다. 이러한 커널은 병렬로 여러 스레드에 의해 실행됩니다. 실행이 완료되면 결과는 장치(GPU)에서 호스트(CPU)로 다시 전송됩니다.\n\n그러니까 두 벡터를 더하는 문제로 돌아가 봅시다:\n\n```js\n#include \u003cstdio.h\u003e\n\nvoid AddTwoVectors(float A[], float B[], float C[]) {\n    for (int i = 0; i \u003c N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\nint main() {\n    ...\n    AddTwoVectors(A, B, C);\n    ...\n}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nCUDA C/C++에서 프로그래머는 CUDA 스레드에 의해 병렬로 N번 실행되는 C/C++ 함수 인 켤널이라고 불리는 함수를 정의할 수 있습니다.\n\n켤널을 정의하려면 **global** 선언 지정자를 사용하고, 이 켤널을 실행하는 CUDA 스레드의 수는 ... 표기법을 사용하여 지정할 수 있습니다:\n\n```js\n#include \u003cstdio.h\u003e\n\n// 켤널 정의\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n    ...\n    // N개의 스레드로 켤널 호출\n    AddTwoVectors\u003c\u003c\u003c1, N\u003e\u003e\u003e(A, B, C);\n    ...\n}\n```\n\n각 스레드는 켤널을 실행하고 내장 변수를 통해 켤널 내에서 액세스할 수있는 고유한 스레드 ID threadIdx가 제공됩니다. 위의 코드는 크기가 N 인 두 벡터 A와 B를 더하여 결과를 벡터 C에 저장합니다. 순차적으로 각 쌍-wise 추가를 실행하는 루프 대신, CUDA는 우리에게 모든 이러한 작업을 N 개의 스레드를 사용하여 동시에 수행할 수 있도록 허용합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하지만 이 코드를 실행하기 전에 다른 수정 작업을 해야 합니다. 커널 함수는 장치(GPU) 내에서 실행되기 때문에 모든 데이터는 장치 메모리에 저장되어야 합니다. 다음 CUDA 내장 함수를 사용하여 이 작업을 수행할 수 있습니다:\n\n```js\n#include \u003cstdio.h\u003e\n\n// 커널 정의\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // 벡터의 크기\n    float A[N], B[N], C[N]; // 벡터 A, B, C를 위한 배열\n\n    ...\n\n    float *d_A, *d_B, *d_C; // 벡터 A, B, C에 대한 장치 포인터\n\n    // 벡터 A, B, C에 대한 장치 메모리 할당\n    cudaMalloc((void **)\u0026d_A, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_B, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_C, N * sizeof(float));\n\n    // 호스트에서 디바이스로 벡터 A와 B 복사\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // N 개의 스레드로 커널 호출\n    AddTwoVectors\u003c\u003c\u003c1, N\u003e\u003e\u003e(d_A, d_B, d_C);\n\n    // 디바이스에서 호스트로 벡터 C 복사\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n}\n```\n\n커널에 변수 A, B, C를 직접 전달하는 대신 포인터를 사용해야 합니다. CUDA 프로그래밍에서는 커널 런치 내에서 호스트 배열(예: 예제의 A, B, C)을 직접 사용할 수 없습니다. CUDA 커널은 장치 메모리에서 작동하므로 커널이 작동하도록 장치 포인터(d_A, d_B, d_C)를 전달해야 합니다.\n\n이를 넘어서 cudaMalloc을 사용하여 장치에 메모리를 할당하고, cudaMemcpy를 사용하여 호스트와 장치 간에 데이터를 복사해야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 벡터 A와 B의 초기화를 추가하고 코드 끝에 cuda 메모리를 새로고침할 수 있습니다.\n\n```js\n#include \u003cstdio.h\u003e\n\n// Kernel 정의\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // 벡터의 크기\n    float A[N], B[N], C[N]; // 벡터 A, B, C에 대한 배열\n\n    // 벡터 A와 B 초기화\n    for (int i = 0; i \u003c N; ++i) {\n        A[i] = 1;\n        B[i] = 3;\n    }\n\n    float *d_A, *d_B, *d_C; // 벡터 A, B, C에 대한 장치 포인터\n\n    // 벡터 A, B, C에 대한 메모리 장치에 할당\n    cudaMalloc((void **)\u0026d_A, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_B, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_C, N * sizeof(float));\n\n    // 호스트에서 장치로 벡터 A 및 B 복사\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // N 스레드를 사용하여 커널 호출\n    AddTwoVectors\u003c\u003c\u003c1, N\u003e\u003e\u003e(d_A, d_B, d_C);\n\n    cudaDeviceSynchronize(); // 커널 호출 뒤 cudaDeviceSynchronize() 추가\n\n    // 장치에서 호스트로 벡터 C 복사\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n    // 장치 메모리 해제\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n}\n```\n\n또한, 커널 호출 후에 cudaDeviceSynchronize();를 추가해야 합니다. 이 함수는 호스트 스레드를 장치와 동기화하는 데 사용됩니다. 이 함수가 호출되면 호스트 스레드는 계속 실행하기 전에 이전에 발행된 모든 CUDA 명령이 장치에서 완료될 때까지 기다립니다.\n\n또한 GPU에서 버그를 식별할 수 있도록 일부 CUDA 오류 확인을 추가하는 것이 중요합니다. 이 확인을 추가하지 않으면 코드가 계속해서 호스트 스레드(CPU)를 실행하고 CUDA 관련 오류를 식별하는 것이 어려울 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래에 두 기술의 구현이 있습니다.\n\n```js\n#include \u003cstdio.h\u003e\n\n// 커널 정의\n__global__ void AddTwoVectors(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // 벡터의 크기\n    float A[N], B[N], C[N]; // 벡터 A, B 및 C를 위한 배열\n\n    // 벡터 A와 B를 초기화\n    for (int i = 0; i \u003c N; ++i) {\n        A[i] = 1;\n        B[i] = 3;\n    }\n\n    float *d_A, *d_B, *d_C; // 벡터 A, B 및 C의 장치 포인터\n\n    // 장치에서 벡터 A, B 및 C에 대한 메모리 할당\n    cudaMalloc((void **)\u0026d_A, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_B, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_C, N * sizeof(float));\n\n    // 호스트에서 장치로 벡터 A와 B 복사\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // N개의 스레드로 커널 호출\n    AddTwoVectors\u003c\u003c\u003c1, N\u003e\u003e\u003e(d_A, d_B, d_C);\n\n    // 오류 확인\n    cudaError_t error = cudaGetLastError();\n    if(error != cudaSuccess) {\n        printf(\"CUDA 오류: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    // 모든 CUDA 스레드가 실행될 때까지 대기\n    cudaDeviceSynchronize();\n\n    // 장치에서 호스트로 벡터 C 복사\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n    // 장치 메모리 해제\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n}\n```\n\nCUDA 코드를 컴파일하고 실행하려면 시스템에 CUDA 툴킷이 설치되어 있는지 확인해야 합니다. 그런 다음 NVIDIA CUDA 컴파일러인 nvcc를 사용하여 코드를 컴파일할 수 있습니다. 만약 컴퓨터에 GPU가 없다면 Google Colab을 사용할 수 있습니다. Runtime → 노트 설정에서 GPU를 선택한 후 code.cu 파일에 코드를 저장하고 다음과 같이 실행하면 됩니다:\n\n```js\n%%shell\nnvcc example.cu -o compiled_example # 컴파일\n./compiled_example # 실행\n\n# 버그 감지 산소화 도구로 코드 실행도 가능합니다\ncompute-sanitizer --tool memcheck ./compiled_example\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나, 우리의 코드는 아직 완벽하게 최적화되지 않았습니다. 위의 예시에서는 크기가 N = 1000인 벡터를 사용했습니다. 그러나 이는 GPU의 병렬화 능력을 완전히 보여주지 못하는 작은 숫자입니다. 또한, 딥러닝 문제를 다룰 때는 종종 수백만 개의 매개변수를 가진 대규모 벡터를 다루게 됩니다. 그러나, 예를 들어 N = 500000으로 설정하고 위의 예시와 같이 1, 500000로 커널을 실행하면 오류가 발생할 것입니다. 이러한 작업을 개선하고 수행하기 위해서는 먼저 CUDA 프로그래밍의 중요한 개념인 Thread 계층 구조를 이해해야 합니다.\n\n# Thread 계층 구조\n\n커널 함수를 호출할 때는 블록의*개수, 블록당*쓰레드\\_개수 표기법을 사용합니다. 따라서, 위의 예시에서는 1개의 블록을 N개의 CUDA 쓰레드로 실행했습니다. 그러나, 각 블록은 지원할 수 있는 쓰레드 개수에 제한이 있습니다. 이는 블록 내의 모든 쓰레드가 동일한 스트리밍 멀티프로세서 코어에 있어야 하고 해당 코어의 메모리 자원을 공유해야 하기 때문에 발생합니다.\n\n다음 코드 스니펫을 사용하여 이 제한을 확인할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nint device;\ncudaDeviceProp props;\ncudaGetDevice(\u0026device);\ncudaGetDeviceProperties(\u0026props, device);\nprintf(\"블록당 최대 스레드 수: %d\\n\", props.maxThreadsPerBlock);\n```\n\n현재 코랩 GPU에서 스레드 블록 당 최대 1024개의 스레드가 포함될 수 있습니다. 그래서 우리는 예제에서 대량의 벡터를 처리하기 위해 훨씬 더 많은 스레드를 실행하기 위해 더 많은 블록이 필요합니다. 또한, 아래 그림에서 보여지는 대로 블록은 그리드로 구성됩니다.\n\n![CUDA Programming](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_4.png)\n\n이제 스레드 ID는 다음과 같이 액세스할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```c\n__global__ void AddTwoVectors(float A[], float B[], float C[], int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i \u003c N) // 배열 범위를 초과하지 않도록\n        C[i] = A[i] + B[i];\n}\n```\n\n그래서 우리의 스크립트는 다음과 같아졌습니다:\n\n```c\n#include \u003cstdio.h\u003e\n\n// 커널 정의\n__global__ void AddTwoVectors(float A[], float B[], float C[], int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i \u003c N) // 배열 범위를 초과하지 않도록\n        C[i] = A[i] + B[i];\n}\n\nint main() {\n    int N = 500000; // 벡터 크기\n    int threads_per_block;\n    int device;\n    cudaDeviceProp props;\n    cudaGetDevice(\u0026device);\n    cudaGetDeviceProperties(\u0026props, device);\n    threads_per_block = props.maxThreadsPerBlock;\n    printf(\"블록 당 최대 쓰레드 수: %d\\n\", threads_per_block); // 1024\n\n    float A[N], B[N], C[N]; // 벡터 A, B 및 C를 위한 배열\n\n    // 벡터 A와 B 초기화\n    for (int i = 0; i \u003c N; ++i) {\n        A[i] = 1;\n        B[i] = 3;\n    }\n\n    float *d_A, *d_B, *d_C; // 벡터 A, B 및 C를 위한 장치 포인터\n\n    // 벡터 A, B 및 C를 위한 장치에 메모리 할당\n    cudaMalloc((void **)\u0026d_A, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_B, N * sizeof(float));\n    cudaMalloc((void **)\u0026d_C, N * sizeof(float));\n\n    // 벡터 A와 B를 호스트에서 장치로 복사\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // 여러 블록과 블록 당 쓰레드 수로 커널 호출\n    int number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n    AddTwoVectors\u003c\u003c\u003cnumber_of_blocks, threads_per_block\u003e\u003e\u003e(d_A, d_B, d_C, N);\n\n    // 오류 확인\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA 오류: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    // 모든 CUDA 쓰레드가 실행될 때까지 대기\n    cudaDeviceSynchronize();\n\n    // 벡터 C를 장치에서 호스트로 복사\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n    // 장치 메모리 해제\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n}\n```\n\n# 성능 비교\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음은 다른 벡터 크기에 대해 CPU 및 GPU 연산을 비교한 것입니다.\n\n![Comparison of CPU and GPU computation](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_5.png)\n\n어떤 경우에는 대규모 벡터 크기 N에 대해 GPU 처리의 이점이 뚜렷해집니다. 또한, 시간 비교는 커널/함수의 실행만을 고려한 것임을 기억해주세요. 호스트와 장치 간 데이터 복사에 소요되는 시간은 고려되지 않았는데, 일반적으로 큰 문제가 아닐 수 있지만, 우리의 경우에는 단순 덧셈 연산만 수행하기 때문에 상당히 중요합니다. 따라서, GPU 계산은 고도로 계산 집약적이고 또한 고도로 병렬화된 계산을 다룰 때만 그 이점을 나타냄을 기억하는 것이 중요합니다.\n\n# 다차원 스레드\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n좋아요, 이제 간단한 배열 작업의 성능을 향상시키는 방법을 알게 되었습니다. 그러나 딥 러닝 모델을 다룰 때는 행렬 및 텐서 작업을 처리해야 합니다. 이전 예제에서는 N 스레드를 사용하여 1차원 블록만 사용했습니다. 그러나 최대 3차원까지 다차원 스레드 블록을 실행할 수도 있습니다. 행렬 작업을 실행해야 할 경우 편리하게 NxM 스레드의 스레드 블록을 실행할 수 있습니다. 이 경우에는 행 = threadIdx.x, 열 = threadIdx.y와 같이 행렬 행 및 열 인덱스를 얻을 수 있습니다. 또한 편리하게 number_of_blocks 및 threads_per_block을 정의하는 데 dim3 변수 유형을 사용할 수 있습니다.\n\n아래 예제는 두 행렬을 더하는 방법을 보여줍니다.\n\n```js\n#include \u003cstdio.h\u003e\n\n// Kernel definition\n__global__ void AddTwoMatrices(float A[N][N], float B[N][N], float C[N][N]) {\n    int i = threadIdx.x;\n    int j = threadIdx.y;\n    C[i][j] = A[i][j] + B[i][j];\n}\n\nint main() {\n    ...\n    // 1개의 NxN 스레드 블록을 사용하여 커널 호출\n    dim3 threads_per_block(N, N);\n    AddTwoMatrices\u003c\u003c\u003c1, threads_per_block\u003e\u003e\u003e(A, B, C);\n    ...\n}\n```\n\n이 예제를 여러 블록을 처리할 수 있도록 확장할 수도 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n#include \u003cstdio.h\u003e\n\n// Kernel definition\n__global__ void AddTwoMatrices(float A[N][N], float B[N][N], float C[N][N]) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i \u003c N \u0026\u0026 j \u003c N) {\n        C[i][j] = A[i][j] + B[i][j];\n    }\n}\n\nint main() {\n    ...\n    // Kernel invocation with 1 block of NxN threads\n    dim3 threads_per_block(32, 32);\n    dim3 number_of_blocks((N + threads_per_block.x - 1) ∕ threads_per_block.x, (N + threads_per_block.y - 1) ∕ threads_per_block.y);\n    AddTwoMatrices\u003c\u003c\u003cnumber_of_blocks, threads_per_block\u003e\u003e\u003e(A, B, C);\n    ...\n}\n```\n\n이런 멀티 차원 데이터를 다루는 법을 알게 되어서 좋습니다. 또한, 커널 내에서 함수를 호출하는 방법을 알아보겠습니다. 기본적으로 이는 **device** 선언 지정자를 사용하여 간단히 수행할 수 있습니다. 이는 기기(GPU)에서 직접 호출할 수 있는 함수를 정의합니다. 따라서 이러한 함수들은 오직 **global** 또는 다른 **device** 함수에서만 호출할 수 있습니다. 아래 예시는 시그모이드 연산을 벡터에 적용하는 방법을 보여줍니다.\n\n```js\n#include \u003cmath.h\u003e\n\n// Sigmoid function\n__device__ float sigmoid(float x) {\n    return 1 / (1 + expf(-x));\n}\n\n// Kernel definition for applying sigmoid function to a vector\n__global__ void sigmoidActivation(float input[], float output[]) {\n    int i = threadIdx.x;\n    output[i] = sigmoid(input[i]);\n\n}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그래서, 이제 CUDA 프로그래밍의 기본적인 중요한 개념을 알았으니 CUDA 커널을 만들기 시작할 수 있어요. 딥 러닝 모델의 경우, 그들은 기본적으로 합, 곱셈, 컨볼루션, 정규화 등과 같은 매트릭스 및 텐서 연산들의 집합입니다. 예를 들어, 단순한 행렬 곱셈 알고리즘은 다음과 같이 병렬화될 수 있어요:\n\n![Image](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_6.png)\n\n```js\n// GPU 버전\n\n__global__ void matMul(float A[M][N], float B[N][P], float C[M][P]) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row \u003c M \u0026\u0026 col \u003c P) {\n        float C_value = 0;\n        for (int i = 0; i \u003c N; i++) {\n            C_value += A[row][i] * B[i][col];\n        }\n        C[row][col] = C_value;\n    }\n}\n```\n\n이제 이것을 아래의 두 행렬 곱셈의 일반 CPU 구현과 비교해보세요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n// CPU 버전\n\nvoid matMul(float A[M][N], float B[N][P], float C[M][P]) {\n    for (int row = 0; row \u003c M; row++) {\n        for (int col = 0; col \u003c P; col++) {\n            float C_value = 0;\n            for (int i = 0; i \u003c N; i++) {\n                C_value += A[row][i] * B[i][col];\n            }\n            C[row][col] = C_value;\n        }\n    }\n}\n```\n\nGPU 버전에는 더 적은 루프가 있어서 작업이 빨라진다는 것을 알 수 있습니다. 아래는 NxN 행렬 곱셈의 CPU와 GPU 성능 비교입니다:\n\n![performance-comparison](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_7.png)\n\n행렬의 크기가 커질수록 GPU 처리의 성능 향상이 더 큰 것을 관찰할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 기본 신경망을 고려해 보세요. 주로 y = σ(Wx + b) 작업을 포함하는데, 아래 그림과 같이 구성됩니다:\n\n![Neural Network Operations](/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_8.png)\n\n이러한 작업은 주로 행렬 곱셈, 행렬 덧셈, 배열에 함수를 적용하는 것으로 이루어져 있습니다. 병렬화 기술에 익숙하신 분들이라면 이미 이들을 알고 계실 것입니다. 따라서 이제 GPU에서 실행되는 자체 신경망을 처음부터 구현할 수 있는 능력이 생겼습니다!\n\n# 결론\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 게시물에서는 GPU 처리에 대한 입문적인 개념을 다루어 딥러닝 모델의 성능을 향상시키는 방법에 대해 알아보았어요. 그러나 본 포스트에서 다룬 내용은 기초적인 것들뿐이며, 더 많은 것을 배울 수 있습니다. PyTorch와 Tensorflow와 같은 라이브러리는 최적화 기술을 구현하고 있으며, 최적화된 메모리 액세스, 배치 연산 등과 같은 보다 복잡한 개념들을 활용합니다 (이들은 cuBLAS 및 cuDNN과 같은 CUDA 기반 라이브러리 위에서 구축된 라이브러리를 활용합니다). 그러나 \"cuda\"로 지정하고 GPU에서 딥러닝 모델을 실행할 때 무슨 일이 벌어지는지에 대한 배경 정보를 이해하는 데 이 게시물이 도움이 되기를 바랍니다.\n\n향후 게시물에서는 CUDA 프로그래밍에 관련된 더 복잡한 개념들을 소개할 예정이에요. 의견을 주시거나 다음에 대해 무엇을 쓰기를 원하시는지 알려 주시면 감사하겠어요! 읽어주셔서 너무 감사해요! 😊\n\n# 추가 자료\n\nNVIDIA CUDA 프로그래밍 문서 — NVIDIA CUDA 프로그래밍 가이드.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nCUDA 문서 — NVIDIA의 완전한 CUDA 문서입니다.\n\nCUDA 신경망 훈련 구현 — 순수 CUDA C++로 구현된 신경망 훈련입니다.\n\nCUDA LLM 훈련 구현 — 순수 CUDA C로 구현된 LLM의 훈련입니다.\n","ogImage":{"url":"/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_0.png"},"coverImage":"/assets/img/2024-05-23-WhyDeepLearningModelsRunFasteronGPUsABriefIntroductiontoCUDAProgramming_0.png","tag":["Tech"],"readingTime":23},{"title":"재밌는 주제 프로덕션 환경에서 신뢰할 수 있는 딥러닝 스택 구축하는 방법 이에 대해 알아보도록 하죠","description":"","date":"2024-05-23 17:14","slug":"2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction","content":"\nPrometheus, Triton, 그리고 Grafana를 사용하여 엔드 투 엔드 모니터링 대시보드를 구축해보세요.\n\nML 시스템을 배포하기 전에, 엔지니어들은 로컬 및 대규모에서 어떻게 성능을 발휘할지 정확한 통찰력이 필요합니다. 병목 현상을 식별하고 예상치 못한 동작을 파악하기 위해.\n\n클래식 ML 추론 파이프라인과 비교하면, 딥 러닝 시스템은 자원 소비, 복잡성 및 확장 가능성의 도전에 따라 낮은 지연 시간, 높은 처리량에 중점을 둔다는 것 때문에 보다 \"중요한\" 및 자세한 모니터링이 필요합니다. 특히, 컴퓨터 비전과 같은 자원 집중적인 응용 프로그램에서 딥 러닝 배포를 위해 ML 엔지니어들은 모니터링에 우선순위를 두어야 합니다.\n\n이 글에서는 배포 설정 및 모니터링의 워크플로우에 대해 다루겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 목차\n\n1. 성능 모니터링 파이프라인 설정 방법\n\n- 컨테이너\n- 설정 파일\n- 도커 컴포즈\n\n2. 메트릭 스크랩 구성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n프로메테우스 타겟을 추가하기\n\n- 그라파나 데이터소스 추가하기\n- 헬스체크 대상 스크랩\n\n3. 대시보드 생성\n\n- GPU 메트릭을 위한 패널\n- CPU/RAM 메트릭을 위한 패널\n\n4. 시각화\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로 넘어가기 전에 사용할 도구들을 살펴봅시다:\n\n- 도커는 가벼우고 휴대용한 컨테이너 내에서 애플리케이션을 개발, 배포, 실행할 수 있는 플랫폼입니다 — ML 엔지니어에게 꼭 필요한 도구입니다.\n- 도커 컴포즈는 멀티 컨테이너 애플리케이션을 정의하고 구성하는 도구입니다.\n- cAdvisor는 구글에서 개발한 리소스 사용량 및 컨테이너 성능 지표를 제공해주는 오픈소스 도구입니다.\n- 프로메테우스는 메트릭을 수집하고 저장하는 모니터링 및 경보 시스템으로, 프로메테우스에 대한 전문 지식은 ML/MLOps 엔지니어에게 큰 장점입니다.\n- 그라파나는 모니터링 및 가시성 플랫폼으로, 배포된 시스템의 메트릭을 생성, 시각화, 경보 및 이해할 수 있게 해줍니다. 모니터링 대시보드를 관리하는 것은 MLOps 엔지니어에게 중요한 기술입니다.\n- Triton Inference Server는 NVIDIA에서 개발한 인기 있는 모델 서빙 프레임워크로, 복잡한 ML 모델을 프로덕션 환경에 배포하는 데 중요한 역할을 합니다. Triton에 대한 전문 지식은 MLOps 엔지니어에게 필수적인 기술입니다.\n\n# 1. 도커 컴포즈 설정\n\n각 서비스가 무엇을 하는지 설명하고, 이러한 서비스를 캡슐화하고 실행할 도커 컴포즈를 준비하는 것부터 시작해봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음과 같은 내용이 있습니다:\n\n![이미지](/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_0.png)\n\n도커 컴포즈 모니터링 yaml 파일을 살펴보겠습니다.\n\n```yaml\n# cat docker-compose-monitoring.yaml\nversion: \"3.4\"\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```yaml\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"${PROMETHEUS_PORT}:${PROMETHEUS_PORT}\"\n    container_name: prometheus\n    restart: always\n    volumes:\n      - \"${MONITORING_CONFIGURATIONS}/prometheus.monitoring.yml:/etc/prometheus/prometheus.monitoring.yml\"\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    command:\n      - \"--config.file=/etc/prometheus/prometheus.monitoring.yml\"\n      - \"--enable-feature=expand-external-labels\"\n    depends_on:\n      - cadvisor\n    networks:\n      monitor-net:\n        ipv4_address: ${PROM_IP}\n  grafana:\n    image: grafana/grafana-enterprise:8.2.0\n    container_name: grafana\n    ports:\n      - \"${GRAFANA_PORT}:${GRAFANA_PORT}\"\n    volumes:\n      - ${MONITORING_CONFIGURATIONS}/datasources:/etc/grafana/provisioning/datasources\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PWD}\n      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}\n    networks:\n      monitor-net:\n        ipv4_address: ${GRAFANA_IP}\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor:latest\n    container_name: cadvisor\n    restart: always\n    ports:\n      - \"${CADVISOR_PORT}:${CADVISOR_PORT}\"\n    volumes:\n      - \"/etc/localtime:/etc/localtime:ro\"\n      - \"/etc/timezone:/etc/timezone:ro\"\n      - \"/:/rootfs:ro\"\n      - \"/var/run:/var/run:rw\"\n      - \"/sys:/sys:ro\"\n      - \"/var/lib/docker:/var/lib/docker:ro\"\n    networks:\n      monitor-net:\n        ipv4_address: ${CADVISOR_IP}\n  triton_server:\n    container_name: tis2109\n    image: nvcr.io/nvidia/tritonserver:21.09-py3\n    privileged: true\n    ports:\n      - \"8002:8002\"\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\n    volumes:\n      - ${TRITON_MODELS_REPOSITORY}:/models\n    command:\n      [\n        \"tritonserver\",\n        \"--model-repository=/models\",\n        \"--strict-model-config=false\",\n      ]\n    networks:\n      monitor-net:\n        ipv4_address: ${TRITON_IP}\nnetworks:\n  monitor-net:\n    driver: bridge\n    internal: false\n    ipam:\n      driver: default\n      config:\n        - subnet: ${SUBNET}\n          gateway: ${GATEWAY}\n```\n\n보시다시피, .yaml 설정에 일부 마스킹된 $'변수'가 있습니다. 이들은 .env 파일 내부에서 자동으로 상속되어 로컬 개발 및 CI/CD 파이프라인에서 최상의 관행을 따르는 흐름을 가지고 있습니다.\n\n이제 .env 파일에 어떤 것이 있는지 살펴봅시다:\n\n```yaml\n# == 모니터링 변수 ==\nPROMETHEUS_PORT=9090\nGRAFANA_PORT=3000\nCADVISOR_PORT=8080\nMONITORING_CONFIGURATIONS=\u003cyour_configuration_files에 대한_경로\u003e\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n# == 자격 증명 ==\nGRAFANA_PWD=admin\nGRAFANA_USER=admin\n# == TIS 변수 ==\nTRITON_MODELS_REPOSITORY=\u003c당신의_triton_모델_저장소_경로\u003e\n# == 기본 네트워크 ==\nSUBNET=172.17.0.0/16\nGATEWAY=172.17.0.1\n# == 서브넷 IP ==\nTRITON_IP=172.17.0.3\nCADVISOR_IP=172.17.0.4\nPROM_IP=172.17.0.5\nGRAFANA_IP=172.72.0.6\n```\n\n대부분의 변수가 설정되었지만, 여기서 살펴봐야 할 주요한 2가지 변수는 다음과 같습니다:\n\n- MONITORING_CONFIGURATIONS\n  이것은 이러한 구조가 있는 폴더를 가리켜야 합니다\n\n```js\n.__ monitoring\n|  |_ datasources\n|  | |_ datasources.yml\n|  |_ prometheus.monitoring.yml\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- TRITON_MODEL_REPOSITORY\n  모델 저장소의 구조는 다음과 같아야 합니다:\n\n```js\nmodel_repository\n└── prod_client1_encoder\n    └── 1\n        └──resnet50.engine\n    └── config.pbtxt\n```\n\n프로메테우스 모니터링 파일인 prometheus.monitoring.yml에는 메트릭을 가져올 대상(컨테이너)을 추가할 것입니다.\n데이터 소스 파일인 datasources.yml에는 그라파나 대시보드의 소스로 프로메테우스를 추가할 것입니다. 그렇게 하면 그라파나 UI를 열 때 나타날 것입니다.\n\n# 2. 프로메테우스 스크래핑 구성 정의\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그럼 Prometheus 대상을 구성해 보겠습니다. `prometheus.monitoring.yml` 파일에 작성하겠습니다.\n\n```yaml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n```\n\n```yaml\nscrape_configs:\n  - job_name: \"prometheus\"\n    static_configs:\n      - targets: [\"172.17.0.5:9090\"]\n  - job_name: \"triton-server\"\n    static_configs:\n      - targets: [\"172.72.0.3:8002\"]\n\n  - job_name: \"cadvisor\"\n    static_configs:\n      - targets: [\"172.72.0.4:8080\"]\n```\n\n3개의 대상이 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 프로메테우스 — 모니터링 프로메테우스 자체를 유지하는 것이 건강한 모니터링을 위한 모범 사례로 작용합니다. 수천 개의 메트릭을 처리하면 병목 현상이 발생할 수 있으며 프로메테우스 자체의 자원 사용량을 알고 있는 것이 유용합니다.\n- Triton Server — 이것은 이 딥러닝 스택의 핵심에 있어 중요합니다. ML 모델을 제공하고 관리하기 때문입니다.\n  Triton은 인퍼런스 프로세스 전반에 걸쳐 다양한 메트릭을 제공하는 포트 8002의 내장된 프로메테우스 엔드포인트가 있습니다.\n- cAdvisor — 이 배포에서 컨테이너 전반의 CPU/RAM 사용량 정보를 얻기 위해 사용됩니다.\n\n모두 구성한 뒤, 컴포저를 시작하고 문제가 있는지 검사할 수 있습니다.\n컨테이너를 시작해 봅시다.\n\n```js\ndocker compose -f docker-compose-monitoring.yaml up -d\n```\n\n프로메테우스 대상을 검사해 봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 웹 브라우저로 가서 전체 Prometheus URL(IP:9090)을 입력해주세요.\n- 상태 → 대상(Targets)으로 이동해주세요.\n- 스크래핑 구성에서 각 대상이 정상인지 확인해주세요(녹색).\n\n이러한 사항을 확인한 후에는 Grafana에서 대시보드를 만들어 진행할 수 있습니다.\n\n# #3 대시보드 생성하기\n\nGrafana WebUI 대시보드에 액세스하려면 브라우저를 열고 `localhost:3000`으로 이동해주세요. 여기서 3000은 Grafana 컨테이너를 실행하는 포트입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n로그인 페이지로 이동하면 사용자 이름/암호 필드에 `admin/admin`을 사용하십시오. 더 높은 보안이 권장되지만, 이는 이 기사의 범위에 포함되지 않습니다.\n\nGrafana 웹을 열었으면 다음을 수행해야 합니다:\n\n- 데이터 소스를 우리의 Prometheus 메트릭 스크래퍼 엔드포인트로 지정합니다.\n- 새 대시보드 생성\n- 관심 있는 메트릭을 집계/시각화하기 위해 차트 추가\n\n#3.1 Prometheus 데이터 소스\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n왼쪽 패널에서 기어 아이콘(설정)을 클릭하고 DataSources를 선택하세요.\n다음과 같은 뷰가 나타날 것입니다:\n\n\"Add data source\"를 클릭한 후 Time Series Databases 아래에서 `Prometheus`를 선택하세요. Grafana는 여러 종류의 메트릭 스크랩을 지원하고 있습니다. 여기서는 Prometheus를 사용할 것입니다. 이런 뷰가 나타날 것입니다:\n\n\u003cimg src=\"/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_1.png\" /\u003e\n\n여기에는 Prometheus 엔드포인트의 URL을 추가해야 합니다. 우리의 도커 컴포즈 배포에서는 `http://prometheus:9090`을 사용할 것입니다. 이 템플릿을 따르면 `http://container_name:container_port`가 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_2.png\" /\u003e\n\n여기까지 오셨군요. 이제 데이터 원본 추가 섹션이 완료되었습니다.\n이제 대시보드를 만들어보겠습니다.\n\n### 3.2 Grafana 대시보드 만들기\n\n왼쪽 패널에서 “+” 표시를 클릭하고 `Dashboard`를 선택하세요. 이렇게 하면 미리 정의된 패널 그룹이 있는 새 대시보드 페이지로 이동합니다. 우리는 모든 것을 처음부터 만들고 있으므로 `Empty Panels`만 사용하여 주요 지표를 표시할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음은 한 예제에 대한 따라할 프로세스입니다:\n\n- 새 쿼리를 추가하고 `promql` (Prometheus 쿼리 언어)를 정의합니다.\n- 시각화 유형, 그래프 스타일, 범례를 구성합니다.\n\n다음은 비어 있는 패널의 모습입니다:\n\n이제, 트리튼 추론 서버 모델 서빙 플랫폼을 모니터링하기 위해 몇 가지 사용자 정의 쿼리를 추가할 것입니다. 하지만 먼저 다음 참고 사항을 유념해야 합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n첫 번째 쿼리를 설정해보겠습니다. 이 쿼리는 성공적인 요청의 수를 고려하여 모델이 하나의 추론 요청을 수행하는 데 걸리는 시간(밀리초)을 측정할 것입니다. 우리는 시간이 지남에 따라 진행 상황을 보고 싶기 때문에 이 차트는 `시계열(time-series)`이 될 것입니다.\n다음은 해당 지표를 작성하는 쿼리입니다:\n\n```js\n(irate(nv_inference_compute_infer_duration_us{job=\"triton-server\"}[$__rate_interval]) / 1000) / irate(nv_inference_request_success{job=\"triton-server\"}[$__rate_interval])\n```\n\n쿼리를 해석해 봅시다:\n\n아래에서 쿼리의 모습을 확인할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 새 차트의 설정을 구성할 때, 오른쪽에 다음을 지정할 수 있습니다:\n\n- 차트 유형 — 직선 / 곡선 / T-스텝 라인 중 선택\n- 메트릭 범위 — 메트릭 선택(예: 밀리초(ms))하고 low_range(예: 0)와 high_range(예: 100ms) 정의\n- 사용자 지정 텍스트 — 범례 또는 다른 필드에 표시될 내용\n\n#3.3 시각화 완료\n\n위의 흐름에 따라, 나머지 차트를 생성할 수 있습니다.\n전체 성능 모니터링 차트를 컴파일하려면 나머지 패널을 추가하십시오. 다음 각각에 대해 새 패널을 만들고 해당 세부 정보로 채워넣습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- GPU 사용된 바이트 - VRAM 사용량의 백분율\n\n```js\n쿼리: nv_gpu_memory_used_bytes{job=\"triton-server\"}/nv_gpu_memory_total_bytes{job=\"triton-server\"}\n차트 유형: 파이\n범례: {인스턴스}\n```\n\n2. GPU 활용도 - 전체 GPU 활용도\n\n```js\n쿼리: nv_gpu_utilization{job=\"triton-server\"}\n차트 유형: 시계열\n범례: NULL\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n3. 입력 시간/요청 — 클라이언트가 입력 데이터를 Triton 서버로 보내는 데 걸린 시간.\n\n```js\nQuery: (irate(nv_inference_compute_input_duration_us{job=\"triton-server\"}[$__rate_interval]) / 1000) / irate(nv_inference_request_success{job=\"triton-server\"}[$__rate_interval])\nChart Type: 시계열\nLegend: {model}-{version}\n```\n\n4. 출력 시간/요청 — 서버가 클라이언트에게 출력을 보내는 데 걸린 시간.\n\n```js\nQuery: (irate(nv_inference_compute_output_duration_us{job=\"triton-server\"}[$__rate_interval]) / 1000)/ irate(nv_inference_request_success{job=\"triton-server\"}[$__rate_interval])\nChart Type: 시계열\nLegend: {model}-{version}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n5. DB 비율 (#요청/#실행) — 성공적인 요청의 전체 요청 대비 비율\n\n```js\n쿼리: sum by (모델,버전) (rate(nv_inference_request_success{job=\"triton-server\"}[$__rate_interval])/rate(nv_inference_exec_count{job=\"triton-server\"}[$__rate_interval]) )\n차트 유형: 시계열\n범례: {모델}-{버전}\n```\n\n6. 대기 시간/요청 — 요청이 처리되기 전에 대기하는 시간\n\n```js\n쿼리: sum by (모델,버전) ((irate(nv_inference_queue_duration_us{job=\"triton-server\"}[$__rate_interval]) / 1000) / irate(nv_inference_request_success{job=\"triton-server\"}[$__rate_interval]))\n차트 유형: 시계열\n범례: {모델}-{버전}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n7. 집계된 입력/추론/출력 - 입력 출력 및 추론을 한 차트에 표시합니다.\n\n```js\n질의:\nA: rate(nv_inference_compute_input_duration_us{job=\"triton-server\"}[$__interval]) / 1000\nB: rate(nv_inference_compute_infer_duration_us{job=\"triton-server\"}[$__interval]) / 1000\nC: rate(nv_inference_compute_output_duration_us{job=\"triton-server\"}[$__interval]) / 1000\n\n차트 유형: 시계열\n범례: {모델}-{버전}\n```\n\n우리가 만든 완벽한 대시보드는 다음을 보여줍니다:\n\n- GPU VRAM 이용률\n- 클라이언트에서 서버로의 입력 송신 시간\n- 서버 추론 요청 시간\n- 서버에서 클라이언트로의 출력 송신 시간\n- 성공 요청/전체 요청 비율\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이런 종류의 대시보드는 배포된 스택의 성능 및 스트레스 테스트 속에서의 동작을 모니터링하기 위한 시작점을 제시합니다.\n\n이는 배포의 실패 및 위험 지점을 연구하는 구체적인 방법을 제공하며 SLI(서비스 수준 지표)를 모니터링하는 데 도움이 됩니다.\n\n서비스 수준 지표는 SLA(서비스 수준 계약)를 준수하고 SLO(서비스 수준 목표)를 달성하기 위해 모니터링되는 메트릭입니다. 우리가 만든 대시보드는 제공되는 서비스 수준 계약을 준수하기 위한 목표에 도달하기 위한 가치 있는 통찰을 제공할 수 있습니다.\n\n또한 이는 다중 복제본을 추가하거나 추론 서빙 프레임워크를 실행하는 여러 기계로의 확장 전략을 계획하는 데 도움이 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n본 문서에서는 Prometheus와 Grafana를 사용하여 자사의 ML 애플리케이션 및 모델 서빙 프레임워크를 위한 성능 모니터링 스택을 설정하고 구축하는 방법을 소개했습니다.\n\n도커 컴포즈 파일을 준비하는 것부터 시작하여 워크플로우의 각 단계를 설명하고 스택을 배포하고 데이터 원본을 구성하며 대시보드 패널을 생성하고 지표를 집계하는 과정을 마무리했습니다.\n\n모니터링은 MLOps 시스템의 중요한 부분입니다!\n이 튜토리얼을 따라 하면 테스트 환경에서 단일 배포로 ML 애플리케이션을 위한 모니터링 파이프라인을 구조화하고 배포하거나, 클라우드 시나리오 설정에서 여러 입력 소스를 결합하고 전체 스택 배포를 모니터링하는 단일 대시보드 소비자 지점을 갖도록 구성할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 더 많은 내용을 원하신다면!\n\n저는 미디엄에 새롭게 등장했습니다. 만약 이 글을 즐겨보셨다면 박수를 보내주시고 제 계정을 팔로우해주세요 - 정말로 감사하겠습니다! 🚀\n\n![How to ensure your deep learning stack is fail-safe in production](/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_3.png)\n\n# 더 많은 글 보기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 게시물과 관련성에 따라 정렬되었습니다.\n","ogImage":{"url":"/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_0.png"},"coverImage":"/assets/img/2024-05-23-Howtoensureyourdeeplearningstackisfail-safeinproduction_0.png","tag":["Tech"],"readingTime":18},{"title":"자기 주의적 문장 임베딩을 사용한 추천 시스템","description":"","date":"2024-05-23 17:13","slug":"2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem","content":"\n![Self-attentive sentence embedding](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_0.png)\n\n# 소개\n\n트랜스포머 레이어와 그의 어텐션 메커니즘은 자연어처리(NLP) 커뮤니티에서 가장 중요한 아이디어 중 하나입니다. 최근 세계를 휩쓴 ChatGPT와 LLaMA와 같은 대규모 언어 모델에서 핵심 역할을 합니다.\n\n하지만 NLP 커뮤니티에서 시작된 다른 흥미로운 아이디어가 있는데, 그 영향은 주로 추천 시스템 분야에서 실현됩니다. 바로 자기주의적 문장 임베딩(self-attentive sentence embedding)입니다. 이 기사에서는 자기주의적 문장 임베딩[1]과 추천 시스템에 적용하는 방법을 살펴볼 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 작동 방식\n\n## 전체 아이디어\n\n이 논문의 주된 아이디어는 문장을 여러 임베딩으로 인코딩하여 문장의 다양한 측면을 포착할 수 있는 더 나은 방법을 찾는 것입니다. 구체적으로, 저자들은 문장을 단일 임베딩으로 인코딩하는 대신 각 행 임베딩이 문장의 다른 측면을 포착하는 2D 행렬로 인코딩하고자 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n문장 임베딩을 얻으면, 문장 분석, 작가 프로파일링, 텍스트 함의 등 다양한 하위 작업에 사용할 수 있습니다.\n\n## 모델 아키텍처\n\n모델 입력은 문장 배치입니다. 각 문장은 n개의 토큰을 가지고 있습니다. 우리는 i번째 문장을 다음과 같이 표현할 수 있습니다:\n\n![image](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nd는 표현의 숨겨진 차원을 나타내며, 우리는 문장 s를 n by d 행렬 H로 인코딩할 수 있습니다:\n\n![image](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_3.png)\n\n여기서 F는 문장의 토큰을 임베딩으로 인코딩하는 모델 함수를 나타냅니다. 논문에서는 단어 임베딩(Word2Vec을 사용하여 초기화)을 이용하여 토큰을 인코딩하고 이를 양방향 LSTM을 통해 전달합니다. 토큰을 임베딩으로 인코딩하는 다양한 방법이 있기 때문에 일반화를 위해 여기서 F를 사용했습니다.\n\n다음으로, 그들은 임베딩 H를 입력으로 사용하여 어텐션 가중치 행렬 A를 학습합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image #1](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_4.png)\n\nHere, the softmax() is applied to the second dimension of its input. We can view the formula as a 2-layer MLP without bias.\n\nAs we can see from the above formula, the attention weight A matrix will have a shape of r by n where r is the number of aspects a sentence can have and n is the sentence length. The authors argue that there are many aspects that make up the semantics of a sentence. Thus, they need r embeddings to focus on different parts of the sentence. In other words, each embedding in A is the sentence attention weight:\n\n![Image #2](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_5.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nTransformer처럼 이 행렬 A의 시각화를 통해 문장에 대한 각 측면의 주의를 더 잘 이해할 수 있습니다.\n\n마지막으로, 우리는 H와 A를 곱하여 r by d 행렬 M을 얻음으로써 문장 임베딩을 생성합니다:\n\n![image](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_6.png)\n\nM의 각 행은 토큰 임베딩과 그 토큰에 대한 측면의 가중치의 가중 합입니다. 시각적으로는 이렇게 보입니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_7.png)\n\n## Regularization\n\nIn the paper, they also introduce a new regularization term:\n\n![image](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_8.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n표를 마크다운 형식으로 변경해주세요.\n\nF는 행렬의 프로베니우스 노름을 나타냅니다.\n\n정규화 항은 2가지 목적을 제공합니다:\n\n- 측면 임베딩이 겹칠 수 있기 때문에 다양성을 높입니다. 즉, 유사할 수 있음을 의미합니다.\n- 각 관심사가 가능한 적은 토큰에 초점을 맞추도록 만듭니다.\n\n이 글에서는 정규화가 중점이 아니기 때문에 정규화가 어떻게 작동하는지에 대해 더 읽어볼 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 추천 시스템에서의 다중 관심사\n\n셀프 어텐티브 문장 임베딩이 어떻게 작동하는지 이해하면, 추천 시스템에서 어떻게 활용할 지에 대해 집중할 수 있습니다.\n\n대규모 추천 시스템에서, 보통 두 개의 타워 모델 아키텍처를 사용합니다. 하나는 사용자 정보를 인코딩하고, 다른 하나는 후보 정보를 인코딩합니다. 사용자 타워에는 사용자의 과거 행동인 클릭, 좋아요, 공유 순서와 사용자 프로필을 사용합니다. 후보 타워에는 아이템 ID와 아이템 카테고리와 같은 후보 특징을 사용합니다.\n\n사용자 임베딩과 후보 임베딩을 내적하여 후보 아이템이 사용자에게 얼마나 관련 있는지를 반영합니다. 레이블은 사용자 시퀀스에서 다음 상호작용할 아이템입니다. 따라서 모델 목표는 사용자가 다음에 상호작용할 아이템을 예측하는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Self-attentive sentence embedding for the recommendation system](/assets/img/2024-05-23-Self-attentive-sentence-embedding-for-the-recommendation-system_9.png)\n\n위 이미지에서 확인할 수 있듯이 사용자 타워의 출력은 모든 사용자 정보를 포함하는 임베딩입니다. 그러나 단일 사용자 임베딩은 모든 사용자의 다양한 관심사를 포착하는 데 좋지 않습니다. 따라서 더 나은 해결책은 사용자의 관심사를 여러 임베딩으로 인코딩하는 것입니다.\n\n사용자의 다양한 관심사를 어떻게 포착할지에 대한 많은 연구가 이루어졌습니다. 가장 두드러지는 두 가지 방법은 self-attentive 임베딩(SA) [2]과 dynamic routing (DR) [3]입니다. 두 방법 모두 비슷한 성능을 보이지만, self-attentive 방법이 더 안정적이고 훈련 속도가 빠릅니다.\n\nself-attentive 방법이 어떻게 작동하는지 이해하면 추천 시스템에 적용하는 것은 간단합니다. 입력으로 문장 토큰 대신에 유튜브에서 시청한 비디오 ID 목록이나 이커머스 플랫폼에서 클릭/주문한 상품 ID와 같은 사용자 행동을 사용합니다. 출력은 각 임베딩이 문장에서의 측면이 아니라 사용자 관심사를 인코딩합니다!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Table 1](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_10.png)\n\nIn the ComiRec paper [2], the authors compare the self-attentive method with the dynamic routing method along with other popular models that produce a single user interest:\n\n![Table 2](/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_11.png)\n\nAs the table shows, the self-attentive method produces results comparable to those of the dynamic routing method. Still, both multi-interest embedding solutions are significantly better than their single-interest embedding counterparts.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 마무리\n\n여러 관심사를 포함한 임베딩을 사용하여 모델을 교육하고 제공하는 것에는 많은 미묘한 점이 있습니다. 이 기사에서는 자가 주목 방법이 작동하는 방법과 이를 추천 시스템에서 어떻게 사용하는지에 대해 안내했습니다. 이러한 모델을 교육하고 제공하는 데 대한 더 자세한 내용은 논문을 읽는 것만큼 좋은 자료는 없습니다. 이 기사는 추천 시스템을 위한 다중 관심 프레임워크를 이해하고자 하는 여정에서 참고할 만한 자료입니다.\n\n# 참고\n\n[1] Lin, Zhouhan, et al. “A structured self-attentive sentence embedding.” arXiv preprint arXiv:1703.03130 (2017).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## References\n\n[1] Cen, Yukuo, et al. “Controllable multi-interest framework for recommendation.” Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining. 2020.\n\n[2] Li, Chao, et al. “Multi-interest network with dynamic routing for recommendation at Tmall.” Proceedings of the 28th ACM international conference on information and knowledge management. 2019.\n","ogImage":{"url":"/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_0.png"},"coverImage":"/assets/img/2024-05-23-Self-attentivesentenceembeddingfortherecommendationsystem_0.png","tag":["Tech"],"readingTime":9},{"title":"포디즘의 새로운 모습","description":"","date":"2024-05-23 17:11","slug":"2024-05-23-ThenewfaceofFordism","content":"\n\u003cimg src=\"/assets/img/2024-05-23-ThenewfaceofFordism_0.png\" /\u003e\n\n미국 로봇 기업인 Figure은 특정 경제 분야에서 발생하는 노동 인력 부족을 해결하기 위해 인간 형상 로봇을 개발 중이며, 독일 자동차 제조사인 BMW과 협약을 발표했습니다. 이 협약에 따라 Figure 01 로봇들이 BMW의 미국에 위치한 Spartanburg 공장에 배치될 예정입니다. 해당 공장은 X 및 XM 시리즈 차량을 하루에 약 1,500대 조립하며 현재 약 11,000명을 고용하고 있습니다.\n\n아직 로봇 수나 그들이 수행할 작업에 대한 구체적인 내용은 알려지지 않았지만, \"그들은 몇 달 동안 특정 작업을 수행할 수 있도록 훈련받은 후, 생산 공정(보디 샵, 시트 금속 및 웨어하우스 등)에 통합될 것이며 앞으로 12개월에서 24개월 안에 배치될 예정입니다.\"라는 말을 제외하고 전반적인 작업에 대한 컨셉 증명입니다.\n\nBMW는 이미 공장에서 사용 중인 Optimus 로봇을 보유한 Tesla와 경쟁을 목표로 하고 있습니다. 이와 같이 인간 형상 로봇을 사용하여 반복적이고 위험한 작업을 수행하는 것에 대해 연구해온 Honda나 Boston Dynamics를 인수한 Hyundai 등의 자동차 제조사들은 기업들이 고용인력을 영입하고 유지하기 어려운 일부 여러 가치를 내지 않는 반복적인 작업이 바로 그들이 가장 어려워하는 작업이며, 회사들은 이에 대해 논의하고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금은 다음 세대 로봇의 개발이 진행되고 있습니다. 이러한 로봇들은 사람들과 함께 공간을 공유하는 환경에서 완전히 안전하며, 일 중에 일어나는 학습 과정이 모방을 통해 이뤄집니다. 실제로 Figure의 접근 방식은 로봇이 기업이 생산성을 높이고 비용을 절감하며 더 안전하고 일관된 작업 환경을 조성하는 데 기여할 것으로 보고 있으며, 회사를 RaaS(로보틱스 서비스) 모델로 설립하고 로봇을 기업에 대여할 계획입니다. Figure는 인간의 형태 요소를 모방하고 손으로 수행되는 작업에서 높은 정밀도를 달성하는 데 초점을 맞추고 있습니다.\n\n자동차 제조 공장에서 작업하는 로봇들. 2024년이 로보틱스의 해가 될 것으로 생각했을 때, 이렇게 빨리 이러한 종류의 보고서들을 듣게 될 줄은 저도 예상치 못했습니다.\n","ogImage":{"url":"/assets/img/2024-05-23-ThenewfaceofFordism_0.png"},"coverImage":"/assets/img/2024-05-23-ThenewfaceofFordism_0.png","tag":["Tech"],"readingTime":2},{"title":"로봇 공학의 미래를 위한 P2P 통신 로봇 에이전트 007 릴리스","description":"","date":"2024-05-23 17:10","slug":"2024-05-23-P2PCommunicationfortheFutureofRoboticsRobotAgent007Release","content":"\n\u003cimg src=\"/assets/img/2024-05-23-P2PCommunicationfortheFutureofRoboticsRobotAgent007Release_0.png\" /\u003e\n\nMerklebot에서는 로봇 플릿 운영자를 위한 데이터 관리 및 개발 도구를 만들고 있습니다. 우리는 다음 15년 안에 100억 대 이상의 로봇이 나올 가능성이 꽤 높다고 믿고 있으며, 이 비전을 실현하기 위해 견고하고 확장 가능하며 안전한 인프라 및 통신 스택을 구축하고 있습니다.\n\n## 로봇 에이전트란?\n\n몇 달 전에 Digital Black Box를 출시했습니다. 이 서비스는 로봇에서 로그, 카메라 피드 및 포인트 클라우드와 같은 데이터를 안전하고 저렴한 분산 저장 네트워크인 Filecoin에 백업하는 기능을 제공합니다. Digital Black Box는 다음으로 구성됩니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 가장자리에서 실행되는 로봇 에이전트\n\n2. 클라우드 연결을 제공하는 Merklebot 플랫폼\n\n첫 번째 버전의 로봇 에이전트는 가장자리 장치에서 데이터를 수집하고 백업하는데 사용되었습니다. 나중에는 Merklebot 플랫폼을 구축하여 Docker 컨테이너를 가장자리 장치에 배포하고 그 위에서 실행 중인 에이전트에 기능을 추가했습니다.\n\n## 로봇 에이전트 0.0.7\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 👀 네트워크에서 다른 로봇의 mDNS 자동 탐색\n- 🤖↔🤖 로봇 간의 Libp2p 메시징\n\n안녕하세요! 오늘은 로봇 에이전트 0.0.7의 새로운 릴리스를 Libp2p 통신 모듈과 함께 발표합니다. 최신 버전의 로봇 에이전트는 이제 Libp2p 피어 탐색, 주소 설정 및 메시지 전송을 갖추고 있습니다. 네트워크를 유연한 토폴로지로 설정할 수 있으며, 위치 변경이나 다른 매개변수를 바꿀 때마다 통신 스택을 구성할 필요가 없습니다. 이제 저희의 Github에서 공개되어 있습니다.\n\n로봇 에이전트 0.0.7를 설치하면 기기가 다른 에이전트를 자동으로 찾아 정보(상태, 이벤트)를 서로 전달할 수 있습니다. 중앙 서버에 접근하지 않고도 기기 간 통신을 수행할 수 있습니다. 이 도구는 MIT 라이선스로 완전히 오픈 소스이며, 여러분의 필요에 맞게 이 도구를 자유롭게 사용할 수 있습니다.\n\n## Merklebot 플랫폼\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMerklebot 플랫폼은 Robot Agents를 여러 기기 필릿에 중앙 집중식으로 롤아웃할 수 있도록 제공하여 로봇, 센서, 장비 및 기타 기계와 같은 다양한 기기를 안전하고 쉽게 관리할 수 있습니다. 전체 Robot Agents 필릿에 코드 및 Docker 컨테이너를 배포하고 데이터를 관리할 수 있습니다.\n\nMerklebot 없이:\n\n- 로봇에 연결\n- 로봇을 vpn 네트워크에 추가하거나 NAT 트래버셜을 우회하는 다른 방법\n- 🧑‍💻 코드를 작성합니다\n- ssh를 통해 로봇에 연결\n- ⏳ git pull\n- ⏳ docker build 및 run my-best-code\n- ⏳ 로봇 로그 보기\n- ⏳ 데이터(카메라 비디오 등)를 가져오기 위해 scp 실행\n- ⏳ 취합할 위치에 저장\n\nMerklebot을 사용하면:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 로봇에 연결하세요\n- install.sh를 실행하세요\n- 🧑‍💻코드를 작성하세요\n- 🚀플랫폼에서 한 번 클릭하거나 API를 호출하세요\n- 😎로그, 비디오 및 기타 데이터를 플랫폼에서 확인하세요\n\n## 다음은 무엇인가요\n\nMerklebot 팀은 오픈 소스 Agent의 기능을 계속 향상시키고 Merklebot 플랫폼에 더 많은 유틸리티를 추가할 새로운 기능 세트에 현재 작업 중입니다.\n\n- 에이전트를 위한 향상된 CLI 도구 (일반 명령 인터페이스). SSH 우회, 구성 업데이트 전달 및 기기에서 편집 없이 에이전트를 관리하세요. 현재 Merklebot 플랫폼에서 활성화되어 있지만, 이를 오픈 소스로 만들 계획을 하고 있어요!\n- 오픈 소스 데이터 관리 및 시각화 도구와의 통합. 로그 수집 및 저장, 데이터 작업 실행 및 시각화 생성을 할 수 있어요.\n- Jenkins와 같은 CI/CD (지속적 통합 지속적 배포) 도구와의 통합. 자동으로 장치에 새 소프트웨어 릴리스 설치하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리 새로운 에이전트를 확인해보세요! 피드백을 기다리고 있어요.\n\n그리고 기기 편대를 관리해야 한다면, Merklebot Platform으로 시작해보세요.\n","ogImage":{"url":"/assets/img/2024-05-23-P2PCommunicationfortheFutureofRoboticsRobotAgent007Release_0.png"},"coverImage":"/assets/img/2024-05-23-P2PCommunicationfortheFutureofRoboticsRobotAgent007Release_0.png","tag":["Tech"],"readingTime":4},{"title":"M1 M2 Mac 실리콘에 LinuxUbuntu 2004 설치하기","description":"","date":"2024-05-23 17:09","slug":"2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon","content":"\n\u003cimg src=\"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_0.png\" /\u003e\n\n안녕하세요, 로봇 공학 애호가 및 리눅스 사용자 여러분! 여러분께서는 강력한 M1 또는 M2 맥을 소유하셨지만 리눅스에서만 실행되는 프로젝트를 실행하고 싶으시죠? 다행히도 가상화 기술을 활용하여 M1/M2 맥에서 리눅스를 실행할 수 있습니다. 이 안내서에서는 UTM 가상 머신을 사용하여 M1/M2 맥에 리눅스를 설정하는 방법을 살펴보겠습니다.\n\nUTM이란 무엇인가요?\n\nUTM은 \"Universal Terminal Machine\"의 약자로, macOS 및 iOS용으로 설계된 오픈 소스 가상화 도구입니다. 이 도구를 사용하면 M1 및 M2 맥을 비롯한 Apple Silicon 기기에서 가상 머신을 실행할 수 있습니다. UTM을 사용하면 리눅스 배포본, Windows 등 다양한 운영 체제를 에뮬레이트할 수 있습니다. Mac 용 UTM 앱은 공식 UTM 다운로드 링크에서 다운로드할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nM1/M2 Mac에 Linux 설치하기:\n\nUTM을 이용하여 Linux를 설치하는 방법에 대한 단계별 안내서입니다. 모든 단계에 대한 사진을 포함하려고 해서 길어 보이지만 이 안내서를 따라 M1/M2 Mac에 Linux를 설치할 수 있습니다:\n\n- UTM 다운로드 및 설치:\n\n- 위의 링크를 통해 UTM 앱을 다운로드하거나 Google에서 'Mac용 UTM'을 검색하여 다운로드할 수 있습니다.\n- 다운로드 후 DMG 파일을 열고 UTM 애플리케이션을 드래그하여 설치하기 위해 Applications 폴더로 이동하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2. 리눅스 디스크 이미지 얻기:\n\n- 설치하려는 리눅스 배포판을 선택합니다. 인기있는 선택지로는 우분투, 페도라, 데비안 등이 있습니다. 이 튜토리얼에서는 우분투 20.04를 설치할 것입니다.\n- 우분투 20.04 데스크톱 버전이 ARM 64용으로 공식 웹사이트에서 제거되었기 때문에, 우분투 20.04 데스크톱을 설치하기 위해 우회 방법을 사용할 것입니다.\n- 여기서 우분투 20.04 (ARM 64 버전) 서버 디스크 이미지를 다운로드합니다. 이 이미지는 서버 이미지이므로 터미널 환경만 제공됩니다. 다음 단계에서 전체 데스크톱 버전을 설치할 것입니다.\n\n3. UTM에서 새 가상 머신 생성하기:\n\n- 응용 프로그램 폴더에서 UTM을 엽니다.\n- 새 가상 머신을 생성하려면 “+” 버튼을 클릭합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_1.png\" /\u003e\n\n- \"Virtualize\" 옵션을 선택하세요. \"Virtualize\"와 \"Emulate\"의 차이점은 Virtualize는 Apple 실리콘, 즉 ARM 64 칩셋을 네이티브로 지원하는 기계를 가상화한다는 것입니다. 그래서 빠릅니다. 반면에 Emulate는 필요한 CPU 아키텍처를 가상화하여 다른 CPU 아키텍처를 실행할 수 있습니다. (진실을 말하면, Emulate를 시도해 봤는데, 너무 느려요. 만약 단순한 명령줄만 실행되는 매우 가벼운 OS가 아니라면 절대 Emulate 옵션 선택하지 말기를 권장합니다.)\n\n\u003cimg src=\"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_2.png\" /\u003e\n\n- 당연히 \"Linux\"를 선택하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Install Linux Ubuntu 20.04 on M1/M2 Mac Silicon Step 3](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_3.png)\n\n- ISO 이미지에서 부팅을 선택하고 Step 2에서 다운로드한 서버 이미지를 찾아 선택하세요.\n\n![Install Linux Ubuntu 20.04 on M1/M2 Mac Silicon Step 4](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_4.png)\n\n- 이제 가상 머신에 필요한 자원을 할당하세요. 최상의 성능을 위해 시스템 자원의 절반을 가상 머신에 할당하는 것을 강력히 권장합니다. 가상 OS에서 작업하는 동안 다른 응용 프로그램을 실행하지 않고 자원을 절약하고 성능을 향상시키기 위해 노력해주세요. 그렇지 않으면 시스템이 다운될 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 5](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_5.png)\n\n![Image 6](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_6.png)\n\n![Image 7](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_7.png)\n\n![Image 8](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_8.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n4. 가상 머신 시작하기:\n\n- 구성이 완료되면 \"만들기\"를 클릭하여 가상 머신을 생성합니다.\n- 목록에서 새로 생성된 가상 머신을 선택하고 \"시작\"을 클릭하여 실행합니다.\n\n5. Linux 설치하기:\n\n- 가상 머신은 Linux ISO 이미지에서 부팅됩니다.\n- 화면 안내에 따라 가상 디스크에 Linux를 설치합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Screenshot 9](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_9.png)\n\n![Screenshot 10](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_10.png)\n\n![Screenshot 11](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_11.png)\n\nSelect the keyboard configuration and the network connection configuration.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Screenshot 1](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_12.png)\n\n만약 외부 세계 \"content😗\"에 액세스하기 위해 어떤 HTTP 프록시도 사용하고 싶지 않다면, 프록시 주소 필드를 비워 두십시오.\n\n다음 단계에서는 미러 주소를 기본 설정으로 유지하고 \"완료\"를 클릭하세요.\n다음 단계에서는 전체 디스크 옵션을 선택해 주세요.\n\n![Screenshot 2](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_13.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_14.png\" /\u003e\n\n파일 시스템은 주어진 그대로 선택하고 계속 진행하세요.\n\n이어진 단계에서 우분투 계정 프로필을 만들어 시스템에 로그인합니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_15.png\" /\u003e\n\n원한다면 다음 단계에서 오픈 SSH 서버를 사용할 수 있습니다!\n\n다음 창에서 프로젝트나 업무에 유용하거나 사용할 서버 스냅을 선택하세요. 원한다면 나중에도 설치 가능합니다!!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image 1](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_16.png)\n\n이제 설치가 시작됩니다 - →\n\n![Image 2](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_17.png)\n\n완료될 때까지 기다리세요. 완료되면 기계를 다시 부팅하라는 메시지가 표시됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기계를 다시 시작한 후 설치된 미디어, 즉 VM 설정에서 iso 파일을 제거하고 기계를 다시 시작하세요.\n\n\u003cimg src=\"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_18.png\" /\u003e\n\n이제 Ubuntu 서버에 로그인하고 좋아하는 데스크톱 환경을 설치해 봅시다.\n\n6. 데스크톱 환경 설치:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 우분투 데스크톱 시스템(gnome 3)을 설치하려면 다음을 실행하세요:\n  $ sudo apt install ubuntu-desktop\n  또는 Gnome 3 데스크톱을 위한 전환 패키지 이름을 사용해보세요:\n  $ sudo apt install ubuntu-gnome-desktop\n  Kubuntu Plasma 데스크톱/넷북 시스템(KDE)을 설치하려면 다음을 실행하세요 (가벼우면서 부드러운 터치를 선호합니다) :\n  $ sudo apt install kubuntu-desktop\n  Lubuntu 데스크톱 환경을 원하시나요? 다음을 실행하세요:\n  $ sudo apt install lubuntu-desktop\n  Xubuntu 데스크톱 시스템을 설치하려면 다음을 실행하세요:\n  $ sudo apt install xubuntu-desktop\n\n- 데스크톱 설치 과정이 완료되면 시스템을 한 번 더 재부팅하면 됩니다. 그 후, 와우! 새로운 깨끗하고 새로운 우분투 데스크톱에 로그인하게 될 것입니다.\n\n![이미지](/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_19.png)\n\n와! 이제 우분투 20.04의 완전한 데스크톱 버전을 성공적으로 설치했습니다. 이제 선호하는 우분투 패키지와 소프트웨어를 설치하고 작업에 원활히 참여할 준비가 모두 되었습니다.\n이 설명서가 맥 실리콘을 위한 우분투 20.04 설치에 도움이 되었길 바랍니다.\n\n만약 설치 중 궁금한 점이나 오류가 발생하면 shubhjain10102003@gmail.com으로 이메일을 보내주시면 감사하겠습니다! 😇.\n","ogImage":{"url":"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_0.png"},"coverImage":"/assets/img/2024-05-23-InstallLinuxUbuntu2004onM1M2MacSilicon_0.png","tag":["Tech"],"readingTime":9},{"title":"새로운 냉전 - 인공지능","description":"","date":"2024-05-23 17:04","slug":"2024-05-23-TheNewColdWarArtificialIntelligence","content":"\n## 최신 기술전쟁 전선에서의 한 모습\n\n![이미지](/assets/img/2024-05-23-TheNewColdWarArtificialIntelligence_0.png)\n\n# 목차\n\n- 최신 기술전쟁 전선에서의 한 모습\n- 목차\n- 2030년 중국산\n- 기술\n- 생성적 인공지능\n- 중국: 장기 계획가?\n- 중국의 함정\n- 참가 상장\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n파티 참석자는 몇 년 전에 중국에 있는 유럽-미국 상공회의소(European-American Chamber of Commerce in China)로부터 초대를 받았습니다. 2박 호텔 숙박 및 식사, 대부분의 여행 경비 보상, 그리고 중국 닝보의 스마트 시티 엑스포(Expo) 부스 무료 이용이 제공되었습니다. 중국에서 사업을 하고 싶은 마음이 생겨, \"왜 안해볼까\" 생각했습니다. 중국 비즈니스의 잠재력을 탐구하는 뜻있는 노력으로 여겼기에, 계약서에 서명했습니다. 몇 주 후, 중국 동부 체장省 닝보의 중요 항구 및 산업 중심지에 자리잡을 기회를 얻게 되었습니다.\n\n최근 소설 쓰기를 위해 제2차 세계대전 시일 중국의 일본과의 전투에 대해 조사를 한 결과, 이 도시에 대해 조금 알고 있었습니다. 안타깝게도, 닝보는 제2차 세계대전 중 일본 제국군이 가장 최악의 잔혹행위를 저질렀던 장소 중 하나였습니다. 일본 전투기가 생화학 전투부대인 유닛 731에 의해 지휘된 잔혹한 생화학 전쟁 행위로 역병, 장티푸스 및 기타 질병이 퍼진 곳이었습니다.\n\n그러나 니보 리셔 국제공항 터미널의 카펫을 건너자마자 전쟁의 생각은 멀리 떠났습니다. 거기서는 자유의 여신상과 에펠탑, 런던의 빅 벤을 담은 보험사 포스터가 나를 맞았습니다. 서양의 요소를 볼 수 있어 기분이 좋았지만, 이 상징이 보는 사람 중 대부분에게 피곤한 의미로 남아있을 것이라고 생각했습니다. 미국에서는 자유, 민주주의, 희망, 기회를 상징하는 것은 자유의 여신상뿐입니다. 그러나 중국에서는 아마 자유와 같이 무겁고 깊은 의미가 담긴 상징은 아니라 생로운 상징이라고 생각했습니다.\n\n## 2030년 중국산 \"Made in China\"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n중국 공산당(CCP)은 14차 다섯 해계획에서 2030년까지 인공지능 우위를 차지하는 것을 목표로 세웠습니다. 하지만 그게 현실적인 목표일까요? 의심의 여지 없이, 중국은 인상적인 기술사를 가지고 있습니다. 로스 앤더슨은 자신의 ‘파노프티콘(Panopticon)은 이미 여기에 있다’라는 기사에서 중국이 글쓰기가 독자적으로 발명된 세곳 중 하나라고 언급합니다. 중국은 또한 데이터가 신속하게 이동하도록 할 수 있는 종이를 발명했으며, 이는 제국을 확고히 통치하기 위해 필요한 의사소통 수단이었습니다. 중국 스님들은 9세기경, 불멸의 엘릭시를 찾던 중 폭약을 발견했습니다. 따라서 혁신은 중국에게 전혀 낯선 개념이 아니었습니다.\n\n중국의 기술사로부터의 진행은 오늘날 기술에 이어집니다. 현재 세계의 이메일 통신(우리의 현대적인 글쓰기 시스템) 대부분을 거래하는 컴퓨터는 중국에서 생산되었습니다. 시작은 서구에서 이루어졌지만, 중국이 현재 대부분의 컴퓨터를 대량생산하고 있습니다. 중국은 단연코 \"세계의 공장\"이라 불리는 것이 타당한 이유가 있습니다.\n\n## 기술\n\n로봇공학\n로버트 D. 애킨슨은 그의 '중국 제조업체는 미국 제조업체보다 임금을 고려할 때 12배 많은 로봇을 사용합니다.'라는 기사에서 \"2021년, 중국은 미국보다 제조업 종사자 당 로봇을 18% 더 설치했습니다. 중국 제조업 임금이 미국 임금보다 상당히 낮았다는 점을 감안할 때, 중국은 미국보다 제조업에서 12배 더 많은 로봇을 사용했습니다.\"라고 보고합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\"그러나 시장 요인이 아닌 Atkinson의 말에 따르면, 이는 정부 정책 때문입니다. 제조업 로봇 도입은 중국 공산당의 최우선 과제이며 관대한 보조금으로 후원하고 있다고 덧붙였습니다. 이는 로봇 도입을 지원하거나 자금을 지원하지 않는 미국 정부의 정책과는 대조적입니다.\n\n로보틱스 기술은 닝보에서 잘 대표되었습니다. ZTE는 참석자들 가운데 어린이들의 상상력을 자극한 5G 로봇 능력을 선보였습니다. \"\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n로봇 개를 만드는 로보틱스 회사(그림 4 참조)가 최신 기술을 선보였어요. 사실을 말하자면, 보스턴 사이언티픽에서 몇 년 전에 본 것과 의심스럽게 비슷했어요.\n\n![로봇 개](https://miro.medium.com/v2/resize:fit:1400/1*pxBUVkAdzZqrYOZYsxAdhQ.gif)\n\n중국의 로봇산업이 인상적으로 보이긴 하지만, 산업용 로봇의 최대 채용국은 아닙니다. Atkinson에 따르면 \"한국이 세계에서 가장 많은 산업용 로봇을 채용한 나라로, 제조업 종사자 10,000명 당 1,000대의 로봇을 보유하고 있었고, 싱가포르가 그 다음으로, 670대를 보유한 나라로 뒤를 잇고 있으며, 일본과 독일이 거의 400대를 보유하고 있어요. 미국은 종사자 10,000명 당 274대의 로봇을 보유하고 있는 반면, 중국은 322대를 가지고 있습니다.\"\n\n내가 본 것은 인상적이었지만, 중국은 로봇 분야에서 한국과 싱가포르에 따라잡기에는 아직 멀은 길을 가고 있어요. 독일은 고품질 복잡한 로봇 제공업체로도 선두입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nGeely는 전시장 외부 주차장에 서있는 자사 모델 중 하나를 운전할 수 있는 차 시뮬레이터를 제공했습니다. 차 안의 운전자는 발이 묶이는 듯하면 브레이크를 밟을 준비를 하고 있었어요 (그림 5 및 6).\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*5DnflsdScg7TvhsaHzpcvA.gif)\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*vN0WUdnmia1Ujan-SO_LWw.gif)\n\n스마트 시티\n스마트 시티 기술은 정보 및 통신 기술을 활용하여 운영 효율성을 개선하고 공공과 정보를 공유하며 주민들에게 더 나은 삶의 질을 제공하는 것을 의미합니다. 다양한 디지털 솔루션을 통합하여 더 나은 결정을 내리고 도시 생활을 향상시키는 것을 포함하고 있습니다. 주요 구성 요소는 다음과 같습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 데이터를 수집하고 분석하는 IoT 장치 통합, 이를 통해 인프라, 공공 시설 및/또는 서비스 개선을 위해 활용합니다.\n- 더 최적화된 에너지 관리를 가능하게 하는 스마트 유틸리티 미터.\n- 인텔리전트한 도시 이동 및 교통망, 스마트 트래픽 라이트, 스마트 주차 등을 포함한 스마트 이동 및 교통, 모든 것은 교통 시스템의 효율성과 지속 가능성을 향상시키기 위한 것입니다.\n- AI 기반 교통 관리 시스템: IoT 장치를 AI 기반 교통 관리 시스템과 통합하면 혼잡을 크게 줄이고 대중교통 효율을 최적화하며 온실 가스 배출량을 줄일 수 있습니다.\n\n중국 정부는 알리바바에게 나라의 스마트 시티 기술을 구축하도록 지시했습니다. 이를 회사는 \"시티 브레인\"이라고 부릅니다. 로스 앤더슨에 의하면, 시티 브레인은 \"도시 환경 전체에 분산된 다양한 센서들로부터의 데이터 스트림을 종합하는 자동화된 신경 센터\"입니다.\n\n앤더슨은 조금 디스토피아적이라고 설명합니다. \"시티 브레인은 잃어버린 어린이나 관광객 또는 테러리스트가 방치한 수화물을 발견하도록 훈련될 수 있습니다. 방황하는 사람들이나 거리에 머묾거나 무정부상태로 있거나 폭력 분자를 신고할 수 있습니다.\" 위험에 처한 사람들은 \"항상 경비 미쳐 탐지될 수 있는 독특한 방식으로 손을 흔들어 도움을 요청할 수 있을 것\"이라고 앤더슨이 덧붙입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-23-TheNewColdWarArtificialIntelligence_3.png)\n\n공정하게 말하자면, 디스토피아는 중국 해안에서 끝나는 것이 아닙니다. 중국의 타이위안은 인구 1,000명당 117개의 카메라로 세계에서 가장 감시를 많이 받는 도시 중 하나일 수 있지만, 런던도 멀지 않습니다. 인구 1,000명당 73개의 카메라를 가지고 있어 세계 순위에서 세 번째에 위치하고 있습니다.\n\n소위 헌법 제4조 권리도 일반 미국인을 충분히 보호해주지 못할 수도 있습니다. 안데르센은 \"미국의 경찰 부서들이 애매한 점에 있다면 아마존의 홈 보안 카메라 영상을 이용하기 시작했다\"고 언급합니다. 따라서 많은 미국인이 당연하게 가지고 있는 비합리적 검색받지 않음의 권리가 허약해지고 있을 수도 있습니다.\n\n중국은 자국 국민에게 이 모니터링 기술을 사용함과 동시에 세계 최대의 AI 기반 감시 장비 공급 업체가 되었다고 안데르센은 말합니다. 말레이시아에서는 알리바바의 시티 브레인 플랫폼과 같이 쿠알라룸푸르 경찰서에 얼굴 인식 기술을 도입하고 있습니다. 싱가포르의 11만 개의 가로등은 안데르센이 보고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 생성적 AI\n\n오픈AI가 2021년에 ChatGPT를 출시하자, \"생성적 AI\"가 즉시 주목을 받았습니다. 대중들에게 상상력뿐만 아니라 심장과 마음까지 사로잡아, 다운로드 현상이 됐죠.\n\nDall-E, Stable Diffusion, Midjourney와 같은 텍스트 대 이미지 솔루션이 ChatGPT를 빠르게 따랐습니다. 이 도구들은 사용자에게 간단한 텍스트 프롬프트로부터 즉시 사실적인 이미지를 빠르게 생성할 수 있는 놀라운 기술임을 입증했습니다.\n\nCNBC의 Arjun Kharpa에 따르면, ChatGPT 출시 후 몇 달 뒤에는 중국 기술 거물인 알리바바, 바이두, JD.com, 넷이스가 유사한 제품을 출시할 의향을 밝혔다고 합니다. 하지만 그들이 진짜로 할 수 있을까요? \"중국 기술 기업들은 새로운 규제 상황에 적응해야 하며, 그들의 ChatGPT 응답에 대한 발표가 조심스러웠던 것은 이 현실을 반영한 것입니다,\" Kharpal은 보도했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 기업들은 포괄적이고 창의적인 AI 플랫폼을 개발하고 있지 않습니다. 이는 베이징의 인터넷 검열 기관들에 문제를 일으킵니다. 그 대신, 알리바바와 넷이즈는 해당 기술을 응용 프로그램 특정 용어로 이야기하고 있다고 Kharpal이 말합니다. 이러한 기업들이 창의적인 AI 분야를 지배할 수는 없습니다. 하지만, 그들이 가진 선택사항은 무엇인가요? 그들은 모든 종류의 정보에 자유롭고 공정한 접근이 필요한 해당 기술에 대한 수준에서 경쟁하기가 어렵습니다. ChatGPT 및 Perplexity와 같은 미국 기업들에 한 점!\n\n\"작년 동안 다양한 정부 기관들에 의해 기술 플랫폼과 AI 알고리즘에 대한 규제에 대한 관심이 집중되었는데, 주요 기술 플랫폼들은 논쟁에 휩쓸리는 채팅 봇/창작 AI 도구를 내놓음으로써 주목을 받고 싶어하지 않습니다,\" 컨설팅 회사 Albright Stonebridge의 기술 정책 담당자인 폴 트리올로는 CNBC에 말했습니다. 지난 몇 년 동안 이러한 기업들은 규제의 엄중한 시험을 받았습니다. 그러나 이는 또한 중국이 세계를 강탄할 혁명적 제품을 만드는 것이 얼마나 어려울지를 보여주는 사례입니다.\n\n# 중국: 장기 계획가?\n\n서양에서 널리 퍼진 중국 비즈니스에 대한 맥시엄이 있습니다. 그것은 중국이 장기적으로 플레이하고 있는 반면 서양 기업들은 목요일 이후를 생각하지 못한다는 것입니다. 이는 매우 잘못된 견해입니다. 세계에서 상위 10곳의 기업 중 9곳은 미국 기업입니다(Saudi Arabia의 Aramco는 그 목록에서 유일한 미국 외 기업입니다). 이들 기업이 그 목록에 올라간 이유는 단기적인 사고를 한 것이 아니라, 그들의 예측 능력이 최고라는 것입니다. 오늘날의 많은 기업의 성공에 큰 역할을 하는 소프트웨어 분석 사업은 미국, 유럽, 일본 전역에 흩어진 칩 제조업체, 소프트웨어 개발자 및 대학 연구소에서 시작되었지만 대부분 미국에서 시작되었습니다. 인터넷은 미국에서 발명되었으며, 실리콘 칩도 그랬습니다. 미국은 AI 및 양자 컴퓨팅 분야에서 선두를 달리고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n미국과 비교했을 때 중국은 독립적인 사법부, 강력한 기업 지적재산권 보호, 정치적 비즈니스 접근 방식, 그리고 다가오는 분기 보고서만 고려하는 혁신적인 마인드에서 부족한 면이 있습니다. 이러한 것들은 장기적인 비즈니스 성공을 위해 필요한 중요한 기반 요소입니다.\n\n중국에서는 정부 규제가 갑자기 나타날 수 있습니다. 기업들은 이제 사업과 관련된 경험이 거의 없는 정부 관료를 이사로 임명받아야 하는 상황이 발생하고 있습니다. 중국은 강력한 창업 생태계가 부족합니다. 기업들은 서양에서처럼 잠재적인 사업 기회를 탐색할 여지를 거의 제공받지 못합니다.\n\n과거에는 실리콘밸리의 벤처 자본가들이 많은 중국 스타트업에 자금을 지원해왔지만, 현재 그 자금은 말라가고 있습니다. 그 자금은 현재 미국, 유럽, 심지어 일부 아시아 기업들로 재투자되고 있습니다. 중국은 대만만만 베이 에어리어(심천, 홍콩, 마카오로 구성)를 구축하고 있을 수 있지만, 실리콘밸리의 벤처 자본가들을 대체할 벤처 공산주의자는 나타나지 않을 것입니다.\n\n지적 재산권을 훔치는 것은 빠른 단기적인 재정 이득을 제공할 수 있지만, 실질적인 연구 및 개발에 투자하는 것이 훨씬 더 장기적인 전략입니다. Cyfirma의 중국 지적 재산권 도난 보고서에 따르면 \"지적 재산권 도난은 중국이 경쟁 국가들의 기업들을 대체하기 위해 사용하는 더 큰 산업 전략의 일부입니다. 미국 기업들만 하더라도 지적 재산권 도난으로 매년 4분의 1에서 5분의 1조 달러의 손실이 발생합니다.\" 보고서는 또한 \"중국 기업들이 자주 지적 재산권 보호를 무시하며, 국가 전반적으로 외국 기업, 주로 서양(그리고 점점 더 아시아) 기업들로부터 지적 재산권을 빼앗기 위한 정책들을 오랜 기간 도입해왔다\"고 덧붙였습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n조지 매그너스는 그의 기사인 중국 경제: 카산드라 vs. 폴리안나에서 중국 경제의 문제로 \"지방 정부, 국영 기업 및 부동산의 과도한 부채, 소비 부족, 과도한 투자 및 자본 분배 부정, 인구 통계의 고속 노화의 결과, 생산성 성장의 약세, 더 많은 통제와 억압적인 지배, 민영 기업 및 기업가들의 지배, 그리고 최근에는 상업 및 비즈니스 분리, 이제 위험 감소로 재브랜딩된 것들을 이 논의합니다.\" 이것들은 사소한 문제가 아닙니다.\n\n중국의 경제적 상황이 너무 심각해져서, 이제 경제가 파괴된 것이 아니냐는 질문이 아닌 누가 이 일을 일으켰는지에 관한 문제입니다. 외교문제지에서 피터슨 연구소의 아담 포센은 중국의 최근 \"고 코로나 정책과 국가 통제로 인해 민간 기업과 기업가들의 신뢰가 저해되어 중국 경제의 활력을 저해시켰다\"고 주장합니다.\n\n그러나 외교위원회의 존유안 리우와 평가대학의 마이클 페티스는 상황을 다르게 보고 있습니다. 그들은 중국의 경제적 문제는 코로나 이전부터 시작되었으며, 중국은 10년 이상 시스템적으로 결함이 있는 경제 발전 모델을 가지고 있었다고 말합니다.\n\n모두 동의하는 점은 중국의 문제들이 쌓여 있다는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n한 자녀 정책은 역사적으로 중요한 인구 틱택 타임봄으로 터져나가고 있습니다. 세계사에서 어떤 나라보다도 중국에서 노인 인구가 더 많아지고 있습니다. 사회 안전망이 부족한 나라에서 이는 매우 우려스럽습니다.\n\n\"누워서 먹고 살기\"와 \"그냥 말리기\" 운동, 젊은이들이 여기저기로 야심을 내어 밀어붙이는 것 같은 동작은 최근 대학 졸업생들이 자녀를 갖는 욕망을 줄이고 있습니다. 그래서 다음 세대에 도움이 되지 않습니다.\n\n2021년, 정부는 급성장하던 온라인 과외 산업을 거의 순식간에 없애버렸습니다. 우연의 법칙의 또 다른 사례로, 과도한 등록금 때문에 중국인들이 자녀를 낳기를 꺼려한다는 우려로, 정부의 권위적 조치로 과외 시장이 붕괴될 수도 있었습니다. 이는 물론 과외를 더 비싸지게 만들었습니다.\n\n2023년 12월 22일, 정부가 게임 산업에 새로운 규정을 도입하여 플레이어가 인게임 구매에 얼마나 돈을 쓸 수 있는지 제한했습니다. 이 조치는 게임 개발사의 수익을 강타하여 800억 달러의 주식시장 폭락을 초래했습니다. 우연의 법칙이 다시 한 번 끔찍한 모습을 드러냈습니다. 하루 후에 중국 국가출판방송총국은 손을 뒤로 젔지만 이미 피해는 입혔습니다. 시장이 가장 혐오하는 것은 침범적인 정부보다는 우유부단한 정부일 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n중국은 외국 투자가 절실한 상황이지만, 투자자들을 위협하는 불투명한 법률을 제정하고 있습니다. 중국이 외국 자본을 절실하게 원하는 이유가 있습니다. 중국인들은 자국 주식 시장에 투자하지 않습니다. 중국 기업의 재무제표가 적어도 의심스러우며, 사실상 부정확하다는 것을 알고 있습니다. 투자자와 비즈니스 커뮤니티 간에 신뢰가 없습니다. 코로나 이전 중국 전역에서 빠르게 성장하던 커피 회사 럭킨 커피는 추후 머디 워터스 단기매도자들 보고서에 걸려, 나는 닝보에서 돌아온 몇 달 후, 약 $310 백만 달러를 조작했다고 알려진 소송에 1억 7500만 달러로 청산했습니다.\n\n“사막에 공산당원을 일시킨다면 모래도 바닥난다,”는 농담은 중국이 직면하는 모든 해결할 수 있는 문제들을 생각할 때 참된 말임을 보여줍니다. 왜 14억 명의 정부가 개발자들이 30억 명분의 주택을 건설하도록 허용하겠습니까? 왜 중국은 송장 도시를 채우는 수백만 개의 빈 집을 가지고 있을 뿐만 아니라 세계에서 유일한 고스트 크레이퍼인 골든 파이낸스 117을 갖고 있습니까?\n\n## 중국 함정\n\n야망과 열망은 쉽게 표현할 수 있지만, 건강한 통치와 견고하고 유연한 기관에 의존한 상황에서 실현하기는 엄청나게 어렵다고 말합니다. 다시 말해, 말은 쉽지만 실행은 굉장히 어렵다는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n혹은 안 좋아하더라도, 현대 중국은 언제나 80년대와 90년대 일본과 비교될 것입니다. 많은 사람들이 알고 있는 기업 이름을 가졌다고 해서 반드시 지속적인 성공으로 이어지는 것은 아니라고 말하는 Magnus의 주장입니다. 알리바바, 텐센트, 그리고 바이두는 단지 소니, 토요타, 히타치, 미쓰비시의 중국 버전일 뿐입니다. 확실히 좋은 기업들이지만, 애플, 구글, 페이스북, 또는 마이크로소프트가 너무 걱정해야 할 상대는 아닙니다.\n\n몇십 년 전, 미국 리더들은 일본 conglomerates이 그들을 이기는 비경쟁력으로 두려워했습니다. 그러나 Magnus가 가리키듯이, \"세계적인 기술 기업을 갖는다고 해서 일본 경제의 주춧돌인 시스템적 거시경제 문제에 대한 보호가 보장되는 것은 아니었습니다. 바로 그 이후에 일본 경제가 발목을 잡힌 것입니다.\" 중국 기업들도 오늘날의 고마진 기술을 제조하기 위해 필요한 정교한 칩이 제공되지 않는 한 비슷한 위기에 직면할 수 있습니다.\n\nMagnus는 \"현대 중국이 1980년대와 1990년대의 일본의 틀에 완벽하게 들어맞지는 않지만, 중국의 인구 1인당 소득은 미국의 20% 미만입니다; 1990년대 초의 일본은 1.5배 더 크었습니다. 둘 다 \"높은 부채, 과대평가된 부동산, 자본 분배의 잘못, 고령화, 그리고 개혁을 위한 기관적 또는 정치적 장벽\" 등의 경제 모델은 매우 유사해 보입니다,\"라고 말합니다. Magnu는 경고합니다. 중국의 부동산 거품은 일본 시기만큼 강하게 충실히 늘어선 것이 아니라, 기업 부채가 작고 정부가 경제를 조절하는 데 사용할 도구가 더 많지만, 중국이 디플레이션과의 싸움을 벌이는 동안 일본화의 위험이 높아진다는 Magnus의 경고입니다.\n\n\"중국의 GDP가 미국을 넘어서기 위한 창문이 거의 닫혔습니다. 그 격차는 실제로 넓어질 수 있으며, 2023년 처음으로 30년만에 그렇게 된 것처럼,\"며 Magnus가 말합니다. 중국 경제에 대한 이러한 비관적 평가는 현재의 중국 감시자들 사이에서 흔한 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n“투자자들이 대부분의 중국 주식 시장을 포기했고, 외국 기업들은 투자와 생산을 아시아를 넘어 다른 국가로 다변화하는 추세에 있다”고 Magnus가 지적했다. 중국 경제는 급격히 사라지기엔 너무 크지만, 평평해지고 있는 것으로 보인다.\n\nMagnus는 “한 나라의 안보는 중장기 경제 전망만큼 안전하다. 이들은 몇 십 년 동안 한 것보다 취약한 상태이다. 발전 모델을 철저히 재시동하지 않는 한, 중국이 두려워하는 중간소득 함정이 점점 가까워지고 있다”고 경고했다.\n\n오늘날 중국과 미국은 경제적으로 정반대편에 있는 것으로 보인다. 미국 시장은 거의 매일 새로운 최고점을 경신했으며(일본도 마찬가지), 한편으로 중국 주식 시장은 10년 최저점으로 떨어졌다. 홍콩 항셍지수는 영국이 식민지로 돌려준 때보다 낮은 수준으로 하락했다. 이것이 현재 홍콩과 중국의 경제 상황에 대해 많은 양을 이야기한다면, 아무 것도 듣기 않은 것이다. 그러나 주식 시장은 늘 선두적인 경제 지표로 여겨졌다. 두 경제에 대해 지금 어떤 것을 보고 있는지 궁금해진다.\n\n## 참여상”\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지금까지 나의 스마트 시티 탐험상(참가자 트로피로도 불리는 (그림 10 참조))은 몇 년 전에 리즈 여행을 상기시키는 나의 아파트 선반 위에 있습니다. 서서히 서부인 참가자들이 이벤트에 참석하기를 갈망하다 보니 비용을 부담해주고 재미있는 상품도 공식식으로 수여되는 것을 생각해볼 때 정말로 값진 경험이었습니다!\n\n중국은 2019년 9월 코로나 바이러스가 확산되기 2개월 전, 그 후에는 전 세계적으로 유행이 급격히 확산되기 시작함으로써 극적으로 변화했습니다. 당시에 만난 몇 명의 연락처는 소식을 들을 수 없게 되었습니다. 그래도 Ningbo 스마트 시티 여행은 일자리 발견으로 이어지지 않았지만 눈을 떴게 해주는 경험이었습니다.\n\n지금의 복잡한 기술 세계에서 중국이 경쟁할 수 있을까요? 물론 가능합니다. 중국은 몇 가지를 올바르게 진행하고 있습니까? 물론 그렇습니다. 그러나 중국의 기업 문화에는 미래의 AI 전쟁에서 승리하는 데 도움이 되지 않는 어떤 점이 제한적으로 존재한다고 생각합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n프랑스 황제가 되기 전 나폴레옹은 중국 지도를 가리키며 말했습니다. \"여기 잠자고 있는 사자가 있다. 그를 자도하라. 그가 깨어날 때 세계를 흔들 것이다.\" 나폴레옹은 두 세기 전에 이 말을 했고, 그 이후로 중국은 많은 나라들보다 격동한 시기를 보냈습니다. 중국 19세기는 식민 지배, 아편전쟁, 그리고 백년굴이 가득 찼습니다. 20세기는 일본 제국주의 지배, 잔인한 내전, 그리고 모든 것을 흔들어놓은 공산주의 전환으로 이어졌는데, 이는 기근과 수백만 명의 사망을 야기했습니다.\n\n오늘날 중국은 미국과 경제적 우월을 다툴만큼 준비되어 있지 않습니다. 그녀는 풍부하고 민첩한 나라조차 극복하기 어려울만한 많은 도전에 직면하고 있습니다.\n\n피낭당을 빠져나가는 비행기에서 난 닝보의 하늘을 바라보면서, 청 일본 폭격기가 닝보에 온천기로 오염된 벼루병 살충제를 투하하려던 그 시절을 상상했습니다. 일본은 바이오 및 화학 무기를 금지하는 제네바 협약에 조인하지 않았기 때문에, 이별하여 민간인 수천 명을 살해하는 것을 제한하는 요소가 없었습니다. 실제로 기도 경계와 생화학 무기를 개발하고 중국 민간인을 살해했다. 명백히 도덕은 선량한 이들을 살해하는 데에 강력한 금지 요인이 되지 않았습니다.\n\n안더슨에게 중국의 AI 우월에 대한 상승은 위협적인 가능성입니다: 중국의 정치 구조는 기술의 최악의 잠재력을 자극시키기보다 제한하지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n중국은 이제 더 이상 다른 사람들의 기술을 그대로 복사하고 표절하고 위조하는 나라가 아닙니다. 그것은 자체적으로 혁신의 세력이 되었지만, 중국 지도자로서 미국을 최초의 방문지로 선택한 이유에 대해 질문 받았을 때 덩샤오핑의 말을 기억했으면 좋겠습니다. \"미국의 동맹국들은 모두 부유하고 강하며, 중국이 부유하고 강하고 싶다면 미국이 필요했다\"고 그가 대답했습니다. 이것은 우리 모두가 많은 것을 배울 수 있는 교훈입니다. 인공지능이 우리가 많은 사람들이 우려하는 대로 잘못되면, 인류는 모든 친구를 필요로할 것입니다.\n\n독자 여러분, 읽어주셔서 감사합니다. 그리고 이 쇼에서 가장 어린 참가자 중 한 명의 작별 조언 (네, 보통이란 지겨움):\n\n![The New Cold War: Artificial Intelligence](/assets/img/2024-05-23-TheNewColdWarArtificialIntelligence_5.png)\n\n그레이 에어리어는 훌륭한 사이버 보안 및 컴퓨터 과학 게시물들의 모음입니다. 그레이 에어리어의 작가가 되고 싶다면 이 양식을 작성해보세요! 그레이 에어리어가 기사를 발행할 때마다 업데이트를 받으시려면, 저희 트위터 페이지 @TGAonMedium을 확인해주세요.\n","ogImage":{"url":"/assets/img/2024-05-23-TheNewColdWarArtificialIntelligence_0.png"},"coverImage":"/assets/img/2024-05-23-TheNewColdWarArtificialIntelligence_0.png","tag":["Tech"],"readingTime":17}],"page":"54","totalPageCount":110,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"54"},"buildId":"YUMR4jSyk_WlOHHc7UfOk","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>