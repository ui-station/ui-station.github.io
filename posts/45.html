<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/45" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/45" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/960f1fe994a0ab5c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/960f1fe994a0ab5c.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/RZIEBQ2aNAp_DXFVTV6eL/_buildManifest.js" defer=""></script><script src="/_next/static/RZIEBQ2aNAp_DXFVTV6eL/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="인간의 시선 AI가 창의력에 미치는 영향 탐구" href="/post/2024-05-18-TheHumanLensExploringAIsImpactonCreativity"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간의 시선 AI가 창의력에 미치는 영향 탐구" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간의 시선 AI가 창의력에 미치는 영향 탐구" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">인간의 시선 AI가 창의력에 미치는 영향 탐구</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법" href="/post/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">3<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드" href="/post/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기" href="/post/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">15<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기" href="/post/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기" href="/post/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">29<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="프롬프트 엔지니어링 기술 분류 및 프롬프트 튜닝" href="/post/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="프롬프트 엔지니어링 기술 분류 및 프롬프트 튜닝" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="프롬프트 엔지니어링 기술 분류 및 프롬프트 튜닝" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">프롬프트 엔지니어링 기술 분류 및 프롬프트 튜닝</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라벨을 놓치지 마세요 계층적 범주에 대한 대체 인코딩 방법" href="/post/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라벨을 놓치지 마세요 계층적 범주에 대한 대체 인코딩 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라벨을 놓치지 마세요 계층적 범주에 대한 대체 인코딩 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">라벨을 놓치지 마세요 계층적 범주에 대한 대체 인코딩 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="희소 라마 70 더 작고, 3배 빠르며, 완벽한 정확도" href="/post/2024-05-18-SparseLlama70Smaller3xFasterandFullAccuracy"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="희소 라마 70 더 작고, 3배 빠르며, 완벽한 정확도" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-SparseLlama70Smaller3xFasterandFullAccuracy_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="희소 라마 70 더 작고, 3배 빠르며, 완벽한 정확도" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">희소 라마 70 더 작고, 3배 빠르며, 완벽한 정확도</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="로지스틱 회귀의 시각적 이해" href="/post/2024-05-18-AVisualUnderstandingofLogisticRegression"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="로지스틱 회귀의 시각적 이해" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="로지스틱 회귀의 시각적 이해" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">로지스틱 회귀의 시각적 이해</strong><div class="PostList_meta__VCFLX"><span class="date">May 18, 2024</span><span class="PostList_reading_time__6CBMQ">20<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link posts_-active__YVJEi" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"인간의 시선 AI가 창의력에 미치는 영향 탐구","description":"","date":"2024-05-18 20:44","slug":"2024-05-18-TheHumanLensExploringAIsImpactonCreativity","content":"\n\n\u003cimg src=\"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png\" /\u003e\n\n“블랭크 슬레이트”에서 스티븐 핑커는 현대 미술을 비판하며 생성적 AI 예술의 폭발과 공감합니다. 현대 미술과 AI 창작물은 우리가 본성적으로 아름다운 것에서 멀어지는 것으로 보입니다. 핑커는 이러한 정체성에 대해 이렇게 얘기합니다: “우리는 아름다움, 감정, 그리고 기술에서 멀어지고 있다.” 어떤 작품들은 특정 대상을 위해 설계되어 완전히 이해하려면 광범위한 맥락이 필요합니다.\n\n알고리즘의 결과가 '예술'로 불릴 때, 이는 예술가에게 모욕이 됩니까? AI는 확실히 기존의 정의에 도전을 제기합니다. 그래도, 이러한 정의를 정제하려는 노력은 새로운 것은 아닙니다. 작가들은 작품을 더 나아가도록 자극하는 이 존재에 직면하며, 일부는 익숙한 것을 버리려는 저항을 합니다. 이러한 변화는 우리에게 뻔한 서사와 기술적 공식을 넘어서도록 이끕니다. 이는 이미 확립된 구조에 익숙한 사람들에게는 불편한 일입니다.\n\n# 모두에게 붓을 전달\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성적 AI는 예술 창작을 민주화하여, 현대 예술에서 핑커가 비판한 엘리트주의를 흔드는 역할을 합니다. 이제 전통적인 재능의 부재로 인해 창의적인 영혼이 억압받을 필요가 없습니다. 스킬이 항상 신체 민첩성에 결합되지 않는다는 점에 주목한 피커는 이를 예상한 것입니다. 이것이 바로 AI 예술이 구현하는 것입니다. \n\n# 새로운 예술적 도전\n\n현대 예술과 AI 예술은 소수만이 다스릴 수 있는 언어처럼 느껴질 수 있습니다. 사상을 유발하고, 더 깊은 수준의 대화를 유발할 수 있는 작품에 대해 끌리는 것이 있습니다. 그러나 기계가 이러한 대화를 예술을 통해 제공할 수 있을까요? AI 생성은 이를 요구하며, 동시에 핑커가 일부 예술적 영역에서 지적한 속세적인 태도를 강조합니다.\n\n# 디지털 시대의 예술 재정의\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n핑커는 특정 예술 형식의 사라짐을 애도하지만, AI는 예술 표현의 완전히 새로운 시대를 열어줍니다. 이 발전은 우리에게 예술을 기술과 아방가르드 아이디어를 넘어서 바라보게 만듭니다. 창의적 영감의 원천이 중요할까요? 기계가 '아티스트'가 될 수 있을까요, 아니면 인간의 지시의 연장으로 존재할까요? 핑커는 마음을 진화의 산물로 보고, AI 아트는 자연적이고 인공적인 창의성을 결합하여 예술이 무엇을 '예술'로 만드는지에 대한 신념적 질문을 던집니다.\n\n# 브러시가 스스로 들고 있는 경우\n\n알고리즘에 의해 휘둘리는 '브러시'로 인해 '인간의 손길'이 어떤 역할을 하는 걸까요? 모든 예술가는 자신의 선택한 매체에서 이 질문에 답하려 노력합니다. AI와 상호작용하는 모든 사람이 '아티스트'가 되는 것은 사실이 아니며, 같은 이치는 캔버스, 악기 또는 기타 전통 도구를 사용하는 사람들에게도 동일합니다.\n\n# 보이지 않는 것을 느낄 수 있을 때\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n핑커는 현대 예술이 감정적 연결이 부족할 수 있다고 주장하지만, AI가 이 논쟁의 불을 지피고 있다고 합니다. 기계는 우리를 움직이는 작품을 만들어내지만, 인간의 영혼이 그것이 발생하는 시점을 결정합니다. 동굴 벽화부터 아티스트들은 형식과 내용을 동기부여하고 깊은 감정을 일으키기 위해 노력했습니다. 지금은 우리를 강요하여 감정의 근원을 재정의하도록 이끕니다. 예술이 본질적으로 공유된 경험인 경우, 인간이 만든 것인지 기계가 만든 것인지 상관이 있을까요? 예술 경험은 관객 안에서 독특하게 이루어집니다.\n\n# 무한한 캔버스의 세계\n\n문화적 참조를 끊임없이 섞을 수 있는 생성적 AI는 예술적 일률성에 대한 핑커의 우려에 도전하며, 그가 예술적 관문을 비판하는 것에 울린다고 합니다. 내재적인 제약이 적어져서 전례 없이 다양한 표현을 허용합니다. 이는 문화적 도용부터 잠재적 편견까지 자체적인 복잡성을 야기하나, 일부 예술계의 지각되는 엘리트주의에 대응합니다.\n\n# 새로운 윤리적 풍경을 탐색하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 예술은 피커가 자주 탐구하는 사회적 갈등을 반영하여 윤리적 문제의 폭풍우를 드러냅니다. 저작권부터 진정성에 대한 우려, 그리고 '영감' 자체의 본질까지, 생성적 AI는 우리에게 현존하는 도덕적 틀을 재고하도록 강요합니다.\n\n생성적 AI는 아름다운 이미지와 윤리적 고민의 폭풍우를 만들어내지만, 피커의 작품 전반에 걸친 통찰은 이 분야에서 놀랍게도 여전히 유의미합니다. 기술과 예술의 본질을 재검토하고 인간의 의도 이상의 감정적 창조력을 사색함으로써, AI 예술은 잠재력과 도발을 제공합니다.\n\n## 이 이야기가 마음에 드셨다면, 다음도 좋아하실 수 있습니다:","ogImage":{"url":"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png"},"coverImage":"/assets/img/2024-05-18-TheHumanLensExploringAIsImpactonCreativity_0.png","tag":["Tech"],"readingTime":3},{"title":"스타일러 AI가 당신의 스쳐그림을 예술작품으로 만드는 방법","description":"","date":"2024-05-18 20:43","slug":"2024-05-18-HowStylarAIcanmakeyourdoodlesintoart","content":"\n\n저는 디자이너와 예술가로 훈련을 받았고, 그림 그리는 것을 좋아했어요. 하지만 안타깝게도 그 시절은 지나갔어요. 신경병증 때문에 손을 제어하는 게 어렵게 되었거든요. 그래서 생성 모델 인공지능이 도움을 준 거예요.\n\n어떤 식으로든지 인공지능이 제게 예술적인 비전을 표현하는 데 도움이 돼요. 제 손 대신 단어로요.\n\nStylar AI를 시도해본 적이 있을 때, 나 자신이 적어도 일부분은 다시 창작에 돌아갈 수 있는 잠재력이 있다는 것에 기쁨을 느꼈어요.\n\nStylar AI는 포토샵과 비슷한 레이어, 배경 및 객체 제거 도구, 얼굴 교체 및 수정 도구, 생성 채우기와 확장 기능 등 독특한 기능이 있는 강력한 도구에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n한 가지 기능 중 하나는 이미지 대 이미지 생성입니다. 다른 생성적 AI 도구에도 있지만, Stylar AI 버전은 가장 간단한 입력과 작업할 수 있는 놀라운 능력이 있어서 뛰어난 고품질 출력물을 생성할 수 있습니다.\n\nStylar AI 홈페이지에는 간단한 손으로 그린 도형이 어떻게 인상적인 로고로 만들어지는지 보여주는 비디오가 심지어 있는데요.\n\n그것이 저에게 영감을 주어 이미지 대 이미지 도구를 사용해 몇 개의 선으로 그린 그림들을 시도해 보았습니다. 첫 번째 그림은 아르누보 양식의 여성 그림이었습니다.\n\n이 결과물들이 정말 마음에 들었고, 놀라울 정도로 Stylar가 선택한 스타일을 적용하는 데 전혀 문제가 없었고 단지 흑백 선으로 이미지를 안내하여 구성했을 뿐이었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 시도는 해변 오두막을 그린 것이었습니다. 결과는 다시 한 번 인상적이었습니다.\n\n나는 비교적 간단한 입력 이미지가 탁월한 결과를 만들 수 있는 것을 확신했습니다. 그래서 다음 시도는 꽃무늬의 추상 만달라 스타일 그림이었습니다.\n\n다양한 스타일이 이 간단한 입력 이미지를 수정하는 방법이 놀라울 정도입니다. 이것은 추상 벽 장식품, 스크린 세이버, 배경 및 기타 이미지 생성용 새로운 스타일을 만드는 무한한 기회를 제공합니다.\n\n가장 최근 예시는 나무 그림에 대한 간단한 예술적인 그림입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시, 훌륭한 결과네요. 이 중에 있는 그림들을 벽에 걸어두는 것이 수치스럽지 않을 거에요.\n\n그리고 여기가 관건이에요: 이 그림 중 일부는 프롬프트만 있으면 만들기가 굉장히 어려울 거에요.\n\n좋아요, 지금까지 사용한 것은 여전히 숙련된 손에 의해 만들어진 정적 이미지였어요. 그래서 다음 실험은 심지어 더 간단한 선들을 기반으로 한 것이었어요. 이는 큰 예술 능력이나 훈련이 필요하지 않았답니다.\n\n내 자신이 빠져들고 있었어요. 간단한 그림을 계속 만들고, 스타일러 AI에 입력한 뒤 멋진 이미지들의 산을 만들어내고 있었어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n네, 손이 잘못됐더라도 다시 만들 수 있었어요. 간단한 선 그림은 정확도나 통제력이 많이 필요하지 않아요. 제 시각과 아이디어의 간단한 낙서가 작동했어요.\n\n당신에게도 도움이 될 거예요. 놀라운 건 이 간단한 스케치가 콘텐츠와 구성을 이끌어 줄 수 있다는 거예요. 스타일은 모든 것을 다 다루죠.\n\n이 그림들을 만들기 위해 프롬프트 창에 단어를 입력할 필요가 없었어요. 그냥 스케치와 선택한 스타일뿐이었어요.\n\n그리고 만약 마음에 드는 스타일이나 원하는 스타일을 찾지 못한다면, Stylar AI에서 손쉽게 여러분만의 스타일을 만들 수 있어요. 이미지를 업로드하기만 하면, 다음에 원할 때 적용하기 쉽도록 여러분의 사용자 정의 스타일 사이에 저장돼요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사의 표지 그림은 스케치와 이미지 대 이미지 도구를 사용해서 만들어졌어요. 이 기술은 아름다운 사실적인 이미지, 일러스트레이션, 스티커, 티셔츠 아트, 그리고 떠오르는 다른 모든 것들을 만들어낼 수 있어요.\n\nStylar AI는 다른 방법으로나 어플로는 어렵게 만들 수 없는 이미지를 만들어 낼 수 있어요.\n\nAivaras Grauzinis","ogImage":{"url":"/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png"},"coverImage":"/assets/img/2024-05-18-HowStylarAIcanmakeyourdoodlesintoart_0.png","tag":["Tech"],"readingTime":3},{"title":"The title translated into Korean would be NLP 대 LLM 주요 차이점을 이해하는 포괄적 가이드","description":"","date":"2024-05-18 20:40","slug":"2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences","content":"\n\n\n![NLP vs LLM](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png)\n\nNLP 및 LLM 기술은 대규모로 사람 언어를 분석하고 생성하는 데 중요합니다. 그들의 증가하는 보급으로, LLM 대 NLP를 구별하는 것은 점점 더 중요해지고 있습니다.\n\nNLP는 인간 언어를 이해, 조작 및 생성하기 위한 일련의 알고리즘을 포함합니다. 1950년대에 처음으로 등장한 이후, NLP는 텍스트 관계를 분석하는 데 진화했습니다. 이는 품사 태깅, 명명된 개체 인식 및 감성 분석 방법을 사용합니다.\n\nOpenAI의 ChatGPT가 보여주는 것처럼, LLM은 깊은 학습을 활용하여 방대한 텍스트 세트로 학습합니다. 인간과 유사한 텍스트를 모방할 수 있지만, 언어의 뉘앙스를 이해하는 것은 제한됩니다. NLP가 언어 분석에 중점을 둔 반면, LLM은 주로 텍스트를 생성합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제안서를 소개해 드릴게요. NLP와 LLMs의 간결하면서 포괄적인 비교를 제공합니다. 이 두 기술의 복잡성을 탐구하고, 다양한 응용 분야를 알아보며, 도전 과제를 살펴볼 것입니다.\n\n# NLP의 독특한 특징 탐구\n\nNLP는 기계가 인간 언어를 의미 있는 방식으로 이해하고 상호 작용하는 데 도움을 줍니다. 스펠 체크, 자동 교정부터 챗봇과 음성 비서에 이르기까지 다양한 응용 분야에 사용될 수 있습니다.\n\nNLP는 인간 언어 생성을 가능하게 하는 알고리즘을 만드는 것입니다. 이것은 디지털 시스템과 인간 간의 소통 간극을 줄입니다. 이 기술은 산업 전반에 걸쳐 증진된 데이터 분석과 통찰력을 위한 길을 엽니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_1.png\" /\u003e\n\n## NLP에서 꼭 알아야 할 기술: 파싱부터 자연어 생성까지\n\n자연어 처리는 컴퓨터가 인간의 언어를 생성할 수 있도록 다양한 프로세스에 의존합니다:\n\n- 파싱. 이 기술은 문장을 문법적 요소로 분해합니다. 기계에게 언어 구조를 간소화해 주며 품사, 문장 구분 및 구문적 연결을 인식하는 데 도움이 됩니다.\n- 의미 분석. 단순한 단어 식별을 넘어 단어의 의미와 관계를 파악하는 과정입니다. 텍스트의 맥락, 관용구 및 유머를 해석하는 데 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_2.png)\n\n- 음성 인식. 구어를 쓰인 텍스트로 변환하여 음성을 읽을 수 있는 형식으로 전환하게 됩니다.\n- 자연어 생성. 음성 인식과는 반대로, NLG는 컴퓨터 데이터에 기반하여 인간의 글쓰기를 모방한 텍스트를 제공합니다. 보고서 작성, 요약, 메시지 작성 등을 포함한 응용분야가 있습니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_3.png)\n\n- 감성 분석. 소셜 미디어 모니터링과 브랜드 평판 관리에 자주 사용됩니다. 글의 감정 톤을 평가하고 고객 피드백 및 시장 동향을 분석합니다. \n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![NLP vs LLMA: Comprehensive Guide to Understanding Key Differences 4](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_4.png)\n\n- 기계 번역. 한 언어에서 다른 언어로 텍스트나 음성을 변환하는 기능을 제공합니다.\n- 명명된 엔터티 인식. 텍스트에서 중요한 정보를 감지하고 분류합니다. 예를 들어 개인, 장소, 조직의 이름 등을 인식합니다.\n\n![NLP vs LLMA: Comprehensive Guide to Understanding Key Differences 5](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_5.png)\n\n- 텍스트 분류 및 분류. 텍스트에 레이블을 할당하여 방대한 데이터 양을 효율적으로 정리하고 관리할 수 있습니다. 이는 문서, 이메일, 그리고 온라인 콘텐츠를 구성하는 데 유용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## NLP Applications: Enhancing Communication and Analysis\n\n![NLP Applications](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_6.png)\n\nNLP의 응용 프로그램은 광범위하며 다양한 섹터에 영향을 미칩니다:\n\n- 텍스트 분석. 대규모 텍스트 데이터 세트를 분석하여 중요한 통찰을 얻습니다. 시장 조사 및 소셜 미디어 감시에 유용할 수 있습니다.\n- 음성 인식. 음성으로 된 지시를 이해하고 실행하는 데 사용되는 음성 활성화 장치 및 응용 프로그램을 구동합니다. 이 기술은 가상 어시스턴트 및 필기 도구의 기반을 제공합니다.\n- 감성 분석. 감정적인 맥락을 분석합니다. 대중 의견을 모니터링하고 시장 조사를 수행하는 데 중요합니다.\n- 기계 번역. 언어 장벽을 깨뜨려 텍스트나 음성을 번역하여 국제적인 의사 소통을 촉진합니다.\n- 콘텐츠 추천. NLP를 사용하여 사용자 선호도와 콘텐츠 특성에 기반한 콘텐츠 제안을 맞춤화합니다. 또한 온라인 스트리밍 플랫폼과 온라인 쇼핑에서 경험을 향상시킵니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## NLP에서의 과제: 제한 사항을 극복하며\n\n진전은 있지만 NLP는 여러 가지 장애물을 극복해야 합니다. 이러한 문제를 해결한다면 NLP의 정확성과 기술 통합을 높일 수 있습니다:\n\n- 문맥적 이해. 언어의 미묘한 차이(비꼬거나 관용적인 표현)를 이해하는 것은 여전히 어려움을 겪고, 오해를 일으킬 수 있습니다.\n- 언어 다양성. 문법과 구문이 각각 다른 여러 언어와 방언의 수가 많은 것은 상당한 난관으로 작용합니다.\n- 언어의 모호성. 인간의 언어적 모호성은 NLP 시스템의 해석을 복잡하게 만들 수 있습니다.\n- 데이터 품질과 이용 가능성. NLP 시스템의 성능은 훈련 데이터의 품질과 양에 좌우됩니다. 이 데이터의 편향은 왜곡된 결과를 초래할 수 있습니다.\n- 계산 리소스. 고급 앱에 대한 상당한 계산 능력 수요는 그 개발과 배포를 제한합니다.\n- 실시간 처리. 동시 번역 및 고객 서비스와 같은 응용에 대한 실시간 처리는 기술적인 도전을 제시합니다.\n\n# 대형 언어 모델의 능력 탐색\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델은 언어 작업에 포괄적인 접근 방식을 제공합니다. 기존의 자연어 처리 시스템 이상으로 유창성과 적응성을 나타냅니다. LLM은 생성적 AI를 위해 정교한 기술 스택을 활용하여:\n\n- 일관성 있고 맥락에 적합한 텍스트 생성\n- 의미 있는 대화를 진행\n- 질문에 대한 답변 제공\n- 인간의 글쓰기와 유사한 콘텐츠 생성\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_7.png)\n\n## LLM의 차별화된 특징\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLMs는 그들을 돋보이게 하는 몇 가지 핵심 속성으로 특징 지어집니다:\n\n- 광범위한 훈련 데이터. LLMs는 다양한 텍스트 소스에서의 방대한 데이터 세트로 훈련됩니다. 이 접근 방식은 다양한 언어 스타일과 형식을 생성할 수 있게 합니다.\n- 적응성. 언어 모델은 특정 작업마다 특별한 훈련이 필요 없이 여러 가지 언어 작업에 대응할 수 있습니다. LLMs는 자동 콘텐츠 생성과 고급 챗봇 기능에 매우 유연하게 대처할 수 있습니다.\n- 문맥적 이해. LLMs는 문맥과 관련된 텍스트를 생성하여 텍스트 단락 사이에 일관성을 유지합니다.\n- 지속적 학습. LLMs는 새로운 데이터에 노출되면 언어 능력을 개선하고 확장할 수 있습니다. 그들은 계속해서 신조어와 용어에 적응하고 있습니다.\n\n## LLMs 뒤의 핵심 기술들\n\n대형 언어 모델의 효과성은 그들의 기술적 기반이에 근간을 두고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 딥 러닝. LLMs는 여러 층의 신경망을 사용하여 자율적으로 학습하고 결정을 내릴 수 있습니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_8.png)\n\n- 트랜스포머 아키텍처. 이러한 모델은 순차적 데이터를 처리하기 위해 설계되어 있으며, 문장에서 다음 단어의 정확한 예측을 가능하게 합니다.\n\n![image](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_9.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 자기주의 메커니즘. LLMs는 각 단어의 중요성을 평가함으로써 관련성 높은 응답을 생성할 수 있습니다.\n- 확장성. 점진적으로 큰 데이터셋으로 LLMs를 학습시킴으로써 그들의 능력을 향상시킬 수 있습니다.\n\n## LLMs의 실용적인 응용\n\nLLMs는 다음과 같은 다양한 분야에서 응용됩니다:\n\n- 콘텐츠 생성. 기사와 보고서 작성부터 시를 창작하는 데 이르기까지.\n- 고객 서비스. 챗봇을 통해 효율적이고 정확한 자동응답을 제공합니다. 예를 들어, ChatGPT 플러그인 개발은 서비스 중심 분야에서 사용자 경험을 향상시킬 수 있습니다.\n- 언어 번역. 언어적 미묘함에 대한 심층적 이해로 LLMs는 전 세계적인 의사소통을 용이하게 할 수 있습니다.\n- 교육 도구. 개인 맞춤형 학습 자료 생성, 숙제 채점, 방대한 텍스트 요약 등을 보조합니다.\n- 의료 분야. 환자 상호작용, 정보 관리, 의료 문서 분석을 지원합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## LLMs의 도전과 윤리적 고려 사항\n\n고급 기능을 가진 LLMs는 신중한 고려가 필요한 제한과 윤리적 딜레마에 직면하고 있습니다:\n\n- 편견과 공정성. LLMs는 기존 데이터에서 학습하기 때문에 공정성과 대표성에 대한 우려가 제기됩니다.\n- 정확성과 신뢰성. 출력물은 때로는 사실적인 정확성보다는 데이터 패턴을 반영할 수 있습니다. 이로 인해 부정확성이나 비논리적인 응답이 발생할 수 있습니다.\n- 진정한 이해 부족. LLMs는 이해를 시뮬레이션하지만 진정한 이해가 부족합니다. 이는 복잡한 상황에서의 오류나 부적절한 출력물로 이어질 수 있습니다.\n- 데이터 프라이버시. 잠재적으로 민감한 데이터를 처리하는 것은 엄격한 데이터 거버넌스의 중요성을 강조합니다.\n- 에너지 소비. NLP처럼, 필요한 중요한 계산 리소스는 환경 및 자원 할당에 관한 우려를 제기합니다.\n\n# Comparative Analysis: NLP vs LLM\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP과 LLM은 언어를 통해 인간-컴퓨터 상호작용을 향상시키는 데 중요한 역할을 합니다. 둘은 공통 목표를 공유하지만 방법론, 능력 및 적용 영역에서 몇 가지 차이가 있습니다. NLP와 LLM의 성능, 확장성, 정확성 및 다양한 분야에서의 유용성에 초점을 맞춰보겠습니다.\n\n## 성능 메트릭\n\nNLP: 구문 분석 및 entity 인식과 같은 전문 작업에서 높은 정확도를 보여줍니다.\n\nLLM: 인간과 비슷한 텍스트 생성 및 다양한 언어 작업을 처리하는 데 뛰어납니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 확장성 및 효율성\n\nNLP: 낮은 계산 요구 사항으로 구체적인 작업을 실행하는 데 더 효율적입니다.\n\nLLM: 다양한 작업을 수행하는 데 매우 확장 가능하며, 더 많은 계산 리소스를 필요로하지만 능숙합니다.\n\n## 정확성 및 신뢰성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**NLP**: 전문 분야에서 높은 정확도와 신뢰성을 보입니다. 문맥의 풍부한 이해를 요구하는 작업에서는 도전을 겪을 수 있습니다.\n\n**LLM**: 일관된 언어 출력을 생성하는 데 신뢰성을 보입니다. 그러나 훈련 데이터에 영향을 받아 부정확하거나 편향된 내용을 생성할 수도 있습니다.\n\n## 건강 관리에서의 활용성\n\n**NLP**: 의료 기록 처리, 관련 환자 정보 추출 및 예측 진단을 가능하게 하는 데 활용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM: 환자 상호 작용을 용이하게 하고 정보를 전파하며 일반 의학적 조언을 제공함.\n\n## 금융 분야에서의 유용성\n\nNLP: 감성 분석, 위험 평가, 그리고 고객 서비스 향상에 활용됨. 특히 은행 분야에서 생성 형태 AI를 통한 금융언어 처리에 능하다.\n\nLLM: 금융 보고서 작성, 시장 분석 수행, 그리고 고객 서비스 상호작용 자동화에 유용함.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 전자 상거래에서의 사용성\n\nNLP: 챗봇을 통한 고객 경험 향상, 맞춤 추천 및 고객 피드백 분석을 통해 결과를 개선합니다.\n\nLLM: 콘텐츠 생성, 대규모 고객 상호 작용 관리 및 디지털 마케팅의 측면을 자동화하는 데 도움이 됩니다.\n\n# NLP와 LLM 통합을 통한 AI 강화\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nNLP와 LLM의 통합은 고급 언어 처리 시스템 개발에서 중대한 발전을 이룬 것입니다. 이 협력은 NLP의 정확한 능력과 LLM의 광범위한 문맥 지식을 결합합니다. 이는 업계 전반에서 AI 응용 프로그램의 효율성과 효과성을 크게 향상시킬 수 있습니다.\n\n## NLP와 대형 언어 모델 통합의 상호 혜택\n\nNLP를 LLM 기술과 통합하는 것은 여러 가지 핵심 이점을 제공합니다:\n\n- 향상된 정확도와 문맥 이해. NLP의 특정 처리 능력을 LLM의 광범위한 문맥 이해와 결합함으로써 언어 작업을 실행할 때 정확도와 관련성을 높일 수 있습니다.\n- 자원 최적화. NLP의 특정 작업 처리 효율성이 LLM의 자원 집약적인 성격을 보완합니다. 이는 확장 가능한 솔루션과 컴퓨팅 자원의 더 나은 할당을 이끌어냅니다.\n- 증가하는 유연성과 적응성. 이러한 기술의 결합은 AI 응용 프로그램의 유연성과 적응성을 향상시킵니다. 그들은 진화하는 요구 사항에 더 민첩하게 대응할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 현실 세계 통합 성공 사례\n\nNLP와 LLM의 협력 잠재력은 다양한 성공적인 응용 프로그램을 통해 입증되었습니다. 이 협력이 AI 응용 프로그램을 혁신화하는 방식을 살펴보겠습니다:\n\n- 의료 분야. IBM 왓슨은 NLP와 LLM을 사용하여 방대한 의료 데이터를 해석합니다. NLP가 구체적 정보 추출에서의 정확성을 발휘하면서 LLM은 보다 넓은 맥락을 이해하는 능력을 결합합니다. 회사는 통찰력 있는 진단 및 치료 권장을 위해 이를 활용합니다.\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 금융분야. Bloomberg과 존스홉킨스 대학의 협력 노력을 통해 탄생한 BloombergGPT. 이 모델은 다양한 금융 업무에서 뛰어난 성과를 내기 위해 방대한 데이터셋으로 훈련되었습니다. 이 모델은 연구 확장, 정보 추출, 의사결정 조율, 편향 식별 및 리스크 관리에 도움을 줍니다.\n- 전자상거래분야. Amazon Comprehend는 이 통합을 활용하여 고객 상호작용, 리뷰 및 지원 문의를 분석합니다. 이를 통해 기업은 고객 행위와 선호도를 보다 깊게 이해할 수 있습니다. 이는 제품 검색, 추천, 고객 지원 및 전반적인 만족도 향상으로 이어질 수 있습니다.\n\n![이미지](/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_11.png)\n\n## NLP 및 LLM 협력의 미래 예측\n\nNLP와 대규모 언어 모델의 지속적인 통합은 새로운 능력과 응용 프로그램을 개방할 것으로 예상됩니다. 확실히 이는 AI 기술과 상호작용하는 방식에 영향을 미칠 것입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- AI 어시스턴트 업그레이드. 미래 AI 어시스턴트는 복잡한 인간 상호작용에 대한 높은 이해도와 반응성을 보일 것으로 예상됩니다. 이는 NLP와 LLM의 통합 덕분에 실현될 것입니다.\n- 자동 컨텐츠 생성 혁신. NLP의 언어 규칙과 LLM의 창의적 능력을 결합하면 더 정교한 컨텐츠 생성 도구가 제공될 것입니다.\n- 로봇의 언어 이해 업그레이드. 이러한 시너지는 로봇의 언어 처리 능력을 크게 향상시킬 수 있습니다. 이는 보다 자연스럽고 효과적인 인간-로봇 상호작용을 이끌어낼 수 있을 것입니다.\n\n# 결론\n\nNLP 대 LLMs는 각각 인간 언어 처리에 대한 독특한 접근 방식을 가지고 있습니다 — NLP는 구체적인 알고리즘 모델링에 초점을 맞추고 LLMs는 대규모 사전 훈련을 통해 포괄적인 능력을 제공합니다 — 그러나 그들은 서로를 잘 보와합니다. 이들의 통합은 더 풍부한 AI 상호작용, 심층적인 산업 통합, 지속적인 AI 윤리 및 기술 발전을 약속합니다. 이러한 기술들의 책임있는 개발과 적용은 매우 중요합니다.\n\n우리가 미래를 바라보며, LLM과 NLP의 교차점은 새로운 AI 기반 솔루션의 시대를 열 것으로 기대됩니다. NLP와 LLM의 잠재력을 탐색하고자 하는 기관들을 위해, Softermii는 이러한 기술을 효과적으로 활용하기 위한 전문 지식과 지원을 제공합니다. 당사 팀에 연락하시어 혁신적이고 윤리적인 AI 애플리케이션을 위한 길을 열어보세요.","ogImage":{"url":"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png"},"coverImage":"/assets/img/2024-05-18-NLPvsLLMAComprehensiveGuidetoUnderstandingKeyDifferences_0.png","tag":["Tech"],"readingTime":9},{"title":"클로드 3를 사용하여 비디오 튜토리얼을 블로그 포스트로 변환하기","description":"","date":"2024-05-18 20:37","slug":"2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost","content":"\n\n## Anthropic이 Karpathy의 비디오 요약 도전에 대한 해결책 재현\n\n이 문서를 작성하는데 출발점은 Andrej Karpathy가 LLM 토큰화에 관한 2시간 13분 비디오 강의를 게시한 직후에 X에서 게시한 글입니다. 이 강의를 책 장/chapter 또는 블로그 글로 자동으로 변환하는 작업에 대한 해결책이 필요한 도전을 받았습니다.\n\n![Image](/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png)\n\n그 후에 Anthropic의 Emmanuel Ameisen과 동료들이 특히 Anthropic의 최신 모델인 Claude 3을 통해 이 작업을 수행할 것을 제안하는 것으로 보이는 해결책이 게시되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_1.png)\n\n사소한 문제와 일관성 부족이 있었지만, 이 방법은 꽤 효율적인 것으로 보였습니다. 결과적으로 생긴 블로그 포스트는 원본 비디오에서 다룬 대부분의 요소와 관련 스크린샷 및 코드 예제를 포함했습니다.\n\n이 작업을 재현하는 데 얼마나 쉽고 비싼지 궁금해졌습니다. 결과적으로, 처음에 기대했던 것보다 더 복잡한 과정이었습니다. 프롬프트는 공유되었으나 코드는 그렇지 않았습니다.\n\n본 문서는 내 구현 방식을 공유하고, 각 단계를 자세히 설명하며, 주요 어려움에 대해 논의합니다. 코드 및 데이터는 이 Github 저장소에서 확인할 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요약:\n\n- 비디오를 블로그 포스트/책 챕터로 변환하는 것은 대형 멀티모달 모델(LMMs)의 또 다른 매력적인 활용 사례이며, 비디오 콘텐츠를 텍스트 형식으로 제공하여 읽기 쉽고 빠르게 살펴보고 검색할 수 있게 만듭니다.\n- 그러나 LMMs를 기반으로 한 텍스트 변환은 다양한 부정확성과 불일치가 포함될 수 있어 철저한 검토와 교정이 필요합니다. 다른 어려움은 결과의 재현 불가능성과 효과적인 프롬프트 식별에 관려된 것입니다.\n- Claude 3 Opus와 같은 LMM을 활용하여 비디오를 텍스트 형식으로 변환하는 것은 저렴하지 않습니다. 본 문서에서 제시된 솔루션은 이 블로그 포스트로 이 비디오를 변환하는 데 약 4달러의 비용을 소요했습니다.\n\n# 워크플로 개요 및 기술적 제약사항\n\nClaude 3 Opus는 Anthropic에서 제공하는 최신이자 가장 성능이 뛰어난 대형 멀티모달 모델(LMM)입니다. 이 모델은 3월 4일에 발표되었으며, claude.ai 웹 인터페이스 또는 API를 통해 액세스할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델은 최대 200K 토큰의 텍스트 또는 이미지를 입력으로 받아들일 수 있고, 최대 4K 토큰의 텍스트를 출력할 수 있습니다. 이것이 정확히 무엇을 의미하는지 조금 더 구체적으로 분석해 보겠습니다:\n\n- 출력의 4K 토큰: 한 토큰이 대략 3/4 단어라는 경험 법칙을 고려하면, 4K 토큰은 대략 3K 단어에 해당합니다. 페이지 당 대략 500단어를 가정한다면, 클로드는 최대 6페이지의 텍스트를 출력할 수 있습니다.\n- 입력의 200K 토큰: 동일한 통계를 따르면, 이는 15만 단어(약 300 페이지)에 해당합니다. 초당 대략 2~3단어의 발화 속도를 전제하면, 약 20시간의 오디오 트랜스크립트를 소화할 수 있으며, 이는 상당히 많은 양입니다. 반면, 1280*720 픽셀(비디오 HD) 해상도의 이미지를 인코딩하는 데에는 약 1.25K 토큰이 필요합니다. 따라서 한 번에 이론상으로는 150여 장의 이미지를 입력으로 제공할 수 있습니다. 실제로는, 토큰 사용량과는 무관하게, 현재 Anthropi API는 입력 이미지 수를 20장으로 제한하고 있음을 참고해야 합니다.\n\n따라서, 주요 제약 사항은 입력으로 제공할 수 있는 이미지의 제한된 수와 모델이 생성할 수 있는 페이지 수의 제한에 있습니다. 해결책은 비디오를 챕터로 분할하여, 각각이 LMM에 의해 별도로 처리되게 하는 것입니다. 결과물은 그 후에 결합되어 최종 문서를 생성합니다.\n\n아래 다이어그램은 워크플로우의 주요 단계를 요약하고 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAmeisen \u0026 Co는 YouTube 비디오 설명에 제시된 장을 기준으로 비디오를 분할했습니다(총 24장). 다른 전략으로는 LLM과 같은 주제 분할 도구를 활용하여 대본을 주요 부분으로 분할하는 방법이 있습니다. 몇 분 간격의 장을 목표로 설정하는 것이 좋으며, 이를 통해 명령과 대본에 함께 들어갈 10~20개의 스크린샷을 포함할 수 있습니다.\n\n마지막으로 처리 비용을 예상해 봅시다. Claude 3 Opus의 토큰 사용 비용은 입력 토큰당 15달러이며, 출력 토큰당 75달러입니다.\n\n5분 간격의 장을 가정하면, 2시간짜리 비디오는 24개의 장을 제공하는데, 각 장은 평균적으로 다음을 필요로 합니다:\n\n- 13,000개의 입력 토큰(텍스트 토큰 1,000개 및 1.2K 토큰/이미지의 10개)\n- 1,000개의 출력 토큰(2페이지)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n총 입력 토큰은 약 13*2≈300천 개이며, 출력 토큰은 1천 * 24 = 24천 개입니다. 백만 토큰당 비용을 곱하면 입력 비용이 15*0.3=4.5달러, 출력 비용이 75*0.024=1.8달러가 됩니다.\n\n따라서 2시간 비디오에서 게시물을 생성하는 총 비용은 대략 5에서 10달러 정도입니다. 최적화 전략을 사용하여 어떤 스크린샷을 포함할지 신중하게 선택하고 입력 비용을 줄일 수 있습니다.\n\n# 구현\n\n이제 우리의 구현으로 넘어가 봅시다. 이는 우리의 워크플로우에서 제시한 네 가지 주요 단계를 따릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 비디오를 다운로드하고 텍스트를 획득합니다.\n- 텍스트와 스크린샷을 정렬한 챕터로 분할합니다.\n- 챕터의 LMM 처리를 수행합니다.\n- LMM 출력을 결합하고 블로그 글을 작성합니다.\n\n명확성을 위해 각 단계에 대해 가장 직관적인 구현 방법을 제시합니다. 동반 노트북에는 때로는 데이터를 처리하는 고급 방법을 사용한 추가 코드가 포함될 수 있습니다.\n\n## 비디오를 다운로드하고 오디오 텍스트를 가져옵니다\n\nYouTube에 동영상이 있는 경우, 먼저 pytube 라이브러리를 사용하여 비디오를 다운로드합니다. 나중에 블로그 글을 생성하기 위해 비디오 프레임이 필요하므로 오디오 스트림만이 아닌 전체 비디오를 다운로드합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nimport pytube\n\n# Andrej Karpathy: GPT Tokenizer 만들기 – https://www.youtube.com/watch?v=zduSFxRajkE\nyoutube_video_id = \"zduSFxRajkE\"\ndef download_youtube_video(video_id, output_path):\n    \"\"\"\n    YouTube 비디오를 다운로드하고 output_path에 저장한 후 비디오 ID를 파일 이름으로 반환합니다.\n    \"\"\"\n    # 비디오 ID로 YouTube 객체 생성\n    youtube = pytube.YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n    # 가장 높은 해상도의 비디오 스트림 가져오기\n    stream = youtube.streams.get_highest_resolution()\n    # 비디오 다운로드\n    video_path = stream.download(output_path=output_path, filename=video_id+\".mp4\")\n    return video_path\n# 330MB 비디오에 대해 약 20초 소요\nvideo_path = download_youtube_video(youtube_video_id, DATA_DIR)\n```\n\n대부분의 Youtube 비디오에는 대본이 이미 제공되어 있습니다. youtube_transcript_api와 같은 라이브러리를 사용하여 Youtube 비디오 ID를 제공함으로써 단순히 대본을 얻을 수 있습니다.\n\n```python\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ntranscript = YouTubeTranscriptApi.get_transcript(youtube_video_id)\n```\n\n2시간 13분의 오디오 스트림 전체가 3422개의 세그먼트로 대본화되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```bash\nlen(transcript)\n3422\n\ntranscript[0:4]\n[{'text': \"hi everyone so in this video I'd like us\",  'start': 0.04,  'duration': 4.04}, {'text': 'to cover the process of tokenization in',  'start': 2.04,  'duration': 4.4}, {'text': 'large language models now you see here',  'start': 4.08,  'duration': 4.2}, {'text': \"that I have a set face and that's\",  'start': 6.44,  'duration': 3.88}]\n```\n\n만약 트랜스크립트를 사용할 수 없다면, 오디오 스트림을 음성 인식 모델을 사용하여 텍스트로 변환해야 합니다. 🤗 Open ASR Leaderboard는 성능이 우수한 모델을 찾을 수 있는 좋은 장소입니다. 동반 노트북에 위스퍼 모델과 효율적인 faster-whisper 구현을 사용하여 트랜스크립트를 가져오는 코드를 제공합니다. 이 프로세스는 Google Colab의 T4에서 약 25분이 걸리며(RTX 4090에서는 12분) 완료됩니다.\n\n# 텍스트와 스크린샷이 정렬된 장이로 나누기\n\n장은 수동으로 식별하거나 YouTube가 제공하는 자동 비디오 장 도구와 같은 도구를 사용하여 식별할 수 있습니다. 예제 비디오의 경우, 비디오 설명에 개요된 24개의 장을 복사하여 Python chapters_list 객체에 저장했습니다. 아래에 설명된 것과 같이요.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\nchapters_list = [\n{'timestamp': 0, 'topic': 'Tokenization을 이해하기 위한 소개 및 동기 부여'},\n {'timestamp': 262, 'topic': 'GPT-2에서 토큰화를 위해 바이트 수준 인코딩을 소개한 논문 소개'},\n {'timestamp': 933, 'topic': '유니코드, UTF-8 인코딩 및 어휘 크기'}\n...\n]\n```\n\n그런 다음이 단계의 핵심은 챕터의 시작/끝 타임스탬프에 따라 텍스트와 스크린샷을 추출하는 것입니다. 이것은 chop_up_in_chapters 함수에 의해 구현되며 각 챕터마다 챕터의 시작 및 끝 타임스탬프를 식별하고 트랜스크립트에서 해당 텍스트를 추출하여 비디오에서 스크린샷을 추출합니다.\n\n스크린샷 추출 전략은 각 주어진 챕터에 대해 최대 10장의 스크린샷을 균일하게 샘플링하여 추출하되, 스크린샷 간에 최소 한 분이 경과하도록합니다.\n\n추출된 텍스트 및 스크린샷은 별도의 폴더에 저장됩니다(챕터 번호를 이름으로 사용).```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef chop_up_in_chapters(chapters_list, video_path, transcript, timestamps_screenshots_list_seconds=None):\n    \"\"\"\n    비디오를 장(chapter)으로 나눕니다. 비디오 장(chapter) 목록을 기준으로 합니다.\n    \"\"\"\n    n_chapters=len(chapters_list)-1\n    print(f\"장 수: {n_chapters}\")\n    # 타임스탬프와 주제에 대해 반복합니다.\n    for current_chapter in range(n_chapters):\n        output_dir=CHAPTERS_DIR+\"/\"+str(current_chapter)\n         # 해당 출력 디렉토리가 없으면 생성합니다.\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        # 현재와 다음 타임스탬프를 가져옵니다.\n        current_chunk_start_time=chapters_list[current_chapter]['timestamp']\n        current_chunk_end_time=chapters_list[current_chapter+1]['timestamp']-1\n        print(f\"장 {current_chapter}; 시작: {current_chunk_start_time}, 끝: {current_chunk_end_time}\")\n        # 현재 장에 대한 텍스트 및 프레임을 추출합니다.\n        get_text_chapter(transcript, current_chunk_start_time, current_chunk_end_time, output_dir)\n        \n        if timestamps_screenshots_list_seconds is not None:\n            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir,timestamps_screenshots_list_seconds[current_chapter])\n        else:\n            get_frames_chapter(video_path, current_chunk_start_time, current_chunk_end_time, output_dir)\n```\n\n# 대규모 다중모달 모델(LLM) 처리\n\n이것은 핵심 단계입니다. 각 장마다 오디오 대본과 선택한 스크린샷이 LMM에 제공되어 이 입력 데이터를 교과서에 포함할 수 있는 출력으로 변환하는 것이 목표입니다.\n\n이 단계의 핵심 요소는 우리가 다음과 같이 설계한 LLM 프롬프트입니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nprompt_instructions = \"\"\"\n\u003cinstructions\u003e\n동영상의 이미지를 다른 타임 스탬프로 제공했으며, \u003ctranscript\u003e 안에 오디오 대본도 포함되어 있습니다.\n대본은 인공지능 음성 인식 도구에 의해 생성되었으며 일부 오류/불일치가 있을 수 있습니다.\n귀하의 작업은 대본을 마크다운 블로그 포스트로 변환하는 것입니다.\n이 대본은 소음이 많습니다. 다음 가이드라인을 사용하여 블로그 장을 위한 마크다운 형식으로 다시 작성하십시오:\n- 유효한 마크다운 출력\n- 적절한 곳에 섹션 제목 및 다른 서식 삽입\n- 대사 블록의 일부만 제공되어 주요 주제만 포함하세요. 소개나 결론 단락은 포함하지 마세요. 대사에서 논의된 주요 주제만 포함해주세요.\n- 이미지, 텍스트, 코드, 강조 및 페이지 레이아웃 및 여백을 일반적인 블로그 게시물 또는 교과서와 같이 보이도록 스타일링\n- 말투적인 속성을 제거하십시오\n- 중복 정보가 있는 경우 반복되는 정보는 한 번만 제시해주세요\n- 대화식 내용을 유지하되 대화의 구조를 따를 수 있도록 제목을 포함하세요\n- 각 대본에는 너무 많은 이미지가 포함되어 있으므로 출력에는 가장 중요한 1-2개의 이미지만 포함해주세요\n- 대사와 관련된 일부를 시각화하는 데 도움이 되는 이미지를 선택해주세요\n- 이미지를 선택할 때는 대사에서 설명한 것과 관련된 완전한 코드를 표시하는 이미지를 선호해주세요\n- 이미지가 대본의 일부를 설명하는 데 도움이 될 경우 포함해주세요\n- 이미지를 포함하려면, \u003cimg src=\"xxxxx.jpg\"/\u003e 태그를 삽입하며, 여기서 xxxxx는 이미지 데이터 위에 삽입된 정확한 이미지 타임스탬프로 대체되어야 합니다\n- 불필요한 정보를 추가하지 마세요. 대본이나 이미지에서 언급된 사항만 포함해주세요\n최종 출력물은 교과서에 포함하기 적합해야 합니다.\n\u003c/instructions\u003e\n\"\"\"\n```\n\n우리는 주로 Ameisen의 안내를 재사용했습니다. 다음과 같은 수정을 가했습니다.\n\n- LMM 출력을 결합하기 위해 출력 형식을 HTML에서 마크다운으로 변경하여 더 직관적으로 만들었습니다 (그리고 마크다운 형식은 블로그 게시물에 시각적으로 잘 어울립니다).\n- 출력 형식이 마크다운으로 정의되었음에도 불구하고 시각적인 요소와 작성 스타일 이미지를 제거했습니다. 이들이 더 유용한 정보를 추가하지 않는다는 결론에 이른 후였습니다.\n- 프롬프트에서 일부 서식을 Anthropic의 가이드라인에 더 잘 따르도록 변경했습니다. 특히, 앞부분에 있는 스크린샷을 이동시키고 지침을 XML 태그로 래핑했습니다.\n\n프롬프트는 장의 스크린샷 및 대본 앞에 오고 있습니다. 우리는 JPG 스크린샷을 Anthropic의 비전 API에 적합한 형식으로 변환하기 위해 `get_screenshots_as_messages` 도우미 함수를 정의했습니다. 이 함수는 모든 스크린샷을 반복하여 각각에 대한 두 가지 메시지를 설명합니다: 스크린샷의 타임스탬프를 지정하는 텍스트 메시지와 그것의 base64로 인코딩된 표현을 포함하는 이미지 메시지입니다. 나중에 하이퍼링크가 추가된 최종 문서에서 원본 비디오로 이동할 수 있게 해주는 타임스탬프가 포함된 텍스트 메시지입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef get_screenshots_as_messages(screenshots):\n  screenshots_as_messages = []\n  for i in range(len(screenshots)):\n    screenshots_as_messages.extend([\n    {\n    \"type\": \"text\",\n    \"text\": f\"The timestamp for the following image is {Path(screenshots[i]).stem}.\"\n    },\n    {\n    \"type\": \"image\",\n    \"source\": {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": base64.b64encode(open(screenshots[i], \"rb\").read()).decode(\"utf-8\"),\n      }\n    }])\n  return screenshots_as_messages\n```\n\n우리는 스크린샷, 대본 및 지침을 모아서 함께 가져오는 또 다른 도우미 함수인 get_prompt_as_messages를 정의했습니다. 이 함수는 추가로 Claude의 출력을 미리 채워 마크다운 제목(\"#\")으로 답변을 시작하도록 만듭니다.\n\n```js\ndef get_prompt_as_messages(chapter_id):\n    folder_path=CHAPTERS_DIR+'/'+str(chapter_id)\n    with open(folder_path+'/transcript.txt', \"r\") as f:\n        transcript = f.read()\n    screenshots=sorted(glob.glob(folder_path+'/*.jpg'))\n    \n    screenshots_as_messages=get_screenshots_as_messages(screenshots)\n    prompt_as_messages = [\n        {\n            \"role\": \"user\",\n            \"content\": screenshots_as_messages+\n            [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"\u003ctranscript\u003e\\n{transcript}\\n\u003c/transcript\u003e\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt_instructions\n                }\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"#\"\n                }\n            ]\n        }\n    ]\n    return prompt_as_messages\n```\n\n그게 다야!\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 챕터는 클로드를 반복적으로 호출하여 처리한 후 결과를 해당 챕터 폴더에 있는 마크다운 파일로 작성할 수 있습니다.\n\n```js\n# 챕터 목록을 반복하여 처리\nfor chapter in range(len(chapters_list)-1): \n  # 현재 챕터에 대한 프롬프트 생성 (스크린샷, 대본 및 지침이 포함된 메시지 목록).\n    prompt_generate_markdown = get_prompt_as_messages(chapter)\n    # 프롬프트를 사용하여 메시지 생성하기\n    message = client.messages.create(\n        model=\"claude-3-opus-20240229\",\n        system=\"당신은 마크다운 블로그 글쓰기 전문가입니다.\",\n        temperature=0,\n        max_tokens=4000,\n        messages=prompt_generate_markdown\n    )\n    # 응답에서 생성된 마크다운 콘텐츠 추출\n    answer = message.content[0].text\n    markdown = \"#\"+answer  # 마크다운 콘텐츠 앞에 헤더 태그 추가\n    \n    # 현재 챕터에 해당하는 마크다운 파일 경로 정의\n    markdown_file = CHAPTERS_DIR + '/' + str(chapter) + '/markdown.md'\n    # 생성된 마크다운 콘텐츠를 파일에 작성\n    with open(markdown_file, \"w\") as f:\n        f.write(markdown)\n```\n\n아래는 Anthropic의 사용 로그 중 마지막 일곱 챕터 처리에 대한 부분을 보고, 처리 시간 및 입력 및 출력 토큰 수의 변동을 감지할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_2.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 긴 장은 마지막에서 두 번째 장이었습니다 (여기에서 장을 확인하세요), 1시 51분부터 2시 10분까지 총 17689 토큰을 처리하는 데 거의 1분이 걸렸습니다. 전체적으로 비디오 및 24 장을 처리하는 데 약 10분이 소요되었고, 18만 토큰의 입력 및 1만 5천 토큰의 출력이 사용되었습니다. 이 과정은 약 4달러의 비용이 소요되었습니다.\n\n# LMM 출력을 결합하여 최종 블로그 포스트 생성\n\n워크플로우의 최종 단계는 주로 두 가지 작업으로 구성됩니다. 먼저 다양한 마크다운 출력을 병합합니다. 그 다음으로는 장 제목 및 이미지에 하이퍼링크를 추가합니다. 이를 통해 최종 마크다운 파일을 해당 타임스탬프에서 원본 YouTube 비디오에 연결할 수 있습니다.\n\n```js\nmerged_markdown=\"\"\n\n# 장 폴더를 반복하여 마크다운 파일 병합\nfor chapter in range(len(chapters_list)-1):\n    markdown_file=CHAPTERS_DIR+'/'+str(chapter)+'/markdown.md'\n    with open(markdown_file, \"r\") as f:\n        markdown = f.readlines()\n    # 각 장 제목에 해당하는 비디오의 링크를 추가\n    url_chapter = f\"https://www.youtube.com/watch?v={youtube_video_id}\u0026t={chapters_list[chapter]['timestamp']}s\"\n    markdown[0] = f\"# [{chapter+1}) {markdown[0][2:].strip()}]({url_chapter})\"\n    markdown = '\\n'.join(markdown)\n    merged_markdown+=\"\\n\"+markdown\n# 병합된 마크다운에서 src 속성의 타임스탬프가 포함된 모든 \u003cimg\u003e 태그를 찾아 해당 비디오의 링크를 추가\ntimestamps_screenshots = re.findall(r'\u003cimg src=\"(\\d+)\\.jpg\"/\u003e', merged_markdown)\ntimestamps_screenshots = [timestamp for timestamp in timestamps_screenshots]\n# 각 이미지에 해당하는 타임스탬프에서 올바른 비디오 링크를 추가\nfor timestamp in timestamps_screenshots:\n    video_link = f'\u003ca href=\"https://www.youtube.com/watch?v={youtube_video_id}\u0026t={int(timestamp)}s\"\u003e비디오 링크\u003c/a\u003e'\n    merged_markdown = merged_markdown.replace(f'\u003cimg src=\"{timestamp}.jpg\"/\u003e', f'\u003cimg src=\"{timestamp}.jpg\"/\u003e\\n\\n{video_link}')\n# 병합된 마크다운에서 이미지를 기반으로한 프레임을 추출하고 merge 폴더에 저장\nget_frames_chapter(video_path, None, None, MERGE_DIR, timestamps_screenshots=timestamps_screenshots)\n# 병합된 마크다운을 markdown 블로그포스트.md 파일로 저장\nmarkdown_file=MERGE_DIR+'/blogpost.md'\nwith open(markdown_file, \"w\") as f:\n        f.write(merged_markdown)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결합된 마크다운 파일은 MERGE_DIR 폴더에 모든 선택한 JPG 스크린샷과 함께 'markdown.md'로 저장됩니다 (최종 출력).\n\n# 토의\n\n결과적으로 포스트는 원래 비디오의 대부분의 내용을 성공적으로 보존하여, Ameisen 및 그의 동료가 설명한 것과 유사한 품질을 달성했습니다. 또한, 의미 있는 스크린샷 및 코드 조각을 정확하게 식별하여 오디오 대본의 이해를 돕습니다. 그러나, 텍스트 변환이 정확하지 않은 부분을 비롯해 일부 결함이 있습니다.\n\n정확하지 않거나 모순된 부분을 해결하기 위해 철저한 편집과 교정이 여전히 필요합니다. (Ameisen의 작업에 발견된 것과 유사한) 문제의 예로는 예를 들어, \"hello world\" 토큰을 2가 아닌 300으로 잘못 세는 오류, \"tokenization\"의 첫 번째 토큰을 잘못 번호 매기는 오류, 공백을 토큰으로 오도록 잘못 인식하는 등이 있습니다 (블로그 포스트의 2장 참조). 이러한 부정확성 외에도, 이 방법론은 효과적인 프롬프트를 만드는 복잡성, 결과의 재현 불가능성 및 LMM 운영에 따른 비용과 같은 다른 어려움을 야기합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 이러한 단점에도 불구하고 비디오를 접근 가능하고 쉽게 탐색할 수 있는 텍스트 블로그 포스트로 변환하는 것은 대형 다중 모달 모델의 가치 있는 응용 프로그램입니다. 특히 경쟁하는 LMM(대형 다중 모달 모델)인 GPT4-V, Gemini Pro Vision 및 오픈 소스 대규모 월드 모델의 비디오/이미지 이해 능력과 비교는 내일의 블로그 포스트 주제가 될 것입니다.\n\n# 유용한 링크\n\n- 동반자 Github 저장소\n- Karpathy의 도전과 Ameisen 및 동료의 저장소\n- 토큰화에 대한 비디오 튜토리얼 : https://www.youtube.com/watch?v=zduSFxRajkE 및 손으로 쓴 튜토리얼 요약 : https://github.com/karpathy/minbpe/blob/master/lecture.md\n- Claude 3 — Vision 문서\n- Misbah Syed의 다른 접근 방식\n\n참고: 별도로 표시되지 않는 한, 모든 이미지는 작성자가 제공한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 게시물을 즐겼나요? 생각을 공유하거나 박수를 보내거나 LinkedIn에서 저와 연락하세요.","ogImage":{"url":"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png"},"coverImage":"/assets/img/2024-05-18-UsingClaude3toTransformaVideoTutorialIntoaBlogPost_0.png","tag":["Tech"],"readingTime":15},{"title":"스피치브레인 모델을 사용하여 음성에서 배경 소음 제거하기","description":"","date":"2024-05-18 20:36","slug":"2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels","content":"\n\nSpeechBrain은 음성 처리를 위해 설계된 오픈 소스, 올인원 툴킷입니다. PyTorch를 기반으로 구축되었으며 음성 인식, 화자 식별 및 음성 향상을 포함한 다양한 음성 관련 작업을 위한 포괄적인 도구 모음을 제공합니다. 모듈식이며 유연한 구조로 연구자와 개발자 모두에게 접근하기 쉽도록 설계되어 최신 음성 처리 모델로의 신속한 개발과 실험을 가능하게 합니다.\n\nSpeechBrain은 2021년에 음성 기술 분야를 발전시키고자 열정을 가진 연구자와 엔지니어들에 의해 세상에 소개되었습니다. 이 프로젝트는 오픈 소스의 성격과 견고한 성능으로 빠르게 주목을 받았습니다. 몇 년 동안, SpeechBrain은 전 세계 AI 커뮤니티로부터 다수의 기여를 받아 음성 처리의 최신 기술 발전을 지속적으로 통합해왔습니다.\n\nSpeechBrain은 매우 모듈식으로, 사용자가 자신의 요구에 따라 툴킷을 쉽게 사용자 정의하고 확장할 수 있습니다. 데이터 전처리부터 모델 훈련 및 평가까지 음성 처리 파이프라인의 각 구성 요소는 독립적으로 수정하거나 대체할 수 있습니다. 이 툴킷은 특정 작업에 대해 즉시 사용하거나 미세 조정할 수 있는 다양한 사전 훈련된 모델을 제공합니다. 이러한 모델은 최신 아키텍처에 기반하고 대규모 데이터셋에서 훈련되어 높은 성능을 보장합니다.\n\n# 음성 분리를 위해 SpeechBrain 사용하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nJupyter Notebook에 SpeechBrain을 설치하는 방법부터 시작하겠습니다.\n\n```js\n!pip install speechbrain\n```\n\n그런 다음 필요한 라이브러리를 가져올 것입니다.\n\n```js\nfrom speechbrain.inference.separation import SepformerSeparation as separator\nimport torchaudio\nfrom IPython.display import Audio\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 HuggingFace에서 모델을 다운로드할 것입니다. 우리는 SpeechBrain으로 구현된 sepformer-wham16k-enhancement 모델을 사용할 예정입니다.\n\n```js\nmodel = separator.from_hparams(source=\"speechbrain/sepformer-wham16k-enhancement\", savedir='pretrained_models/sepformer-wham16k-enhancement')\n```\n\n이제 모델을 사용하여 오디오에서 배경 소음을 분리할 준비가 끝났습니다.\n\n```js\naudio_sources = model.separate_file(path='original_audio.wav')\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델은 텐서 출력을 생성합니다. 우리는 torchaudio를 사용하여 텐서를 오디오 파일로 변환할 수 있습니다.\n\n```python\ntorchaudio.save(\"converted_audio.wav\", audio_sources[:, :, 0], 16000)\n```\n\n# 샘플 오디오 파일로 테스트하기\n\nSpeechBrain의 효과를 테스트하기 위해 Wikimedia에서 제공하는 두 개의 샘플 오디오 파일을 사용할 것입니다: 배경 소음이 많이 섞인 파일과 배경 소음이 적은 파일입니다. 이 오디오를 SpeechBrain을 사용하여 처리하고 결과를 비교할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 샘플 오디오 1: 가벼운 백그라운드 소음\n\n오디오 원본:\n\n이 오디오 파일의 출처 페이지입니다. 우리는 처음 10초를 사용하고 있습니다.\n\n처리된 오디오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 샘플 오디오 2: 강한 백그라운드 소음\n\n원본 오디오:\n\n이 오디오 파일의 출처 페이지입니다. 처음 10초를 사용하고 있습니다.\n\n처리된 오디오:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 발견 사항\n\nSpeechBrain의 소음 제거 능력을 평가한 결과가 복합적이었습니다. 낮은 배경 소음이 포함된 샘플에 모델을 적용했을 때, 모델은 만족스럽게 작동하여 소음을 효과적으로 줄이고 음성의 선명도를 향상시켰습니다. 이는 SpeechBrain이 적당한 수준의 소음을 다루는 데 강점을 가지고 있으며, 일상적인 오디오 개선 작업에 유용한 도구임을 보여줍니다.\n\n그러나, 고 배경 소음이 포함된 샘플에서는 모델이 어려움을 겪었습니다. 일부 소음을 줄이기는 했지만, 출력물은 오디오 클리핑으로 인해 음성의 이해를 저해하는 문제가 있었습니다. 이는 SpeechBrain의 현재 소음 제거 모델이 극도로 시끄러운 배경 환경에 적합하지 않을 수 있다는 것을 나타냅니다.\n\n# 이상적인 사용 사례\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 결과를 고려해봤을 때, SpeechBrain은 다음과 같은 경우에 적합할 것으로 생각됩니다:\n\n- 팟캐스트 및 보이스오버: 비교적 조용한 환경에서 녹음하는 콘텐츠 크리에이터들에게, SpeechBrain은 오디오를 깔끔하게 정리하여 전문적인 음질을 보장할 수 있습니다.\n- 원격 회의 및 통화: 주변 소음이 적당한 전문적인 환경에서는 SpeechBrain이 발화의 명확성을 향상시킬 수 있어서 참여자들이 방해 없이 서로 이해하기 쉬워집니다.\n- 교육 컨텐츠: 교육자 및 온라인 강좌 제작자들은 SpeechBrain을 사용하여 녹화한 콘텐츠를 깔끔하게 정리함으로써 강의와 발표가 명확하고 이해하기 쉽도록 할 수 있습니다.\n\n위의 테스트와 결과에 대해 어떻게 생각하시는지 아래에 알려주세요.\n\n모두를 위한 오픈 소스 대화형 AI. SpeechBrain. (출처: https://speechbrain.github.io/)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSpeechbrain/Sepformer-WHAM16K-enhancement · Hugging Face. speechbrain/sepformer-wham16k-enhancement · Hugging Face. (n.d.). [https://huggingface.co/speechbrain/sepformer-wham16k-enhancement](https://huggingface.co/speechbrain/sepformer-wham16k-enhancement)","ogImage":{"url":"/assets/img/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels_0.png"},"coverImage":"/assets/img/2024-05-18-RemovingbackgroundnoisefromspeechusingSpeechBrainmodels_0.png","tag":["Tech"],"readingTime":4},{"title":"자체 데이터에 대한 질문 응답을 위해 Langchain 사용하기","description":"","date":"2024-05-18 20:32","slug":"2024-05-18-UsinglangchainforQuestionAnsweringonOwnData","content":"\n\n## langchain을 사용하여 자신의 데이터로 대화하는 단계별 안내서\n\n대형 언어 모델은 학습된 주제에 관한 질문에 대답할 수 있습니다. 그러나 그들은 우리의 개인 데이터나 회사의 소유 문서 또는 LLM 학습 이후에 작성된 기사에 관한 질문에 대답할 수 없습니다. 우리 자신의 문서와 대화하고 이러한 문서에서 질문에 대답하는 데 LLM을 사용할 수 있다면 정말 멋질 것입니다. 본 문서는 LangChain: Chat with Your Data라는 강의에서 Andrew Ng 교수와 LangChain 창립자 Harrison Chase로부터 대부분의 내용을 가져왔습니다. 이 글은 langchain에 관한 세 번째 글입니다. 첫 번째 글은 langchain이 LLM 응용 프로그램 개발에 어떻게 사용될 수 있는지에 대해 논의하고 있습니다. 두 번째 글은 LLM 응용 프로그램 개발에 체인과 에이전트를 사용하는 방법에 대해 논의하고 있습니다.\n\nLangChain은 LLM 응용 프로그램을 구축하기 위한 오픈 소스 개발자 프레임워크입니다. 본 글에서는 LangChain의 특정 사용 사례인 LangChain을 사용하여 자신의 데이터와 대화하는 방법에 중점을 둘 것입니다. 이 글에서는 주로 다음 주제를 다룰 것입니다:\n\n- 문서 로딩\n- 문서 분할\n- 벡터 저장소 및 임베딩\n- 검색\n- 질문 응답\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 문서 로딩\n\n검색 증강 생성 (RAG) 프레임워크에서 LLM은 실행 중 일환으로 외부 데이터셋에서 문맥적 문서를 검색합니다. 이는 우리가 특정 문서 (예: PDF, 비디오 등)에 대해 질문하고 싶을 때 유용합니다. 데이터와 대화할 수 있는 애플리케이션을 만들고 싶다면 먼저 데이터를 작업할 수 있는 형식으로 로드해야 합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png)\n\n이를 위해 LangChain의 문서 로더를 사용합니다. 문서 로더는 다양한 형식과 소스에서 데이터에 액세스하고 변환하는 구체적인 사항을 다룹니다. 구조화된 데이터 소스나 구조화되지 않은 데이터 소스에서 로드할 수 있습니다. 예를 들어 웹사이트, 데이터베이스, YouTube, arxiv, Twitter, Hacker News 또는 Figma, Notion과 같은 소유 데이터 소스 또는 Airbyte, Stripe, Airtable과 같은 소스에서 데이터에 액세스하고 로드해야 할 수 있습니다. 이러한 문서는 pdf, html, json, word, PowerPoint과 같은 다양한 데이터 유형이거나 표 형식일 수 있습니다. 문서 로더는 이러한 데이터 소스로부터 데이터를 가져와 내용과 관련 메타데이터로 구성된 표준 문서 객체로 로드합니다. 또한 langchain에는 아래에서 확인할 수 있는 80가지가 넘는 다양한 문서 로더가 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Screenshot](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_1.png)\n\n## PyPDF DataLoader\n\n이제 PyPDF 로더를 사용하여 pdf를 로드할 것입니다. Andrew Ng의 유명한 CS229 강의에서 MachineLearning-Lecture01.pdf를 로드할 것입니다.\n\n```js\nfrom langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n\n#Load the document by calling loader.load()\npages = loader.load()\n\nprint(len(pages))\nprint(pages[0].page_content[0:500])\n\nprint(pages[0].metadata)\n# {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n```\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 문서 목록을 불러옵니다. 이 경우에는 PDF에 22개의 서로 다른 페이지가 있습니다. 각 페이지는 문서이며 문서에는 페이지 콘텐츠와 메타데이터가 포함되어 있습니다. 페이지 콘텐츠는 페이지의 내용이며 메타데이터 요소는 각 문서와 관련된 메타데이터가 포함되어 있습니다.\n\n## YouTube DataLoader\n\nLangChain은 YouTube에서 비디오를 불러오는 YoutubeAudioLoader를 제공합니다. 이 loader를 사용하여 비디오나 강의에서 질문을 하거나 할 수 있습니다.\n\n```js\nfrom langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders.parsers import OpenAIWhisperParser\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n\nurl=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\nsave_dir=\"docs/youtube/\"\nloader = GenericLoader(\n    YoutubeAudioLoader([url],save_dir),\n    OpenAIWhisperParser()\n)\ndocs = loader.load()\n\nprint(docs[0].page_content[0:500])\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nYouTubeAudioLoader는 YouTube 링크에서 오디오 파일을 로드하고 OpenAIWhisperParser를 사용합니다. OpenAIWhisperParser는 OpenAI의 음성 대 텍스트 Whisper 모델을 사용하여 YouTube 오디오를 작업할 수 있는 텍스트 형식으로 변환합니다. YouTube URL과 오디오 파일을 저장할 디렉토리를 지정해야 합니다.\n\n## WebBaseLoader\n\nWebBaseLoader는 인터넷에서 URL을 로드하는 데 사용됩니다.\n\n```js\nfrom langchain.document_loaders import WebBaseLoader\n\n# 깃허브 페이지에서 마크다운 파일 사용\nloader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")\n\ndocs = loader.load()\nprint(docs[0].page_content[:500])\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위에 나온 출력물에는 많은 공백이 있으므로 해당 출력물에 후처리를 해야 합니다.\n\n## NotionDirectoryLoader\n\nNotionDirectoryLoader을 사용하여 Notion에서 데이터를 로드합니다. Notion은 개인 및 회사 데이터를 저장하기 위한 인기 있는 툴입니다. Notion Space에서 페이지를 복제하고 페이지를 마크다운/CSV 파일로 내보낼 수 있습니다.\n\n```js\nfrom langchain.document_loaders import NotionDirectoryLoader\nloader = NotionDirectoryLoader(\"docs/Notion_DB\")\ndocs = loader.load()\n\nprint(docs[0].page_content[0:200])\nprint(docs[0].metadata)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 우리는 다양한 소스에서 데이터를로드하고 표준화 된 문서 인터페이스로 가져 오는 방법에 대해 다뤘습니다. 그러나 이러한 문서가 크다면, 작은 청크로 나누어야 할 수도 있습니다. 이것은 검색 확장 생성의 경우, 우리가 우리에게 가장 관련이있는 콘텐츠 조각들을 검색해야하기 때문에 중요합니다.\n\n# 문서 분할\n\n문서 분할은 문서를 작은 청크로 나누는 것이 필요합니다. 문서 분할은 데이터를 표준화 된 문서 형식으로로드 한 후에 일어나지만 벡터 저장소로 이동하기 전에 발생합니다.\n\n![image](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n문서를 작은 조각으로 나누는 것은 중요하며 각 조각 사이의 의미 있는 관계를 유지해야 하는 부분이 까다롭습니다. 예를 들어, Toyota Camry에 대한 2개의 조각이 있다면:\n\n이 경우에 간단히 나누었더니 한 조각에 문장의 일부가, 다른 조각에는 다른 부분이 포함되어 있습니다. 따라서 Camry의 사양에 관한 질문에 대답할 수 없게 될 수 있습니다. 따라서 의미론적으로 관련된 조각으로 나누는 것이 중요합니다.\n\n이제 아래와 같이 RecursiveCharacterTextSplitter와 CharacterTextSplitter를 초기화합니다:\n\n```js\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n\nchunk_size =26\nchunk_overlap = 4\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n입력 텍스트는 지정된 청크 크기 및 정의된 청크 중첩에 따라 분할됩니다. 청크 크기는 청크의 크기를 측정하는 길이 함수입니다. 이는 주로 문자 또는 토큰입니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_3.png)\n\n청크 중첩은 두 청크 사이에 작은 중첩이 있도록 사용되며, 이는 2개의 청크 사이에 어떤 일관성 개념을 가질 수 있도록 합니다. 페이지에 나와 있는 것처럼 Lang Chain에는 다양한 유형의 스플리터가 있습니다:\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLang Chain의 Text Splitters에는 문서 생성 및 문서 분할이라는 2가지 메소드가 있습니다. 둘 다 내부에서 동일한 논리를 가지고 있지만 하나는 텍스트 목록을, 다른 하나는 문서 목록을 입력으로 받습니다. 이러한 텍스트 분할기들은 청크를 나누는 방식(문자 또는 토큰으로)이나 청크의 길이를 측정하는 방식과 같은 다양한 차원에서 다를 수 있습니다. 때로는 문장의 끝을 결정하기 위해 다른 작은 모델을 사용하고 해당 정보를 사용하여 청크를 분할할 수도 있습니다. 메타데이터는 텍스트/문서를 청크로 분할할 때 중요합니다. 모든 청크를 통일된 메타데이터를 유지하면서 새로운 메타데이터 조각을 추가해야 할 수도 있습니다. 때로는 청크의 분할이 문서 유형에 특정할 수 있습니다. 코드에서 분할할 때 나타날 수 있습니다. Python, Ruby, C와 같은 서로 다른 언어에 대해 서로 다른 분리자를 사용하는 언어 텍스트 분할기를 사용합니다.\n\n이제 LangChain의 몇 가지 텍스트 분할기 예제를 살펴보겠습니다.\n\n```js\n# Recursive text Splitter\ntext1 = 'abcdefghijklmnopqrstuvwxyz'\nr_splitter.split_text(text1)\n# Output - ['abcdefghijklmnopqrstuvwxyz']\n\n# Character Text Splitter\ntext2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\nr_splitter.split_text(text2)\n# Output - ['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']\n\n# Recursive text Splitter\ntext3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\nr_splitter.split_text(text3)\n# output - ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n\n# Character Text Splitter\nc_splitter.split_text(text3)\n# output - ['a b c d e f g h i j k l m n o p q r s t u v w x y z']\n\n# Character Text Splitter with separator defined\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separator = ' '\n)\nc_splitter.split_text(text3)\n# Output - ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n```\n\n## RecursiveSplitting\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 실제 예시 몇 가지를 시도해 보겠습니다. 재귀 텍스트 분할기와 문자 텍스트 분할기가 어떻게 다르게 작동하는지 살펴볼 것입니다.\n\n```js\nsome_text = \"\"\"When writing documents, writers will use document structure to group content. \\\nThis can convey to the reader, which idea's are related. For example, closely related ideas \\\nare in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\nParagraphs are often delimited with a carriage return or two carriage returns. \\\nCarriage returns are the \"backslash n\" you see embedded in this string. \\\nSentences have a period at the end, but also, have a space.\\\nand words are separated by space.\"\"\"\n\nlen(some_text) -\u003e 496\n\nc_splitter = CharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separator = ' '\n)\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0, \n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n```\n\n여기서 CharacterTextSplitter는 공백을 구분자로 사용하며, RecursiveCharacterTextSplitter의 경우 구분자 목록을 전달합니다.\n\n첫 번째 경우에, 다음 출력이 나옵니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```json\n['문서를 작성할 때 작성자는 문서 구조를 사용하여 콘텐츠를 그룹화합니다. 이는 독자에게 관련된 아이디어를 전달할 수 있습니다. 예를 들어, 밀접하게 관련된 아이디어는 문장에 있습니다. 비슷한 아이디어들은 단락에 있습니다. 단락은 문서를 형성합니다. \\n\\n 단락은 종종 개행 문자 또는 두 개의 개행 문자로 구분됩니다. 개행 문자는 이 문자열에 포함된 \"백슬래시 n\"입니다. 문장은 끝에 마침표가 있지만 또한 띄어쓰기를 하고 단어들은 띄어쓰기로 구분됩니다.']\n```\n\n두 번째 경우에 대한 출력은 다음과 같습니다:\n\n```json\n['문서를 작성할 때 작성자는 문서 구조를 사용하여 콘텐츠를 그룹화합니다. 이는 독자에게 관련된 아이디어를 전달할 수 있습니다. 예를 들어, 밀접하게 관련된 아이디어는 문장에 있습니다. 비슷한 아이디어들은 단락에 있습니다. 단락은 문서를 형성합니다.',\n '단락은 종종 개행 문자 또는 두 개의 개행 문자로 구분됩니다. 개행 문자는 이 문자열에 포함된 \"백슬래시 n\"입니다. 문장은 끝에 마침표가 있지만 띄어쓰기를 하고 단어들은 띄어쓰기로 구분됩니다.']\n```\n\nRecursiveCharacterTextSplitter의 경우, 구분자 목록으로는 두 번의 개행, 한 개의 개행, 공백, 빈 문자열이 있습니다. 따라서 이는 텍스트를 두 번의 개행으로 분할하고, 그 다음으로 한 개의 개행 뒤에 공백이 따라오는 경우와 마지막으로 문자별로 분할합니다. RecursiveTextSplitter는 이중 개행을 기준으로 텍스트를 분할하므로 텍스트가 두 개의 단락으로 분할됩니다. 첫 번째 단락이 450자보다 짧은 것을 확인할 수 있으며, 이는 두 번의 개행을 기준으로 분할하는 것이 더 나은 분할인 것으로 생각됩니다. 문자 텍스트 분할은 띄어쓰기에 따라 분할되므로, 문장 중간에 이상한 구분이 발생하는 것을 확인할 수 있습니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자 이제 PDF 파일을 사용한 TextSplitter의 실제 예제를 하나 더 실행해 보겠습니다.\n\n```js\nfrom langchain.document_loaders import PyPDFLoader\nloader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\npages = loader.load()\n\nfrom langchain.text_splitter import CharacterTextSplitter\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\",\n    chunk_size=1000,\n    chunk_overlap=150,\n    length_function=len\n)\n\ndocs = text_splitter.split_documents(pages)\n\nlen(docs) -\u003e 77\nlen(pages) -\u003e 22\n```\n\n여기서는 Python의 기본 길이 함수를 전달한 것입니다.\n\n## 토큰 분할\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 문자 기반으로 텍스트를 분할해 왔습니다. 이제 토큰 수를 기준으로도 분할할 수 있습니다. 이는 LLMs에서 종종 토큰으로 지정된 컨텍스트 창을 가지고 있기 때문에 유용할 수 있습니다. 토큰은 일반적으로 ~4개의 문자로 이루어져 있습니다.\n\n```js\nfrom langchain.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n\ntext1 = \"foo bar bazzyfoo\"\ntext_splitter.split_text(text1)\n# ['foo', ' bar', ' b', 'az', 'zy', 'foo']\n```\n\n```js\ntext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\ndocs = text_splitter.split_documents(pages)\n\ndocs[0]\n# Document(page_content='MachineLearning-Lecture01  \\n', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0})\n\npages[0].metadata\n# {'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n```\n\n## 컨텍스트에 따라 분할하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n체킹의 목적은 공통 컨텍스트를 가진 텍스트를 함께 묶는 것입니다. 텍스트 분할은 종종 문장이나 다른 구분자를 사용하여 관련된 텍스트를 함께 유지하지만 많은 문서(예: Markdown)는 구조(헤더)가 있으므로 분할에 명시적으로 사용할 수 있습니다.\n\n이를 위해 헤더 텍스트 스플리터를 사용하여 헤더 메타데이터를 유지하는 목적으로 마크다운 파일을 분할할 수 있습니다. 헤더나 하위 헤더를 기반으로 마크다운 파일을 나누고 해당 헤더를 메타데이터 필드에 내용으로 추가하여 해당 분할에서 파생된 모든 청크로 전달됩니다.\n\nMarkdown 형식의 표를 다음과 같이 변경해보겠습니다:\n\n```js\nfrom langchain.document_loaders import NotionDirectoryLoader\nfrom langchain.text_splitter import MarkdownHeaderTextSplitter\n```\n\n제목이 있는 문서와 그 후 부제목 (장 1)과 여러 문장이 있는 섹션이 있습니다. 그런 다음 다른 섹션에 부제목 (장 2)과 그곳에 몇 개의 문장이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nmarkdown_document = \"\"\"# Title\\n\\n \\\n## Chapter 1\\n\\n \\\n안녕하세요, 제임스입니다\\n\\n \\\n안녕하세요, 조입니다\\n\\n \\\n### 섹션\\n\\n \\\n안녕하세요, 랜스입니다\\n\\n \\\n## Chapter 2\\n\\n \\\n안녕하세요, 말리입니다\"\"\"\n\n\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n```\n\n이제 MarkdownHeaderTextSplitter를 정의합니다.\n\n```js\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=headers_to_split_on\n)\nmd_header_splits = markdown_splitter.split_text(markdown_document)\n```\n\n마지막으로, 텍스트 분할 결과를 얻습니다:```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nmd_header_splits[0]\n# 문서(page_content='안녕하세요, 제 이름은 짐입니다  \\n안녕하세요, 제 이름은 조입니다', metadata={'Header 1': '제목', 'Header 2': '장 1'})\n\nmd_header_splits[1]\n# 문서(page_content='안녕하세요, 제 이름은 랜스입니다', metadata={'Header 1': '제목', 'Header 2': '장 1', 'Header 3': '섹션'})\n```\n\n의미론적으로 관련있는 청크를 적절한 메타데이터와 함께 얻을 수 있었습니다. 이제 이 데이터 청크를 벡터 저장소로 이동할 것입니다.\n\n# 벡터 저장소와 임베딩\n\n우리는 문서를 작은 청크로 나누었고, 이제 이러한 청크를 색인에 넣어두어 이 문서에 대한 질문에 답변할 때 쉽게 검색할 수 있도록 합니다. 이를 위해 임베딩과 벡터 저장소를 사용합니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_5.png)\n\n텍스트 분할 이후에는 벡터 저장소와 임베딩이 필요합니다. 문서를 쉽게 접근할 수 있는 형식으로 저장해야 합니다. 임베딩은 텍스트를 가져와 텍스트의 수치적 표현을 만듭니다. 즉, 의미론적으로 유사한 내용을 가진 텍스트는 임베딩 공간에서 유사한 벡터를 가집니다. 따라서 임베딩(벡터)을 비교하고 유사한 텍스트를 찾을 수 있습니다.\n\n전체 파이프라인은 문서로 시작합니다. 이러한 문서를 작은 덩어리로 분할하고 이러한 분할 또는 문서의 임베딩을 만듭니다. 마지막으로, 모든 이러한 임베딩을 벡터 저장소에 저장합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_6.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 저장소는 나중에 비슷한 벡터를 쉽게 찾을 수 있는 데이터베이스입니다. 질문과 관련 있는 문서를 찾을 때 유용합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_7.png)\n\n따라서 질문에 대한 답변을 얻고 싶을 때, 우리는 질문의 임베딩을 만들고 이를 벡터 저장소 내 모든 다른 벡터들과 비교하여 가장 유사한 n개를 선택합니다. 마지막으로, n개의 가장 유사한 청크를 질문과 함께 LLM에 전달하고 답변을 얻습니다.\n\n이제 우리는 어떻게 문서 세트를 벡터 저장소에 로드하는지 알아봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom langchain.document_loaders import PyPDFLoader\n\n# PDF 파일 로드하기\nloaders = [\n    # 일부러 문서를 중복하여 넣어 지저분한 데이터를 만듭니다.\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n]\ndocs = []\nfor loader in loaders:\n    docs.extend(loader.load())\n```\n\n문서가 로드된 후 RecursiveCharacterTextSplitter를 사용하여 청크를 생성합니다.\n\n```js\n# 텍스트 스플리터 정의\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=150\n)\n\n# 텍스트 스플리터를 사용해 문서를 나눕니다.\nsplits = text_splitter.split_documents(docs)\n```\n\n이제 PDF의 모든 청크에 대한 임베딩을 생성한 다음 벡터 저장소에 저장할 것입니다. 우리는 OpenAI를 사용하여 이러한 임베딩을 만들 것입니다. 우리는 Chroma를 우리의 경우에 벡터 저장소로 사용할 것입니다. Chroma는 가벼우며 메모리에 저장되어 쉽게 시작할 수 있습니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nembedding = OpenAIEmbeddings()\n```\n\n저희는 이 벡터 저장소를 지속적인 디렉토리에 저장하여 나중에 사용할 수 있도록 합니다.\n\n```js\npersist_directory = 'docs/chroma/'\n\n# 벡터 저장소 생성\nvectordb = Chroma.from_documents(\n    documents=splits,\n    embedding=embedding,\n    persist_directory=persist_directory\n)\n\nprint(vectordb._collection.count())\n```\n\n저희는 이전에 생성된 splits, embedding (OpenAI 임베딩 모델), 그리고 지속 디렉토리를 전달하여 벡터 저장소를 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 유사성 검색\n\n유사성 검색 방법을 사용하여 k를 전달하고 반환할 문서의 수를 지정하는 질문을 하겠습니다.\n\n```js\nquestion = \"도움을 요청할 수 있는 이메일이 있나요\"\n\ndocs = vectordb.similarity_search(question,k=3)\n\n# 문서의 길이 확인\nlen(docs)\n\n# 첫 번째 문서의 내용 확인\ndocs[0].page_content\n\n# 나중에 사용하기 위해 데이터베이스 유지\nvectordb.persist()\n```\n\n## 유사성 검색: 극단적인 경우\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적인 유사성 검색은 대부분의 결과를 올바르게 가져옵니다. 그러나 유사성 검색이 실패하는 특이한 경우도 있습니다. 이제 다른 쿼리를 수행하여 중복 결과를 확인할 것입니다.\n\n```js\nquestion = \"matlab에 대해 어떻게 말했나요?\"\n\n# k = 5로 유사성 검색\ndocs = vectordb.similarity_search(question,k=5)\n\n# 처음 두 결과 확인\nprint(docs[0])\nprint(docs[1])\n```\n\n여기서 처음 두 결과는 중복 PDF(duplicate MachineLearning-lecture01.pdf)를 로드했기 때문에 동일합니다. 따라서 중복 청크를 받아서 언어 모델로 넘겼습니다. 코드를 실행하면 의미론적 검색이 모든 비슷한 문서를 가져오지만 다양성은 보장하지 않는다는 결론을 내릴 수 있습니다. 다음 섹션에서 어떻게 관련성이 있으면서도 다른 청크를 동시에 가져오는 방법을 다루겠습니다.\n\n다른 쿼리로 유사성 검색에서 다른 실패 사례가 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nquestion = \"세 번째 강의에서 회귀에 대해 무슨 이야기를 했나요?\"\n\ndocs = vectordb.similarity_search(question,k=5)\n\n\n# 유사성 검색 결과의 메타데이터를 출력합니다\nfor doc in docs:\n    print(doc.metadata)\n\nprint(docs[4].page_content)\n```\n\n우리는 검색 결과의 메타데이터, 즉 이 결과가 어떤 강의에서 나왔는지 확인했습니다. 우리는 결과가 세 번째 강의, 두 번째 강의 및 첫 번째 강의에서 나온 것을 볼 수 있습니다. 이 실패의 이유는 우리가 제 3 강의에서만 문서를 원하는 사실이 구조화된 정보 조각이지만, 우리는 임베딩을 기반으로 의미적 조회만을 수행하고 있으며 임베딩은 아마도 '회귀'라는 단어에 더 초점을 맞추고 세 번째 강의에 대한 정보를 캡처하지 못합니다. 따라서 우리는 회귀와 관련이 있는 모든 결과를 받고 있습니다. 이를 확인하기 위해 다섯 번째 문서를 출력하여 실제로 회귀라는 단어가 언급되었는지 확인할 수 있습니다.\n\n# 검색\n\n검색은 검색 보강 생성(RAG) 플로우의 핵심입니다. 문서에서 질문에 대답을 시도할 때 가장 큰 어려움 중 하나는 검색입니다. 질문에 대한 답변이 실패하는 대부분의 경우, 그 원인은 검색에서 실수하는 것입니다. 또한 LangChain에서 자체 쿼리 및 맥락 압축과 같은 고급 검색 메커니즘에 대해 논의할 것입니다. 검색은 쿼리가 입력되고 가장 관련성이 높은 분할을 검색하고자 할 때 쿼리 시간에 중요합니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![img](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_8.png)\n\n의미 검색이 많은 사용 사례에 대해 꽤 잘 작동했음을 확인했습니다. 그러나 일부 극단적인 상황에서 실패했습니다. 따라서 우리는 검색을 깊이 파고들어 이러한 극단적인 상황을 극복하기 위한 몇 가지 다른 고급 방법을 논의할 것입니다.\n\n- 벡터 스토어에서 데이터 액세스/색인화\n- 기본 의미 유사성\n- 최대 주변 유사성\n- 메타데이터 포함\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. LLM 지원 검색\n\n## 3. 문맥적 압축\n\n## 1. 최대 주변 유사성(MMR)\n\nMMR은 검색 결과에서 다양성을 강화하는 중요한 방법입니다. 의미적 검색의 경우, 임베딩 공간에서 쿼리와 가장 유사한 문서를 얻게 되며, 우리는 다양한 정보를 놓칠 수 있습니다. 예를 들어, 쿼리가 \"큰 열매체를 가진 모든 흰색 버섯에 대해 말해주세요\"인 경우, 첫 두 번째로 유사한 결과를 얻어 쿼리와 관련된 정보인 과일체와 모두 흰색에 대한 정보를 얻을 것입니다. 그러나 첫 두 문서와 유사하지 않지만 중요한 정보를 놓칠 수 있습니다. 여기서 MMR은 다양한 문서를 선택하는데 도움을 주어 이 문제를 해결하는 데 도움이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_9.png)\n\nMMR의 아이디어는 먼저 벡터 저장소를 쿼리하고 \"fetch_k\" 가장 유사한 응답을 선택하는 것입니다. 이제 \"fetch_k\" 문서의 작은 집합에서 작업을 수행하여 쿼리에 대한 관련성과 결과물 간의 다양성을 동시에 달성합니다. 마지막으로, 이러한 \"fetch_k\" 응답 중에서 \"k\"개의 가장 다양한 응답을 선택합니다. 처음 2개의 문서의 처음 100자를 출력하면, 위와 같은 유사도 검색을 사용할 경우 동일한 결과를 얻는 것을 발견할 것입니다. 이제 MMR로 검색 쿼리를 실행하고 처음 몇 가지 결과를 확인해 보겠습니다.\n\n```js\ntexts = [\n    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n]\n\nsmalldb = Chroma.from_texts(texts, embedding=embedding)\nquestion = \"Tell me about all-white mushrooms with large fruiting bodies\"\nsmalldb.max_marginal_relevance_search(question, k=2, fetch_k=3)\n```\n\n여기서 우리는 MMR 검색을 사용하여 결과를 다양하게 만들 수 있었습니다. 이제 유사도 검색과 최대 여유성 검색 결과를 비교해 보겠습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 유사도 검색과 MMR 검색 결과를 비교하세요.\n\n```bash\nquestion = \"matlab에 대해 말한 내용은 무엇인가요?\"\ndocs_ss = vectordb.similarity_search(question, k=3)\ndocs_ss[0].page_content[:100]\ndocs_ss[1].page_content[:100]\n\ndocs_mmr = vectordb.max_marginal_relevance_search(question, k=3)\ndocs_mmr[0].page_content[:100]\ndocs_mmr[1].page_content[:100\u003e\n```\n\n유사도 검색에서 처음 2개 문서의 처음 100자가 동일한 것을 확인할 수 있지만, MMR 검색으로는 처음 2개 문서의 처음 100자가 다른 것을 확인할 수 있습니다. 따라서 쿼리 결과에서 다양성을 얻을 수 있습니다.\n\n## 2. 메타데이터\n\n메타데이터는 검색의 특이성을 조정하는 데 사용됩니다. 이전에 \"세 번째 강의에서 회귀에 대해 어떤 내용을 이야기했습니까?\"라는 질문에 대한 답변이 세 번째 강의뿐만 아니라 첫 번째와 두 번째 강의에서도 반환된 것을 발견했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 문제를 해결하기 위해 메타데이터 필터를 지정할 것입니다. 많은 벡터 저장소는 메타데이터에 대한 작업을 지원합니다. 따라서 소스가 세 번째 강의 PDF와 같아야 한다는 정보를 전달할 것입니다. 여기서 메타데이터는 각 포함된 청크에 대한 맥락을 제공합니다.\n\n```js\nquestion = \"세 번째 강의에서 회귀에 대해 얘기한 내용은 무엇인가요?\"\n\ndocs = vectordb.similarity_search(\n    question,\n    k=3,\n    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n)\n\n# 검색된 문서의 메타데이터 출력\nfor d in docs:\n    print(d.metadata)\n```\n\n이제 검색된 문서의 메타데이터를 살펴보면 모든 문서가 세 번째 강의에서 검색되었음을 확인할 수 있습니다.\n\n## 3. 직접 쿼리\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n셀프 쿼리는 쿼리 자체에서 메타데이터를 추론하려는 경우 중요한 도구입니다. 우리는 SelfQueryRetriever를 사용할 수 있습니다. 이는 LLM을 사용하여 다음을 추출합니다.\n\n- 벡터 검색에 사용할 쿼리 문자열\n- 전달할 메타데이터 필터\n\n여기서는 메타데이터를 기준으로 결과를 필터링하기 위해 언어 모델을 사용합니다. 하지만, 이전에 수동으로 필터를 지정할 필요는 없고 대신 메타데이터와 함께 셀프 쿼리 리트리버를 사용할 수 있습니다.\n\n```js\nfrom langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법은 우리가 의미론적으로 찾으려는 콘텐츠 이외에도 적용할 메타데이터도 포함하는 쿼리를 가지고 있을 때 사용됩니다.\n\n메타데이터에는 source와 page 두 필드가 있습니다. 이러한 속성 각각에 대해 이름과 유형에 대한 설명을 제공해야 합니다. 이 정보는 언어 모델에서 사용되므로 이 설명을 최대한 상세하게 만들어야 합니다.\n\n```js\nmetadata_field_info = [\n    AttributeInfo(\n        name=\"source\",\n        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"page\",\n        description=\"The page from the lecture\",\n        type=\"integer\",\n    ),\n]\n```\n\n또한 문서 저장소에 실제로 들어 있는 정보에 대해 구체적으로 명시해야 합니다. 여기서 LLM은 메타데이터 필터와 함께 전달해야 하는 쿼리를 추론합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndocument_content_description = \"강의 노트\"\nllm = OpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectordb,\n    document_content_description,\n    metadata_field_info,\n    verbose=True\n)\n```\n\n이제 다음 질문으로 검색기를 실행합니다.\n\n```js\nquestion = \"세 번째 강의에서 회귀에 대해 언급한 내용은 무엇인가요?\"\ndocs = retriever.get_relevant_documents(question)\n```\n\n예를 들어, \"1980년에 만들어진 외계인 영화는 어떤 것들이 있나요?\"라는 쿼리를 가질 수 있습니다. 이 쿼리는 2가지 구성 요소를 가지고 있으며, 원본 질문을 메타데이터 필터와 검색 용어로 나누기 위해 언어 모델을 사용할 수 있습니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Using langchain for Question Answering on Own Data](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_10.png)\n\n예를 들어, 이 경우에는 영화 데이터베이스에서 외계인을 검색하고 각 영화의 메타데이터를 1980년도로하는 형태로 필터링합니다. 대부분의 벡터 스토어는 메타데이터 필터를 지원하기 때문에 새로운 데이터베이스나 인덱스가 필요하지 않습니다. 대부분의 벡터 저장소가 메타데이터 필터를 지원하므로, 예를 들어 영화의 년도가 1980년인 경우와 같이 메타데이터를 기반으로 레코드를 쉽게 필터링할 수 있습니다.\n\n## 4. 컨텍스트 압축\n\n압축은 검색된 문서의 품질을 향상시키는 또 다른 방법입니다. 전체 문서를 응용 프로그램을 통해 전달하면 더 많은 비용이 들고 더 나쁜 응답이 될 수 있으므로 검색된 단락의 가장 관련성이 높은 부분만 추출하는 것이 유용합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef pretty_print_docs(docs):\n    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n\n\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\n# Wrap our vectorstore\nllm = OpenAI(temperature=0)\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever()\n)\n\nquestion = \"what did they say about matlab?\"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\npretty_print_docs(compressed_docs)\n\n\nThrough compression, all documents are processed using a language model to extract the most relevant segments, which are then passed into a final language model call.\n```\n\n![Using langchain for Question Answering on Own Data](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_11.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이는 랭귀지 모델을 더 많이 호출할 수 있게 되는 대신, 최종 답변을 가장 중요한 내용에만 집중할 수 있도록 도와줍니다. 그래서 이것은 조금은 교환해야 할 부분이 있습니다.\n\n# 질문응답\n\n우리는 검색에서 방금 검색한 문서를 사용하여 질문응답을 하는 방법을 논의했습니다. 이제 이러한 문서와 원래의 질문을 가져와서 랭귀지 모델로 전달하여 모델에게 질문에 답변을 하도록 요청합니다.\n\n![이미지](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_12.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 검색 QA 체인\n\n먼저 벡터 저장소에서 여러 관련 분할이 검색된 후 질문 응답을 어떻게 수행하는지 살펴봅니다. 또한 관련 분할을 LLM 컨텍스트에 맞게 압축해야 할 수도 있습니다. 마지막으로 이러한 분할을 시스템 프롬프트와 인간 질문과 함께 언어 모델로 전송하여 답변을 가져옵니다.\n\n![image](/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_13.png)\n\n기본적으로 우리는 모든 청크를 동일한 컨텍스트 창에, 동일한 언어 모델 호출에 넣습니다. 그러나 문서 수가 많을 경우 모두를 동일한 컨텍스트 창에 넣을 수 없을 때 다른 방법도 사용할 수 있습니다. MapReduce, Refine 및 MapRerank는 문서 수가 많을 경우 사용할 수 있는 세 가지 방법입니다. 이제 우리는 이러한 방법을 자세히 살펴보겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전에 유지했던 벡터 데이터베이스를 먼저 로드할 거에요.\n\n```js\n# 이전에 유지했던 벡터 데이터베이스를 로드하고 컬렉션 수를 확인해요\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\npersist_directory = 'docs/chroma/'\nembedding = OpenAIEmbeddings()\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\nprint(vectordb._collection.count())\n```\n\n데이터베이스가 제대로 작동하는지 확인하기 위해 유사성 검색을 먼저 수행할 거에요.\n\n```js\nquestion = \"이 수업의 주요 주제는 무엇인가요?\"\ndocs = vectordb.similarity_search(question, k=3)\nlen(docs)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 우리는 이 질문에 대한 답변을 얻기 위해 RetrievalQA 체인을 사용할 것입니다. 첫 번째로, 언어 모델 (ChatOpenAI 모델)을 초기화합니다. 온도를 영으로 설정합니다. 온도를 영으로 설정하면 모델이 낮은 변이성, 최고의 충실도 및 신뢰할 수 있는 답변을 얻기에 좋습니다.\n\n```js\nfrom langchain.chat_models import ChatOpenAI\nllm = ChatOpenAI(model_name=llm_name, temperature=0)\n```\n\n또한 리트리벌QA 체인이 필요합니다. 이것은 리트리버를 사용하여 지원되는 질문 답변을 수행하는 과정입니다. 언어 모델과 벡터 데이터베이스를 리트리버로 전달하여 생성됩니다.\n\n```js\nfrom langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever()\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금, 우리는 묻고 싶은 질문을 가지고 qa_chain을 호출합니다.\n\n```js\n# 질문을 qa_chain에 전달\nquestion = \"이 수업의 주요 주제는 무엇인가요?\"\nresult = qa_chain({\"query\": question})\nresult[\"result\"]\n```\n\n## Prompt로 RetrievalQA 체인 사용하기\n\n이제 좀 더 자세히 이 프로세스를 이해해 봅시다. 먼저 prompt 템플릿을 정의합니다. prompt 템플릿에는 컨텍스트를 사용하는 방법에 대한 지침이 포함되어 있습니다. 또한 컨텍스트 변수를 위한 자리 표시자도 있습니다. 우리는 질문에 대한 답변을 얻기 위해 prompts를 사용할 것입니다. 여기서 prompt는 문서와 질문을 받아서 언어 모델에 전달합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nfrom langchain.prompts import PromptTemplate\n\n# Prompt 템플릿 생성\ntemplate = \"\"\"아래의 문맥을 사용하여 마지막 질문에 대답하세요. 답을 모른다면 그냥 모르는 것으로 말하고 대답을 꾸미지 마세요. 세 문장 이내로 답하세요. 답변은 최대한 간결하게 유지하세요. 답변 끝에 \"질문해 줘서 고마워!\"라고 항상 말씀해 주세요.\n{context}\n질문: {question}\n도움이 되는 답변:\"\"\"\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n\n# 체인 실행\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n)\n```\n\n언어 모델과 벡터 데이터베이스를 사용하여 새로운 검색 QA 체인을 생성했어요.\n\n```js\n# 체인 초기화\n# source 문서를 받으려면 return_source_documents를 True로 설정하세요\n# chain_type을 prompt template으로 정의하세요\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n)\n```\n\n이번에는 새로운 질문을 시도해보고 결과를 확인해 볼 거예요.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n질문 = \"확률은 수업 주제인가요?\"\n결과 = qa_chain({\"query\": 질문})\n# 쿼리 결과 확인\n결과[\"result\"]\n# 참고 문서 확인\n결과[\"source_documents\"][0]\n```\n\n지금까지 기본적으로 \"stuff\" 메서드를 사용했습니다. 이 방법은 모든 문서를 최종 프롬프트에 넣습니다. 이는 언어 모델에 한 번의 호출만을 의미합니다. 그러나 문서가 너무 많은 경우, 문서가 컨텍스트 창 안에 맞지 않을 수 있습니다. 이러한 경우, 맵-리듀스, 정제, 그리고 map_rerank와 같은 다른 기술을 사용할 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_14.png\" /\u003e\n\n이 기술에서 각 개별 문서는 먼저 언어 모델로 전송되어 원본 답변을 얻은 후, 이러한 답변이 최종 답변으로 구성되며 최종적으로 언어 모델에 대한 호출이 이루어집니다. 이는 더 많은 언어 모델 호출을 필요로 하지만, 임의의 많은 문서에 대해 작동할 수 있는 장점이 있습니다.```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법의 두 가지 제한 사항이 있습니다. 첫째, 이전 방법보다 속도가 느리고 둘째, 결과물이 이전 결과물보다 나쁠 수 있습니다. 이는 두 개의 문서에 정보가 분산되어 있을 때 동일한 맥락에서 정보가 나타나지 않을 수 있기 때문입니다.\n\n```js\nqa_chain_mr = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type=\"map_reduce\"\n)\nresult = qa_chain_mr({\"query\": question})\nresult[\"result\"]\n```\n\nRetrievalQA 체인이 하부에서 MapReduceDocumentsChain을 호출할 때 발생합니다. 이는 각 문서에 대해 언어 모델(ChatOpenAI의 경우)에 대해 네 번의 별도 호출을 수행합니다. 이러한 호출의 결과는 최종 체인(StuffedDocumentsChain)에 결합되며, 이는 이러한 응답을 모두 최종 호출에 삽입합니다. StuffedDocumentsChain은 시스템 메시지, 이전 문서의 네 가지 요약 및 사용자 질문을 사용하여 답변을 얻습니다.\n\n```js\nqa_chain_mr = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever(),\n    chain_type=\"refine\"\n)\nresult = qa_chain_mr({\"query\": question})\nresult[\"result\"]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 우리가 검색을 위해 \"refine\"을 체인 유형으로 사용한다면, RetrievalQA 체인은 RefineDocumentsChain을 호출하며, 이는 LLM 체인에 대한 네 번의 연속적인 호출을 수반합니다. 이 네 번의 호출 각각은 언어 모델로 전송되기 전에 프롬프트를 포함합니다. 이 네 번의 호출에는 이전에 정의된 프롬프트 템플릿에 따른 시스템 메시지가 포함됩니다. 시스템 메시지에는 문맥 정보, 검색한 문서 중 하나, 사용자 질문 및 답변이 포함됩니다. 우리는 다음 언어 모델을 호출합니다. 다음 언어 모델에 보내는 최종 프롬프트는 이전 응답과 새 데이터를 결합하고 추가된 문맥을 포함하여 향상된/정제된 응답을 요청하는 시퀀스입니다. 이 작업은 이전 응답을 새 데이터와 결합하여 정보를 순차적으로 결합함으로써 정보의 지속 전달을 더 많이 유도하는 정제 체인에서 더 나은 답변을 얻을 수 있습니다. 이 과정은 네 번 실행되며 최종 답변에 도달하기 전 모든 문서를 대상으로 실행됩니다. Refine 체인에서는 정보를 순차적으로 결합하여 정보의 지속적 전달이 더 많아져 MapReduce 체인보다 더 나은 답변을 얻을 수 있습니다.\n\n## RetrievalQA 한계\n\nRetrievalQA 체인의 가장 큰 단점 중 하나는 QA 체인이 대화 내력을 보존하지 못하는 것입니다. 이를 확인할 수 있습니다:\n\n```js\n# QA 체인 생성\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectordb.as_retriever()\n)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 체인에 질문하겠습니다.\n\n```js\nquestion = \"확률은 수업 주제입니까?\"\nresult = qa_chain({\"query\": question})\nresult[\"result\"]\n```\n\n이제 두 번째 질문을 체인에 하겠습니다.\n\n```js\nquestion = \"왜 해당 선수과목이 필요한가요?\"\nresult = qa_chain({\"query\": question})\nresult[\"result\"]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 답변과 관련이 없는 체인으로부터 답변을 받을 수 있었습니다. 기본적으로 RetrievalQA 체인은 상태 개념을 가지고 있지 않습니다. 이전 질문이나 이전 답변이 무엇이었는지 기억하지 않습니다. 이전 질문이나 이전 답변을 기억하게 하려면 메모리 개념을 도입해야 합니다. 이전 질문이나 이전 답변을 기억하는 능력은 챗봇의 경우 필요합니다. 챗봇에 후속 질문을 할 수 있거나 이전 답변에 대해 설명을 요청할 수 있기 때문입니다.\n\nLangChain을 사용하여 다양한 문서에서 데이터를로드하는 방법에 대해 토론했습니다. 또한 문서를 청크로 분할하는 방법을 배웠습니다. 그 후에 이러한 청크에 대한 임베딩을 생성하고 벡터 저장소에 저장했습니다. 이 벡터 저장소를 사용하여 의미 검색을 수행했습니다. 의미 검색은 특정 엣지 케이스에서 실패합니다. 그런 다음, 엣지 케이스를 극복하기 위한 다양한 검색 알고리즘을 논의하는 검색을 다루었습니다. 우리는 검색을 LLM과 결합하여 질문 응답에서 사용했는데, 여기서 우리는 검색된 문서와 사용자 질문을 가져와 LLM에 전달하여 우리가 한 질문에 대한 답변을 생성했습니다. 질문 응답의 대화식 측면에 대해서는 논의하지 않았으며, 나중에 데이터 위에 종단간 챗봇을 생성함으로써 그에 대해 나중에 논의할 것입니다.","ogImage":{"url":"/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png"},"coverImage":"/assets/img/2024-05-18-UsinglangchainforQuestionAnsweringonOwnData_0.png","tag":["Tech"],"readingTime":29},{"title":"프롬프트 엔지니어링 기술 분류 및 프롬프트 튜닝","description":"","date":"2024-05-18 20:29","slug":"2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning","content":"\n\n\u003cimg src=\"/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_0.png\" /\u003e\n\n현재 계속 발전 중인 prompt engineering 분야는 기술의 명확한 분류가 부족합니다. 다양한 논문과 웹사이트를 살펴보면, 이러한 기술들이 서로 다르며 구조가 부족함을 알 수 있습니다. 이러한 혼란 때문에 실무자들은 종종 가장 간단한 방법에 고수합니다. 이 글에서는 prompt engineering 기술의 개요와 명확한 분류를 제안하여 여러분이 이 개념을 파악하고 어플리케이션에서 효과적으로 사용할 수 있도록 도와드리겠습니다. 또한, prompt tuning 및 평가를 포함한 Data Science 프로세스로써 prompt engineering을 수행하는 방법에 대해 이야기하겠습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_1.png\" /\u003e\n\n많은 연구 노력에도 불구하고, 대형 언어 모델은 여전히 일부 문제에 직면하고 있습니다. 그들의 주요 함정은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 자료 인용. LLMs는 외부 자원을 참조한 것처럼 신뢰성 있는 콘텐츠를 생성할 수 있지만, 인터넷에 접속할 수 없기 때문에 자료를 인용할 수 없다는 것을 기억하는 것이 중요합니다.\n- 편향. LLMs는 답변에서 편견을 드러낼 수 있으며 종종 편견적이거나 선입견 있는 콘텐츠를 생성할 수 있습니다.\n- 환각. LLMs는 가끔 “환각”을 일으키거나 알 수 없는 질문에 대해 거짓 정보를 생성할 수 있습니다.\n- 수학 및 상식 문제. 고급 기술을 갖추었음에도 불구하고, LLMs는 가끔 심지어 가장 간단한 수학적이거나 상식적인 문제를 해결하는 데 어려움을 겪을 수 있습니다.\n- 프롬프트 해킹. 사용자에 의해 조작되거나 “해킹” 당할 수 있어 개발자의 지시를 무시하고 특정한 콘텐츠를 생성할 수 있습니다.\n\n대부분의 프롬프트 엔지니어링 기술은 환각과 수학 및 상식적인 작업 해결 두 가지 문제에 대응합니다. 프롬프트 해킹을 완화하기 위한 특정 기술들이 있지만, 이는 별도로 논의할 주제입니다.\n\n# 공통 규칙\n\n구체적인 기술에 대해 논의하기 전에, 명확하고 구체적인 지침을 작성하는 데 도움이 되는 프롬프트의 공통 규칙에 대해 이야기해 봅시다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제가 알기로는 표를 마크다운 형식으로 변경해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 단일 프롬프트 기술은 하나의 프롬프트에 대한 응답을 최적화하는 것을 목표로 합니다.\n- 다음은 몇 가지 프롬프트를 결합하는 기술입니다. 이들은 공통적으로 작업을 해결하기 위해 모델(또는 모델)을 몇 번 쿼리하는 개념에 있습니다.\n- 마지막으로, 다양한 대형 언어 모델과 외부 도구를 결합하는 방법론 그룹이 있습니다.\n\n# 단일 프롬프트 기술\n\n![프롬프트 기술](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_3.png)\n\n어떤 기술들이 단일 프롬프트에서 작업을 해결하기 위해 사용되나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 제로 샷,\n- 퓨 샷,\n- 체인 오브 소트,\n- 프로그램 지원 언어.\n\n하나씩 공부해 봅시다.\n\n제로 샷 프롬프팅\n\n자연어 지시를 사용하는 가장 간단한 기술입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_4.png)\n\nFew-Shot Prompting\n\nLLMs are extremely good at one-shot learning but they still may fail at complicated tasks. The idea of few-shot learning is to demonstrate to the model similar tasks with correct answers (Brown et al. (2020)).\n\n![Image 2](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_5.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMin 등(2022) 논문에서는 데모의 레이블이 잘못되었을 때 분류 및 다중 선택 과제의 성능에 거의 영향을 미치지 않음을 보여줍니다. 대신, 데모가 레이블 공간의 몇 가지 예, 입력 테스트의 분포 및 순서의 전반적인 형식을 제공하는 것이 중요합니다.\n\nChain of Thought Prompting (CoT)\n\nChain-of-Thought 프롬프팅은 중간 추론 단계를 통해 복잡한 추론 능력을 활성화합니다. 이 기술은 모델이 각 단계를 반복하고 추론할 수 있도록 하는 것을 목표로 합니다.\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCoT은 제로샷 또는 퓨샷 러닝과 함께 사용됩니다. 제로샷 CoT의 아이디어는 모델이 해결책에 이르기 위해 단계별로 생각하도록 제안하는 것입니다. 이 접근법의 저자들(Kojima et al. (2022))은 산술, 기호 및 기타 논리 추론 작업에서 제로샷 LLM의 성능을 크게 능가하는 것을 입증했습니다.\n\n퓨샷 CoT를 선택하는 경우, 설명이 포함된 다양한 예제를 갖도록 해야 합니다(Wei et al. (2022)). 이 접근법의 경험적인 이득은 산술, 상식 및 기호적 추론 작업에서 두드러집니다.\n\n프로그램 지원 언어 모델 (PAL)\n\n프로그램을 지원하는 언어 모델은 코드로 자연어 설명을 확장하는 것으로 Chain-of-Thought 프롬프팅을 확장하는 접근법입니다(Gao et al. (2022)).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_7.png)\n\nThe technique can be implemented using LangChain PALChain class.\n\n# Multiple Prompt Techniques\n\nThe next group of prompts is based on different strategies of combining prompts of one or a few of models:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 투표. 정확한 답변을 얻기 위해 투표를 적용하는 아이디어입니다. 기술: 자기일관성.\n- 분할정복. 이 그룹의 프롬프트는 복잡한 작업을 몇 가지 프롬프트로 분할하는 것에 기반합니다. 기술: 방향성 자극, 지식 생성, 프롬프트 연결, 테이블 연결 및 Least-to-Most 프롬프팅.\n- 자가평가. 이 접근 방식은 출력물이 지시 사항을 충족하는지 확인하는 단계를 프레임워크에 포함하는 것을 제안합니다. 기술: 반성, 사고의 나무.\n\n자기일관성(SC)\n\n자기일관성은 \"복잡한 추론 문제는 일반적으로 해당 독특한 올바른 답변으로 이어지는 여러 가지 다른 사고 방식을 허용한다\"는 직관에 기반합니다 (Wang et al. (2022)).\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_8.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n같은 Chain-of-Thought 프롬프트를 여러 번 수행하여 다양한 추론 경로를 생성한 후 투표를 통해 가장 일관된 답변을 선택합니다.\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_9.png)\n\nWang et al. (2022)에서는 산술 및 상식 작업에 대한 자기 일관성 적용의 이득이 일반적인 벤치마크에서 4%~18%였습니다.\n\n## Directional Stimulus Prompting (DSP)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 컨셉은 \"Divide and Conquer\"입니다. DSP에서는 두 단계가 있습니다: 자극(예: 키워드)을 생성하고 그들을 사용하여 응답의 품질을 개선합니다.\n\n리 등은 2023년에 요약, 대화 응답 생성 및 연상 추론 작업을 위해 방향성 자극 프롬프팅을 제안했습니다. 이는 두 개의 모델로 구성됩니다:\n\n- 조정 가능한 소형 정책 LM은 자극(힌트)을 생성하기 위해 훈련됩니다.\n- 블랙 박스로 고정된 대형 LM은 질문과 이전 단계에서의 자극에 기반하여 요약을 생성하는 데 사용됩니다.\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_10.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n정책 모델은 지도 학습을 통해 라벨이 지정된 데이터를 사용하여 최적화하고, LLM의 출력을 기반으로 오프라인 또는 온라인 보상을 받아 강화 학습을 통해 미세 조정할 수 있습니다. \n\n생성된 지식 프롬프팅 (GK)\n\n\"분할 정복\" 개념 하에 다음 프롬프팅 기술인 생성된 지식은 Liu 등(2022)에서 제안되었습니다. 이 아이디어는 별도의 프롬프트를 사용하여 먼저 지식을 생성한 다음 더 나은 응답을 얻기 위해 사용하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성된 지식 유도에는 두 단계가 포함됩니다:\n\n- 지식 생성: 몇 가지 샷의 데모를 사용하여 언어 모델에서 관련 질문에 대한 지식을 생성합니다.\n- 지식 통합: 각 지식 명세를 사용하여 두 번째 언어 모델을 활용하여 예측을 수행한 다음, 가장 높은 확신을 갖는 예측을 선택합니다.\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_12.png)\n\n본 방법은 지식 통합에 대한 작업 특정 감독이나 구조화된 지식베이스에 대한 접근이 필요하지 않지만, 대규모, 최신 기술 모델의 성능을 개선시킵니다. 공감 추리 작업에서 더 나은 성과를 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 1](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_13.png)\n\nPrompt Chaining\n\nPrompt chaining is a simple yet powerful technique in which you should split your task into subproblems and prompt the model with them one by one.\n\n![Image 2](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_14.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프롬프트 체이닝은 상세한 프롬프트로 인해 LLM이 처리하기 어려워하는 복잡한 작업을 수행하는 데 유용합니다. 또한 LLM 애플리케이션의 투명성을 높이고, 제어 가능성과 신뢰성을 증가시킬 수 있습니다.\n\n![image](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_15.png)\n\n최소에서 최대 프롬핑\n\n최소에서 최대 프롬핑은 모델이 작업을 하위 문제로 나누는 방법을 결정해야 하는 단계를 추가하여 조금 더 진보합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Zhou et al. (2022) - Experimental Results](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_16.png)\n\nZhou et al. (2022)의 실험 결과에 따르면, least-to-most prompting은 symbol manipulation, compositional generalization 및 math reasoning과 관련된 작업에서 잘 수행되고 있습니다.\n\n![Chain-of-Table Prompting](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_17.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최근 연구에서 (Wang et al. (2024))는 새로운 접근 방식을 제안했습니다. 여기서는 표 형식 데이터가 중간 생각의 대리인으로 추론 체인에서 명시적으로 사용됩니다.\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_18.png)\n\n이 알고리즘에는 다음과 같은 두 단계의 사이클이 포함되어 있습니다:\n\n- LLM이 입력 쿼리 및 이전 작업 이력 (작업 체인)을 기반으로 작업 풀에서 다음 운영을 샘플링하는 동적 계획,\n- 인수 생성은 이전 단계에서 선택된 작업에 대해 인수를 생성하는 과정을 포함합니다 (예: 새로운 열 이름). 이를 위해 LLM을 사용하고 프로그래밍 언어를 적용하여 작업을 실행하고 해당 중간 테이블을 생성합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_19.png)\n\nThe next two approaches implement the concept of Self-Check — there’s a step in the framework that checks the solution. Example of Cgain-of-Table implementation can be found by the link.\n\nTree of Thoughts (ToT)\n\nTree of Thoughts generalizes over the Chain-of-Thought approach allowing the model to explore multiple reasoning steps and self-evaluate choices.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nToT 기법을 실행하려면 네 가지 질문에 대해 결정해야 합니다:\n\n- 중간 과정을 사고 단계로 어떻게 분해할 것인가,\n- 각 상태에서 잠재적인 생각을 어떻게 생성할 것인가,\n- 상태를 어떻게 휴리스틱하게 평가할 것인가(상태 평가자 프롬프트 사용),\n- 어떤 검색 알고리즘을 사용할 것인가 (Yao et el. (2023))\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_20.png)\n\n입력 프롬프트는 문제를 해결하기 위한 중간 단계의 설명과 샘플된 생각 또는 그들의 생성 방법에 대한 지침이 포함되어야 합니다. 상태 평가자 프롬프트는 다음 단계에 대한 선택할 프롬프트에 대한 지침을 제공해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_21.png)\n\nYao et al. (2023)의 실험에서는 ToT가 복잡한 계획이나 탐색을 필요로 하는 작업에 대해 성공을 보여주었습니다. LangChain에는 langchain_experimental.tot.base.ToTChain 클래스에 Tree-of-Thought 기술을 구현하고 있습니다.\n\n반성\n\n반성은 언어 에이전트를 언어적 피드백을 통해 강화하는 프레임워크입니다. 반성 에이전트는 작업 피드백 신호에 대해 언어적으로 반성한 후, 자신의 반성적 텍스트를 에피소딕 메모리 버퍼에 유지하여 후속 시행에서 더 나은 의사 결정을 유도합니다(Shinn et al. (2023)).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_22.png)\n\nReflexion 프레임워크는 세 가지 독립적인 모델로 구성되어 있습니다:\n\n- Actor: 상태 관측에 따라 텍스트와 액션을 생성하는 LLM 모델 (CoT와 ReAct 사용),\n- Evaluator: Actor가 생성한 출력을 점수로 평가하는 LLM 모델,\n- Self-Reflection: Actor의 자기 개선을 돕기 위해 언어적인 강화 신호를 생성하는 LLM 모델입니다.\n\n![이미지](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_23.png)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nReflexion은 순차적인 의사 결정, 코딩, 언어 추론이 필요한 작업에서 잘 수행됩니다.\n\n링크를 통해 구현 내용을 확인해보세요.\n\n# 외부 도구를 활용한 LLM 프레임워크\n\n이 섹션에서 두 가지 접근 방식, 즉 Retrieval Augmented Generation과 ReAct를 다룰 예정입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 검색 증진 생성 (RAG)\n\nRAG는 정보 검색 구성 요소와 텍스트 생성 모델을 결합합니다.\n\n- 검색. 검색 단계에서 시스템은 일반적으로 벡터 검색을 사용하여 질문에 답할 수 있는 관련 문서를 검색합니다.\n- 생성. 다음으로, 관련 문서는 초기 질문과 함께 LLM에 컨텍스트로 전달됩니다 (Lewis et al. (2021)).\n\n대부분의 경우 RAG-시퀀스 방식이 사용됩니다. 이는 k개의 문서를 검색하여 사용자 쿼리에 답변하는 모든 출력 토큰을 생성하는 것을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n마크다운 형식에서 테이블 태그를 변경하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nReAct\n\n요 등(2022)은 ReAct라는 프레임워크를 소개했습니다. 이 프레임워크에서 LLMs는 추론 트레이스와 과제별 동작을 교차적으로 생성하는 데 사용됩니다. 추론 트레이스는 모델이 행동 계획을 유도, 추적, 업데이트하고 예외를 처리하는 데 도움을 주며, 동작은 외부 소스(예: 지식 베이스 또는 환경)와 상호 작용하고 추가 정보를 수집하는 데 도움을 줍니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_26.png\" /\u003e\n\nReAct 프레임워크는 사용 가능한 도구(예: 검색 엔진, 계산기, SQL 에이전트) 중 하나를 선택하고 적용하여 결과를 분석하여 다음 동작을 결정할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_27.png\" /\u003e\n\nReAct은 단순한 Wikipedia API와 상호 작용하여, 사고 체인 추론에서 환각 및 오류 전파의 일반적인 문제를 극복하며, 이러한 추론 흔적이 없는 기준선보다 해석 가능한 인간과 유사한 작업 해결 궤적을 생성합니다 (Yao et al. (2022)).\n\nLangchain 도구를 사용한 ReAct 구현 예시를 확인해보세요.\n\n# Prompt Tuning and Evaluation\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프롬프트 엔지니어링 기술의 선택은 LLM의 응용 및 사용 가능한 리소스에 크게 의존합니다. 프롬프트를 실험해 본 적이 있다면, 대형 언어 모델은 인간이 생성한 프롬프트의 작은 변화에 민감하며 최적이 아니거나 주관적일 수 있다는 것을 알고 있을 것입니다.\n\n어떤 프롬프팅 기술을 선택하든, 애플리케이션을 개발 중이라면 프롬프트 엔지니어링을 데이터 과학 프로세스로 생각하는 것이 매우 중요합니다. 즉, 테스트 세트를 생성하고 메트릭을 선택하며 프롬프트를 조정하고 그 영향을 테스트 세트에 대해 평가하는 것을 의미합니다.\n\n![image](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_28.png)\n\n프롬프트를 테스트하기 위한 메트릭은 응용 프로그램에 크게 의존할 것이지만, 여기에는 몇 가지 가이드라인이 있습니다(Data Science Summit 2023):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_29.png)\n\n- Faithfulness and relevancy:\n  - how factually accurate the generated answer is,\n  - how relevant the generated answer is to the question.\n\n2. Retrieval — for RAG and ReAct pipelines mainly but can be applied to generated knowledge and directional stimulus prompting:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정밀도 — 검색된 문서가 얼마나 관련 있는지,\n- 재현율 — 모든 관련 문서가 검색되었는지.\n\n3. 내부적 사고:\n\n- 에이전트 및 도구 선택 정확도 — ReAct의 경우,\n- 도구 인수 추출 — 정확한 인수가 문맥에서 검색되고 올바르게 변환되었는지 — ReAct의 경우,\n- 장기 대화에서 사실 기억 — ReAct의 경우,\n- 올바른 논리적 단계 — ReAct 및 Chain-of-Thought 프롬프팅의 경우.\n\n4. 비기능적:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 답변의 스타일과 어조,\n- 편향성의 부재,\n- 준수 및 안전 점검,\n- 빠른 삽입 테스트.\n\n사용 사례에 따라 측정 항목을 선택하고 테스트 셋에서 당신의 프롬프트 변경이 테스트 응답의 품질을 저하시키지 않도록 영향을 추적하세요.\n\n# 요약\n\n모든 기술을 다 다루지는 못했다고 주장하지는 않습니다. 기술이 너무 많아 곧 누군가가 전체 교과서를 출판할 것이기 때문입니다. 그러나 만약 당신이 이것을 읽어오셨다면, 모든 기술의 개념들이 상당히 흔하고 직관적임을 알게 되었을 것입니다. 나는 좋은 프롬프트를 작성하는 모든 규칙을 작은 목록으로 요약할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞서까지 함께 해주셔서 감사합니다! 앞으로도 즐겁게 프롬프팅하세요!","ogImage":{"url":"/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_0.png"},"coverImage":"/assets/img/2024-05-18-PromptEngineeringClassificationofTechniquesandPromptTuning_0.png","tag":["Tech"],"readingTime":12},{"title":"라벨을 놓치지 마세요 계층적 범주에 대한 대체 인코딩 방법","description":"","date":"2024-05-18 20:24","slug":"2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals","content":"\n\n\u003cimg src=\"/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_0.png\" /\u003e\n\n데이터 과학자로 일하면서 다양한 레이블을 많이 볼 수 있어요. 우편 번호 레이블, 성별 레이블, 의료 진단 레이블, 직책 레이블, 주식 코드 레이블 등 다양한 종류의 레이블이 데이터에 포함됩니다. 레이블은 간단한 것(S, M, L 같은 셔츠 사이즈)일 수도 있고 복잡한 것(7만 가지가 넘는 의료 질병을 인코딩하는 국제질병분류체계처럼)일 수도 있어요.\n\n데이터에 포함된 레이블을 범주형 특성이라고 해요. \"많은\" 가능한 값을 가지는 범주형 특성을 고차원 범주형 특성이라고 해요. 고차원 범주형 특성은 머신러닝 모델에서 사용할 때 어려움을 겪게 될 수 있어요. 높은 차원 수는 직접 사용하기 불가능하거나 비실용적이기 때문이에요(\"차원의 저주\"라고 합니다). 그래서 이러한 특성을 간단하게 만드는 다양한 인코딩 방법이 사용되어요.\n\n낮은 빈도 또는 보이지 않는 코드도 고차원 범주형 특성에 도전을 제공할 수 있어요. 예를 들어 어떤 우편번호는 인구가 드물게 분포되어있고, 다른 우편번호는 수백만 명의 사람을 포함하고 있어요; 우리는 어떤 것의 인코딩에 더 확신을 갖게 될 수 있어요. 또한 의료 진단 코드와 같은 일반적인 코드 집합은 정기적으로 업데이트되어, 교육에 사용할 수 없는 보이지 않는 값들을 발생시킬 수 있어요. 보이지 않는 값과 낮은 빈도의 코드는 성장 중인 비즈니스나 제품 또는 교육 데이터가 제한되어있을 때 중요할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인코딩은 반응 정보를 유지하면서 오버피팅을 방지하는 방식으로 저빈도 및 미처 보지 못한 값들을 고려해야 합니다. 트리 기반 모델에서 사용되는 표준 타깃 인코딩에서는 이를 샘플 평균과 전체 평균을 카운트에 비례하여 혼합함으로써 수행합니다.\n\n고차원 범주형 변수들은 그룹으로 구성할 수 있습니다. 예를 들어 우편번호는 군, 주, 또는 지역으로 (대략) 집계될 수 있습니다.\n\n그룹 정보는 낮은 볼륨이거나 보지 못한 코드들을 예측하는 데 도움이 될 수 있습니다. 이 정보를 타깃 인코딩에 통합하여 상위 그룹 수준의 평균을 계층적으로 혼합하는 방식으로 사용할 수 있습니다. 저는 제 마지막 블로그 포스트에서 이를 시도해 봤고, 보지 못한 코드에 대해 성능이 향상된 것을 보았지만, 일부 그룹화에서는 오버피팅이 발생했습니다.\n\n그 이후로, 계층적 범주형 변수에 대한 대안 처리 방법에 대해 고민하고 있습니다. 코드 시스템에 대한 가능한 많은 정보를 포함하는 기능을 설계하는 것이 더 나은지, 아니면 모델이 작업을 수행하게 하는 방법을 찾는 것이 더 나은지 궁금해지기 시작했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서는 XGBoost 모델에서 계층적 블렌딩 대안을 테스트합니다. 계층 수준을 분리해서 사용하는 것이 단일 특성으로 블렌딩하는 것보다 성능 개선을 보입니다. 낮은 볼륨 코드를 null로 설정하고 불확실성을 나타내는 다른 값으로 설정하는 간단한 임계치 설정은 오버피팅에 특히 강합니다. 표준 코드 계층에 대해서는 계층 블렌딩이 약간 더 우세하지만 일부 코드 그룹에 대해서는 심하게 오버피팅됩니다.\n\nShapley 테스트 결과, 다중 특성 인코딩을 사용하면 모델이 범주적 계층의 상위 수준 정보와 다른 예측 변수 정보를 활용할 수 있게 됩니다.\n\n주요 주의점은 모델이 보지 못한 경우에 대해 일반화하기 위해 낮은 볼륨 사례에서 훈련되어야 한다는 것입니다. 제가 검토한 대부분의 타겟 인코딩 변형에는 이 상황이 적용되지만 계층적 인코딩을 제외하고는 예외가 있습니다. 또한, 이 게시물에서 논의된 모든 인코딩 방법은 낮은 볼륨 또는 누락된 코드가 보이지 않는 코드와 다를 경우 편향 리스크를 가지고 있을 겁니다.\n\n그러므로, 훈련 데이터에 null(또는 다른) 값의 무작위 삽입이 모델이 보이지 않는 코드에 대해 일반화하는 법을 가르칠 수 있는지 궁금합니다. 뉴럴 네트워크의 엔티티 임베딩에 대해 비슷한 것을 테스트하고, 보이지 않는 코드에 대해 상당한 성능 향상이 있었습니다. 아마도 XGBoost에 도움이 될 수 있는 유사한 전략이 있을거라 생각됩니다. 훈련 반복/에폭에서 무작위성을 섞어넣을 수 있다면 가장 좋을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 방법들\n\n제가 사용하는 XGBoost 이진 분류 모델은 미국 소기업 행정청(SBA) 대출 데이터 세트에서 기관들의 대출 연체를 예측하는 데 사용됩니다. 이 데이터는 공개 데이터셋이며(CC BY 4.0) 방법들은 주로 이전에 설명되었습니다. 모든 코드는 GitHub에서 확인하실 수 있습니다.\n\n저는 NAICS(산업 특성)를 인코딩합니다. NAICS 코드는 공식적으로 5단계 계층 구조를 가지며, 다양한 정밀도의 Bucket으로 기관의 유형을 그룹화합니다. 예시는 다음과 같습니다:\n\n![Table 1](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거 방법과의 중요한 차이점 중 하나는 가끔 블렌딩 중점의 다른 값(100)을 사용한다는 것입니다. 이것은 중점에 민감한 새로운 방법과 비교하기 위해 성능을 비교하는 데 도움이 됩니다. 결과는 [3]의 것과 매우 유사하지만 완전히 동일하지는 않습니다.\n\n# 대안 인코딩 탐구\n\n다음은 내가 시도할 인코딩 변형들입니다:\n\n1. 타겟 인코딩 (NAICS만): NAICS 특성의 표준 타겟 인코딩(특성 1개)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n2. Hierarchical Blending: 관층 층위의 그룹 평균을 관측된 비율에 혼합하여 응답을 더 잘 추정하는 대상 인코딩 (1 특성)\n\n3. 대상 인코딩 (모두): NAICS 계층구조의 모든 수준에 대한 표준 대상 인코딩 (5 특성)\n\n4. 대상+카운트 인코딩: 4와 동일하지만 추가 카운트 인코딩 특성 포함 (10 특성)\n\n5. 대상-임계 인코딩: 표준 대상 인코딩과 유사하지만, 낮은 체적/보이지 않는 값이 대상(또는 기타) 평균 방향으로 축소되는 대신, 절단점 아래의 값만 null로 설정합니다. (5 특성)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 모든 경우에 있어서, 가장 낮은 수준의 NAICS 특성의 인코딩은 매우 유사합니다. 실제로 1, 3 및 4에 대해 동일합니다. 2 및 4에 대해서는 변형이 낮은 볼륨이나 보이지 않는 코드에서만 발생합니다. 메소드 3-5는 가장 낮은 수준의 코드를 인코딩하는 것을 넘어 추가적인 기능을 포함합니다.\n\n타겟 인코딩 (NAICS만): 표준 타겟 인코딩은 범주형 특성을 해당 범주에 대한 평균 응답(대출 채무 불이행률)으로 대체합니다. 평균은 누출을 피하기 위해 학습 데이터에서 계산됩니다.\n\n낮은 볼륨의 코드에 대해 평균 추정치는 신뢰할 만하지 않으며, 오버피팅 위험이 있습니다. 따라서 타겟 인코딩은 일반적으로 매우 낮은 볼륨(또는 보이지 않는) 코드에 대해 타겟 평균과 블렌딩하는 것을 포함하여 낮은 볼륨 코드에 대해 타겟 평균과 유사한 값을 얻는 동시에 높은 볼륨 코드는 거의 실제 응답으로 매핑됩니다. \"낮은 볼륨\"의 의미는 타겟 비율에 따라 다르며, 블렌딩은 매개변수화됩니다. 여기서는 블렌딩 중점과 폭을 변형하는 시그모이드함수를 사용합니다(일부 테스트에서 중점을 변형합니다).\n\n계층적 블렌딩: 계층적 블렌딩은 타겟 인코딩의 변형으로, 높은 수준의 NAICS 그룹 평균을 사용하여 낮은 볼륨이나 보이지 않는 코드에 대한 평균 응답을 더 잘 추정합니다. 응답은 계층의 모든 가능한 수준의 평균과 함께 사용되며, 각 수준을 가중하는 데에 동일한 시그모이드 함수를 사용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nTarget Encoding (All): 이 방법은 NAICS만 사용하는 대상 인코딩(Target Encoding)과 동일하지만 더 높은 수준의 코드를 대상으로 인코딩합니다. 예를 들어 NAICS 부문이나 산업 그룹을 대상으로 인코딩하여 총 5개의 기능을 생성합니다.\n\n비선형 모델이 필요할 때 더 높은 수준의 기능을 \"자동으로\" 포함시킬 수 있다는 것은 어느 정도 이해할 만한 일입니다. 반면에 대부분의 코드에 대해서는 높은 수준의 그룹 기능이 추가 정보를 제공하지 않을 수 있습니다.\n\nTarget+Count Encoding: XGBoost가 카운트 정보를 활용하여 대상 인코딩된 기능이 저수량 또는 보이지 않는 코드에 대해 신뢰할 수 없다는 것을 추론할 수 있을지 궁금했습니다. 따라서 제가 시도하는 \"Target+Count\" 인코딩은 Target Encoding (All)에 카운트 인코딩 기능을 추가하는 방식으로 총 10개의 기능을 생성합니다. 일정 수준 이상에 해당하는 카운트 인코딩 값을 임계값으로 설정하며, 이는 응답 비율의 95% 이상이 샘플 평균에 의해 혼합되었다고 볼 수 있는 지점에 해당합니다.\n\nTarget-Thresh Encoding: 비선형 모델이 카운트 정보를 사용하여 특징에 대한 응답을 수정할 수 있다면, 누락된 값이나 특정 값에서 유사한 정보를 얻을 수 있을지도 모릅니다. 저수량 코드의 평균을 알 수 없다고 모델에게 알려주고 스스로 결정하게 하는 것이 어떨까요? Target-Thresh 인코딩은 높은 수준의 부피 코드에 대해 대상 인코딩과 동일하지만 저수량 또는 보이지 않는 코드에 대한 값은 null로 설정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 그렇다면, 그들의 성능은 어떤가요?\n\n일반적인 무작위 학습/검증/테스트 분할을 수행하며, 또한 NAICS 코드의 10%를 샘플로 설정하여 보지 못한 값들에 대한 성능을 평가합니다 (자세한 내용은 [3] 참조). 저는 정밀도-재현율 곡선 아래 면적(PR-AUC) 메트릭을 보고하는데, 이는 부도 감지를 강조하기 때문에 높은 값일수록 더 좋습니다.\n\n![image](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_2.png)\n\n테스트 데이터\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFigure 1의 왼쪽 패널은 테스트 데이터셋에서의 성능을 보여줍니다. 표준 타겟 인코딩과 계층적 블렌딩은 대부분의 중간 지점에서 매우 유사합니다. 그러나 타겟 인코딩(모두), 타겟+카운트, 타겟-임계치와 같은 세 가지 다중 변수 인코딩 방법은 약간의 성능 향상을 보여줍니다. 이 세 곡선은 서로 매우 유사합니다.\n\n모든 인코딩 방법에서 성능은 매우 높은 중간 지점에서 악화되는데, 이는 평균 응답에 대한 정보가 손실되기 때문으로 예상됩니다 (전체 타겟 비율은 약 20%임에 유의하십시오). 악화가 가장 심한 것은 코드 계층 구조에서 정보를 포함하지 않는 표준 타겟 인코딩입니다.\n\n만약 Figure 1B의 왼쪽 패널만 보면, 다중 필드 인코딩 중 하나가 가장 좋다고 결론을 내릴 수 있을 것입니다. 그렇다면 보이지 않는 코드들은 어떨까요?\n\n홀드아웃 데이터\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFigure 1의 오른쪽 패널은 보이지 않는 NAICS 코드에 대한 성능을 보여줍니다. 보편적으로, 왼쪽 패널보다 값이 낮게 나타날 수 있습니다. 이는 훈련 시에 사용되지 않은 코드의 평균 응답이 없기 때문입니다.\n\nNAICS만 사용하는 타겟 인코딩은 NAICS 계층을 완전히 무시하는 유일한 방법이지만, 성능이 가장 나쁩니다. 현재 계층적 블렌딩이 가장 강력해 보이며, 블렌딩 중간점에 민감하지 않은 것으로 나타납니다. 특성 다중 방식은 미묘한 성능 향상이 있어 보입니다. 나중에 이에 대해 더 자세히 이야기하겠습니다.\n\nFigure 1의 오른쪽 패널 결과를 살펴보면, 보이지 않는 코드가 중요할 때 계층적 블렌딩이 선호될 것으로 결론 낼 수 있을 것 같습니다. 그러나 이전 글\\[3\\]에서 일부 코드 그룹에 대해 계층적 블렌딩이 과적합 문제에 직면했다는 것을 알았습니다. 그러므로 더 많은 실험을 해봐야 할 것 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다른 코드 계층에 대해 어떻게 생각하세요?\n\nNAICS에는 널리 사용되는 표준 계층이 있습니다. 그러나 다른 체계를 갖는 다른 코드는 어떨까요? 레벨 수가 다르거나 세분화가 다를 수도 있습니다. 모든 계층 테스트에서는 중간점/임계값을 100으로 고정합니다.\n\nNAICS 표준 계층 변형\n\n모델이 계층 구조 변형에 어떻게 반응하는지 감을 잡기 위해 동일한 인코딩 방법을 사용하지만 NAICS 계층 구조의 여러 지점에서 시작합니다. 모든 5단계를 사용하는 대신 기본 NAICS를 사용한 다음, 예를 들어 3단계부터 그룹화합니다. 도표 2는 표준 NAICS 계층에서 시작점을 변경했을 때의 성능을 보여줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Random Test Dataset](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_4.png)\n\n랜덤 테스트 데이터셋의 경우(왼쪽 패널), Figure 2 결과는 Figure 1과 매우 유사합니다. 계층적 블렌딩은 표준 타겟 인코딩과 매우 유사하지만, 멀티 필드 인코딩 방법은 성능을 더 향상시킵니다. 이 패턴은 인코딩이 이루어지는 레벨에 관계없이 발생합니다.\n\nFigure 2의 오른쪽 패널은 보이지 않는 코드에 대한 성능이 계층이 변경됨에 따라 강하게 변하는 것을 보여줍니다. 계층적 블렌딩의 경우, 블렌딩에 가장 높은 수준의 그룹(섹터)만 사용하는 것은 실제로 간단한 타겟 인코딩보다 성능이 더 나빠집니다(이 결과는 이전에도 보였음 [3]).\n\nDGI 피처 기반 그룹화\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기존 NAICS 계층을 수정하는 것 외에도 완전히 다른 코드 그룹을 시도해보겠어요. 제 마지막 글 [3]에서 Deep Graph Infomax (DGI) 및 클러스터링을 시도하여 예측 필드를 기반으로 그룹을 생성했어요. DGI 그룹은 모델 응답과 상관관계가 있지만 예측 필드 이상의 추가 정보는 담고 있지 않아요.\n\n이전에 DGI 그룹은 계층적 블렌딩에 대한 과적합을 초래했는데, 이 방법은 다른 예측자들과 중복되는 경우에 위험할 수 있다는 것을 시사합니다 [3]. 다른 방법들은 DGI 그룹에 어떻게 반응할까요?\n\n![그림 3](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_5.png)\n\n그림 3은 DGI 그룹이 과적합으로 성능 저하를 일으키는 경우가 많다는 것을 보여줍니다. 이는 타겟 인코딩 (모든) 및 타겟+카운트 인코딩에 대해 가장 심하게 나타납니다. 계층 블렌딩은 일부 계층 수준에 대해 과적합되지만, 다른 것에 대해서는 과적합이 발생하지 않아요. DGI 그룹에 대해서는 Target-thresh가 과적합에 저항하는 것처럼 보여요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n완전히 무작위 그룹\n\nDGI 그룹화로 과적합 현상이 발생할 때, 무작위 그룹을 사용해 볼까 하는 생각이 들었습니다.\n\n무작위 그룹은 코드 구조가 문제와 관련이 없는 경우를 시뮬레이션합니다. 코드 시스템은 종종 정부나 산업 전문가들에 의해 그들 자신의 목적을 위해 유지보수되는데, 이는 귀하의 관심 대상과 관련이 없을 수 있습니다. 예를 들어, 대출 채무 모델에서 산업을 알파벳순으로 구성한 계층구조를 사용했다고 상상해 보세요.\n\n![이미지](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_6.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**표 4**는 완전히 무작위로 생성된 그룹의 결과를 보여줍니다. 대부분의 인코딩 방법들은 계층을 전혀 고려하지 않으며, 이는 안심스럽습니다. 그러나 계층적 블렌딩은 상당히 오버피팅됩니다!\n\n**특성 중요도**\n\nShapley 테스트는 다른 인코딩 방법들이 예측에 어떤 영향을 미치는지에 대한 상세 정보를 제공할 수 있습니다. Figure 5에서는 개별 SHAP 값들이 집계되어, 특정 테스트 케이스에 대한 전체 응답을 보여줍니다.\n\n![Figure 5](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFigure 5에서 많은 내용이 있지만 먼저 파란 막대부터 시작해보세요. 파란 막대는 \"기타\" (NAICS 관련이 아닌) 예측 변수를 나타냅니다. 이러한 막대는 왼쪽 열의 것보다 오른쪽 열의 플롯에서 더 깁니다. 보이지 않는 코드에 대해서는 모델이 NAICS 인코딩 외의 특성에 더 의존하게 됩니다. 이 효과는 다중 특성 방법(target encoding(전체), target+count encoding, target-thresh encoding)에서 가장 강합니다.\n\n주황색 막대는 저수준 NAICS 코드의 인코딩 중요성을 보여줍니다. 시험 데이터에서는 차이가 작지만, 홀드아웃 데이터에서는 계층적 혼합이 두드러집니다. 이러한 특성에 대한 높은 의존성은 위에서 관찰된 과적합과 일관성이 있습니다.\n\nTarget encoding은 NAICS만을 기반으로 대출 연체를 예측하는 간단한 모델일 뿐이며, 이러한 예측값은 XGBoost에 공급됩니다. 계층적 혼합은 더 복잡한 전단 모델입니다. 여기에는 바이어스-분산 균형이 존재하는 것으로 보이며, 계층적 혼합은 과적합됩니다.\n\n마지막으로, 녹색 막대는 계층 구조의 상위 수준에서 파생된 중요한 특성을 보여줍니다(또는 카운트); 이러한 특성은 다중 특성 방법에만 존재합니다. 상위 수준 효과는 표준 NAICS 계층 구조(플롯의 두 번째 행)에서 가장 강합니다. 표준 계층 구조는 모델에 추가 정보를 제공하는 반면, DGI 및 무작위 인코딩은 거의 가치를 제공하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 간단한 인코딩 방법을 사용하면 기본 기능이 더 뒤로 밀릴 수 있어요. 정보가 불완전한 경우 특성에 너무 의존하는 대신 모델이 대안 소스를 찾도록 합시다.\n\n# 결측값에 대해 어떻게 생각하세요?\n\n나는 훈련 데이터의 구성, 즉 저량 및 결측 코드의 모두가 중요한 고려사항이라고 생각해요.\n\n내 데이터셋에는 결측값이 없어요. 원본 데이터에는 주로 오래된(2000년 이전) 대출에 누락값이 있었는데, 그 경우들을 삭제했어요 [3]. 장기 대출은 주로 낮은 채무 불이행률을 보이는 편이에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n임의의 결측값이 결과값과 상관없는 것은 모델이 알 수 없는 값을 처리하는 방법을 학습하는 데 도움이 될 것이라고 생각합니다. 그러나 이 데이터셋에서는 알 수 없는 값에 편향이 있을 가능성이 있습니다. 왜냐하면 그 값들이 대출 연령을 반영하기 때문입니다. 따라서, target+thresh가 보이지 않는 코드들을 위해 null을 사용한다면, 성능이 좋지 않을 것으로 예상되며, 특히 결측값이 적은 코드보다는 행이 더 많을 가능성이 있기 때문입니다.\n\n훈련 데이터에 결측값이 없다면, target 인코딩은 보이지 않는 레이블에 대해 일반화하기 위해 낮은 빈도 코드에 의존해야 합니다. Figure 1을 기억해보세요. 거기서 풀드 메소드의 성능이 약 100 앞에서 보이지 않는 코드에 대해 상승했던 것을 기억하실 겁니다. 보통 target 인코딩의 이상적인 중간점은 대부분 타겟 비율에 의존한다고 생각합니다. 그러나 Figure 1의 경우, 중간점은 교육에 충분한 낮은 빈도 케이스들이 더 의존할 수 있도록 하는 데 더 많이 의존합니다. 창 비유로 돌아가보겠습니다. 그 창은 충분히 열려 있어야 합니다. 모델이 다른 기능에 의존하는 방법을 배우기 위해 불확실한 비율을 대표하는 예가 충분해야 하기 때문입니다.\n\nFigure 1에서도 보이듯이 보이지 않는 코드에 일반화하기에 충분히 높은 중간점과 너무 높은 중간점으로 인한 정보 손실 사이에 긴장이 있음을 알 수 있습니다. 충분한 데이터가 있고 NAICS가 불균형하게 분포되어 있으므로 작동 가능한 범위가 있습니다. 항상 이렇게 될까요? 올바른 균형이 없는 데이터셋도 있을까요?\n\n아래 그림에서 Figure 1과 똑같은 인코딩을 수행하지만, XGBoost 모델을 적합하기 전에 훈련 데이터에서 낮은 빈도 코드를 제거합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![그림](/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_8.png)\n\n피규어 6는 훈련 데이터에 널 값이 없는 경우 target+thresh가 성능이 좋지 않음을 보여줍니다. 그림 6의 시나리오에서 훈련 데이터 평균으로 채워 target+thresh를 사용하는 경우, target 인코딩(모두) 및 target+count(표시되지 않음)와 매우 유사한 성능을 보입니다.\n\n그림 6은 피팅 전에 훈련 데이터의 변경에 영향을 받지 않는 계층적 블렌딩 결과를 보여줍니다. 계층 구조의 의미에 대한 모든 정보는 인코딩된 피처에 포함되어 있습니다. 그러나 다른 방법은 모델이 낮은 볼륨 또는 누락된 코드를 보상하는 것을 학습하는 데 의존합니다.\n\n저는 결과가 훈련 데이터의 낮은 볼륨 및 보이지 않는 경우의 특성에 얼마나 의존하는지 조심스럽게 생각합니다. 트레이닝 값을 누락되거나 타겟 평균으로 설정하는 어떤 형태의 무작위화가 도움이 될 수 있다고 생각합니다. 무작위 누락된 케이스를 사용하면 편향이 감소되고 충분한 관련 훈련 예제가 있는지 확실할 수 있습니다. 이러한 무작위화의 단점은 정보 손실인데, 훈련하는 동안 무작위화 및 각 부스팅 라운드마다 다른 관측을 널 값으로 설정하는 것이 좋을 수도 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 다른 모델 유형은 어떻게 되나요?\n\nXGBoost를 테스트해보았고, 결과가 다른 부스팅 트리 모델에도 적용될 것으로 예상합니다. 누락된 값이 허용되지 않는 모델의 경우, target-thresh는 상수 값을 채워야 합니다. 이 부분에 대해 여러 번 테스트를 진행한 결과, 상수 값으로 채우는 것이 괜찮다고 보입니다. 실제로, 그림 6에서 상수 값으로 채우는 것이 훈련 데이터에 낮은 양의 코드가 있는 경우에 선호될 수 있다는 것을 보여줍니다. 그러나 특정 경우에는 여전히 문제가 될 수도 있지 않을까 싶습니다. 예를 들어, 일부 코드의 기본 비율이 전체 평균과 유사한 경우 등.\n\n다중 특성 인코딩의 경우, 랜덤 포레스트 모델은 고수준 그룹 특성을 더 많이 활용할 것으로 예상됩니다 (XGBoost의 열 샘플링이 유사한 효과를 낼 수 있음). 이는 모든 관측치에 영향을 미칠 것이지만, 낮은 양의 또는 보지 못한 코드에 대해 도움이 될 수도 있습니다.\n\n트리 기반 모델에서는 타겟 인코딩이 자주 사용되지만, 신경망 모델에서는 entity embeddings에 대한 유사한 고려 사항이 적용됩니다. 이전에, 신경망 모델이 unseen 케이스에 대해 훈련 데이터 입력을 무작위로 설정하지 않으면 심하게 과대적합되는 것을 발견했습니다. 과대적합이 심한 경우에는 N=2 예시를 든 entity embeddings에 무작위화가 일반화에 도움이 될 것으로 제안되었습니다. 현재 몇 명의 학생들에게도 이것을 권고했는데, 이들 또한 과대적합이 크게 감소했습니다. 따라서, N=2 예시에서는 무작위화가 entity embeddings의 일반화에 도움되는 것으로 보입니다. 앞으로 유망한 전략일지도 모릅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마지막으로\n\n저는 고차원 범주형 데이터에 의존하는 모델을 구축하여 생계를 유지하는 사람입니다. 제가 원하는 것은 보고 있거나 보지 못한 코드에 대해 양호한 성능을 제공하며, 과적합 위험이 낮은 쉽게 사용할 수 있는 시스템입니다. 아직은 테스트한 방법 중 어느 것도 완전하다고 느끼지 않지만, 어떤 방법들은 근접하고 있는 것 같습니다.\n\n편리한 시스템은 목표 임계치 부여 및 최적 임계치를 결정하기 위한 피팅이 포함된 것일 수 있습니다(scikit-learn의 TargetEncoder와 유사한 방식). 또한, 모델이 고수준 기능으로부터 학습할 수 있도록 일종의 무작위 무효화/채움이 필요할 것입니다. 훈련 데이터에 충분한 양의 저주파 코드가 있는지 여부를 학습할 수 있어야 하며, 이미 존재하는 결측치가 데이터 편향을 반영하는 경우에도 모델이 학습할 수 있어야 합니다. 사용 편의성을 위해, 무효화가 모델 훈련에 내장되어 있으면 좋고, 데이터 섞기도 정보 손실을 줄일 수 있습니다.\n\n앞으로 이러한 아이디어를 더 탐구하고 싶습니다! 댓글에서 제안을 듣고 싶습니다. 여러분은 범주형 특성을 어떻게 처리하나요? 어떤 문제를 겪어보셨나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료\n\n[1] D. Micci-Barreca, Extending Target Encoding (2020), Towards Data Science.\n\n[2] D. Micci-Barreca, A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems (2001), ACM SIGKDD Explorations 3 (1).\n\n[3] V. Carey, Exploring Hierarchical Blending in Target Encoding (2024), Towards Data Science.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[4] M. Li, A. Mickel and S. Taylor, 해당 대출을 승인해야 할까요 거부해야 할까요?: 클래스 할당 지침이 포함된 대규모 데이터셋 (2018), 통계 교육 저널 26 (1). (CC BY 4.0)\n\n[5] M. Toktogaraev, 해당 대출을 승인해야 할까요 거부해야 할까요? (2020), 캐글. (CC BY-SA 4.0)\n\n[6] V. Carey, GitHub 저장소, https://github.com/vla6/Blog_gnn_naics.\n\n[7] 미국 센서스국, 북미 산업 분류체계.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[8] Scikit-Learn Documentation, Target Encoder의 내부 교차 적합 (2024).","ogImage":{"url":"/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_0.png"},"coverImage":"/assets/img/2024-05-18-NoLabelLeftBehindAlternativeEncodingsforHierarchicalCategoricals_0.png","tag":["Tech"],"readingTime":14},{"title":"희소 라마 70 더 작고, 3배 빠르며, 완벽한 정확도","description":"","date":"2024-05-18 20:23","slug":"2024-05-18-SparseLlama70Smaller3xFasterandFullAccuracy","content":"\n\nCerebras와 Neural Magic은 가지치기 기술과 희소 사전 훈련을 결합하여, 정확도를 희생하지 않고 매개변수를 최대 70%까지 줄일 수 있었다.\n\n예를 들어, Llama 2를 50-70%로 희소화하여 어려운 하위 작업의 정확도를 유지하면서도 성공적으로 수행되었다. Neural Magic의 DeepSparse 엔진은 밀집 모델 대비 최대 3배 더 빠른 추론을 제공한다.\n\n깊은 학습의 희소화는 계산 및 메모리 비용을 줄이는 데 목적이 있다. 가지치기가 컴퓨터 비전 모델의 크기를 효과적으로 줄였지만, LLMs에 대해서는 유사한 결과를 내지 못했다. LLMs는 매개변수가 많고, 가지치기는 매개변수 사이의 섬세한 균형을 방해할 수 있어서, 채팅 및 코딩과 같은 작업에서 큰 정확도 손실을 야기할 수 있다. 이러한 복잡성으로 인해, 중요한 LLMs는 희소성을 거의 활용하지 않는다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLlama 모델을 70%로 희소화하는 것은 인상적이지만, 모델의 정확도를 유지하기 위한 프로세스는 매우 복잡합니다. 새 데이터셋에 대해 후속 사전 훈련을 수행해야 합니다.\n\n![이미지](/assets/img/2024-05-18-SparseLlama70Smaller3xFasterandFullAccuracy_1.png)\n\n그 결과, LLM을 희소화하는 것은 비용이 많이 듭니다. 이미 희소화된 모델을 저장하는 모델 동물원을 가지고 있습니다. 이것이 Llama 3와 같은 보다 최근의 LLM이 아직 희소 모델로 제공되지 않는 이유입니다. 변환이 시간이 걸립니다.\n\n희소 모델로 효율적 추론을 하기 위해, 그들은 vLLM을 수정했습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- GitHub: neuralmagic/nm-vllm\n\n그들은 자신들의 방법을 설명한 기술 보고서를 발표했습니다:\n\n효율적인 사전 학습 및 배포로 높은 희소성 LLama 모델 활성화\n\n내 작업을 지원하려면, 최신 AI 발전에 대한 더 많은 기사/튜토리얼을 보려면 내 뉴스레터를 구독해 주세요.","ogImage":{"url":"/assets/img/2024-05-18-SparseLlama70Smaller3xFasterandFullAccuracy_0.png"},"coverImage":"/assets/img/2024-05-18-SparseLlama70Smaller3xFasterandFullAccuracy_0.png","tag":["Tech"],"readingTime":2},{"title":"로지스틱 회귀의 시각적 이해","description":"","date":"2024-05-18 20:20","slug":"2024-05-18-AVisualUnderstandingofLogisticRegression","content":"\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_0.png\" /\u003e\n\n로지스틱 회귀는 이진 분류에서 사용되는 통계 모델입니다. 이진 분류 문제에서 대상은 두 가지 범주만 가지고 있으므로 기계 학습 알고리즘은 데이터를 이 두 범주 중 하나로 분류해야 합니다. 로지스틱 회귀는 각 범주에 속할 확률을 예측하는 데 사용되는 로지스틱 함수에서 유래했습니다. 로지스틱 회귀는 지도 기계 학습, 금융, 의학 및 사회과학 등 여러 분야에 응용됩니다.\n\n본 문서에서는 로지스틱 회귀의 시각적 이해를 제시하고, 이 모델의 각 요소의 역할을 설명할 것입니다. 이 글을 읽으면 독자는 로지스틱 회귀와 그 한계에 대한 직관적인 이해를 가질 수 있습니다.\n\n본 문서의 모든 이미지는 저자에 의해 제작되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 데이터셋\n\n로지스틱 회귀가 분류 문제를 해결하는 방법을 보여주기 위해 간단한 데이터셋을 만들겠습니다. 먼저 필요한 모든 Python 라이브러리를 가져옵니다.\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib.colors import ListedColormap\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n우리의 데이터셋은 두 개의 특성 (x₁, x₂)과 100개의 예제가 있습니다. 이는 두 개의 클러스터로 구성되어 각각 정규 분포를 사용하여 만들어졌습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nnp.random.seed(0)\nx1 = np.random.randn(50, 2) * 0.4 + np.array([-1, -1])\nx2 = np.random.randn(50, 2) * 0.4 + np.array([2.6, 2.6])\n\ny = 50*[0]+50*[1]\nX = np.vstack((x1, x2))\n```\n\n이 데이터셋에 대한 target 또는 label 열 (y)도 정의했습니다. 첫 번째 클러스터의 데이터 포인트들의 레이블은 0이고, 두 번째 클러스터의 데이터 포인트들의 레이블은 1입니다. 따라서 target 열에는 2개의 레이블만 있어서 binary classification 문제가 됩니다. 이제 이 데이터셋을 플롯합니다. 결과는 아래 그림에서 확인할 수 있습니다.\n\n```python\nplt.scatter(x1[:, 0], x1[:,1], label=\"y=0\")\nplt.scatter(x2[:, 0], x2[:,1], label=\"y=1\")\nplt.legend(loc=\"best\", fontsize=14)\nplt.xlabel(\"$x_1$\", fontsize=16)\nplt.ylabel(\"$x_2$\", fontsize=16)\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_1.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금 이 데이터셋을 분류하기 위해 로지스틱 회귀 모델을 사용할 수 있습니다. 이 모델을 훈련시켜 이 데이터셋의 데이터 포인트의 이진 레이블을 예측할 것입니다. 또한 이 모델은 이 훈련 데이터셋에 없는 어떤 보이지 않는 데이터 포인트에 대한 예측을 총체화할 수 있어야 합니다.\n\n로지스틱 회귀 방정식\n\n로지스틱 회귀 모델을 이해하려면 먼저 그 방정식을 자세히 살펴봐야 합니다:\n\n![로지스틱 회귀 방정식](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 P는 데이터 포인트 (x₁, x₂)가 레이블 1일 확률을 예측한 값입니다. 이 방정식은 로지스틱 회귀 모델의 핵심입니다. 그냥 데이터 포인트를 가져와서 해당 레이블이 1일 확률을 계산하는 것이죠. 이 함수는 표준 로지스틱 또는 시그모이드 함수라고 불립니다. 아래 소개된 그림 2는 이 함수의 플롯을 보여줍니다. x가 ∞로 다가갈수록 y는 1로 수렴하고, x가 -∞로 다가갈수록 y는 0으로 수렴함을 주목하세요. 게다가 x=0에서 y=0.5인 것을 알 수 있습니다. 따라서 y는 항상 0과 1 사이에 제한됩니다. 우리는 확률이 항상 [0,1] 범위 내에 있음을 알고 있기 때문에 결과의 확률을 표현하기 위해 시그모이드 함수를 사용할 수 있습니다. 이 함수는 임의의 실수 값을 가진 입력(x)을 0과 1 사이의 확률 값으로 매핑할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 방정식 1이 특징 x₁과 x₂로 표현된 데이터 포인트를 입력하여 해당 레이블이 1일 확률로 변환하는 방법을 알아봅시다.\n\n차원 축소\n\n방정식 1을 두 부분으로 나눌 수 있습니다. 먼저 입력 데이터(x₁, x₂)를 선형 항으로 변환합니다.\n\n![equation](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서 w₀, w₁ 및 w₂는 모델의 매개변수이며 이 값은 모델을 학습한 후에 결정될 것입니다. 이것은 두 개의 특징 (x₁, x₂)으로 시작하여 방정식 2에 의해 제공된 단일 숫자로 변환하는 차원 축소의 예입니다. 실제로 우리는 입력 데이터 포인트의 차원을 2에서 1로 줄입니다. 이 차원 축소가 기하학적으로 어떻게 이루어지는지 살펴보겠습니다. 데이터 포인트 (x₁, x₂)로 시작합니다. 우리는 이를 2차원 공간에서 점 또는 벡터로 표시할 수 있습니다(Figure 3). 또한 벡터 w를 다음과 같이 정의할 수 있습니다:\n\n![vector w equation](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_6.png)\n\n벡터 u를 w의 단위 벡터로 정의할 수 있도록 하는 다음 방정식을 사용하여 정의합시다:\n\n![unit vector equation](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벡터 u가 w와 같은 방향을 가지지만 길이는 1입니다. 이제 벡터 x를 w에 평행한 벡터와 수직인 벡터 두 가지 구성 요소 벡터로 분해할 수 있습니다. 평행 벡터는 x를 w에 투영한 벡터로 불리며 x^로 표시됩니다(그림 3). 또한 x와 w의 내적을 사용하여 x^를 얻을 수 있습니다:\n\n![Figure 3](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_8.png)\n\n![Image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_9.png)\n\nx^의 길이는 x를 w에 투영한 스칼라 투영이라고 하며 다음과 같이 주어집니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_10.png\" /\u003e\n\n이제 x^를 ||w||로 곱하면 d로 표시된 새로운 벡터를 얻습니다:\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_11.png\" /\u003e\n\n그리고 d의 길이는 u가 단위 벡터이기 때문에 w.x와 동일합니다. 내적의 정의에 따라 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_12.png\" /\u003e\n\n이것은 방정식 2에서 정의된 용어 일부를 제공합니다. 그러나 w₀를 추가해야 합니다.\n\n이를 위해 다음과 같은 벡터를 정의합니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_13.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제, d에서 o를 뺀다면 다음과 같습니다:\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_14.png)\n\n즉, d-o의 길이는 방정식 2에서 정의된 용어와 같다는 것을 의미합니다 (그림 4). 이것은 벡터 d의 새로운 원점을 정의하는 것과 같습니다. 기하학적 관점에서 보면, 우리는 벡터 o의 끝 지점을 기준으로 d의 길이를 측정합니다. 반면 2차원 공간의 원점을 기준으로 하지 않습니다. (이 그림에서 w₀가 양수라고 가정하였기 때문에 벡터 o는 w의 반대 방향에 있습니다).\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_15.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 친근한 톤으로 한국어로 번역하겠습니다.\n\n요약하자면, 방정식 2의 용어 역할은 차원 축소입니다. 입력 데이터 점의 차원을 1로 줄입니다. 따라서 변환된 데이터 점은 모두 원점을 통과하고 벡터 w를 따라 있는 선 l로 가정할 수 있습니다. 이 선을 새로운 축으로 생각하면, 원점은 벡터 o의 끝에 있는 새 축이라는 것을 알 수 있습니다. 이제 이 축 위의 변환된 데이터 점의 좌표는 방정식 2에 의해 주어집니다.\n\n벡터 x가 입력 데이터 점을 나타낸다고 가정했습니다. 새로운 축 l 상의 변환된 데이터 점을 얻기 위해 우리는 먼저 직교 투영을 수행하고 x를 w에 투영한 벡터를 찾았습니다. 그런 다음 결과 벡터인 (x^)에 w의 길이를 곱하여 벡터 d를 얻었습니다. 벡터 d는 새로운 축 l 상의 변환된 데이터 점을 나타내지만, 그 좌표는 o에서 d를 뺀 d-o로 주어집니다.\n\n이제 우리는 장난감 데이터 세트에서 변환된 일차원 데이터 점을 계산할 수 있습니다. 여기서는 scikit-learn 라이브러리의 로지스틱 회귀 모델을 사용합니다. 데이터 세트를 fitting한 후, 방정식 2의 선형 항의 계수를 검색할 수 있습니다.\n\n```python\nlg=LogisticRegression()\nlg.fit(X,y)\nw = lg.coef_[0]\nw1, w2 = w[0], w[1]\nw0 = lg.intercept_[0]\nprint(\"w0={}, w1={}, w2={}\".format(w0, w1, w2))\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```md\nw0=1.2124, w1=0.9033, w2=0.9075\n```\n\n이제 w₁와 w₂의 값을 사용하여 벡터 w를 형성할 수 있습니다. w의 단위 벡터는 다음과 같이 정의됩니다:\n```md\n\\[ w = \\begin{bmatrix} 1.2124 \\\\ 0.9033 \\\\ 0.9075 \\end{bmatrix} \\]\n```\n다음과 같이 계산됩니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nlength_w = np.linalg.norm(w)\nu = w / length_w\n```\n\n변환된 일차원 데이터 포인트들은 이 벡터를 따라 놓이게 될 것이고, u와 w가 동일한 방향을 가지고 있기 때문에 w도 따라 늘어날 것입니다. 이 선의 원점은 벡터 o=-w₀u의 끝에 위치합니다.\n\n```js\no = -w0 * u\n```\n\n다음 코드 스니펫을 통해 원본 데이터 세트, 벡터 w와 o, 그리고 변환된 데이터 포인트를 플롯합니다. 결과는 Figure 5에 표시됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n```python\nlt.figure(figsize=(6,6))\n\n# 원본 데이터 세트 플롯\nplt.scatter(X[y==0, 0], X[y==0,1], label=\"y=0\", alpha=0.4, color=\"red\")\nplt.scatter(X[y==1, 0], X[y==1,1], label=\"y=1\", alpha=0.4, color=\"blue\")\n\n# 변환된 포인트 플롯\ntransformed_points = np.dot(X, w).reshape(-1,1) * np.tile(w, (len(X), 1))\nplt.scatter(transformed_points[:, 0], transformed_points[:, 1],\n            alpha=0.5, color='green', label=\"변환된\\n포인트\")\n\n# 포인트 o 플롯\nplt.scatter(o[0], o[1], color='black', s=35, zorder=10)\n# 벡터 w와 o 플롯\nplt.quiver([0], [0], w[0], w[1], color=['b'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\nplt.quiver([0], [0], o[0], o[1], color=['black'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\n\n# 벡터 w를 따라 나아가는 선\nplt.plot([-12*u[0], 19*u[0]],\n         [-12*u[1], 19*u[1]], color='gray')\n\n# 축 생성\nplt.axhline(0, color='black', linewidth=0.8)\nplt.axvline(0, color='black', linewidth=0.8)\nplt.text(0.3, 1.2, \"$\\mathregular{w}$\", color='b', fontsize=14,\n         weight=\"bold\", style=\"italic\")\nplt.text(-1.7, -1, \"$\\mathregular{o}$\", color='black', fontsize=14,\n         weight=\"bold\", style=\"italic\")\n\nplt.xlim([-8, 6])\nplt.ylim([-8, 6])\nax = plt.gca()  \nax.set_aspect('equal')\nplt.legend(loc=\"best\", fontsize=13)\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\n\nplt.show()\n```\n\n![그림](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_17.png)\n\n모든 변환된 데이터 포인트는 w 벡터를 따라 있는 선상에 있음을 유의해주세요. 이 선의 원점은 점 o에 위치합니다. 원본 데이터 세트의 각 데이터 포인트 (x₁, x₂)는 이 선상의 데이터 포인트로 변환되며, 변환된 데이터 포인트 (각 녹색 점)의 점 o로부터의 거리는 w₀+w₁x₁+w₂x₂와 같습니다.\n\n시그모이드 함수 추가\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 우리는 방정식 1을 두 부분으로 나눴다는 것을 기억해야 해요. 먼저 입력 데이터 (x₁, x₂)를 선형 항인 w₀+w₁x₁+w₂x₂로 변환합니다. 이는 차원 축소 과정으로, 변환된 일차원 데이터 포인트를 만들어냅니다. 다음 부분은 이러한 변환된 데이터 포인트에 시그모이드 함수를 정의합니다. 이 함수는 변환된 데이터 포인트가 레이블 1을 가지는 확률을 계산합니다. 이 확률을 계산하기 위해 각 변환된 데이터 포인트의 좌표 (l=w₀+w₁x₁+w₂x₂)를 시그모이드 함수에 넣는 것입니다:\n\n![이미지](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_18.png)\n\n다음 코드 스니펫은 이 확률을 계산하고 도표 6에 시그모이드 함수를 그리는 것입니다:\n\n```js\nplt.figure(figsize=(15,5))\n\ntransformed_points = np.dot(X, w) + w0\nplt.scatter(transformed_points, [0]*len(transformed_points),\n            s=280, color='green', alpha=0.4,\n            label=\"변환된 데이터 포인트\")\nl_array = np.linspace(-12, 8, 100)\nP = 1 / (1+np.exp(-l_array))\nplt.plot(l_array, P, color='black', label=\"시그모이드 함수\")\n\nplt.xlim([-8, 8])\nplt.ylim([0, 1.05])\nplt.legend(loc=\"best\", fontsize=18)\nplt.xlabel('$l$', fontsize=22)\nplt.ylabel('$P$', fontsize=22)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.show()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_19.png\" /\u003e\n\n그래서 각 변환된 데이터 포인트마다 y=1의 확률이 있습니다. 그러나 실제 레이블을 얻기 위해서는 확률 임계값을 정의해야 합니다 (y의 실제 값). 이 임계값은 이진 분류 결정을 내릴 확률을 정의합니다. 기본적으로 로지스틱 회귀는 P=0.5의 임계값을 선택합니다. 시그모이드 곡선은 원점에서 값이 0.5임을 기억해 주세요. 따라서 w₀+w₁x₁+w₂x₂≥0 (y^=1)인 모든 포인트에 대해 예측된 레이블은 1이며, `w₀+w₁x₁+w₂x₂\u003c0 (y^=0)`인 모든 포인트에 대해 예측된 레이블은 0입니다. 따라서 확률 임계값은 각 변환된 데이터 포인트의 예측된 확률(P)을 y^로 나타내는 예측된 이진 레이블로 변환합니다. 이것은 Figure 7에 표시되어 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_20.png\" /\u003e\n\n우리는 또한 이 시그모이드 곡선을 원래 2차원 공간에 그릴 수 있습니다. 이 결과는 Figure 8에 표시되어 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nplt.figure(figsize=(6,6))\n\n# 원본 데이터셋 플롯\nplt.scatter(X[y==0, 0], X[y==0,1], label=\"y=0\", alpha=0.7, color=\"red\")\nplt.scatter(X[y==1, 0], X[y==1,1], label=\"y=1\", alpha=0.7, color=\"blue\")\n\n# 투영된 점 플롯\ntransformed_points = np.dot(X, w).reshape(-1,1) * np.tile(w, (len(X), 1))\nplt.scatter(transformed_points[:, 0], transformed_points[:, 1],\n            alpha=0.5, color='green', label=\"Transformed\\n data points\")\n\n# 점 o 플롯\nplt.scatter(o[0], o[1], color='black', s=35)\n# 벡터 w 플롯\nplt.quiver([0], [0], w[0], w[1], color=['b'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\n\n\n# 시그모이드 곡선 플롯\nk = 200\nl_array = np.linspace(-12, 8, k).reshape(-1, 1)\nw_array = l_array * np.tile(u, (k, 1)) \n# 벡터 w를 따라 선 그리기\nplt.plot([-12*u[0], 19*u[0]],\n         [-12*u[1], 19*u[1]], color='gray')\n\nsigm_x_array = ((w_array - o) /u)[:,0]\nsigm_prob = 1 / (1+np.exp(-sigm_x_array))\nnorm_vector = np.array([-w2, w1]) if w1\u003e=0 else np.array([w2, -w1])\nsigm_y_array = sigm_prob.reshape(k, 1) * np.tile(norm_vector, (k, 1)) \nsigm_curve_array = sigm_y_array + w_array\n\nplt.plot(sigm_curve_array[:, 0], sigm_curve_array[:, 1], color=\"blue\")\n# 시그모이드 곡선의 y축 플롯\nplt.plot([o[0], o[0]+2*norm_vector[0]],\n         [o[1], o[1]+2*norm_vector[1]], color='gray')\n\n# 축 그리기\nplt.axhline(0, color='black', linewidth=0.8)\nplt.axvline(0, color='black', linewidth=0.8)\nplt.text(0.3, 1.2, \"$\\mathregular{w}$\", color='b', fontsize=14,\n         weight=\"bold\", style=\"italic\")\nplt.text(-0.8, -1.4, \"$\\mathregular{o}$\", color='black', fontsize=14,\n         weight=\"bold\", style=\"italic\")\nplt.text(-2.9, 1.3, \"$P$\", color='black', fontsize=14,\n         weight=\"bold\", style=\"italic\", rotation = 50)\n\nplt.xlim([-8, 6.2])\nplt.ylim([-8, 6.2])\nax = plt.gca()  \nax.set_aspect('equal')\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\n\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_21.png\" /\u003e\n\n하지만 2차원 공간에서 결정 경계를 어떻게 찾을까요? 이를 위해 2차원 공간의 모든 점을 찾아야 합니다. 이러한 점들은 1차원 공간의 원점으로 매핑됩니다 (Figure 8의 점 o). Figure 9에서 이러한 점들을 찾을 수 있는 방법을 보여줍니다.\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_22.png\" /\u003e\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서는 x라는 지점을 찾고 있습니다.\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_23.png)\n\n이러한 점들은 x^에서 l에 수직인 선상에 있습니다. 우리는 이 선을 s로 표시할 것입니다 (도표 9). 모든 점들이 2차원 평면의 원점으로부터의 거리는 |w₀| / ||w||입니다(점과 선 사이의 거리는 그 선에 수직이고 해당 점을 통과하는 선분의 길이입니다). 이제 s의 모든 점들에 대한 벡터 d는 다음과 같습니다:\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_24.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 데이터 지점이 선 s에 있으면, 해당 변환된 지점은 지점 o(1차원 공간의 원점)에 있고, 그 확률은 0.5가 됩니다. 이로써 선 s가 2차원 공간의 로지스틱 회귀의 결정 경계라는 것을 결론짓게 되었습니다. 왜냐하면 이 선 상의 모든 데이터 지점은 1차원 공간의 시그모이드 곡선의 결정 경계로 매핑되기 때문입니다. 따라서 이제 우리 모델의 결정 경계를 쉽게 그릴 수 있습니다.\n\n```js\nboundary_point = (-w0 / length_w**2) * w\n\nplt.figure(figsize=(6,6))\n\nplt.scatter(X[y==0, 0], X[y==0,1], label=\"y=0\", alpha=0.7, color=\"red\")\nplt.scatter(X[y==1, 0], X[y==1,1], label=\"y=1\", alpha=0.7, color=\"blue\")\n\nplt.scatter(o[0], o[1], color='black', s=35)\n# 벡터 w 그리기\nplt.quiver([0], [0], w[0], w[1], color=['b'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\n\nplt.quiver([0], [0], boundary_point[0], boundary_point[1], color=['black'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\n\n# 결정 경계 그리기\nplt.plot([boundary_point[0], boundary_point[0]-6],\n         [boundary_point[1], boundary_point[1]-6*(-w1/w2)],\n         color='black', linestyle=\"--\")\nplt.plot([boundary_point[0], boundary_point[0]+6],\n         [boundary_point[1], boundary_point[1]+6*(-w1/w2)],\n         color='black', linestyle=\"--\")\n\n# 시그모이드 곡선 그리기\nk = 200\nl_array = np.linspace(-8.4, 8, k).reshape(-1, 1)\nw_array = l_array * np.tile(u, (k, 1)) \n\n# 벡터 w를 따른 선 그리기\nplt.plot([-9*u[0], 8*u[0]],\n         [-9*u[1], 8*u[1]], color='gray')\n\nsigm_x_array = ((w_array - o) /u)[:,0]\nsigm_prob = 1 / (1+np.exp(-sigm_x_array))\nnorm_vector = np.array([-w2, w1]) if w1\u003e=0 else np.array([w2, -w1])\nsigm_y_array = sigm_prob.reshape(k, 1) * np.tile(norm_vector, (k, 1)) \nsigm_curve_array = sigm_y_array + w_array\n\nplt.plot(sigm_curve_array[:, 0], sigm_curve_array[:, 1], color=\"blue\")\n# 시그모이드 곡선의 y축 그리기 \nplt.plot([o[0], o[0]+2*norm_vector[0]],\n         [o[1], o[1]+2*norm_vector[1]], color='gray')\n\n# 축 그리기\nplt.axhline(0, color='grey', linewidth=0.8)\nplt.axvline(0, color='grey', linewidth=0.8)\n\nplt.text(0.35, 1, \"$\\mathregular{w}$\", color='b', fontsize=14,\n         weight=\"bold\", style=\"italic\")\nplt.text(-0.95, -1.35, \"$\\mathregular{o}$\", color='black', fontsize=14,\n         weight=\"bold\", style=\"italic\")\nplt.text(-3.15, 0.5, \"$P$\", color='black', fontsize=14,\n         weight=\"bold\", style=\"italic\", rotation = 50)\nplt.text(-4, 3, \"결정 경계\", color='black', fontsize=14)\nplt.text(-0.3, -0.7, r\"$\\frac{-w_0}{\\mathregular{||w||^2}\\mathregular{w}$\",\n         color='black', fontsize=15, weight=\"bold\", style=\"italic\")\n\n\nplt.xlim([-5.6, 4.2])\nplt.ylim([-5.6, 4.2])\nax = plt.gca()  \nax.set_aspect('equal')\n\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\n\nplt.show()\n```\n\n\u003cimg src=\"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_25.png\" /\u003e\n\n우리는 또한 여기서 발견한 결정 경계의 위치를 유효성 검사할 수 있습니다. 이를 위해 모델의 경계를 다른 방법을 사용하여 그리는 함수를 정의합니다. 먼저 2차원 공간에 메시 그리드를 생성하고 이를 사용하여 훈련된 로지스틱 회귀 모델을 사용하여 해당 지점의 목표를 예측합니다. y^=0 및 y^=1인 지점은 서로 다른 색으로 표시되므로 그리드가 충분히 잘 그려진 경우 모델의 결정 경계를 쉽게 확인할 수 있습니다. 결과는 Figure 11에 나와 있으며 이전에 발견한 결정 경계와 일치합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef plot_boundary(X, y, clf, lims):\n    gx1, gx2 = np.meshgrid(np.arange(lims[0], lims[1], (lims[1]-lims[0])/300.0),\n                           np.arange(lims[2], lims[3], (lims[3]-lims[2])/300.0))\n    \n    cmap_light = ListedColormap(['lightsalmon', 'aqua'])\n            \n    gx1l = gx1.flatten()\n    gx2l = gx2.flatten()\n    gx = np.vstack((gx1l,gx2l)).T\n    gyhat = clf.predict(gx)\n    gyhat = gyhat.reshape(gx1.shape)\n\n    plt.pcolormesh(gx1, gx2, gyhat, cmap=cmap_light)\n    plt.scatter(X[y==0, 0], X[y==0,1], label=\"y=0\", alpha=0.7, color=\"red\")\n    plt.scatter(X[y==1, 0], X[y==1,1], label=\"y=1\", alpha=0.7, color=\"blue\")\n    plt.legend(loc='upper left')\n\n\nplt.figure(figsize=(6,6))\nplot_boundary(X,y,lg, lims=[-5.6, 4.2, -5.6, 4.2])\n\n# Plot the vector w\nplt.quiver([0], [0], w[0], w[1], color=['b'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\n\nplt.quiver([0], [0], boundary_point[0], boundary_point[1], color=['black'],\n           width=0.01, angles='xy', scale_units='xy', scale=1, zorder=5)\n\n# plot the decision boundary\nplt.plot([boundary_point[0], boundary_point[0]-6],\n         [boundary_point[1], boundary_point[1]-6*(-w1/w2)],\n         color='black', linestyle=\"--\")\nplt.plot([boundary_point[0], boundary_point[0]+6],\n         [boundary_point[1], boundary_point[1]+6*(-w1/w2)],\n         color='black', linestyle=\"--\")\n\n# Plot the line along the vector w \nplt.plot([-9*u[0], 8*u[0]],\n         [-9*u[1], 8*u[1]], color='gray')\n\n# Draw axes\nplt.axhline(0, color='grey', linewidth=0.8)\nplt.axvline(0, color='grey', linewidth=0.8)\n\nplt.text(0.35, 1, \"$\\mathregular{w}$\", color='b', fontsize=14,\n         weight=\"bold\", style=\"italic\")\n\nplt.text(-2, 3, \"$\\hat{y}=1$\\nregion\", color='blue', fontsize=14)\nplt.text(1, -5, \"$\\hat{y}=0$\\nregion\", color='red', fontsize=14)\nplt.text(-0.3, -0.7, r\"$\\frac{-w_0}{\\mathregular{||w||^2}\\mathregular{w}$\",\n         color='black', fontsize=15, weight=\"bold\", style=\"italic\")\n\nplt.xlim([-5.6, 4.2])\nplt.ylim([-5.6, 4.2])\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\nplt.show()\n```\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_26.png)\n\n여기에 결과를 요약해보겠습니다. 두 가지 특성을 갖는 데이터셋에서 로지스틱 회귀 모델의 의사결정 경계는 직선으로 형성됩니다. 이 직선은 모델의 매개변수 w₀, w₁, w₂에 의해 결정됩니다. 의사결정 경계는 벡터를 연장한 선에 수직입니다.\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_27.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 라인과 의사결정 경계의 교차점은 (-w₀ / ||w||²)w 벡터에 의해 결정됩니다.\n\n고차원에서의 의사결정 경계\n\n저희가 데이터셋에서 더 많은 피쳐를 가지고 있는 경우에 어떻게 될지 살펴봅시다. 우리는 같은 컨셉을 고차원으로 쉽게 적용할 수 있습니다. x₁, x₂, x₃라는 세 개의 피쳐를 가지고 있다고 상상해보겠습니다. 이제 로지스틱 회귀 방정식은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 매개변수는 w₀, w₁, w₂ 및 w₃입니다. 차원 축소 부분은 동일하며, 원본 데이터 포인트는 여전히 1차원 공간에 매핑됩니다. 변환된 데이터 포인트는 여전히 직선 l 상에 있으며 해당 벡터를 확장합니다:\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_29.png)\n\n결정 경계는 w로의 벡터 투영이 (-w₀ / ||w||²)w인 모든 포인트의 위치입니다. 이러한 포인트는 차원 축소 후 P=0.5를 갖게 됩니다. 따라서 결정 경계는 3차원 공간에서 평면입니다(도 12 참조). 이 평면은 l에 수직이며, l과의 교차점은 (-w₀ / ||w||²)w 벡터로 주어집니다.\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_30.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n보다 일반적으로, n차원 공간에서 로지스틱 회귀 모델은 n개의 매개변수 w₀, w₁, …, w_n을 가지고 있습니다. 여기에서, 만약 벡터를 확장한다면,\n\n![image](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_31.png)\n\n선 l로, 결정 경계는 n차원 초평면입니다. 이 초평면은 l에 수직이며, (-w₀ / ||w||²)w 벡터는 초평면과 l의 교차점을 나타냅니다.\n\n로지스틱 회귀는 항상 n차원 공간에서 1차원 공간으로 차원 축소를 시작합니다. 따라서 그 결정 경계는 곡률이 없는 초평면입니다. 결정 경계가 초평면인 분류기는 선형 분류기라고 하며, 로지스틱 회귀는 그러한 분류기의 한 예입니다. 다른 예시로는 퍼셉트론과 서포트 벡터 머신(SVM)이 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터 세트(특성이 n개 있는)는 이진 대상을 가지고 있고 n차원 초평면을 사용하여 서로 다른 라벨을 가진 데이터 포인트들을 완전히 분리할 수 있다면 선형 분리 가능하다고 합니다. 따라서 선형 분류기는 선형 분리 가능한 데이터 세트에 대한 완벽한 모델입니다. 지금까지 사용된 토이 데이터 세트는 선형 분리 가능했습니다(Figure 1), 그러나 많은 데이터 세트는 선형 분리가 불가능하며 로지스틱 회귀와 같은 모델은 그에 적합하지 않을 수 있습니다. Figure 13는 선형 분리가 불가능한 데이터 세트의 예시를 보여줍니다.\n\n![Figure 13](/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_32.png)\n\n여기서 데이터 세트는 원 모양을 가지고 있으며, 직선을 사용하여 y=0 및 y=1을 가진 데이터 포인트들을 완벽하게 나눌 수 없습니다. 따라서 이러한 분류 문제에 로지스틱 회귀 모델을 사용할 수 없습니다.\n\n이 기사에서는 선형 대수를 사용하여 로지스틱 회귀의 시각적 해석을 제공하려고 노력했습니다. 로지스틱 회귀는 1차원 공간으로의 차원 축소부터 시작하고, 그런 다음 변환된 데이터 포인트에 대한 확률을 할당합니다. 확률 임계값을 정의함으로써, 해당 데이터 포인트의 이진 대상에 대한 최종 예측을 얻을 수 있습니다. 1차원 공간으로의 차원 축소로 인해 로지스틱 회귀는 선형 분류기가 됩니다. 따라서 n개의 특성을 가진 데이터 세트에 적용되는 경우 의사 결정 경계는 n차원 초평면이 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사를 즐겁게 읽었으면 좋겠어요. 제 기사가 도움이 된다면, 저를 Medium에서 팔로우해 주세요.","ogImage":{"url":"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_0.png"},"coverImage":"/assets/img/2024-05-18-AVisualUnderstandingofLogisticRegression_0.png","tag":["Tech"],"readingTime":20}],"page":"45","totalPageCount":68,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"45"},"buildId":"RZIEBQ2aNAp_DXFVTV6eL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>