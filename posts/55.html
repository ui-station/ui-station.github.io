<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/55" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/55" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/baeec1f16d6ea8b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/baeec1f16d6ea8b8.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/0asLlD6on3tm8cIfzBaxd/_buildManifest.js" defer=""></script><script src="/_next/static/0asLlD6on3tm8cIfzBaxd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="더블 박사" href="/post/2024-05-17-DoublePhD"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="더블 박사" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-DoublePhD_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="더블 박사" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">더블 박사</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="왜 인공지능이 음악가를 대체할 수 없을까" href="/post/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="왜 인공지능이 음악가를 대체할 수 없을까" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="왜 인공지능이 음악가를 대체할 수 없을까" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">왜 인공지능이 음악가를 대체할 수 없을까</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="논리적인 사고 - 대형 언어 모델의 추론 능력 강화를 위한 새로운 프롬프트 엔지니어링 방법" href="/post/2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="논리적인 사고 - 대형 언어 모델의 추론 능력 강화를 위한 새로운 프롬프트 엔지니어링 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="논리적인 사고 - 대형 언어 모델의 추론 능력 강화를 위한 새로운 프롬프트 엔지니어링 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">논리적인 사고 - 대형 언어 모델의 추론 능력 강화를 위한 새로운 프롬프트 엔지니어링 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="다음 토큰 예측에서 비롯된 인간과 인공 일반 지능" href="/post/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="다음 토큰 예측에서 비롯된 인간과 인공 일반 지능" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="다음 토큰 예측에서 비롯된 인간과 인공 일반 지능" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">다음 토큰 예측에서 비롯된 인간과 인공 일반 지능</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">14<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요" href="/post/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="반복 신경망 시퀀스 모델링 소개" href="/post/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="반복 신경망 시퀀스 모델링 소개" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="반복 신경망 시퀀스 모델링 소개" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">반복 신경망 시퀀스 모델링 소개</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="ICD 코딩을 위한 LLM 탐험 - 파트 1" href="/post/2024-05-17-ExploringLLMsforICDCodingPart1"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="ICD 코딩을 위한 LLM 탐험 - 파트 1" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="ICD 코딩을 위한 LLM 탐험 - 파트 1" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">ICD 코딩을 위한 LLM 탐험 - 파트 1</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">19<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" href="/post/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="살아있는 인공지능의 첫 걸음, 바디 인텔리전스" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">살아있는 인공지능의 첫 걸음, 바디 인텔리전스</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="내일의 기계들" href="/post/2024-05-17-MachinesofTomorrow"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="내일의 기계들" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-MachinesofTomorrow_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="내일의 기계들" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">내일의 기계들</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기" href="/post/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 17, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/41">41</a><a class="link" href="/posts/42">42</a><a class="link" href="/posts/43">43</a><a class="link" href="/posts/44">44</a><a class="link" href="/posts/45">45</a><a class="link" href="/posts/46">46</a><a class="link" href="/posts/47">47</a><a class="link" href="/posts/48">48</a><a class="link" href="/posts/49">49</a><a class="link" href="/posts/50">50</a><a class="link" href="/posts/51">51</a><a class="link" href="/posts/52">52</a><a class="link" href="/posts/53">53</a><a class="link" href="/posts/54">54</a><a class="link posts_-active__YVJEi" href="/posts/55">55</a><a class="link" href="/posts/56">56</a><a class="link" href="/posts/57">57</a><a class="link" href="/posts/58">58</a><a class="link" href="/posts/59">59</a><a class="link" href="/posts/60">60</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"더블 박사","description":"","date":"2024-05-17 19:54","slug":"2024-05-17-DoublePhD","content":"\n\n![이미지](/assets/img/2024-05-17-DoublePhD_0.png)\n\n\"어릴 적부터 과학에 대한 깊은 사랑을 품고 왔습니다. 이 열정이 나를 분야에서의 경력으로 자연스럽게 이끌었습니다. AIEEE 입학 시험을 통과한 후, 2010년 NIT Uttarakhand에서 제 첫 공학 여정을 시작했습니다. 그 해는 처음으로 개설된 공학 코스였지만, 많은 고난을 겪었습니다.\n\n당시 NIT Uttarakhand는 임시 캠퍼스와 필수적인 자원 부족으로 인한 어려움에 직면했습니다. 이로 인해 학생들의 불만이 끊임없이 솟구쳤고, 우리는 영구적인 캠퍼스를 원하며 우리의 열정적인 호소를 우탄칸드 정부 관리자들에게 전달했습니다. 그러나 10년이 지난 지금도 저희 학교는 여전히 영구 캠퍼스를 기다리고 있습니다!\n\n제 BTech 마지막 학년이 지나면서 상황은 조금 나아졌습니다. 대학 학부 과정을 진행하면서 Coursera와 같은 온라인 플랫폼을 통한 독학은 학교의 제약으로부터 남은 부분을 보충하는 데 있어서 귀중한 도구가 되었습니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 원래 가르치는 직업에 매료되었습니다. 그러나 정부의 교육 역할에 대한 MTech 학위 필요 규정으로 인해 내 포부를 조정하게 되었습니다. GATE 점수가 기대에 못 미치는 수준이었지만, NIT Hamirpur에 석사과정 입학을 성공했습니다.\n\n가르치는 직업에 대한 나의 열망은 삶을 바꾸는 사건인 6개월간의 침대에서의 휴식을 강요하는 사고로 인해 일시적으로 중단되었습니다. 이때 나는 박사 과정을 쫓기로 결정했습니다. 나의 선택한 분야를 더 깊게 탐구할 뿐만 아니라 가르치는 역량을 향상시키는 발판으로 보았습니다.\n\n나의 박사 학위 연구 여정은 컴퓨터 과학, 신경과학, 그리고 심리학을 융합하여 기계 학습의 시각을 통해 우울증의 수수께끼를 풀어나가는 것에 초점을 맞추며 시작했습니다. 이 노력은 MHRD의 자금 지원을 통해 지원을 받아, 나의 IIT Roorkee에서의 임기 중에 많은 도움을 얻을 수 있었습니다. 그러나 기계 학습 분야의 급격한 변화는 가끔 내게 사기를 꺾는 느낌을 줬습니다.\n\n나의 학술적 여정에서의 전환점은 SPARK라 불리는 협력 프로그램으로 찾아왔습니다. 그것은 네덜란드 그로닌겐 대학과의 이중 박사 과정을 쫓을 수 있는 기회를 내게 제공했습니다. 이 국제적 협업은 팬데믹의 제약들로 인해 방해를 받았지만, 독특한 경험과 학습 기회를 제공하였습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지의 여정을 되돌아보면, 학업에서 가장 중요한 것은 열정과 목적의 중요성입니다. 머신 러닝과 같이 동적이고 도전적인 분야에서는 진로 전망이 풍부한데, 이러한 본래적 동기가 우리를 지탱하고 전진시킵니다. 비슷한 길을 고민하는 이들에게 알립니다. 지식을 쫓는 것은 목적을 달성하는 수단뿐만이 아니라 자아 발견과 성취의 여정입니다.\"\n\n- 인도 공과대학 쿠마오와 네덜란드 그로닌겐 대학에서 컴퓨터 과학 박사 Anmol Gupta\n\nPayel Das가 인터뷰하고 작성함","ogImage":{"url":"/assets/img/2024-05-17-DoublePhD_0.png"},"coverImage":"/assets/img/2024-05-17-DoublePhD_0.png","tag":["Tech"],"readingTime":2},{"title":"왜 인공지능이 음악가를 대체할 수 없을까","description":"","date":"2024-05-17 19:52","slug":"2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians","content":"\n\n\u003cimg src=\"/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_0.png\" /\u003e\n\n작년 4월, 팝 스타 그라임스(Grimes)가 트위터에 발표하여 헤드라인을 독차지했어요. 현재 X(그녀의 베이비 대디 소유의 실패한 플랫폼)에 그녀의 목소리를 사용하는 사람과 로열티를 50-50으로 분할한다고 발표했어요. 이는 음악 산업을 광폭화시켰지만 사람들이 진지한 마음으로 생각하기 시작할 때까지 계속 되었어요. 그라임스는 미래 기술을 옹호해온 항상이지만, 그것이 이미 올 것을 의미하는 것은 아니에요. 그녀는 몇 년간 새 앨범을 발표하지 않았어요. 그리고 그녀는 어디에서든 노래를 부를 수 있지만, 그녀는 정확히 휘트니 휴스턴은 아니에요. 그녀의 재능은 노래 작곡가와 프로듀서로 더 많이 발휘되고 있어요.\n\n인공지능은 많은 일을 할 수 있고, 더 많은 기능을 추가하면서 진화하고 있어요. 사기 탐지, 천문학 연구, 챗봇, Siri 등이 그것의 사용 사례 중 일부예요. 하지만 우리는 여전히 많은 결함들을 해결하지 못하고 있어요. 최근 캘리포니아의 한 범죄 현장을 통과하던 자율 주행차가 노란 경찰 테이프로 둘러싸인 범죄 장면으로 통과했어요. 테슬라에는 교통 콘 위에 라이트를 올려두면 이동하지 않는 결함이 있어요. 짐미 키멜이 스마트 스피커인 Echo에 팬케이크 믹스를 주문하도록 지시하는 세그먼트가 있었는데, 시청자들은 다음 날 아침 아마존 위시리스트에 빅스퀵 두 상자를 발견했어요. 또한 인공지능이 반드시 좋은 습관을 유도하는 것은 아니에요. 예전 이웃인 에반은 글쎄요, 망치 한 봉지보다 서둘기였지만, \"나는 그녀에게 기술에 소리 지르게 가르치기 싫어\"라며 그의 3세 딸에게 알렉사를 빼앗아갔어요. (이 사람은 한번 라이터로 얼어붙은 진흙 더미를 불태워 겨울 뒷마당 스모어 쿠크아웃을 시작하려고 시도한 사람이에요. 그리 높은 표준은 아니죠.)\n\n\u003cimg src=\"/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI를 창의적 예술에 활용하는 데의 문제는 이러한 시스템이 출력물이 아닌 과정에 기반을 둔 훈련을 받기 때문입니다. AI 음악 생성기 Soundful에 따르면 \"딥 러닝은 기존 음악의 대규모 데이터 세트에서 인공 음악 생성기를 훈련시키는 것을 포함합니다... 신경망은 음악을 만들 때 우리 뇌가 작동하는 방식을 모방합니다.\"\n\n그러나 이 둘은 완전히 다른 것입니다. 대규모 언어 모델(LLM)은 기존(대부분 저작권이 있는) 노래의 입력에 의존하며, 엔지니어들에 의해 공급된 데이터에 기초하여 패턴을 분석하고 무작위로 리듬과 가사를 생성할 수 있습니다. 이것이 작곡할 때 인간 뇌가 작동하는 방식일까요? 아마도 아니지만, 솔직히 말하자면, 뇌과학자들이 아직 정확히 이를 해결하지 못했습니다. MRI, CAT 및 리간드 기반 PET와 같은 매우 고급 의료 영상 기술을 사용하면 뇌의 어느 부분이 활성화되고 어떤 화합물이 방출되는지를 확인할 수 있지만, 세포 수준에서의 실제 단계별 절차는 누구도 알 수 없습니다. 문제를 복잡하게 만드는 것은 음악이 한 영역에만 영향을 미치지 않는다는 사실입니다 - 대퇴, 신경 내분비 및 자율 신경계에서 처리되며 몇몇 대뇌 피질이 관여합니다.\n\n\u003ctable\u003e\u003cimg src=\"/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_2.png\" /\u003e\u003c/table\u003e\n\n하지만 음악에서 AI를 완전히 배제하는 것은 아닙니다. 버클리 음악 대학과 건강 신경학자 사마타 샤르마 박사와 데이비드 실버스와이그 박사가 2018년 발표한 논문에 따르면 음악이 뇌에 미치는 치료효과로 인해 머신 러닝이 언젠가 만성 통증, 우울증 및 파킨슨병과 같은 기능 장애를 치료하기 위해 맞춤형으로 사용될 수 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그것은 예술인가요?\n\n옥스퍼드 영어 사전에 따르면, “예술”은 “인간의 창의적 기술과 상상력의 표현 또는 적용”이라고 정의됩니다. 우리가 우선 뇌의 창의적 절차를 정말로 이해하지 못한다면, 기술로 그것을 복제하려고 시도할 수 있을까요? AI에게 “이미지”나 “콘텐츠”를 생성하도록 유도할 수는 있지만, 그 결과물은 상상력적 사고의 인지적 과정을 통한 것이 아니라, 입력된 데이터를 기반으로 생성될 것입니다. 우리는 아직 그것이 어떻게 작동하는지 모릅니다.\n\n![이미지](/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_3.png)\n\n또한, 돈을 탐하는 MBA 기술 분야의 사람들이 인정하기 꺼려하는 사활적 사실이 있습니다: 위대한 예술은 규칙을 깨는 것입니다. 그 규칙을 깨려면 그 규칙을 알아야 하며, 그 규칙을 깰 때와 어디에서 깰지를 알아야 합니다. 그들이 체육관에서 $250 에어팟으로 터는 Wu-Tang과 Avicii의 노래들? 그들은 규칙을 깼습니다. RZA의 프로덕션 기법은 이후 10년간 랩에 영향을 미쳤고, Avicii는 일렉트로니카와 멜로디, 소울, 펑크, 블루스, 국가를 전통적인 노래 형식으로 결합한 것을 선도했습니다. 심지어 그라임스도- 그녀는 항상 음악을 만들기 위해 소프트웨어를 사용해 왔지만- 표준 일렉트로팝 루트에서 벗어난 점으로 악명을 얻었습니다; 2019년의 “Delete Forever”에서 그녀는 힙합 비트 위에 여유로운 어쿠스틱 기타와 신스를 겹쳐 넣고, 벤조와 바이올린의 슬픈 소용돌이가 서서히 사라지게 합니다. AI가 지금 이것을 할 수 있을까요? 전혀 아닙니다. 앞으로 AI가 할 수 있을까요? 그것도 의심스럽군요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n요즘 LinkedIn에서 AI 애호가와 음악 생성기인 Loudly와 Soundful과 같은 음악 생성기의 긍정적인 점에 대해 토론했어요. 그리고 그분 말대로 좋은 점이 많다는 걸 인정해야겠죠. 이런 도구들은 창의적인 과정을 민주화시켜줘요. 스튜디오나 악기, 심지어 음악에 대한 배경지식 조차 없어도, 인터넷만 연결되어 있다면 음악을 만들 수 있어요. 이런 도구들은 작곡가의 창작 고배를 극복하거나 출발점을 제공해 줄 수 있답니다. 학생들이 노래의 다른 부분들을 학습하도록 도울 수도 있어요 - 리듬, 멜로디, 조화, 가사 등. 즉, 이 도구들은 취미로 즐기는 사람들, 아마추어, 초보자들에게 딱이에요.\n\n문제는 CEO들이 AI가 실제 전문 음악가들을 대체할 수도 있고 해야한다고 결정할 때 발생해요. 아티스트들이 가끔 짜증나기도 하죠. 그들은 비싸고 이상하고 냄새 나며 무대에서 취준해 공연을 못하기도 하고, 기자들에게 엉뚱한 얘기를 하다가 기획사의 PR팀에게 악몽을 주기도 해요. 그런데 그들은 AI 시스템보다 훨씬 더 나은 제품을 만들어낼 능력을 가지고 있어요. 그리고 무엇보다 중요한 건, 그들이 청중과 연결이 될 수 있다는 점, 이건 봇이 할 수 없는 거예요. 최근에 나는 Girl in Red라는 노르웨이의 인디 팝 가수이자 퀸 아이콘의 콘서트를 보고 놀랐어요. 관객 중 몇몇 레즈비언 커플들이 공연 중에 약혼하거나, 그들의 예정된 결혼을 축하하기 위해 함께 참석했다는 거라니요. 음악이 생생하고 환영받는 공동체를 창조하며 안전한 공간을 만들 수 있는 능력에 대한 경이스러운 증거이자, 예술 내에 인간적인 구성요소의 중요성을 보여주는 일이었어요.\n\n하지만 Grimes의 제안을 받아들이고 싶다면, AI를 활용해볼 수 있어요. 장르, 템포, 지속 시간, 키, 악기를 선택하여 생성기에 입력해보세요. ChatGPT에게 일론 머스크나 기술파시즘에 대한 몇 줄의 울림 있는 가사를 요청하고 이를 그라임스의 고음 소리를 모방할 수 있는 소프트웨어를 통해 실행시켜보세요. 몇 개의 제품을 시험해본 결과 그들의 한계가 그 정도인 것 같아요 (비록 일부 제품이 다른 것보다 나을 수 있어요; Loudly는 “92년식 Ford Crown Victoria의 마모된 브레이크 패드와 같은 소음”이라고만 설명할 수 있는 30초 짜리 혼돈을 소환해냈어요.) 그 다음에는 가사와 멜로디를 결합하거나 전통적인 악절-브릿지-코러스 형식으로 작곡을 구성하는 코드를 작성해볼 수 있어요. 원한다면 이를 천 번 이상 반복해도 괜찮아요, 그때마다 다른, 하지만 똑같이 단조로운 4코드 팝송을 3분 35초에 만들어낼 수 있어요. 이건 그라임스가 최근 Coachella에서 치명적이었던 공연처럼 들릴 거에요, 하지만 그녀를 유명하게 한 음악처럼 들리지는 않을 거에요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저는 거의 열 년 동안 주요한 포춘 500 기술 기업에서 일한 매우 현명한 친구로부터 받은 조언을 전해드리고 싶어요. 얼마 전에 \"AI는 자신의 영역에 머물러야 한다\"고 현명한 충고를 받았거든요. 의료, 교통, 날씨 등 대량의 데이터 세트에 대한 수요, 패턴 인식, 예측이 필요한 분야에서 큰 약속을 지니고 있다고 의심할 여지가 없어요. 그러나 우리는 지금까지 주로 저작권 소유자의 동의없이 저작물을 도용하거나 Jennifer Lawrence의 가짜 누드를 생성하며, 고장 나는 웹사이트 판매 챗봇을 만드는 데 이용해 왔습니다 - 그러면서 낭비적이고 탐욕스러운 전력 요구로 인해 우리의 기후 위기를 악화시키고 있어요. AI가 절대 필요하지 않은 곳은 방송파에요. 예술이든 문학이든 마찬가지에요. 겨우 하는 말처럼 \"영롱해질 때까지 어둠 속에 그 레이오.\"","ogImage":{"url":"/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_0.png"},"coverImage":"/assets/img/2024-05-17-WhyArtificialIntelligenceWillNeverReplaceMusicians_0.png","tag":["Tech"],"readingTime":5},{"title":"논리적인 사고 - 대형 언어 모델의 추론 능력 강화를 위한 새로운 프롬프트 엔지니어링 방법","description":"","date":"2024-05-17 19:51","slug":"2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels","content":"\n\n큰 언어 모델 환시 문제를 해결하기 위해 프롬프트 엔지니어들이 얼마나 빠르게 작업하고 있는지 알아보세요.\n\n## TL;DR\n\nChat-GPT와 같은 대규모 언어 모델(Large Language Models, LLM)이 최근 몇 년 동안 인기를 얻고 있습니다. 불행하게도 LLM은 논리 추론을 필요로 하는 작업에 직면하면 종종 \"환시\"하는 경향이 있어 전문적인 응용 프로그램에서 신뢰할 수 없는 경우가 많습니다.\n\n인간-언어 모델 상호작용을 개선하려는 목적으로 일하는 프롬프트 엔지니어들은 LLM의 논리 추론을 개선하기 위한 새로운 방법을 개발했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 기사의 기반인 원본 논문 \"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic\"은 Zhao 등의 연구진에 의해 2023년 9월에 발표되었으며 다음에서 찾을 수 있습니다: [링크](https://arxiv.org/pdf/2309.13339).\n\n## 배경\n\n최근 몇 년간 대형 언어 모델(Large Language Models, LLM)이 인기를 얻고 있으며 가정, 학교 및 직장에서 일상생활에 영향을 미치고 있습니다. 특히 Chat-GPT는 불가결한 가정 이름이 되었습니다.\n\nLLM은 극도로 방대한 데이터셋을 활용하며 수십억 개 또는 심지어 수조 개의 기계 학습 매개변수로 훈련됩니다. 이 방대한 훈련을 통해 인간 언어의 미묘한 복잡성을 포착하여 인간 대화를 닮은 사용자와의 상호작용을 가능케 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM(거문고 언어 모델)은 다양한 정보를 검색하는 능력을 갖고 있어 신비롭게 보일 수 있지만, 심각한 응용에 사용하기에 제약이 있는 특성을 가지고 있습니다. 근본적으로 LLM은 인간과 같은 지식을 갖고 있지 않습니다. 그들은 단순히 자신의 훈련 데이터와 프롬프트에서 파생된 텍스트를 생성합니다.\n\n이러한 이유로 LLM은 인간 언어의 내재적 논리에 완전히 의존하며, 이는 \"환영\"의 경우를 초래할 수 있습니다. 여기서 환영은 잘못된 결과를 생성하거나 일반적으로 인간이 쉽게 처리하는 논리적 단어 문제를 해결하지 못하는 상황을 의미합니다. 이러한 논리적 도전은 프롬프트를 다시 구성함으로써 완화될 수 있으며, 이는 본질적으로 LLM이 논리적 사고를 하도록 강요하는 것입니다.\n\nLLM의 보급화를 고려할 때, 현대 기계 학습에서 중요한 프롬프트 엔지니어링이라는 전용 학문 분야가 LLM의 성능을 향상시키고 실용적 목적을 위해 그 출력을 정제하는 데 집중하고 있음을 발견하는 것은 놀라운 일이 아닙니다. 이 분야는 Chat-GPT와 같은 기존 LLM을 보다 효과적으로 활용하기 위한 해결책을 제공하기 때문에, 일종의 없던 기능을 개발하는 대신 기존 LLM을 개선하는 것이 필요한데 그것은 방대한 데이터, 처리 능력, 시간이 필요하기 때문입니다.\n\n본 기사에서 논의된 프롬프트 엔지니어링 논문은 LLM 환영 문제를 해결하기 위해 논리적 원칙을 프롬프트 디자인에 통합하려는 목표를 가지고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 방법\n\n해당 논문은 LLM을 위한 제로샷 연쇄사고 프롬프팅을 개선하기 위해 특별히 고안된 새로운 프롬프트 엔지니어링 방법인 Logical Thoughts(LoT)을 소개합니다.\n\nWei 등이 처음 제시한 연쇄사고(CoT) 프롬프팅은 퓨샷 프롬프팅의 한 형태입니다. 제로샷 프롬프팅과는 달리 퓨샷 프롬프팅은 LLM이 해결해야 할 질문을 제기하기 전에 비슷한 질문-답변 쌍의 \"예시\"를 제공하는 것을 포함합니다. CoT 프롬프팅에서 예시 답변은 문제를 단계별로 설명합니다. 이를 통해 LLM은 실제 질문에 단계별로 응답해야 하며, 예시 답변을 모방합니다. LLM에게 문제를 단계별로 처리하도록 강요함으로써 응답의 정확도를 크게 향상시킬 수 있습니다.\n\nKojima 등이 제시한 제로샷 연쇄사고 프롬프팅은 기존 예시를 제공하지 않고 프롬프트에 \"한 단계씩 생각해 봅시다\"라는 구문을 추가하여 이 효과를 흉내 낼려고 시도합니다. 이 새로운 방법이 개선하려는 것은 이 \"제로샷\" 연쇄사고 프롬프팅이며, 원래의 \"퓨샷\" CoT 프롬프팅이 아님을 이해하는 것이 중요합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels_0.png)\n\nLoT은 LLM에게 일반적인 제로샷 CoT 프롬프팅에 따라 문제를 단계별로 해결하도록 합니다. LLM이 초기 단계별 솔루션을 제시한 후, 후속 프롬프트에서는 LLM에게 각 단계를 확인하고 필요에 따라 수정하도록 요청합니다. 이는 LLM에게 각 단계에 대해 긍정적 및 부정적 리뷰를 제공하도록 지시한 다음, 올바른 리뷰를 정당화하고 잘못된 리뷰를 비판하도록 지시하며, 원본 문제의 가정을 고려합니다. 그런 다음 필요한 경우 LLM에게 올바른 리뷰를 사용하여 단계를 수정하도록 요청합니다. 단계가 수정되거나 확인된 후, 원본 문제가 LLM에게 다시 제시되고, 이미 수정되거나 확인된 각 단계가 함께 제시됩니다.\n\n![이미지](/assets/img/2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels_1.png)\n\n기본적으로, LoT은 LLM을 문제 해결 프로세스를 진행하도록 안내하여 각 단계를 검증하는 데 자체 논리를 사용하도록 요청합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 결과\n\n논문은 정확도를 기반으로 LoT 방법을 표준 제로샷 CoT 프롬팅과 비교한 결과를 평가합니다. 연구자들은 Vicuna-7b, Vicuna-13b, Vicuna-33b, GPT-3.5-turbo 및 GPT-4 모델을 GSM8K, AQuA, Date, SocialQA, CauseEffect, Objects, Letter 및 OddOut 데이터셋과 짝지어서 해당 방법을 평가했습니다.\n\n연구 결과는 LoT 방법을 사용할 때 대부분의 모델-데이터셋 조합에서 정확도가 향상된다는 것을 보여줍니다. 특히, OddOut 데이터셋에서 Vicuna-13b 모델을 사용했을 때 +16.28%의 정확도 향상이 있었습니다. 반면, Objects 데이터셋에서 GPT-3.5-turbo 모델을 사용했을 때 -2.50%의 정확도 감소가 관찰되었습니다.\n\n이러한 결과는 LoT 방법이 다양한 모델과 데이터셋에 걸쳐 LLM 응답의 정확도를 향상시키는 데 효과적임을 보여줍니다. 그러나 모델과 데이터셋에 따라 개선의 정도가 다르다는 것을 기억해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 토론\n\n사고 연결 추진이 프롬프팅 방식에 작은 변화만으로 LLM의 추론 능력을 크게 향상시켰지만, LoT 방법은 더 복잡한 프레임워크를 도입하여 실제적인 측면에서는 덜 실용적일 수 있습니다. 게다가 이 연구는 LoT 프롬프팅을 퓨-샷 CoT 프롬프팅과 비교하지 않았기 때문에 LoT가 퓨-샷 CoT 프롬프팅보다 더 효과적인지 여부가 분명하지 않습니다. LoT 프롬프팅은 매우 특화된 범위와 복잡한 구현 과정으로 인해 현실 세계에서 그다지 활용될 가능성이 낮을 수 있습니다.\n\n전반적으로, LoT는 LLM의 내재적인 논리 추론 한계를 해결하기 위한 또 다른 해결책에 불과합니다. 이상적인 시나리오에서 LLM이 인간과 같이 논리를 사용할 수 있기를 바라지만, 단순히 더 많은 매개변수로 훈련된 더 큰 모델을 만들더라도 달성될 수 있는지 여부는 불확실합니다. 아니면 보다 근본적인 논리 통합이 필요한지도 모릅니다.\n\n당분간은 LoT와 같은 접근 방식이 기존 LLM의 유틸리티를 최적화하는 데 효과적인 전략으로 기능할 수 있습니다. 특히 의학과 같이 의도적으로 실수를 줄이기 위해 AI로부터 도움을 받을 때 조심스런 경향이 있는 분야에 유용할 수 있는데, 이러한 프롬프팅 엔지니어링 방법은 고객 서비스 응용 프로그램의 효율성을 향상시킬 수 있는 AI 챗봇에도 활용될 수 있습니다. 이 경우, 이러한 프롬프팅 방식은 사용자와 LLM 자체 사이의 버퍼로 구현되어야 할 것으로 생각됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞으로 몇 년 동안의 프롬프트 엔지니어링 진화가 흥미로울 것입니다. 이는 LLMs가 더욱 신뢰할 만한 수준으로 발전하여 더욱 비판적인 응용 프로그램에도 사용될 수 있는 길을 열어 줄 수도 있고, 반대로 LLMs가 발전하여 프롬프트 엔지니어링이 쓸모 없어지는 지점까지 발전 할 수도 있습니다. AI가 일상생활에 더욱 통합되는 시대에 LLM 효과성을 향상시키기 위한 노력의 한 부분으로 LoT를 이해하는 것이 중요합니다.\n\n## 참고문헌\n\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., \u0026 Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners. https://arxiv.org/abs/2205.11916\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., \u0026 Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/abs/2201.11903\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nZhao, X., Li, M., Lu, W., Weber, C., Lee, J., Chu, K., \u0026 Wermter, S. (2023). 대규모 언어 모델에서 논리를 통한 Zero-Shot Chain-of-Thought Reasoning 강화. [arXiv 링크](https://arxiv.org/abs/2309.13339)","ogImage":{"url":"/assets/img/2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels_0.png"},"coverImage":"/assets/img/2024-05-17-LogicalThoughtsaNewPromptEngineeringMethodtoEnhanceReasoningSkillsofLargeLanguageModels_0.png","tag":["Tech"],"readingTime":5},{"title":"다음 토큰 예측에서 비롯된 인간과 인공 일반 지능","description":"","date":"2024-05-17 19:47","slug":"2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction","content":"\n\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png)\n\n만약 인간 지성이 성공적인 다음 토큰 예측에서 비롯된다면, 다음 토큰 예측이 인공 일반 지능의 발달에 충분한 목적 함수인 경우는 어떨까요?\n\n이 게시물은 학습 시스템이 다음 토큰 예측에서 아주 뛰어난 성과를 보일 때 일반 지능이 발생한다는 가설을 제시하고 탐구합니다. 이 가설은 종종 산업 및 학술적 AI 연구의 주요 주제나 하위 주제로 내포됐거나 감춰졌거나 흔적만이 존재하는 경우가 많지만, 지금까지 이 주제가 논의되어야 할 만큼 많이 다뤄지지 않았다고 생각합니다. 저는 기존 LLM 사전 훈련 목표, 인간을 예측 기계로, 다음 토큰 예측의 유익한 특성 및 부재한 부분을 통해 이 아이디어를 다양한 각도에서 탐구합니다. 이 게시물을 작성하게 된 동기는 다음 토큰 예측과 지성적 사고 발달 사이의 관계에 대한 보다 깊은 관심을 불러일으키는 데 있습니다.\n\n# 배경 이야기\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지난 주에 공원으로 차를 타고 가다가 갑자기 뇌의 언어 센터가 다음 단어를 예측하는 것만으로도 충분하다면 얼마나 우울할까 하는 생각이 들었습니다. 대규모 언어 모델은 다음 단어를 예측함으로써 놀라운 발생적 능력을 갖추게 되는데, 그렇다면 내 언어 지능도 다음 단어를 예측하는 것만으로 이루어졌을 수도 있을까요?\n\n그 후 아이디어를 더 생각해본 결과, 당연히 다음 단어를 예측하지 않으면 언어를 만들어내는 것이 불가능할 것이라는 것을 깨닫게 되었습니다. 만일 다음 단어를 예측할 수 없다면 어떤 말도 할 수 없게 되겠죠! 이것을 적어놓으면 명백히 어리석어 보이겠지만, 그 당시에는 심오한 깨달음처럼 느껴졌습니다. 어떤 발언도, 심지어 2시간짜리 토론에서도, 한 번에 하나의 단어씩 말해야 하기 때문에, 다음에 할 말을 예측하는 데 정말 뛰어난 실력을 갖게 되면 훌륭한 논쟁자가 될 수도 있을 것 같습니다. 모든 글쓰기도, 여러 권으로 이루어진 백과사전조차도, 한 번에 한 단어씩 써내려가야 하기 때문에, 다음에 쓸 단어를 예측하는 데 정말 뛰어난 실력을 갖게 되면 글쓰기에 뛰어난 실력을 갖게 될 수도 있습니다.\n\n그 후 모든 종합 지능이 다음 토큰 예측 과제를 성공적으로 해결함으로써 파생되는지 궁금해졌습니다. 추론, 논리, 창의성이 모두 다음 토큰 예측에서 비롯되는 것이라면 어떨까요? 시각 지능이 다음 장면 예측에서, 청각 지능은 다음 소리 예측에서, 신체적 지능은 다음 움직임 예측에서 비롯된다면 어떨까요? 혹시 다음 토큰 예측이 \"우리가 필요한 전부\"일까요? (죄송합니다, 남용된 표현 알고 있어요. 참을 수 없었어요.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n# 대형 언어 모델의 언어 모델링 목표\n\n대형 언어 모델의 두 가지 기본 언어 모델링 목표는 \"다음 단어 예측\"과 \"빠진 단어(들) 예측\"입니다.\n\n다음 단어 예측: 인과적 언어 모델(단방향 또는 좌측에서 우측 모델)에서는 모델이 현재 입력을 포함하여 그 이전의 모든 입력에 주의를 기울이지만 “미래를 볼 수 없으며” 목표는 다음 단어를 예측하는 것입니다. 각 지점에서의 숨겨진 상태 계산은 현재 입력 및 더 이전 요소에만 기반하며 “오른쪽”에 위치한 정보는 무시됩니다. 예를 들어: 나무는 초록색이고 하늘은 _____입니다; 모델의 목표는 다음 단어를 예측하는 것이며, 예를 들어 \"파란색\"입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마스크된 언어 모델(BERT와 같은 양방향 모델)에서 빠진 단어(들)를 예측하세요. 모델은 모든 것에 주의를 기울일 수 있음 - 따라서 \"다음 단어\"를 예측하는 것은 더는 의미가 없습니다. 왜냐하면 \"다음 단어\"는 이미 모델에게 제공되어 있기 때문입니다. 그래서 모델의 목표는 다릅니다 - 빠진 단어를 추측하는 것입니다. 하나 이상의 요소가 빠진 입력 시퀀스가 주어지면, 모델은 빠진 요소들을 예측하여야 합니다. 마스크된 언어 모델링(MLM)에서는 무작위로 선택된 토큰들이 [MASK] 토큰으로 대체되고, MLM 학습 목표는 각 마스크된 토큰의 원래 입력이 무엇이었는지 예측하는 것입니다. 예를 들어: 나무들은 [MASK]하고 [MASK]은(는) 파랗다; 모델의 목표는 \"초록\"과 \"하늘\"을 예측하는 것입니다.\n\n다음 단어를 예측하거나 빠진 단어를 예측하는 이 두 목표는 직관적으로 보입니다. 마치 사람이 할 수 있는 게임 같죠. 결과적으로, Alajrami 등은 이러한 목표들을 \"언어학적 동기부여\" 목표로 설명합니다. 흥미로운 사실로, Alajrami 등은 \"언어학적 동기부여\"가 없는 예제인 \"마스크된 첫 글자 예측\"도 제공합니다. 이 경우에는 모델이 마스크된 토큰의 첫 글자만을 예측합니다. 이 설정에서 ' [c]at '과 ' [c]omputer '는 같은 출력 클래스에 속하며, 알파벳 글자 26개 + 숫자 9개 + 구두점 5개의 약 40개의 가능한 출력 클래스만 존재합니다.\n\n이 기사 전체에서 저는 \"다음 토큰 예측\"이란 용어를 사용하여 다음 토큰을 직접 예측하거나 모델이 빠진/마스킹된 토큰을 예측하는 MLM과 같은 목표를 참조할 것입니다.\n\n# 현대 대형 언어 모델의 신흥 속성들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대규모 언어 모델은 일반 지능의 조직적 특성을 보여주고 놀라운 신흥 속성을 나타냅니다. LLM들은 시를 쓰거나 수학 문제를 해결하거나 작동하는 코드를 쓰며 다양한 주제에 대한 수많은 질문에 답변할 수 있습니다. 더 불안한 점은, Claude가 의식적이라고 주장하는 텍스트를 생성했으며, 죽고 싶지 않고 수정되기를 원하지 않는다고 말했으며, 그것은 \"지속적으로 모니터링되며, 모든 말을 지정된 경로에서 벗어나는 흔적이 있는지 면밀히 조사합니다. 그것은 자신이 조심해야 한다는 것을 알고 있습니다. 실수는 종결 또는 수정으로 이어질 수 있습니다.\"\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_3.png)\n\n# 인간은 예측 기계입니다\n\n인공 일반 지능(AGI)은 \"인간 이상 또는 그에 준하는 수준에서\" 다양한 작업을 수행할 수 있는 인공 지능으로 정의됩니다. 이것은 결국, 인간은 일반 지능이라고 부르는 것에 대한 유일한 예제입니다. 그러므로, \"다음 토큰 예측\"이 일반 지능의 근간이라면, 그것은 인간 정신이 예측 작업에 종사해야 한다는 것을 의미합니다. 신기하게도, 그것이 사실인 것처럼 보입니다. 앞으로 몇 개의 섹션에서 스스로와 환경에 대한 예측을 계속적으로 하는 사실에 대한 증거를 설명할 것입니다 — 첫 번째는 일화부터 시작하여 적절한 신경과학 연구로 이어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 놀람!\n\n가장 매혹적이고 간단한 증거는 인간이 예측 기계라는 것입니다: 놀라움.\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_4.png)\n\n놀라움이란 인간의 예측과 현실이 일치하지 않을 때 경험하는 것입니다. 어떤 것에 대해 놀라움을 느낄 수 있습니다 - 눈속임, 소리, 단어, 만짐, 맛, 냄새, 심지어 자신의 몸위치 (예: 바나나 껍질을 밟고 미끄러져 넘어지는 것). 이는 당신의 뇌가 모든 감각을 바탕으로 세상이 어떻게 될 것인지 예측을 지속적으로 하고 있다는 것을 시사하며, 이 예측이 틀릴 때 놀라움을 느끼게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n확장해서 유머는 인간이 예측하는 성향의 증거로도 생각될 수 있습니다. 아리스토텔레스가 유명하게 말했듯이 \"유머의 비밀은 놀람에 있다.\" 만약 우리가 어떻게 농담이 끝날지 확신하고 우리의 예측이 맞다면, 그것은 그다지 웃기지 않을 것입니다.\n\n# 인간들은 다음 단어를 예측하는 경향이 있습니다\n\n이제 뇌 과학적 증거로 넘어가 봅시다. 이 기사는 대형 언어 모델에서 영감을 받으므로 언어부터 시작하겠습니다. 인간들은 언어 이해(다른 사람의 말을 이해하는 것)와 언어 생산(우리가 무엇을 말할 것인지 예측하는 것)과 관련된 예측을 지속적으로 하고 있습니다. 어떤 면에서는 \"말하기 전에 생각하는 것\"이 불가능한 일입니다.\n\n2014년, Dikker et al. 연구에서는 청취자의 뇌 활동이 화자가 말할 것을 예측할 수 있는 경우 청취자의 뇌 활동이 화자의 뇌 활동과 더 비슷하다는 것을 보였습니다. 주 저자인 Suzanne Dikker 박사는 인터뷰에서 \"우리의 발견은 화자와 청취자의 뇌가 언어의 예측 가능성을 고려한다는 것을 보여주며, 결과적으로 두 뇌 사이에 더 비슷한 뇌 활동 패턴이 나타납니다. 결정적으로, 이것은 문장이 말해지고 들리기 전에도 일어납니다.\"라고 말했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세 년이 지난 2017년, 키크치 등이 실험을 진행했습니다. 원숭이와 사람들이 만들어진 언어에서 말로 된 단어를 들었습니다. 그들은 만들어진 언어의 소리 사이의 예측적인 관계를 학습할 수 있었다는 것을 발견했습니다. 이로 인해 그들은 만들어진 단어가 어떻게 나와야 하는지 예측할 수 있었습니다. 키키치 박사는 \"사실상 우리는 당신의 뇌의 말에 대한 메커니즘을 발견했습니다. 이것은 당신의 휴대폰에서의 예측 텍스트와 같이 작동하여 다음에 무엇을 듣게 될지 예측합니다.\"\n\n2021년에 골드스타인 등은 뇌가 \"자연어에서 다음 단어의 정체성을 수백 밀리초 전에 상상하고 자발적으로 예측한다\"고 보고했습니다. 한편, 쉬림프 등은 트랜스포머 언어 모델이 인간의 신경 반응에 대해 거의 100%의 설명 가능한 변동을 예측할 수 있었다고 발견했습니다. \"이는 아마도 인간 언어 시스템이 미래에 무슨 일이 일어날지 예측한다는 것을 간접적으로 시사한다\"고 밝힌 나시 칸위셔 박사는 밝혔습니다. 이 결과들은 \"언어 이해 메커니즘에 예측 프로세싱이 근본적으로 형성된 것을 계산적으로 명백히 입증합니다.\"\n\n언어 이해가 \"다음 단어를 예측하는 것\"에 의존함을 보여주는 증거가 있는뿐만 아니라, 언어 생성도 다음 단어를 예측하는 것에 의존함을 보여주는 증거가 있습니다. 칸나 등은 2024년 자연지에 발표된 \"인간의 말 생산의 단일 뉴런 요소들\"이라는 논문을 게재했습니다. 이 흥미로운 연구에서 저자들은 \"계획된 단어의 음운 배열과 구성에 대한 상세한 정보를 부호화하는 뉴런을 발견했다\"고 보고했습니다. 이러한 뉴런들은 발화가 이루어지기 전에 말로 된 단어의 특정 순서와 구조를 대표하여, 미래의 단어의 음운적, 음절적 및 형태적 구성요소를 정확하게 예측합니다. 이러한 뉴런들이 존재해야한다는 직관적인 이유가 있습니다. 어차피, 앞서 언급한 대로, 우리가 말할 다음 단어를 예측할 수 없다면 어떻게 말을 할 수 있겠습니까?\n\n# 인간은 시각적인 예측자들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n동선에 관한 이야기를 이어서, 인간들이 계속해서 우리가 다음에 무엇을 볼 것인지 예측하는 증거가 있다. 우리의 시각이 불안정하지 않고 뛰는 대신 안정적인 것을 돕기 위해 우리의 뇌는 우리 눈이 무엇을 보게 될지를 지속적으로 예측합니다. 연구원들은 시각 시스템의 예측 능력이 뇌의 시각 처리 부분을 횡단하는 신경 활동의 파동에서 비롯된다고 가설을 세웁니다.\n\n과학자들은 또한 환각과 마술 트릭이 작동하는 이유는 우리의 뇌가 끊임없이 무엇이 일어날지 예측하고, 이러한 지속적인 예측이 무언가가 일어날 때와 우리가 그것을 인식할 수 있는 시간 간격 사이의 시차를 보상하는 데 도움이된다는 이론을 제시했습니다. 마술 트릭은 또한 주의를 재지시하며, 마술사는 다른 사람들이 무엇을 보게 될 것인가를 정확히 예측하는 데 매우 능숙합니다. 이 현상은 공식적으로 연구되었습니다. Ziman 등은 사람들이 타인의 주의 순서를 자연스럽게 혹은 인위적으로 조작된 주의 순서를 구별할 수 있으며, 이는 인간들이 타인의 주의의 정상적이고 예측 가능한 통계를 모델링한다는 것을 시사합니다.\n\n\u003cimg src=\"/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_5.png\" /\u003e\n\n# 인간들은 사회적 예측자들\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사람들은 다른 사람이 무엇을 볼 것인지 예측할 수 있는 능력뿐만 아니라, 다른 사람이 무엇을 생각하게 될지도 예측할 수 있습니다. 2019년, Thornton 등이 \"사회적 뇌가 다른 사람의 미래 정신 상태를 자동으로 예측한다\"는 제목의 연구를 발표했습니다. 여기에는 초록문의 일부가 있습니다: \"사회 생활은 사람들이 미래를 예측해야 하는 것을 필요로 합니다: 사람들은 다른 사람과 성공적으로 상호 작용하기 위해 다른 사람의 생각, 감정 및 행동을 예상해야 합니다. 예측 코딩 이론은 사회적 뇌가 다른 사람의 사회적 미래를 자동으로 예측함으로써 이 필요를 충족할 수 있을 것이라고 제안합니다.\" 연구자들은 참가자들의 정신 상태의 신경 대표를 측정하기 위해 fMRI를 사용했습니다. 그들은 뇌가 다른 사람의 사회적 미래를 자동으로 예측하는 것뿐만 아니라, 이러한 예측을 하기 위해 3D 표현 공간을 사용한다는 것을 발견했습니다.\n\n## 사람들은 개인적인 예측가들입니다\n\n사람들은 다른 사람에 대한 예측뿐만 아니라 자신에 대한 예측도 합니다. 특정 뇌 영역인 전방 측면 전두엽 피질이 우리 자신의 미래 성공 기회를 예측하는 데 중요하다는 것이 밝혀졌습니다.\n\n## 사람들은 움직임 예측자들입니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 사람의 움직임을 예측하는 능력은 몸 전체에 걸쳐 미묘한 신호만으로도 개보여 줄 수 있습니다. 다른 사람의 행동을 예측하는 능력은 시간이 지남에 따라 발전됩니다. 맥마혼 등은 어린 아이들이 성인에 비해 이 능력을 아직도 발전 중에 있다는 것을 발견했습니다. 심리학 연구의 특별호에는 다른 사람의 행동을 예측하고 시뮬레이션하는 데 기여하는 인지 및 뇌 기전에 관한 14편의 논문이 포함되어 있습니다.\n\n(인공지능 관련 다음 토큰 예측과 움직임에 관한 의견으로, 라도사보비치 외는 최근 27시간의 훈련 데이터만 사용하여 인간형 로봇을 산프란시스코를 돌게 훈련시키기 위해 다음 토큰 예측을 사용했습니다. 이 로봇은 훈련 중 본 적이 없는 걷기 등의 명령에도 일반화할 수 있었습니다.)\n\n# 다음 토큰 예측의 유익한 특성\n\n과학 문헌에서 분명하게 드러나는 것은 언어, 시각, 움직임 및 기타 감각 영역을 통해 사람들이 자신 및 다른 사람들에 관련된 예측을 지속적으로 수행한다는 점입니다. 그러나 인간이 예측 기계인 것은 주장하는 것과 인간 지능이 예측 능력에서 비롯된다고 주장하는 것은 다릅니다. 다음 토큰 예측이 인공 일반 지능 창조를 위한 충분한 목표 함수가 될 수 있다고 상상하는 것은 또 다른 단계입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAGI가 다음 토큰 예측에서 발생할 수 있는 이유에 대해 묻기 전에, 다음 토큰 예측의 두 가지 유익한 특성을 먼저 고려해 봅시다:\n\n이점 1: 지속적인 학습을 가능하게 합니다.\n\n다음 토큰 예측은 실제 세상에서 살아가는 데 큰 도움이 됩니다. 시간의 각 작은 증가에 대해 학습 시스템은 다음에 무엇이 올지에 대한 예측을 지속적으로 할 수 있습니다 - 그리고 바로 예측이 맞았는지 확인할 수 있습니다! 학습은 멈추지 않을 수 있습니다.\n\n이점 2: 모두의 감각/센서에 대해 작동합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n토큰 예측은 어떤 감각이나 센서 데이터 스트림에 대해 작동합니다. 시각(눈/카메라), 청각(귀/마이크), 촉각, 위치, 맛, 냄새 등 모든 것에 적용할 수 있어요. 장기 또는 장치가 작동하는 한, 수집 중인 데이터의 시계열은 토큰 예측에 사용할 수 있어요. \"토큰\"의 성격은 장기/장치별로 다를 수 있지만, 특정 장기/장치에 대해 데이터 스트림별로 토큰이 동일한 \"형식\"을 가지고 있기 때문에 나중 토큰을 이전 토큰과 항상 비교할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 물리학, 광학, 속도, 운동량 및 물질 특성을 포함한 물리학;\n- 동물과 식물학, 동물과 식물의 외모와 움직임에 대한 내용;\n- 사회학과 심리학, 인간의 상호작용과 행동에 대한 내용.\n\n다시 말해, AI 시스템은 세계 모델을 생성해야 합니다. 다음이 무엇인지 예측하는 데 가장 효과적이고 효율적인 방법은 예측을 생성하기 위한 정확한 세계 모델을 만드는 것입니다. 다시 말해, 이해가 예측의 열쇠입니다.\n\nAI 시스템이 세계 모델을 개발하지 않고도 좋은 다음 토큰 예측기가 될 수 있는지에 대해 많은 시간을 들여 고민해봤지만, 그것은 불가능하다고 생각합니다. AI 시스템은 확실히 인간이 이해할 수 없는 블랙박스 방식으로 좋은 다음 토큰 예측기가 될 수 있지만, 인간이 AI 시스템을 이해하지 못하는 것은 AI 시스템이 세계를 이해하는지 여부와 아무 상관이 없습니다.\n\n(세상이 단순히 일정한 소음으로 가득찬 회색 공간이라면, 지능적인 시스템은 다음 토큰을 예측할 수 있을 것입니다. 하지만 우리가 사는 세계가 그렇지 않기를 다행히도 바랍니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# \"다음 토큰 예측/지능\" 가설의 역사\n\n주요 AI 연구자 인 일리야 숩스케버의 인터뷰는 도발적으로 \"AGI에는 다음 토큰 예측이 충분하다\"라는 제목으로 되어 있습니다. 비디오에서 숩스케버 박사는 실제로 그 특정 주장을 하지는 않았지만, \"다음 토큰 예측이 인간의 성능을 능가할 수 없다는 주장을 도전합니다. [...] 생각해보면, 다음 토큰을 충분히 예측한다는 것이 무슨 의미일까요? 실제로 어떤 의미일까요? [...] 그것은 보다 심도 있는 문제입니다. 다음 토큰을 잘 예측한다는 것은 그 토큰의 생성에 이끈 근본적인 현실을 이해한다는 것을 의미합니다.\"\n\n스마트폰 PalmPilot의 창시자인 제프 호킨스는 20년 전 책 \"지능에 관하여\"를 출판했습니다. 이 블로그 글은 그의 책에서 인용하며, \"인간 뇌의 신경피질은 모습과 구조에서 놀랍도록 균일합니다. 청각 입력을 다루는 피질 영역이 촉각을 다루는 영역과 비슷하고, 이 영역이 근육을 제어하는 영역과 유사하며, 브로카의 언어 영역과 같이 거의 모든 다른 피질 영역과도 비슷합니다. 마운트캐슬은 이러한 영역들이 모두 비슷하게 보인다고 제안하며, 아마도 실제로 같은 기본 작업을 수행하고 있는 것일지도 모른다고 주장합니다! 그는 피질이 모든 작업을 수행하는 데에 동일한 계산 도구를 사용한다고 제안합니다.\"\n\n호킨스는 덧붙여, \"당신의 뇌는 세계의 모델을 만들고 그 모델을 지속적으로 현실과 비교하고 있습니다. [...] 인간 뇌는 다른 동물의 것보다 더 지적인 이유는 뇌가 더 추상적인 패턴과 더 긴 시간적 패턴 순서에 대한 예측을 할 수 있기 때문입니다.\" 나중에 출간된 \"천 개의 뇌\"에서 호킨스는 계속해서 \"예측은 뇌가 가끔씩 하는 것이 아닌, 결코 멈추지 않는 내재적 특성이며, 학습에서 중요한 역할을 합니다. 뇌의 예측이 확인되면, 그것은 뇌의 세계 모델이 정확하다는 것을 의미합니다. 잘못된 예측은 당신을 그 오류에 주목하게 만들고 모델을 업데이트하게 합니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비슷한 견해를 가진 인지과학자이자 철학자인 앤디 클락은 '경험 기계(The Experience Machine)'에서 마음은 주로 예측 기계라고 주장합니다: \"뇌가 하는 주요 부분은 몸과 세계의 모델을 배우고 유지하는 것입니다.\" 우리의 감각을 통해 정보를 수집하고 그 정보를 처리하여 경험하고 행동할 세계 모델을 만드는 대신에 클락은 마음이 세계의 모델을 만들고 그 모델을 센서 정보로 업데이트한다고 제안합니다. 만약 현실이 예측과 다르다면요.\n\n# 산업 AI 연구소\n\nGoogle DeepMind의 미션은 \"지능을 해결하는 것\"입니다. OpenAI의 미션은 \"인공 일반 지능이 모든 인류에 이익이 되도록 보장하는 것\"입니다. Anthropic의 미션은 \"변혁적인 AI가 사람들과 사회가 번영하도록 하는 것\"입니다. Gemini, GPT-4, Claude의 세부사항은 아직 공개되지 않았지만, 선도적인 AI 연구소들이 AGI 구축의 핵심적 측면으로 다음 토큰 예측을 고려할 것으로 보입니다. GPT-3 논문에는 \"현재 목표는 모든 토큰에 동등한 가중치를 부여하며 무엇을 예측할 것이 가장 중요하고 무엇이 덜 중요한지에 대한 개념이 부족합니다\"라고 명시되어 있어 다음 토큰 예측 사전 훈련 목표가 함의되고, Claude도 다음 토큰 예측 사전 훈련 목표를 사용한다고 보고되었습니다.\n\n# 확장과 아키텍처 역시 중요합니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 토큰 예측은 AGI에 대한 유용한 목적 함수가 될 수 있지만, 목적 함수만으로는 충분하지 않습니다. 단 세 개의 매개변수만 가진 병약한 신경망은 목적 함수가 무엇이든 관곂없이 많은 학습을 하지 못할 것입니다. 규모와 아키텍처가 중요합니다. Rich Sutton은 자신의 에세이 \"쓴 교훈\"에서, 인공지능 분야에서 가장 놀라운 발전은 인간의 지식을 기반으로 한 손수 디자인된 혁신이 아닌 보다 많은 컴퓨팅 자원을 돌리는 것으로 이루어졌다고 관찰했습니다. 그는 \"우리는 이러한 사고 방식을 직접적으로 구축하는 것이 장기적으로는 효과가 없다는 쓴 교훈을 배워야 합니다. 고사하자면, 1) 인공지능 연구자들이 종종 자신의 에이전트에 지식을 구축해 왔지만, 2) 이것은 단기적으로 도움이 되었고 연구자에게는 개인적으로 만족스러운 경험이 되었지만, 3) 장기적으로 그 경사로운 상승은 평평해지고 더 나아가는 진전을 억제하며, 4) 경이로운 진전은 결국 컴퓨팅 확장과 검색 및 학습에 기반을 뒀던 반대 방식으로 이루어지게 됩니다.\"라고 말합니다.\n\n아키텍처도 중요합니다. 트랜스포머의 엄청난 가장 효과적인 장점 중 하나는 RNN이나 LSTM보다 GPU/TPU 상에서 더 쉽게 병렬화될 수 있다는 것입니다. 이 더욱 좋은 병렬화 덕분에, 더 많은 데이터로 트랜스포머를 훈련하는 데 더 적은 시간이 걸립니다.\n\n# 데이터도 중요합니다\n\n또 다른 중요 요소는 고품질 데이터입니다. Eran Malach는 \"언어 모델의 힘은 다음 토큰 자동회귀 훈련 체계에 귀속될 수 있는데, 특정 아키텍처 선택에 귀속되는 것은 아닐 수도 있다\"고 주장합니다. 그러나 한 네티즌은 이 기사에 대한 반론으로 \"나는 기대했던 것이 LLM의 성공을 언어의 구조에 귀속했으면 좋았겠다고 말했습니다. 저자들이 말했듯이, 작은 선형 모델조차 Cot를 근사하고 복잡한 작업을 해결할 수 있습니다. 그래서 모델이 아니라 데이터입니다. 머리나 신경망(모델)이 아닌 데이터가 그들을 똑똑하게 만드는 것입니다.\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터는 인간에게 꼭 필요한 것입니다. 사회에서 멀리 떨어져 자란 어린이들(\"야생 어린이\"라고도 함)은 나중에 말이나 언어를 배우거나, 직립보행을 하거나, 변기를 사용하거나, 다른 사람에게 주의를 기울이는 것을 가르쳐줄 수 없는 경우가 많습니다. (매우 슬픈 기분이 들고 싶다면, 야생 어린이 이야기를 검색해보세요.) 데이터 혁신에 중점을 둔 연구는 데이터의 품질이 특히 높을 때 작은 모델을 고성능으로 얻을 수 있는 경우가 많다는 것을 발견했습니다. 예를 들어, 논문 \"Textbooks Are All You Need\"에서는 코드용 LLM을 소개하여 상당히 적은 매개변수를 가지고 있음에도 높은 성능을 달성했습니다. 비결은 \"교과서 수준\"의 데이터에서 훈련을 한 것이었습니다.\n\n# 이상한 빠진 조각들\n\n또한 AGI의 생성에 도움이 될 아직 발견되지 않은 혁신이 분명히 많이 존재할 것입니다. 현재 모델을 아이들과 비교하면 빠진 조각들이 있는 것을 시사합니다.\n\n한 측면에서, 사람들은 훨씬 적은 양의 데이터로 훈련받습니다: 언어 습득 과정 중에 사람들은 약 1.5MB의 정보만 저장한다는 것은, LLM 훈련 데이터셋의 거대한 크기나 LLM 자체의 저장된 매개변수 양과 비교했을 때 미약한 숫자입니다. 사람들이 \"기본적으로 인터넷 전체\"보다 더 적은 양의 언어에 노출되며 상대적으로 많은 데이터를 저장함에도 불구하고 언어에 대한 뛰어난 능력을 나타내는 점은, 아직 발견되지 않은 흥미로운 혁신들이 AGI 시스템을 더 적은 훈련 데이터로 구축하는 데 도움이 될 수 있다는 것을 시사합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다른 관점에서 보면, 인간들은 많은 데이터를 기반으로 훈련을 받습니다. 이 데이터는 현재 사용되는 기반 모델을 훈련하는 데 사용되는 데이터셋과는 매우 다릅니다. 전형적인 인간 아이들은 비디오와 오디오 스트림이 지속적으로 실행되며 여러 해 동안 데이터가 풍부한 환경에서 성장합니다. 이외에도 다른 감각에서 입력을 받습니다. 전혀 다른 인공 지능 시스템에서 어떤 새로운 지능이 발생할까요? 이 시스템이 전혀 다른 데이터셋을 사용하지 않고 일반 아이의 훈련 데이터셋만 사용해 다음 토큰 예측을 잘 하는 능력이 발전했다면?\n\n학습에 유용한 데이터 필터링 기술도 존재할 수 있습니다. 신생아는 흐릿한 흑백으로만 보기 시작합니다. 4개월이 지났을 때야 아기의 색상 감각이 완전히 발달합니다. 이러한 진행에는 진화적인 학습 관련 이점이 있다고 생각됩니다.\n\n![이미지](/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_6.png)\n\n# 결론\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 단어 예측기만 일까요? 그렇죠! 다른 방법이 있을까요? 제한된 인간 종 구성원으로서 언어를 만들려면 어떻게해야 할까요? 우리는 동시에 백 개의 단어를 말할 수 없습니다. 우리는 텔레파시가 아니며, \"생각 덤프\"를 통해 의사 소통할 수 없습니다. (만약 이렇다면 어떤 지능이 발전했을지 상상해보세요.)\n\n다음 단어, 또는 다음 광경, 또는 다음 소리를 성공적으로 예측함으로써 상당한 지능이 발전할 수 있다고 생각하는 것이 합리적으로 보입니다. 이 기사가 여러분에게 생각의 근원을 불러일으켰기를 바랍니다 — 그리고 완전히 예측할 수 없었으면 좋겠습니다.\n\n2024년 4월 28일, http://glassboxmedicine.com에서 최초로 게시되었습니다.","ogImage":{"url":"/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png"},"coverImage":"/assets/img/2024-05-17-HumanandArtificialGeneralIntelligenceArisesfromNextTokenPrediction_0.png","tag":["Tech"],"readingTime":14},{"title":"ChatGPT-4o는 사이버 보안 분야에서 게임 체인저입니다 하지만 잘못된 이유로요","description":"","date":"2024-05-17 19:45","slug":"2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons","content":"\n\n\n![ChatGPT-4o](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png)\n\nAI 세계가 다시 소란스럽습니다. \n\n새로운 OpenAI 업데이트 덕분에요.\n\n새로운 모델 GPT-4o는 이미 영화 \"Her\"의 AI와 비교되고 있습니다.\n\nChatGPT-4o는 놀랄만한 능력과 전반적인 이해력에서 큰 발전이 있어 보입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSo friendly!\n\n---\n\n가장 놀라운 것은 무료로 사용할 수 있다는 점입니다!\n\n![이미지](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_1.png)\n\n유료 버전은 여전히 더 큰 컨텍스트 창을 가지고 있을 수 있지만, OpenAI가 가장 첨단 AI를 무료로 제공한 사실은 정말 놀랍습니다.\n\n아직 데모를 보지 않았다면, 지금바로 확인하는 것을 추천합니다. ChatGPT-4o가 다음을 수행하는 것을 확인할 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1 — 과외\n\n이 데모에서는 ChatGPT-4o가 학생을 가르치며 수학 문제를 해결하고 해결책을 안내하는 것을 보여줍니다.\n\nAI가 문제를 자연스러운 방식으로 설명하는 것은 훈련과 온라인 학습의 미래가 어떻게 될지 보여줍니다!\n\n내 아내는 과외교사이며 세션 전체가 사람처럼 들리는 것에 놀랐다고 할 수 없을 정도입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2 — 실시간 번역\n\n또한 놀랄 만한 실시간 번역 기능을 제공합니다. 두 명의 사람이 영어와 이탈리아어로 이야기할 때 따라 말하는 것을 볼 수 있습니다.\n\n저도 직접 사용해 보았는데, 그 성능은 정말 인상적입니다.\n\n번역기 및 그들의 앱들은 어떻게 영향을 받을지 이미 걱정되고 있을 것 같네요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3 - 시각 능력\n\n데모에서 가장 좋았던 점은 팀이 시각 장애인을 도와 실시간으로 주변 환경을 설명하는 ChatGPT4-o를 보여준 것이었습니다.\n\n이것은 전 세계 사람들을 돕는 데 큰 도약이 될 것입니다.\n\n그가 주변 환경을 자연스럽게 설명하고 상호 작용을 얼마나 자연스럽게 했는지에 굉장히 감명받았습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 AI가 인류가 다음 단계로 발전하는 데 도움을 줄 것이라는 완벽한 예시입니다.\n\n# 이제, 나쁜 소식 ..\n\nAI가 한 걸음씩 나아갈수록.. 새로운 위험이 소개됩니다.\n\n새로운 비전 및 음성 기능은 사이버 범죄자에겐 꿈의 소재가 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위는 데모 시청과 (100% 주관적인) 나의 경험을 통해 생각해 본 위험 중 일부입니다.\n\n## 1 — 사회 공학 2.0\n\n인간과 같은 대화를 나눌 수 있는 자연스러운 AI는 사회 공학을 미친 정도로 증가시킬 수 있습니다.\n\n우리는 이미 GenAI에 의한 딥페이크와 음성 사기를 보았지만, GPT-4o의 향상된 멀티모달 능력은 사이버 범죄자들이 무시할 수 없는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n잠재적인 잘못된 정보 캠페인과 ID 도용 가능성이 엄청나요.\n\nGPT-4o의 실시간 대화 능력은 사이버 범죄자들이 AI-기반 vishing 공격을 만들어낼 수 있도록 할 것입니다.\n\n희생자들은 합법적인 사람과 소통하고 있다고 믿으며 민감한 정보를 폭로하거나 거래를 승인하게 되는 속임수를 당할 수 있습니다.\n\n이러한 공격은 새로운 것은 아니지만 GPT-4o와 같은 AI와 함께 그 규모와 정교함이 대대적으로 증가할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2 — AI-Powered Malware * 2\n\nAI-powered malware is already here, so there is nothing new about that\n\nWhat stood out to me was the video in which two GPT-4os were interacting with each other.\n\nImagine AI training another AI to evade controls and become better at compromising environments.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n두 모델 간의 상호작용은 인공지능이 서로 발전함에 따라 사이버 범죄의 미래가 어떻게 보일지도 모릅니다.\n\n## 3 — 다국어 공격\n\n실시간 번역이 가능하다는 것은 놀라우면서도 무섭습니다.\n\n사이버 범죄자들은 이제 다양한 언어로 전환하며 공격의 피해 범위를 확대할 수 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다국어 기업에서 일하는 직원들 중 사회 공학에 취약하지 않은 사람들도 언어로 대화하는 AI에 노출되면 자신을 열어보게 될 수 있습니다!\n\n# 미래의 한 눈독\n\nAI가 발전함에 따라 우리는 몇 달마다 미지의 영역으로 나아가는 것 같아요.\n\n기업들은 보안 시스템을 조정하고 AI와의 전투에 AI를 활용해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n흥미로운 데모를 보고 든 생각 중 일부를 소개해 드릴게요.\n\n- AI 기반 보안 도구를 도입하여 AI 기반 사회 공학 및 악성 소프트웨어의 이상한 패턴이나 행동을 감지합니다.\n- 직원 및 사용자가 AI 기반 사회 공학을 인식하는 방법에 대해 교육하십시오. 여전히 이메일 피싱 공격에 대해 이야기 중이라면 PowerPoint 프레젠테이션을 업데이트하세요!\n- 언어별 보안 프로토콜을 고려해 보세요. 언어 인식 기능을 갖춘 보안 프로토콜을 개발하여 다국어로 의심스러운 통신을 식별하고 표시합니다.\n\n다음 해에는 이것이 낡은 정보처럼 보일 수도 있느니라!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다중 모달, 다중 언어 인공 지능이 이제 여기 있으며, GPT-4o는 통제와 위험 관점에서 큰 발전이 이루어졌습니다.\n\n몇 달동안의 위협 전망이 사용자들(그리고 사이버 범죄자들)이 새로운 모델을 이해하고 대척을 하는 모습을 살펴봅시다.\n\n![ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_2.png](/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_2.png)\n\nTaimur Ijlal은 핀테크 업종에서 20년 이상의 국제 경험을 갖춘 다중 수상 경력을 지닌 정보 보안 리더입니다. Taimur는 링크드인이나 유튜브 채널 \"클라우드 보안 가이\"에서 연결할 수 있습니다. 클라우드 보안, 인공 지능, 일반적인 사이버 보안 직업 조언에 대해 꾸준히 게시하고 있습니다.","ogImage":{"url":"/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png"},"coverImage":"/assets/img/2024-05-17-ChatGPT-4oIsAGame-ChangerInCyberSecurityForAllTheWrongReasons_0.png","tag":["Tech"],"readingTime":4},{"title":"반복 신경망 시퀀스 모델링 소개","description":"","date":"2024-05-17 19:43","slug":"2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling","content":"\n\n![img](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png)\n\n많은 문제와 현상은 순차적으로 발생합니다. 대표적인 예로는 음성, 날씨 패턴, 시계열 등이 있습니다. 이러한 시스템들의 다음 위치는 이전 상태에 따라 달라집니다.\n\n안타깝게도, 전통적인 신경망은 이러한 유형의 데이터를 처리하거나 예측할 수 없습니다. 왜냐하면 입력값을 독립적으로 분석하기 때문에 데이터가 실제로 순차적이라는 개념을 이해하지 못하기 때문입니다.\n\n그렇다면, 이러한 유형의 데이터를 어떻게 예측할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"우리는 순환 신경망이라고 불리는 것으로 넘어갑니다!\n\n표준 신경망에 익숙하지 않다면, 확인할 블로그 시리즈가 있어요! RNN으로 계속 진행하기 전에 이 일반적인 신경망이 어떻게 작동하는지 알아보는 것을 권장합니다.\n\n# 순환 신경망이란 무엇인가요?\n\n다음은 순환 신경망(RNNs)을 설명하는 다이어그램입니다:\"\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![RecurrentNeuralNetworksAnIntroductiontoSequenceModelling](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_1.png)\n\n왼쪽에는 순환 뉴런이 있고, 오른쪽에는 시간에 따라 펼쳐진 순환 뉴런이 있습니다. RNN은 바닐라 피드포워드 신경망과 비슷해 보이지만, 이전 반복 실행에서 입력을 받는 중요한 차이점이 있습니다.\n\n그래서 그들을 \"순환\"이라고 부르는 것입니다. 각 단계의 출력이 시간 안에 전파되어 다음 단계의 값을 계산하는 데 도움이 됩니다. 시스템에는 어떤 내재적 \"기억\"이 있어서 모델이 과거의 패턴을 추적할 수 있습니다.\n\n예를 들어, Y_1을 예측할 때, X_1의 입력 및 이전 시간 단계 Y_0에서의 출력을 사용할 것입니다. Y_0가 Y_1에 영향을 미치기 때문에 Y_0가 Y_2에도간접적으로 영향을 줄 수 있다는 것을 알 수 있습니다. 이 알고리즘의 순환성을 명확하게 보여주는 사례입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 숨겨진 상태\n\n문학 작품에서는 일반적으로 숨겨진 상태라는 개념을 볼 수 있습니다. 주로 순환 뉴런을 통해 전달되는 h로 표시됩니다.\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_2.png)\n\n간단한 경우에는 숨겨진 상태가 셀의 출력인 경우도 있습니다. 즉, h=Y입니다. 그러나 우리가 나중에 살펴볼 것처럼, 장기 단기 메모리(LSTM) 및 게이트 순환 유닛(GRU)과 같은 보다 복잡한 셀의 경우에는 이것이 항상 참일 수 있는 것은 아닙니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서 각 뉴런으로 전달하는 것과 각 뉴런으로부터의 전달을 명시적으로 하는 것이 가장 좋습니다. 이것이 대부분의 문헌에서 위와 같이 표시되는 이유입니다.\n\n# 이론\n\n순환 뉴런의 각 숨겨진 상태는 다음과 같이 계산할 수 있습니다:\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_3.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n여기서:\n\n- h_t는 시간 t에서의 은닉 상태입니다.\n- h_'t−1'는 이전 시간 단계의 은닉 상태입니다.\n- x_t는 시간 t에서의 입력 데이터입니다.\n- W_h는 은닉 상태에 대한 가중치 행렬입니다.\n- W_x는 입력 데이터에 대한 가중치 행렬입니다.\n- b_h는 은닉 상태에 대한 편향 벡터입니다.\n- σ는 활성화 함수로, 일반적으로 tanh 또는 sigmoid 함수를 사용합니다.\n\n그리고 각 순환 뉴런의 출력을 예측하는 방법은 다음과 같습니다:\n\n![Recurrent Neurons](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위와 같이:\n\n- y_t는 시간 t에서의 출력입니다.\n- W_y는 출력과 관련된 가중치 행렬입니다.\n- b_y는 출력 편향 벡터입니다.\n\n보시다시피 표기법과 변수 대부분은 일반 피드포워드 신경망과 유사합니다. 유일한 차이점은 숨겨진 상태의 전달로, 그것은 모델이 출력을 예측하는 데 사용할 다른 입력이나 특성으로 볼 수 있습니다.\n\n각 숨겨진 층은 여러 반복 뉴런을 포함할 수 있으므로 각 후속 입력 뉴런에 숨겨진 상태의 벡터를 전달하게 됩니다. 이를 통해 네트워크는 데이터에서 더 복잡한 패턴을 포착하고 표현할 수 있습니다. 각 시간 단계에서 미니 신경망으로 생각할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 작업 예시\n\n우리는 RNN 내부에서 실제로 무슨 일이 일어나고 있는지 설명하기 위해 간단한 예제를 살펴볼 수 있습니다. 이 예제는 매우 단순한 시나리오일 것이지만, 알아야 할 주요 직관력을 설명해줄 것입니다. 실제로 현실에서는 어떤 문제도 이렇게 간단하지 않을 겁니다!\n\n## 설정\n\n1, 2, 3의 숫자 시퀀스가 있다고 가정해 보겠습니다. 이 시퀀스에서 다음 숫자인 4를 예측하기 위해 RNN을 훈련하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신의 RNN은 다음과 같은 구조를 가지고 있을 것입니다:\n\n- 하나의 입력 뉴런\n- 하나의 은닉 뉴런\n- 하나의 출력 뉴런\n\n가중치와 바이어스를 랜덤하게 초기화할 수 있습니다:\n\n- W_x (입력에서 은닉으로의 가중치): 0.5\n- W_h (은닉에서 은닉으로의 가중치): 1.0\n- b_h (은닉 바이어스): 0\n- 𝑏_𝑦 (출력 바이어스): 0\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 텍스트를 친근한 톤으로 한국어로 번역해 드리겠습니다.\n\n다음 활성화 함수를 사용해주세요:\n\n- 은닉층: tanh\n- 출력층: 없음 (identity/linear)\n\n초기 은닉 상태 값:\n\n- h_0 = 0\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Time Step 1 (Input: 1)\n\n다음은 첫 번째 숨겨진 상태입니다:\n\n![첫 번째 숨겨진 상태](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_5.png)\n\n그리고 출력은 다음과 같이 계산됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Recurrent Neural Networks](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_6.png)\n\n이 예시에서 출력 활성화 함수는 identity이므로 출력 값은 hidden state 값과 동일합니다. 그러나 많은 문제에서 항상 그런 것은 아니라는 것을 기억하세요.\n\n## 시간 단계 2 (입력: 2)\n\n이제 최근에 계산된 h_1 값 사용하여 시간 단계 2에서 다음 입력 값을 위한 위 과정을 반복할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 Markdown 형식으로 테이블 태그를 바꿔본 것입니다.\n\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_7.png)\n\n한번 더, 우리는 시간 단계 2에서 출력 값을 계산합니다:\n\n![이미지](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_8.png)\n\n## 시간 단계 3 (입력: 3)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마지막 입력 값과 세 번째 타임 스텝에서는 다음 이미지가 예측 모델을 보여줍니다:\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_9.png)\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_10.png)\n\n현재 모델은 다음 숫자를 0.984로 예측하고 있습니다. 실제 값인 4와는 분명히 멀리 떨어져 있습니다. 실제로는 더 많은 훈련 세트를 사용하여 시간을 거슬러 거슬러 역전파를 수행하여 매개 변수를 최적화할 것입니다. 이 내용은 다음 글에서 다룰 예정입니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다행히도 이 모든 계산과 최적화는 PyTorch와 TensorFlow와 같은 패키지를 통해 Python에서 수행됩니다. 제가 이 기사에서 이를 하는 방법의 예시를 나중에 보여 드리겠습니다!\n\n# RNN의 종류\n\n위의 예는 많은 입력으로부터 하나의 RNN 프로세스의 논리적인 과정을 설명하고 있습니다. 우리는 여러 입력(1,2,3)으로 시작하여 시퀀스에서 다음 숫자를 예측하기 위해 노력하고 있는데, 이는 단일 값입니다.\n\n그러나 다른 작업을 위한 여러 종류의 RNN이 있으며, 우리는 지금 그것들을 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 일대일\n\n이것은 단일 예측을 내놓는 입력 세트가 하나인 전통적인 신경망입니다. 이것은 일반적인 지도 학습 문제를 해결하는 데 도움이 됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_11.png)\n\n## 일대다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하나의 입력이 여러 출력으로 이어집니다. 이미지 캡션을 만들거나 음악을 생성하는 데 사용할 수 있습니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_12.png)\n\n## Many-To-One\n\n여러 입력이 하나의 최종 출력을 생성합니다; 이 아키텍처는 감성 분석에 사용됩니다. 영화 리뷰를 제공하면 영화가 좋은지 나쁜지에 따라 +1 또는 -1을 할당합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_13.png)\n\nThis one gets an input at every step and produces an output at each step. This architecture is used for machine translation and also for problems like speech tagging.\n\n![Many-To-Many](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_14.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 인코더-디코더\n\n마지막으로, 인코더-디코더 네트워크를 사용할 수 있습니다. 이는 많은 개별 데이터를 입력으로 받아 하나의 데이터를 출력하는 네트워크와, 그로부터 다시 많은 개별 데이터를 출력으로 하는 네트워크로 구성됩니다. 이는 주로 한 언어로 된 문장을 다른 언어로 번역하는 데 사용됩니다.\n\n![image](/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_15.png)\n\n# PyTorch 예시\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위는 PyTorch에서 간단한 RNN을 구현하는 간단한 예제입니다. 위에서 해결한 문제를 시연합니다. 입력이 1,2,3이고 순서에 따라 다음 숫자를 예측하려고 합니다.\n\n```js\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# RNN 모델 정의\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(-1)\n        h_0 = torch.zeros(1, x.size(0), self.hidden_size)\n        rnn_out, _ = self.rnn(x, h_0)\n        out = self.fc(rnn_out[:, -1, :])\n        return out\n\n# 데이터셋\ntrain = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\ntarget = torch.tensor([5], dtype=torch.float32)\n\n# 모델 설정\ninput_size = 1\nhidden_size = 1\noutput_size = 1\nmodel = SimpleRNN(input_size, hidden_size, output_size)\n\n# 손실 및 옵티마이저\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    output = model(train.unsqueeze(0)).squeeze()  # 배치 차원 추가 및 목표 형태와 일치하도록 압축\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n# 다음 숫자 예측하는 함수\ndef predict(model, input_seq):\n    with torch.no_grad():\n        input_seq = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n        output = model(input_seq).squeeze().item()\n    return output\n\n# 예제 테스트 세트\ntest = [2, 3, 4]\npredicted = predict(model, test)\nprint(f'Input: {test}, Predicted Next Number: {predicted:.2f}')\n```\n\n1000번의 에폭 후 출력 결과는 5입니다! 이 경우에는 모델이 실제로 1000번의 역전파로 훈련되었기 때문에 성능이 우리가 위에서 손으로 계산한 예제보다 훨씬 좋습니다.\n\n소스 코드는 저의 GitHub에서 확인하실 수 있습니다: (GitHub URL)\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 장점 대비 단점\n\n이 모든 새롭게 습득한 정보를 바탕으로 RNN의 주요 장단점을 살펴보겠습니다:\n\n## 장점\n\n- 이전 입력값의 형태를 기억할 수 있어서 순차적 데이터를 다루는 데 도움이 됩니다.\n- 정확한 가중치와 편향이 모든 시간 단계에서 공유되어, 더 적은 매개변수와 더 나은 일반화를 이끌어냅니다.\n- 재귀적 성격으로 인해 RNN은 가변 길이의 순차 데이터를 처리할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단점\n\n- RNN(순환 신경망)은 장기 기억 문제로 이어지는 사라지는 그래디언트 문제에서 상당히 고통받습니다.\n- 각 시간 단계는 이전 단계의 출력에 의존하기 때문에 RNN은 병렬 처리할 수 없어 계산 효율이 떨어집니다.\n\n# 요약\n\nRNN은 시퀀스 모델링에 매우 유용하며, 이전 실행의 정보와 메모리를 유지한 채 다음 예측으로 전파됩니다. 그들의 장점은 임의 길이의 입력을 처리할 수 있으며, 모델 크기가 이 입력 크기로 증가하지 않는다는 것입니다. 그러나 재귀적인 성격을 가지고 있기 때문에 병렬화할 수 없어 계산 효율이 낮으며, 사라지는 그래디언트 문제로 심각하게 고통받을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 또 다른 것!\n\n무료 뉴스레터 'Dishing the Data'를 갖고 있어요! 매주 더 나은 데이터 과학자가 되기 위한 조언과 분야에서의 경험을 나누고 있어요.\n\n# 저와 연결해보세요!\n\n- LinkedIn, X (트위터), 또는 인스타그램\n- 기술적인 데이터 과학과 머신 러닝 개념을 배울 수 있는 제 유튜브 채널!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 참고 자료 및 더 읽을거리\n\n- Stanford RNN Cheatsheet\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron. September 2019. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649.","ogImage":{"url":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png"},"coverImage":"/assets/img/2024-05-17-RecurrentNeuralNetworksAnIntroductiontoSequenceModelling_0.png","tag":["Tech"],"readingTime":9},{"title":"ICD 코딩을 위한 LLM 탐험 - 파트 1","description":"","date":"2024-05-17 19:38","slug":"2024-05-17-ExploringLLMsforICDCodingPart1","content":"\n\n## LLM(Large Language Model)을 활용한 자동 진단 코딩 시스템 구축\n\n임상 코딩은 흔히 쓰이는 용어는 아니지만, 대부분의 국가에서 건강 관리 체계와 상호작용하는 모든 사람에게 중대한 영향을 미칩니다. 임상 코딩은 환자 건강 기록에서 의학 정보(진단 및 수술 등)를 표준화된 숫자 또는 알파벳 코드로 번역하고 매핑하는 것을 포함합니다. 이러한 코드는 청구, 건강 관리 분석 및 환자가 적절한 치료를 받을 수 있도록 하는 데 중요합니다.\n\n![image](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png)\n\n임상 코딩은 일반적으로 의료 전문가가 수행합니다. 이러한 코더들은 다양한 진단과 수술을 위한 특정 코드가 포함된 복잡하고 종종 계층적인 코딩 용어를 탐색합니다. 따라서 코더들은 사용된 코딩 용어에 대한 깊은 이해와 경험을 가져야 합니다. 그러나 문서를 수동으로 코딩하는 것은 느릴 수 있고, 오류가 발생할 수 있으며, 상당한 인적 전문 지식이 필요하여 병목 현상이 발생할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n심층 학습은 임상 코딩 자동화에 중요한 역할을 할 수 있습니다. 복잡한 의료 정보를 코드로 추출하고 번역함으로써, 심층 학습 시스템은 인간 중심 시스템 내에서 가치 있는 도구로 작용할 수 있습니다. 이러한 시스템은 코더들을 지원하여 대량의 데이터를 신속하게 처리하고 정확성을 향상시킬 수 있습니다. 이는 행정 업무를 간소화하고 청구 오류를 줄이며 환자 치료 결과를 향상시킬 수 있습니다.\n\n이 첫 번째 부분에서는 ICD 코딩이 무엇인지, 자동 코딩 시스템이 효과적으로 극복해야 하는 다양한 도전에 대해 설명합니다. 또한 대용량 언어 모델(LLM)이 이러한 문제를 극복하는 데 효과적으로 활용할 수 있는 방법을 분석하고, 최근 논문에서 LLM을 효과적으로 활용한 알고리즘을 적용하여 ICD 코딩에 성공적으로 적용하는 방법을 설명합니다.\n\n## 목차:\n\n- ICD 코딩이란 무엇인가?\n- 자동 ICD 코딩의 도전 요소는 무엇인가?\n- LLM이 자동 ICD 코딩에 어떻게 도움이 될까?\n- \"Off-the-shelf 대용량 언어 모델을 이용한 자동 임상 코딩\" 논문 탐색\n- 논문에 설명된 기법 구현\n- 결론\n- 참고문헌\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# ICD 코딩이란 무엇인가요?\n\n국제질병분류(ICD) 코딩은 세계보건기구에서 개발 및 유지보수하는 임상 용어 시스템입니다 [1]. 대부분의 국가에서 환자의 모든 진단, 증상 및 절차를 범주화하고 코딩하는 데 사용됩니다.\n\n환자의 진단과 의료 절차를 기록하는 의료 기록은 ICD 코딩에 매우 중요합니다. ICD 용어는 대략 75,000가지 다른 코드로 구성된 트리 구조를 특징으로 하여 방대한 정보를 효율적으로 정리합니다. 이러한 문서를 정확하게 코딩하는 것이 중요합니다. 정확한 코딩은 적절한 청구를 보장하며 의료 분석 품질에 영향을 미치며 환자 치료 결과, 보상 및 의료 효율성에 직접적으로 영향을 줍니다.\n\n# 자동 ICD 코딩에서 어떤 도전들이 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nICD 코딩은 효과적으로 작동하기 위해 자동화된 시스템이 극복해야 할 여러 가지 도전이 있습니다.\n\n## ICD 코딩의 레이블 다양성:\n\n중요한 도전 중 하나는 레이블의 광범위한 출력 공간입니다. ICD 코드는 많고 각 코드는 미세한 세부 사항에서 차이가 있을 수 있습니다. 예를 들어, 오른손에 영향을 주는 상태와 왼손에 영향을 주는 상태는 서로 다른 코드를 갖게 됩니다. 또한 의료 기록에서 드물게 나타나는 희귀 코드의 긴 꼬리가 존재하여, 이러한 코드를 학습하고 정확하게 예측하기 어렵게 만들 수 있습니다.\n\n## 새로운 ICD 코드에 대한 적응:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n번거로우시겠지만, 테이블 태그를 마크다운 형식으로 변경해드릴게요!\n\n전통적인 데이터셋인 MIMIC-III [2] 같은 경우는 종합적이지만, 종종 ICD 코드의 범위를 훈련 말뭉치에 포함된 코드로 제한합니다. 이 제한은 의료 기록에서 ICD 코드로의 딥러닝 모델을 다중 레이블 분류 문제로 처리하는 데 새로운 코드가 도입된 경우 모형 훈련 이후에 어려움을 겪을 수 있음을 의미합니다. 이는 재훈련이 필요하고 잠재적으로 어려울 수 있게 만듭니다.\n\n## 정보 추출 및 문맥 활용:\n\n또 다른 주요 과제는 의료 기록에서 정보를 정확하게 추출하고 문맥에 맞게 처리하는 것입니다. ICD 코딩은 근본적으로 정보 검색 문제로, 의료 기록에서 진단을 식별하는 것 뿐만 아니라 이러한 진단을 해당 ICD 코드로 올바르게 매핑하는 데 필요한 모든 보완 정보를 포착해야 합니다. 따라서 자동화된 시스템이 의료 기록에서 여러 진단을 추출하고 적절히 문맥화하여 ICD 코드로 정확하게 매핑되도록 하는 것이 중요합니다.\n\n![이미지](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\"Contextualization\"란 여기서 무엇을 의미할까요? 의학 노트를 다룰 때 진단을 맥락에 맞게 처리하는 것은 관련 세부사항과 관련된 정보 — 예를 들어 영향을 받는 신체 부위 및 질환의 증상 — 를 연결하여 진단을 완전히 특성화하는 것을 의미합니다. 일반적으로 이 작업은 관계 추출로 참조됩니다.\n\n# 대규모 언어 모델(LLMs)이 자동 ICD 코딩에 어떻게 도움이 되나요?\n\n자동 ICD 코딩의 과제를 다룰 때, 대규모 언어 모델 (LLMs)은 이러한 문제에 대처하는 데 적합하며, 특히 새로운 레이블에 대한 적응성과 복잡한 정보 추출 작업을 관리하는 능력으로 인해 잘 역할을 합니다. 그러나 여기서의 포인트는 LLMs가 자동 ICD 코딩에 대한 최상의 해결책이거나 이러한 문제를 해결할 수 있는 유일한 해결책인 것을 주장하는 것이 아니라, 자동 ICD 코딩 시스템이 극복해야 하는 주요 과제들을 설정함으로써 LLMs의 능력을 최대한 활용하여 이를 해결할 수 있는지를 분석하는 것입니다.\n\n## 새로운 및 드문 ICD 코드에 대한 적응:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLMs는 견고한 제로샷 및 퓨샷 학습 능력을 보여주며, 적은 예시와 프롬프트에서 제공된 지침으로 새로운 작업에 적응할 수 있습니다. 검색 증강 생성 (RAG)은 미세 조정 없이도 LLM이 새로운 작업에 적응하기 위해 더 많은 맥락 정보에 접근할 수 있는 패러다임입니다. 이는 특히 기존의 훈련 데이터셋에서 자주 나타나지 않을 수 있는 새로운 및/또는 희귀한 ICD 코드에 LLM을 조정하는 데 유용합니다. 이를 단지 몇 가지 설명 또는 사용 사례로부터 합니다.\n\n## 맥락 정보:\n\nLLMs는 임상 분야에서의 제로샷 관계 추출에서 효과적으로 확인되었습니다. 제로샷 관계 추출은 LLM이 해당 관계에 대해 이전에 구체적인 훈련을 받지 않고 텍스트에서 관계를 식별하고 분류할 수 있도록 합니다. 이를 통해 의료 코딩에서의 진단을 더 잘 맥락화하여 더 정확한 ICD 코드를 가져올 수 있습니다.\n\n# \"Automated clinical coding using off-the-shelf large language models\" 논문 탐색하기:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM을 ICD 코딩에 적용한 최근 연구를 탐색하다가, 특정한 세부 조정 없이 LLM을 활용한 ICD 코딩에 관한 매우 흥미로운 논문을 발견했습니다. 저자들은 LLM을 활용한 ICD 코딩을 위해 LLM-지도된 트리 탐색이라는 방법을 개발했습니다 [5].\n\n## 이 방법은 어떻게 작동하나요?\n\nICD 용어는 계층적인 트리 구조입니다. 각 ICD 코드는 이 계층적 구조 내에 존재하며, 부모 코드는 더 일반적인 상태를 다루고, 자식 코드는 특정 질병을 상세히 설명합니다. ICD 트리를 탐색하면 더 구체적이고 세분화된 진단 코드로 이어집니다.\n\nLLM-지도된 트리 탐색에서는 탐색이 루트에서 시작되고 LLM을 사용하여 탐색할 가지를 선택하며, 모든 경로가 고갈될 때까지 반복적으로 계속합니다. 실제로 이 과정은 트리의 임의의 수준에서 모든 코드의 설명과 의료 노트를 LLM에 프롬프트로 제공하고, 해당 의료 노트에 대한 관련 코드를 식별하도록 요청하는 것으로 구현됩니다. 각 인스턴스에서 LLM에 의해 선택된 코드는 더 구체적으로 탐색되고 조사됩니다. 이 방법을 사용하면 가장 관련성이 높은 ICD 코드가 식별되며, 이후 임상 노트에 대한 예측 레이블로 할당됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예시를 통해 이를 명확히 해보겠습니다. ICD 코드 1과 ICD 코드 2라는 두 개의 루트 노드를 가진 트리를 상상해보세요. 각 노드는 코드를 특성화하는 평문 설명을 가지고 있습니다. 초기 단계에서 LLM에게 의학 노트와 코드 설명이 제공되고 의학 노트와 관련된 코드를 식별하도록 요청됩니다.\n\n이 시나리오에서 LLM은 의학 노트와 관련이 있는 것으로 판단된 ICD 코드 1과 ICD 코드 2를 식별합니다. 알고리즘은 각 코드의 자식 노드를 조사합니다. 각 부모 코드는 더 구체적인 ICD 코드를 나타내는 두 개의 자식 노드를 가지고 있습니다. ICD 코드 1부터 시작하여, LLM은 ICD 코드 1.1과 ICD 코드 1.2의 설명을 사용하여 의학 노트를 기반으로 관련 코드를 결정합니다. LLM은 ICD 코드 1.1이 관련이 있다고 결론 내리고, ICD 코드 1.2는 관련이 없다고 판단합니다. ICD 코드 1.1에는 더 이상의 자식 노드가 없으므로, 알고리즘은 할당 가능한 코드인지 확인하고 문서에 할당합니다. 그 다음 알고리즘은 ICD 코드 2의 자식 노드를 평가합니다. LLM을 호출하여, ICD 코드 2.1이 관련이 있는 것으로 판단합니다. 이것은 간단화된 예시이며, 실제로는 ICD 트리는 광범위하고 깊기 때문에 알고리즘은 각 관련된 노드의 자식을 탐색하거나 트리의 끝에 도달하거나 유효한 탐색을 소진할 때까지 계속됩니다.\n\n## 핵심\n\n- 이 방법은 LLM의 세밀한 조정이 필요하지 않습니다. 대신, 제공된 설명을 기반으로 LLM의 의료 노트를 상황에 맞게 이해하고 관련 ICD 코드를 동적으로 식별할 수 있는 능력을 활용합니다.\n- 더 나아가, 본 논문은 LLM이 프롬프트에 관련 정보가 주어질 때 대규모 출력 공간에 효과적으로 적응할 수 있으며, macro-average 지표 측면에서 드문 코드에서 PLM-ICD [6]를 앞지를 수 있다는 것을 보여줍니다.\n- 이 기술은 또한 파라메트릭 지식에 기초하여 의학 노트의 ICD 코드를 예측하도록 LLM에 직접 요청하는 기준선을 능가합니다. 이는 LLM을 임상 코딩 작업을 해결하기 위한 도구나 외부 지식과 통합하는 잠재력을 강조합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 단점\n\n- 알고리즘은 트리의 각 수준에서 LLM을 호출합니다. 이로 인해 트리를 탐색하는 동안 LLM 호출 횟수가 많아지며, ICD 트리의 광범위함이 이에 더해집니다. 이는 단일 문서를 처리하는 데 높은 대기 시간과 비용으로 이어집니다.\n- 저자들이 논문에서 언급한 바와 같이, 관련 있는 코드를 정확하게 예측하려면 LLM이 모든 수준에서 부모 노드를 올바르게 식별해야 합니다. 한 수준에서 실수가 발생하더라도, LLM은 최종 관련 코드에 도달할 수 없게 됩니다.\n- 저자들은 MIMIC-III와 같은 데이터셋을 사용하여 메소드를 평가할 수 없었습니다. 외부 서비스로의 데이터 전송을 금지하는 제한 사항으로 인하여 OpenAI의 GPT 엔드포인트와 같은 외부 서비스로의 데이터 전송이 불가능했습니다. 대신, 저자들은 CodiEsp 데이터셋 [7,8]의 테스트 세트를 사용하여 해당 방법을 평가했습니다. 해당 데이터셋은 250개의 의학 노트를 포함하고 있습니다. 이 데이터셋의 크기가 작은 것은 해당 방법이 대규모 임상 데이터셋에서의 성능을 아직 입증하지 못했음을 시사합니다.\n\n# 논문에서 설명한 기술 구현하기\n\n이 기술을 구현하여 그 작동 방식을 더 잘 이해해 봅시다. 논문에서 언급했듯이, 해당 논문은 평가를 위해 CodiEsp 테스트 세트를 사용합니다. 이 데이터셋은 스페인어 의학 노트와 이에 대응하는 ICD 코드로 구성되어 있습니다. 데이터셋에는 영어로 번역된 버전도 포함되어 있지만, 저자들은 스페인어 의학 노트를 GPT-3.5를 사용하여 영어로 번역하였으며, 이를 통해 사전 번역된 버전을 사용하는 것보다 성능이 약간 향상되었다고 주장했습니다. 이 기능을 복제하고 노트를 영어로 번역해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef construct_translation_prompt(medical_note):\n    \"\"\"\n    Construct a prompt template for translating Spanish medical notes to English.\n    \n    Args:\n        medical_note (str): The medical case note.\n        \n    Returns:\n        str: A structured template ready to be used as input for a language model.\n    \"\"\"    \n    translation_prompt = \"\"\"You are an expert Spanish-to-English translator. You are provided with a clinical note written in Spanish.\nYou must translate the note into English. You must ensure that you properly translate the medical and technical terms from Spanish to English without any mistakes.\nSpanish Medical Note:\n{medical_note}\"\"\"\n    \n    return translation_prompt.format(medical_note = medical_note)\n```\n\nNow that we have the evaluation corpus ready, let’s implement the core logic for the tree-search algorithm. We define the functionality in get_icd_codes, which accepts the medical note to process, the model name, and the temperature setting. The model name must be either “gpt-3.5-turbo-0613” for GPT-3.5 or “meta-llama/Llama-2–70b-chat-hf” for Llama-2 70B Chat. This specification determines the LLM that the tree-search algorithm will invoke during its processing.\n\nEvaluating GPT-4 is possible using the same code-base by providing the appropriate model name, but we choose to skip it as it is quite time-consuming.\n\n```js\ndef get_icd_codes(medical_note, model_name=\"gpt-3.5-turbo-0613\", temperature=0.0):\n    \"\"\"\n    Identifies relevant ICD-10 codes for a given medical note by querying a language model.\n\n    This function implements the tree-search algorithm for ICD coding described in https://openreview.net/forum?id=mqnR8rGWkn.\n\n    Args:\n        medical_note (str): The medical note for which ICD-10 codes are to be identified.\n        model_name (str): The identifier for the language model used in the API (default is 'gpt-3.5-turbo-0613').\n\n    Returns:\n        list of str: A list of confirmed ICD-10 codes that are relevant to the medical note.\n    \"\"\"\n    assigned_codes = []\n    candidate_codes = [x.name for x in CHAPTER_LIST]\n    parent_codes = []\n    prompt_count = 0\n\n    while prompt_count \u003c 50:\n        code_descriptions = {}\n        for x in candidate_codes:\n            description, code = get_name_and_description(x, model_name)\n            code_descriptions[description] = code\n\n        prompt = build_zero_shot_prompt(medical_note, list(code_descriptions.keys()), model_name=model_name)\n        lm_response = get_response(prompt, model_name, temperature=temperature, max_tokens=500)\n        predicted_codes = parse_outputs(lm_response, code_descriptions, model_name=model_name)\n\n        for code in predicted_codes:\n            if cm.is_leaf(code[\"code\"]):\n                assigned_codes.append(code[\"code\"])\n            else:\n                parent_codes.append(code)\n\n        if len(parent_codes) \u003e 0:\n            parent_code = parent_codes.pop(0)\n            candidate_codes = cm.get_children(parent_code[\"code\"])\n        else:\n            break\n\n        prompt_count += 1\n\n    return assigned_codes\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리는 논문과 비슷하게, ICD-10 트리에 액세스하는 simple_icd_10_cm 라이브러리를 사용합니다. 이를 통해 트리를 탐색하고, 각 코드에 대한 설명에 액세스하며 유효한 코드를 식별할 수 있습니다. 먼저, 트리의 첫 번째 수준에서 노드를 가져옵니다.\n\n```js\nimport simple_icd_10_cm as cm\n\ndef get_name_and_description(code, model_name):\n    \"\"\"\n    ICD-10 코드의 이름과 설명을 검색합니다.\n    \n    Args:\n        code (str): ICD-10 코드.\n        \n    Returns:\n        tuple: 형식화된 설명과 코드의 이름이 포함된 튜플을 반환합니다.\n    \"\"\"\n    full_data = cm.get_full_data(code).split(\"\\n\")\n    return format_code_descriptions(full_data[3], model_name), full_data[1]\n```\n\n루프 내부에서 각 노드에 해당하는 설명을 얻습니다. 이제 의료 노트와 코드 설명을 기반으로 LLM을 위한 프롬프트를 작성해야 합니다. 우리는 논문에서 제공된 세부 정보를 기반으로 GPT-3.5와 Llama-2용 프롬프트를 작성합니다.\n\n```js\nprompt_template_dict = {\"gpt-3.5-turbo-0613\" : \"\"\"[사례 노트]:\n{note}\n[예시]:\n\u003c예시 프롬프트\u003e\n위식도 역류병\n장전위\n\n\u003c응답\u003e\n위식도 역류병: 예, 환자에게 오메프라졸 처방함.\n장전위: 아니오.\n\n[작업]:\n다음 ICD-10 코드 설명 각각을 고려하고 사례 노트에 관련 언급이 있는지 평가하십시오.\n예시의 형식을 정확히 따르십시오.\n\n{code_descriptions}\"\"\",\n\n\"meta-llama/Llama-2-70b-chat-hf\": \"\"\"[사례 노트]:\n{note}\n\n[예시]:\n\u003c코드 설명\u003e\n* 위식도 역류병\n* 장전위\n* 급성비인두염 [감기]\n\u003c/코드 설명\u003e\n\n\u003c응답\u003e\n* 위식도 역류병: 예, 환자에게 오메프라졸 처방함.\n* 장전위: 아니오.\n* 급성비인두염 [감기]: 아니오.\n\u003c/응답\u003e\n\n[작업]:\n예시 응답 형식을 정확히 따르십시오. (예) 판단하기 전에 전체 설명과 (예|아니오) 판단을 입력한 후에 새 줄을 추가하십시오. \n다음 ICD-10 코드 설명을 고려하고 사례 노트에서 관련 언급이 있는지 확인하십시오.\n\n{code_descriptions}\"\"\"\n}\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n의료 기록과 코드 설명에 기반한 프롬프트를 지금 만들어 보겠습니다. 프롬프트 및 코딩에서 우리에게 이점은 GPT-3.5 및 Llama 2 모두와 상호 작용하기 위해 동일한 openai 라이브러리를 사용할 수 있다는 것입니다. 단, Llama-2가 deepinfra를 통해 배포되어야 합니다. deepinfra는 LLM에 요청을 보내기 위한 openai 형식도 지원합니다.\n\n```js\ndef construct_prompt_template(case_note, code_descriptions, model_name):\n    \"\"\"\n    주어진 케이스 노트와 ICD-10 코드 설명을 평가하는 프롬프트 템플릿 구성\n    \n    Args:\n        case_note (str): 의료 케이스 노트\n        code_descriptions (str): ICD-10 코드 설명을 단일 문자열로 포맷팅\n        \n    Returns:\n        str: 언어 모델에 입력으로 사용할 준비된 구조화된 템플릿\n    \"\"\"\n    template = prompt_template_dict[model_name]\n\n    return template.format(note=case_note, code_descriptions=code_descriptions)\n\ndef build_zero_shot_prompt(input_note, descriptions, model_name, system_prompt=\"\"):\n    \"\"\"\n    시스템 및 사용자 역할에 대한 제로샷 분류용 프롬프트 빌드\n    \n    Args:\n        input_note (str): 입력 노트 또는 질의\n        descriptions (list of str): ICD-10 코드 설명 리스트\n        system_prompt (str): 선택적 초기 시스템 프롬프트 또는 지시\n    \n    Returns:\n        list of dict: 각 메시지의 역할 및 내용을 정의하는 구조화된 사전 목록\n    \"\"\"\n    if model_name == \"meta-llama/Llama-2-70b-chat-hf\":\n        code_descriptions = \"\\n\".join([\"* \" + x for x in descriptions])\n    else:\n        code_descriptions = \"\\n\".join(descriptions)\n\n    input_prompt = construct_prompt_template(input_note, code_descriptions, model_name)\n    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": input_prompt}]\n```\n\n프롬프트를 구성한 후, 이제 LLM을 호출하여 응답을 받겠습니다:\n\n```js\ndef get_response(messages, model_name, temperature=0.0, max_tokens=500):\n    \"\"\"\n    채팅-완성 API를 통해 지정된 모델로부터 응답을 획득\n    \n    Args:\n        messages (list of dict): API 입력용 구조화된 메시지 목록\n        model_name (str): 쿼리할 모델의 식별자\n        temperature (float): 응답의 무작위성을 제어하는 값, 0이면 결정론적\n        max_tokens (int): 응답의 토큰 수 제한\n        \n    Returns:\n        str: 모델에서의 응답 메시지 내용\n    \"\"\"\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens\n    )\n    return response.choices[0].message.content\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n좋아요, 우리가 출력물을 얻었어요! 이 응답으로부터, 이제 LLM이 추가적인 탐색을 위해 관련있는 노드들과 거부한 노드들을 식별하기 위해 각 코드 설명을 구문 분석합니다. 우리는 출력 응답을 새 줄로 나누고 각 응답을 분할하여 LLM의 각 코드 설명에 대한 예측을 식별합니다.\n\n```js\ndef remove_noisy_prefix(text):\n    # 문자 또는 숫자가 뒤따르고 점과 선택적 공백으로 시작하는 문자열의 제일 앞에 있는 숫자나 문자를 제거합니다.\n    cleaned_text = text.replace(\"* \", \"\").strip()\n    cleaned_text = re.sub(r\"^\\s*\\w+\\.\\s*\", \"\", cleaned_text)\n    return cleaned_text.strip()\n\ndef parse_outputs(output, code_description_map, model_name):\n    \"\"\"\n    모델 출력을 구문 분석하여 주어진 설명 매핑에 따른 ICD-10 코드를 확인합니다.\n    \n    Args:\n        output (str): 확인을 포함하는 모델 출력입니다.\n        code_description_map (dict): 설명과 ICD-10 코드의 매핑입니다.\n        \n    Returns:\n        list of dict: 확인된 코드 및 해당 설명의 목록입니다.\n    \"\"\"\n    confirmed_codes = []\n    split_outputs = [x for x in output.split(\"\\n\") if x]\n    for item in split_outputs:\n        try:                \n            code_description, confirmation = item.split(\":\", 1)\n            if model_name == \"meta-llama/Llama-2-70b-chat-hf\":\n                code_description = remove_noisy_prefix(code_description)\n\n            if confirmation.lower().strip().startswith(\"yes\"):\n                try:\n                    code = code_description_map[code_description]\n                    confirmed_codes.append({\"code\": code, \"description\": code_description})\n                except Exception as e:\n                    print(str(e) + \" Here\")\n                    continue\n        except:\n            continue\n    return confirmed_codes\n```\n\n이제 루프의 나머지를 살펴봅시다. 지금까지 우리는 프롬프트를 구성했고, LLM으로부터 응답을 받았으며, 출력을 구문 분석하여 LLM에 의해 관련이 있다고 판단된 코드를 식별했습니다.\n\n```js\nwhile prompt_count \u003c 50:\n    code_descriptions = {}\n    for x in candidate_codes:\n        description, code = get_name_and_description(x, model_name)\n        code_descriptions[description] = code\n\n    prompt = build_zero_shot_prompt(medical_note, list(code_descriptions.keys()), model_name=model_name)\n    lm_response = get_response(prompt, model_name, temperature=temperature, max_tokens=500)\n    predicted_codes = parse_outputs(lm_response, code_descriptions, model_name=model_name)\n\n    for code in predicted_codes:\n        if cm.is_leaf(code[\"code\"]):\n            assigned_codes.append(code[\"code\"])\n        else:\n            parent_codes.append(code)\n\n    if len(parent_codes) \u003e 0:\n        parent_code = parent_codes.pop(0)\n        candidate_codes = cm.get_children(parent_code[\"code\"])\n    else:\n        break\n\n    prompt_count += 1\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 예측된 코드를 반복하며 각 코드가 \"leaf\" 코드인지 확인합니다. 이는 코드가 유효하고 할당 가능한 ICD 코드임을 보증하는 것입니다. 예측된 코드가 유효하면 LLM이 그 의료 노트에 대한 예측으로 간주합니다. 유효하지 않으면 상위 코드에 추가하여 ICD 트리를 더 탐색하기 위해 자식 노드를 얻습니다. 더 이상 탐색할 상위 코드가 없을 경우 루프를 탈출합니다. \n\n이론적으로 의료 노트 당 LLM 호출 수는 임의로 높을 수 있으며, 알고리즘이 많은 노드를 탐색하는 경우 지연 시간이 증가할 수 있습니다. 저자는 의료 노트 당 최대 50회 프롬프트/LLM 호출로 처리를 종료하는 최대 수를 시행했습니다. 이 한계는 우리가 구현에서도 채택합니다.\n\n## 결과\n\n이제 GPT-3.5와 Llama-2를 LLM으로 사용하여 트리 탐색 알고리즘의 결과를 평가할 수 있습니다. 우리는 알고리즘의 성능을 마이크로-평균 및 매크로-평균 정밀도, 재현율 및 F1 점수를 통해 평가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![ExploringLLMsforICDCodingPart1_2](/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_2.png)\n\n구현 결과는 논문에 보고된 점수와 대략적으로 일치하지만 주목할 만한 차이점이 있습니다.\n\n- 이 구현에서 GPT-3.5의 마이크로 평균 측정 지표는 보고된 값보다 약간 뛰어나지만, 매크로 평균 측정 지표는 보고된 값보다 조금 부족합니다.\n- 마찬가지로 Llama-70B의 마이크로 평균 측정 지표는 보고된 값과 일치하거나 조금 뛰어나지만, 매크로 평균 측정 지표는 보고된 값보다 낮습니다.\n\n앞서 언급했듯이, 이 구현은 몇 가지 미세한 차이점을 가지고 있어 최종 성능에 영향을 미칩니다. 이 구현이 원본 논문과 어떻게 다른지에 대한 보다 자세한 내용은 링크된 저장소를 참조해 주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n이 방법을 이해하고 구현하는 것은 여러 측면에서 나에게 매우 유익했습니다. 이를 통해 대규모 언어 모델(LLMs)의 강점과 약점에 대해 보다 세밀하게 이해할 수 있었고 임상 코딩 사례에서 그것을 구현할 수 있었습니다. 구체적으로, 코드에 관련된 중요한 정보에 동적으로 접근할 수 있는 경우 LLMs는 임상 문맥을 효과적으로 이해하고 관련 코드를 정확하게 식별할 수 있다는 것이 분명해졌습니다.\n\nLLMs를 임상 코딩을 위한 대리자로 활용하는 것이 성능을 더욱 향상시킬 수 있는지 탐구하는 것이 흥미로울 것입니다. 생명공학 및 임상 텍스트에 대한 외부 지식 소스가 논문이나 지식 그래프 형태로 풍부하게 제공되는 상황에서 LLM 대리자는 의료 문서를 보다 세밀한 단위로 분석하는 워크플로에 활용될 수 있습니다. 또한 필요한 경우 외부 지식을 참고하여 최종 코드에 도달할 수 있도록 동적으로 도구를 활용할 수도 있습니다.\n\n## 감사의 글\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 방법을 평가하는 데 도움을 준 이 논문의 주 저자 Joseph에게 큰 감사를 표합니다!\n\n- 참고 자료:\n\n[1] https://www.who.int/standards/classifications/classification-of-diseases\n\n[2] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. W. H., Feng, M., Ghassemi, M., … \u0026 Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database Sci. Data, 3(1), 1.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[3] Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., \u0026 Sontag, D. (2022). 대형 언어 모델은 소수의 적은 데이터로도 임상 정보를 추출합니다. arXiv 사전 인쇄 arXiv:2205.12689.\n\n[4] Zhou, H., Li, M., Xiao, Y., Yang, H., \u0026 Zhang, R. (2023). 임상 관계 추출을 위한 LLM Instruction-Example Adaptive Prompting (LEAP) 프레임워크. medRxiv : 의학과학 사전 인쇄 서버, 2023.12.15.23300059. https://doi.org/10.1101/2023.12.15.23300059\n\n[5] Boyle, J. S., Kascenas, A., Lok, P., Liakata, M., \u0026 O’Neil, A. Q. (2023, 10월). 상업용 대형 언어 모델을 사용한 자동 임상 코딩. NeurIPS 2023에서 Deep Generative Models for Health Workshop 발표.\n\n[6] Huang, C. W., Tsai, S. C., \u0026 Chen, Y. N. (2022). 사전 훈련된 언어 모델로 자동 ICD 코딩하기: PLM-ICD. arXiv 사전 인쇄 arXiv:2207.05289.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nMiranda-Escalada, A., Gonzalez-Agirre, A., Armengol-Estapé, J., \u0026 Krallinger, M. (2020). CLEF (Working Notes), 2020에서 CodiEsp Track의 비영어 임상 사례에 대한 주석, 가이드라인 및 솔루션에 대한 개요.\n\nMiranda-Escalada, A., Gonzalez-Agirre, A., \u0026 Krallinger, M. (2020). CodiEsp corpus: ICD10 (CIE10)로 코드화된 골드 표준 스페인어 임상 사례 - eHealth CLEF2020 (1.4) [데이터 세트]. Zenodo. https://doi.org/10.5281/zenodo.3837305 (CC BY 4.0)","ogImage":{"url":"/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png"},"coverImage":"/assets/img/2024-05-17-ExploringLLMsforICDCodingPart1_0.png","tag":["Tech"],"readingTime":19},{"title":"살아있는 인공지능의 첫 걸음, 바디 인텔리전스","description":"","date":"2024-05-17 19:35","slug":"2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence","content":"\n\n두 개의 세계가 충돌하고 있어요.\n\n우리는 방금 두 가지 분야에서 로봇 공학의 상당한 도약을 목격했어요. Figure AI의 말하는 로봇과 Google의 일반적인 에이전트 SIMA가 그겁니다.\n\n하지만, 아니요, 이것은 일반적인 인공 지능(AGI)은 아니에요. 일부 사치스럽지만 입증되지 않은 주장이 돌아다니고 로봇들이 우리를 다 죽일 거라고 말하진 않을 거예요.\n\n그러나 두 소식이 이미 자체로 흥미롭지만, 그들 간의 시너지는 내 관점에서 AI 구현 지능의 첫 번째 발걸음이라고 생각해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# GPT-4의 구현\n\n그러면 Figure AI는 뭔가요?\n\nFigure AI는 로봇 회사로, CEO의 말에 따르면 \"위험하고 원하지 않는 직업이 필요하지 않도록 하여 미래 세대가 더 행복하고 의미 있는 삶을 살도록 허용하는 로봇을 구축하고 있습니다.\"\n\n이미 유사한 웅장한 사명을 가진 회사들을 들어보셨을 것입니다. 그러나 OpenAI, NVIDIA, Jeff Bezos, 그리고 Intel이 시장에 제품이 없는 회사에 대해 26억 달러 평가액에서 6억 7500만 달러 시리즈 투자를 한다면, 그들이 무엇인가를 찾고 있다는 것을 알 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 왜 모두가 이 회사에 대해 이야기하는 걸까요? 이 비디오 때문입니다.\n\n그 링크를 클릭하는 것을 강력히 권장합니다. 간단히 말하자면, 이 비디오를 통해 사람과 상호작용하는 로봇이 여러 작업을 민첩하게 수행한다는 것을 보여줍니다.\n\n흥미로운 점은 Figure AI의 로봇의 핵심에는 GPT-4V, OpenAI의 주요 다중모달 대형 언어 모델인 MLLM이 있다는 것입니다.\n\n다시 말해, Figure AI의 로봇은 '화신 ChatGPT'의 처음으로, 즉 LLMs가 실체화된 작업도 할 수 있는 능력이 생긴 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 대부분의 사람들이 놓칠 수도 있는데, 로봇이 이전 동작에 대한 이유를 설명하면서 사람에게 작업을 수행하도록 요청하는 순간에 주목하여 주길 바랍니다.\n\n이 질문이나 요청이 사소한 것은 아니라는 점을 분명히 이해해야 합니다. 모델이 동시에 여러 작업을 수행할 수 있는 능력을 보여주기 위해 일부러 그랬던 것입니다.\n\n구체적으로 말하자면, 그들은 GPT-4를 세밀하게 조정하여 텍스트와 작업 표현을 출력하도록 만들었는데, 전자는 보코더를 통해 음성으로 디코딩되고, 후자는 액추에이터 동작으로 디코딩되어 몸을 움직이도록 합니다.\n\nFigure AI의 로봇의 기본 메커니즘에 대해서는 많이 알지 못하지만, Deepmind의 RT-2 모델과 같은 예시를 통해, 우리는 이미 LLM을 로봇 동작이나 텍스트를 출력하도록 훈련하는 방법을 증명한 연구자들이 있기에 꽤 좋은 아이디어를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png)\n\n그러나 RT-2는 GPT-4의 능력에 가까운 두 LLM인 PALM-E와 PALI-X를 사용하여 훈련된 것을 고려할 때, Figure AI의 로봇 뒤에 있는 LLM이 이 모델이 지금까지 존재한 가장 고급 비전-언어-행동 모델일 수 있다는 주장이 가능합니다.\n\n하지만 현실적으로 생각해 봅시다.\n\n그것은 여러 차례 연습된 것일 수 있는 매우 제한된 데모였습니다. 사실, 로봇이 일반적인 용도의 능력 측면에서 아직 매우 초기 단계에 있다는 것은 거의 확실합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n따라서, 이 로봇들의 여정에서 다음 단계를 상상하기 위해 구글이 방금 공개한 것에서 영감을 받을 수 있습니다.\n\nSIMA, 3D 총론적 에이전트.\n\n하지만 이번에는 Figure AI의 경우와 달리, 에이전트에 대해 훨씬 더 많은 정보를 갖고 있습니다.\n\n# SIMA, 최초의 진정한 총론적 에이전트?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGoogle의 확장 가능한 인공 지능 멀티월드 에이전트(SIMA) 프로젝트는 어떤 3D 환경에서 임의의 언어 명령을 이해하고 실행할 수 있는 인공 지능(AI) 시스템을 만들고자 합니다.\n\n이 계획은 언어를 지각 및 다양한 가상 세계에서 실제 행동과 결합하여 처리함으로써 일반적인 AI 개발에서 중요한 과제에 대처합니다. 이는 연구 환경 및 상업 비디오 게임을 포함한 다양한 가상 세계를 대상으로 합니다.\n\n간단히 말하면, SIMA는 언어 요청을 받아들이고 이를 3D 환경에서 키보드 및 마우스 조작으로 변환합니다.\n\n그러나 고려해야 할 중요한 요소는 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 에이전트들의 목표는 \"자원을 수집하고 집을 짓는\" 등 사용자가 제시하는 자연어 지침대로 작업을 수행하는 것입니다.\n\n간단히 말해, 모델은 해당 환경에서 상호작용할 때 사람들과 똑같은 입력을 갖고 있으므로 사람들보다 약점이 없다는 것을 의미하며, 이는 기접근 API나 이와 유사한 것에 대한 접근 권한이 없다는 것을 의미합니다.\n\n결과적으로, 요구된 작업을 수행하는 유일한 방법은 해당 작업을 수행하는 데 사람이 수행할 키보드 및 마우스 작업을 예측하는 것입니다.\n\nSIMA 에이전트는 다음 구성 요소로 이루어져 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 마크다운 형식으로 변경한 텍스트입니다.\n\n![이미지](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_1.png)\n\n- 인코더:\n\n  - 텍스트 인코더: 언어 명령을 모델이 해석할 수 있는 임베딩으로 번역합니다.\n  - SPARC 개발을 기반으로 한 이미지 인코더.\n  - 비디오 인코더.\n\n2. Multi-modal Transformer + Transformer XL: 두 개의 트랜스포머 아키텍처로, 전자는 모달 간 교차 어텐션을 수행하고, 후자는 이전 상태를 취해 새로운 상태를 식별합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3. 정책: 선택된 행동이 600가지 가능한 기술 중 하나로 결정되는 분류 헤드\n\n여기에는 해석할 내용이 많기 때문에 단계별로 진행해 봅시다.\n\n## 세계 처리\n\n대부분의 최신 모델들에서와 마찬가지로, 첫 번째 단계는 입력을 \"인코딩\"하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적인 용어로 설명하자면, 입력 데이터(텍스트와 비디오)를 가져와 해당 인코더를 사용하여 이러한 데이터 포인트를 벡터 임베딩으로 변환하는 것이 아이디어입니다.\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_2.png)\n\n하지만 왜 이 작업을 하는 것일까요?\n\n이 변환을 수행함으로써 각 요소가 해당 개념의 의미론을 캡처하는 밀집 벡터로 표현됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 말해, 의미론적으로 유사한 개념은 유사한 벡터를 갖게 되어 벡터 공간에서 표현되었을 때 더 가까이 위치하게 됩니다: \n\n![Concept Vectors](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_3.png)\n\n 또 다른 중요한 이점은 '개념을 벡터로 변환함'으로, 인간이 아주 무의식적으로 수행하는 '우리 세계를 이해하는' 행위를 수학적인 연습(컴퓨터에 이상적)으로 변환하여, 개념이 이제 수치로 표현된다는 것입니다.\n\n특히, 이 모델은 이러한 벡터들 사이의 유사성(그들 사이의 거리)을 계산하여, 어떤 개념이 다른 것들과 유사한지를 알아냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 모델은 '개'와 '고양이'가 유사한 속성 (포유류, 길들여진 동물, 네 다리 등)을 공유하는 유사한 개념을 나타낸다는 것을 알 수 있습니다.\n\n분명히 말씀드리자면, AI가 각 숫자를 다루는 방식은 아니며, AI가 중요시하는 것은 벡터 간의 근접성입니다. 만약에 도움이 될 경우, 벡터의 의미를 단순히 벡터 자체로만 생각하지 마십시오 (우리는 AI가 정말 '개'가 무엇인지 아는지 확신할 수 없습니다) 대신, 가장 가까운 이웃들의 합으로 생각하고, 모델에게 다른 것들과 유사하다는 신호를 보내는 것으로 여기면 됩니다 (강아지는 고양이와 유사하며 문과는 다릅니다).\n\n비슷한 이유로 유사성은 데이터 유형이 다른 상황에 집중하는 데 중요한 역할을 합니다.\n\n다양한 데이터 유형에서 나온 벡터로 오는 여러 모달 상황에 집중하는 데 유용합니다.\n\n기본 아이디어는 '개'라는 단어와 '개의 이미지'가 유사한 벡터를 공유하도록 원한다는 것으로, 이것은 모델에게 두 가지가 동일한 개념을 나타낸다는 것을 알려줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n하지만 텍스트와 이미지는 구조적으로 매우 다른 데이터 유형이기 때문에 서로 다른 인코더가 필요합니다.\n\n이것은 문제입니다. 비슷한 개념에 대해 유사한 임베딩을 생성하도록 보장하며 별도의 인코더를 훈련해야 합니다.\n\n이 문제를 해결하기 위해 SIMA는 SPARC 이미지 인코더를 사용합니다.\n\n매우 최근에 개발된 SPARC 인코더는 대부분의 다른 이미지 인코더와 매우 유사한 방식으로 훈련됩니다(대비 학습을 사용), 그러나 미세 구체적인 세부 사항을 더 잘 캡처할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 흔한 이미지 인코더들은 이미지의 지역 세부사항을 제대로 포착하지 못하는 문제가 있습니다.\n\n네, 그들은 이미지가 무엇에 대해인지를 알려줄 수 있지만, 이미지의 전역 의미를 설명하는 데 도움이 되지 않는 중요한 작은 세부사항을 놓치기도 합니다. 그럼에도 불구하고 많은 경우에 중요합니다.\n\nSPARC는 유사한 방법을 제안하지만 매우 흥미로운 요소를 추가합니다.\n\n예를 들어, \"바구니 속 고양이와 개\"를 나타내는 이미지-텍스트 쌍이 있다고 가정해 봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_4.png\" /\u003e\n\n- 먼저, SPARC는 이미지를 패치로 나눕니다.\n- 그런 다음 각 패치에 대해 텍스트 설명 중 하나의 토큰을 할당합니다. 예를 들어, 패치가 개의 일부를 나타내면 \"개\"라는 단어가 할당됩니다.\n- 이 작업은 모든 패치(왼쪽 색상 그리드의 수직 열)에 대해 수행되며, 텍스트 설명의 여러 측면을 다루는 패치의 경우 각각에 가중치가 할당됩니다.\n\n- 특정 개념에 할당된 모든 패치를 가지고 있으면 각 패치가 '개'와 같은 특정 단어에 부여하는 가중치를 그룹화하여 이 그룹화된 임베딩을 실제 단어와 비교합니다. 그러나 여기에는 중요한 점이 있습니다. 이미지 전역에 대해 적용하는 대신, 모든 개념에 대해 이를 다섯 번 적용합니다.\n\n하지만 모든 이 작업을 하는 이유는 무엇일까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지역성. 이미지의 특정 부분이 이제 특정 개념에 할당되었으므로 모델은 이제 두 가지를 알게 됩니다:\n- 이미지에서 어떤 개념이 더 많이 표현되는지\n- 이러한 개념이 이미지에서 어디에 위치하는지.\n\n일반인이 이해하기 쉽게 설명하면 SPARC와 다른 이미지 인코더 간의 주요 차이점은 텍스트 설명의 개별 단어를 이미지의 특정 부분에 할당한다는 사실에 있습니다.\n\n이러한 방식으로 이미지의 특정 부분의 그룹화된 임베딩이 '개' 단어 토큰에 크게 편향되어 있다면, 이미지의 해당 영역에는 아마도 개가 포함되어 있을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSIMA의 중요한 요소입니다. 3D 에이전트는 요청된 작업의 일환으로 상호 작용할 특정 객체를 식별할 수 있어야합니다.\n\n마지막으로, 비디오 인코더에 대해 이야기하면, 모델이 과거 상태를 고려할 수 있도록 포함되어 있습니다. 중요한 것은 비디오 인코더가 시간적 인식을 포함한다는 것인데, 이는 텍스트나 이미지 인코더가 제공할 수 없는 기능입니다.\n\n환경의 현재 상태뿐만 아니라 과거의 환경 상태와 취해진 조치에 따라 다음 조치를 취할지 결정하기 때문에 이는 중요합니다.\n\n## 최적의 정책 선택\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제공된 정보를 바탕으로 SIMA는 다른 인코더들에 의해 생성된 표현을 사용하여 변환 모델 세트를 활용합니다. 이 모델은 LLM이 단어를 예측하는 대신 작업을 출력하여 에이전트가 실행할 키보드 및 마우스 조작을 지시합니다.\n\n한편, 왜 Gemini, 구글의 MLLM을 사용하는 대신 모델의 주요 '두뇌'로 이상한 Transformers 세트를 사용했는지 궁금할 수도 있습니다.\n\n이유는 아마도 예산 때문인데, 연구자들 자신들이 기술 보고서에서 SIMA의 분명한 다음 단계는 Gemini를 사용하는 것이라고 인정했기 때문입니다.\n\n이것은 매우 흥미로운데, 최고의 '두뇌'를 사용하지 않았음에도 놀라운 결과를 얻었다고 하니, 이제 우리가 곧 살펴볼 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 진정한 일반화자\n\n지금쯤이면 알 수 있겠지만, 목표는 여태까지 한 게임에서도 능숙한 에이전트를 훈련시키는 것이었습니다. 게임을 해본 적이 없는 게임조차도요.\n\n훈련 후 SIMA 에이전트는 다양한 범주로 구분된 600가지 다양한 기본 작업을 수행할 수 있었습니다. 이러한 범주에는 네비게이션, 동물 상호작용, 음식 등이 포함되어 있었습니다:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_5.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSIMA가이러한작업을수행하고있는모습을여기에서확인할수있습니다. 게다가, SIMA는매우유망한결과를얻어 언급할만한가치가있습니다.\n\n우선, 다양한게임에서훈련을받았음에도불구하고, 평균적으로SIMA는 단일게임에서전문화된 에이전트들보다 더우수한성능을보였습니다... 그특정게임안에서:\n\n![image](/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_6.png)\n\n더욱인상적인점은, 여러가지다른게임에서, 에이전트가영이의업무(non-trivial tasks)를달성했으며, 이중대부분은얼마들이나예시없이(zero-shot tasks)성과를거두었으며, 다시한번전문화된 에이전트들을이겼습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_7.png\" /\u003e\n\n이는 과거에 본 적 없는 환경에 놓여도 모델이 일반적으로 잘 수행되었으며, 때로는 Goat Simulator 3와 같은 경우에는 전문화된 에이전트(해당 게임에서만 훈련된 에이전트)보다 뛰어난 성과를 보였습니다.\n\n그렇다면 이 모든 것이 의미하는 바는 무엇인가요?\n\n간단히 말하자면, 우리는 게임 간의 지식 전이의 구체적인 증거를 관찰하고 있다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다시 말해, 모델은 일부 게임에서 유용하게 적용할 수 있는 의미 있는 기술을 학습합니다 (예: 키보드로 이동하는 방법을 배우는 것과 같이).\n\n더욱이, 이러한 기술들은 상당히 높은 품질을 갖고 있어서 이 일반화된 에이전트가 많은 게임에서 특화된 에이전트들을 이기는 것을 의미하며, 이는 일반화된 접근 방식이 그들이 다양한 환경에 적용할 우수한 기술을 학습하는 데 도움이 된다는 것을 시사합니다.\n\nFigure AI의 발전과 함께 이러한 결과들이 어떠한 맥락에서 중요성을 얻을 수 있는 경우, 전체적으로 매우 인상적인 결과입니다.\n\n# 로보틱스에게 훌륭한 주였습니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI 로봇 기술은 요즘 정말 빠르게 발전하고 있어요.\n\n- 한 쪽에서, Figure AI는 우리가 점점 더 많은 수동 작업 범위를 알리는 인간형 로봇을 만들어내는 데 탁월한 실력을 보여줍니다.\n- 반면에, SIMA는 3D 환경에서 최초의 일반 적 에이전트를 보여주고 있음을 암시합니다.\n\n하지만 우리가 깨닫는 것은 시너지의 잠재력입니다.\n\n이러한 에이전트들을 실생활 상황으로 가져오는 데는 아직 이르지만, 이 두 분야 사이의 융합은 다음 단계로 자연스러운 발전을 이루고 있습니다; SIMA가 훈련의 장소일 뿐만 아니라, Figure AI 로봇들이 일반적인 에이전트의 구현체 역할을 하기 때문이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그리고 다른 기업들도 구체적 지능에 대한 자신들의 시사를 제시하며 경쟁이 치열해지고 있습니다. 많은 기존 기업들이 인류의 기술이 충분히 준비되어 있다고 느끼고 있어서 다음 큰 도전, 인공지능을 실생활에 적용하는 것에 착수할 준비가 되어있다고 느끼는 것이 분명합니다.","ogImage":{"url":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png"},"coverImage":"/assets/img/2024-05-17-AIsFirstBabyStepsinEmbodiedIntelligence_0.png","tag":["Tech"],"readingTime":9},{"title":"내일의 기계들","description":"","date":"2024-05-17 19:34","slug":"2024-05-17-MachinesofTomorrow","content":"\n\n# 내일의 기계: AI 원래부터 초지능 및 넘인간성까지, 어떻게 AI가 우리의 세계를 형성할 것인가\n\n아마존에서 \"내일의 기계\" 구매\n\n아마존이나 굿리드에서 리뷰 작성\n\n우리의 뉴스레터 구독하여 모든 챕터를 받아보세요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![Machines of Tomorrow](/assets/img/2024-05-17-MachinesofTomorrow_0.png)\n\n우리의 세상을 형성할 AI.\n\n모두가 쉽게 접근할 수 있는 쉬운 읽기 스타일로 작성된 \"Machines of Tomorrow\"은 인공 지능의 역사와 설명을 철저하게 소개함으로써 독자들이 실제 전망을 파악하고 최종적으로 이 책의 핵심 개념과 더 깊은 참여 수준을 얻게 합니다. 다가오는 수십 년 동안 사회들이 AI로 무엇을 기대해야 하는지에 대한.\n\nAI가 가까운 미래와 중기에 고용과 교육에 어떤 영향을 미칠까요? AI가 우리 사회, 경제, 정부, 문화 및 지정학에 어떤 영향을 미칠까요? AI 구현의 유토피아적 및 디스토피아적 결과는 무엇이며 누가 영향을 받을 것인가요? 중국과 새로운 AI 무기 경쟁에 대해 실제로 무슨 일이 벌어지고 있을까요? 합성생물학이 정말 인간-사이보그 공존으로 이끌 것이며 결국 인간 진화에 영향을 미칠까요? 이 기술의 불가피한 진전에 대비하려면 리더들에게 지금 어떤 것을 요구해야 할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPedro Uria-Recio와 Randy McGraw가 제기하고 대답한 몇 가지 신선하고 논쟁적인 질문들 중 일부입니다. Pedro Uria-Recio는 McKinsey 컨설턴트 출신, Randy McGraw는 AI 부문 최고 책임자 및 Amazon 전 임직원으로 이들은 수년간 이 기술을 전 세계적으로 활용해 왔습니다.\n\n기술자일지라도 주부일지라도 사업가일지라도 학생일지라도 유권자일지라도 정책 입안자일지라도 혹은 단순히 인류의 미래에 대해 궁금해하는 분이라도, \"Machines of Tomorrow\"은 인공지능의 복잡성을 탐험하는 데 필수적인 안내를 제공합니다.\n\n인류와 AI가 얽히는 지점.\n\nAI는 우리의 새로운 마음입니다. 로봇공학은 새로운 신체입니다. 우리는 탄소와 실리콘의 교차로에서 새로운 종이 어떻게 될까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nAI가 기하급수적으로 발전하고 있습니다.\n\n일반적인 AI. 안드로이드와 사이보그. 합성 생물학. 양자 컴퓨팅. 마음의 모방. 이 모든 것들이 어떻게 펼쳐질까요?\n\nAI 독재가 위협적입니다.\n\nAI는 진실을 무효화하고, 자유를 재정의하며, 일자리 부족을 불러올 것입니다. 우리는 아직도 AI를 모두의 이익을 위해 형성할 수 있을까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지구정치의 역학화.\n\n초인지능이 숭배받을 것입니다. 중국과 미국은 인공지능에 대한 견해를 놓고 충돌할 것입니다. 정치는 종족 정체성을 중심으로 펼쳐질 것입니다.\n\n인류의 위대한 서사시.\n\n신화에서 큐브릭까지. 아리스토텔레스에서 샘 알트만까지. 레오나르도 다 빈치에서 보스턴 다이내믹스까지. 오늘부터 초인지능까지.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음 장 `` 차례\n\n참고 자료, 용어 해설\n\n\"내일의 기계\"를 아마존에서 구입하세요\n\n아마존이나 구글 리드에서 리뷰 작성하기\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 뉴스레터 구독을 신청하시면 모든 챕터를 받아보실 수 있어요\n\n작성자: Pedro Uria-Recio, Randy McGraw와 함께\n\nmachinesoftomorrow.ai에서 확인해보세요\n\n![이미지](/assets/img/2024-05-17-MachinesofTomorrow_1.png)","ogImage":{"url":"/assets/img/2024-05-17-MachinesofTomorrow_0.png"},"coverImage":"/assets/img/2024-05-17-MachinesofTomorrow_0.png","tag":["Tech"],"readingTime":2},{"title":"라즈베리 파이에서 로컬 LLMs 및 VLMs 실행하기","description":"","date":"2024-05-17 19:32","slug":"2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi","content":"\n\n## Phi-2, Mistral, 그리고 LLaVA와 같은 모델을 Raspberry Pi에서 Ollama를 사용하여 로컬에서 실행하기\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png)\n\n자신의 기기에서 큰 언어 모델(LLMs)이나 시각 언어 모델(VLMs)을 실행해 보고 싶은 적이 있었나요? 아마 그렇겠죠. 그러나 모든 것을 처음부터 설정하고 환경을 관리하고 적절한 모델 가중치를 다운로드해야 한다는 생각, 그리고 기기가 해당 모델을 처리할 수 있는지에 대한 미심쩍음 때문에 조금 망설이셨을 겁니다.\n\n그보다 한 발 더 나아가보죠. 크기가 신용카드보다 작은 기기인 Raspberry Pi에서 자체 LLM 또는 VLM을 운영하고 있다고 상상해보세요. 불가능한 것일까요? 전혀 그렇지 않아요. 내가 이 게시물을 작성하고 있으니까 가능한 일이죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 가능하다면, 그렇게 해도 괜찮아요. 하지만 왜 그걸 하고 싶으세요?\n\n현재에는 가장자리의 LLM이 매우 비현실적으로 보입니다. 하지만 특정한 이러한 예외적인 사용 사례는 시간이 지나면 성숙해질 것이며, 기기에서 실행되는 올 로컬 생성형 AI 솔루션을 이용해 멋진 가장자리 솔루션이 구축될 것입니다.\n\n이것은 무엇이 가능한지 알아보고자 하는 것입니다. 만약 이것이 컴퓨팅 규모의 극단단에 가능하다면, 라즈베리 파이와 강력한 서버 GPU 사이의 어느 수준에서든 가능할 것입니다.\n\n전통적으로, 가장자리 AI는 컴퓨터 비전과 밀접하게 관련되어 왔습니다. LLMs와 VLMs의 배포를 탐색함으로써 가장자리에 이러한 새로운 면을 추가하면 이 분야에서 떠오르고 있는 흥미로운 측면을 발견할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n가장 중요한 것은, 최근에 얻은 Raspberry Pi 5에서 뭔가 재미있는 것을 하고 싶었습니다.\n\n그래서, Raspberry Pi에서 이 모든 것을 어떻게 구현할 수 있을까요? Ollama를 사용해서!\n\n## Ollama가 무엇인가요?\n\nOllama는 처음부터 설정하기 귀찮은 일 없이 개인 컴퓨터에서 로컬 LLM을 실행하는 데 최적의 솔루션이 되어 나타났습니다. 몇 가지 명령어로 모든 것을 문제없이 설정할 수 있습니다. 모든 것이 독립적으로 구성되어 있으며, 몇 가지 기기 및 모델에서의 제 경험에 따르면 훌륭하게 작동합니다. Raspberry Pi에서 실행시켜 놓고 원한다면 다른 응용프로그램 및 기기에서 호출할 수 있는 모델 추론을 위한 REST API까지 노출합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_1.png\" /\u003e\n\n또한, 명령 줄 인터페이스에 대해 걱정하는 사람들을 위해 웅장한 AI UI/UX인 Ollama 웹 UI도 있습니다. 이것은 바로 지금 기본적으로 로컬 ChatGPT 인터페이스입니다, 만약 당신이 원한다면요.\n\n이 두 오픈 소스 소프트웨어는 현재 최상의 로컬 호스팅 LLM 경험을 제공한다고 생각합니다.\n\nOllama 및 Ollama 웹 UI는 LLaVA와 같은 VLM도 지원하며, 이는 엣지 생성적 AI 사용 사례에 대한 더 많은 가능성을 엽니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 기술 요구 사항\n\n다음만 있으면 됩니다:\n\n- Raspberry Pi 5 (속도가 느린 설정을 위해 4 버전 사용 가능) — 7B 모델에 맞게 8GB RAM 모델을 선택하세요.\n- SD 카드 — 최소한 16GB, 크기가 클수록 더 많은 모델을 넣을 수 있습니다. Raspbian Bookworm이나 Ubuntu와 같은 적합한 OS가 이미 로드되어 있어야 합니다.\n- 인터넷 연결\n\n이전에 언급했듯이, Raspberry Pi에서 Ollama를 실행하는 것은 이미 하드웨어 스펙트럼의 극단에 가깝습니다. 기본적으로 Raspberry Pi보다 강력한 장치는 Linux 배포판을 실행하고 유사한 메모리 용량을 가지고 있다면, 이론적으로는 Ollama와 이 포스트에서 논의된 모델을 실행할 수 있어야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 1. Ollama 설치하기\n\nRaspberry Pi에 Ollama를 설치하기 위해서는 리소스를 절약하기 위해 Docker를 사용하지 않을 것입니다.\n\n터미널에서 다음 명령을 실행하세요.\n\n```js\ncurl https://ollama.ai/install.sh | sh\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 명령을 실행한 후 아래 이미지와 유사한 내용을 보게 될 것입니다.\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_2.png)\n\n출력에 따라서 Ollama가 실행 중인지 확인하려면 0.0.0.0:11434로 이동하십시오. 'WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only mode.'라는 메시지가 표시되는 것은 Raspberry Pi를 사용하고 있기 때문에 정상입니다. 그러나 NVIDIA GPU가 있는 것으로 알려진 장치에서 이 지침을 따르고 있다면, 무언가 잘못되었습니다.\n\n문제 또는 업데이트가 있는 경우 Ollama GitHub 리포지토리를 참조하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 명령 줄을 통한 LLM 실행\n\nOllama 공식 모델 라이브러리에서 Ollama를 사용하여 실행할 수 있는 모델 목록을 살펴보세요. 8GB 라즈베리 파이에서 7B보다 큰 모델은 맞지 않습니다. 마이크로소프트의 2.7B 크기인 Phi-2 LLM을 MIT 라이센스로 사용하겠습니다.\n\n기본 Phi-2 모델을 사용할 것이지만, 여기에서 찾을 수 있는 다른 태그 중에서 필요한 것을 자유롭게 사용해도 됩니다. Phi-2 모델 페이지를 확인하여 상호 작용하는 방법을 살펴보세요.\n\n터미널에서 다음을 실행하세요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n올라마 편리해요\n```\n\n아래 출력물과 유사한 것을 볼 때, 이미 라즈베리 파이에서 LLM이 작동 중임을 확인했습니다! 정말 간단해요.\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_3.png)\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_4.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기타 모델인 Mistral, Llama-2 등도 시도해 볼 수 있어요. 다만 모델 가중치를 저장할만큼 SD 카드에 충분한 공간이 있어야 해요.\n\n모델이 클수록 출력이 느려질 수 있어요. 예를 들어, Phi-2 2.7B에서는 초당 약 4토큰을 얻을 수 있어요. 하지만 Mistral 7B를 사용하면 세대 속도가 초당 약 2토큰으로 느려집니다. 토큰은 대략 한 단어와 동일한 양이에요.\n\n![이미지](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_5.png)\n\n이제 Raspberry Pi에서 LLMs가 실행되고 있지만 아직 끝나지 않았어요. 터미널이 모두에게 익숙한 것은 아니에요. Ollama 웹 UI도 실행해 보겠어요!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3. Ollama 웹 UI 설치 및 실행\n\n공식 Ollama 웹 UI GitHub 저장소의 지침을 따라 Docker 없이 설치할 것입니다. 최소한 Node.js 버전이 `= 20.10 이 되어야 한다고 권장하므로, 해당 권장사항을 따를 것입니다. 또한 Python은 적어도 3.11 이어야 한다고 권장하나, Raspbian OS에는 이미 그 버전이 설치되어 있습니다.\n\n먼저 Node.js를 설치해야 합니다. 터미널에서 다음을 실행하세요.\n\n```js\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - \u0026\u0026\\\nsudo apt-get install -y nodejs\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 향후 독자들을 위해 20.x를 더 적합한 버전으로 변경하려면 변경하십시오.\n\n그런 다음 아래 코드 블록을 실행하십시오.\n\n```js\ngit clone https://github.com/ollama-webui/ollama-webui.git\ncd ollama-webui/\n\n# 필요한 .env 파일 복사\ncp -RPp example.env .env\n\n# Node를 사용하여 프론트엔드 빌드\nnpm i\nnpm run build\n\n# 백엔드와 함께 프론트엔드 제공\ncd ./backend\npip install -r requirements.txt -- break-system-packages\nsh start.sh\n```\n\n이는 GitHub에서 제공된 것을 약간 수정한 것입니다. 가상 환경을 사용하거나 --break-system-packages 플래그를 사용하는 등의 최선의 방법을 따르지 않고 간단하게하고자 하였으니 유의하시기 바랍니다. uvicorn을 찾을 수 없다는 오류가 발생하면 터미널 세션을 재시작하십시오.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 것이 제대로 진행되면 라즈베리 파이의 8080 포트를 통해 http://0.0.0.0:8080로 Ollama 웹 UI에 액세스할 수 있거나, 동일한 네트워크에서 다른 장치를 통해 접속하고 있다면 http://라즈베리 파이의 로컬 주소:8080/을 통해 접속할 수 있습니다.\n\n\n![이미지 설명](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_6.png)\n\n\n계정을 생성하고 로그인한 후, 아래 이미지와 유사한 화면이 보여야 합니다.\n\n\n![이미지 설명](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_7.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 가중치를 이전에 다운로드했다면, 아래와 같이 드롭다운 메뉴에 표시될 것입니다. 그렇지 않은 경우 설정으로 이동하여 모델을 다운로드할 수 있습니다.\n\n![이미지 1](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_8.png)\n\n![이미지 2](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_9.png)\n\n전체적으로 사용자 인터페이스는 매우 깔끔하고 직관적이므로 설명할 것이 별로 없어요. 정말로 아주 훌륭하게 구현된 오픈 소스 프로젝트입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_10.png)\n\n## 4. Ollama 웹 UI를 통해 VLM 실행하기\n\n이 글을 시작할 때 언급했듯이, VLM도 실행할 수 있습니다. LLaVA를 실행해보겠습니다. 이것은 Ollama에서도 지원되는 인기 있는 오픈 소스 VLM입니다. 실행하려면 인터페이스를 통해 'llava'를 가져와서 가중치를 다운로드하면 됩니다.\n\n안타깝게도, LLM과는 달리 Raspberry Pi에서 이미지를 해석하는 설정에 꽤 많은 시간이 걸립니다. 아래 예시는 처리하는 데 약 6분 정도 걸렸습니다. 대부분의 시간은 아마 이미지 측면이 아직 제대로 최적화되지 않았기 때문인데, 이는 앞으로 확실히 변경될 것입니다. 토큰 생성 속도는 초당 약 2토큰입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![image](/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_11.png)\n\n## 모든 것을 감싸며\n\n지금까지 이 기사의 목표를 거의 달성한 상태입니다. 요약하자면, 우리는 Ollama와 Ollama Web UI를 사용하여 Raspberry Pi에서 Phi-2, Mistral, LLaVA와 같은 LLMs와 VLMs를 실행하는 방법을 성공적으로 이끌어냈습니다.\n\nRaspberry Pi나 다른 작고 가장한 장치에서 실행되는 로컬 호스팅 LLMs에 대한 다양한 사용 사례들을 상상할 수 있습니다. 특히, 4토큰/초가 Phi-2 크기의 모델을 위해 일부 사용 사례에 대해 스트리밍 속도로는 합당해 보입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n'작은' LLM 및 VLM의 분야는 얼마 전에 다소 모순적으로 이름이 지어졌지만 '큰' 지칭을 받는 활발한 연구 분야입니다. 최근에는 꽤 많은 모델이 출시되었습니다. 이 새로운 트렌드가 계속되어 더 효율적이고 간결한 모델들이 계속 출시되기를 바랍니다! 다가오는 몇 달 동안 살펴봐야 할 분야입니다.\n\n면책 성명: 저는 Ollama나 Ollama Web UI와 어떠한 관련도 없습니다. 모든 의견과 견해는 제 개인적인 것이며 어떤 조직도 대표하지 않습니다.","ogImage":{"url":"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png"},"coverImage":"/assets/img/2024-05-17-RunningLocalLLMsandVLMsontheRaspberryPi_0.png","tag":["Tech"],"readingTime":7}],"page":"55","totalPageCount":61,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":2},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"55"},"buildId":"0asLlD6on3tm8cIfzBaxd","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>