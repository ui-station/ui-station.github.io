<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/72" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/72" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BHFR6GTH9P"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-BHFR6GTH9P');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/a22d13b8e6bc8203.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a22d13b8e6bc8203.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/bb_yO9GbCvdfc_n71SfUf/_buildManifest.js" defer=""></script><script src="/_next/static/bb_yO9GbCvdfc_n71SfUf/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="Keras 30 튜토리얼 엔드투엔드 딥 러닝 프로젝트 가이드" href="/post/2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Keras 30 튜토리얼 엔드투엔드 딥 러닝 프로젝트 가이드" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Keras 30 튜토리얼 엔드투엔드 딥 러닝 프로젝트 가이드" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">Keras 30 튜토리얼 엔드투엔드 딥 러닝 프로젝트 가이드</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">25<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="주체적인 RAG 개인 맞춤 및 최적화된 지식 보조 언어 모델" href="/post/2024-05-20-AgenticRAGPersonalizingandOptimizingKnowledge-AugmentedLanguageModels"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="주체적인 RAG 개인 맞춤 및 최적화된 지식 보조 언어 모델" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-AgenticRAGPersonalizingandOptimizingKnowledge-AugmentedLanguageModels_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="주체적인 RAG 개인 맞춤 및 최적화된 지식 보조 언어 모델" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">주체적인 RAG 개인 맞춤 및 최적화된 지식 보조 언어 모델</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인간 수준의 로봇을 위한 훌륭한 AI로의 길" href="/post/2024-05-20-ThePathtoGreatAIforHuman-CapableRobots"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인간 수준의 로봇을 위한 훌륭한 AI로의 길" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-ThePathtoGreatAIforHuman-CapableRobots_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인간 수준의 로봇을 위한 훌륭한 AI로의 길" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">인간 수준의 로봇을 위한 훌륭한 AI로의 길</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="하이브리드 A star에 대해 부드럽게 소개하기" href="/post/2024-05-20-GentleintroductiontoHybridAstar"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="하이브리드 A star에 대해 부드럽게 소개하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-GentleintroductiontoHybridAstar_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="하이브리드 A star에 대해 부드럽게 소개하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">하이브리드 A star에 대해 부드럽게 소개하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="무엇보다도 새로운 글로벌 디자인 트렌드 " href="/post/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="무엇보다도 새로운 글로벌 디자인 트렌드 " loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="무엇보다도 새로운 글로벌 디자인 트렌드 " loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">무엇보다도 새로운 글로벌 디자인 트렌드 </strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">12<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Raspberry Pi 5에 ROS2 Humble을 설치하고 Docker를 사용하여 micro-ROS를 통해 ESP32와 통신하는 방법" href="/post/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Raspberry Pi 5에 ROS2 Humble을 설치하고 Docker를 사용하여 micro-ROS를 통해 ESP32와 통신하는 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Raspberry Pi 5에 ROS2 Humble을 설치하고 Docker를 사용하여 micro-ROS를 통해 ESP32와 통신하는 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">Raspberry Pi 5에 ROS2 Humble을 설치하고 Docker를 사용하여 micro-ROS를 통해 ESP32와 통신하는 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">18<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="라즈베리 파이 Pico와 MicroPython으로 DIY 쿼드콥터 드론을 만들어봅시다" href="/post/2024-05-20-TakingFlightwiththeRaspberryPiPicoMicroPythonDIYQuadcopterDrone"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="라즈베리 파이 Pico와 MicroPython으로 DIY 쿼드콥터 드론을 만들어봅시다" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-TakingFlightwiththeRaspberryPiPicoMicroPythonDIYQuadcopterDrone_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="라즈베리 파이 Pico와 MicroPython으로 DIY 쿼드콥터 드론을 만들어봅시다" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">라즈베리 파이 Pico와 MicroPython으로 DIY 쿼드콥터 드론을 만들어봅시다</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Raspberry Pi로 Slurm HPC 클러스터 구축하기 단계별 안내" href="/post/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Raspberry Pi로 Slurm HPC 클러스터 구축하기 단계별 안내" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Raspberry Pi로 Slurm HPC 클러스터 구축하기 단계별 안내" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">Raspberry Pi로 Slurm HPC 클러스터 구축하기 단계별 안내</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">30<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="인텔 N100 알더 레이크를 활용한 예산 친화적 미니 홈 서버 구축하기" href="/post/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="인텔 N100 알더 레이크를 활용한 예산 친화적 미니 홈 서버 구축하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="인텔 N100 알더 레이크를 활용한 예산 친화적 미니 홈 서버 구축하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">인텔 N100 알더 레이크를 활용한 예산 친화적 미니 홈 서버 구축하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="Raspberry Pi Pico를 MicroPython으로 사용하여 MPU-6050을 어떻게 사용하는지 알아보기" href="/post/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="Raspberry Pi Pico를 MicroPython으로 사용하여 MPU-6050을 어떻게 사용하는지 알아보기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="Raspberry Pi Pico를 MicroPython으로 사용하여 MPU-6050을 어떻게 사용하는지 알아보기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">Raspberry Pi Pico를 MicroPython으로 사용하여 MPU-6050을 어떻게 사용하는지 알아보기</strong><div class="PostList_meta__VCFLX"><span class="date">May 20, 2024</span><span class="PostList_reading_time__6CBMQ">4<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><button type="button" class="page_button -prev">&lt;</button><a class="link" href="/posts/61">61</a><a class="link" href="/posts/62">62</a><a class="link" href="/posts/63">63</a><a class="link" href="/posts/64">64</a><a class="link" href="/posts/65">65</a><a class="link" href="/posts/66">66</a><a class="link" href="/posts/67">67</a><a class="link" href="/posts/68">68</a><a class="link" href="/posts/69">69</a><a class="link" href="/posts/70">70</a><a class="link" href="/posts/71">71</a><a class="link posts_-active__YVJEi" href="/posts/72">72</a><a class="link" href="/posts/73">73</a><a class="link" href="/posts/74">74</a><a class="link" href="/posts/75">75</a><a class="link" href="/posts/76">76</a><a class="link" href="/posts/77">77</a><a class="link" href="/posts/78">78</a><a class="link" href="/posts/79">79</a><a class="link" href="/posts/80">80</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Keras 30 튜토리얼 엔드투엔드 딥 러닝 프로젝트 가이드","description":"","date":"2024-05-20 20:09","slug":"2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide","content":"\n![이미지](/assets/img/2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide_0.png)\n\n# 소개\n\n조금 전부터 Pytorch를 사용하기 시작했지만, 여전히 Keras의 간결한 코드 스타일과 신경망 모델을 몇 줄의 코드로 구현할 수 있던 옛날을 그리워합니다.\n\n그래서 Keras가 지난 11월에 TensorFlow에 추가하여 Pytorch와 Jax를 백엔드로 지원한다고 발표했을 때 매우 흥분했습니다!\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러나 모든 것이 완벽하지는 않았습니다: 최근에 Keras 3.0이 출시되었기 때문에 관련 튜토리얼과 문서는 아직 따라가지 못한 상태였고, 코드 이관 중에 몇 가지 어려움을 겪었습니다.\n\n다행히도 노력 끝에 이제 버전 3.0을 원활하게 사용하여 다양한 end-to-end 모델 개발을 할 수 있게 되었습니다.\n\n이 글에서는 Keras 3.0을 사용하는 데 도움이 되는 실용적인 경험 몇 가지를 공유하겠습니다. Keras 3.0의 서브클래싱 API를 사용하여 end-to-end 프로젝트를 완전히 처음부터 구축하는 방법을 알려주기 위해 전형적인 인코더-디코더 순환 신경망을 예로 들 것이며, 백엔드로 Pytorch를 사용할 때 고려해야 할 세부 사항에 대해 논의할 것입니다.\n\n자, 시작해 보겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 프레임워크 설치 및 환경 설정\n\n## 프레임워크 설치\n\n케라스 3.0 (또는 최신 버전)를 설치하는 것은 간단합니다. 공식 웹사이트의 시작 가이드 문서를 따라하면 됩니다.\n\n케라스를 설치하기 전에, 해당 CUDA 버전에 맞춰 Pytorch를 먼저 설치하는 것을 권장합니다. 사용하는 그래픽 카드 드라이버 지원에 따라 CUDA 11.8 또는 CUDA 12.1이 작동합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n파이토치를 백엔드로 사용할 수는 있지만, Keras 설치 과정에서 여전히 기본적으로 Tensorflow 버전 2.16.1이 설치됩니다.\n\n이 버전의 Tensorflow는 CUDA 12.3에 기반하여 컴파일되었기 때문에 Keras를 설치한 후 CUDA가 없다는 경고 메시지를 만날 수 있습니다(이 이슈를 확인하세요).\n\n```js\nCould not find cuda drivers on your machine, GPU will not be used.\n```\n\n우리는 백엔드로 파이토치를 사용 중이므로 이 경고를 무시하는 것을 권장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n혹시라도 Tensorflow 로그를 계속 보이지 않도록 영구적으로 설정하고 싶으시면, 시스템 변수를 설정할 수도 있어요.\n\n```js\nimport os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n```\n\n## 환경 설정\n\nPyTorch와 Keras를 모두 설치한 후에는, Keras의 백엔드를 PyTorch로 설정하기 위해 환경 변수를 설정해야 해요. 이 작업은 두 가지 방법으로 할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 설정 파일을 수정해주세요.\n- 환경 변수를 설정해주세요.\n\n먼저, 설정 파일 방법에 대해 논의해보겠습니다.\n\nKeras의 설정 파일은 ~/.keras/keras.json에 있습니다. Windows를 사용 중이라면, 이 파일은 `사용자 디렉토리`/.keras/keras.json에 있습니다.\n\n물론, KERAS_HOME 환경 변수를 설정하여 .keras 디렉토리의 위치도 변경할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n먼저 Keras를 처음 설치한 후에는 .keras 디렉토리를 바로 찾을 수 없을 수 있습니다. 이럴 때는 IPython이나 Jupyter Notebook에서 import keras를 실행하여 디렉토리를 찾을 수 있습니다.\n\n그런 다음, keras.json 파일의 \"backend\" 키의 값을 \"torch\"로 변경하십시오.\n\n```js\n{\n    \"floatx\": \"float32\",\n    \"epsilon\": 1e-07,\n    \"backend\": \"torch\",\n    \"image_data_format\": \"channels_last\"\n}\n```\n\n프로덕션 시스템이거나 Colab과 같은 클라우드 환경을 사용하는 경우, 구성 파일을 수정할 수 없을 수 있습니다. 이런 경우에는 환경 변수를 설정하여 이 문제를 해결할 수 있습니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nos.environ[\"KERAS_BACKEND\"] = \"torch\";\n```\n\nKeras 백엔드를 설정한 후, 다음 코드로 확인할 수 있어요:\n\n```js\nIn:  import keras\n     keras.config.backend()\n\nOut: 'torch'\n```\n\n준비가 완료되면, 오늘의 프로젝트 실습을 공식적으로 시작할 거예요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 프로젝트 실습: 시작부터 끝까지의 예시\n\n프레임워크를 배우는 가장 빠른 방법은 실제 프로젝트를 실습하는 것입니다. 그래서 이제 제 약속을 이행할 시간입니다.\n\n나는 당신에게 서브클래싱 API를 사용하여 신경 기계 번역(NMT) 모델을 구현하는 단계별 안내를 제공하고, Keras 3.0을 사용하는 몇 가지 세부 정보를 설명할 것입니다.\n\n## 이론 소개\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nNMT 모델에 익숙하지 않다면, 간단히 소개해 드리겠습니다:\n\nNMT는 인코더-디코더 구조를 기반으로 한 순환 신경망 모델의 한 유형입니다.\n\n이 구조에서는 임베딩 레이어와 RNN(이 글에서는 LSTM을 사용합니다) 레이어가 인코더를 형성하고, 또 다른 임베딩 레이어와 RNN 레이어가 디코더를 형성합니다.\n\n원본 텍스트는 벡터화된 후 인코더 모듈로 입력됩니다. 일련의 단계를 거쳐 최종 상태가 디코더 모듈로 입력됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n또한 대상 텍스트도 디코더 모듈에 입력되지만, 디코더로 들어가기 전에 한 단계 앞으로 오프셋이 적용됩니다. 따라서 대상 텍스트의 처음 부분은 시작 시퀀스(SOS) 플레이스홀더로 시작합니다.\n\n인코더의 입력 상태와 대상 텍스트의 입력은 디코더에서 순환 계산을 통해 처리되어 최종적으로 Dense 레이어에 출력됩니다. 거기서 각 텍스트 벡터의 확률을 계산해 대상 텍스트의 단어 벡터와 비교하고 손실을 계산합니다.\n\n따라서 대상 텍스트의 끝을 표시하기 위해 대상 텍스트의 끝에 종결 시퀀스(EOS) 플레이스홀더를 추가합니다.\n\n아래 다이어그램에서 전체 아키텍처가 표시됩니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n당연히 Transformer 아키텍처의 인기로 인해 케라스의 KerasNLP 패키지도 Bert 및 GPT와 같은 다양한 사전 훈련 모델을 제공합니다. 이 기사는 하지만 Keras 3.0을 사용하는 방법에 중점을 두므로 기본적인 RNN 네트워크를 사용하는 것이 충분합니다.\n\n**모듈 및 플로우차트**\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n프로덕션에 완전히 준비된 프로젝트이기 때문에, 우리는 Keras 3.0의 서븁클래싱 API를 기반으로 모듈을 구축합니다.\n\n각 모듈과 그들의 상호작용을 명확히 이해하기 위해, 아래의 플로우차트를 만들었습니다:\n\n![flowchart](/assets/img/2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide_2.png)\n\n플로우차트의 디자인에 따라 코드를 작성할 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 패키지 가져오기\n\nJupyter Notebook 환경에서는 프로젝트 첫부분에 관련 패키지를 모두 가져오는 것을 좋아해요.\n\n이렇게 하면 중간에 뭔가 빠트린 게 있어도 한 군데에서 추가하면 되니까 import 셀을 찾느라 시간을 낭비할 필요가 없어요:\n\n```js\nfrom pathlib import Path\nimport pickle\n\nimport keras\nfrom keras import layers, utils\nimport numpy as np\n\nutils.set_random_seed(42)\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n약간의 소소한 팁이에요: utils.set_random_seed 메서드를 사용하면 Python, Numpy, Pytorch의 랜덤 시드를 한 줄의 코드로 설정할 수 있어서 정말 편리해요.\n\n## 데이터 준비\n\n시작하기 전에 적절한 데이터를 선택해야 해요. 과거의 인코더-디코더 모델과 마찬가지로, 저희는 spa-eng 텍스트 데이터셋을 선택했어요.\n\n다운로드 후, 먼저 spa.txt 파일의 내용을 확인해볼게요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n비슷한 점이 두드러져요. La similitud es extraña. CC-BY 2.0 (프랑스) 속성: tatoeba.org #2691302 (CM) \u0026 #5941808 (albrusgher)\n비슷한 점이 엄청나네요. El parecido es asombroso. CC-BY 2.0 (프랑스) 속성: tatoeba.org #2691302 (CM) \u0026 #6026125 (albrusgher)\n결과가 약속 있어 보여요. Los resultados se antojan prometedores. CC-BY 2.0 (프랑스) 속성: tatoeba.org #8480484 (shekitten) \u0026 #8464272 (arh)\n부자들은 많은 친구들을 가지고 있어요. Los ricos tienen muchos amigos. CC-BY 2.0 (프랑스) 속성: tatoeba.org #1579047 (sam_m) \u0026 #1457378 (marcelostockle)\n\n위 내용을 보시다시피 적어도 세 개의 열을 포함하는 내용이 있습니다. 첫 번째 열은 원문이고 두 번째 열은 대상 텍스트로 탭으로 구분되어 있습니다.\n\n파일이 크지 않기 때문에 numpy의 genfromtxt 메서드를 사용하여 이 데이터셋을 직접 읽어올 수 있습니다.\n\n```python\ntext_file = Path(\"./temp/eng-spanish/spa-eng/spa.txt\")\n\npairs = np.genfromtxt(text_file, delimiter=\"\\t\", dtype=str,\n                     usecols=(0, 1), encoding=\"utf-8\",\n                     autostrip=True,\n                     converters={1: lambda x: x.replace(\"¡\", \"\").replace(\"¿\", \"\")})\nnp.random.shuffle(pairs)\nsentence_en, sentence_es = pairs[:, 0], pairs[:, 1]\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로, 처리 결과를 확인해봅시다:\n\n```js\nIn:   print(f\"{sentence_en[0]} =\u003e {sentence_es[0]}\")\n\nOut:  I'm really sorry. =\u003e Realmente lo siento.\n```\n\n좋아요, 문제 없어요.\n\n## 데이터 전처리\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로, 텍스트 콘텐츠를 단어 벡터 데이터로 변환하기 위해 전처리해야 합니다.\n\n우선, 몇 가지 상수를 정의합니다:\n\n```js\nclass Configure:\n    VOCAB_SIZE: int = 1000\n    MAX_LENGTH: int = 50\n    SOS: str = 'startofseq'\n    EOS: str = 'endofseq'\n```\n\n그런 다음 데이터 처리 파이프라인을 시작합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nKeras 3.0에서는 백엔드로 PyTorch를 선택했더라도 TextVectorization 레이어는 여전히 TensorFlow를 기반으로 구현되어 있습니다.\n\n따라서 Keras 모델 내에서 TextVectorization을 레이어로 사용할 수 없고, 전처리 파이프라인에서 별도로 사용해야 합니다.\n\n이로 인해 문제가 발생합니다. 훈련된 모델을 추론 작업을 위한 프로덕션 시스템으로 이관할 때 TextVectorization 어휘 없이는 벡터화를 수행할 수 없습니다.\n\n그래서 우리는 어휘를 지속시키고 재사용해야 하지만, Keras 3.0의 TextVectorization 지속에 관한 몇 가지 문제가 있습니다. 이 부분에 대해 나중에 논의하겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n텍스트 전처리 모듈을 사용하여 벡터화를 수행할 것입니다. 여기에 구체적인 코드가 있습니다:\n\n```js\nclass TextPreprocessor:\n    def __init__(self,\n                 en_config=None, es_config=None):\n        if en_config is None:\n            self.text_vec_layer_en = layers.TextVectorization(\n                Configure.VOCAB_SIZE, output_sequence_length=Configure.MAX_LENGTH\n            )\n        else:\n            self.text_vec_layer_en = layers.TextVectorization.from_config(en_config)\n\n        if es_config is None:\n            self.text_vec_layer_es = layers.TextVectorization(\n                Configure.VOCAB_SIZE, output_sequence_length=Configure.MAX_LENGTH\n            )\n        else:\n            self.text_vec_layer_es = layers.TextVectorization.from_config(es_config)\n\n        self.adapted = False\n        self.sos = Configure.SOS\n        self.eos = Configure.EOS\n\n    def adapt(self, en_sentences: list[str], es_sentences: list[str]) -\u003e None:\n        self.text_vec_layer_en.adapt(en_sentences)\n        self.text_vec_layer_es.adapt([f\"{self.sos} {s} {self.eos}\" for s in es_sentences])\n        self.adapted = True\n\n    def en_vocabulary(self):\n        return self.text_vec_layer_en.get_vocabulary()\n\n    def es_vocabulary(self):\n        return self.text_vec_layer_es.get_vocabulary()\n\n    def vectorize_en(self, en_sentences: list[str]):\n        return self.text_vec_layer_en(en_sentences)\n\n    def vectorize_es(self, es_sentences: list[str]):\n        return self.text_vec_layer_es(es_sentences)\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n\n    def get_config(self):\n        en_config = self.text_vec_layer_en.get_config()\n        en_config['vocabulary'] = self.en_vocabulary()\n        es_config = self.text_vec_layer_es.get_config()\n        es_config['vocabulary'] = self.es_vocabulary()\n        return {'en_config': en_config,\n                'es_config': es_config}\n\n    def save(self, filepath: str):\n        if not self.adapted:\n            raise RuntimeError(\"Layer hasn't been adapted yet.\")\n        if filepath is None:\n            raise ValueError(\"A file path needs to be defined.\")\n        if not filepath.endswith('.pkl'):\n            raise ValueError(\"The file path needs to end in .pkl.\")\n        pickle.dump({\n            'config': self.get_config()\n        }, open(filepath, 'wb'))\n\n    @classmethod\n    def load(cls, filepath: str):\n        conf = pickle.load(open(filepath, 'rb'))\n        instance = cls(**conf['config'])\n        return instance\n```\n\n이 모듈이 하는 일을 설명해 드리겠습니다:\n\n- 원본 텍스트와 대상 텍스트 모두 벡터화해야 하기 때문에, 이 모듈에는 두 TextVectorization 레이어가 포함되어 있습니다.\n- 적응 후에는 이 모듈이 원본 및 대상 텍스트의 어휘를 보유합니다. 이렇게 하면, 프로덕션 시스템에 배포할 때 TextVectorization을 다시 적응시킬 필요가 없습니다.\n- 이 모듈은 영속성을 제공하기 위해 pickle 모듈을 사용합니다. get_config 메서드를 사용하여 두 TextVectorization 레이어의 구성을 가져와 저장할 수 있습니다. 또한 from_config를 사용하여 저장된 구성에서 모듈의 인스턴스를 직접 초기화할 수 있습니다.\n- 그러나 get_config 메서드를 사용할 때 어휘가 검색되지 않았습니다 (현재 Keras 버전 3.3을 사용하고 있으며 이것이 버그인지 확실하지 않습니다), 따라서 어휘를 따로 가져오기 위해 get_vocabulary 메서드를 사용해야 했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n텍스트 전처리기 = TextPreprocessor()\n텍스트_전처리기.적응(영어_문장, 스페인어_문장)\n텍스트_전처리기.저장('./데이터/텍스트_전처리기.pkl')\n```\n\n두 언어의 어휘를 확인해보세요:\n\n```js\nIn:   텍스트_전처리기.en_vocabulary()[:10]\nOut:  ['', '[UNK]', 'i', 'the', 'to', 'you', 'tom', 'a', 'is', 'he']\n\nIn:   텍스트_전처리기.es_vocabulary()[:10]\nOut:  ['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'no', 'tom', 'a', 'la']\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n문제없어요.\n\nTextPreprocessor 모듈이 준비되면 훈련 및 검증 세트를 분할하고 벡터화 작업을 시작할 수 있어요. 대상 텍스트는 디코더 모듈의 입력으로도 작동하기 때문에 X_train_dec 및 X_valid_dec와 같은 두 가지 추가 기능 세트가 있어요:\n\n```js\nX_train = text_preprocessor.vectorize_en(sentence_en[:100_000])\nX_valid = text_preprocessor.vectorize_en(sentence_en[100_000:])\n\nX_train_dec = text_preprocessor.vectorize_es([f\"{Configure.SOS} {s}\" for s in sentence_es[:100_000]])\nX_valid_dec = text_preprocessor.vectorize_es([f\"{Configure.SOS} {s}\" for s in sentence_es[100_000:]])\n\ny_train = text_preprocessor.vectorize_es([f\"{s} {Configure.EOS}\" for s in sentence_es[:100_000]])\ny_valid = text_preprocessor.vectorize_es([f\"{s} {Configure.EOS}\" for s in sentence_es[100_000:]])\n```\n\n## 인코더-디코더 모델 구현\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이전 아키텍처 다이어그램에서 설명했듯이 전체 모델은 인코더 및 디코더 부분으로 나뉩니다. 따라서 각 부분에 대해 keras.layers.Layer를 기반으로 두 개의 사용자 정의 하위 클래스를 구현합니다.\n\n각 사용자 정의 Layer에 대해 **init**, call 및 get_config 메서드를 구현하는 것이 중요합니다.\n\n- **init** 메서드는 Layer의 멤버 변수, 가중치 및 하위 레이어를 초기화합니다.\n- call 메서드는 Keras의 Functional API와 유사하게 작동하며 입력을 매개변수로 받아 처리 후 Layer의 출력을 반환합니다.\n- get_config 메서드는 모델을 저장할 때 Layer의 구성을 검색하는 데 사용됩니다.\n\n인코더 Layer:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n@keras.saving.register_keras_serializable()\nclass Encoder(keras.layers.Layer):\n    def __init__(self, embed_size: int = 128, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_size = embed_size\n\n        self.encoder_embedding_layer = layers.Embedding(input_dim=Configure.VOCAB_SIZE,\n                                                        output_dim=self.embed_size,\n                                                        mask_zero=True)\n        self.encoder = layers.LSTM(512, return_state=True)\n\n    def call(self, inputs):\n        encoder_embeddings = self.encoder_embedding_layer(inputs)\n        encoder_outputs, *encoder_state = self.encoder(encoder_embeddings)\n        return encoder_outputs, encoder_state\n\n    def get_config(self):\n        config = {\"embed_size\": self.embed_size}\n        base_config = super().get_config()\n        return config | base_config\n```\n\n인코더에서는 LSTM의 return_state 매개변수를 True로 설정했습니다. 이렇게 하면 LSTM의 최종 상태를 디코더 레이어가 사용할 수 있도록 출력으로 반환합니다.\n\n디코더 레이어:\n\n```js\n@keras.saving.register_keras_serializable()\nclass Decoder(keras.layers.Layer):\n    def __init__(self, embed_size: int = 128, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_size = embed_size\n\n        self.decoder_embedding_layer = layers.Embedding(input_dim=Configure.VOCAB_SIZE,\n                                                        output_dim=self.embed_size,\n                                                        mask_zero=True)\n        self.decoder = layers.LSTM(512, return_sequences=True)\n\n    def call(self, inputs, initial_state=None):\n        decoder_embeddings = self.decoder_embedding_layer(inputs)\n        decoder_outputs = self.decoder(decoder_embeddings,\n                                       initial_state=initial_state)\n        return decoder_outputs\n\n    def get_config(self):\n        config = {\"embed_size\": self.embed_size}\n        base_config = super().get_config()\n        return config | base_config\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nDecoder에서 데이터 입력을받는 것 외에도 call 메서드는 초기 상태 함수를 통해 Encoder의 입력도 받아 모듈의 출력을 반환합니다.\n\n또한 keras.layers.Layer와 유사하게 **init**, call 및 get_config 메서드를 구현해야 하는 사용자 정의 모델인 NMTModel을 구현합니다.\n\n```python\n@keras.saving.register_keras_serializable()\nclass NMTModel(keras.models.Model):\n    embed_size: int = 128\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.encoder = Encoder(self.embed_size)\n        self.decoder = Decoder(self.embed_size)\n\n        self.out = layers.Dense(Configure.VOCAB_SIZE, activation='softmax')\n\n    def call(self, inputs):\n        encoder_inputs, decoder_inputs = inputs\n\n        encoder_outputs, encoder_state = self.encoder(encoder_inputs)\n        decoder_outputs = self.decoder(decoder_inputs, initial_state=encoder_state)\n        out_proba = self.out(decoder_outputs)\n        return out_proba\n\n    def get_config(self):\n        base_config = super().get_config()\n        return base_config\n```\n\n- Model에서는 Dense 레이어를 초기화하여 Decoder의 출력을 단어 벡터 결과로 변환합니다.\n- call 메서드는 두 개의 입력을 가져와서 쉽게 구분할 수 있습니다.\n- Layer 및 Model은 모델을 저장할 때 올바른 직렬화를 보장하기 위해 @keras.saving.register_keras_serializable() 데코레이터를 가져야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 모델 훈련\n\n모델을 정의한 후, 훈련 단계로 진행합니다:\n\n```js\nnmt_model = NMTModel();\nnmt_model.compile(\n  (loss = \"sparse_categorical_crossentropy\"),\n  (optimizer = \"nadam\"),\n  (metrics = [\"accuracy\"])\n);\ncheckpoint = keras.callbacks.ModelCheckpoint(\n  \"./data/nmt_model.keras\",\n  (monitor = \"val_accuracy\"),\n  (save_best_only = True)\n);\nnmt_model.fit(\n  (X_train, X_train_dec),\n  y_train,\n  (epochs = 1),\n  (validation_data = ((X_valid, X_valid_dec), y_valid)),\n  (batch_size = 128),\n  (callbacks = [checkpoint])\n);\n```\n\n이 코드 부분에서:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 먼저 모델 인스턴스를 컴파일하는 compile 메소드를 호출합니다. 여기서는 손실(loss), 옵티마이저(optimizer), 및 메트릭(metrics) 등을 정의합니다.\n- ModelCheckpoint 콜백을 설정하여 훈련 후 최고의 val_accuracy를 가진 모델을 저장합니다.\n- fit 메소드를 사용하여 X_train 및 X_train_dec를 x 매개변수에 튜플로 전달하고 validation_data도 유사하게 처리합니다.\n- 이것은 따뜻한 모델이라 epochs를 1로 설정했습니다. epochs 및 batch_size 값을 필요에 따라 조정할 수 있습니다.\n- Keras 3.0은 Pytorch의 DataLoader도 지원하며 keras.utils.PyDataset을 기반으로 한 backend에 대한 이식 가능한 전처리 파이프라인을 구현할 수도 있습니다. 다음 기사에서 이들을 사용하는 방법을 설명할 수 있어요.\n\n훈련이 완료되면 모델을 저장해야 합니다.\n\n## 추론 작업\n\n훈련 후, 저장된 어휘 및 모델과 함께 해당 코드 모듈을 제품 시스템으로 추론 작업을 위해 배포할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델의 Dense 레이어가 어휘 사전의 각 단어 벡터에 대한 확률을 출력하기 때문에 추론된 각 단어를 이전 결과와 병합하여 원본 텍스트와 함께 다음 단어를 예측하기 위해 다시 입력해야 합니다:\n\n```js\npreprocessor = TextPreprocessor.load('./data/text_preprocessor.pkl')\nnmt_model = keras.saving.load_model('./data/nmt_model.keras')\n\ndef translate(sentence_en):\n    translation = \"\"\n    for word_index in range(50):\n        X = preprocessor.vectorize_en([sentence_en])\n        X_dec = preprocessor.vectorize_es([Configure.SOS + \" \" + translation])\n        y_proba = nmt_model.predict((X, X_dec), verbose=0)[0, word_index]\n        predicted_word_id = np.argmax(y_proba)\n        predicted_word = preprocessor.es_vocabulary()[predicted_word_id]\n        if predicted_word == Configure.EOS:\n            break\n        translation = translation + \" \" + predicted_word\n    return translation.strip()\n```\n\n간단한 테스트 결과를 확인하기 위해 다음 메소드를 작성해 봅시다:\n\n```js\nIn: translate(\"It was pretty cool.\");\nOut: \"era bastante [UNK]\";\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n비록 매우 정확하지는 않지만, 이 글의 목표는 Keras 3.0 서브클래싱 API를 사용하는 방법을 배우는 것이므로 여전히 이 모델을 최적화할 여지가 많습니다, 맞죠?\n\n# 결론\n\nKeras 3.0의 출시로 Keras의 간결한 API를 사용하여 Pytorch 또는 Jax를 백엔드로 사용하면서 효율적으로 모델을 구현할 수 있게 되었습니다.\n\n그러나 최신 버전이 출시된 지 얼마 되지 않아 도움이 되는 문서가 아직 완벽하지 않으므로 새로운 버전을 시도할 때 어려움을 겪을 수도 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 기사는 끝에서 끝으로 실용적인 예제를 통해 Keras 3.0의 환경 설정과 기본 개발 프로세스를 설명하여 빠르게 시작할 수 있도록 도와줍니다.\n\n안타깝게도, Keras 3.0 프로젝트는 아직 초기 단계에 있어 TensorFlow에 대한 의존성을 완전히 벗어날 수 없으며 TensorFlow의 일부 이해할 수 없는 문제도 있습니다.\n\n하지만 저는 여전히 이 버전에 낙관적입니다. 시간이 지나면서 다중 백엔드 지원이 개선되면서 Keras가 살아나고, 딥러닝 기술을 더 쉽게 이용할 수 있게 하고, 딥러닝의 학습 곡선을 줄일 수 있는데 도움을 줄 것이라고 믿습니다.\n\nKeras 3.0에 대해 더 알고 싶은 사항이 있으시면 자유롭게 댓글을 남기고 토론해 주세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 글을 즐겨 읽으셨나요? 더 많은 최신 데이터 과학 팁을 받으시려면 지금 구독해주세요! 피드백과 질문은 언제나 환영합니다 — 댓글에서 함께 이야기해요!\n\n본 기사는 Data Leads Future에서 원문으로 게시되었습니다.\n\n좋은 소식! Scikit-learn은 이제 Display 클래스를 제공하여 from_estimator와 from_predictions와 같은 메서드를 사용할 수 있게 해주어 다양한 상황에 맞는 그래프를 그리는 것이 훨씬 쉬워졌어요:\n","ogImage":{"url":"/assets/img/2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide_0.png"},"coverImage":"/assets/img/2024-05-20-Keras30TutorialEnd-to-EndDeepLearningProjectGuide_0.png","tag":["Tech"],"readingTime":25},{"title":"주체적인 RAG 개인 맞춤 및 최적화된 지식 보조 언어 모델","description":"","date":"2024-05-20 20:07","slug":"2024-05-20-AgenticRAGPersonalizingandOptimizingKnowledge-AugmentedLanguageModels","content":"\n대형 언어 모델(LLM)은 인공 지능 분야에서 혁명적인 힘으로 부상하여 자연어 처리 작업에서 놀라운 성능을 보여주고 있습니다. 그러나 그들의 인상적인 성능에도 불구하고, LLM은 종종 환각, 시간적 불일치 및 맥락 처리 문제와 같은 제한에 직면할 수 있습니다. 이러한 도전 과제를 해결하기 위해 연구는 외부 지식 원본과 통합하여 LLM을 향상시키는 데 초점을 맞추고 있습니다. 이 방법을 통해 RAG(검색 증강 생성)이라 불리는 것이죠.\n\nRAG는 언어 모델의 능력을 향상시키기 위해 외부 지식 원본에서 관련 정보를 검색하고 통합함으로써 정확한 응답을 이해하고 생성할 수 있도록 하는 것을 목표로 합니다. 이 추가적인 맥락을 활용함으로써 RAG 시스템은 복잡한 질문에 더 정확하고 맥락적으로 답변하기에 상당한 개선을 보여주고 있습니다. 그러나 RAG 프레임워크가 발전하고 확장됨에 따라, 새로운 도전 과제가 특히 검색 품질, 효율성 및 개인화 분야에서 발생하고 있습니다.\n\n이러한 제약을 극복하기 위해 연구자들은 ERAGent라는 첨단 RAG 프레임워크를 소개하였습니다. ERAGent는 분야에서의 중요한 발전을 총망라하여 검색 증강 언어 모델의 정확성, 효율성 및 개인화를 향상시키기 위해 여러 혁신적인 구성 요소와 기술을 통합하고 있습니다.\n\n![이미지](/assets/img/2024-05-20-AgenticRAGPersonalizingandOptimizingKnowledge-AugmentedLanguageModels_0.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 향상된 질문 재작성\n\nERAGent의 핵심 구성 요소 중 하나는 향상된 질문 재작성 모듈입니다. 이 모듈은 모호하거나 구어체인 질문을 명확하고 표준화된 쿼리로 개선하여 정보 검색의 품질을 향상하는 데 중요한 역할을 합니다. 전문 용어나 도메인 특정 용어를 식별하고 대체함으로써, 이 모듈은 대화형 언어와 지식 베이스에서 사용되는 기술 용어 사이의 간극을 좁히는 데 도움이 됩니다 [1].\n\n또한, 향상된 질문 재작성은 간단한 다시 말하기를 넘어 원래 질문의 다른 의미적 측면을 포착하는 여러 세분화된 쿼리를 생성함으로써 이와 같은 접근 방식은 후속 검색 프로세스가 다양한 각도에서 관련 정보를 철저히 검색하고 식별할 수 있도록 보장하여 전체적인 검색 품질과 정확도를 향상시킵니다 [1].\n\n예를 들어 임상 의학 시나리오에서 향상된 질문 재작성 모듈은 의사의 구어와 전문 용어로 가득 찬 질문을 인식하고 표준화된 도메인 특정 쿼리의 세트로 변환할 수 있습니다 [1]. 이 과정은 질문의 의도를 명확히하는데 그치지 않고 관련 의학적 지식을 보다 정확하고 포괄적으로 검색하며, 최종적으로 더 정확하고 정보화된 응답을 이끌어냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 효율적인 지식 검색\n\nERAGent의 또 다른 중요한 측면은 지식 검색의 효율을 최적화하는 데 초점을 맞추고 있다는 것입니다. 이는 검색 트리거 모듈과 경험 학습자 모듈의 협력 작용을 통해 달성됩니다.\n\n검색 트리거 모듈은 AI 어시스턴트의 지식 경계 내 또는 외에 주어진 쿼리가 속하는지를 평가하도록 설계되었습니다 [1]. 쿼리의 \"인기도\"에 기초하여 특정 주제에 대한 시스템의 숙련도를 추정함으로써, 검색 트리거는 외부 지식 검색이 필요한지 아니면 쿼리를 기존 정보를 사용하여 충분히 처리할 수 있는지 신중하게 판단할 수 있습니다.\n\n이러한 지식 검색의 선택적 접근은 경험 학습자 모듈에 의해 지원됩니다. 이 모듈은 AI 어시스턴트가 과거 상호 작용에서 지식 베이스를 지속적으로 확장하고 사용자 프로필을 점진적으로 모델링할 수 있게 합니다 [1]. 이전 대화를 보존하고 학습함으로써, 시스템은 매우 관련성 높은 과거 지식을 활용하여 정확한 응답을 생성함으로써 중복된 외부 검색이 필요성을 줄이고 전반적인 효율성을 향상시킬 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n평가 결과에 따르면 ERAGent는 응답 시간을 약 40% 정도 단축할 수 있으며 응답 품질을 희생하지 않습니다 [1]. 이 효율성과 정확성 사이의 최적 균형은 경험 기반 학습 모듈을 활용하고 검색 트리거의 인기 임계값을 적절한 수준으로 조정하여 달성됩니다.\n\n# 강력한 지식 필터링\n\n검색 품질과 효율성을 향상시키는 것이 중요한 가운데, ERAGent는 검색된 지식을 정제하여 해당 지식이 관련성과 정확성을 보장하는 것이 중요하다는 사실을 인지합니다. 이는 지식 필터 모듈이 작용하는 곳입니다.\n\n지식 필터 모듈은 자연 언어 추론 (NLI) 기술을 활용하여 각 검색된 지식 단편이 원래 질문과 얼마나 관련이 있는지를 평가합니다 [1]. 검색 정보와 질문-답변 쌍 간의 관계를 분류함으로써, 모듈은 관련 없는 맥락을 효과적으로 걸러내고 가장 중요한 지식만 유지할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 필터링 과정은 검색 단계에서 방대한 정보가 얻어지고 있지만 그 중 일부는 의미적으로 관련이 있지만 질문에 정확하게 대답하는 데 직접적으로 사용할 수 없는 경우에 특히 가치가 있습니다. Knowledge Filter 모듈은 부가적인 콘텐츠를 버리는 것으로 정보가 언어 모델에 입력되는 것이 최고의 관련성과 품질을 갖도록 보장하여 생성된 응답의 정확성과 일관성을 향상시킵니다.\n\n# 개인화된 응답 생성\n\n정확성과 효율성을 향상시키는 것 외에, ERAGent는 언어 모델 응답의 개인화 특성에도 대응합니다. 개인화된 LLM 리더 모듈은 이 노력의 최전선에 있으며, 사용자 프로필, 선호도 및 컨텍스트에 맞추어 응답을 맞춥니다.\n\n언어 모델의 프롬프트에 학습된 사용자 프로필을 통합함으로써, 개인화된 LLM 리더는 단순히 사실적인 것뿐만 아니라 사용자의 고유한 배경, 관심사 및 필요에 부합하는 응답을 생성할 수 있습니다 [1]. 이 개인화는 표면적인 맞춤 이상으로 나아가, 각 사용자의 선호도, 태도 및 문맥 요소의 미묘한 차이를 탐구하여 정말로 맞춤화되고 매력적인 응답을 만들어냅니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어, 근육을 키우는 데 도움이 되는 식단 권장사항을 제공할 때, 개인 맞춤형 LLM 리더는 사용자의 환경 의식, 식이 제한 사항, 그리고 환경 운동가 또는 늦게 일어나는 사람과 같은 개인 선호도를 고려할 수 있습니다. 이러한 요소를 고려함으로써, 해당 모듈은 사용자의 특정한 라이프스타일과 가치관을 고려하는 적절한 식물성 단백질 공급원, 식사 준비 전략, 그리고 수면 권장을 제안할 수 있습니다.\n\n평가 결과는 ERAGent가 다양한 주제와 사용자 프로필에 걸쳐 개인화된 응답을 생성하는 능력을 입증했습니다. 비교 분석을 통해, 개인 맞춤형 LLM 리더가 사용자의 선호도와 과거 상호작용을 고려한 응답 정렬에서 비개인화된 대체품을 일관되게 능가함으로써, 사용자와 AI 어시스턴트 간의 이해와 연결감을 촉진합니다.\n\n# 평가 및 결과\n\nERAGent의 효과는 다양한 질문-답변 작업 및 데이터 세트에서 엄격한 평가를 거쳐, 정확성, 효율성 및 개인화 측면에서 우수한 성능을 보여주며, 그 우수성을 빛내고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n원 라운드 공개 도메인 QA 작업에서 Natural Questions, PopQA, AmbigNQ와 같은 데이터셋을 사용한 경우, ERAGent는 기본 모델에 비해 응답 정확도에서 상당한 개선을 보였습니다 [1]. 향상된 질문 재작성기와 지식 필터 모듈의 시너지 효과로 인해 정확 일치, 정밀도, 재현율 및 히트율과 같은 메트릭스에서 상당한 향상이 이루어졌습니다.\n\n또한 HotpotQA 및 2WikiMQA와 같은 데이터셋에서의 원 라운드 다중-홉 추론 QA 작업에서 향상된 질문 재작성기와 지식 필터의 공동 적용이 LLM이 정확한 응답을 생성하는 능력을 획기적으로 향상시켰으며 다른 기본 모델을 능가했습니다 [1].\n\n가장 주목할 만한 점으로, ERAGent의 다중 세션, 다중 라운드 QA 시나리오에서의 성능은 맞춤화와 최적화된 검색 효율성에 그 강점을 보여주었습니다. 사용자 프로필과 일관되게 다양한 주제에 걸쳐 응답을 조율함으로써 ERAGent는 맞춤화 능력에서 비맞춤화 모델들과 비교하여 우수함을 입증했습니다 [1].\n\n게다가 체험학습자 모듈을 활용하고 검색 트리거의 인기 임계값을 조정함으로써 ERAGent는 응답 시간을 현저히 줄이면서도 답변 품질을 저해하지 않았습니다. 효율성과 정확성 간의 최적 균형은 프레임워크가 실제 시나리오에서 실용적으로 적용 가능함을 강조합니다 [1].\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\nERAGent은 새로운 구성 요소와 기술을 통합한 포괄적인 프레임워크를 통해 키 한계와 도전에 대응하여, 검색 보조 언어 모델 분야에서 중대한 발전을 나타냅니다.\n\n강화된 질문 재작성 모듈은 애매한 쿼리를 정제하고 포괭적인 정보 검색을 위해 다중 측면의 세밀한 쿼리를 생성함으로써 검색 품질을 향상시킵니다. 검색 트리거 및 경험학습 모듈은 사용자의 이전 지식과 컨텍스트를 활용하여 중복된 외부 검색을 줄이고 응답 품질을 희생시키지 않으면서 검색 효율성을 최적화하기 위해 협력합니다.\n\n지식 필터 모듈은 자연어 추론 기술을 사용하여 검색된 지식을 정제하고 관련 없는 정보를 걸러내어 응답 정확도를 더욱 향상시킵니다. 특히, 개인화 된 LLM 리더 모듈은 개별 사용자 프로필, 선호도 및 컨텍스트에 맞춰 응답을 제공하여 더욱 흥미롭고 개인화된 사용자 경험을 유도하는데 있어 ERAGent을 독특하게 만듭니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다양한 데이터셋 및 질의응답 작업을 통해 엄격한 평가를 거쳐, ERAGent는 정확성, 효율성 및 맞춤화 측면에서 일정한 우수한 성능을 보여왔습니다. [1].\n\nERAGent는 혁명적인 패러다임 변화보다는 점진적인 진전을 나타내지만, 이러한 발전은 검색 기반 지식 질문응답 시스템 분야에서 미래 연구 및 개발을 위한 견고한 기초를 확립합니다. 실용적인 장벽에 대처하고 맞춤화 및 최적화를 탐색함으로써 ERAGent는 지식이 풍부하고 정확한 AI 도우미뿐만 아니라 효율적이고 사용자 중심 그리고 개인의 필요와 선호도에 따라 진정으로 맞춤화된 AI 도우미의 계속적인 진화를 열어놓습니다.\n\n## 참고문헌\n\n[1] Shi, Y., Zi, X., Shi, Z., Zhang, H., Wu, Q., \u0026 Xu, M. (2024). ERAGent: 정확성, 효율성 및 맞춤화가 향상된 검색 증강 언어 모델. arXiv 사전 인쇄 arXiv:2405.06683.\n","ogImage":{"url":"/assets/img/2024-05-20-AgenticRAGPersonalizingandOptimizingKnowledge-AugmentedLanguageModels_0.png"},"coverImage":"/assets/img/2024-05-20-AgenticRAGPersonalizingandOptimizingKnowledge-AugmentedLanguageModels_0.png","tag":["Tech"],"readingTime":8},{"title":"인간 수준의 로봇을 위한 훌륭한 AI로의 길","description":"","date":"2024-05-20 20:04","slug":"2024-05-20-ThePathtoGreatAIforHuman-CapableRobots","content":"\n\u003cimg src=\"/assets/img/2024-05-20-ThePathtoGreatAIforHuman-CapableRobots_0.png\" /\u003e\n\n내가 가장 좋아하는 스타워즈 장면은 츄바카가 우키의 울음소리를 내고 작은 토스트기 크기의 마우스 드로이드가 짹짹 거리며 뒤로 물러나는 장면입니다. 그 배경에는 무슨 일이 일어나고 있는지 상상해보세요? 그 로봇은 그 소리를 내는 것이 자신을 뭉개버릴 수 있는 무서운 협박하는 존재임을 인식해야 합니다. 두려움에 반응해야 합니다. 그리고 뒤로 바퀴로 이동하여 도망가야 합니다.\n\n오늘날의 로봇들은 이러한 이해와 제어 수준을 갖고 있지 않습니다. 이로 인해 그들은 자연스러워 보이지 않습니다. 현재의 로봇들은 일반적으로 딱딱하고 자연스럽지 않은 모습을 하고 있습니다. 또는 이상한 부조리한 계곡에 앉아서 자연스러워 보이려고 하지만 그것을 완전히 이룰 수 없습니다.\n\n# 시뮬레이션 및 유전 알고리즘에 대한 초기 작업\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n1994년에 Karl Sims가 \"진화하는 가상 생물체\"라는 논문을 썼습니다. 그 논문은 물 속에서 움직이고 땅 위에서 움직이는 혁신적인 블록 생물체들을 보여주었습니다. Karl은 유전 알고리즘을 사용하여 생물체와 움직임을 진화시키는 아이디어를 시연했습니다. 그 비디오는 30년이 지난 지금도 여전히 매혹적입니다.\n\n이 연구는 새로운 행동과 새로운 제어를 갖는 혁신적인 생물체가 전진하거나 회전하는 등의 작업을 배울 수 있다는 것을 보여줬습니다. 시뮬레이션을 통해 건설함으로써 Karl Sims는 혁신적인 생물체들이 나타날 때까지 수많은 반복과 실험을 할 수 있었습니다. 게다가, 유전 알고리즘은 이전에 작동한 것을 채택하고 발전시킴으로써 점점 더 나은 것으로 수렴할 수 있다는 것을 보여줬습니다.\n\n# 강화 학습의 등장\n\n리치 서튼 박사는 1984년에 발표한 박사 논문에서 강화 학습의 개념을 고안했다고 인정받고 있습니다. 그의 말에 따르면, \"강화 학습은 보상으로부터 배움으로, 세계와의 평범한 상호작용 중에 시행착오를 통해 배우는 것\"입니다. 이후에는 Karl Sims의 작업과 비슷하게 시뮬레이션 세계에서 학습하도록 적응되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n강화 학습은 딥마인드가 아타리 게임을 플레이하는 시스템을 가르칠 수 있다는 것을 증명한 경우를 포함하여 탐험의 핫한 분야로 부상했습니다. 이 시스템은 입력 비디오 프레임을 받아 조이스틱 컨트롤을 모방한 컨트롤 출력을 생성했습니다(비디오).\n\n이것은 RL이 시행착오를 통해 행동을 배우고 최적화할 수 있는 능력을 증명한 중대한 순간이었습니다. 이는 단순히 게임 그 자체를 숙달하는 데서 그치는 것이 아니라 기계가 원시 시각 입력과 게임 환경으로부터의 피드백만을 이용하여 복잡한 작업을 처음부터 배울 수 있다는 것을 입증한 것이었습니다.\n\n이러한 움직임을 기반으로, 오픈AI의 DOTA 2에 대한 작업은 복잡한 멀티플레이어 온라인 배틀 아레나 게임에서 큰 발전을 이뤘습니다(비디오). 강화 학습을 통해 훈련된 신경망 팀 오픈AI Five는 프로 수준의 인간 플레이어와 경쟁하고 이기는 능력을 보여주었습니다. 이 성취는 RL이 복잡성과 동적성이 증가하는 작업을 처리할 수 있는 능력, 전략적 계획, 팀워크, 예측할 수 없는 상대에 대한 실시간 의사 결정을 포함하는 작업을 다룰 수 있는 능력을 강조했습니다.\n\n오픈AI의 학습 능력 프로젝트는 RL이 달성할 수 있는 영역을 더 넓혀주었습니다. 로봇 손을 훈련시켜 인간 손과 유사한 미세 조작 능력을 갖도록 하는 것을 통해, 이 프로젝트는 RL이 미세한 운동 통제와 적응력을 필요로 하는 물리적 작업에서의 잠재력을 강조했습니다(비디오). 이 프로젝트는 손을 훈련시키기 위해 실제 환경으로 그 기술을 옮기기 전에 시뮬레이션 환경을 사용한 것으로, sim-to-real 전이라고 알려진 기술을 사용한 점에서 특히 주목할 만했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 강화 학습의 한계 극복하기\n\n강화 학습의 첫 번째 큰 난제는 많은 시행 착오를 하는 데 비용이 많이 든다는 것입니다. 특히 학습 초기 단계에서 많은 테스트된 가설이 정말로 좋지 않은 것들이 많습니다. 따라서 나쁜 가치를 구별하여 목표에 근접한 것을 찾아내려고 하는 것은 어렵습니다. 또한 필요한 시행 착오 횟수는 액추에이터 수와 함께 증가합니다. 이것은 부트스트래핑 문제로 볼 수 있습니다... 강화 학습이 더 효율적일 수 있도록 모델을 시작할 수 있는 방법은 무엇인가요?\n\n이 문제를 해결하는 데 도움이 될 수 있는 다양한 기술이 있습니다:\n\n- 먼저 시뮬레이션 사용: 실제 세계에서 작동을 확인하기 전에 시뮬레이션에서 모든 시행 착오를 수행하여 작동하는 솔루션에 더 가까워지려고 노력합니다.\n- 모방 학습: 강화 학습을 세부 조정하기 전에 기본 모델을 얻기 위해 다른 기술을 사용합니다. 이 기술 중 하나는 사람이 액추에이터를 제어하도록 하고, 그 후에 학습하는 것입니다. 또는 \"One-shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning\"에서 시연된 것처럼 비디오에서 모방합니다. 최근 CMU 로봇 공학 연구소의 논문 \"SloMo: A General System for Legged Robot Motion Imitation from Casual Videos\"에서는 개와 고양이의 비디오에서 배운 것을 다리로봇에 전달하는 능력을 보였습니다.\n- 액추에이터 공간의 차원 축소: 인간의 손은 40개 이상의 제어 차원을 가지고 있으며 OpenAI Detrous Manipulation 프로젝트에서 사용된 Shadow Hand와 같은 장치는 26개의 차원을 가지고 있습니다. Columbia는 \"eigengrasps\"를 만들어 알려진 잡음 상태에서의 접근 문제를 단순화하기 위해 손을 간단한 문제로 전환하려고 노력했습니다.\n- 문제 단순화: 특히 로봇 픽킹에서 널리 사용되는 방법은 물건을 집을 때 기존 것을 잡아들이기에 걱정할 필요 없도록 매우 높은 유동성 진공을 단순히 사용하는 것입니다. Agility는 로봇의 다리를 설계하여 다리 제어 모델을 탄성 질량 진자로 모델링할 수 있도록 하여 제어 문제를 단순화했습니다.\n- 행동 클로닝: 이미 작동하는 시스템이 있다면 다른 시스템이 그 행동을 복제하도록 시도할 수 있습니다. 처음에는 지도 학습 기술을 사용해보는 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n강화 학습의 두 번째 큰 도전은 매우 견고하지 않으며 지정된 보상에 과적합될 수 있다는 것입니다. 예를 들어, 인간 형상 로봇이 농구 골대에 골을 넣도록 훈련하려면, 목표가 단순히 골을 넣는 것이라면 로봇은 보다 성공적인 방법으로 공을 던질 것입니다. 그러나 인간이 농구를 차는 것과 같이 보이기를 원한다면, 골을 넣는 것과 인간처럼 보이는 것 둘 다 보상하는 더 정교한 보상 함수가 필요합니다.\n\n- 도메인 랜덤화: 입력 공간을 교란하고 동일한 목표를 달성하려고 하면 모델이 더 견고해질 수 있습니다.\n- 인간 취향: 인간들은 성공이 어떻게 보이어야 하는지에 대해 보다 세밀한 버전을 가지고 있습니다. 그래서 목표를 달성하는 것으로 시스템을 보상하는 대신에, 주로 인간이 선호하는 대로 행동하도록 시스템에 보상을 줄 수 있습니다. 일반적으로 인간에게 두 가지 예시를 나란히 보여주고 어느 쪽이 더 좋은지 묻습니다.\n\n강화 학습의 세 번째 큰 도전은 한 번에 한 가지 작업만 배운다는 것입니다! 문을 열어본 방법을 배웠다고 해서 다른 어떤 문의 손잡이나 심지어 높이가 2인치 낮은 문의 손잡이를 열 수 있다는 것을 의미하지 않습니다. 사실, 이 문제는 전혀 잘 해결되지 않습니다. 이를 해결하기 위한 몇 가지 대처 방법이 있습니다:\n\n- 동적 손재능 로봇 행동의 순차적 구성: 이 논문은 한 제어 체제에서 다른 체제로 부드럽게 전환하는 기술을 유도했는데, 두 제어 체제 간에 중첩이 있을 때 전환하는 방법을 제시합니다. 이는 매우 통찰력있는 논문입니다. Boston Dynamics은 아틀라스가 팽이를 하도록 만들기 위해 이 기술을 사용하거나 이 기술의 진화형을 사용한다고 여겨집니다.\n- 환경 제한: 작업을 매우 구체적인 상황으로 제한하면 문제를 크게 단순화할 수 있습니다. 예를 들어, 일반적인 세계에서 물건을 움켜쥐는 것은 매우 어렵습니다. 하지만 그냥 통에 있는 물건을 집는 것으로 제한한다면 문제를 상당히 단순화하고 그 후 하나의 작업만 배울 수 있습니다. 마찬가지로, 로봇이 할 수 있는 것을 충족하기 위해 모든 문 손잡이를 변경할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n로봇 공학 분야에서 이 문제는 매우 큰 문제이며, 로봇 공학 커뮤니티 외부에서는 잘 이해되지 않는 문제입니다. 진짜 인상적인 로봇 액션의 데모를 자주 보게 되는데, 우리는 로봇이 백플립을 할 수 있는 로봇이라면 당연히 몽키 바도 할 수 있을 것이라고 바로 생각합니다. 왜냐하면 우리는 7세 어린이가 백플립을 할 수 있는 아이라면 몽키 바에는 문제가 없을 것이기 때문입니다. 하지만 로봇은 그렇지 않습니다. 만약 로봇이 몽키 바를 할 수 있도록 훈련받지 않았다면, 그것은 그 일을 할 능력이 전혀 없을 것입니다.\n\n인간이 할 수 있는 모든 것을 하나씩 배우려고 할 때 우리는 영원히 걸릴 것입니다.\n\n# 데이터의 역할\n\n우리가 알려진 정답 테스트 세트와 같은 결과를 생성해내는 모델을 고안하는 지도 학습에서는, 데이터가 많을수록 좋습니다. 음성 인식 분야의 초기 진전의 많은 부분은 지도 학습을 통해 이루어졌습니다. Tellme에서는, Nuance의 음성 인식 엔진을 사용하고 있음에도 불구하고 더 많은 데이터를 가지고 있기 때문에 어느 순간 Nuance보다 더 나은 음성 인식을 할 수 있었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n사람들이 더 많은 데이터가 이긴다고 가정하는 것으로 이어졌습니다. 어느 정도는 여전히 사실이지만, 지도 학습을 시도하는 경우 테스트 세트가 필요합니다. 그리고 결과물은 당신의 테스트 및 훈련 세트의 크기만큼 좋습니다. 그래서 더 많은 데이터를 원하게 됩니다. Scale AI가 한 초기 작업 중 하나는, 자율 주행 자동차 회사들이 자동차, 정지 신호, 보행자, 교통 가로등 등이 어디에 있는지 보여 주는 레이블된 데이터 세트를 구축하는 데 도움을 주었습니다. 이러한 훈련 세트들을 구축하는 데는 매우 비용이 많이 들며 우리가 희망하는 것만큼 결과가 좋지 않습니다.\n\n이를 해결하기 위해 연구자들은 일부 레이블 데이터를 대규모의 레이블되지 않은 데이터 코퍼스와 융합하는 방법을 찾았습니다. 예를 들어, 아마존은 7,000시간의 레이블된 음성 및 100만 시간의 레이블되지 않은 음성을 사용하여 음성에 대한 그들의 음향 모델을 개선했습니다. 테슬라는 지금 자동 레이블링을 할 수 있는 충분한 레이블 데이터가 있다고 주장하지만, 그 정보는 희박합니다.\n\n그래서 더 많은 데이터가 승리하는 건가요? 그런데, 로봇 공학 분야에선 조금 다르게 작용할 수 있습니다. 로봇 공학의 문제는 텍스트, 음성, 이미지 또는 차량에서의 비디오와 달리, 우리가 데이터를 수집하고 다니는 로봇이 많지 않다는 점입니다. 세계에 있는 대부분의 데이터는 사람들이 한 결과물입니다... 우리가 쓰거나 말했거나, 사진을 찍었거나, 우리가 운전했거나 했습니다.\n\n만약 길을 운전하는 사람으로부터 어떠한 데이터도 캡처할 수 없다면 자율 주행 자동차를 만드는 것을 상상해 보세요. 더 나아가, 자율 주행이 되지 않은 자동차는 거의 유용하지 않습니다. 이것이 로봇과 관련된 문제입니다. 넓은 범위의 숙련된 작업을 수행하지 못하는 로봇들은 본질적으로 덜 유용하지만, 우리는 실제 세계에서 사람들이 하는 일들로부터 데이터를 캡처하는 좋은 방법을 가지고 있지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n어느 정도의 실험으로 우리는 움직임 캡처 슈트를 입거나 Mobile Aloha와 같은 릭을 사용하여 이 데이터(비디오)를 캡처하는 방식을 시도해 보았습니다.\n\n우리가 그 방법에 이를 수 있는 이론 중 하나는 매우 복잡한 로봇이 매우 간단한 작업을 수행하고, 그런 다음 우리가 음성 인식과 같이 점진적으로 개선하는 것일 것이라는 것입니다. 이 주장은 좋아 보이지만, 실제로는 아무런 의미가 없습니다. 복잡한 로봇을 사용해서 간단한 작업을 수행하는 데는 엄청난 비용이 들기 때문에 그 중 많은 수를 배치하지 않을 것이므로, 우리는 많은 데이터를 갖지 않게 됩니다.\n\n또 다른 도전 과제는 세계가 굉장히 복잡하다는 것입니다. 수백만개의 레이블이 붙은 항목들이 있어도, 자율 주행 차량은 여전히 이전에 본 적이 없는 상황에 직면할 수 있습니다. 수십 년 동안의 데이터 레이블링과 전통적 기술들도 몇 개의 도시를 넘어선 일반화된 무인 운전차를 만드는 데 성과를 내지 못했습니다.\n\n# 트랜스포머와 토큰\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nTransformer Architecture는 훨씬 더 나은 모델을 생성하는 데 차별화를 가져온 혁신이었습니다. Transformer 아키텍처는 토큰 시퀀스를 살펴보고 다음 토큰을 예측할 수 있도록 패턴을 찾아 작동합니다. ChatGPT 4의 Transformer 아키텍처는 인터넷에서 수집된 대량의 데이터 코퍼스로부터 13조 개의 토큰(사실상 \"단어\"들)로 훈련되었습니다. 우리는 많은 텍스트가 있고 그 텍스트는 토큰 시퀀스로 잘 구조화되어 있습니다.\n\n우리는 ChatGPT를 다음 단어 텍스트를 생성하는 것으로 생각하지만, 우리는 또한 토큰 간 번역을 할 수 있으며, 이것은 텍스트와 음성의 기계 번역을 놀라울 만큼 좋게 만들었습니다.\n\n그러나 transformer 아키텍처의 핵심은 공간을 transformer architecture에 적합하게 표현하기 위해 어떻게 토큰화하는 지를 결정하는 것입니다. 로봇 공학에서, 이러한 토큰들은 액션 스트림이 될 수 있습니다. TRI의 최근 확산 정책 작업은 견고한 제어의 개발을 가속화하는 데 큰 성과를 거두며 이것을 수행합니다. 그러나 그들은 여전히 한 번에 하나의 제어 정책만을 학습하고 있습니다.\n\n그러나 실제로 일반화하려면 많은 양의 토큰이 필요합니다. 우리는 충분한 양의 로봇이 토큰을 생성하지 않아 심각한 토큰 저장소를 구축하기에 충분하지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 토큰의 원천\n\n토큰의 하나의 원천은 간단히 말해서 로봇 동작의 일반적인 표현을 고안하여 연구자들 사이에서 토큰을 공유할 수 있는 방법입니다. 첼시 핀과 다른 연구자들이 이를 시작하고 있습니다. 이제 충분히 흥미로운 토큰을 얻는 것이 여전히 난제입니다.\n\n또 다른 잠재적인 토큰의 원천은 대형 언어 모델의 잠재적 이해력을 활용하여 모션 플랜을 생성하고, 그 모션 플랜을 시뮬레이터에 공급한 다음 해당 시뮬레이션에서 모션의 토큰을 추출하는 것입니다. 이에 대한 논문들이 있는지는 아직 접하지 못했지만, ChatGPT가 학습한 월드 모델은 우리를 놀라게 하며 모션에 대한 깊은 본질적인 이해력을 가지고 있을 수 있습니다. 오늘은 아니더라도, 비디오와 이미지로부터 훈련된 다중 모달 모델들은 점차적으로 더 나은 모션에 대한 본질적인 이해력을 갖게 될 것으로 예상됩니다.\n\n또 다른 원천은 이미 존재하는 풍부한 비디오 자료입니다. 90년대 중반, 저는 MIT의 컴퓨터 과학 연구소 내 그래픽스 연구실에서 석사 논문 작업을 했습니다. 세스 텔러 교수님은 제 논문 지도 교수였습니다. 제 논문은 컴퓨터 그래픽을 위한 교육 플랫폼으로 웹을 활용하는 것에 중점을 둔 반면, 제 동료는 도시 맵핑을 위한 로봇 데이터 수집 장치를 구축하고 있었습니다. 도시 맵핑 프로젝트는 이 데이터 수집 장치로 수집된 2D 이미지에서 3D 지오메트리를 재구성하기 위해 계산 기하학을 활용했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n동일한 컴퓨터 기하학 기법은 SLAM (Simultaneous Localization and Mapping)의 기초를 형성합니다. 이것은 현재의 로봇들이 2D 비디오에서 3D 기하를 재구성하여 우리 주변 세계를 탐색할 수 있는 방법입니다. 이러한 기법을 사용하여 스켈레톤을 추적하고 사람의 동작을 재구성할 수도 있습니다.\n\n이것은 많은 동작 토큰들의 잠재적 출처를 만듭니다.\n\n## 인간 수준의 로봇 구축에 대한 영향\n\n오늘날 우리는 양쪽 다 처리와 조작 분야에서 복잡한 실제 상호작용을 위해 안정적으로 인간형 로봇을 제어할 수 있는 모델이 충분히 좋지 않다는 문제를 가지고 있습니다. Boston Dynamics가 다양한 작업의 좋은 조합을 보여줄 수 있고, Agility가 매우 견고한 걷기 동역학을 보여주고, 연구소들이 박스를 오르내리며 멋진 스크래블을 하는 네 다리 동물을 보여줄 수 있을지라도, 우리는 여전히 새로운 상황이나 새로운 작업에 안정적으로 일반화할 수 없습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하지만 이 복잡한 로봇들이 더 일반화된 강력한 제어 없이 의미 있는 작업을 수행할 수 없다면, 우리는 단순히 많은 수의 로봇을 배치할 수 없을 것이며, 캡쳐할 수 있는 데이터의 양이 제한될 것입니다.\n\n이전에 썼던 “인간 수준 로봇에 이르는 길”이라는 글에서 설명한 대로, 나는 현재 고성능 로봇을 신속히 투입하는 것이 최선의 방법이라고 생각합니다. 그러나 인간 형태 제약에서 벗어나 인간이 실제로 하는 일에 대한 관점에서 데이터를 수집해야 합니다.\n\nLLM(Large Language Models)의 내재 지식을 비디오에서 추출된 토큰과 결합하고 소량의 실제 데이터를 시뮬레이션에서 재구성하여, 우리는 어떤 로봇 형태도 가능하게 할 로봇 제어의 기본 모델을 구축하기 시작할 수 있을 것입니다. 그러면 Diffusion Policies의 기술을 이용하여 인간이 이끄는 다중 모델 제어가 지원된 기본 모델을 지도하는 방식으로 로봇을 가르쳐 7세처럼 어떤 작업이든 수행할 수 있게 할 수 있을 것입니다.\n\n저는 그 미래를 기대합니다. 이미 유용한 코로봇들은 이 기본 모델이 사용 가능해지면 더욱 더 유능한 협업자가 될 것입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 다른 흥미로운 논문 및 강연:\n\n- Chelsea Finn - MIT에서 2023년 10월 23일에 로봇 학습의 일반화 및 민첩성에 관한 발표\n- Russ Tedrake - 프린스턴 로보틱스 - Russ Tedrake - 확산 정책을 이용한 민첩한 조작\n- Pieter Abbeel - 로봇 조각 집기를 위한 기반 모델 구축에 대해\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nChelsea Finn \u0026 Pieter Abbeel S3 E2 스탠포드 교수 첼시 핀: 항상 변화하는 세상에 대응할 수 있는 AI를 구축하는 방법\n\n브래드 포터는 Collaborative Robotics, Inc의 CEO이자 설립자로, Sequoia, Khosla 및 Mayo Clinic이 후원하는 캘리포니아 산타클라라에 본사를 둔 로봇 기업입니다. Cobot을 창립하기 전에, 브래드는 아마존의 물류 네트워크를 위한 로봇 기술을 감독하며 10,000명의 글로벌 팀을 이끄는 부사장 겸 탁월한 엔지니어였습니다. 그는 또한 Scale AI의 CTO, Tellme Networks의 플랫폼 아키텍트, 그리고 Netscape의 초기 엔지니어였습니다. 브래드는 MIT에서 컴퓨터 과학 학사학위와 석사학위를 취득했으며, 프로페서 Seth Teller 아래에서 컴퓨터 그래픽스에 중점을 둔 연구를 진행했습니다.\n","ogImage":{"url":"/assets/img/2024-05-20-ThePathtoGreatAIforHuman-CapableRobots_0.png"},"coverImage":"/assets/img/2024-05-20-ThePathtoGreatAIforHuman-CapableRobots_0.png","tag":["Tech"],"readingTime":13},{"title":"하이브리드 A star에 대해 부드럽게 소개하기","description":"","date":"2024-05-20 20:01","slug":"2024-05-20-GentleintroductiontoHybridAstar","content":"\n![Hybrid A*](/assets/img/2024-05-20-GentleintroductiontoHybridAstar_0.png)\n\n하이브리드 A\\*는 자동차 형태의 로봇들을 위한 가장 인기 있는 경로 계획 알고리즘 중 하나로 간주됩니다. 이는 현재 웨이모(Waymo), 구글의 자율 주행 자동차 프로젝트의 공동 최고 경영자로 있는 Dmitri Dolgov가 개발했습니다.\n\n학문적으로, 하이브리드 A\\*는 DARPA 챌린지에서 우수한 성능을 보여주면서 1000회 이상 인용되었습니다. 심지어 2023년에도 이 방법은 자동차의 운동 계획을 위한 주요한 프론트엔드 알고리즘 중 하나로 자주 사용됩니다(예: Bai Li나 Zhang의 논문들).\n\n이 논문을 처음 읽었을 때 격자 형태의 도메인에서 확장해야 해서 이해하기 어려웠습니다. 이 글은 여러분이 이 원칙과 특성을 이해하는 데 도움이 되도록 작성되었습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n어떤 방식으로 구현된 코드들을 찾을 수 있습니다.\n\n- KTH 논문 (ROS1) (제가 개인적으로 이 버전을 조사했습니다).\n- nav2 (ROS2)\n- Unity\n- Python\n- MATLAB\n\n## 먼저, Djkstra와 A\\*\n\n하이브리드를 이해하기 위해 먼저 Djkstra 알고리즘과 A\\* 알고리즘을 복습해보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 1. Djikstra\n\nDjikstra 알고리즘을 올바르게 이해하는 방법은 가장 짧은 경로 트리를 찾아내기 위해 \"역전파\" 하는 것으로 생각하는 것입니다. 이 알고리즘의 목적은 단일 목표에 대한 최단 경로를 찾는 것이 아닙니다! 모든 가능한 노드에 대해 찾아냅니다.\n\n![image](https://miro.medium.com/v2/resize:fit:1114/1*43bXSP_BVTTfpm0QgxO2Qg.gif)\n\n위의 애니메이션에서 회색 원은 시작(파란 점)에서 셀(즉, 노드)까지의 최단 경로를 계산했다는 것을 나타냅니다. 경로 찾기처럼 보이는 이유는, 구현이 목표에 도달했을 때 전파를 종료했기 때문입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n따라서, 검색은 단일 목표와는 아무 관련이 없습니다.\n\n## 2. A\\*\n\n로봇 연구자들이 대부분의 시간을 보내는 작업은 경로를 \"단일\" 목표로 찾는 것입니다 (목표는 변경될 수 있음에도). 이를 위해, 로봇 연구자들은 Djikstra 알고리즘을 수정하여 단일 목표로 향하도록 만들려고 노력했습니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1114/1*ugjVvXCDjAoykSSEUiRUXA.gif)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n검색을 안내하기 위해 두 가지 용어를 기반으로 \"예상대로\" 최적의 방향으로 진행합니다:\n\n- 지금까지의 비용(이미 결정됨): 이 셀이 시작점부터 이 지점에 도달할 때까지 더 낮은 비용이 누적되었나요?\n- 목표 지점까지의 비용(정확히 알지 못 함): 이 셀의 우수성을 목표 지점에 도달하기 위한 선호도로 얼마나 낙관적으로 추정하나요?\n\n첫 번째 용어는 종종 g(x)로 표시되고 후자는 h(x)로 표시됩니다.후자를 휴리스틱이라고 부릅니다. 탐욕스럽게 다음 x*next를 검색합니다: 탐색 큐 내에서 g(x*'next') + h(x\\_'next')가 가장 낮은 값.\n\n## 휴리스틱 함수의 합리성\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nh(x)에 의존하는 안내형 검색을 수행하기 때문에 이 함수는 신중하게 설계되어야 합니다. 전역 최적해를 찾기 위해서는 휴리스틱에 대한 필수 조건이 있습니다. 그것은 목표까지의 예상 비용을 최상으로 낙관적으로 출력해야 한다는 것입니다. 그리드에서 A\\*를 사용하는 경우, 이 함수는 일반적으로 x에서 목표까지의 유클리드 거리로 선택됩니다.\n\n이 선택의 이유는 우리가 최선의 해인 전역 최적해를 발견하는 기회를 놓치지 않기 위함입니다. 이 함수가 이 조건을 충족하지 않으면 A가 (전역 최적해를 나타내는 표기법)임이 보장되지 않습니다.\n\n## 3. 운동학을 반영한 합성 A\\*\n\n그리드 영역에서 A* 알고리즘이 컴퓨터 게임을 포함한 다양한 응용에 잘 작동합니다. 그러나 이러한 그리드에서의 안내형 검색은 자동차와 같은 비할모논 시스템에 적용하려고 할 때 제한이 있습니다. A*의 출력이 그리드에서 전역적으로 최적일 수 있지만, 실제 추적 시 경로는 역학에 의해 추적될 때 비효율적일 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Hybrid A* explained](/assets/img/2024-05-20-GentleintroductiontoHybridAstar_1.png)\n\n위의 예에서 파란색 직선이 x\\_'g'에 도달하는 전역 최적이지만, 자동차의 초기 헤딩 때문에 경로를 정확히 추적할 수 없습니다. 즉, A*의 결과가 유용하지 않을 수 있습니다. 원래 논문에서는 이 문제를 어떻게 해결할 수 있을까 고민했습니다. 자동차의 키네마틱스를 고려하면서 A*와 같은 경로 탐색을 안내할 수 있을까요?\n\n# Hybrid A\\* 설명\n\n본 섹션에서는 하이브리드 A\\*에 대해 설명합니다. 저는 Dimitri의 원래 논문을 기반으로 설명하겠습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 1. 목적\n\n일반적인 A*은 R²의 위치 x₀에서 R²의 목표 위치 x_f까지 가장 짧은 경로를 찾으려고 합니다 (물론 세 가지 차원으로도 사용할 수 있습니다. 그러나 간단하게 하기 위해 2차원으로 제한합니다). 그러나 이번에는 하이브리드 A*가 SE(2)에서 안전한 자세의 연속체를 찾으려고 노력할 것입니다. 여행 길이를 최소화하면서 SE(2)에서 s₀ = (x₀, θ₀)에서 s₀ = (x_f, θ_f)으로의 시퀀스를 찾으려고 노력할 것입니다. 따라서, 검색하는 요소의 도메인은 SE(2)입니다.\n\n## 운동학적 모델\n\n논문에서 모델은 아래와 같이 자동차처럼 이동한다고 가정됩니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-GentleintroductiontoHybridAstar_2.png\" /\u003e\n\n논문에서 다음 단계 n+1로의 확장은 세 가지 중 하나를 따릅니다: 최대 곡률 두 개 (왼쪽 / 오른쪽)과 직선. 따라서 어떤 두 상태 s_a와 s_b 사이의 가장 짧은 이동은 Dubins 경로를 통해 해석적으로 계산될 수 있습니다.\n\n이 모델은 역 이동을 갖도록 확장할 수 있으며, 이로써 여섯 가지 가능한 확장이 생깁니다. 이 경우, Reed Shepp을 따라 가장 짧은 경로로 어떤 두 상태도 연결할 수 있습니다. 물론, 이 논문과 같이 시나리오에 맞게 확장 모델을 확장시킬 수 있습니다.\n\n# 2. 확장 (opening)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n상태 x_n \\in R²가 A 그리드 필드에서 전파될 때, 우리는 이웃한 셀들을 열었습니다. 이는 그리드를 따라 이동하는 것과 같습니다. 또한, 상태들은 각 셀의 중앙에 해당됩니다. 이것들은 하이브리드 A에서 다릅니다.\n\n## 셀 열기\n\n![이미지](/assets/img/2024-05-20-GentleintroductiontoHybridAstar_3.png)\n\n하이브리드 A\\*에서 상태 s*n = (x_n, \\theta_n)의 전파는 위의 그림과 같이 운동학을 시뮬레이션하여 새로운 셀들을 엽니다. 따라서, 새로운 상태 s'n+1' = (x*'n+1', \\theta\\_'n+1')은 거의 중앙에 도달하지 못할 수 있습니다. (이를 흰색 원으로 보여주려고 노력했습니다).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Cost-so-far g(s)\n\n각 상태 s*'n+1'의 비용을 계산하는 것은 비교적 유사합니다. 우리는 s_n과 s*'n+1' 사이의 아크의 길이를 고려하여 비용을 결정합니다. 차이점은 우리가 덜 원하는 움직임에 패널티를 부여할 수 있는 능력에 있습니다.\n\n예를 들어, 위의 그림에서 턴 움직임 (s*'n+1'² 및 s*'n+1'³)에 대해 직진 움직임 (s\\_'n+1'¹)보다 더 높은 패널티를 부과할 수 있습니다. 또한, 확장을 위해 역방향 움직임을 사용할 때, 역방향 움직임도 더 높은 패널티를 부담하게 됩니다.\n\n## 가지치기\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 시점에서 혼란스러울 것입니다: 만약 두 개 이상의 상태가 동일한 각도 구간에 있는 동일한 셀을 공유한다면 어떻게 해야 할까요? 여기서 가지치기가 등장합니다.\n\n![이미지](/assets/img/2024-05-20-GentleintroductiontoHybridAstar_4.png)\n\n성능상의 이유로, 동일한 이산화(동일한 2D 셀 및 각도 구간) 내 다른 상태가 지금까지 더 낮은 비용을 얻었다면 상태를 제거합니다. 위 그림에서, 회전 이동이 더 벌점을 받는다고 가정하면, 빨간 원은 두 회전 이동을 거치기 때문에 가지치기될 것입니다. 물론, 이 가지치기는 각도 이산화에 의해 각도가 다르게 처리된다면 일어나지 않습니다.\n\n# 3. 휴리스틱 h(s)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 섹션에서는 상태 s\\_'n'에서 셀을 확장하고 열어보는 방법을 설명했습니다. 원래의 A\\*와 마찬가지로 현재 반복에서 휴리스틱 플러스 현재까지의 비용에 기반하여 전파할 최상의 상태 s_n을 선택해야 합니다.\n\n## 비비학적 휴리스틱\n\n우리가 논의한 대로, 휴리스틱은 타당할 정도로 낙관적이어야 합니다. 휴리스틱의 개념은 A\\* 그리드와 유사합니다. 목표까지의 기대 최단 경로를 의미합니다. 그러나 물론 여기서 목표는 위치 도메인이 아닌 SE(2)에 있습니다.\n\n제가 여기서 말했듯이, 다른 포즈에서 포즈로 도달하는 것은 Dubins 경로나 Reeds Shepp 경로로 달성할 수 있습니다. 그렇다면 이것을 사용할 수 있을까요? 타당하지만 장애물을 고려하여 남은 거리를 보다 정확하게 추정할 수 있습니다. Karl의 멋진 사진을 살펴보세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![약간 Hybrid A*에 대한 친절한 소개](/assets/img/2024-05-20-GentleintroductiontoHybridAstar_5.png)\n\n이 그림에서는 미래 비용을 과대 평가할 수 있습니다. 이는 우리가 잘못된 방향(이 경우에는 벽으로 향함..)으로 전파를 낭비할 수 있습니다. 이를 방지하기 위해 위치적 영역에서 가장 짧은 경로를 계산하여 도움을 줄 수 있습니다.\n\n## 장애물 고려한 미치적 경향\n\n이 경우에 그리드 방식으로 가장 짧은 경로를 어떻게 알 수 있을까요? 원래 저자들은 동적 프로그래밍과 같은 내용을 제외하고 자세히 언급하지 않습니다. 구현에 따라 이 부분이 다르다는 것을 발견했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어, nav2의 Smac 플래너는 Dijkstra와 유사한 다이나믹 프로그래밍을 사용했습니다. 그러나 목표지점부터 출발지점이 아닌 가장 짧은 거리를 전파하는 방식입니다. 자세한 내용은 여기에서 확인해주세요. Karl의 구현은 목표 위치에 도달할 때까지 A\\*를 실행합니다. 어떤 방식이든지 진행하려면 두 가지 휴리스틱을 사용하여 2D 그리드에서 작업해야 하며, 계산을 줄이기 위해 다운샘플링 해상도를 적용해야 할 수 있습니다.\n\n## Hybrid A\\*의 휴리스틱\n\n두 휴리스틱 중 최대값을 우리 최종 휴리스틱 h(s)로 선택합니다. 다음 확장을 위해 g(s) + h(s)의 최소값을 갖는 최적의 상태 s\\_'next'를 선택합니다. 최적의 선택은 우선순위 큐에서 팝하면 자동으로 처리됩니다. 이 데이터 구조에 익숙하지 않다면, 꼭 공부해보세요. A\\* 알고리즘을 구현하는 데 꼭 필요한 개념입니다.\n\n# 4. 분석적 확장 (즉, 목표로의 슛)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 과정은 최종 결과에 큰 영향을 미치는 중요하고 독특한 측면입니다.\n\n## 최적의 경로를 탑재하는 최고의 노드는 무엇일까요? 그것이 해결책입니다!\n\n우리의 휴리스틱 가이드 방법론에서는 욕심쟁이 서치 방식을 채택하여 대기열에서 최고의 후보를 우선시합니다. 실제로 최적의 해결책을 얻을 수 있습니다. 만약 최고의 후보를 충돌 없이 목표지점에 직접 연결할 수 있다면요. 최고의 후보는 가장 낙관적인 행동을 보여주며, 이는 이상적인 선택입니다.\n\n예를 들어, A\\* 검색 알고리즘에서, 만약 마지막으로 팝된 노드와 목표 지점을 충돌 없는 직선으로 연결할 수 있다면, 우리는 해결책을 찾았습니다. 안타깝게도 충돌 가능성이 있기 때문에, 검색 과정 중에는 이 작업을 시도하지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 가장 낙관적인 접근법을 시도해 보세요!\n\n하이브리드 A\\*에서는 유망해 보일 때에 이 직선적인 목표로의 접근법을 탐색하는 기회를 살펴봅니다. 탐색 중에는 최적의 후보 자세와 목표 자세를 Dubins 경로(또는 역모션을 고려할 때 Reeds-Shepp 경로)로 연결하려고 노력합니다.\n\n이러한 접근법이 충돌 없이 성공하면 탐색을 종료하고 최적의 해를 찾은 것으로 간주됩니다. 이 기술은 원본 논문에서 \"해석적 확장\"이라고 합니다. 이러한 접근법의 성공은 하이브리드 A\\* 탐색의 실시간 효율성에 크게 기여합니다. 장애물이 없는 영역에서는 하이브리드 탐색이 몇 번의 반복으로 완료되는 경우가 많습니다. 본질적으로 이는 시작 위치에서 목표 자세로 직접 Dubins 경로를 푸는 것과 동등해집니다.\n\n## 희소 영역에서는 작동하지만 밀집된 지역에서는 그렇지 않습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n장애물이 많은 지역에서는 촬영이 실패할 수 있습니다. 이러한 경우, 탐색은 장애물 휴리스틱을 활용하여 탐사를 안내합니다. 빈 공간을 만나면 성공적인 샷을 활용하여 탐색을 신속히 종료할 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-GentleintroductiontoHybridAstar_6.png)\n\n제공된 그림에서는 트리를 확장하는 도중 갑자기 탐색 프로세스가 종료되었는데, 이는 성공적인 샷이 최적 후보와 목표 위치를 충돌 없이 연결했기 때문입니다.\n\n# 5. 분석\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 당신의 실제 로봇을 위한 최적성\n\n하이브리드 A\\*: 널리 인기를 얻은 전역 최적 경로 탐색 알고리즘이지만, 다음과 같은 이유로 실제로 전역 최적적인 것은 아닌 한계가 있습니다:\n\n## 제어 이산화\n\n우리 차의 제어 메커니즘은 Dubins 경로와 같이 세 개(또는 여섯 개)의 조향 모드에 기반한 것이 아닙니다. Dubins 경로에서 가정된 것과는 달리, 우리 로봇은 특정 사전 정의된 회전 아크를 따르지 않습니다. 대신, 연속적으로 회전 아크를 변경할 수 있는 유연성이 있습니다. 이는 우리 차의 움직임에 대한 입력이 Dubins 경로 가정에 엄격하게 따라가는 대신 이산적으로 고려된다는 것을 의미합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 노드 가지치기\n\n하이브리드 A\\* 알고리즘에서는 특정 셀의 노드를 \"지금까지의 비용\"만을 고려하여 가지치기할 수 있습니다. 그러나 이 과정에서 가지치기된 노드들이 더 나은 경로로 이어질 수도 있습니다. 지금까지의 비용만을 기반으로 한 이 기믹적 가지치기 방식은 최적 결과를 얻는데 제약을 줄 수 있습니다. 이런 한계를 극복하기 위해 가지치기 과정에서 휴리스틱 정보를 고려하는 것이 전체 경로의 최적성을 향상시킬 수 있습니다.\n\n요약하면, 하이브리드 A\\*는 강력한 경로 탐색 알고리즘이지만 차량의 제어 입력이 이산적이고 지금까지의 비용만을 고려한 기믹적 가지치기로 인해 실제 전역 최적성이 제약되어 있습니다. 이러한 제약을 극복하기 위해 더욱 진보된 제어 메커니즘과 휴리스틱을 고려한 가지치기 전략 등을 통해 알고리즘의 최적성을 높일 수 있습니다.\n\n## 성능\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n실시간 응용 프로그램에 이 알고리즘을 적용해도 괜찮습니다. 합리적으로 복잡한 환경에서 msecs 계산 시간 순서를 보여줍니다. 더 자세한 정보는 nav2 구현에서 이 결과를 읽는 것을 권장합니다.\n","ogImage":{"url":"/assets/img/2024-05-20-GentleintroductiontoHybridAstar_0.png"},"coverImage":"/assets/img/2024-05-20-GentleintroductiontoHybridAstar_0.png","tag":["Tech"],"readingTime":13},{"title":"무엇보다도 새로운 글로벌 디자인 트렌드 ","description":"","date":"2024-05-20 19:58","slug":"2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch","content":"\n## Alex Alspach과 Punyo 팀\n\nPunyo 팀: Alex Alspach (기술 리드), Andrew Beaulieu (기술 리드), Kate Tsui (기술 리드), Jose Barreiros, Aditya Bhat, Bisi Chikwendu, Sam Creasey, Eric Dusel, Aimee Goncalves, Manabu Nishiura, Aykut Önol 및 Leticia Priebe Rocha\n\n## “Punyo 만나기” 소개 동영상 (YouTube)\n\nTRI에서는 사람을 대체하는 것이 아닌 보완하는 로봇 능력을 개발하고 있습니다. 우리는 손끝만으로는 부족한 일상적인 작업을 돕는 데 도움을 주기 위해 노력하고 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리의 연구 플랫폼인 Punyo는 이 미션을 대변합니다. Punyo 팀은 TRI의 세밀한 로봇 손과 그리퍼 기반의 민첩성을 보완하기 위해 팔과 가슴을 사용한 거대 물체 조작에 초점을 맞추고 있습니다. 우리는 진정으로 능숙한 로봇이 대형, 무거운, 그리고 다루기 힘든 물건들을 다룰 수 있도록 하는 하드웨어와 알고리즘을 개발하고 있습니다.\n\n사람들은 주변 세계를 다루기 위해 창의적인 방법으로 자신의 몸을 사용합니다. 한 번에 식료품을 집 안으로 가져오는 것을 생각해보세요. 여러 개의 봉지를 팔로 들고 문을 팔꿈치로 열고, 엉덩이로 문을 열어놓고 들어가는 것일 수 있습니다. 또한 큰 상자를 들어 올리고 보관하거나 가구를 옮기거나 빨래 더미를 모을 때와 같이 가슴, 팔 및 기타 신체 부위를 사용하여 이러한 작업을 수행합니다.\n\n사람들과 로봇 모두에게 부드럽고 그립감있는 피부를 가지고 물체를 몸 가까이 다루면 더 적은 힘으로 더 무거운 물건을 다룰 수 있습니다. 그럼에도 불구하고 오늘날의 로봇들이 무거운 물체를 손만 사용하여 옮기는 것이 흔하지만 비효율적입니다. Punyo는 다르게 하고 있습니다. 그것은 많은 접촉을 두려워하지 않으며, 손을 뻗어서 잡는 대신 온몸을 활용하여 더 많은 물건을 들고 조작할 수 있습니다.\n\n# Punyo가 무엇인가요?\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n푸뇨는 저희 로봇의 이름입니다. 일본어로는 \"푸뇨\" (ぷにょ)란 부드럽고 귀여우며 탄력 있는 것을 설명합니다. 이는 토요타의 미래 가정 로봇이 안전하고 능력이 있으며 일하는 것이 즐거워야 한다는 우리의 철학을 나타냅니다. 이러한 로봇공학 접근은 다양한 형태로 나타날 수 있지만, 저희를 움직이게 하는 비전은 친근하고 안전하게 가정에서 일상적인 도전 과제를 수행할 수 있는 인간형 로봇입니다.\n\n![푸뇨](/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_0.png)\n\n## 푸뇨 하드웨어 플랫폼\n\n푸뇨의 손, 팔, 그리고 가슴은 유연한 소재와 촉각 센서로 덮여 있어서 접촉을 느낄 수 있습니다. 이러한 부드러움은 푸뇨가 다루는 물건에 적응하여 안정성, 마찰력 증가, 그리고 균일하게 분산된 접촉력을 가능하게 합니다. 촉각 감지를 통해 푸뇨는 물체에 조절된 힘을 가할 수 있고, 접촉을 감지하며 (예기치 못한 접촉도), 물체의 미끄러짐이나 충돌에 반응할 수 있습니다. 촉각 감지는 또한 사람들과 상호 작용하는 데 중요합니다. 무거운 물건을 들거나 사람들을 신체적으로 지원하는 상황에서, 로봇은 자신의 몸을 인식하고 적절히 상호 작용해야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nPunyo는 소프트 로봇으로 간주되지만 그 소프트함의 기초에는 두 개의 \"단단한\" 로봇 팔, 강성의 상체 프레임 및 허리 액추에이터가 있습니다. 저희 방식은 기존 로봇의 정밀성, 강도 및 신뢰성을 기계식 소프트 로봇 시스템의 적응성, 충격 저항성 및 감각 단숨함과 결합한 것입니다.\n\n![Punyo Soft Robot](/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_1.png)\n\n어깨부터 손목까지, Punyo의 팔은 공기로 채워진 블래더 또는 \"버블\"로 덮여 있습니다. 이 블래더는 우리 뼈를 덮는 살과 비슷합니다. 각 버블은 압력 센서에 튜브를 통해 연결되어 버블의 외부 표면에 가해지는 힘을 감지합니다. 각각의 버블(각 팔에 13개)은 원하는 강성으로 개별적으로 압력을 가할 수 있으며, 로봇 팔의 표면에 대략 5cm의 유연성을 추가합니다. 이것은 로봇의 큰 표면에 적응성 및 촉각 감지 기능을 추가하는 저렴하고 가벼운 모듈식 방법입니다.\n\n![Punyo Soft Robot](/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n팔을 덮고 있는 거품은 성형 열 밀봉 PVC 패널을 사용하여 만들어졌어요. 큰 하나의 거품이 푸뇨(Punyo)의 손목을 덮고 있지만 팔 아래의 관절이 움직이는 데 제약이 없어요. 나머지 팔 거품은 각각 두 개의 챔버로 이루어진 여섯 개의 작은 링으로 구성되어 있어요. 팔을 덮고 있는 것은 거품을 보호하고 케이블 걸림을 방지하며 외부 접촉 표면 소재를 수정할 수 있도록 하는 맞춤형 패브릭 슬리브입니다. 슬리브는 유지보수 용이하게 제거 가능하며 미적인 사용자 정의 기회를 제공해요.\n\n푸뇨에는 그립퍼가 없기 때문에 손가락이나 엄지가 없어요. 적어도 아직은요. 대신, 푸뇨는 \"발\"을 가지고 있어요. 각 발은 안에 카메라가 있는 단일 고 마찰 라텍스 거품이에요 (TRI의 소프트-버블 비수용각감각 센서 기반, punyo.tech/bubblegripper). 이 거품 내부에는 점 패턴이 인쇄되어 있어요. 거품이 무언가에 닿으면 점 패턴이 변형돼요. 내부 카메라는 이 변형을 사용하여 힘을 추정하고 이미지를 바로 학습된 시각 운동 정책에 전달해요.\n\n# 전신 기술 교육을 위한 원격 작동 도구\n\n우리는 다양한 전신 작업에서 하드웨어를 테스트했어요. 푸뇨는 고감각 정책을 학습하기 위해 두 가지 강력한 방법인 확산 정책과 예시 지도 강화 학습을 사용해요. 하드웨어에서 직접 새로운 작업을 시도하고 이러한 학습 파이프라인에 예시 시연을 제공하기 위해 전신 기술에 직관적인 원격 운영 인터페이스를 개발 중이에요. 작년 TRI에서 발표된 확산 정책은 사람의 시범을 사용하여 어려운 모델링 작업을 위한 견고한 감각 운동 정책(카메라 및 촉각 피드백 사용)을 학습해요. 예시 지도 강화 학습(EGRL)은 작업이 시뮬레이션에서 모델링되어야 하며 작은 시범 집합이 로봇의 탐색을 안내해야 해요. 두 가지 방법 모두 유연성을 활용하고 촉각 피드백을 통합하는 견고한 정책을 생성해요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n### Synergy Teleop을 이해하세요: 핵심 전신 조작 작업, 예를 들어 붙잡기와 들기와 같은 작업은 손을 사용한 붙잡기와 유사한 시너지로 분할되어 있습니다. 이러한 동작은 우리가 게임패드 인터페이스를 통해 개별적으로 제어하는 동작입니다. 팔을 독립적으로 들린 후 내리고, 각 팔의 붙잡기를 열고 닫음으로써 단일 및 양손으로 붙잡고 다시 붙잡고 들기 및 붙잡은 상태에서 조작하는 팔을 위해 조합합니다. 표준적이고 저렴한 게임패드만 필요로 하며, 실험은 연구실이나 야생에서 아무 곳에서나 수행될 수 있습니다.\n\n![이미지](/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_3.png)\n\n### 계층적 운영-공간 Teleop: 더 복잡하고 정교하거나 섬세한 작업을 위해, 발과 팔꿈치, 그리고 상체 각도를 직접 제어할 수 있습니다. 이들을 제어함으로써 우리는 예를 들어 몸을 앞으로 숙여 가슴에 물건을 모을 수 있고, 다른 팔을 물체 위에 감아놓거나 다른 팔은 아래에 위치시키고, 뒤로 기울여 들 수 있습니다. 움직임 캡처 카메라를 사용하여 우리의 원격 조종자의 등, 어깨, 팔꿈치, 그리고 손에 부착된 톱니들을 추적하고 이를 로봇의 유사한 지점(운영 지점)에 대응시킵니다. 원격 조종자의 동작은 Punyo로 리디렉팅되어 어떤 사람이든 체형과 관계없이 시스템을 운영할 수 있습니다.\n\n우리는 전신 움직임 제어 접근 방식을 통해 원격 조작 및 자율 확산 정책을 전개합니다. 예를 들어, 원격 조작 중에는 최우선 순위 작업인 종단부 자세 추적이 필요해 Punyo의 발이 원격 조종자의 발을 신뢰할 수 있게 추적해야 합니다. 팔꿈치 자세 추적은 중요도가 낮을 수 있으며, Punyo가 최우선 순위인 종단부 추적과 간섭하지 않는 한 원격 조작 팔꿈치 자세를 달성할 수 있습니다. 계층적 움직임 제어 프레임워크를 사용하면 제약 조건과 운영 지점 추적을 추가, 제거, 조정 및 재우선화하여 원격 조작 인터페이스를 신속히 반복할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nThe hierarchical framework also provides an interface for exploring physically reactive motions and human-robot interaction. An example is Punyo moving its elbow out of the way when an accidental bump is sensed. Because of the redundant joints in our arms, we can move the elbow without sacrificing end-effector motion. This control scheme can be used to allow a person to push Punyo’s arm out of the way mid-task to quickly grab something.\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*wGszjFTuqa5Ludd8foyQXA.gif)\n\n## Guiding Reinforcement Learning with Human Examples\n\nWe employ Example-Guided Reinforcement Learning (EGRL) to develop sturdy manipulation policies for tasks we can simulate. Demonstrating the task enhances the learning process efficiency and lets us influence the style of motion the robot uses to complete the task. We utilize Adversarial Motion Priors (AMP), typically used for stylizing computer-animated characters, to introduce human motion imitation into our reinforcement learning pipeline. All policies displayed below were trained with just one demonstration.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리의 파이프라인은 시범 본따기 보상과 작업 목표 보상의 비율인 λ변수를 노출합니다. 이 변수를 사용하면 Punnyo가 동작을 모방하는 것과 순전히 탐구를 통해 작업을 성공적으로 수행하는 것 사이에서 어떻게 균형을 맞출지 조절할 수 있습니다. 예를 들어, 로봇의 어깨 위로 큰 항아리를 올리는 작업을 배우는 경우를 생각해보세요. 다이얼을 한 방향으로 돌리면(λ = 0), 로봇은 단순히 \"자세를 따라 하는\" 것으로, 항아리가 탁자를 떠나는지 여부에 관계없이 동일한 작업에 대한 인간 시범 동작을 모방합니다. 다이얼을 다른 방향으로 돌리면(λ = 1), 로봇은 전혀 시범을 무시하고, 항아리를 탁자에서 내리는 데만 집중하며 임무를 수행하는 데 필요한 어떤 비자연스러운 동작이라도 (희망적으로) 수렴합니다. 그 사이의 λ는 시범의 스타일로 작업을 완료하는 정책을 생성합니다.\n\n작업 설명은 단지 항아리를 올리는 것보다 복잡할 수 있습니다. 예를 들어, 우리는 속도, 저에너지 소비, 또는 보다 견고한 작업 완료를 장려할 수도 있습니다. 기능 요구 사항을 인간 시범 스타일과 섞어서 결합하면 효율적이지만 사람들이 예측하고 가인화하기가 더 쉬운 로봇이 될 수 있으며, 그들이 더 편안하고 생산적으로 협력할 수 있게 해줍니다. 다양한 작업, 상황, 로봇, 인간 협력자에 대한 적절한 균형을 발견하기를 고대합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*UbzHiHf4pgipEfifAtmnSw.gif)\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*Ne8RXDgCjVWAZenDHi5zxA.gif)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# Plan-Guided Reinforcement Learning\n\n강화 학습은 훈련을 위해 작업을 시뮬레이션에서 모델링해야 합니다. 따라서 우리는 텔레오퍼레이션 대신 모델 기반 플래너를 사용하여 데모를 위한 계획을 세울 수 있습니다. 이를 Plan-Guided Reinforcement Learning (PGRL)라고 부릅니다. 플래너를 활용하면 텔레오퍼레이션으로는 어려운 장기 과제들도 수행할 수 있습니다. 또한 인간 입력에 대한 의존성을 줄여 자동으로 여러 데모를 생성할 수 있으며, 이는 Punyo가 처리할 수 있는 작업 수를 확장하는 한 걸음입니다. 이 능력은 언젠가 Punyo가 스스로 새로운 기술을 학습할 수 있게 할 수도 있습니다.\n\n현재 최신 모델 기반 플래너는 복잡한 접촉 시퀀스를 포함하는 동작 계획을 생성할 수 있지만, 그 결과를 온라인에서 폐쇄 루프 방식으로 사용하기에는 충분히 빠르지 않습니다. 또한 모델의 부정확함과 가정으로 인해 시뮬레이션에서도 물리적으로 실행할 수 없는 경우가 있을 수 있습니다. 따라서 로봇 하드웨어에서 열린 루프로 재생될 때 원하는 대로 대상 객체를 조작하지 못할 가능성이 있는 궤적이 남습니다. 그러나 이대로 오픈 루프로 실행되는 경우 원하는 대상 객체를 조작하지 못할 가능성이 매우 큽니다. 그러나 예제 기반 강화 학습을 사용하여 이러한 불안정한 궤적을 가져다가 실행 가능한 피드백 정책으로 변경할 수 있습니다.\n\n짧은 접촉을 통해 장기적인 행동을 합성하는 데 접촉 묵시 플래너를 사용합니다. 플래너는 글로벌 접촉 추론을 가능하게 하는 몇 가지 가정을 합니다. 그런데 이로 인해 해당 결과를 하드웨어에서 직접 사용할 수 없습니다. 그러나 이 궤적은 EGRL 파이프라인에 시드로 작업에 필요한 동작 및 접촉 시퀀스를 제공하는 훌륭한 데모로 작용합니다. 작업 목표, 도메인 랜덤화 및 이러한 초기 데모로 동작 탐색을 가리키는 경우, Punyo는 모션 계획을 따르는 폐쇄 루프 정책을 효율적으로 학습하여 하드웨어에서 견고하게 어려운 작업을 수행할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*XuViGzzbwQISa9HyDmIzMw.gif)\n\n![Image](https://miro.medium.com/v2/resize:fit:1400/1*XiqFuPa83jIeSndMtEsczg.gif)\n\n# 안전하고 생산적인 협력을 향해\n\nTRI와 다른 곳에서 능숙한 능력이 급증하는 가운데, 현재의 로봇 및 조작 전략은 여러 작업과 기술을 이루리 하는 데 많은 어려움을 겪고 있습니다. 손으로 다루기에 너무 큰 물체, 팔이 안정화해야 하는 물건 더미, 한 팔에 들고 있는 물체와 다른 물체를 다루는 작업, 그리고 좁은 공간과 사람 주변에서 안전하게 운영하는 능력이 필요합니다. 그리퍼 기반 민첩성과 병행하여 전체 몸을 이용한 조작을 위한 하드웨어, 지식 및 데이터셋 개발은 다양한 능력을 갖춘 조작 플랫폼을 만들기 위해 중요합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Punyo Soft Robot](/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_4.png)\n\n로봇의 가슴, 팔 및 기타 몸 표면을 조작하기 위해 잠금 해제하는 것은 기계적으로 유리합니다. 준수 및 마찰과 결합하여, 로봇은 더 적은 에너지로 큰 물건을 잡고 조작할 수 있어서 페이로드와 배터리 수명을 늘리고 더 저렴한 시스템을 가능하게합니다. 부드럽기 때문에 충격을 흡수할 수 있어 로봇의 안전과 주변 사람들의 안전을 위해 사용될 수 있습니다. 촉각 감지를 추가하면 접촉력을 밀접하게 모니터링하고 제어하여 부드럽고 복잡하며 상호작용적이고 안정적인 조작이 가능합니다.\n\nTRI의 Punyo 팀은 이 문제를 해결하기 위해 구축되었습니다. 부드러운 로봇공학, 접촉을 고려한 계획 및 학습, 촉각 감지 및 인간-로봇 상호작용에 대한 우리의 전문가들은 로봇과 사람이 안전하게, 생산적으로, 행복하게 공존할 수 있는 미래에 전념하고 있습니다.\n\n# 감사의 글\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n러스트 테드레이크, 토피 알비나, 막스 바지라차리야, 길 프랫, 꽁대짱 하시모토(푸뇨라는 이름으로!), 벤 버치필, 알레한드로 카스트로, 에릭 쿠시너, 홍카이 다이, 리처드 데니토, 에블린 딕슨, 지미 도르니에, 세라 에브치민, 시유안 펑, 크리스 기드웰, 스테이시 기드웰, 담롱 구오이, 브렌든 해쓰어웨이, 앨리슨 헨리, 피비 호르간, 제나 홀만, 스티브 이아코비노, 루카스 카울, 나빈 쿠퍼스와미, 앨리사 라우, 존 라이히티, 수잔 마이클, 고든 리차드슨, 파스 샤, 리사 토바스코, 아비나시 우딤찬다니, 트리스탄 위팅, 자로드 윌슨, 존 요우, 맹채오 장, 알렉스 알렉자니안, 리드 알스팩, 윌 나이트, 리즈 펄먼, 그리고 우리 아기들과 소중한 이들에게 특별한 감사를 ❤️\n","ogImage":{"url":"/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_0.png"},"coverImage":"/assets/img/2024-05-20-MeetPunyoTRIsSoftRobotforWhole-BodyManipulationResearch_0.png","tag":["Tech"],"readingTime":12},{"title":"Raspberry Pi 5에 ROS2 Humble을 설치하고 Docker를 사용하여 micro-ROS를 통해 ESP32와 통신하는 방법","description":"","date":"2024-05-20 19:55","slug":"2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker","content":"\n\u003cimg src=\"/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_0.png\" /\u003e\n\n어려움을 겪은 끝에, Raspberry Pi 5를 ESP32 마이크로컨트롤러에 연결하는 데 성공했습니다. 목표는 Raspberry Pi 5와 ESP32-WROOM-32 마이크로컨트롤러 간에 마이크로-ROS를 통한 통신을 활성화하는 것이었습니다.\n\n다른 사람들이 시간을 절약할 수 있도록, 이 상세한 튜토리얼을 작성하기로 결정했습니다. 아래 단계에 따라 따라할 수 있습니다:\n\n- Raspberry Pi 5에 Ubuntu 23.10 Server 설치\n- Raspberry Pi 5에 Docker 설치\n- ROS2 Humble 설치를 위한 Dockerfile 생성\n- ROS2 Humble 설치(Dockerfile 내에서)\n- 마이크로-ROS 설치(Dockerfile 내에서)\n- Docker 이미지 빌드\n- ROS2 Humble 도커 컨테이너 실행\n- 마이크로-ROS 예제 빌드 및 플래시\n- 마이크로-ROS 에이전트 생성 및 실행\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 라즈베리 파이5에 우분투 23.10 서버 설치하기\n\n우선, Raspberry Pi Imager를 사용하여 라즈베리 파이 5에 우분투 23.10 서버 운영 체제를 설치했습니다. 이를 통해 화면을 사용하지 않고 SSH를 통해 장치에 연결할 수 있었습니다.\n\n## Docker 설치하기\n\nSSH를 통해 Raspberry Pi 5에 액세스한 후 Docker를 설치했습니다. ROS2의 Humble 배포의 지원하는 운영 체제인 Ubuntu 22.04를 사용하기 위해 Docker를 사용하면 Raspberry Pi 5에서 호환 환경을 만들 수 있습니다. (https://docs.docker.com/engine/install/ubuntu/)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nsudo apt-get update\n\nsudo apt-get install ca-certificates curl\n\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\necho   \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n$(. /etc/os-release \u0026\u0026 echo \"$VERSION_CODENAME\") stable\" |   sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nsudo usermod -aG docker $USER\n```\n\n## Raspberry Pi5용 ROS2 Humble 설치를 위한 Dockerfile\n\n원하는 ROS2 버전이 이미 포함된 Docker 이미지로 시작할 수 있습니다. 제 경우에는 필요한 Ubuntu 버전인 22.04(Jammy)에서 시작하는 Dockerfile을 만들기로 결정했습니다.\n\n```js\nFROM ubuntu:jammy\n\nRUN locale  # UTF-8 확인\nRUN apt update \u0026\u0026 apt install locales -y\nRUN locale-gen it_IT it_IT.UTF-8\nRUN update-locale LC_ALL=it_IT.UTF-8 LANG=it_IT.UTF-8\nRUN export LANG=it_IT.UTF-8\n\n# 지역 시간대 설정\nENV ROS_VERSION=2\nENV ROS_DISTRO=humble\nENV ROS_PYTHON_VERSION=3\nENV TZ=Europe/Rome\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026\u0026 echo $TZ \u003e /etc/timezone\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n도커 파일은 Ubuntu 22.04의 기본 이미지로 시작합니다. 그 다음으로 로캘 및 시간대가 설정됩니다. 저의 경우에는 이탈리아로 설정되어 있으며, 이는 ROS2를 설치하는 데 필요합니다.\n\n이 설정 이후에는 ROS2 패키지를 설치할 때 시간대를 입력하지 않고도 필요한 환경 변수를 몇 개 설정했습니다.\n\nMarkdown 형식으로 표를 바꿔주겠습니다:\n\n| 명령어                                                                                                                                                                                          | 설명                                              |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- | ---------------- |\n| RUN apt install software-properties-common -y                                                                                                                                                   | 소프트웨어 속성 공통 부분 설치                    |\n| RUN add-apt-repository universe                                                                                                                                                                 | Universe 저장소 추가                              |\n| RUN apt update \u0026\u0026 apt install curl -y                                                                                                                                                           | 업데이트 후 curl 설치                             |\n| RUN curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg                                                                     | ROS 키 다운로드                                   |\n| RUN echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release \u0026\u0026 echo $UBUNTU_CODENAME) main\" | tee /etc/apt/sources.list.d/ros2.list \u003e /dev/null | ROS2 리스트 추가 |\n| RUN apt update \u0026\u0026 apt upgrade -y                                                                                                                                                                | 업데이트 및 업그레이드                            |\n\n이 부분의 도커 파일에서는 ROS2 설치를 위해 시스템을 구성하며, 필요한 저장소를 추가하고 시스템 패키지를 업데이트합니다(설치에 관한 공식 가이드는 여기에서 확인할 수 있습니다: https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debians.html).\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_1.png\" /\u003e\n\n시스템을 구성한 후에는 ROS2 Humble 및 micro-ROS를 설치할 준비가 되었습니다. 두 개의 스크립트를 사용했어요: install_ros2.sh와 install_microros_esp32.sh입니다. 이 스크립트들은 작업 디렉토리인 /ros2_project에 복사되고 실행 가능하게 만들었습니다.\n\n```js\nWORKDIR /ros2_project\nCOPY scripts/*.sh /ros2_project/\n\nRUN chmod +x ./*.sh\nRUN ./install_ros2.sh\nRUN ./install_microros_esp32.sh\n```\n\n## ROS2 Humble 설치\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n# install_ros2.sh\n#!/bin/bash\n\n# ROS2 베이스 및 개발 도구 설치\napt install ros-humble-ros-base -y\napt install ros-dev-tools -y\n\n# 셸을 설정하여 ROS2 설정 파일을 자동으로 소스로 지정합니다.\n# echo 'source /opt/ros/humble/setup.bash' \u003e\u003e ~/.bashrc (optional)\necho 'source /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bash' \u003e\u003e ~/.bashrc\n```\n\n## 마이크로-ROS 설치\n\nmicro-ROS를 설치하고 사용하려면 먼저 ESP32 칩에 호환되는 소프트웨어 개발 환경을 설정해야 합니다. 저는 micro-ROS 개발자들에 의해 테스트된 버전 5.2를 선택했습니다.\n\n```js\n# install_espressif.sh\n#!/bin/bash\n\n# 'esp' 폴더가 있는지 확인합니다.\nif [ ! -d ~/esp ]; then\n\n    # 패키지 목록을 업데이트하고 필요한 종속성을 설치합니다.\n    apt-get update -y\n    apt-get install git wget flex bison gperf python3 python3-pip python3-venv cmake ninja-build ccache libffi-dev libssl-dev dfu-util libusb-1.0-0 -y\n\n    # ESP 개발을 위한 디렉토리를 생성하고 ESP-IDF 리포지토리를 복제합니다.\n    mkdir -p ~/esp\n    cd ~/esp\n    git clone -b v5.2 --recursive https://github.com/espressif/esp-idf.git\n\n    # ESP-IDF 디렉토리로 이동하여 ESP32 도구체인을 설치합니다.\n    cd ~/esp/esp-idf\n    ./install.sh esp32\n\n    # IDF_PATH 환경 변수를 설정하고 지속적으로 유지합니다.\n    export IDF_PATH=$HOME/esp/esp-idf\n    echo export IDF_PATH=$HOME/esp/esp-idf \u003e\u003e /root/.bashrc\n    . $IDF_PATH/export.sh\nelse\n    echo \"'esp' 폴더가 이미 있습니다.\"\nfi\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n# install_microros_esp32.sh\n\n#!/bin/bash\n# ESP32 설정 스크립트 실행하기\n./install_espressif.sh\n\n# 작업 공간 생성 및 micro-ROS 도구 다운로드하기\ngit clone -b $ROS_DISTRO https://github.com/micro-ROS/micro_ros_espidf_component.git\n\n# 의존성 설치하기\nexport IDF_PATH=$HOME/esp/esp-idf\nsource $IDF_PATH/export.sh \u0026\u0026 pip3 install catkin_pkg lark-parser colcon-common-extensions\n```\n\n## 도커 이미지 빌드\n\n이 단계에서 우리의 Dockerfile은 다음과 같이 나타납니다:\n\n```js\nFROM ubuntu:jammy\n\nRUN locale  # UTF-8 확인\nRUN apt update \u0026\u0026 apt install locales -y\nRUN locale-gen it_IT it_IT.UTF-8\nRUN update-locale LC_ALL=it_IT.UTF-8 LANG=it_IT.UTF-8\nRUN export LANG=it_IT.UTF-8\n\n# 시간대 설정\nENV ROS_VERSION=2\nENV ROS_DISTRO=humble\nENV ROS_PYTHON_VERSION=3\nENV TZ=Europe/Rome\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026\u0026 echo $TZ \u003e /etc/timezone\n\n# 필요한 리포지토리 추가 및 시스템 패키지 업데이트\nRUN apt install software-properties-common -y\nRUN add-apt-repository universe\nRUN apt update \u0026\u0026 apt install curl -y\nRUN curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\nRUN echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release \u0026\u0026 echo $UBUNTU_CODENAME) main\" | tee /etc/apt/sources.list.d/ros2.list \u003e /dev/null\nRUN apt update \u0026\u0026 apt upgrade -y\n\n# 작업 디렉토리 지정하고 스크립트 복사\nWORKDIR /ros2_project\nCOPY scripts/*.sh /ros2_project/\n\n# 스크립트를 실행 가능하도록 하고 실행하기\nRUN chmod +x ./*.sh\nRUN ./install_ros2.sh\nRUN ./install_microros_esp32.sh\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n도커 파일이 있는 디렉토리에서 다음 명령을 실행하세요: docker build . -t `저장소_이름:태그`\n\n출력은 다음과 같이 나타납니다:\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_2.png)\n\n생성된 이미지를 확인하려면 \"docker images\" 명령을 사용하여 사용 가능한 모든 도커 이미지를 볼 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_3.png)\n\n## ROS2 Humble 도커 컨테이너 실행하기\n\n이미지를 만든 후에는 해당 이미지를 사용하여 컨테이너를 실행할 수 있습니다. 컨테이너는 가벼우며 응용 프로그램과 의존성을 실행하는 격리된 환경으로, 가상 머신과 유사하지만 오버헤드가 적습니다.\n\n컨테이너를 실행하려면:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```bash\ndocker run -it \\\n    --name=ros2_humble \\\n    --net=host \\\n    --privileged \\\n    -v /dev:/dev \\\n    ros2rover:humble-pi5-esp32-v1 \\\n    bash\n```\n\n여기서:\n\n- -it: 의사-TTY를 할당하고 STDIN을 열어 놓아 컨테이너와 상호 작용할 수 있게 함.\n- --name=ros2_humble: 컨테이너의 이름을 ros2_humble로 지정함\n- --net=host: 호스트와 네트워크 네임스페이스를 공유하여 컨테이너가 호스트 네트워크 인터페이스를 사용할 수 있게 함.\n- --privileged: 컨테이너를 권한 부여 모드로 실행하여 호스트의 모든 장치에 액세스할 수 있게 함.\n- -v /dev:/dev: 호스트의 /dev 디렉토리를 컨테이너에 마운트하여 컨테이너가 호스트의 모든 장치 파일에 액세스할 수 있게 함.\n- ros2rover:humble-pi5-esp32-v1: 컨테이너 생성 시 사용할 Docker 이미지의 이름을 지정함.\n- bash: 컨테이너 내에서 실행할 명령을 지정함. 이 경우에는 Bash 셸을 시작함.\n\n이 명령을 실행하면 컨테이너 내의 작업 디렉토리에서 Bash 셸 프롬프트가 열리며, 컨테이너 환경과 직접 상호 작용할 수 있게 됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 미크로-ROS int32_publisher 예제 빌드 및 플래시\n\n![예제 이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_4.png)\n\n기능을 확인하려면 복제된 micro-ROS 저장소에 포함된 예제 중 하나를 빌드할 수 있습니다. 다음은 따라야 할 단계입니다:\n\n- 다음 명령어로 예제 폴더로 이동하세요:\n\ncd micro_ros_espidf_component/examples/int32_publisher\n\n- ESP-IDF를 위해 소프트웨어 개발 환경을 활성화하세요:\n\n. $IDF_PATH/export.sh\n\n- 빌드할 타겟을 설정하세요 (이 경우 ESP32):\n\nidf.py set-target esp32\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_5.png)\n\n- 와이파이 연결 및 IP 호스트 대상 설정 (Raspberry Pi5 IP):\n  runidf.py menuconfig을 실행한 후 micro-ROS 설정으로 이동하여 micro-ROS Agent IP를 호스트 IP로 설정하고 (Raspberry Pi5 IP를 알고 싶다면 bash 쉘에서 ifconfig를 실행하세요) Raspberry Pi5와 ESP32가 연결될 네트워크의 SSID 및 비밀번호로 WiFi 구성을 설정하세요.\n  그런 다음 “S”로 저장하고 “Q”로 나가세요.\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_6.png)\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_7.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_8.png)\n\n- 명령어를 사용하여 예제를 빌드하세요: idf.py build\n\n![image](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_9.png)\n\n- micro-USB 케이블을 사용하여 ESP32 보드를 라즈베리 파이5에 연결하고 읽기 및 쓰기 권한이 있는지 확인하세요.\n  연결하는 USB 포트에 할당된 장치 이름을 확인하려면 해당 포트에 연결하기 전에 다음 명령어를 실행하세요: journalctl --follow (라즈베리 파이5 셸에서 실행, 컨테이너에서는 실행하지 마세요).\n  장치 이름을 알면 다음 명령어를 사용하여 라즈베리 파이5가 읽고 쓸 수 있게 설정할 수 있습니다: 내 경우에는 `device name`이 ttyUSB0과 같습니다.\n  sudo chmod 666 /dev/`device name`\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_10.png)\n\n- 마지막으로 ESP32 보드에 펌웨어를 플래시하는 방법입니다: idf.py flash\n  여러 USB 장치가 연결된 경우 포트를 지정할 수 있습니다: idf.py -p /dev/ttyUSB0 flash\n\n## 마이크로-ROS 에이전트를 생성하고 ESP32에서 발행된 메시지를 읽어보세요\n\n마이크로-ROS 에이전트는 ESP32의 마이크로-ROS 노드와 ROS 2 네트워크 간의 다리 역할을 합니다. 통신을 수립하려면 적절한 이미지를 사용하여 Docker를 통해 마이크로-ROS 에이전트를 실행하세요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# udp4은 IPv4 네트워크 상에서 통신을 위해 UDP를 사용하는 것을 특히 나타냅니다.\n\n도커 실행 -it --rm --net=host microros/micro-ros-agent:humble udp4 --port 8888 -v6\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_11.png)\n\nESP32 보드를 전원을 다시 켜고 끈 후에는, micro-ROS Agent 컨테이너 쉘에서 다음 출력을 볼 수 있어야 합니다:\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_12.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nROS2 Humble 컨테이너에서 ROS2 Humble 환경을 소스로 지정한 다음에는 다음을 작성하세요: ros2 topic list.\n이제 /freertos_int32_publisher 토픽이 나타날 것이며, 여기에 ESP32가 정수 (int32) 데이터를 발행 중입니다.\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_13.png)\n\n이제 해당 토픽에서 발행된 메시지를 읽을 수 있습니다. ESP32가 /freertos_int32_publisher 토픽에서 발행한 메시지를 구독하고 표시하려면 다음 명령을 사용하여 ROS2 Humble 컨테이너 내에서 메시지를 구독하세요: ros2 topic echo /freertos_int32_publisher\n\n![이미지](/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_14.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 결론\n\n이렇게까지 와주셔서 축하드립니다! 이 포괄적인 안내를 따라가면, Docker를 통해 ROS2 Humble과 micro-ROS를 활용하여 Raspberry Pi 5와 ESP32 마이크로컨트롤러 간의 통신을 성공적으로 활성화할 수 있습니다.\n\n이 설정은 Docker가 제공하는 유연성과 격리 기능을 활용하여 안정적인 환경을 제공하여 마이크로-ROS 애플리케이션을 개발하고 테스트할 수 있습니다.\n\n이 단계를 따라가면, 이제 마이크로-ROS를 활용하여 Raspberry Pi 5와 ESP32 간의 통신을 브릿지하여 다양한 사물 인터넷(IoT) 애플리케이션에 활용할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n행운을 빌며 코딩을 즐기세요! ROS2, Raspberry Pi 5 및 ESP32로 가능한 범위를 넓히는 실험을 망설이지 마세요.\n\n저와 연락해요: [https://www.linkedin.com/in/antonioconsiglio/](https://www.linkedin.com/in/antonioconsiglio/)\n","ogImage":{"url":"/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_0.png"},"coverImage":"/assets/img/2024-05-20-HowtoInstallROS2HumbleonRaspberryPi5andEnableCommunicationwithESP32viamicro-ROSUsingDocker_0.png","tag":["Tech"],"readingTime":18},{"title":"라즈베리 파이 Pico와 MicroPython으로 DIY 쿼드콥터 드론을 만들어봅시다","description":"","date":"2024-05-20 19:54","slug":"2024-05-20-TakingFlightwiththeRaspberryPiPicoMicroPythonDIYQuadcopterDrone","content":"\n![Quadcopter](/assets/img/2024-05-20-TakingFlightwiththeRaspberryPiPicoMicroPythonDIYQuadcopterDrone_0.png)\n\n라즈베리 파이 피코는 라즈베리 파이 재단에서 처음으로 직접 설계한 마이크로컨트롤러인 RP2040을 장착한 매우 다재다능한 플랫폼으로 매우 저렴한 비용으로 구매할 수 있습니다!\n\n스마트 홈 시스템, LED 조명 제어, 공기 질 센서 등과 같은 것들을 제어하는 데 라즈베리 파이 피코로 할 수 있는 일의 한계는 우리의 상상력뿐인 것 같습니다!\n\n2023년 여름에 라즈베리 파이 피코를 \"두뇌\"로 사용하여 파이썬 기반의 사용자 정의된 비행 컨트롤러를 실행시키는 DIY 쿼드콥터 드론을 개발했습니다. 이 작업은 계산적으로 큰 도전이었지만, 많은 시행착오를 거치면서 RP2040에서 충분한 성능을 뽑아내어 안정적이고 기민한 비행을 가능하게 했습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n처리 능력 뿐만 아니라, 소형이면서도 강력한 Raspberry Pi Pico는 쿼드콥터에 필요한 다양한 센서 및 구성 요소를 쉽게 통합할 수 있게 해주었습니다. GPIO 핀을 통해 다음과 같이 쉽게 통합할 수 있었어요:\n\n- I2C 프로토콜을 통해 MPU-6050 가속도계 및 자이로스코프로부터 텔레메트리 읽기\n- UART를 통해 기판 수신기로부터 무선통신으로 레이디오 명령 수신\n- PWM을 통해 ESC를 통해 네 개의 독립된 모터 제어\n\n# 12부작 시리즈\n\n전 이 DIY 쿼드콥터 드론을 완전히 오픈소스로 공개했어요. '스카우트'라는 이 쿼드콥터 개발 과정을 문서화한 12부작 시리즈를 작성했어요. 아래 시리즈에서 스카우트의 각 조각에 대한 자세한 자습서를 찾을 수 있답니다. 이 시리즈는 개념 및 코드 설명, 샘플 코드, 배선 다이어그램, 안전 지침 등으로 구성된 완전한 내용을 제공해요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 스카웃 비행 컨트롤러 소개\n- 쿼드콥터 비행 역학\n- 자이로스코프를 사용한 텔레메트리 캡처\n- RC 수신기로 조종사 입력 수신\n- PID 컨트롤러로 비행 안정화\n- ESC 및 PWM을 사용한 브러시리스 모터 제어\n- 쿼드콥터 하드웨어 설정\n- 전체 비행 컨트롤러 코드 및 설명\n- 비행 시작\n- 끈기에 관한 한 가지 교훈\n- 잠재적인 향후 개선점\n- 보너스 코드\n\n질문이 있으시면 Twitter/X에서 @TimHanewich로 연락해주세요!\n","ogImage":{"url":"/assets/img/2024-05-20-TakingFlightwiththeRaspberryPiPicoMicroPythonDIYQuadcopterDrone_0.png"},"coverImage":"/assets/img/2024-05-20-TakingFlightwiththeRaspberryPiPicoMicroPythonDIYQuadcopterDrone_0.png","tag":["Tech"],"readingTime":2},{"title":"Raspberry Pi로 Slurm HPC 클러스터 구축하기 단계별 안내","description":"","date":"2024-05-20 19:50","slug":"2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide","content":"\n이 게시물에서는 Raspberry Pi를 사용하여 Slurm 하이퍼파워 컴퓨팅(HPC) 클러스터를 구축한 시도를 공유하려고 합니다. 전에 이 클러스터를 시작했을 때는 GPU 컴퓨팅도 지원하는 더 큰 HPC 클러스터를 만들기 위한 시범용으로 사용했습니다. HPC 설정의 다양한 구성 요소를 실제로 직접 다루어 보며 모두가 어떻게 맞물려 있는지 파악했습니다. SLURM 구성 요소를 설정하는 것이 실제로 그 중요한 부분 중 하나였고, 약간의 검토 후에 나만의 HPC 클러스터를 성공적으로 구축했습니다. 이 기계를 설정하는 것이 꽤 간단하기 때문에, 다양한 소프트웨어 패키지 및 라이브러리를 빨리 시험해보거나 최적의 클러스터 하드웨어를 조정하는 데 효과적이었습니다.\n\n여기서의 목표는 여러 컴퓨팅 노드를 처리할 수 있는 HPC 클러스터를 구축하는 것입니다. 이러한 시스템을 처음부터 만드는 것은 어려운 시도입니다. 또한 Slurm 클러스터를 구성하는 데 필요한 모든 단계를 다루는 포괄적인 자습서를 찾는 것은 쉽지 않을 수 있습니다, 적어도 제 경험상으로는요. 그런 관계로 여러분이 HPC 클러스터를 구축하는 여정에서 이 단계별 가이드가 유용하게 활용되기를 바랍니다.\n\n![Building Slurm HPC Cluster with Raspberry Pis](/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_0.png)\n\n## 목차\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 게시물은 HPC에 대한 간단한 소개로 시작되어 주로 사용되는 자원 관리자 및 작업 일정 관리자인 Slurm의 중요성에 대해서 소개합니다. 그 후에 클러스터 토폴로지를 설명하며 필수 전제 조건, 하드웨어 사양 및 운영 체제 설정에 대해 논의할 것입니다. 그 후에는 스토리지 노드 설정을 안내하고, 소스에서 Slurm을 빌드하고, 설치하고, 마스터 노드에서 구성하는 방법과 추가 계산 노드를 추가하는 것에 관해 설명하겠습니다. 마지막으로 구성된 Slurm 클러스터의 상태를 보여주고 작업 제출이 어떻게 처리되는지 몇 가지 예제를 제시할 것입니다.\n\n## 기능\n\n결과적으로 자신만의 HPC 클러스터를 구축할 수 있게 될 것입니다.\n\n- Slurm 워크로드 관리자\n- 중앙 집중식 네트워크 저장소\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제까지 너무 길어지지 않도록 최소한의 기능에 집중했습니다. 목표는 먼저 핵심 기능을 갖춘 HPC 클러스터를 설정하되, 나중에 개선할 수 있는 유연성을 유지하는 것입니다.\n\n# HPC 클러스터란?\n\nHPC 클러스터는 다양한 분야에서 높은 속도로 대량의 데이터를 처리해 계산 문제를 해결하기 위해 설계된 상호 연결된 컴퓨터 네트워크입니다. 이러한 클러스터는 다수의 컴퓨팅 노드로 구성되어 있으며, 각 노드에는 프로세서, 메모리 및 종종 GPU와 같은 전문 가속기가 장착되어 있어 연구원과 과학자들이 계산적으로 요구되는 작업 및 시뮬레이션에 대응할 수 있습니다. Slurm은 리소스 관리를 위한 간단한 Linux 유틸리티를 의미하며, 오픈 소스 HPC 작업 스케줄러 및 리소스 관리자입니다. 이는 컴퓨팅 자원을 효율적으로 할당하고 작업 스케줄링을 관리하며 HPC 클러스터에서 병렬 계산을 조정하는 데 중요한 역할을 합니다. 쿠버네티스 클러스터에 대안으로, Slurm은 과학 연구, 시뮬레이션 및 데이터 분석 작업에서 일반적으로 발견되는 일괄 컴퓨팅 워크로드의 관리에 특화되어 있습니다. 반면 쿠버네티스는 컨테이너화된 애플리케이션 및 마이크로서비스에 더 중점을 둡니다.\n\n## 클러스터 토폴로지\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nHPC 클러스터를 구성하는 다양한 토폴로지가 있습니다. 각각은 특정 성능 기대치와 애플리케이션 요구 사항에 맞게 설계되어 있어요. 우리의 시나리오에서는 서브넷(10.0.0.x) 안에서 라우터와 스위치를 통해 세 대의 라즈베리 파이를 연결하는 데 초점을 맞춥니다. 라우터에 DHCP 서버를 설정하여 각 라즈베리 파이에 고유한 MAC 주소를 기반으로 고정 IP 주소를 할당했어요. 하지만 라즈베리 파이 자체에 정적 IP(192.168.0.x)를 직접 구성하여 간소화할 수도 있어요. 또한, 가정의 ISP 라우터를 통해 라즈베리 파이를 상호 연결하는 대신 Wi-Fi 액세스 포인트를 사용할 경우, 라우터와 이더넷 스위치가 필요 없어요. 빠른이나 안정적인 연결이 필요 없는 테스트 클러스터를 위한 대안으로 유용해요. 더불어 인터넷 연결은 라우터를 가정의 ISP Wi-Fi 액세스 포인트에 연결하고 장치들에게 공유함으로써 제공됩니다. 아래 다이어그램은 클러스터 네트워크의 시각적 표현을 제공해요:\n\n![cluster_network](/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_1.png)\n\n이 토폴로지는 휴대성의 장점을 제공하여 클러스터를 쉽게 다른 액세스 포인트에 연결할 수 있어요. 또한, 데이터 관리용으로 한 대의 노드를 사용하여 전용 네트워크 저장 서버로 활용했습니다. 나머지 두 대의 라즈베리 파이는 하나는 마스터 노드 및 계산 노드로 사용되고, 다른 하나는 추가 계산 노드로 사용됩니다.\n\n# 준비사항\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 하드웨어 구성 요소\n\n이번 프로젝트에서 다음과 같은 장비들을 사용했어요.\n\n- Raspberry Pi 4 Model B 2GB 보드 (호스트명 rpnode01)\n  이 장치는 마스터 노드 및 컴퓨팅 노드로 사용됩니다.\n- Raspberry Pi 4 Model B 2GB 보드 (호스트명 rpnode02)\n  이 장치는 2번째 컴퓨팅 노드로 작동합니다.\n- Raspberry Pi 3+ 1GB 보드 (호스트명 filenode01)\n  네트워크 저장 서버로 설정되어 있습니다.\n- USB 파워 허브\n  여러 Raspberry Pi의 전원 요구 사항을 동시에 지원할 수 있는 다중 포트 USB 충전기가 필요합니다 (각 Raspberry Pi 당 적어도 2A).\n- 라우터 및 이더넷 스위치 (선택 사양)\n  라우터는 외부 연결을 관리하고 스위치는 내부 장치 통신을 처리합니다.\n\n![이미지](/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_2.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 운영 체제\n\n저는 리소스가 제한된 시스템에 적합한 경량 운영 체제인 Debian bookworm (버전 12) OS Lite 64-bit를 사용했습니다. 이 OS는 라즈베리 파이와 같은 시스템에서 유용하게 사용할 수 있습니다. 다음 조정 사항은 모든 장치에 적용되도록 예상됩니다.\n\n- 사용자 이름이 \"pi\"이고 패스워드는 \"testpass\"인 더미 패스워드를 사용했습니다. 그러나 LDAP나 기타 인증 메커니즘을 활용하여 사용자 및 그룹 ID를 클러스터 전체에서 동기화하는 것이 가능합니다.\n- 노드에서 SSH 액세스를 활성화합니다. 노드에 액세스하기 위해 SSH 키 공유를 사용하는 것이 편리합니다.\n- CPU 및 메모리를 위한 제어 그룹을 활성화합니다. cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory swapaccount=1를 입력하여 /boot/firmware/cmdline.txt를 수정합니다. 이 변경 사항을 적용한 후 시스템을 다시 부팅합니다.\n- /etc/hosts에 다음 호스트 이름을 추가합니다.\n  10.0.0.1 rpnode01 rpnode01.home.local\n  10.0.0.2 rpnode02 rpnode02.home.local\n  10.0.0.3 filenode01 filenode01.home.local\n- (필요한 경우) 언어 및 지역 설정을 구성합니다.\n\n  $ sudo raspi-config\n\n- 마지막으로 시스템 패키지를 업데이트하고 업그레이드합니다.\n\n  $ sudo apt update \u0026\u0026 sudo apt upgrade\n\n![Raspberry Pi 클러스터 구축 스텝바이스텝 가이드](/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_3.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 저장 노드\n\nHPC 클러스터에서 연산 노드는 상태를 유지하지 않는(stateless) 설계로 되어 있습니다. 이는 연산 노드가 지속적인 데이터나 상태를 저장하지 않음을 의미합니다. 대신, 모든 응용 프로그램 소프트웨어와 사용자 데이터는 중앙 공유 저장소에 저장됩니다. 이 아키텍처는 여러 이점을 제공합니다. 먼저, 새로운 연산 노드를 간편하게 추가할 수 있어 확장성을 향상시킵니다. 데이터를 여러 기계에 복제할 필요 없이 간단히 새로운 연산 노드를 추가할 수 있습니다. 둘째, 이는 사용자들이 클러스터 내의 모든 연산 노드에서 응용프로그램과 데이터에 액세스할 수 있도록 유연성을 제공합니다. 셋째, 중앙 저장 노드에 모든 데이터를 저장하는 것은 데이터 무결성과 일관성을 보장하여 개별 연산 노드에 로컬로 데이터를 저장할 때 발생할 수 있는 불일치에 대한 우려를 제거합니다. 마지막으로, 상태를 유지하지 않는 연산 노드 아키텍처는 소프트웨어 업데이트, 하드웨어 교체, 문제 해결과 같은 유지 보수 작업을 간단하게 만들어줍니다. 개별 연산 노드에 로컬로 저장된 데이터를 전송하거나 백업할 필요가 없기 때문입니다.\n\n## NFS 서버\n\n전용 노드 filenode01에 NFS(Network File System) 서버를 설정했습니다. 이곳에서 네트워크 저장소를 사용할 수 있습니다. HPC 클러스터의 맥락에서 연산 노드를 상태를 유지하지 않는 것으로 언급할 때, 해당 연산 노드 자체가 지속적인 데이터를 저장하지 않음을 의미합니다. 사용자 홈 디렉토리 데이터와 응용프로그램 소프트웨어는 중앙 저장 노드에 저장됩니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n해당 작업을 위해 NFS 서버를 apt 패키지 관리자에 설치했습니다. 이 작업은 터미널에서 다음 명령을 실행하여 수행할 수 있습니다:\n\n```js\n$ sudo apt install nfs-kernel-server\n```\n\n그런 다음 /etc/exports 파일을 구성하여 NFS를 통해 공유하려는 디렉터리를 정의했습니다. 이 파일은 유닉스류 운영 체제에서 NFS 서버에 의해 사용되는 구성 파일입니다. 이 파일은 서버에서 NFS 클라이언트와 공유되는 디렉터리 및 해당 디렉터리에 대한 액세스 권한을 정의합니다. 이 디렉터리가 존재하는지 확인하고 다음 명령을 사용하여 항목을 추가했습니다:\n\n```js\n$ sudo mkdir -p /home /nfs\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n$ sudo bash -c \"cat \u003e\u003e /etc/exports \u003c\u003c EOF\n/home  *(rw,sync,no_root_squash,no_subtree_check)\n/nfs  *(rw,sync,no_root_squash,no_subtree_check)\nEOF\"\n```\n\n여기서 \\*을 사용하여 모든 노드에서 액세스를 허용하고 필요한 옵션을 지정했습니다. 예를 들어, rw는 읽기-쓰기 액세스를 나타냅니다. exports 파일을 편집한 후에는 변경 사항을 적용하려면 다음을 실행하세요:\n\n```js\n$ sudo exportfs -ra\n```\n\n노드에 방화벽이 활성화되어 있는 경우 NFS 포트를 열어야 할 수 있습니다. NFSv4는 TCP 및 UDP 포트 2049를 사용하며, NFSv3는 추가 포트를 사용합니다. 방화벽 구성에 따라 ufw 또는 iptables을 사용하여 이러한 포트를 열 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nNFS 공유가 제대로 설정되었는지 확인할 수 있습니다. showmount 명령어를 사용하면 해당 노드에서 내보낸 디렉토리 목록을 표시할 수 있습니다.\n\n```js\n$ sudo showmount -e\nExport list for filenode01:\n/home *\n/nfs *\n```\n\n## NFS 클라이언트\n\nNFS 클라이언트 노드인 rpnode01 및 rpnode02에 네트워크 저장소 액세스를 활성화하려면 /etc/fstab 파일을 수정하여 NFS 마운트 지점을 통합할 수 있습니다. 이 파일은 시스템 부팅 중 파일 시스템을 자동으로 마운트할 수 있게 하는 시스템 파일입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n클라이언트 측에 /home 및 /nfs 디렉토리가 존재하는지 확인한 후에 이 파일을 수정하기 전에 아래 명령어를 실행해 주세요:\n\n```js\n$ sudo mkdir -p /home /nfs\n```\n\n```js\n$ sudo bash -c \"cat \u003e\u003e /etc/fstab \u003c\u003c EOF\nfilenode01:/home /home nfs defaults 0 0\nfilenode01:/nfs /nfs nfs defaults 0 0\nEOF\"\n```\n\n/etc/fstab에 추가된 각 행은 고유한 NFS 마운트 지점을 정의합니다. 이는 시스템에게 NFS 서버의 호스트 이름이 filenode01인 /home (/nfs) 디렉토리를 로컬 /home (/nfs) 디렉토리에 마운트하도록 지시합니다. 이전에 언급했듯이, /home은 사용자 데이터로 지정되고, /nfs 디렉토리는 공유 소프트웨어 스택을 위해 의도되었습니다. 노드를 다시 부팅하세요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n업데이트된 /etc/fstab 파일의 정확성을 확인하기 위해 아래 명령인 mount -a를 사용합니다.\n\n```bash\n$ sudo mount -av\n/nfs    : 성공적으로 마운트됨\n/home    : 성공적으로 마운트됨\n```\n\n시스템이 시작될 때 /etc/fstab을 읽어 아직 마운트되지 않은 파일 시스템을 마운트합니다.\n\n# 마스터 노드\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Slurm 빌드 및 설치\n\n저는 항상 Slurm을 빠돌린 패키지 대신 소스 코드에서 컴파일하는 것을 선호합니다. 이렇게 하면 사용자 정의가 가능해지며, 최신 기능 및 수정 사항에 액세스할 수 있으며 교육적 가치를 제공합니다. 이곳에서 설명된 대로 Slurm 패키지를 소스 코드에서 빌드하고 설치 가능하도록 만드는 지침은 거의 동일하지만 약간의 변경을 적용했는데, 이어지는 내용에서 설명하겠습니다.\n\n- 필수 라이브러리\n\nSlurm 구성을 진행하기 전에 다음 라이브러리 또는 헤더 파일이 설치되어 있는지 확인하십시오. 이를 apt 패키지 관리자를 통해 쉽게 설치할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n$ sudo apt install libpmix-dev libpam-dev libmariadb-dev \\\n  libmunge-dev libdbus-1-dev munge\n```\n\n2. Making from source\n\n이제 소스 코드로부터 빌드해 보겠습니다.\n\nSchedMD Github 저장소에서 이 게시물 작성 시점인 가장 최근 버전인 Slurm 버전 23.11을 다운로드하겠습니다. 우리는 x86_64 대신 aarch64 아키텍처용으로 빌드할 것입니다.\n\n```js\n$ sudo mkdir /opt/slurm \u0026\u0026 cd /opt/slurm\n$ sudo wget https://github.com/SchedMD/slurm/archive/refs/tags/slurm-23-11-6-1.tar.gz\n$ sudo tar -xf slurm-23-11-6-1.tar.gz\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSlurm을 컴파일하는 데는 모든 것을 처음부터 만들기 때문에 몇 분 정도의 시간이 필요합니다.\n\n```js\n$ cd slurm-slurm-23-11-6-1\n$ sudo ./configure \\\n  --prefix=/opt/slurm/build \\\n  --sysconfdir=/etc/slurm \\\n  --enable-pam \\\n  --with-pam_dir=/lib/aarch64-linux-gnu/security/ \\\n  --without-shared-libslurm \\\n  --with-pmix\n$ sudo make\n$ sudo make contrib\n$ sudo make install\n```\n\n--prefix 옵션은 컴파일된 코드를 설치할 베이스 디렉토리를 지정합니다. /usr에 직접 설치하는 대신에 /opt/slurm/build로 설정했습니다. 이유는 Slurm을 추가 컴퓨팅 노드에 설치할 때 설치 가능한 패키지를 생성하는 데 활용하려는 것입니다.\n\n3. Debian 패키지 빌드하기.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nfpmtool을 사용하여 컴파일된 코드의 Debian 패키지를 만들었습니다. 이 작업을 위해서 추가 패키지를 설치해야 합니다.\n\n```js\n$ sudo apt ruby-dev\n$ sudo gem install fpm\n```\n\n이 도구는 slurm-23.11_1.0_arm64.deb 라는 패키지 파일을 생성할 것입니다. Slurm 23.11.0부터는 Debian 패키지를 빌드하는 데 필요한 파일이 Slurm에 포함되어 있음을 언급해 두겠습니다.\n\n```js\n$ sudo fpm -s dir -t deb -v 1.0 -n slurm-23.11 --prefix=/usr -C /opt/slurm/build .\nCreated package {:path=\u003e\"slurm-23.11_1.0_arm64.deb\"}\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n4. Debian 패키지 설치\n\n다음으로, dpkg 명령어를 사용하여 이 패키지를 설치합니다:\n\n```js\n$ sudo dpkg -i slurm-23.11_1.0_arm64.deb\nslurm-23.11_1.0_arm64.deb를 풀기 위한 준비를 합니다...\nslurm-23.11(1.0)을(를) (1.0) 위에 덮어씁니다...\nslurm-23.11(1.0)을(를) 설정합니다...\nman-db(2.11.2-2)에 대한 트리거를 처리 중...\n```\n\n또한, slurm 시스템 사용자를 생성하고 필요한 디렉토리를 올바른 액세스 권한으로 초기화해야 합니다. userslurm 사용자가 존재하고 클러스터 전체에서 사용자 ID가 동기화되어 있는지 확인해주세요. Slurm 컨트롤러가 사용하는 파일 및 디렉토리는 userslurm에 의해 읽거나 쓸 수 있어야 합니다. 또한, 로그 파일 디렉토리 /var/log/slurm과 상태 저장 디렉토리 /var/spool/slurm은 쓰기 권한이 있어야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n여기요, 사용자 slurm과 해당 그룹 ID를 151로 수정했습니다.\n\n```js\n$ sudo adduser --system --group --uid 151 slurm\n```\n\n또한 아래 명령어를 실행하여 예상 권한으로 필요한 디렉토리를 생성했습니다:\n\n```js\n$ sudo mkdir -p /etc/slurm /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm\n$ sudo chown slurm: /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Slurm 구성\n\n지금까지 모든 것이 순조롭게 진행되고 있어요. 지금까지 Slurm 패키지를 빌드하고 설치했습니다. 다음 단계는 각 요소를 구성하고 서비스로 실행하는 것입니다.\n\n- Slurm 데이터베이스 데몬\n\n우리는 Slurm 데이터베이스 데몬 (slurmdbd)을 설정하여 각 작업에 대한 상세한 회계 정보를 수집할 것입니다. 모든 회계 데이터는 데이터베이스에 저장됩니다. 먼저 마스터 노드에 데이터베이스 서버를 만들어야하는데, 이상적으로는 별도의 노드에 있어야 합니다. 저는 MySQL과 호환되는 오픈 소스 데이터베이스인 MariaDB를 선택했습니다. 다음 지침에 따라 데이터베이스 서버를 배포할 수 있어요.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```bash\n$ sudo apt install mariadb-server\n$ sudo mysql -u root\ncreate database slurm_acct_db;\ncreate user 'slurm'@'localhost';\nset password for 'slurm'@'localhost' = password('slurmdbpass');\ngrant usage on *.* to 'slurm'@'localhost';\ngrant all privileges on slurm_acct_db.* to 'slurm'@'localhost';\nflush privileges;\nexit\n```\n\n그 후에는 /etc/slurm/slurmdbd.conf를 만들고 인증, 데이터베이스 서버 호스트 이름, 로깅 등을 지정하는 등 필요한 구성을 추가해야 합니다. 아래 명령을 실행하여 구성 파일을 추가하세요.\n\n```bash\n$ sudo bash -c \"cat \u003e /etc/slurm/slurmdbd.conf \u003c\u003c EOF\n# 인증 정보\nAuthType=auth/munge\n\n# slurmDBD 정보\nDbdAddr=localhost\nDbdHost=localhost\nSlurmUser=slurm\nDebugLevel=3\nLogFile=/var/log/slurm/slurmdbd.log\nPidFile=/run/slurmdbd.pid\nPluginDir=/usr/lib/slurm\n\n# 데이터베이스 정보\nStorageType=accounting_storage/mysql\nStorageUser=slurm\nStoragePass=slurmdbpass\nStorageLoc=slurm_acct_db\nEOF\"\n```\n\n이 파일은 Slurm 데이터베이스 데몬 구성 정보를 설명합니다. slurmdbd가 실행되는 컴퓨터에만 존재해야 하며, slurm 사용자만 읽을 수 있어야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로, slurmdbd를 systemd 서비스로 설정해야 합니다. 이를 위해서는 /etc/systemd/system/slurmdbd.service 파일을 생성해야 합니다.\n\n```bash\n$ sudo bash -c \"cat \u003e /etc/systemd/system/slurmdbd.service \u003c\u003c EOF\n[Unit]\nDescription=Slurm DBD accounting daemon\nAfter=network.target munge.service\nConditionPathExists=/etc/slurm/slurmdbd.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmdbd\nExecStart=/usr/sbin/slurmdbd $SLURMDBD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/run/slurmdbd.pid\n\n[Install]\nWantedBy=multi-user.target\nEOF\"\n```\n\n이제 slurmdbd.service를 활성화하고 시작할 수 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n$ sudo systemctl enable slurmdbd.service\n$ sudo systemctl start slurmdbd.service\n```\n\n```js\n$ sudo systemctl | grep slurmdbd\n  slurmdbd.service      loaded active running   Slurm DBD accounting daemon\n```\n\n만약 모든 것이 잘 진행된다면, slurmdbd 서비스가 실행 중인 것을 확인할 수 있어요. 그렇지 않다면, /var/log/slurm/slurmdbd.log 파일이나 systemd 상태를 확인해주세요.\n\n2. Slurm 컨트롤러 데몬\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n슬러름 컨트롤러 데몬 (slurmctl)은 슬러름 활동을 조정하며 슬러름의 중앙 관리 데몬입니다. 다른 모든 슬러름 데몬 및 리소스를 모니터링하고 작업을 수락하며 해당 작업에 리소스를 할당합니다. 우리는 /etc/slurm/slurmd.conf 파일을 생성해야 합니다. 이 구성 파일은 슬러름이 리소스와 상호 작용하고 작업을 관리하며 다른 구성 요소와 통신하는 방식을 정의합니다. 이 파일에는 다양한 매개변수가 포함되어 있으며 클러스터의 각 노드에서 일관되게 사용 가능해야 합니다. 필요한 구성을 만들고 추가하려면 터미널에서 다음 명령어를 사용하세요.\n\n```js\n$ sudo bash -c \"cat \u003e /etc/slurm/slurm.conf \u003c\u003c EOF\nClusterName=raspi-hpc-cluster\nControlMachine=rpnode01\nSlurmUser=slurm\nAuthType=auth/munge\nStateSaveLocation=/var/spool/slurm/ctld\nSlurmdSpoolDir=/var/spool/slurm/d\nSwitchType=switch/none\nMpiDefault=pmi2\nSlurmctldPidFile=/run/slurmctld.pid\nSlurmdPidFile=/run/slurmd.pid\nProctrackType=proctrack/cgroup\nPluginDir=/usr/lib/slurm\nReturnToService=1\nTaskPlugin=task/cgroup\n\n# 스케줄링\nSchedulerType=sched/backfill\nSelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE\n\n# 로깅\nSlurmctldDebug=3\nSlurmctldLogFile=/var/log/slurm/slurmctld.log\nSlurmdDebug=3\nSlurmdLogFile=/var/log/slurm/slurmd.log\nJobCompType=jobcomp/none\n\n# 회계\nJobAcctGatherType=jobacct_gather/cgroup\nAccountingStorageTRES=gres/gpu\nDebugFlags=CPU_Bind,gres\nAccountingStorageType=accounting_storage/slurmdbd\nAccountingStorageHost=localhost\nAccountingStoragePass=/run/munge/munge.socket.2\nAccountingStorageUser=slurm\nAccountingStorageEnforce=limits\n\n# 컴퓨팅 노드\nNodeName=rpnode01 CPUs=4 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=1800 State=idle\nNodeName=rpnode02 CPUs=4 Sockets=1 CoresPerSocket=4 ThreadsPerCore=1 RealMemory=1800 State=idle\n\n# 파티션\nPartitionName=batch Nodes=rpnode[01-02] Default=YES State=UP DefaultTime=1-00:00:00 DefMemPerCPU=200 MaxTime=30-00:00:00 DefCpuPerGPU=1\nEOF\"\n```\n\n그리고 /etc/system/systemd/slurmctld.service를 시스템 데몬으로 실행하도록 만들어야 합니다. 다음 명령어를 실행하여 다음 내용을 추가할 수 있습니다.\n\n```js\n$ sudo bash -c \"cat \u003e /etc/systemd/system/slurmctld.service \u003c\u003c EOF\n[Unit]\nDescription=Slurm controller daemon\nAfter=network.target munge.service\nConditionPathExists=/etc/slurm/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmctld\nExecStart=/usr/sbin/slurmctld $SLURMCTLD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/run/slurmctld.pid\n\n[Install]\nWantedBy=multi-user.target\nEOF\"\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 slurmctld.service를 활성화하고 시작할 준비가 되었습니다:\n\n```bash\n$ sudo systemctl enable slurmctld.service\n$ sudo systemctl start slurmctld.service\n```\n\n```bash\n$ sudo systemctl | grep slurmctld\nslurmctld.service       loaded active running   Slurm controller daemon\n```\n\n3. Slurm 노드 데몬\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마스터 노드를 컴퓨트 노드로도 사용하려면 Slurm의 컴퓨트 노드 데몬인 slurmd를 설정해야 합니다. slurmd 데몬은 모든 컴퓨트 노드에서 실행되어야 합니다. 이는 컴퓨트 노드에서 실행 중인 모든 작업을 모니터링하고 작업을 수락하며 실행 및 중지 요청 시 실행 중인 작업을 중지합니다. 이 데몬은 slurmd.conf와 함께 cgroup.conf 및 cgroup_allowd_devices_file.conf 두 가지 추가 파일을 함께 읽습니다. 다음 명령어를 사용하여 두 개의 필요한 제어 그룹 (cgroup) 파일을 만드세요:\n\n```js\n$ sudo bash -c \"cat \u003e /etc/slurm/cgroup.conf \u003c\u003c EOF\nConstrainCores=yes\nConstrainDevices=yes\nConstrainRAMSpace=yes\nEOF\"\n```\n\n```js\n$ sudo bash -c \"cat \u003e /etc/slurm/cgroup_allowed_devices_file.conf \u003c\u003c EOF\n/dev/null\n/dev/urandom\n/dev/zero\n/dev/sda*\n/dev/cpu/*/*\n/dev/pts/*\n/dev/nvidia*\nEOF\"\n```\n\n그런 다음, systemd 서비스로서 slurmd를 실행하기 위해 systemd.service 파일을 다시 만들어야 합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```bash\n$ sudo bash -c \"cat \u003e /etc/systemd/system/slurmd.service \u003c\u003c EOF\n[Unit]\nDescription=Slurm 노드 데몬\nAfter=network.target munge.service\nConditionPathExists=/etc/slurm/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/run/slurmd.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\nEOF\"\n```\n\n마지막으로 아래 명령어를 사용하여 slurmd 서비스를 활성화하고 시작합니다:\n\n```bash\n$ sudo systemctl enable slurmd.service\n$ sudo systemctl start slurmd.service\n```\n\n```bash\n$ sudo systemctl | grep slurmd\n  slurmd.service       loaded active running   Slurm 노드 데몬\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 모든 설정이 완료되었으므로 Slurmsinfo 명령어를 사용하여 Slurm 노드 및 파티션에 대한 정보를 확인할 수 있습니다:\n\n```js\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nbatch*       up 30-00:00:0      1   idle rpnode01\n```\n\n이 출력을 보면 Slurm이 성공적으로 설치되고 구성된 것입니다. 수고하셨습니다!\n\n# Slurm 회계\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSlurm은 실행된 모든 작업과 작업 단계에 대한 회계 정보를 수집합니다. 또한 데이터베이스로 직접 회계 기록을 지원합니다. 테스트 목적으로, Slurm 데이터베이스에서 클러스터 \"raspi-hpc-cluster\"와 계정 \"compute\"를 다음과 같이 정의합니다.\n\n```js\n$ sudo sacctmgr add cluster raspi-hpc-cluster\n$ sudo sacctmgr add account compute description=\"Compute account\" Organization=home\n```\n\n```js\n$ sudo sacctmgr show account\n   Account                Descr                  Org\n---------- -------------------- --------------------\n   compute      Compute account                home\n      root default root account                root\n```\n\n그리고 사용자 pi를 일반 Slurm 계정으로 연결합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```bash\n$ sudo sacctmgr add user pi account=compute\n$ sudo sacctmgr modify user pi set GrpTRES=cpu=4,mem=1gb\n$ sudo sacctmgr show user\n      User   Def Acct     Admin\n---------- ---------- ---------\n        pi    compute      None\n      root       root Administ+\n\n```\n\n모든 것이 순조롭게 진행되었다면, 단일 노드 Slurm 클러스터가 작업 제출을 위해 준비된 상태여야 합니다. 이제 사용 가능한 노드의 현재 상태에 대한 정보를 먼저 표시해보겠습니다.\n\n```bash\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nbatch*       up 30-00:00:0      1   idle rpnode[01]\n```\n\n이제 간단한 Slurm srun 명령어를 실행하고 출력을 확인해봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n$ srun hostname\nrpnode01\n\n이것은 우리의 작업이 Slurm 작업으로 성공적으로 실행되고 컴퓨팅 노드의 호스트 이름인 rpnode01을 반환했음을 나타냅니다.\n\n# 컴퓨팅 노드\n\n추가 컴퓨팅 노드를 포함하는 Slurm 클러스터를 확장하는 것은 다음의 주요 단계를 포함합니다:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 필요한 라이브러리와 헤더 파일 설치하기\n- 마스터 노드에서 /etc/munge/munge.key를 컴퓨트 노드로 복사하고, 소유자를 munge 사용자로 변경한 후 munge.service를 다시 시작하기\n- slurm-23.11_1.0_arm64.deb를 설치하기\n- slurm 사용자 및 필요한 Slurm 디렉토리 만들기\n- slurm.conf, cgroup.conf 및 cgroup_allowed_devices_file.conf 파일을 /etc/slurm/으로 복사하기\n- slurmd.service를 활성화하고 시작하기\n\n테스트\n\n새로운 노드 newpnode02의 상태를 idle로 업데이트하려면 먼저 다음을 실행해보세요\n\n```js\n$ scontrol update nodename=rpnode02 state=idle\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n$ sinfo\n| PARTITION | AVAIL | TIMELIMIT  | NODES | STATE | NODELIST |\n|-----------|-------|------------|-------|-------|----------|\n| batch*    | up    | 30-00:00:00| 2     | idle  | rpnode[01-02] |\n```\n\nAnd again run the hostname job on the new node using\n\n```js\n$ srun -w rpnode02 hostname\nrpnode02\n```\n\nAs you can see, this job was executed on the second compute node, so it returns the hostname rpnode02 this time.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 예시\n\n## 클러스터 정보\n\nSlurm에서의 파티션은 클러스터를 논리적 노드 그룹으로 나누는 방법으로, 자원을 효율적으로 관리하고 할당할 수 있도록 도와줍니다. 여기서는 Slurm에서 구성된 배치 파티션에 대한 상세 정보를 표시합니다.\n\n```js\n$ scontrol show partition\nPartitionName=batch\n   AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL\n   AllocNodes=ALL Default=YES QoS=N/A\n   DefaultTime=1-00:00:00 DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO\n   MaxNodes=UNLIMITED MaxTime=30-00:00:00 MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED MaxCPUsPerSocket=UNLIMITED\n   Nodes=rpnode[01-02]\n   PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO\n   OverTimeLimit=NONE PreemptMode=OFF\n   State=UP TotalCPUs=8 TotalNodes=2 SelectTypeParameters=NONE\n   JobDefaults=DefCpuPerGPU=1\n   DefMemPerCPU=200 MaxMemPerNode=UNLIMITED\n   TRES=cpu=8,mem=3600M,node=2,billing=8\n```\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그리고 rpndoe01 상태를 보여주는 중입니다\n\n```js\n$ scontrol show nodes\nNodeName=rpnode01 Arch=aarch64 CoresPerSocket=4\n   CPUAlloc=0 CPUEfctv=4 CPUTot=4 CPULoad=0.01\n   AvailableFeatures=(null)\n   ActiveFeatures=(null)\n   Gres=(null)\n   NodeAddr=rpnode01 NodeHostName=rpnode01 Version=23.11.6\n   OS=Linux 6.6.28+rpt-rpi-v8 #1 SMP PREEMPT Debian 1:6.6.28-1+rpt1 (2024-04-22)\n   RealMemory=1800 AllocMem=0 FreeMem=297 Sockets=1 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A\n   Partitions=batch\n   BootTime=2024-05-19T13:58:15 SlurmdStartTime=2024-05-19T14:20:03\n   LastBusyTime=2024-05-19T14:26:11 ResumeAfterTime=None\n   CfgTRES=cpu=4,mem=1800M,billing=4\n   AllocTRES=\n   CapWatts=n/a\n   CurrentWatts=0 AveWatts=0\n   ExtSensorsJoules=n/a ExtSensorsWatts=0 ExtSensorsTemp=n/a\n```\n\n## 작업 제출\n\n홈 디렉토리에 간단한 Slurm 배치 파일을 만들어봅시다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n$ cat \u003e ~/submit.sh \u003c\u003c EOF\n#!/usr/bin/sh\n\n#SBATCH --job-name=testjob\n#SBATCH --mem=10mb\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --time=00:01:00\n\nsrun sleep 10\nEOF\n```\n\n이제 우리는 이 작업을 제출할 수 있어요\n\n```js\ncd ~/\n$ sbatch submit.sh\n제출된 작업 8\n```\n\n제출된 작업의 상태를 기본 배치 대기열에서 확인할 수 있어요\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n$ squeue -al\nSun May 19 14:26:03 2024\nJOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n    8     batch  testjob       pi  RUNNING       0:03      1:00      1 rpnode01\n```\n\n이 작업은 rpnode01에서 실행되고 1분의 시간 제한이 있습니다.\n\n# 다음 단계\n\n이전 글에서 언급했듯이, 이 HPC 클러스터는 테스트 환경으로 사용됩니다. 현재 기본 Slurm 및 중앙 저장 기능을 갖추고 있지만, 향후 확장 및 향상이 가능합니다. 사용자 계정, 디스크 할당량 관리, 환경 모듈 및 Conda 패키지 매니저를 활용한 소프트웨어 스택 설정, MPI 구현, Jupyterhub 서비스 설정 등을 다루는 후속 글을 쓸 계획이 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n제가 몇 년 전에 작성한 HPC 클러스터 설정 안내서를 위해 만든 GitHub 저장소를 자유롭게 확인해보세요. 이 저장소에서는 언급된 HPC 기능의 설정 프로세스를 다루고 있습니다. 정보가 다소 오래되었을 수 있고 현재 버전과 호환되도록 몇 가지 조정이 필요할 수 있습니다.\n\n이 게시물이 좋은 시작점이 되었으면 좋겠습니다. 읽어 주셔서 감사합니다!\n","ogImage":{"url":"/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_0.png"},"coverImage":"/assets/img/2024-05-20-BuildingSlurmHPCClusterwithRaspberryPisStep-by-StepGuide_0.png","tag":["Tech"],"readingTime":30},{"title":"인텔 N100 알더 레이크를 활용한 예산 친화적 미니 홈 서버 구축하기","description":"","date":"2024-05-20 19:49","slug":"2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake","content":"\n하이퍼링크를 Markdown 형식으로 변경하십시오.\n\n[Build a Budget-Friendly Mini Home Server with Intel N100 Alder Lake](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_0.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Building a Budget-Friendly Mini Home Server with Intel N100 Alder Lake](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_1.png)\n\n그러나 리눅스 배포판을 탐색하고 Windows에서 특정 응용 프로그램을 실행하는 과정에서 라즈베리 파이의 제한 사항을 만나게 되었습니다. 또한 해당 장치는 Plex를 재색인하거나 NextCloud에 저장된 수천 개의 파일을 찾아보는 등 무거운 작업을 동시에 처리하는 데 필요한 전원이 부족합니다.\n\n최근 라즈베리 파이 5가 출시된 후, 라즈베리 파이를 기반으로 한 새로운 서버를 소유하고 운영할 열정이 사라지고 있습니다. 새로운 장치가 기술적 한계에 직면하면서도 더 높은 가격표를 갖게 되었습니다. 다양한 하드웨어 옵션을 탐색한 후, 최근 출시된 Intel Alder Lake N100이 매력적인 선택이라는 것을 발견했습니다:\n\n- 전체 미니 하드웨어 세트가 약 150달러 정도로 저렴하게 제공됩니다. 이는 전체 라즈베리 파이 5 세트의 가격과 비슷합니다.\n- N100 CPU는 상대적으로 낮은 6W의 TDP로 운영하면서 라즈베리 파이 5보다 훨씬 더 많은 성능을 제공합니다. 또한 Intel x86-64 아키텍처는 iGPU 통과 기능을 지원하는 VM을 지원합니다.\n- N100 칩은 최대 32GB의 DDR5 RAM을 지원하며, 라즈베리 파이 5의 8GB DDR4 제한을 크게 넘어섭니다.\n- 일부 N100 세트는 2.5G로 클럭된 듀얼 LAN 포트를 제공하여 빠른 파일 전송과 연결 중복성을 제공합니다.\n- N100은 주로 NVMe Gen 3+ 스토리지를 완벽하게 지원하는 마더보드와 번들로 제공되며, 라즈베리 파이 5의 SD 카드, USB 3 또는 PCIe 솔루션에 비해 훨씬 높은 속도의 읽기/쓰기 속도를 제공합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_2.png\" /\u003e\n\n## 하드웨어 옵션\n\nIntel N100 설정에 대한 여러 예산 친화적인 옵션이 있습니다. 아마존에서 이 미니 컴퓨터와 같은 제품이 제공됩니다.\n\n또는 일부 사용자는 미니 ITX 마더보드에 N100 설정을 구축하고 RAID 구성을 위한 충분한 저장소를 추가할 수도 있습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저에게 있어 간단함이 가장 중요했어요. 전 최소한의 설정만 선택하고 블랙 프라이데이 기간에 SSD와 RAM을 별도로 구매해 좋은 혜택을 누렸어요.\n\n![이미지](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_3.png)\n\n이 장치는 이동하기 매우 쉽고, 발열이 적고, 소음이 없는 선풍기가 있어요. 보통 조건에서 전력 소비가 10W 미만으로 유지돼요.\n\n![이미지](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_4.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 운영 체제 옵션\n\n많은 보통 사용자들에게 미니 데스크톱을 사용하는 주요 목적은 외부 4K 모니터에 연결하면서 Windows 10 또는 11을 사용하여 일상 업무를 처리하는 것입니다. 그러나 제 경우에는 제 미니 데스크톱을 가상 머신을 여러 대 실행할 수 있는 헤드리스 홈 서버로 변신하겠다는 선택을 했습니다.\n\n저는 Proxmox, 일반적인 Debian/Ubuntu 설정에 VM 응용 프로그램을 적용하거나 TrueNAS 또는 Unraid와 같은 전용 NAS/홈 서버 운영 체제를 실행하는 여러 시나리오를 탐구했습니다.\n\n각 시나리오는 고유한 학습 곡선을 제시하여 하드웨어 제약 사항을 이해하고 최적의 VM 성능을 위해 패스스루를 설정하는 데 상당한 시간을 요구했습니다. 특히 하드웨어 렌더링 및 디코딩과 같은 그래픽 집약적 작업에 대한 최적의 성능을 위해.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Unraid을 선택했어요\n\n많은 실무 경험과 탐험 끝에, 저는 결국 Unraid를 선택했습니다. 이 플랫폼은 훌륭한 생태계와 45% 할인을 제공하여 경제적인 솔루션으로 제공되는 무어스럽고 매끈한 경험을 제공했습니다. 하드웨어 구성이 약 2개의 VM과 약 열 개의 컨테이너만 지원하는 경우, Unraid의 기능은 제 요구에 완벽하게 부합했어요.\n\n다른 사용자들이 무료이면서 오픈 소스 옵션을 찾는다면, TrueNAS도 좋은 선택이 될 거예요.\n\n![이미지](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_5.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n새 서버를 설정하는 것은 소프트웨어 설치, 백업 복원 및 예약된 작업 설정으로 인해 시간이 걸릴 수 있습니다.\n\n## 내 미니 홈 서버 조직하는 방법\n\n새 서버에서는 모든 응용 프로그램을 Docker 컨테이너로 완전히 실행하고 대부분 VM 내에서 실행하기로 결정했습니다. Unraid는 이 기능을 제공하지만 Proxmox만큼 VM에 최적화되어 있지 않습니다.\n\n하드웨어의 성능에도 불구하고, NextCloud 및 Plex/Jellyfin과 같은 무거운 작업을 호스트에서 직접 실행하는 것이 더 효율적인 것으로 판단했습니다. 나머지 응용 프로그램의 경우, 가벼운 Linux VM 상에서 Docker 컨테이너로 실행하며 특히 Headless Debian에서 실행하여 간소화되고 효율적인 설정을 보장합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nUnraid OS에 대해 배울 수 있는 귀중한 자원은 Spaceinvader One YouTube 채널입니다. 기본 설정부터 고급 기능까지 포괄적인 자습서를 제공하여 모든 능력 수준의 사용자에게 훌륭한 자원이 됩니다.\n\n## 지금까지 배운 것\n\nUnraid는 주로 NAS (Network Attached Storage) 서버용으로 설계되어 있으며, RAID 구성, 캐시 읽기/쓰기 및 네트워크 파일 공유를 지원합니다. Slackware 기반으로 만들어진 Unraid는 특정 명령줄 인터페이스 (CLI) 기능에 대한 액세스를 제한하여 우연한 시스템 변경의 위험을 최소화함으로써 사용자 경험을 간소화합니다. 그러나 사용자는 플러그인을 설치하여 추가 기능을 해제할 수 있습니다.\n\nUnraid는 앱 스토어에서 사용 가능한 다양한 응용 프로그램을 자랑하며, 이 중 많은 것들이 Docker 컨테이너입니다. 이러한 컨테이너는 운영 체제를 수정할 필요 없이 쉽게 배포하고 실행할 수 있어 Unraid의 서버 관리에 대한 사용자 친화적인 접근 방식을 더욱 강화합니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_6.png\" /\u003e\n\n## 안전하고 빠른 홈 서버에 대한 내 개인 팁\n\n소프트웨어 측면에 대해 심층적으로 다루지는 않겠습니다. 이미 전에 라즈베리 파이에 대한 이야기를 다뤘기 때문입니다. 그러나 새로운 홈 서버에서 배운 중요한 세 가지 교훈을 강조하고 싶습니다.\n\n1. 클라우드플레어 제로 트러스트 워프 앱을 통한 원격 접속: 클라우드플레어 제로 트러스트 워프 앱을 클라우드플레어 터널과 호스트 또는 가상 머신에 설치하여 전 세계 어디에서나 안전하게 홈 서버에 원격 액세스할 수 있습니다. 이 설정은 무료뿐만 아니라 설정도 매우 쉽습니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n2. 홈 IP로 VM 활용하기: VPN을 통해 내 집 네트워크에 연결하여, VM에 쉽게 액세스할 수 있습니다. 여행 중에도 귀찮음 없이 집 IP 주소를 사용하여 다양한 작업을 안전하게 수행할 수 있어서 매우 유용합니다. 끊김 없는 액세스를 보장하기 위해 여분의 VM 몇 대를 항상 준비해 둡니다. 놀랍게도, 제 작은 집 서버는 MacOS를 포함한 다양한 운영 체제를 지원합니다.\n\n![이미지](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_7.png)\n\n3. 최적화된 원격 데스크톱 경험: 더 부드러운 사용자 경험을 위해 Linux VM에는 NoMachine을, Windows VM에는 Microsoft Remote Desktop을 사용하는 것을 추천합니다. 하드웨어 패스스루 기능을 활용하여, Intel iGPU를 성공적으로 설치하여 Windows 10 Tiny 버전에서 가벼운 게임을 즐기고 4K YouTube 동영상을 원활하게 재생할 수 있었습니다.\n\n![이미지](/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_8.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 마무리\n\n내 새로운 홈 서버를 설정하는 과정은 몇 주 동안의 여정이었지만, VM, 자원 공유 및 여러 리눅스 서버 운영 체제에 대한 이해에서 큰 발전이 이루어졌습니다. 이 과정을 통해 서버 관리에 대한 소중한 통찰과 지식을 얻었습니다.\n\n나의 경험을 공유함으로써 예산을 고려한 강력한 미니 홈 서버를 VM 기능과 함께 구축하려는 다른 사람들에게 도움이 되었으면 좋겠습니다. 본 안내서가 프로그래밍과 셀프 호스팅의 영역으로 나아가려는 사람들에게 발판이 되기를 바랍니다.\n","ogImage":{"url":"/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_0.png"},"coverImage":"/assets/img/2024-05-20-BuildingaBudget-FriendlyMiniHomeServerwithIntelN100AlderLake_0.png","tag":["Tech"],"readingTime":8},{"title":"Raspberry Pi Pico를 MicroPython으로 사용하여 MPU-6050을 어떻게 사용하는지 알아보기","description":"","date":"2024-05-20 19:47","slug":"2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython","content":"\n2023년 여름에 MicroPython을 사용하여 Raspberry Pi Pico에서 실행되는 고유의 쿼드콥터 비행 컨트롤러를 처음부터 개발했어요. 이 프로젝트의 일환으로, 내 비행 컨트롤러는 기체에 탑재된 IMU(관성 측정 장치)에서 지속적으로 텔레메트리 데이터를 수집해야 했어요.\n\n저는 MicroPython에서 MPU-6050 드라이버를 작성해 MPU-6050이라는 저렴한 3축 가속도계 및 자이로스코프를 I2C 프로토콜을 사용해 연결할 수 있도록 했어요. 제가 언급한 쿼드콥터 비행 컨트롤러 프로젝트를 위해 이를 작성했지만 이는 어떤 응용프로그램에서든 사용할 수 있기 때문에 해당 코드를 오픈소스로 공유하고 있어요.\n\n온라인에서 찾을 수 있는 다른 드라이버들과 달리, 제 드라이버는 .py 파일 하나(한 모듈)뿐이며 매우 직관적으로 읽고 수정하고 사용할 수 있도록 설계되었으며 MPU-6050 텔레메트리 데이터를 네이티브 파이썬 값 유형(tuples)으로만 제공해요.\n\n라즈베리 파이 Pico에서 MicroPython을 사용해 MPU-6050에서 텔레메트리를 수집하기 시작하는 기본 단계를 설명할게요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 필요한 하드웨어\n\n기본 예제를 위해 준비해야 할 것:\n\n- 라즈베리 파이 Pico (Pico W도 가능)\n- MPU-6050\n- 브레드보드\n- 수초 메스 투 수초 점퍼 와이어 4개\n- 컴퓨터에 연결할 USB\n\n# 단계 1: 배선\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython_0.png\" /\u003e\n\n- Raspberry Pi Pico를 브레드보드에 꽂습니다.\n- MPU-6050을 Raspberry Pi Pico 바로 아래에 브레드보드에 꽂습니다.\n- MPU-6050의 VCC 핀을 Raspberry Pi Pico의 VBUS 핀(핀 40)에 연결합니다.\n- MPU-6050의 GND 핀을 Raspberry Pi Pico의 GND 핀 중 하나에 연결합니다(여기서는 핀 38을 사용하였습니다).\n- MPU-6050의 SCL 핀을 Raspberry Pi Pico의 GP15 핀(핀 20)에 연결합니다.\n- MPU-6050의 SDA 핀을 Raspberry Pi Pico의 GP14 핀(핀 19)에 연결합니다.\n\n# 단계 2: MPU6050.py 모듈을 Pico에 업로드합니다.\n\n아래는 MPU-6050을 위한 MicroPython 드라이버입니다.\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 코드 스니펫을 사용할 수 있지만이 코드의 업데이트는 내 GitHub의 MicroPython-Collection 저장소에서 확인할 수 있습니다.\n\n라즈베리 파이 Pico에 이 코드 스니펫을 업로드하려면 아래 단계를 따라하세요:\n\n- 위의 스니펫에서 파일을 다운로드하고 MPU6050.py로 컴퓨터에 저장합니다.\n- 이미 MicroPython이 설치된 라즈베리 파이 Pico를 USB 케이블로 컴퓨터에 연결하세요. Thonny를 엽니다.\n- 단계 1에서 다운로드한 MPU6050.py 파일을 파일 창에서 찾아 위쪽 오른쪽에 있는 파일 창에서 해당 파일을 찾아 마우스 오른쪽 버튼을 클릭한 후 /로 업로드하세요:\n\n![image](/assets/img/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython_1.png)\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n4. 라즈베리 파이 Pico에 MPU6050.py 파일을 업로드한 후, 이제 Pico 디렉토리에 파일이 나타나는 것을 확인할 수 있습니다:\n\n![MPU6050.py](/assets/img/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython_2.png)\n\n# 단계 3: 텔레메트리 캡처!\n\nMPU6050.py 모듈을 Pico로 로드했으므로 이제 MPU-6050에서 텔레메트리 수집을 시작할 준비가 되었습니다! Thonny에서 새 코드 파일을 열려면 창의 왼쪽 상단에있는 새로 만들기 버튼을 클릭하십시오. 다음 테스트 코드를 붙여넣으세요:\n\n\u003c!-- ui-station 사각형 --\u003e\n\n\u003cins class=\"adsbygoogle\"\nstyle=\"display:block\"\ndata-ad-client=\"ca-pub-4877378276818686\"\ndata-ad-slot=\"7249294152\"\ndata-ad-format=\"auto\"\ndata-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음으로, 왼쪽 상단의 녹색 '현재 스크립트 실행' 버튼을 클릭하고 라즈베리 파이 Pico가 I2C를 사용하여 MPU-6050에서 텔레메트리를 읽기 시작하는 것을 관찰해보세요!\n","ogImage":{"url":"/assets/img/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython_0.png"},"coverImage":"/assets/img/2024-05-20-HowtouseanMPU-6050withaRaspberryPiPicousingMicroPython_0.png","tag":["Tech"],"readingTime":4}],"page":"72","totalPageCount":110,"totalPageGroupCount":6,"lastPageGroup":20,"currentPageGroup":3},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"72"},"buildId":"bb_yO9GbCvdfc_n71SfUf","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>