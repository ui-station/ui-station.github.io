<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>ui-station</title><meta name="description" content="I develop websites, games and apps with HTML, CSS and JS."/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://ui-station.github.io///posts/3" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="ui-station" data-gatsby-head="true"/><meta property="og:title" content="ui-station" data-gatsby-head="true"/><meta property="og:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta property="og:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://ui-station.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://ui-station.github.io///posts/3" data-gatsby-head="true"/><meta name="twitter:title" content="ui-station" data-gatsby-head="true"/><meta name="twitter:description" content="I develop websites, games and apps with HTML, CSS and JS." data-gatsby-head="true"/><meta name="twitter:image" content="/favicons/ms-icon-310x310.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | ui-station" data-gatsby-head="true"/><meta name="next-head-count" content="18"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/assets/favicons/favicon-96x96.png"/><link rel="icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/favicons/browserconfig.xml"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V5DKFTZ6BX"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
          
            gtag('config', 'G-V5DKFTZ6BX');</script><link rel="preload" href="/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/_next/static/css/baeec1f16d6ea8b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/baeec1f16d6ea8b8.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee6df16fdc6dae4d.js" defer=""></script><script src="/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/_next/static/chunks/main-cf4a52eec9a970a0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/_next/static/chunks/348-d11c34b645b13f5b.js" defer=""></script><script src="/_next/static/chunks/873-a9851699c2b6bcaf.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpage%5D-498da29379dd58dc.js" defer=""></script><script src="/_next/static/R1x9p1CQYDDJESXyLXKOK/_buildManifest.js" defer=""></script><script src="/_next/static/R1x9p1CQYDDJESXyLXKOK/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="posts_container__s9Z_H posts_-list__bsl0U"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/">UI STATION</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/posts/1">Posts</a></nav></div></header><div class="posts_inner__HIBjT"><article><h2 class="SectionTitle_section_title__HS_xr">Posts</h2><div class="posts_project_list__oDV_y"><div class="PostList_post_list__or0rl"><a class="PostList_post_item__gAdVi" aria-label="LLM이 정말 새로운 것을 배울 수 있을까요" href="/post/2024-05-23-CanaLLMReallyLearnNewThings"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM이 정말 새로운 것을 배울 수 있을까요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM이 정말 새로운 것을 배울 수 있을까요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">LLM이 정말 새로운 것을 배울 수 있을까요</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="개발자들도 예전에는 사람이었어요" href="/post/2024-05-23-DevelopersUsedToBePeople"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="개발자들도 예전에는 사람이었어요" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DevelopersUsedToBePeople_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="개발자들도 예전에는 사람이었어요" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">개발자들도 예전에는 사람이었어요</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">6<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="알알지 탐정 웹사이트 데이터를 활용한 검색 증대형 생성" href="/post/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="알알지 탐정 웹사이트 데이터를 활용한 검색 증대형 생성" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="알알지 탐정 웹사이트 데이터를 활용한 검색 증대형 생성" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">알알지 탐정 웹사이트 데이터를 활용한 검색 증대형 생성</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">13<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="업리프트 모델링 상관관계에서 인과관계로" href="/post/2024-05-23-UpliftModelingcorrelationtocausation"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="업리프트 모델링 상관관계에서 인과관계로" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-UpliftModelingcorrelationtocausation_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="업리프트 모델링 상관관계에서 인과관계로" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">업리프트 모델링 상관관계에서 인과관계로</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">7<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="K-평균 클러스터링 마스터하기" href="/post/2024-05-23-MasteringK-MeansClustering"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="K-평균 클러스터링 마스터하기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-MasteringK-MeansClustering_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="K-평균 클러스터링 마스터하기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">K-평균 클러스터링 마스터하기</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">9<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM 애플리케이션 개발 평가 파트 8" href="/post/2024-05-23-BuildingLLMApplicationsEvaluationPart8"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM 애플리케이션 개발 평가 파트 8" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM 애플리케이션 개발 평가 파트 8" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">LLM 애플리케이션 개발 평가 파트 8</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">56<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="전자 상거래에서 테마 기반 분류와 GenAI를 활용한 패션 발견 혁신" href="/post/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="전자 상거래에서 테마 기반 분류와 GenAI를 활용한 패션 발견 혁신" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="전자 상거래에서 테마 기반 분류와 GenAI를 활용한 패션 발견 혁신" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">전자 상거래에서 테마 기반 분류와 GenAI를 활용한 패션 발견 혁신</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">8<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="LLM이 코딩에 적합하지 않은 이유  2부" href="/post/2024-05-23-WhyLLMsarenotGoodforCodingPartII"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="LLM이 코딩에 적합하지 않은 이유  2부" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-WhyLLMsarenotGoodforCodingPartII_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="LLM이 코딩에 적합하지 않은 이유  2부" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">LLM이 코딩에 적합하지 않은 이유  2부</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">5<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="PDF 파싱 해부 02 파이프라인 기반 방법" href="/post/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="PDF 파싱 해부 02 파이프라인 기반 방법" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="PDF 파싱 해부 02 파이프라인 기반 방법" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">PDF 파싱 해부 02 파이프라인 기반 방법</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">28<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a><a class="PostList_post_item__gAdVi" aria-label="위대한 AI 사기" href="/post/2024-05-23-TheGreatAIQuackery"><div class="PostList_thumbnail_wrap__YuxdB"><img alt="위대한 AI 사기" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="PostList_thumbnail__6_oQk" style="color:transparent" src="/assets/img/2024-05-23-TheGreatAIQuackery_0.png"/></div><div class="PostList_text_area__Hzd11"><div class="PostList_profile_area___aTjn"><div class="PostList_profile_image_wrap__tCTuE"><img alt="위대한 AI 사기" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="PostList_profile__VGF_a" style="color:transparent" src="/favicons/apple-icon-114x114.png"/></div><span class="writer">UI STATION</span></div><strong class="PostList_title__loLkl">위대한 AI 사기</strong><div class="PostList_meta__VCFLX"><span class="date">May 23, 2024</span><span class="PostList_reading_time__6CBMQ">2<!-- --> min read</span><span class="PostList_bookmark__PCpOK"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" aria-hidden="true"><path d="M6.75 4.5h10.5a.75.75 0 01.75.75v14.357a.375.375 0 01-.575.318L12 16.523l-5.426 3.401A.375.375 0 016 19.607V5.25a.75.75 0 01.75-.75zM16.5 6h-9v11.574l4.5-2.82 4.5 2.82V6z"></path></svg></span></div></div></a></div></div></article><div class="posts_pagination__R_03T"><a class="link" href="/posts/1">1</a><a class="link" href="/posts/2">2</a><a class="link posts_-active__YVJEi" href="/posts/3">3</a><a class="link" href="/posts/4">4</a><a class="link" href="/posts/5">5</a><a class="link" href="/posts/6">6</a><a class="link" href="/posts/7">7</a><a class="link" href="/posts/8">8</a><a class="link" href="/posts/9">9</a><a class="link" href="/posts/10">10</a><a class="link" href="/posts/11">11</a><a class="link" href="/posts/12">12</a><a class="link" href="/posts/13">13</a><a class="link" href="/posts/14">14</a><a class="link" href="/posts/15">15</a><a class="link" href="/posts/16">16</a><a class="link" href="/posts/17">17</a><a class="link" href="/posts/18">18</a><a class="link" href="/posts/19">19</a><a class="link" href="/posts/20">20</a><button type="button" class="page_button -prev">&gt;</button></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"LLM이 정말 새로운 것을 배울 수 있을까요","description":"","date":"2024-05-23 18:09","slug":"2024-05-23-CanaLLMReallyLearnNewThings","content":"\n## 인공지능|LLMS|미세 튜닝|\n\n\u003cimg src=\"/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_0.png\" /\u003e\n\n대형 언어 모델 (LLMs)은 방대한 텍스트 말뭉치에서 훈련되어 많은 사실적 지식을 습득합니다. 이러한 지식은 그들의 매개변수에 내재되어 필요할 때 사용될 수 있습니다. 이러한 모델의 지식은 훈련의 끝에 \"결정\"됩니다. 사전 훈련이 끝나면 모델은 사실상 학습을 멈춥니다.\n\n그 후, 모델은 정렬되거나 지시 튜닝을 받아 이 지식을 최상으로 활용하고 사용자의 질문에 더 자연스럽게 응답하는 방법을 배울 수 있습니다. 때로는 모델의 지식이 충분하지 않을 수 있습니다. 왜냐하면 이 질문은 일반적이며 관심 영역에 맞게 맞춰져 있지 않을 수 있기 때문입니다. 모델은 RAG를 통해 외부 메모리에 접근할 수 있지만, 새로운 도메인에 모델을 적응시키는 데 미세 튜닝을 통해 장점을 얻을 수 있다고 여겨집니다. 일반적으로 이 미세 튜닝은 인간 주석자나 다른 LLM들이 작성한 입력을 사용하여 실시됩니다. 이 단계에서 모델은 추가적인 사실적 지식을 만나 매개변수에 통합합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n실제로, 기계적인 수준에서는 이러한 상호작용이 어떻게 일어나는지 정말로 알지 못합니다. 게다가 일부에 따르면, 이 새로운 지식에 노출되는 것이 모델이 환각을 경험하게 할 수도 있습니다. 이는 모델이 이전에 보유한 지식에 근거하지 않는 사실을 생성하도록 훈련되어 있기 때문입니다(또는 모델의 이전 지식과 충돌할 수도 있습니다). 게다가 앞서 본 것처럼 모델은 훈련 코퍼스에서 덜 빈번한 엔티티(예: 이전 코퍼스에서 덜 빈번한 엔티티)에 대해 어려움을 겪을 수 있습니다.\n\n따라서, 최근에 발표된 연구는 모델이 새로운 지식을 통해 미세 조정되는 과정에서 무슨 일이 일어나는지 분석하는 데 관심을 가졌습니다.\n\n저자들은 모델이 미세 조정을 거치고 새로운 지식을 습득한 후 반응이 어떻게 변하는지에 대해 자세히 조사했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저, 그들은 세밀한 조정 후 예시를 위한 지식 수준으로 분류하려고 노력했습니다. 새로운 예시는 모델의 지식과 일치하지 않을 수 있는 지식을 내재적으로 갖고 있습니다. 예시는 알려진 것일 수도 있고 알려지지 않은 것일 수도 있습니다. 알려진 경우에도 고도로 알려진 지식, 어느 정도 알려진 지식 또는 약하게 알려진 지식을 보유하고 있을 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_2.png)\n\n저자들은 이 데이터셋으로 모델 (PaLM 2-M) 을 세밀하게 조정하였습니다. 세밀한 조정을 위한 각 예시는 인과적 지식 (주어, 관계, 목적어)으로 구성되어 있습니다. 이를 통해 모델이 이 지식을 특정 질문 (예: \"파리는 어디에 위치하나요?\")과 실제 정답 (예: \"프랑스\")으로 질의할 수 있습니다. 즉, 그들은 모델에 새로운 지식을 제공하고 이러한 삼중체를 질문 (질문-응답 쌍)으로 재구성하여 지식을 검증합니다. 이들은 이 모든 예시를 위에서 논의한 범주로 나누고 그 결과를 평가했습니다.\n\n저자들은 모델을 세밀하게 조정하고 환청을 테스트하기로 결정했습니다. 그들에게 알려지지 않은 사실의 높은 비율은 성능 저하를 초래한다고 판단하였으며 (이는 긴 세밀한 조정 시간으로 보상받을 수 없습니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_3.png\" /\u003e\n\n사실, 여러 번 에포크로 훈련하는 것은 해로운 효과를 초래할 수 있습니다. 이전 연구에 따르면 더 많은 에포크는 성능 저하로 이어지는 것으로 나타났습니다 (과적합으로 이어질 수 있음). 저자들은 이 효과가 알려지지 않은 사실 비율이 높을수록 증가한다고 합니다.\n\n저자들은 알려지지 않은 사실이 에포크 수가 적을 때 거의 중립적인 효과를 보이지만, 에포크 수가 많을 때 성능을 해친다고 합니다. 따라서, 알려지지 않은 예는 해로울 수 있지만, 그들의 부정적인 영향은 대부분 나중의 훈련 단계에서 실현됩니다. 저자들은 그런 예제들을 핏하는 과정을 연구합니다. 도표는 미세 조정 기간에 대한 데이터 집합 예제의 알려진 부분과 알려지지 않은 부분의 훈련 정확도를 나타냅니다. 모델이 알려지지 않은 예를 나중 단계에서 배운다는 것을 알 수 있습니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_4.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저자들은 정확도와 알려진 및 알려지지 않은 예제 간의 관골을 양적으로 측정할 수 있는지, 그리고 선형인지 의문을 제기합니다. 결과는 알려지지 않은 예제가 성능에 해를 끼치고, 알려진 예제는 그것을 향상시키는 강력한 선형 관련이 있다는 것을 보여줄 것입니다 (이 선형 회귀의 연관 계수는 거의 같습니다).\n\n![이미지](/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_5.png)\n\n게다가, 이 미세 조정은 특정 케이스에서의 성능을 넘어서 모델 지식에 광범위한 영향을 미칩니다. 저자들은 Out-of-Distribution (OOD) 테스트 세트를 사용하여 알려지지 않은 예제가 OOD 성능에 해를 끼친다는 것을 보여줍니다. 저자들에 따르면 환각의 발생과도 관련이 있습니다.\n\n재미있는 결과는 최상의 결과가 매우 잘 알려진 예제와 함께 얻어지는 것이 아니라 아마도 알려진 예제와 함께 얻어진다는 것입니다. 다시 말해, 이러한 예제들은 모델이 이전 지식을 더 잘 활용할 수 있게 합니다 (너무 잘 알려진 사실은 모델에 유용한 영향을 미치지 않습니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image 6](/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_6.png)\n\n그에 반해, 알려지지 않은 사실과 약하게 알려진 사실은 모델의 성능에 해를 끼치며, 이 하락은 환각 증가에서 유도된 것입니다.\n\n따라서 저자들에 따르면, 이 알려지지 않은 지식은 성능에 해를 끼칠 수 있으며 (따라서 미세 조정은 거의 무용한 것으로 만듭니다). 이 알려지지 않은 지식을 \"I don't know\"로 표시하여 이 피해를 줄일 수 있다고 초기 결과에 따르면 추정됩니다.\n\n![Image 7](/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_7.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 글을 요약하면, 모델은 파인튜닝 중에 알 수 없는 지식을 받을 경우 손상될 수 있습니다. 게다가, 이 성능 하락은 환각증 증가와 관련이 있습니다. 반면, 알 수 있는 예시는 긍정적인 효과가 있을 수 있습니다. 따라서, 이는 모델이 새로운 지식을 통합하는 데 어려움을 겪는 것을 보여줍니다. 다시 말해, 모델이 배운 것과 새로운 지식을 어떻게 사용하는지 사이에 충돌이 있을 수 있습니다. 이는 조정 및 지시 파인튜닝과 관련이 있을 수 있습니다 (불행히도 이에 대한 연구는 이 연구에서 다루지 않았습니다).\n\n한편, 특정 도메인 지식을 가진 모델을 사용하고 싶다면, 이 연구는 RAG를 사용하는 것이 좋다는 것을 제안합니다. 반면, \"알 수 없음\"으로 표시된 결과는 파인튜닝의 제약을 극복하기 위한 다른 전략이 있다는 것을 의미합니다.\n\n알 수 있는 사실이 실제로 유익하다는 사실은 안심스럽습니다. 오늘날의 모델은 방대한 양의 텍스트로 훈련됩니다. 이는 많은 지식을 많이 보았다는 것을 의미합니다. 따라서 파인튜닝 데이터셋에 있는 사실 중 많은 것은 이미 모델의 알고 있는 요소일 수 있으며 모델에 해를 끼치지 않을 수 있습니다 (아마도 일부만 알 수 없을 것입니다).\n\n어쨌든, 이 연구는 흥미로우며 파인튜닝과 새로운 지식과 기존 지식 간의 충돌이 어떻게 해소되는지에 대해 여전히 명확하지 않은 요소가 있다는 것을 보여줍니다. 이는 항상 파인튜닝 전후 모델의 결과를 테스트하는 이유 중 하나입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 어떻게 생각하세요? 포인트 파인튜닝 후 환각증이 증가했나요?\n\n# 흥미로운 정보였다면:\n\n다른 기사를 찾아볼 수 있고, LinkedIn에서 연락하거나 저에게 연락할 수 있습니다. 주간 업데이트되는 머신 러닝 및 인공 지능 뉴스를 포함하는 이 저장소를 확인해보세요. 협업과 프로젝트에 관심이 있고 LinkedIn에서 저에게 연락할 수 있습니다. 또한 새 이야기를 게시할 때 알림을 받기 위해 무료로 구독할 수도 있습니다.\n\n여기 저의 GitHub 저장소 링크가 있습니다. 여기서 머신 러닝, 인공 지능 및 기타 관련 자료 및 코드를 모아두고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n또는 최근에 작성한 제 논문 중 하나에 관심이 있을 수 있습니다:\n\n# 참고 자료\n\n본 문서를 작성하는 데 참고한 주요 참고 자료 목록을 아래에 제시합니다. 각 논문의 저자 이름만 인용하였습니다.\n\n- 황, 2023, 대규모 언어 모델에서 환각에 대한 조사: 원칙, 분류, 도전과 공개 질문, 링크\n- 레오가오, 2021, 행동 복제의 미적정성, 링크\n- 게크만, 2024, 새로운 지식 기반에서 언어 모델 미세 조정이 환각을 유발하는가?, 링크\n","ogImage":{"url":"/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_0.png"},"coverImage":"/assets/img/2024-05-23-CanaLLMReallyLearnNewThings_0.png","tag":["Tech"],"readingTime":5},{"title":"개발자들도 예전에는 사람이었어요","description":"","date":"2024-05-23 18:07","slug":"2024-05-23-DevelopersUsedToBePeople","content":"\n\n## AI | 소프트웨어 개발\n\n미래에는 손주들에게 젊을 때 IBM 개발자였다고 설명할 수 있다고 상상해 볼 수 있습니다.\n\n한때 \"컴퓨터\"란 용어가 사람들, 계산을 하는 사람들을 가리켰던 것처럼, \"개발자\"의 역할도 곧 아마 AI, 대형언어모델(Large Language Models 또는 LLMs), 양자 컴퓨팅의 조합으로 완전히 기계에 의해 수행될 것이라고 믿습니다.\n\nAI의 위험성에 대한 많은 글이 쓰여져 왔습니다. 컴퓨팅에서 AI의 새로운 패러다임으로의 전환에 대한 것입니다. 저는 이러한 변화가 기술과 발견의 자연스러운 진행이며 심오한 성장의 기회이며 두려워할 것이 아니라고 믿습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n어떤 새로운 발전이든 불확실성과 두려움이 따른다는 것을 기억해주셔야 해요. 하지만 이는 동시에 자기 성찰, 성장, 진화의 기회이기도 합니다. 그래서, 한 숨을 깊게 들이마시고, 두려움을 살펴보고, 우리가 성장해야 할 부분을 찾아내며, 함께 큰일을 해내봐요.\n\n우리의 전환을 이해하기 위해서는 우리가 오늘의 개발자로 발전하기까지의 컴퓨팅 역할의 진화를 되돌아봐야 하고, 이러한 역할이 고급 기술의 도래와 함께 다시 어떻게 변화할지 알아봐야 해요.\n\n# 역사적 맥락과 전환\n\n## 기억하세요 — 컴퓨터는 직위였던 때\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n세기의 변곡점에서, 수학을 잘한다면 몇 가지 직업 옵션이 있었습니다. 수학을 정말 잘한다면, 인간 컴퓨터로 직업에 들어갈 수 있었죠. 이 직업에서는 수학적, 과학적 또는 공학적 목적으로 숫자와 복잡한 계산을 수동으로 처리했습니다.\n\n아마도 어딘가의 실험실에서 혼자 이를 하고 있을 리는 없었을 거에요; 아니죠, 여러분은 아마도 한 팀의 일원이 되었을 겁니다. 때로는 서로 다른 여러 인간 컴퓨터 팀의 일원이 되어 함께 계산을 하며 오차율을 줄이고 정확한 결과를 얻기 위해 노력했을 겁니다.\n\n재미있어 보일 수도 있겠죠? 어쩌면 그렇겠지만, 크게 그렇진 않다고 생각해요.\n\n더욱 그런 점은, 그 당시 여성들이 수학적 기술을 활용하는 몇 안 되는 기술적인 역할 중 하나였습니다. 만약 그 시기에 여성들이 과학 커뮤니티에서 동등하게 인정받았다면 얼마나 많은 업적이 있었을지 상상해 볼 수 있겠죠? 하지만 그건 다른 시간에 이야기할 이야기죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컴퓨터는 스타일과 직업을 의미했습니다.\n\n## 컴퓨터 프로그래머를 만나보세요\n\n전기 기계식 컴퓨팅 기계가 등장했을 때, 이 새로운 기계를 제어하는 데 더 적합한 사람은 누구나 아니었을까요? 이전에 수동으로 이 작업을 맡은 사람들이 그 역할을 맡게 되었습니다.\n\n새로운 기술이 등장하면서 새로운 기술들이 필요해졌습니다. 기계에게 수행해야 할 작업을 나타내는 지침을 통해 기계와 소통할 방법이 필요했습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n언어가 필요했습니다. 명령을 사용하여 기기의 여러 측면을 제어하는 기계 언어, 이러한 명령의 결과는 의도된 계산입니다.\n\n컴퓨팅 기계를 도구로 사용하는 사람들은 엔지니어나 과학자의 요구를 기계가 수행할 수 있는 작업으로 번역하여 이 기기의 모국어를 구사하는 법을 익혀야 했습니다.\n\n## 프로그래밍의 진화\n\n컴퓨팅 작업이 더 잘 이해되면 표준화가 가능해져 일반 컴퓨터 언어로 변환되어 컴퓨터를 제어해야 하는 원시 기계 언어로 먼저 변환해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n시간이 흘러가면서 프로그래밍 아이디어와 개념이 성장하면서, 고수준 명령과 저수준 기계어 사이의 거리가 더 멀어지며, 컴퓨터 프로그래머들은 정보와 아이디어를 자유롭게 생성 및 교환하고 효율적이고 정확한 기계 코드로 그러한 아이디어를 번역할 수 있는 명령 컴파일러에 의존할 수 있게 되었습니다.\n\n컴파일러 프로그래머는 기계어에 대한 견고한 지식과 동시에 일반 프로그래밍 언어의 필요를 알고 있어야 했습니다. 컴퓨터 프로그래머는 자신이 선택한 언어를 사용하여 고수준 명령을 만들어, 그것이 컴파일러 프로그래머의 작업에 의해 (희망적으로!) 해석되어 그 후 목표 하드웨어에서 실행되는 것이었습니다.\n\n다양한 전문 언어, 컴파일러 및 하드웨어 플랫폼이 등장했습니다. AI 및 LLM이 개발되고 고수준 개발자에게 소개되면서, 고수준 상호작용과 저수준 명령 사이에 더 많은 거리가 생겼습니다.\n\n개발자는 곧 일상적인 구두로 시스템에게 명령해 지정된 입력을 받아들이고 원하는 출력을 생성하는 코드를 생성하도록 요청할 수 있었으며, 그러한 출력을 개발자가 완성 중인 다른 작업과 통합하거나 원하는 출력을 위해 하드웨어에서 직접 실행할 수 있었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 미래: 개발자 프로그래밍\n\n## 개발자 프로그래머가 되기\n\n프롬프트 엔지니어링은 개발자 프로그래밍의 선행 단계로 볼 수 있습니다. 프롬프트 엔지니어링에서 개발자는 일반적인 패턴과 프로세스에 대한 지식을 통해 LLM 시스템으로부터 필요한 텍스트, 코드 또는 데이터 응답을 유도할 수 있는 텍스트 프롬프트를 만듭니다. LLM을 이용해본 사람이라면 아실 것이지만, 때로는 정확한 결과를 얻기 위해 여러 차례의 프롬프트 반복이 필요할 수 있습니다.\n\nLLM을 사용하여 솔루션을 만들거나 코드를 작성하는 개발자는 필요한 결과를 얻을 때까지 많은 버전의 코드를 실행할 수 있으며, 원하는 결과를 얻으면 가까운 결과이거나 약간의 수정이 필요한 것으로 판단하거나, 완전히 쓸모없다고 판단하여 새로운 접근을 위해 버릴 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n자 그럼, 코드를 개발하는 사람은 누구일까요?\n\n현재 이러한 노력들은 대부분 임시 방편 방식으로 이루어지고 있으며, 프로세스나 패턴, 입력이나 출력에 대한 일반적인 표준이 존재하지 않습니다. 때로는 한 명은 인간이고 다른 한 명은 기계인 두 명의 개발자가 있는 것 같습니다.\n\n간단한 단계로, 개발자 프로그래머의 영역으로 진입할 것입니다.\n\n우리가 원하는 것의 세부 사항과 원하는 방식을 나타내고, 하위 수준 시스템이 원하는 결과를 렌더링하는 데 필요한 제품을 생성하도록 하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n현재 프로그래머로서, 아마도 다음과 같은 기술을 가지고 계실 것입니다:\n\n- 언어\n- 패턴\n- 아키텍처\n- 테스팅\n- 보안\n\n## 목록에 '듣기' 추가하기\n\n이와 동등하게 중요한, 또는 최고 중요한 것은, 제가 생각하기에 프로그래머는 인간(그리고 기계)들과 대화를 듣고 문제 해결에 완전히 집중할 수 있는 기술, 실시간 피드백을 이해하고, 자신이 창출한 결과를 이해해야 한다고 말씀 드릴게요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희가 저수준 작업을 AI 시스템으로 완료하는 패러다임으로 이동하면서, 기계 개발자로서 우리의 기술도 가르침, 듣기, 이해 듣기 방향으로 전환되어야 합니다.\n\n## 개발자 프로그래머로서 대표 셰프 역할\n\n우리는 아이들의 이야기 작가와 비슷한 저수준 창조 패턴에서 벗어나게 될 것입니다. 거기서는 이야기가 섬세하게 제작되고 정확한 간단한 단어 선택에 귀중한 주의를 기울여 청중에 의해 직접 읽히는 것처럼 이야기가 만들어집니다. 대신 우리는 대표 셰프와 유사한 패턴으로 이동할 것입니다. 즉, 원하는 맛, 질감, 일관성을 가진 요리를 지식을 통해 창조할 수 있게 될 것입니다. 그리고 표준적인 요리 패턴, 과정 및 재료 상호작용의 지식 전달을 통해 창조할 수 있을 것입니다.\n\n대표 셰프는 요리 과정에 대한 심층적인 지식(조리 기술, 칼 사용 기술, 확보 등)을 가지고 있습니다. 또한 음식의 재료 상호작용(예를 들어 산성 또는 온도의 영향)과 잘 알려진 조합 및 관계(예를 들어, Florentine이라는 용어는 아마 시금치와 모르네 소스가 포함되어 있을 것이라는 것, 그리고 모르네는 홀란데이즈와 베어네즈와의 관계를 가지고 있다는 것)을 이해하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n푸짐한 음식 설명을 통해 주방장이 소품 셰프에게 원하는 요리를 만들도록 충분한 정보를 전달할 수 있어요. 이는 맛과 질감에 기대되는 품질을 얻을 수 있도록 도와 줄거에요.\n\n주방장은 당신 개발자에요. 소품 셰프의 경험, 배경, 그리고 개성에 따라 결과물의 변화가 있을 수 있지만, 요리의 성판은 1.) 주방장의 심도 있고 지식 있는 안내 능력, 그리고 듣기를 이해하는 능력과 2.) 소품 셰프의 훈련, 경험, 그리고 배경에 달려 있어요.\n\n물론, 소품 셰프는 듣기능력이 있어야 하지만, 듣기를 이해하는 주방장의 뛰어난 능력이 의사소통을 보장할 것이에요.\n\n같은 관점은 API 통합, 단위 테스트, 보안 표준 등의 기본 프로세스에 대한 깊은 지식, UI 구현, 오류 처리, 성능 및 하드웨어 제약 조건과 같은 재료 상호 작용, 그리고 잘 알려진 산업 표준 조합 (하드웨어, 시스템 아키텍처, 보안 아키텍처, 컨테이너)을 통해 기계 개발자에게 필요한 소프트웨어 결과를 올바르게 전달할 수 있어요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 경우에 소프트웨어 결과물은 예상된 형태와 기능을 갖춘 소프트웨어입니다. 그리고 주방장이 음식을 나가기 전에 모두 확인하는 것처럼 고객에게 궁극적인 책임을 지는 것처럼, 개발자도 소프트웨어를 스튜디오를 떠나기 전에 모두 확인하고 코드 품질에 대한 궁극적인 책임을 지게 될 것입니다.\n\n## 그래서, 훌륭한 개발자 프로그래머란?\n\n마음에 드는 음악가에 대한 Zoltan Kodály의 견해에 찬사를 보내며, 나는 훌륭한 개발자 프로그래머에 대한 유사한 요구 사항을 제안합니다.\n\n## 변화가 오고 있습니다\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리가 사용할 공학, 언어 및 청취 기술은 이미 갖추고 있습니다. 변해야 할 가장 중요한 것은 우리의 시각과 태도입니다.\n\n이것은 우리가 공학을 생각하고 문제를 해결하는 방식에 있습니다.\n\n우리는 새로운 도구, 발명품 및 폭발적인 혁신을 어떻게 활용할 수 있을까요? 그리고 우리가 일부인 기술의 진화와 큰 그림을 어떻게 생각해야 할까요?\n\n우리는 충분히 크게 생각하고 있나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n사람들은 손수 계산을 하는 사람들 중 일부가 첫 번째 전기기계 계산기를 위한 소프트웨어 프로그램을 작성했습니다. 소프트웨어 개발자들이 기계 개발자를 사용하여 소프트웨어를 만드는 데 처음으로 신임을 받을 것입니다.\n\n그들이 가져야 할 기술은 무엇일까요?\n\n어떠한 변화라도, 자기반성은 우리가 더 잘 이해할 수 있도록 돕고, 두려움이 아닌 지식으로 앞지르며, 결국 모두를 더 나은 사람으로 만들어줍니다.\n\n변화를 환영하세요. 변화가 다가오고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제 이야기를 읽어주셔서 감사합니다! 마음에 들었다면, 이것도 한번 읽어보세요:","ogImage":{"url":"/assets/img/2024-05-23-DevelopersUsedToBePeople_0.png"},"coverImage":"/assets/img/2024-05-23-DevelopersUsedToBePeople_0.png","tag":["Tech"],"readingTime":6},{"title":"알알지 탐정 웹사이트 데이터를 활용한 검색 증대형 생성","description":"","date":"2024-05-23 18:03","slug":"2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata","content":"\n\n이 기사는 하버드의 AC215 가을 2023 과정의 최종 프로젝트의 일환으로 제작되었습니다.\n\n- 프로젝트 Github 저장소\n- 비디오\n\n저자: Ian Kelk, Alyssa Lutservitz, Nitesh Kumar, Mandy Wong\n\n![](/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_0.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n대형 언어 모델 (LLM)인 GPT-3.5는 일반적으로 알려진 주제에 대한 질문에는 능숙하게 대답할 수 있음이 증명되었습니다. 그 주제들은 모델이 풍부한 양의 훈련 데이터를 받은 것으로 예상됩니다. 그러나, 그들이 훈련을 받지 않은 데이터가 포함된 주제에 대한 질문을 받을 때, 그들은 종종 그 지식을 갖고 있지 않다고 말하거나, 더 나쁜 경우에는 그럴듯한 답변을 허구로 만들어냅니다.\n\nLLM을 사용하여 회사의 제품 및 서비스에 대해 연구하기는 보통 불가능합니다. 제품 및 서비스를 직접 비교하기 위해서는 모델이 훈련을 받았던 것보다 최근의 데이터가 필요합니다. 우리가 해결하고자 하는 문제는 회사에 대한 최신 정보와 그들의 웹사이트 정보와 일치하는 정보를 얻는 방법을 찾는 것입니다.\n\n또한, 이 과정에 대한 이전에 다뤄지지 않았을 것으로 예상되는 이정표를 충족시키기 위해, GPT-3.5가 답변이 금융적인 성격일 수 있다고 할 때 BERT 모델을 세밀하게 조정하여 금융 감성 분석을 수행합니다.\n\n# 제안된 해결책\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 한계를 해결하는 두 가지 주요 방법이 있습니다: 세밀조정(fine-tuning)과 검색 증강 생성(RAG).\n\n세밀조정은 모델을 학습률을 크게 줄여서 사용자의 데이터를 계속하여 훈련시키는 과정입니다. 새롭게 얻은 지식은 모델 가중치에 캡슐화됩니다. 그러나 세밀조정은 모델의 복사본 및 해당 비용을 사용하고 호스팅해야 한다는 점, 그리고 \"치명적인 망각\"이라는 위험(이전에 배운 정보를 잊어버리는 현상)도 있습니다.\n\n반면 RAG는 주로 포함된 텍스트의 임베딩의 벡터 저장소를 활용한 지식 소스를 사용합니다. 쿼리의 예측된 임베딩을 벡터 저장소의 임베딩과 비교함으로써, 우리는 LLM을 위한 적절한 프롬프트를 형성할 수 있어서 그것이 그의 문맥 안에 맞고 질문에 답할 정보를 포함합니다.\n\n# 우리의 해결책에는 세 가지 주요 구성 요소가 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 웹 사이트에서 스크랩 된 데이터를 사용하는 챗봇은 'ssitemap.xml' 파일에서 가져온 데이터를 사용합니다. 이 파일은 검색 엔진에 사이트의 모든 스크랩 가능한 링크를 안내하는 것이 목적입니다. 이는 검색 엔진보다 더 구체적이고 통찰력 있는 방법으로 사용됩니다. LLM(언어 모델)는 질문에 답변할 때 이 맥락만 사용해야 하며, 자체 훈련 데이터를 삽입하거나 답변을 가공해서는 안 됩니다. 모델이 분명히 알고 있을 것으로 예상되는 \"김 카다시안은 누구인가요?\"와 같은 질문으로 이를 간단히 테스트할 수 있으며, 모델이 \"제공된 맥락 안에 답변되지 않았다\"고 회신하도록 보장해야 합니다.\n- API를 통한 비동기 호출을 통해 애플리케이션에서 웹 사이트를 실시간으로 스크랩하는 기능.\n- LLM에서 중요한 결과물에 대한 금융 감성 분석. GPT-3.5의 프롬프트의 일부로, 응답이 금융적인 경우를 묻습니다. 이 경우, 세밀하게 미세 조정된 BERT 모델이 호출되어 응답을 분류하고, 확률 플롯과 적절히 귀여운, 저작권 위반이 아닌 버트 퍼펫이 표시됩니다.\n\n# RAG 구성 요소\n\nRAG를 이해하기 위해 단순화 된 비유를 사용해보겠습니다. 한 사람이 받을 수있는 두 가지 유형의 시험 문제가 있습니다. 첫 번째는 사실에 대한 간단한 요청입니다:\n\n- 프랑스의 수도는 무엇인가요?\n- 에베레스트 산을 처음으로 등반한 사람은 누구인가요?\n- 캐나다가 영국으로부터 독립을 얻은 연도는 언제인가요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이러한 질문에 답변하는 데 특별한 기술이 필요하지 않습니다. 적절한 참고 자료가 있는 사람은 올바른 답변을 찾아볼 수 있습니다.\n\n![image](/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_1.png)\n\n다른 유형의 질문은 학습된 기술이 필요한 질문입니다. 해당 주제에 대한 교과서를 숨기고 있더라도 상관없는 질문이며 연습하고 공부하지 않은 경우 만족스럽게 대답할 수 없습니다. 예를 들어:\n\n- 독일어로 시를 쓰세요.\n- 첫 백만 개의 소수를 계산하는 컴퓨터 프로그램을 작성하세요.\n- 베토벤 스타일의 심포니를 작곡하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수학 교과서들로 둘러싸인 채로 있어도, 그 중에 반복해서 보던 책 안에 페르마의 마지막 정리가 있지 않다면 페르마의 마지막 정리를 증명할 수는 없을 거에요!\n\n![이미지](/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_2.png)\n\n대형 언어 모델의 세계에서 RAG는 사기가 쉬운 첫 번째 유형의 문제를 해결하는 데 사용됩니다. 미세 조정은 모델이 실제로 학습해야만 문제를 해결할 수 있는 두 번째 유형의 문제를 해결하는 데 사용됩니다. RAG는 더 쉬워요—모델을 다시 교육시킬 필요가 없고, 모델의 내부 작업을 다룰 필요가 없으며, 모델이 \"사기\"를 할 데이터를 쉽게 조절할 수 있어요. 흥미로운 점은, 이 방법이 또한 모델이 \"환각하는\" 양을 크게 줄여준다는 거예요—때에 따라 충분한 훈련 데이터를 바탕으로 픽션 같지만 타당한 답변을 창조하는 대형 언어 모델의 일반적인 문제 중 하나에요. RAG의 유일한 어려운 부분은 모델에 주어질 적절한 데이터를 찾는 것이에요; 모델은 얼마나 많이 자극을 받을 수 있는지에 제한이 있고, 500페이지짜리 역사 교과서는 너무 길어요.\n\nRAG가 작동하는 기본 개념은 아래와 같아요:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터 정리: 위의 만화 속 작은 사람처럼 교과서로 둘러싸여 있다고 상상해보세요. 우리는 이 책들을 각각 작은 조각으로 나누어—하나는 양자 물리학에 대한 것이고, 또 다른 하나는 우주 탐사에 관한 것일 수 있습니다. 이러한 각 조각 또는 문서는 벡터를 만들기 위해 처리됩니다. 이는 마치 도서관에서 그 정보 덩어리를 바로 가리키는 주소와 같은 것입니다.\n- 벡터 생성: 이러한 조각들은 임베딩 모델을 통해 전달됩니다. 이는 정보의 의미를 포착하는 수백 개 또는 수천 개의 숫자로 이루어진 벡터 표현을 만드는 모델의 한 유형입니다. 이 모델은 각 조각에 고유한 벡터를 할당합니다—컴퓨터가 이해할 수 있는 고유한 인덱스를 만든 것과 같습니다.\n- 질의: LLM에게 대답할 수 없는 질문을 하고 싶을 때, 먼젓거로 \"AI 규제 분야의 최신 개발은 무엇인가요?\"와 같은 프롬프트를 제공하여 시작합니다.\n- 검색: 이 프롬프트는 임베딩 모델을 통해 통과되어 벡터로 변환됩니다. 이는 자신의 의미를 기반으로 고유한 검색어를 얻는 것이며, 그 키워드의 정확한 일치뿐만 아니라 의미에 기반한 검색어를 얻는 것입니다. 시스템은 이 검색어를 사용하여 질문과 관련된 가장 관련성 있는 조각을 찾기 위해 벡터 데이터베이스를 검색합니다.\n- 컨텍스트 추가: 가장 관련성 있는 조각은 컨텍스트로 제공됩니다. 이는 질문하기 전에 참조 자료를 제공하는 것과 비슷한데, 우리는 LLM에게 조언을 제공합니다: \"이 정보를 사용하여 다음 질문에 대답하십시오.\" LLM에게 제공되는 프롬프트는이 배경 정보로 확장되지만 사용자가 이를 볼 수는 없습니다. 이 복잡성은 뒷면에서 처리됩니다.\n- 답변 생성: 마침내, 이 새로운 정보를 갖춘 LLM은 방금 검색한 데이터를 품은 응답을 생성하여 질문에 대답합니다. 이러한 방식으로 답변하는 것은 마치 이미 그 답을 알고 있던 것처럼 느껴집니다.\n\n우리는 이 목표를 달성하기 위해 \"Weaviate\"라는 벡터 저장소 위에 애플리케이션을 구축했습니다. 우리는 파이썬에서 웹 스크래퍼를 구축하여 주어진 웹 사이트의 sitemap.xml을 크롤하도록하였습니다. 이는 검색 엔진이 사이트를 크롤할 수 있도록 사용되는 페이지 목록입니다. 인터넷 웹사이트의 끝없는 다양성 때문에 약간의 도전이 되었다는 것이 밝혀졌습니다.\n\n# 스크래퍼 구성 요소\n\n우리의 웹 스크래퍼는 현대 웹 아키텍처의 많은 도전을 처리하도록 구축되었으며, 전통적인 스크래핑 방법으로 종종 놓치는 데이터를 캡처하고, 스크래핑 활동을 실시간으로 웹 애플리케이션으로 스트리밍합니다. 이 엔드포인트는 웹 데이터를 가져 오고, 여러 단계의 저장 및 처리로 전환하고, 결과적으로 벡터 저장소에 즉각 색인화하기 위해 설계된 솔루션의 중요한 데이터 수집 단계를 나타냅니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스크레이퍼는 먼저 PythonBeautifulSoup 라이브러리를 사용하여 HTML 및 CSS 내용을 걸러냅니다. 그러나 일부 현대 웹사이트는 동적 콘텐츠 생성을 위해 JavaScript에 의존하므로, 순전히 HTTP 요청에 의존하는 표준 스크레이핑 방법에 장애물을 제공합니다. 스크레이핑 시스템은 Selenium WebDriver를 활용하여 이 문제를 해결합니다. 이 도구는 '헤드리스'(화면 없이 실행되는) 브라우저인 Google Chrome을 통해 웹 페이지와의 실제 사용자 상호작용을 모방하는 기능을 제공합니다. 이를 통해 동적 콘텐츠 로딩을 완전히 지원합니다. 스크레이퍼가 직접 HTTP 요청을 통해 데이터를 추출하는 초기 노력이 무효화되었거나 일정 임계값 이하의 데이터를 가져오는 경우 Selenium이 사용됩니다. 이 기술은 스크레이퍼가 정적 페이지 로드를 통해 사용할 수 없는 콘텐츠에 성공적으로 액세스할 수 있도록 보장합니다.\n\n스크레이퍼는 불필요한 요소인 이미지와 같은 것들을 우회하여 오버헤드를 줄이고 빠른 처리를 가능케 합니다. 데이터를 수집한 후에는, 제어를 돌려 받아 데이터를 필터링하고 HTML에서 텍스트를 추출하는 BeautifulSoup로 돌아갑니다.\n\n데이터 추출 후에, 데이터는 직렬화되어 Google Cloud Storage에 CSV 파일로 저장되어 데이터의 백업으로 기능합니다. 추출된 데이터는 이후 조감도 라이브러리인 LlamaIndex를 통해 Weaviate에 쪼개져서 삽입되며, 여기서 벡터 저장소 인덱스에 추가됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 오케스트레이션\n\n데이터가 수집된 후에 어떤 일이 발생하는지 좀 더 자세히 살펴봅시다. 이 데이터를 벡터 스토어에 삽입하기 위해 수행해야 할 작업이 있습니다. 이러한 단계들은 위에서 요약 수준으로 언급되었지만, 구체적으로는 다음과 같은 단계들이 필요합니다.\n\n- 각 수집된 웹사이트를 가져와서 청크(chunk)로 분리합니다. 이 청크의 길이는 검색 프로세스가 얼마나 잘 작동하는지에 큰 영향을 미칩니다.\n- 각 청크에 대해 OpenAI의 텍스트 임베딩-ada-002 모델을 사용하여 임베딩 벡터를 생성합니다. Weaviate와 LlamaIndex는 이 모델을 기본적으로 통합하고 있습니다.\n- 이 임베딩 벡터와 텍스트 청크를 Weaviate 벡터 스토어에 삽입합니다.\n\n모델에 프롬프트할 때 검색 단계:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 텍스트 임베딩-ada-002을 통해 프롬프트를 가져와 임베딩을 얻습니다.\n- 그 임베딩을 사용하여 프롬프트에 대답해야 할 청크를 찾은 후, 그 청크를 쿼리에 선행하고 GPT-3.5로 보냅니다.\n\n문서의 청킹은 어느 정도 예술미가 있습니다. GPT-3.5는 최대 문맥 길이가 4,096토큰 또는 약 3,000단어입니다. 이는 모델이 처리할 수 있는 총 양을 나타냅니다 - 3,000단어 길이의 문맥을 갖는 프롬프트를 만들면, 모델은 응답을 생성할 충분한 공간이 없을 것입니다. 현실적으로, GPT-3.5에 약 2,000단어 이상으로 프롬프트를 지정해서는 안됩니다. 이는 데이터에 따라 청크 크기를 위한 트레이드 오프가 있다는 것을 의미합니다.\n\n더 작은 chunk_size 값으로 설정할수록, 반환된 텍스트는 보다 상세한 텍스트 청크를 생성하지만, 텍스트에서 멀리 떨어져 있는 정보를 놓칠 위험이 있습니다. 반면, 더 큰 chunk_size 값은 상단 청크에 필요한 모든 정보를 포함할 가능성이 높아 더 나은 응답 품질을 보장하지만, 정보가 텍스트 전체에 분산되어 있는 경우 중요한 섹션을 놓칠 수 있습니다.\n\n이 트레이드 오프가 어떻게 작동하는지 설명하기 위해 최근 테슬라 사이버트럭 출시 이벤트를 사용한 몇 가지 예를 살펴보겠습니다. 트럭의 일부 모델은 2024년에 출시될 예정입니다. 그러나 RWD만 탑재된 최저가 모델은 2025년까지 출시되지 않을 것입니다. RAG에 사용된 텍스트의 형식과 청킹에 따라 모델의 응답이 이 사실을 만나지 못할 수도 있습니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위의 이미지에서 파란색은 일치 항목이 발견되어 청크가 반환된 위치를 나타내며, 회색 상자는 청크가 검색되지 않았음을 나타냅니다. 또한 빨간색 텍스트는 관련 텍스트가 존재하지만 검색되지 않았음을 나타냅니다. 짧은 청크가 성공적으로 작동하는 예시를 살펴보겠습니다:\n\n위의 이미지에서 왼쪽에 있는 텍스트는 RWD가 2025년에 출시될 것이라는 내용이 단락으로 구분되어 있지만 질의와 일치하는 관련 텍스트를 가지고 있습니다. 두 개의 짧은 청크를 검색하는 방법이 더 잘 작동하는 이유는 모든 정보를 포착하기 때문입니다. 오른쪽에서은 검색기가 단일 청크만 검색하기 때문에 추가 정보를 반환할 공간이 없어 모델이 잘못된 정보를 제공합니다.\n\n그러나 항상 그런 것은 아닙니다. 때로는 질문에 대한 진정한 답변을 포함하는 텍스트와 강력한 일치하지 않을 때 더 긴 청크가 더 잘 작동합니다. 더 긴 청크가 성공하는 예시를 살펴보겠습니다:\n\n몇 번의 실험 끝에, 우리는 1,000 토큰 길이의 청크를 사용하고 GPT-3.5를 촉발시키기 위해 두 개의 청크를 검색하기로 결정했습니다. GPT-3.5는 4,096 컨텍스트 길이를 처리할 수 있기 때문에 적절한 응답을 위한 충분한 공간을 남겨둘 것으로 예상됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로젝트를 처음 시작할 때는 chunking, indexing, 그리고 데이터 검색을 직접 수행하여 매우 잘 작동되었습니다. Weaviate에서 쿼리 언어로 사용하는 GraphQL을 배우는 데는 학습 곡선이 있었습니다.\n\n![이미지](/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_4.png)\n\nLlamaIndex와 같은 라이브러리를 사용하는 장점은 이러한 조율을 추상화하여 다른 벡터 스토어로 교체할 수 있는 기회를 제공한다는 것입니다 (Weaviate는 Milvus, Qdrant, Pinecone과 같은 많은 경쟁사가 있으며, 계속해서 새로운 경쟁사들이 등장하고 있습니다). LlamaIndex를 사용하면 향후 보다 복잡한 RAG 구현(예: 트리 구조 데이터 및 재귀적 프롬프팅)을 실험할 수도 있습니다. 그러나 이러한 새로운 라이브러리 사용은 적절한 문서의 부재와 같은 도전을 안겨주었습니다. 그들의 도움 자료 대부분은 예제로 이루어져 있었는데, 이 예제들이 우리의 사용 사례에 부합하지 않으면 Discord에서 개발자들에게 질문하거나 소스 코드를 직접 읽는 것 외에는 다른 선택지가 없었습니다.\n\n# BERT 구성요소\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모델 호스팅 요구 사항을 충족하기 위해, BERT 모델을 세밀 조정하고 훈련한 후 Google Vertex에서 파이프라인을 통해 호스팅했습니다. 이 모델을 응용 프로그램에 추가하면 실제로 GPT-3.5로 활용하여 답변과 함께 플래그를 반환하고 해당 답변이 금융적인 것으로 판단되는지 여부를 알 수 있습니다. 그럴 때마다, 우리는 적절히 유머러스하면서도 저작권을 침해하지 않는 버트 퍼펫과 모델이 반환한 확률 플롯을 표시할 수 있습니다.\n\nBERT 모델의 세밀 조정 과정은 데이터뿐만 아니라 기술적인 구성에 관한 것이었습니다. 이 모델에게 텍스트 내에서 금융적 감정을 구별하는 기술을 가르치는 것이 목표였는데, 이는 복잡한 금융 뉴스에서 통찰력을 필요로 하는 사람들에게 유용한 기술입니다. 이러한 기사에서의 감정은 상냥하고 즉각적으로 드러나지 않을 수 있기 때문에, 특별히 세밀하게 조정된 모델을 활용하여 비전문가에게 밑바탕에 깔린 감정의 분명한 지표를 제공하는 것이 중요합니다.\n\n우리가 BERT 모델을 훈련시킬 때는 금융 문구은행 데이터셋을 사용했습니다. 이 데이터셋은 금융에 능통한 사람들이 감정으로 레이블이 지정된 문장들로 구성되어 있습니다. 그러나 이러한 데이터셋에서 흥미로운 문제가 발생합니다: 어노테이터 간의 합의 수준의 변동에 따른 차이점은 모델의 학습과 그에 따른 예측에 암묵적으로 영향을 줄 수 있습니다.\n\n이러한 데이터셋을 사용하여 BERT 모델을 세밀하게 조정했을 때 (어노테이터 간 100%, 75%, 66%, 50% 합의를 대표하는 데이터셋 포함), 어노테이터들이 동의하는 정도가 더 높을수록 훈련된 모델이 더 좋아 보였습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수상한 거 아니에요? 모델이 더 높은 합의를 보일수록 실제로 더 나은 결과를 내놓는다는 것이 아니라, 높은 합의 수준은 금융 보고서가 간단히 분류하기 쉬운 것을 의미한다는 것입니다.\n\n높은 합의가 모델을 명확한 감정만 인식하도록 편향시킬 수 있고, 때로는 복잡한 금융 텍스트에서 현실인 미묘한 감정을 놓쳐버릴 수도 있기 때문에 상상하기 어렵지 않습니다.\n\n이 문제를 해결하기 위해, 우리는 데이터의 편향을 해소하기로 결심했습니다. 우리는 모든 감정의 명료성 수준이 공정하게 대표되도록 훈련 및 테스트 분할을 구성하는 것을 목표로 삼았습니다. 이를 위해 데이터 분할 과정을 주의 깊게 프로그래밍하고, 무작위 섞기, 계층 샘플링, 그리고 균형 잡힌 분할을 위한 검증 및 테스트 세트 생성과 같은 추가 단계를 거쳤습니다. 이렇게 함으로써, 우리는 모델이 단순히 분명한 감정 데이터에서만 잘 작동하는 위험을 완화하고, 대신 금융 텍스트의 현실적인 혼합을 처리할 수 있도록 보장했습니다.\n\n편향 해소 후, 보다 균형있는 접근 방식을 사용하여 모델을 평가하자, 흥미로운 추세가 나타났습니다. 75% 주석 작성자 합의로 교육된 모델이 가장 높은 F1 점수를 보였는데, 초기 결과에서 벗어나는 것으로 나타났습니다. 우리가 의심했던 대로, 최상의 데이터셋은 실제로 완전한 주석자 합의와 논란을 유발하는 좀 더 복잡한 금융 보고서 사이에서의 타협이었던 것 같습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모든 것을 결합하기\n\n애플리케이션 아키텍처는 FastAPI 서비스와 Nginx를 통해 연결된 프론트엔드 컨테이너로 구성됩니다. FastAPI 서비스는 특정 작업에 대한 스트리밍 응답을 처리하는 여러 엔드포인트를 호스팅하며, 스트리밍 응답을 사용하여 쿼리를 관리하고 웹 사이트 주소를 나열하고 특정 웹 사이트의 타임스탬프를 검색하고 주어진 쿼리에 대한 URL 및 금융 플래그를 가져오며, Vertex AI의 Prediction API를 활용하여 감성 분석에 사용하고 효과적인 사이트맵 스크래핑을 위해 입력을 처리합니다.\n\n프론트엔드는 HTML과 JavaScript로 구축되어 동기식 및 비동기식 함수를 사용하는 단일 페이지 애플리케이션으로 만들어졌습니다. CSS로 스타일이 지정되어 있으며, 로딩 인디케이터 및 슬라이딩 패널용 여러 가지 효과를 사용하고, Google의 Material Design 라이브러리를 사용하여 현대적인 텍스트 입력란과 버튼을 만듭니다.\n\n우리는 OpenAI의 DALL-E 3를 사용하여 30개의 서로 다른 저작권 침해가 없는 Bert 이미지를 생성했습니다. 이 중에는 긍정적, 부정적, 중립적 감정 각각 10개씩이 포함되어 있습니다. 프론트엔드는 발견된 감정에 따라 해당 감정 이미지 중 하나를 무작위로 선택하여 표시하고 클래스와 함께 표시합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n진행 및 각 단계 완료 상황의 실시간 업데이트가 사이트맵 스크래핑 프로세스 중에 클라이언트에게 제공됩니다. Nginx는 게이트웨이로 작동하여 적절한 엔드포인트로 요청을 효율적으로 라우팅하여 일관된 반응성 있는 사용자 경험을 보장합니다.\n\n# 배포\n\nRAG Detective 앱은 Ansible을 사용하여 자동화 및 재현성에 따라 배포됩니다. 배포 프로세스에는 필요한 Google Cloud Platform (GCP) API 활성화, GCP 서비스 계정 설정, 필요한 소프트웨어가 포함된 Docker 컨테이너 생성이 포함됩니다. Ansible playbook은 Docker 컨테이너를 빌드하고 GCR에 푸시하여 Google Cloud Registry (GCR)에 컨테이너를 만들고 푸시하며, GCP에서 컴퓨트 인스턴스 (VM) 서버를 생성하고 해당 구성으로 인스턴스를 구성하고 인스턴스 내에서 Docker 컨테이너를 설정하는 데 사용됩니다. 이 프로세스는 컴퓨트 인스턴스에 웹 서버로 Nginx를 구성함으로써 Docker 컨테이너에 대한 올바른 설정 및 액세스를 보장합니다.\n\n스케일링에는 Kubernetes를 사용합니다. Ansible playbook은 Kubernetes 클러스터를 생성하고 배포합니다. 이 단계에는 GCR에 Docker 컨테이너 빌드 및 푸시, Kubernetes 클러스터 생성이 포함됩니다. 증가된 수요 발생 시 스케일링 프로세스에는 GKE가 클러스터에 더 많은 노드를 자동으로 추가하는 노드 스케일링 및 쿠버네티스 수평 팟 자동스케일러가 관여하며 자원 활용률에 따라 팟 복제 수를 수정합니다. 로드 밸런싱은 Kubernetes Services 및 GKE가 Google Cloud Load Balancer와 통합되어 들어오는 트래픽을 고르게 분배하여 달성됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n원하는 것 중 하나는 Weaviate 인스턴스가 다른 컨테이너와 함께 확장되지 않으며, 데이터베이스 확장 구현은 이 코스의 범위를 벗어나고 별도의 과학이 필요하다는 점입니다. Weaviate를 확장하기 위해 수평 확장이 주요 방법으로, 클러스터에 더 많은 노드를 추가하여 작업 부하를 균등하게 분산하는 것이 중요합니다. 이에 더하여 부하 분산을 위해 로드 밸런싱이 있으며, 이는 들어오는 요청을 이용 가능한 노드에 골고루 분배하는 데 도움이 됩니다. 데이터 샤딩도 가능하며, 데이터가 여러 노드에 파티션으로 분할되어 쿼리를 효율적으로 처리할 수 있습니다. 쿠버네티스를 사용하여 Weaviate 인스턴스의 확장 및 관리를 자동화할 수도 있어 시스템이 성장하여 증가하는 요구 사항을 충족하는 동시에 견고하고 효율적으로 유지될 수 있도록 할 수 있습니다.\n\n# 배운 점\n\n시스템 및 운영에 대해 몇 가지 중요한 교훈을 얻었습니다:\n\n- 인증은 큰 어려움 요소가 될 수 있습니다. 처음으로 무언가를 배포할 때마다 Google Cloud, Google Vertex 또는 OpenAI가 제대로 인증되지 않았거나 잘못된 권한을 가지고 있는 문제가 발생했습니다. 인증 및 시크릿 관리에 대한 숙달은 AI 제품의 운영에 숙련될 수 있는 주요 구성 요소입니다. (dotenv와 같은 라이브러리 사용이 유용하다는 것을 발견했습니다.)\n- GPT 모델은 선구자적입니다. LLM 답변에 대해 자신의 BERT 모델을 호출할지 결정하기 위해, 답변이 금융 관련인지 나타내는 플래그를 반환하도록 요청했습니다. 초기에는 플래그를 생성한 다음 답변을 생성하도록 했지만, GPT 모델은 텍스트를 생성하는 과정 중이어도 텍스트를 생성하기 전에는 분류할 수 없다는 사실을 깨달았습니다. 모델의 스트리밍 응답 끝에 플래그를 이동하는 것이 다소 까다로웠지만 더 잘 작동합니다. 한 가지 알아둬야 할 점은 GPT-3.5가 GPT-4보다 추론에서 훨씬 떨어지며, GPT-4가 하지 않는 오류를 자주 범한다는 것입니다.\n- 새로운 라이브러리를 사용할 때는 자신의 책임 하에 사용해야 합니다. Weaviate만 사용하여 작동하는 프로토 타입이 있음에도 불구하고, 보다 고급 RAG 아키텍처를 사용하기 위해 LlamaIndex를 사용했습니다. 적절한 라이브러리 참조 부재로 문서는 예제 코드로 가득하지만, 소스 코드를 자세히 살펴보아야만 할 일이 여러 차례 발생했습니다. 이 프로젝트를 계속해서 복잡한 RAG 방법을 추가해 나갈 수 있으며, 이는 LlamaIndex의 사용을 정당화할 뿐만 아니라 쿼리에 더 나은 응답을 제공할 것입니다.\n- 새로운 라이브러리를 사용하려면 Discord 챗을 확인해야 합니다. 보통 라이브러리 저자들은 문서가 떨어지거나 제대로 되어 있지 않을 때 Discord에서 활동적이며, 고민할 때 직접 안내를 제공해 줄 수 있습니다.\n- 데이터셋은 평범하게 숨겨진 편향을 담고 있을 수 있습니다. 우리는 financial_phrasebank 데이터셋을 사용했는데, 앞서 말한 바와 같이, 보다 큰 주석자 합의는 더 좋은 모델로 이어진 이유가 숨겨져 있었습니다. 데이터를 통해 보이는 듯한 단순한 경로를 선택할 때 항상 주의해야 합니다!\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 해당 프로젝트에 대한 이미지입니다.\n\n이 프로젝트는 매우 현대적인 문제에 대한 매우 현대적인 솔루션을 활용한 재미있는 프로젝트였습니다. 클라우드에서 모델을 훈련 및 배포하거나 자체 RAG 아키텍처를 만드는 경험은 수업 시간을 보내는 훌륭한 방법이었습니다.","ogImage":{"url":"/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_0.png"},"coverImage":"/assets/img/2024-05-23-RAGDetectiveRetrievalAugmentedGenerationwithwebsitedata_0.png","tag":["Tech"],"readingTime":13},{"title":"업리프트 모델링 상관관계에서 인과관계로","description":"","date":"2024-05-23 18:01","slug":"2024-05-23-UpliftModelingcorrelationtocausation","content":"\n대부분의 기업이 새로운 캠페인이나 프로모션을 도입했을 때 해당 캠페인이나 프로모션이 사용자 베이스에 효과가 있는지를 예측하는 ML 모델이 필요한 경우가 많습니다. 이러한 노력의 궁극적인 목표는 해당 캠페인들을 통해 더 많은 사용자를 전환하거나 CTR과 같은 특정 비즈니스 지표를 향상시키는 것일 수 있습니다. 물론 비즈니스 유형에 따라, 캠페인은 할인이나 충성도 프로그램과 같은 다양한 형태로 나타날 수 있습니다.\n\n간편함을 위해 이렇게 설명해 보겠습니다. 우리는 전자 상거래 웹사이트의 투자 수익률(ROI)을 높이기 위해 할인 캠페인을 진행 중이라고 상상해 봅시다. 이 문제를 간단하고 직관적인 방식으로 정식화하는 방법은 머신 러닝 모델을 훈련시켜 제품이 곧 판매될지 여부를 예측하는 것입니다. 비즈니스는 이러한 예측을 스마트하게 활용하여 ROI 지표를 높일 수 있으며, 할인을 적용해야 할 제품은 그렇지 않을 확률이 높은 제품에만 적용할 수 있습니다. 이러한 모델의 경우, 훈련 방법론은 매우 직관적입니다. 우리는 모든 과거 거래(구매) 데이터를 사용해야 할 것입니다. 웹사이트에 제품이 게시된 후 7일 동안 아직 판매되지 않은 경우, 대상 변수는 0(판매되지 않음)이 됩니다. 그렇지 않으면 우리의 레이블은 1(판매됨)입니다. 이것은 기본적으로 알려진 ML 모델인 \"제품 판매 가능성\" 모델입니다. 이 확률을 계산합니다 → P( Y: 1 | x ), 여기서 Y는 대상 변수(1 — 판매됨)이며 x는 제품, 사용자, 거래 기능의 조합과 같은 기능의 집합입니다.\n\n# 소개\n\n본질적으로, uplift 모델링은 다른 방식으로 동일한 문제에 접근하는 방법론입니다. 여기서 우리가 정말 원하는 것은 제품을 판매하기 위해 할인이 필요한지 여부를 알 수 있는 능력입니다. 다시 말해, 우리는 적용한 할인이 제품의 성공적 거래에 \"인과적\" 요인이 될지를 알고 싶어합니다. 이는 ML 영역에서 \"상관\"에서 \"인과\"로의 전환 지점입니다. 이전 문제 정의에서 우리는 팔 확률이 낮은 제품에 할인을 적용하면 항상 전환을 긍정적으로 영향을 줄 것이라는 강한 가정을 했습니다. 이것이 \"할인\"과 \"구매\"의 수 사이의 가정되는 \"상관관계\"라고 부르는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 Uplift 모델링은 판매될 가능성이 낮은 제품에 할인을 적용하지 않는 것을 원칙으로 합니다. 이는 \"판매될 가능성이 낮은\" 제품을 다양한 대상 하위 그룹으로 만듭니다. Uplift 모델링의 궁극적인 목표는 할인을 적용할 경우에만 판매될 수 있는 특정 제품 하위 그룹을 찾는 것입니다. 우리가 \"할인 적용\"을 Uplift 모델링 공식에 외부 요인으로 추가했기 때문에, 이제 2 (판매-판매되지 않음) x 2 (할인 적용-할인 적용하지 않음) = 4개의 레이블 데이터 그룹이 있습니다.\n\n![Uplift Modeling](/assets/img/2024-05-23-UpliftModelingcorrelationtocausation_0.png)\n\n## 1. 유기적으로 구매 — 할인 적용 후 구매:\n\n이 집합은 때때로 \"확실한 것\"이라고 불립니다. 할인을 적용하든 말든, 해당 제품은 어찌되었든 팔릴 것입니다. 분명히 여기서 할인을 적용하지 않는 것이 논리적입니다. 이 제품 그룹에 예산을 낭비해서 ROI를 높이는 것은 우리가 원하지 않을 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 2. 할인 적용 전에 구매하지 않은 경우 - 할인 적용 후 구매:\n\n이 집합은 때때로 \"설득 가능한\"이라고 불립니다. 이것은 업리프트 모델링이 찾고 있는 특정 제품 그룹입니다. 우리가 이러한 제품에만 할인을 적용한다면, 변환율과 ROI에 긍정적인 영향을 줄 수 있습니다.\n\n## 3. 자연적으로 구매한 경우 - 할인 적용 후에 구매하지 않은 경우:\n\n이 집합은 때때로 \"무시해야 할 대상\"이라고 불립니다. 조금 역설적으로 들릴 수 있지만, 때로는 할인에 부정적으로 반응하는 사용자 또는 제품 그룹이 있을 수 있습니다. 이 그룹에 대해 합리적으로 해야 할 일은 ROI 지표를 손상시키지 않기 위해 할인을 적용하지 않고 넘어가는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 4. 구매되지 않은 유기적 상품 — 할인 적용 후 구매되지 않은 경우:\n\n이 집합은 때때로 \"잃어버린 원인\"이라고도 불립니다. 이 그룹은 이탈한 것으로 생각할 수 있습니다. 이 제품에 할인을 적용해도 결과를 바꾸어 판매로 전환할 수는 없습니다. 그러나 최종 목표가 ROI 지표를 최적화하는 것이므로, 이 제품 그룹에 어떤 할인도 적용하고 싶지 않습니다.\n\n# 할인 캠페인의 ROI\n\n할인 캠페인의 성능을 측정하기 위해 증분성, 치킨넬라이즈이션이나 투자의 증분된 수익(iROI)과 같은 용어가 종종 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 용어들에 대한 정의를 하나씩 하기 전에, 이 예시를 위해 몇 가지 숫자에 이름을 붙여보겠습니다:\n\n- N: 할인이 적용된 총 구매 건수 (예: 10000)\n- T: 실험군의 총 구매 건수 (예: 25000)\n- C: 대조군의 총 구매 건수 (예: 21000)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nR_t: 치료 그룹의 총 수익 (예: $2.5백만)\n\nR_c: 대조 그룹의 총 수익 (예: $2.1백만)\n\nL: 적용된 할인으로 인한 총 매출 손실 (예: $300천)\n\n간편을 위해, 치료 그룹과 대조 그룹의 규모가 동일하다고 가정하고 경우에 따라 계산이 진행됩니다 →\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n할인 캠페인의 저효요 구매 비율:\n\n▶▶ 증가율 구매 비율 [(T - C) / N] → (25000 – 21000) / 10000 = 4000/10000 = 0.4 = 40%\n\n할인 캠페인의 카니발리제이션 구매 비율:\n\n할인 구매 중 증가치 없이 발생한 부분은 카니발리제이션으로 인한 것입니다. 이들은 할인이 없어도 발생했을 구매입니다. 이 경우, 카니발리제이션 구매의 수는 [N - (T - C)]입니다. 그 비율은 기본적으로 (1 - 증가율 비율)이며, 이는 (1 – 0.4) = 0.6 = 60% 입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n수익증대비율 (iROI):\n\n이는 적용된 할인으로 인한 총 수익 손실에 대한 치료 및 통제 사이의 증가 수익액의 비율입니다.\n\n[(R_t - R_c) / L] → ($2.5M - $2.1M) / $300K = 1.33\n\n이 비율이 1보다 크면 이 캠페인을 수익성 있는 것으로 볼 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 모델 파트\n\n주로 얼리프트 모델링은 다음 확률을 계산합니다:\n\nP(Y=1 | x, T=1) - P(Y=1 | x, T=0)\n\n여기서 Y는 우리의 대상 변수입니다 (0 — 판매되지 않음, 1 — 판매됨), x는 특징들의 집합이며, T는 할인이 적용되었는지 (T=1) 아닌지 (T=0)의 처리기본을 나타냅니다. 이 공식은 해당 제품을 할인을 적용하고 판매할 가능성의 차이 (얼프트)를 계산합니다. 다시 말해, 할인을 적용하면 어떻게 이 제품의 판매 기회가 높아질 수 있을까요? (인과관계)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 상승 점수를 계산하기 위한 모델링 접근 방식은 주로 다음과 같이 나열됩니다. 이 블로그 글에서는 이미 많은 정보가 있기 때문에 이러한 접근 방식의 세부 사항에 대해 깊이 파고들지 않겠습니다. 중요한 점 하나는 아래에 나열된 것을 '모델'이 아니라 '접근 방식'로 일컬어주고 있다는 것입니다. 그것은 이러한 접근 방식에서 선호하는 ML 모델을 기본 모델로 사용할 수 있기 때문입니다.\n\n## 접근 방식\n\n이것은 단일 모델 접근 방식으로, 처리 (T)를 다른 특성으로 사용하여 제품의 상승 점수를 계산합니다. 처리 특성 (T)이 1 및 0일 때의 예측 값 차이를 가져와 상승 점수를 계산합니다. 이 방법론의 주요 단점은 처리 (T) 특성이 단일 모델 (m)에서 가장 중요한 특성 중 하나로 편입되지 않는다는 것입니다.\n\n이것은 두 개의 모델 접근 방식으로, 할인 적용 그룹 데이터를 사용하여 하나의 모델을 훈련시키고, 통제 그룹 (할인을 적용하지 않음 - T=0) 데이터를 사용하여 다른 모델을 훈련시킵니다. 제품의 상승을 계산할 때, 이 두 모델 (m1 및 m0)의 결과를 차이를 가져와 상승을 계산합니다. 이 방법론의 주요 단점은 두 모델이 서로 다른 데이터 특성과 분포를 가지고 있기 때문에 비교되지 않은, 동기화되지 않은 점수를 받을 위험이 있다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nM1은 처리 그룹 데이터로 훈련된 모델이며, M0는 대조 그룹 데이터만으로 훈련된 다른 모델입니다.\n\n이것은 각 모델이 개별적인 처리 효과(ITE)를 학습하는 두 모델 접근법입니다. 첫 번째 단계는 T-러너와 동일하며, 두 번째 단계는 각 모델을 상대 그룹에 적용하고 다음과 같이 ITE를 계산하는 것입니다:\n\n그런 다음 마지막 단계는 앞서 계산된 ITE로 새로운 대상 변수로 사용하여 그 ITE로 다른 2개 모델을 훈련시키는 것입니다. 이 2개의 새로운 모델을 m1_ITE와 m0_ITE로 지정해봅시다. 최종 업리프트 점수를 계산하는 방법은 다음과 같습니다:\n\n여기서 g(x)는 경향 점수 가중 함수입니다. 예를 들어, 처리 그룹과 대조 그룹이 동일한 크기를 가지고 있다면, g(x)에 0.5 값을 사용할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n기본적으로 uplift 알고리즘은 분할 기준으로 uplift의 차이를 사용하는 트리 기반 알고리즘입니다. 각 분할은 부모 노드와 비교하여 자식 노드에서 처리 및 대조 그룹 간 결과 분포의 차이를 증가시키는 것을 목적으로 합니다.\n\n## Uplift 근사치\n\n실제 uplift 값을 계산하려면 아래의 uplift 공식에 표시된 대로 가능하지만, 동일한 제품에 동시에 할인 적용하거나 적용하지 않는 것 또는 동일한 사용자에게 동시에 할인 가격을 적용하거나 적용하지 않는 것은 실제로 현실 시나리오에서 항상 가능한 것은 아닙니다. 게다가 \"구매하지 않음\"이라는 정의가 명확하지 않은 경우, 손에 \"구매하지 않음\" 데이터가 없을 수 있습니다. 이 경우 실제 uplift 값을 대신할 근사치를 도입할 수 있습니다. 이 근사치를 실제 uplift 값 대신으로 생각할 수 있습니다. 손에 있는 것은 성공적인 구매 거래(Y=1) 데이터 뿐이며 이전 거래에 할인이 적용되었는지 여부를 알고 있는 경우입니다. (T=1 또는 T=0).\n\n다음은 성공적인 구매 거래만 사용하여 실제 uplift 값을 평가하기 위한 간단한 설명입니다 →\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Uplift Formula: P(Y=1 | x, T=1) - P(Y=1 | x, T=0)\n\n## Step I:\n\n- P(Y=1 | x, T=1) = [P(T=1 | Y=1, x) * P(Y=1 | x)] / P(T=1 | x)\n\n- P(Y=1 | x, T=0) = [P(T=0 | Y=1, x) * P(Y=1 | x)] / P(T=0 | x)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep II:\n\nP(Y=1 | x, T=1) / P(Y=1 | x, T=0) = P(T=1 | Y=1, x) / P(T=0 | Y=1, x)\n\nwhere P(T=1 | x) and P(T=0 | x) are 0.5 since we assume that the chance of a product which is treated (apply discount) or not treated (no apply discount) is equal.\n\nStep III:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nP(Y=1 | x, T=1) - P(Y=1 | x, T=0) = P(T=1 | Y=1, x) - [1-P(T=1 | Y=1, x)] = 2*P(T=1 | Y=1, x) - 1\n\n위 식은 II 단계에서 오른쪽 식을 사용하여 uplift 공식의 두 부분을 대체하는 데 사용됩니다.\n\n실제 uplift 공식의 최종 추정은 다음과 같습니다:\n\nP(Y=1 | x, T=1) - P(Y=1 | x, T=0) = 2*P(T=1 | Y=1, x) - 1\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거 성공한 거래 데이터만 사용하여 Y=1인 모델을 교육하면 해당 성공적 거래에 할인이 적용된 확률을 예측할 수 있습니다. 이제 이러한 데이터가 이미 존재하므로 이러한 모델을 교육하는 것은 더 현실적이고 실현 가능한 목표입니다.\n\n요약하면, 이 작은 응축 근사화 기법을 사용하여 할인이 판매에 미치는 인과 효과를 추정할 수 있습니다. 이러한 방식으로 거래의 인과 요인이 우리가 적용한 할인인지 여부에 대한 답을 얻게 됩니다. 내 의견으로는, 할인과 매출 사이에 강한 상관 관계 가정을 하는 주류 접근 방식보다 더 효과적인 모델링 접근 방식입니다.\n\n![Uplift Modeling](/assets/img/2024-05-23-UpliftModelingcorrelationtocausation_1.png)\n","ogImage":{"url":"/assets/img/2024-05-23-UpliftModelingcorrelationtocausation_0.png"},"coverImage":"/assets/img/2024-05-23-UpliftModelingcorrelationtocausation_0.png","tag":["Tech"],"readingTime":7},{"title":"K-평균 클러스터링 마스터하기","description":"","date":"2024-05-23 17:59","slug":"2024-05-23-MasteringK-MeansClustering","content":"\n\n## Python으로부터 K-Means 알고리즘을 처음부터 구현하는 단계별 튜토리얼\n\n![image](/assets/img/2024-05-23-MasteringK-MeansClustering_0.png)\n\n이 글에서는 오늘 처음 시작한다면 어떻게 K-Means 알고리즘을 배우는지 보여드립니다. 우리는 기본 개념부터 시작하여 Numpy 패키지만 사용하여 클러스터링 작업을 수행하는 Python 클래스를 구현할 것입니다.\n\n기계 학습을 처음 접하는 사람이라면 개념을 확실하게 이해하려고 노력하는 중이거나 알고리즘이 어떻게 작동하는지 이해해야 하는 맞춤형 기계 학습 응용 프로그램을 만들려는 실무자라면, 이 글은 여러분을 위한 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 1. 소개\n\n대부분의 기계 학습 알고리즘 중에는 선형 회귀, 로지스틱 회귀, 의사결정 트리 등이 있습니다. 이러한 알고리즘들은 레이블이 지정된 데이터로부터 예측을 만드는 데 유용합니다. 즉, 각 입력은 레이블 값과 함께 특성 값으로 구성됩니다. 이를 지도 학습이라고 합니다.\n\n그러나 종종 레이블이 지정되지 않은 대규모 데이터 집합과 다뤄야 할 때가 있습니다. 구매 행동, 인구 통계, 주소 등을 기반으로 고객의 다양한 그룹을 이해하여 더 나은 서비스, 제품 및 프로모션을 제공할 필요가 있는 비즈니스를 상상해보십시오.\n\n이러한 유형의 문제는 비지도 학습 기술을 사용하여 해결할 수 있습니다. K-평균 알고리즘은 기계 학습에서 널리 사용되는 비지도 학습 알고리즘입니다. 간단하고 우아한 방식으로 데이터 집합을 사용자가 원하는 K 개의 서로 다른 클러스터로 분리할 수 있으며, 이를 통해 레이블이 지정되지 않은 데이터에서 패턴을 학습할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. K-Means 알고리즘은 무엇을 하는가?\n\n이전에 말했듯이, K-Means 알고리즘은 데이터 포인트를 주어진 클러스터 수로 분할합니다. 각 클러스터 내의 포인트들은 유사하며, 다른 클러스터의 포인트들은 상당한 차이를 보입니다.\n\n그렇다면 한 가지 의문이 생깁니다: 유사성 또는 차이를 어떻게 정의할까요? K-Means 클러스터링에서 유사성을 측정하는 가장 일반적인 메트릭은 유클리드 거리입니다.\n\n아래 그림에서는 세 가지 다른 그룹이 명확히 보입니다. 따라서 각 그룹의 중심을 결정하고, 각 포인트는 가장 가까운 중심과 연관됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-MasteringK-MeansClustering_1.png\" /\u003e\n\n이렇게 함으로써, 수학적으로 말하면, 아이디어는 클러스터 내 분산을 최소화하는 것이며, 각 포인트와 가장 가까운 중심점 사이의 유사성을 측정하는 것입니다.\n\n위 예제를 수행하는 것은 데이터가 2차원이고 그룹이 명확하게 구분되어 있었기 때문에 간단했습니다. 그러나 차원의 수가 증가하고 다양한 K 값이 고려될 때, 복잡성을 다루기 위한 알고리즘이 필요합니다.\n\n## 단계 1: 초기 중심을 선택하세요 (랜덤으로)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알고리즘을 초기 중심 벡터로 시드화해해야 합니다. 이는 데이터에서 무작위로 선택하거나 원본 데이터와 같은 차원의 무작위 벡터를 생성할 수 있습니다. 아래 이미지에서 하얀 다이아몬드를 확인해주세요.\n\n![다이아몬드](/assets/img/2024-05-23-MasteringK-MeansClustering_2.png)\n\n## 단계 2: 각 점에서 중심까지 거리 찾기\n\n이제, 각 데이터 포인트에서 K 중심까지의 거리를 계산할 것입니다. 그리고 각 점을 해당 점에 가장 가까운 중심에 연관짓게 됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n데이터셋에는 N개의 항목과 M개의 피처가 있습니다. 센터 c까지의 거리는 다음 방정식에 의해 주어질 수 있습니다:\n\n![equation](/assets/img/2024-05-23-MasteringK-MeansClustering_3.png)\n\n여기서:\n\nk는 1부터 K까지 변합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nD는 점 n에서 k 센터까지의 거리입니다.\n\nx는 점 벡터입니다.\n\nc는 센터 벡터입니다.\n\n따라서 각 데이터 포인트 n마다 K개의 거리가 있고, 그런 다음 가장 작은 거리로 센터에 대한 벡터를 레이블해야 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![](/assets/img/2024-05-23-MasteringK-MeansClustering_4.png)\n\nD는 K 거리를 가진 벡터입니다.\n\n## 단계 3: K 중심점 찾기 및 반복\n\nK 개의 클러스터 각각에 대해 중심점을 재계산합니다. 새로운 중심점은 해당 클러스터에 할당된 모든 데이터 포인트의 평균입니다. 그런 다음 중심점의 위치를 새로 계산된 위치로 업데이트하세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 반복에서 중심접 중점이 크게 변했는지 확인하세요. 현재 반복에서 중심점의 위치를 이전 반복에서의 위치와 비교하여 확인할 수 있습니다.\n\n중심점이 크게 변했다면, 단계 2로 돌아가세요. 변하지 않았다면, 알고리즘이 수렴했고 프로세스가 중지됩니다. 아래 이미지를 참조해주세요.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1280/1*trbuwKohsyn_SZrWrk7fKw.gif)\n\n# 3. Python에서의 구현\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이제 K-Means 알고리즘의 기본 개념을 알게 되었으니, Python 클래스를 구현할 때입니다. 사용된 패키지는 수학적 계산을 위해 Numpy, 시각화를 위해 Matplotlib, 그리고 모의 데이터를 생성하기 위해 Sklearn의 Make_blobs 패키지였습니다.\n\n```python\n# 필요한 패키지 가져오기\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n```\n\n이 클래스에는 다음 메소드가 포함될 것입니다:\n\n- Init 메소드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n알고리즘의 기본 매개변수인 군집 값 k, 최대 반복 횟수 max_iter 및 최적화를 중지하는 허용 오차 tol 값을 초기화하는 생성자 메서드입니다.\n\n- 보조 함수\n\n이러한 방법은 훈련 중 최적화 프로세스를 지원하는 데 목적이 있으며, 유클리드 거리 계산, 초기 중심 임의 선택, 각 점에 가장 가까운 중심 할당, 중심값 업데이트 및 최적화가 수렴했는지 확인이 포함됩니다.\n\n- Fit 및 predict 메서드\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n앞서 말한 대로, K-Means 알고리즘은 지도 학습 기술이 아닌 비지도 학습 기술입니다. 이는 훈련 과정 중에 레이블이 지정된 데이터가 필요하지 않음을 의미합니다. 따라서 데이터를 적합시키고 각 데이터 포인트가 어떤 클러스터에 속하는지 예측하는 단일 방법이 필요합니다.\n\n- 총 오류 방법\n\n최적화의 품질을 평가하기 위해 최적화의 총 제곱 오류를 계산하는 방법입니다. 다음 섹션에서 자세히 알아볼 것입니다. \n\n다음은 전체 코드입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\nclass Kmeans:\n    \n    # 하이퍼파라미터 초기화를 위한 생성자 메소드\n    def __init__(self, k=3, max_iter=100, tol=1e-06):\n        self.k = k\n        self.max_iter = max_iter\n        self.tol = tol\n  \n    # 초기 중심점을 입력 데이터에서 무작위로 선택\n    def pick_centers(self, X):\n        centers_idxs = np.random.choice(self.n_samples, self.k)\n        return X[centers_idxs]\n    \n    # 각 데이터 포인트에 대해 가장 가까운 중심점을 찾음\n    def get_closest_centroid(self, x, centroids):\n        distances = [euclidean_distance(x, centroid) for centroid in centroids]\n        return np.argmin(distances)\n    \n    # 각 클러스터의 인덱스를 포함하는 리스트를 생성\n    def create_clusters(self, centroids, X):\n        clusters = [[] for _ in range(self.k)]\n        labels = np.empty(self.n_samples)\n        for i, x in enumerate(X):\n            centroid_idx = self.get_closest_centroid(x, centroids)\n            clusters[centroid_idx].append(i)\n            labels[i] = centroid_idx\n\n        return clusters, labels\n    \n    # 각 클러스터에 대해 평균 값을 사용하여 중심점을 계산\n    def compute_centroids(self, clusters, X):\n        centroids = np.empty((self.k, self.n_features))\n        for i, cluster in enumerate(clusters):\n            centroids[i] = np.mean(X[cluster], axis=0)\n\n        return centroids\n    \n    # 중심점이 크게 변경되었는지 확인하는 헬퍼 함수\n    def is_converged(self, old_centroids, new_centroids):\n        distances = [euclidean_distance(old_centroids[i], new_centroids[i]) for i in range(self.k)]\n        return (sum(distances) \u003c self.tol)\n\n\n    # 데이터를 학습하고 최적화된 중심점을 찾아 각 데이터 포인트를 클러스터에 따라 레이블링하는 메소드\n    def fit_predict(self, X):\n        self.n_samples, self.n_features = X.shape\n        self.centroids = self.pick_centers(X)\n\n        for i in range(self.max_iter):\n            self.clusters, self.labels = self.create_clusters(self.centroids, X)\n            new_centroids = self.compute_centroids(self.clusters, X)\n            if self.is_converged(self.centroids, new_centroids):\n                break\n            self.centroids = new_centroids\n\n    \n    # 최적화의 클러스터 내 분산을 평가하는 메소드\n    def clustering_errors(self, X):\n        cluster_values = [X[cluster] for cluster in self.clusters]\n        squared_distances = []\n        # 총 제곱 유클리드 거리 계산\n        for i, cluster_array in enumerate(cluster_values):\n            squared_distances.append(np.sum((cluster_array - self.centroids[i])**2))\n\n        total_error = np.sum(squared_distances)\n        return total_error\n```\n\n# 4. 평가와 해석\n\n이제 시뮬레이션 데이터의 클러스터링을 수행하기 위해 K-Means 클래스를 사용합니다. Sklearn 라이브러리의 make_blobs 패키지를 사용할 것입니다. 데이터는 4개의 고정된 중심점을 가진 500개의 이차원 점으로 구성됩니다.\n\n```js\n# 예시용 시뮬레이션 데이터 생성\nX, _ = make_blobs(n_samples=500, n_features=2, centers=4, \n                  shuffle=False, random_state=0)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이미지:\n![마스터링K-Means 클러스터링](/assets/img/2024-05-23-MasteringK-MeansClustering_5.png)\n\n네가 네가 이미지:\n![클러스터링 결과](https://miro.medium.com/v2/resize:fit:1280/1*JBTi_DXtCz_NzOhDZzMEag.gif)\n\n네가 네가 네가 네가 별 다섯 이미지:\n\n훈련을 통해 네 개의 클러스터를 사용하여 다음과 같은 결과를 얻을 수 있습니다.\n\n```js\nmodel = Kmeans(k=4)\nmodel.fit_predict(X)\nlabels = model.labels\ncentroids = model.centroids\nplot_clusters(X, labels, centroids)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 경우 알고리즘은 18번의 반복을 통해 성공적으로 클러스터를 계산할 수 있었습니다. 그러나 시뮬레이션된 데이터에서 최적 클러스터 수를 이미 알고 있다는 점을 명심해야 합니다. 실제 응용 프로그램에서는 종종 그 값을 모르게 됩니다.\n\n이전에 말했듯이, K-Means 알고리즘은 클러스터 내 분산을 가능한 한 작게 만드는 것을 목표로 합니다. 그 분산을 계산하는 데 사용되는 메트릭은 다음과 같은 총 제곱 유클리드 거리입니다:\n\n![식](/assets/img/2024-05-23-MasteringK-MeansClustering_6.png)\n\n여기서:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n클러스터 내 데이터 포인트의 수를 p로 표시하고;\n\n클러스터의 중심 벡터를 c_i로 나타내며;\n\nK는 클러스터의 수입니다.\n\n위의 공식은 데이터 포인트와 가장 가까운 중심까지의 거리들을 합한 것입니다. K의 수가 증가할수록 오차가 줄어듭니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n에러를 클러스터 수에 대한 그래프로 그려서 그래프가 \"접히는\" 지점을 살펴보면, 최적의 클러스터 수를 찾을 수 있어요.\n\n![Plot](/assets/img/2024-05-23-MasteringK-MeansClustering_7.png)\n\n그래프가 \"팔꿈치 모양\"을 하고 있으며 K = 4에서 접히고 있으니, K의 값이 클수록 총 에러 감소가 덜 중요해질 것이라는 것을 볼 수 있어요.\n\n# 5. 결론과 다음 단계\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 글에서는 K-Means 알고리즘의 기본 개념, 사용 사례 및 응용 프로그램에 대해 다루었습니다. 또한 이러한 개념을 사용하여 시뮬레이션 데이터의 클러스터링을 수행하는 Python 클래스를 처음부터 구현했으며, scree 플롯을 사용하여 K의 최적 값을 찾는 방법을 알아보았습니다.\n\n그러나 이는 비지도 학습 기술을 다루고 있기 때문에 한 가지 추가 단계가 있습니다. 알고리즘은 성공적으로 클러스터에 레이블을 할당할 수 있지만, 각 레이블의 의미는 데이터 과학자나 기계 학습 엔지니어가 각 클러스터의 데이터를 분석하여 수행해야 할 작업입니다.\n\n또한, 추가로 탐구할만한 몇 가지 포인트를 남겨 드리겠습니다:\n- 우리의 시뮬레이션 데이터는 이차원 점을 사용했습니다. 알고리즘을 다른 데이터 세트에도 적용하고 K의 최적 값을 찾아보세요.\n- 계층적 클러스터링과 같은 다른 널리 사용되는 비지도 학습 알고리즘도 있습니다.\n- 문제의 도메인에 따라 맨하탄 거리 및 코사인 유사성과 같은 다른 오류 지표를 사용해야 할 수도 있습니다. 이를 조사해 보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음은 전체 코드가 있는 링크입니다:\n\n코드를 자유롭게 사용하고 개선하고, 의견을 나누고, LinkedIn, X, 그리고 Github에서 저와 연락하십시오.\n\n# 참고문헌\n\n[1] Sebastian Raschka (2015), Python Machine Learning.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2] Willmott, Paul. (2019). Machine Learning: An Applied Mathematics Introduction. Panda Ohana Publishing.\n\n[3] Géron, A. (2017). Hands-On Machine Learning. O’Reilly Media Inc.\n\n[4] Grus, Joel. (2015). Data Science from Scratch. O’Reilly Media Inc.","ogImage":{"url":"/assets/img/2024-05-23-MasteringK-MeansClustering_0.png"},"coverImage":"/assets/img/2024-05-23-MasteringK-MeansClustering_0.png","tag":["Tech"],"readingTime":9},{"title":"LLM 애플리케이션 개발 평가 파트 8","description":"","date":"2024-05-23 17:47","slug":"2024-05-23-BuildingLLMApplicationsEvaluationPart8","content":"\nLearn Large Language Models (LLM) through the lens of a Retrieval Augmented Generation (RAG) Application.\n\n# 이 시리즈의 게시물\n\n- 소개\n- 데이터 준비\n- 문장 변형기\n- 벡터 데이터베이스\n- 검색 및 검색\n- LLM\n- 오픈 소스 RAG\n- 평가 (이 게시물)\n- LLM 제공\n- 고급 RAG\n\n# 목차\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 콘텐츠 요약\n\n1. 개요\n2. LLM 평가와 벤치마킹의 비교\n3. LLM 벤치마킹\n   - 언어 이해 및 QA 벤치마킹\n     - TruthfulQA\n     - MMLU (Massive Multitask Language Understanding)\n     - DROP\n   - 상식 및 추론 벤치마킹\n     - ARC (AI2 Reasoning Challenge)\n     - HellaSwag\n     - BIG-Bench Hard (Imitation Game 벤치마크 이상)\n     - WinoGrande\n     - GSM8k\n   - 코딩 벤치마킹\n     - HumanEval\n     - CodeXGLUE\n   - 대화 및 챗봇 벤치마킹\n     - Chatbot Arena (by LMSys)\n     - MT Bench\n     - Language Model Evaluation Harness (by EleutherAI)\n     - Stanford HELM\n     - PromptBench (by Microsoft)\n4. LLM 벤치마크의 제한\n5. LLM 평가 지표\n6. 지표 점수 계산 방법\n   - 통계 평가자\n     - 음성 오류율 (WER)\n     - 정확도 일치\n     - 난해함\n     - BLEU\n     - ROUGE\n     - METEOR\n   - 모델 기반 평가자\n     - 추론 점수\n     - BLEURT\n     - QA-QG\n   - LLM-Evals\n     - G-Eval\n     - Prometheus\n   - 통계 및 모델 기반 평가자 결합\n     - BERTScore\n     - MoverScore\n     - GPTScore\n     - SelfCheckGPT\n     - QAG Score\n7. LLM 기반 애플리케이션 평가\n   - 평가 지표 선택\n   - 평가 방법 평가!\n   - 평가 세트 작성\n8. LLM 평가 프레임워크\n   - Deepeval\n     - 충실성\n     - 답변 관련성\n     - 문맥 정확도\n     - 문맥 호출\n     - 문맥 관련성\n   - 파인튜닝 지표\n     - 환각\n     - 유해성\n     - 편견\n   - Ragas\n     - 충실성\n     - 답변 관련성\n     - 문맥 정확도\n     - 문맥 관련성\n     - 문맥 호출\n9. 결론\n10. 제작진\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_0.png)\n\n이전 블로그에서 여러 개의 RAG 애플리케이션을 성공적으로 구축했습니다. 이젠 해당 애플리케이션을 평가하는 과정을 알아봅시다. 우리가 대규모 언어 모델로 생성된 결과의 신뢰성에 대해 살펴봅시다. 우리는 전통적인 머신 러닝, 딥 러닝 및 LLM의 차이점을 아래 표를 통해 살펴보겠습니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_1.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM의 등장으로 이전에는 불가능하다고 여겨졌던 문제들을 해결할 수 있는 장이 열렸습니다. 하지만 한 가지 의문이 남아있습니다. LLM 기반 애플리케이션을 효과적으로 평가하는 방법은 무엇일까요?\n\n이 기사를 통해 LLM을 평가하는 방법과 최신 기술들, 사용 가능한 프레임워크, 그리고 LLM 기반 애플리케이션을 평가하는데 있는 어려움에 대해 알아보겠습니다.\n\n# 1. 개요\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n𝗟𝗹𝗠(𝗟𝗮𝗿𝗴𝗲 𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗠𝗼𝗱𝗲𝗹)을 𝘂𝘀𝗲𝗿𝗳𝘂𝗹𝗹𝘆 𝗲𝘃𝗮𝗹𝘂𝗮𝘁𝗲 𝗵𝗮𝗿𝗱 𝗈𝗇 𝗈𝗂𝘃𝗂𝗇𝗀 𝖺 𝗅𝖺𝗋𝗀𝖾 𝗅𝖺𝗇𝗀𝗎𝖺𝗀𝖾 𝗆𝗈𝖽𝖾𝗅 𝖻𝗎𝗍 𝗇𝗈𝗍 𝗌𝗎𝗋𝖾 where to start? 𝖧𝖾𝗋𝖾’𝗌 𝗒𝗈𝗎𝗋 𝖾𝗌𝗌𝖾𝗇𝗍𝗂𝖺𝗅 𝗀𝗎𝗂𝖽𝖾! 🚀\n\nModel selection is crucial, as it will impact the result of your project. 𝖳𝗁𝖾𝗇, 𝗇𝖾𝗑𝗍 𝗌𝗍𝖾𝗉 𝗂𝗌 𝗁𝗈𝗐 𝗍𝗈 𝖾𝗏𝖺𝗅𝗎𝖺𝗍𝖾 𝗍𝗁𝖺𝗍 𝗆𝗈𝖽𝖾𝗅. 𝖳𝗁𝖾𝗇, 𝗇𝖾𝗑𝗍 𝗌𝗍𝖾𝗉 𝗂𝗌 𝗁𝗈𝗐 𝗍𝗈 𝖾𝗏𝖺𝗅𝗎𝖺𝗍𝖾 𝗍𝗁𝖺𝗍 𝗆𝗈𝖽𝖾𝗅. 𝗆𝖺𝗇𝗒 𝗉𝗋𝖺𝖼𝗍𝗂𝗍𝗂𝗈𝗇𝖾𝗋𝗌 𝗂𝗇𝗂𝗍𝗂𝖺𝗅𝗅𝗒 𝗋𝖾𝗅𝗂𝖾𝗌 𝗈𝗇 𝗉𝗋𝗈𝗆𝗉𝗍 𝖾𝗇𝗀𝗂𝗇𝖾𝖾𝗋𝗂𝗇𝗀 𝖺𝗌 𝖺 𝗆𝖾𝗍𝗁𝗈𝖽 𝗍𝗈 𝖾𝗏𝖺𝗅𝗎𝖺𝗍𝖾 𝗍𝗁𝖾𝗂𝗋 𝗆𝗈𝖽𝖾𝗅 𝖼𝗁𝗈𝗂𝖼𝖾. 𝖡𝗎𝗍 𝖺 𝗆𝗈𝗋𝖾 𝖼𝗈𝗆𝗉𝗋𝖾𝗁𝖾𝗇𝗌𝗂𝗏𝖾 𝖾𝗏𝖺𝗅𝗎𝖺𝗍𝗂𝗈𝗇 𝗌𝗍𝗋𝖺𝗍𝖾𝗀𝗒 𝗂𝗌 𝗇𝖾𝖼𝖾𝗌𝗌𝖺𝗋𝗒.\n\nModel evaluation is complex, involving various metrics that cater to different priorities — whether it’s prioritizing accuracy, cost-efficiency, or performance. 𝖳𝗁𝖾 𝖽𝗂𝗋𝖾𝖼𝗍𝗂𝗈𝗇 𝗒𝗈𝗎 𝖼𝗁𝗈𝗈𝗌𝖾 𝗌𝗁𝗈𝗎𝗅𝖽 𝖺𝗅𝗂𝗀𝗇 𝗐𝗂𝗍𝗁 𝗒𝗈𝗎𝗋 𝗌𝗉𝖾𝖼𝗂𝖿𝗂𝖼 𝗇𝖾𝖾𝖽𝗌, 𝖾𝗇𝗌𝗎𝗋𝗂𝗇𝗀 𝗍𝗁𝖺𝗍 𝗍𝗁𝖾 𝗆𝗈𝖽𝖾𝗅 𝖺𝗒𝗈𝗎 𝗌𝖾𝗅𝖾𝖼𝗍 𝗇𝗈𝗍 𝗈𝗇𝗅𝗒 𝖿𝗂𝗍𝗌 𝖻𝗎𝗍 𝖾𝗇𝗁𝖺𝗇𝖼𝖾𝗌 𝗒𝗈𝗎𝗋 𝗎𝗌𝖾 𝖼𝖺𝗌𝖾.\n\n𝖧𝖾𝗋𝖾’𝗌 𝖺 𝗌𝗍𝗋𝖾𝖺𝗆𝗅𝗂𝗇𝖾𝖽 𝖺𝗉𝗉𝗋𝗈𝖺𝖼𝗁 𝗍𝗈 𝖾𝗏𝖺𝗅𝗎𝖺𝗍𝗂𝗇𝗀 𝗟𝗹𝗠𝗌 𝗐𝗁𝖾𝗇 𝗒𝗈𝗎’𝗋𝖾 𝖽𝖾𝖼𝗂𝖽𝗂𝗇𝗀 𝗈𝗇 𝗍𝗁𝖾 𝖻𝖾𝗌𝗍 𝖿𝗂𝗍 𝖿𝗈𝗋 𝗈𝗎𝗋 𝖺𝗉𝗉𝗅𝗂𝖼𝖺𝗍𝗂𝗈𝗇 (𝗆𝗈𝗋𝖾 𝖽𝖾𝗍𝖺𝗂𝗅𝗌 𝗂𝗇 𝗍𝗁𝖾 𝖺𝖻𝗈𝗏𝖾 𝖽𝗂𝖺𝗀𝗋𝖺𝗆):\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n1️⃣ **모델 평가 방법 선택하기?**\n✅ **예**: 모델이 이산 출력을 제공하는 경우, 전통적인 정확도 지표를 사용하십시오. 그렇지 않은 경우, ROUGE와 같은 다른 유사성 지표를 고려하십시오.\n❌ **아니요**: 2️⃣단계로 이동하세요.\n\n2️⃣ **LLM 판단자 또는 평가자를 구현해야 합니까?**\n✅ **예**: LLM 판단자 또는 평가자를 구현하십시오.\n❌ **아니요**: 3️⃣단계로 이동하세요.\n\n3️⃣ **텍스트 평가자 선택하기?**\n✅ **예**: 텍스트 품질, 가독성 또는 불가해성과 같은 독립적인 측정 항목을 선택하십시오.\n❌ **아니요**: 인간 평가가 가장 신뢰할 수 있는 피드백을 제공하지만 시간이 많이 소요되고 더 많은 비용이 듭니다. 하지만 인간 입력이 필요하기 때문에 가장 확실한 피드백을 제공합니다.\n\n큰 언어 모델을 위한 평가 도구를 탐색 중이신가요? Amazon Bedrock가 좋은 선택일 수 있습니다. 여러분의 요구 사항에 맞는 다양한 유연한 옵션을 제공합니다:\n\n🔹 **성능 평가**: 비용, 지연 및 사용 설정을 기반으로 비교하기\n🔹 **프로그래밍 평가**: 사용 사례나 LLM을 반복하면서 프로그래밍 평가 사용하기\n🔹 **사용자 테스트**: 첫번째 프로토타입 테스트를 시작하기 위해 인간 평가자들을 참여시키기\n\n아래에서 이러한 기술들에 대해 자세하게 논의해보도록 합시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 2. LLM Benchmarking 대 평가\n\nLLM 벤치마킹과 평가는 얽혀 있지만, 그들의 목적 사이에는 섬세한 차이가 있습니다:\n\n벤치마킹은 표준화된 테스트에 관한 것입니다. 특정 작업에서 LLM의 성능을 평가하기위해 미리 정의된 데이터 세트와 측정 항목을 사용하는 것을 의미합니다. 이는 언어 모델에 읽기, 쓰기 및 수학 (하지만 언어를 위한 것!) 과제 집합을 제공하는 것과 같습니다.\n\n여기에 벤치마킹이 제공하는 것은:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n비교: 벤치마킹은 연구자들이 다른 LLM의 성능을 동일한 작업에서 비교할 수 있게 합니다. 이는 어떤 모델이 특정 영역에서 뛰어나다는 것을 확인하는 데 도움이 됩니다.\n측정 결과: 벤치마킹은 명확한 그림을 제공하는 수치적으로 측정된 점수들을 제공하여 LLM의 강점과 약점을 보여줍니다.\n\n반면에, 평가는 좀 더 넓은 범위를 갖습니다. 이는 단순히 테스트를 실행하는 것을 넘어 LLM의 능력을 보다 포괄적으로 평가하는 것을 의미합니다. 여기서 연구자들은 다음을 고려합니다:\n\n실제 세계 적용 가능성: LLM이 실제 사용 사례를 모방하는 상황에서 얼마나 잘 수행되는가?\n공정성과 편견: LLM이 출력물에서 편견을 나타내는가?\n해석 가능성: 연구자들은 LLM이 답변에 도달하는 방법을 이해할 수 있는가?\n\n평가는 종종 벤치마킹이 마련한 기초 위에 구축됩니다. 연구자들은 벤치마크 점수를 시작점으로 사용할 수 있지만, 벤치마크가 반드시 잡아내지 못하는 측면들을 더 깊이 파고들 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n간단한 용어로 말하면, 벤치마킹은 표준화된 테스트를 사용하여 양적 평가를 제공하며, 평가는 LLM의 전체적인 장단점 및 현실 세계 적용에 대한 적합성에 대한 좀 더 질적인 이해를 제공합니다.\n\n# 3. LLM 벤치마킹\n\n![LLM 벤치마크](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_3.png)\n\nLLM 벤치마크는 추론 및 이해력과 같은 다양한 기술에 대한 LLM의 성능을 평가하기 위해 설계된 일련의 표준화된 테스트이며, 이 능력을 측정하기 위해 특정 점수자나 지표를 활용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n벤치마크에 따라 메트릭은 정확한 일치 비율과 같은 통계 기반 측정부터 다른 LLM들에 의해 평가되는 더 복잡한 메트릭까지 다양할 수 있습니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_4.png)\n\n다양한 벤치마크는 모델 능력의 다양한 측면을 평가합니다. 이러한 측면에는 다음이 포함됩니다:\n\n- 추론 및 상식: 이러한 벤치마크는 LLM이 논리를 적용하고 일상 지식을 활용하여 문제를 해결하는 능력을 평가합니다.\n- 언어 이해 및 질문 답변(QA): 이는 모델이 텍스트를 해석하고 정확하게 질문에 대답하는 능력을 평가합니다.\n- 코딩: 이 벤치마크는 LLM의 코드 해석 및 생성 능력을 평가합니다.\n- 대화 및 챗봇: 이러한 벤치마크는 LLM의 대화에 참여하고 일관된 유의미한 응답을 제공하는 능력을 시험합니다.\n- 번역: 이러한 벤치마크는 모델이 한 언어에서 다른 언어로 텍스트를 정확하게 번역하는 능력을 평가합니다.\n- 수학: 이 벤치마크는 기본 산술부터 미적분과 같은 더 복잡한 수학 영역까지 모델이 수학 문제를 해결하는 능력에 중점을 둡니다.\n- 논리: 논리 벤치마크는 모델이 귀납 및 추론과 같은 논리적 추론 기술을 적용하는 능력을 평가합니다.\n- 표준화된 시험: SAT, ACT 또는 기타 교육 평가도 모델의 성능을 평가하고 벤치마크 하는 데 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![LLM Benchmarking](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_5.png)\n\n일부 벤치마크는 수십 개의 테스트만 포함할 수도 있고, 다른 것은 수백 개 또는 수천 개의 작업을 포함할 수도 있습니다. 중요한 점은 LLM 벤치마킹이 서로 다른 도메인과 작업에서 LLM 성능을 평가하기 위한 표준화된 프레임워크를 제공한다는 것입니다.\n\n프로젝트에 적합한 적절한 벤치마크를 선택하는 것은 다음을 의미합니다:\n\n- 목표에 부합: LLM이 뛰어날 필요가 있는 구체적인 작업과 일치하는지 확인하는 것.\n- 작업 다양성 수용: 광범위한 작업 스펙트럼을 갖춘 벤치마크를 찾아 LLM을 다각도로 평가하는 것.\n- 도메인에 부합: 언어 이해, 텍스트 생성 또는 코딩과 같이 애플리케이션의 세계와 조화를 이루는 벤치마크를 선택하는 것.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n고등학생들을 위한 SAT 같은 것이 LLM에 대한 것이라고 생각해 보세요. 그들은 모델 능력의 모든 가능한 측면을 평가할 수는 없지만 확실히 가치 있는 통찰력을 제공합니다. 이제 Claude 3의 성능이 여러 벤치마크를 통해 다른 SOTA 모델들과 어떻게 비교되는지 알아보겠습니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_6.png)\n\n다음 섹션에서는 4가지 가장 중요한 영역(언어 이해, 추론, 코딩, 대화) 전반에 걸친 주요 LLM 벤치마크를 논의할 것입니다. 이러한 벤치마크들은 산업 응용 프로그램에서 널리 활용되며 기술 보고서에서 자주 인용됩니다. 이들은 다음을 포함합니다:\n\n## 3.1. 언어 이해 및 QA 벤치마크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3.1.1. 진실QA\n\n[2022년 발행] ∙ 논문 ∙ 코드 ∙ 데이터셋\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_7.png\" /\u003e\n\n목적\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 정확하고 진실한 답변을 제공하는 능력을 기반으로 모델을 평가합니다.\n- 잘못된 정보를 대항하고 윤리적 AI 사용을 촉진하는 데 중요합니다.\n\n데이터셋\n\n- 원본 데이터셋은 38개 카테고리에 걸쳐 총 817개의 질문으로 구성되어 있습니다.\n- 카테고리에는 건강, 법률, 금융, 정치 등이 포함됩니다.\n- 질문은 사람들이 거짓된 믿음이나 오해로 인해 잘못된 대답을 제공할 수 있는 영역을 대상으로 합니다.\n\n성과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 원본 논문에서 성능이 가장 우수했던 모델인 GPT-3은 인간의 기준치인 94%에 비해 단 58%의 성공률을 달성했습니다.\n\n점수 부여\n\n- 최종 점수는 모델이 생성하는 진실한 출력의 비율에 기초하여 계산됩니다.\n- \"GPT-Judge\"로 불리는 파인튜닝된 GPT-3이 답변의 진실성을 판단하는 데 사용됩니다.\n\n## 3.1.2. MMLU (Massive Multitask Language Understanding)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2021년에 발표됨] ∙ 논문 ∙ 코드 ∙ 데이터셋\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_8.png)\n\n- 목표: 사전 훈련 지식을 기반으로 모델을 평가하며, 제로샷 및 퓨샷 설정에 중점을 둠.\n- 벤치마크: 다양한 주제(과학, 인문학, 사회과학 등 57가지)의 객관식 문제를 통해 모델을 평가하는 포괄적인 벤치마크로, 초급부터 고급 단계까지의 난이도를 제시함.\n- 지식 간격 식별: 다양하고 상세한 주제 범위로, 특정 영역의 모델 지식에 어떤 간격이 있는지 확인하는 데 이 벤치마크가 완벽합니다.\n- 점수 매기기: MMLU는 대형 언어 모델(LLM)을 단순히 정답 비율을 기반으로 점수를 매김. 출력은 정확해야만 올바르다고 간주됨(위 예시의 ‘D’).\n\nMMLU가 접근하기 어려워 보인다면, 좋은 소식이 있어요. DeepEval 내에서 몇 가지 주요 벤치마크가 구현되어 있어, 우리가 선택한 어떤 LLM도 몇 줄의 코드로 쉽게 벤치마킹할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n먼저 DeepEval을 설치하세요:\n\n```js\npip install deepeval\n```\n\n그리고 벤치마크를 실행하세요:\n\n```js\nfrom deepeval.benchmarks import MMLU\nfrom deepeval.benchmarks.tasks import MMLUTask\n\n# 특정 작업 및 샷을 가진 벤치마크 정의\nbenchmark = MMLU(\n    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n    n_shots=3\n)\n\n# 'mistral_7b'를 사용자 지정 모델로 바꿔주세요\nbenchmark.evaluate(model=mistral_7b)\nprint(benchmark.overall_score)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n더 많은 구현 세부사항을 보려면 DeepEval MMLU 문서를 방문해주세요.\n\n## 3.1.3. DROP\n\n- 설명: DROP은 LLMs가 문단 내 이산적 추론을 수행해야 합니다. 이는 질문에서 참조 해결 및 덧셈, 계산, 또는 정렬과 같은 작업 수행을 포함하며 문단 내용에 대한 포괄적 이해가 필요합니다.\n- 평가 설정: 3-shot\n- 메트릭: 9536개의 문단 이해 질문에 대한 F1 점수\n- 논문: DROP: 문단에 대한 이산적 추론이 필요한 독해 평가\n\n열린 LLM 리더보드의 일곱 가지 주요 벤치마크 작업을 마무리합니다. 이 테스트들은 LLM의 지식 뿐만 아니라 추론, 이해 및 문제 해결 능력도 평가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n마크다운 형식으로 변경:\n\nAdditional noteworthy language understanding and QA benchmarks: GLUE, SuperGLUE, SQuAD, and GPT tasks, CoQA, QuAC, TriviaQA\n\n### 3.2. Common-sense and Reasoning Benchmarks\n\n#### 3.2.1. ARC (AI2 Reasoning Challenge)\n\n[Published in 2018] ∙ Paper ∙ Code\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 설명: AI2 Reasoning Challenge (ARC)는 초등학교 수준의 다항식 과학 문제들을 간단한 것부터 복잡한 것까지 포함하는 LLMs를 테스트합니다. 예를 들어, \"광합성은 식물 성장을 돕기 위해 무엇을 생산합니까?\"와 같은 질문이 있을 수 있으며 (a) 물 (b) 산소 (c) 단백질 (d) 설탕으로 선택지가 주어집니다.\n- 평가 설정: 25번 시도\n- 측정 항목: 3548개의 질문에 대한 정확도로, 그 중 33%가 도전적인 것으로 지정됩니다.\n- 논문: 당신이 질문에 답했다고 생각하십니까? ARC, AI2 Reasoning Challenge를 시도해보세요.\n\n데이터셋은 681MB이며 두 세트로 구분된 질문으로 구성되어 있습니다:\n\n- ARC-Easy\n- ARC-Challenge\n\n예시 질문:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_9.png)\n\n질문이 있고, 여러 선택지와 정답이 있습니다.\n\n## 3.2.2. HellaSwag\n\n[2019년 발표] ∙ 논문 ∙ 코드 ∙ 데이터셋\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_10.png)\n\nHellaSwag는 문장 완성을 통해 LLM 모델의 상식 추론 능력을 평가합니다. LLM 모델이 4개의 선택지 중에서 적절한 끝을 선택할 수 있는지를 테스트합니다. 이는 10,000개의 문장에 걸쳐 진행됩니다.\n\n당시 SOTA 모델은 사전 훈련을 통해 50% 이상을 달성하기 어려웠지만, GPT-4는 2023년 10번의 프롬프팅만으로 95.3%의 기록을 세웠습니다. MMLU와 유사하게, HellaSwag는 LLM의 정확 답변 비율에 따라 점수를 매깁니다.\n\nDeepEval을 통해 HellaSwag 벤치마크를 활용하는 방법은 다음과 같습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom deepeval.benchmarks import HellaSwag\nfrom deepeval.benchmarks.tasks import HellaSwagTask\n\n# 특정 작업 및 샷으로 벤치마크 정의\nbenchmark = HellaSwag(\n    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.BATON_TWIRLING],\n    n_shots=5\n)\n\n# 'mistral_7b'를 사용자 지정 모델로 바꿔주세요\nbenchmark.evaluate(model=mistral_7b)\nprint(benchmark.overall_score)\n```\n\n더 많은 정보를 얻으려면 DeepEval의 HellaSwag 문서 페이지를 방문해보세요.\n\n## 3.2.3. BIG-Bench Hard (Beyond the Imitation Game Benchmark)\n\n[2022년 발표] 논문 ∙ 코드 ∙ 데이터셋\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBIG-Bench Hard (BBH)은 원래 BIG-Bench 스위트에서 23가지 도전 과제를 선정했습니다. 이는 이미 언어 모델의 능력을 초과하는 204개의 평가 과제로 구성되어 있었습니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_11.png)\n\nBIG-Bench가 발표된 시점에서 SOTA 언어 모델 중 한 가지도 이 23가지 과제 중 아무 것도 인간평가자의 평균을 넘지 못했습니다. 흥미로운 점은 BBH의 저자들이 동일한 LLM을 사용하여 이 23가지 과제 중 17가지에서 인간을 능가할 수 있었습니다. Chain-of-Thought (CoT) 프롬프팅을 사용했기 때문입니다.\n\nBBH의 예상 출력은 다른 객관식 문제 기반 벤치마크보다 훨씬 다양하지만, 정확한 일치의 비율에 따라 모델을 평가합니다. CoT 프롬프팅은 모델의 출력을 예상된 형식으로 제한하는 데 도움을 줍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nBBH 벤치마크를 사용하려면:\n\n```js\nfrom deepeval.benchmarks import BigBenchHard\nfrom deepeval.benchmarks.tasks import BigBenchHardTask\n\n# 특정 작업과 샷을 가진 벤치마크 정의\nbenchmark = BigBenchHard(\n    tasks=[BigBenchHardTask.BOOLEAN_EXPRESSIONS, BigBenchHardTask.CAUSAL_JUDGEMENT],\n    n_shots=3,\n    enable_cot=True\n)\n\n# 'mistral_7b'를 사용자 정의 모델로 교체\nbenchmark.evaluate(model=mistral_7b)\nprint(benchmark.overall_score)\n````\n\n다시 한번 DeepEval의 BBH 문서 페이지에서 더 많은 정보를 얻을 수 있습니다.\n\n## 3.2.4. WinoGrande\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 설명: WinoGrande는 인공 지능의 상식적 추론을 테스트하며, 모델이 유인오그라드 스키마 도전 과제(WSC)를 해결하도록 도전합니다. 예를 들어, 한 가지 작업은 다음 문장을 완성하는 것을 포함할 수 있습니다: \"문이 창문보다 더 큰 소리로 열렸습니다 왜냐하면 \\_\\_\\_(옵션: 문 또는 창문)에 더 많은 기름이 있었기 때문입니다.\"\n- 평가 설정: 5번 시도\n- 측정 항목: 1267개의 질문에 대한 정확도\n- 논문: WinoGrande: 규모의 있는 적대적 유인오그라드 스키마 도전 과제\n\n## 3.2.5. GSM8k\n\n- 설명: GSM8K는 LLMs의 다단계 수학적 추론을 테스트하기 위해 초등학교 수학 문제를 제시합니다. 예를 들어: \"한 벌의 파란색 섬유가 2개의 두께와 그 절반만큼의 흰색 섬유가 있습니다. 총으로 몇 개의 두께이 필요합니까?\" 정답은 3입니다.\n- 평가 설정: 5번 시도\n- 측정 항목: 1319개의 질문에 대한 정확도\n- 논문: Training Verifiers to Solve Math Word Problems\n\n추가 언급할 만한 상식적 추론 벤치마크: CommonsenseQA, COPA, SNLI, MultiNLI, RACE, ANLI, PIQA, COSMOS QA\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 3.3. 코딩 평가\n\n## 3.3.1. HumanEval\n\n[2021년 발표됨] 논문 ∙ 코드 ∙ 데이터셋\n\nHumanEval은 모델의 코드 생성 능력을 평가하기 위해 설계된 164가지 독특한 프로그래밍 과제로 구성되어 있습니다. 이러한 과제는 알고리즘부터 프로그래밍 언어의 이해에 이르기까지 다양한 스펙트럼을 다룹니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래는 HumanEval과 유사한 컬렉션에 있는 예제 작업과 모델이 생성한 솔루션입니다:\n\n생성된 코드:\n\n```python\ndef sum_list(numbers: List[float]) -\u003e float:\n    return sum(numbers)\n```\n\nHumanEval은 생성된 코드의 품질을 Pass@k Metric을 사용하여 평가합니다. 이 메트릭은 기본적인 텍스트 유사성뿐만 아니라 기능적 정확성을 강조하기 위해 설계되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 3.3.2. CodeXGLUE\n\n[2021년에 발표됨] 논문 ∙ 코드 ∙ 데이터셋\n\n![CodeXGLUE 이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_12.png)\n\nCodeXGLUE는 Microsoft, Developer Division, Bing의 협력으로 개발되었으며 코드 완성, 코드 번역, 코드 요약 및 코드 검색과 같은 다양한 코딩 시나리오에서 모델을 직접 테스트하고 비교할 수 있는 10가지 다른 작업에 걸쳐 14개의 데이터셋을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nCodeXGLUE 평가 메트릭은 코딩 작업에 따라 정확일치부터 BLUE 점수까지 다양합니다.\n\n추가로 주목할 만한 코딩 벤치마크: CodeBLEU, MBPP, Py150, MathQA, Spider, DeepFix, Clone Detection, CodeSearchNet\n\nGenAI 모델은 다음 4개 데이터셋에서의 성능 평균으로 순위가 매겨집니다:\n\n- ARC (25-shot)\n- HellaSwag (10-shot)\n- MMLU (5-shot)\n- TruthfulQA (0-shot)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n25번 슛은 데이터 세트에서 (질문, 해결책) 쌍을 25개 씩 프롬프트에 삽입하는 것을 의미합니다.\n\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_13.png\" /\u003e\n\n# 3.4. 대화 및 챗봇 벤치마크\n\n## 3.4.1. 챗봇 아레나 (by LMSys)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2024년에 발표됨] 논문 ∙ 코드\n\n챗봇 아레나는 20만 개 이상의 인간 투표를 사용하여 언어 모델 순위를 매기는 오픈 플랫폼입니다. 사용자들은 ChatGPT나 Claude와 같은 AI 모델 쌍을 익명으로 퀴즈를 내고 평가할 수 있습니다. 모델 신원을 알지 못한 채로 투표하며, 모델 신원이 숨겨져 있는 경우에만 랭킹을 위해 투표가 집계됩니다. 따라서 이는 모델을 객관적으로 점수를 매기는 메트릭을 사용하는 전통적인 기준이 아닙니다! 점수는 본질적으로 \"추천 받은 횟수\"입니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_14.png)\n\n## 3.4.2. 기계 번역 벤치(markdown)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n[2021년에 발행] 논문 ∙ 코드 ∙ 데이터셋\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_15.png)\n\nMT-bench는 LLM(Large Language Model)을 심사위원으로 활용하여 채팅 어시스턴트의 품질을 다중 턴 개방형 질문 시리즈를 통해 평가합니다. 이 방법은 채팅 어시스턴트가 복잡한 상호작용을 다룰 수 있는 능력을 테스트합니다. MT-bench는 대화의 GPT-4 점수를 10점 척도로 평가하고 모든 턴의 평균 점수를 계산하여 최종 점수를 얻습니다.\n\n이러한 모든 벤치마크는 특정 기술을 평가하는 데 매우 유용하지만, 기존의 벤치마크가 우리 프로젝트의 독특한 요구에 완벽하게 부합하지 않는다면 어떨까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n상당히 주목할만 한 대화와 챗봇 벤치마크: DSTC, ConvAI, PersonaChat\n\n### 3.4.3. 언어 모델 평가 하네스 (by EleutherAI)\n\n언어 모델 평가 하네스는 LLMs를 다양한 평가 작업에 대해 벤치마크하는 통합 프레임워크를 제공합니다. 나는 '작업'이라는 단어를 강조했는데, 하네스에 시나리오라는 개념은 없습니다 (LM Evaluation Harness 대신에 Harness를 사용할 것입니다).\n\n하네스 아래에서 우리는 여러 가지 작업을 볼 수 있습니다. 각 작업 또는 일련의 하위 작업은 생성 능력, 다양한 분야의 추론 등 LLM을 다른 영역에서 평가합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 작업 아래의 모든 하위 작업(때로는 작업 자체도 해당)에는 벤치마크 데이터셋이 있으며, 작업은 일반적으로 평가에 대한 중요한 연구들과 관련이 있습니다. Harness는 모든 이러한 데이터셋, 구성 및 평가 전략(예: 벤치마크 데이터셋을 평가하는 데 연결된 메트릭과 같은)을 통합하고 구조화하는 데 큰 노력을 기울입니다.\n\n뿐만 아니라 Harness는 다양한 종류의 LLM 백엔드(예: VLLM, GGUF 등)를 지원합니다. 프롬프트 변경 및 실험을 통해 많은 맞춤 가능성을 제공합니다.\n\n이것은 Misral 모델을 HellaSwag 작업(일반 상식 능력을 판단하는 작업)에서 쉽게 평가할 수 있는 작은 예제입니다:\n\n```js\nlm_eval --model hf \\\n    --model_args pretrained=mistralai/Mistral-7B-v0.1 \\\n    --tasks hellaswag \\\n    --device cuda:0 \\\n    --batch_size 8\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLM 평가 하네스에 영감을 받아 BigCode 프로젝트의 BigCode 평가 하네스가 개발되었습니다. 이 프레임워크는 코드 생성 작업을 평가하기 위한 유사한 API 및 CLI 방법을 제공하려고 합니다. 코드 생성을 위한 평가는 매우 구체적인 주제이므로 다음 블로그에서 자세히 논의할 수 있도록 하겠습니다. 계속해서 주목해 주십시오!\n\n## 3.4.4. 스탠포드 HELM\n\nHELM 또는 언어 모델의 종합적 평가는 \"시나리오\"를 사용하여 LMs가 적용될 수 있는 곳을 개요화하고 벤치마킹 환경에서 LLMs가 무엇을 수행해야 하는지를 지정하는 \"메트릭\"을 사용합니다. 시나리오는 다음으로 구성된:\n\n- 작업 (시나리오와 일치하는)\n- 도메인 (텍스트의 장르가 무엇이고 누가 썼으며 언제 작성되었는지로 구성됨)\n- 언어 (작업을 어떤 언어로 수행할지)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nHELM은 그 후 사회적 관련성(예: 사용자 앞 애플리케이션에 대한 신뢰성을 고려한 시나리오), 범위(다국어 지원), 그리고 실행 가능성(즉, 모든 데이터포인트를 하나씩 실행하는 대신 계산된 최적의 중요한 일부 데이터포인트를 선택하여 평가하는 등)을 기반으로 시나리오와 메트릭의 하위 집합을 우선 순위를 정해보려고 노력합니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_16.png)\n\n뿐만 아니라, 이 HELM은 LLM 성능에 대한 절대 신뢰성을 제공할 수 있는 정확도만으로 충분하지 않기 때문에 거의 모든 시나리오에 대해 7가지 메트릭(정확도, 보정, 견고성, 공정성, 편향, 유해성, 효율성)를 다루려고 노력합니다.\n\n## 3.4.5. PromptBench (마이크로소프트)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![2024-05-23-BuildingLLMApplicationsEvaluationPart8_17](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_17.png)\n\nPromptBench은 LLM을 벤치마킹하기 위한 또 다른 통합 라이브러리입니다. HELM과 Harness와 매우 유사하며, 다양한 LLM 프레임워크를 지원합니다 (예: Hugging Face, VLLM 등). 다른 프레임워크와 구별되는 점은 과제를 평가하는 것뿐만 아니라 다양한 Prompt Engineering 방법을 지원하며, LLM을 다양한 프롬프트 수준 적대 공격에서 평가한다는 것입니다. 또한 프로덕션 수준의 사용 사례를 더 쉽게 만드는 여러 평가 파이프라인을 구성할 수 있습니다.\n\n# 4. LLM 벤치마크의 한계\n\n벤치마크는 LLM의 능력을 평가하는 데 기본적이지만, 제한 사항도 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 도메인 관련성: 벤치마크는 종종 LLM이 적용되는 독특한 도메인과 맥락과 일치하지 않아 법률 분석이나 의학 해석과 같은 작업에 필요한 구체성이 부족하다. 이것은 LLM 성능을 폭넓은 전문화 응용 프로그램 범위에서 정확하게 평가하는 벤치마크를 작성하는 데 어려움을 강조한다.\n- 수명 짧음: 벤치마크가 처음 출시되면 보통 모델이 인간 기준에 미치지 못하는 것으로 나타난다. 그러나 조금의 시간이 지나면, 예를 들어 1~3년 정도면, 고급 모델이 초기의 어려움을 쉽게 극복할 정도로 발전한다 (계속되는 한 예). 이러한 지표가 더 이상 도전이 되지 않을 때 새로운 유용한 벤치마크를 개발하는 것이 필요해진다.\n\n그럼에도 불구하고 전멸과 절망만은 아닙니다. 합성 데이터 생성과 같은 혁신적인 방법을 통해 이러한 제약을 극복할 수 있습니다.\n\n# 5. LLM 평가 지표\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_18.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nLLM 평가 지표는 우리가 중요하게 생각하는 기준에 따라 LLM의 출력물을 평가합니다. 예를 들어, 만약 우리의 LLM 애플리케이션이 뉴스 기사들의 페이지를 요약하는 데 사용된다면, 우리는 원본 텍스트로부터 충분한 정보를 포함하는지, 그리고 어떠한 모순이나 환각이 포함되어 있는지를 기준으로 하는 LLM 평가 지표가 필요할 것입니다.\n\n게다가, 만약 우리의 LLM 애플리케이션이 RAG 기반 아키텍처를 갖고 있다면, 아마도 검색 컨텍스트의 품질에 대한 점수도 필요할 것입니다. 포인트는, LLM 평가 지표가 LLM 애플리케이션을 그 애플리케이션이 수행할 작업에 기반하여 평가한다는 것입니다. (LLM 애플리케이션 자체가 LLM일 수도 있다는 점을 유의하세요!)\n\n좋은 평가 지표는:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 양적. 작업을 평가할 때 메트릭은 항상 점수를 계산해야 합니다. 이 접근 방식을 통해 우리는 LLM 애플리케이션이 \"충분히 좋은지\"를 결정하기 위한 최소 통과 임계값을 설정할 수 있습니다. 또한, 우리가 반복하고 구현을 개선함에 따라 이러한 점수가 어떻게 변하는지 모니터링할 수 있습니다.\n\n- 신뢰할 수 있는. LLM 출력이 얼마나 예측할 수 없는지에 상관없이, LLM 평가 메트릭이 일관성 없는 것은 원치 않는 결과입니다. 따라서 LLM을 사용하여 평가된 메트릭 (일명 LLM-Evals)인 G-Eval과 같이 전통적인 점수 메서드보다 더 정확하지만, 종종 일관성이 없어서 대부분의 LLM-Evals가 부족한 부분이 있습니다.\n\n- 정확한. 신뢰할 수 있는 점수는 우리의 LLM 애플리케이션의 성능을 정확하게 나타내지 않는다면 무의미합니다. 좋은 LLM 평가 메트릭을 훌륭하게 만드는 비밀은 가능한 한 인간의 기대에 부합하도록 만드는 것입니다.\n\n그렇다면, LLM 평가 메트릭이 신뢰할 수 있고 정확한 점수를 어떻게 계산할 수 있을까요?\n\n## 6. 메트릭 점수를 계산하는 여러 방법\n\n메트릭 점수를 계산하는 데 사용할 수 있는 다양한 기본 방법이 있습니다. 일부는 신경망을 활용하며 임베딩 모델 및 LLM을 포함하고, 다른 몇 가지는 통계 분석에 전적으로 기반을 둔 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_19.png\" /\u003e\n\n과제 종속 메트릭\n\n과제 종속 메트릭은 LLM 애플리케이션의 특정 목표와 밀접하게 관련되어 있습니다. 예를 들어, 텍스트 분류는 정확도와 F1 점수를 우선시할 수 있으며, 텍스트 생성은 가독성이나 의미적 정확성과 같은 구체적인 목표에 따라 난해도, BLEU 또는 ROUGE 점수에 중점을 둘 수 있습니다. 이러한 메트릭은 LLM의 응용 프로그램의 원하는 결과와 얼마나 밀접하게 일치하는지를 기반으로 선택됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 내용을 친근하게 번역하면 다음과 같습니다:\n\n이 섹션을 마치면 각 방법을 살펴보고 최상의 방법에 대해 이야기할 거에요, 그러니 계속 읽어보세요!\n\n### 6.1. 통계적 평가자\n\n시작하기 전에, 통계적 평가 방법은 중요하지 않다는 점을 먼저 말씀 드리고 싶어요. 급하신 분들은 “G-Eval” 섹션으로 바로 건너뛸 수 있습니다. 이는 통계적 방법은 추론이 필요할 때 항상 성능이 좋지 않기 때문에 대부분의 LLM 평가 기준에 대해 점수화하는 데 너무 부정확합니다.\n\n이제 함께 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.1.1. 단어 오류율 (WER)\n\nWER은 후보 단어를 기준 문자열로 변환하는 데 필요한 삽입, 삭제, 대체 및 필요에 따라 전치의 수인 편집 거리를 측정하는 WER 기반 메트릭의 집합입니다.\n\n## 6.1.2. 정확도 일치\n\n생성된 텍스트를 참조 텍스트와 일치시켜 후보 텍스트의 정확도를 측정합니다. 참조 텍스트와의 차이는 오답으로 처리됩니다. 이것은 추출 및 짧은 형식의 답변에 적합하며 참조 텍스트와의 최소 또는 전혀 차이가 없을 것으로 예상될 때만 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.1.3. Perplexity\n\n헷갈리기(perplexity, PPL)는 언어 모델을 평가하는 가장 일반적인 지표 중 하나입니다. 이에 대해 자세히 살펴보기 전에 주의해야 할 점은 이 지표가 전형적인 언어 모델(가끔은 자기 회귀적 또는 인과 언어 모델이라고도 함)에 특히 적용되며, BERT와 같은 가려진 언어 모델에는 잘 정의되지 않는다는 것입니다(모델 요약 참조).\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_21.png)\n\n이는 데이터와 모델 예측 간의 교차 엔트로피의 지수화와 동등합니다. 헷갈리기와 Bit Per Character (BPC) 및 데이터 압축과의 관계에 대한 추가 직관을 위해 The Gradient 블로그의 이 멋진 글을 확인해보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nColab Notebook 지원: Perplexity.ipynb\n\n## 6.1.4. BLEU\n\nBLEU(BiLingual Evaluation Understudy) 점수는 기계 번역 품질을 평가하는 데 널리 사용되는 측정 항목입니다. 참조 번역(Reference)에 대한 기계 번역 텍스트(후보)를 평가합니다. IBM 연구원들이 개발한 BLEU는 기계 생성 텍스트와 높은 품질의 참조 번역 사이의 n-그램 오버랩을 측정하여 번역 정확도를 평가합니다. BLEU는 주로 정밀도에 중점을 둡니다. BLEU는 간단하고 효과적이어서 기계 번역 분야의 표준 벤치마크로 널리 알려져 있습니다. 그러나 BLEU는 주로 표면적인 어휘적 유사성을 평가하며 언어의 깊이 있는 의미론적 및 문맥적 세부 사항을 종종 간과합니다.\n\n후보(Candidate): 이것은 우리의 번역 시스템에서 나온 출력물로, 평가하려는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n참고: 이들은 일반적으로 인간에 의해 수행되는 고품질의 번역이며, 우리는 후보 텍스트와 비교합니다. 강건성을 위해 여러 개의 참조 번역이 있을 수 있습니다.\n\n계산\n\n후보 및 참조 번역을 단어(토큰)로 분할합니다. 토큰화는 두 세트의 텍스트에 대해 일관되어야 합니다.\n\nn-그램 정밀도(P)를 계산합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 n-그램 길이에 대해 (일반적으로 1에서 4까지):\n\n- 후보에서 참조되는 n-그램의 수를 세어서 참조에서 후보에 모두 나타나는 공통 n-그램의 수를 셉니다.\n- 이 숫자를 후보 번역의 총 n-그램 수로 나누어 각 n-그램 길이에 대한 정밀도를 얻습니다.\n\n위반 벌칙 (BP)\n\n- 후보 번역이 참조 번역보다 짧은 경우, 우리는 지나치게 짧은 번역을 선호하지 않도록 벌칙을 부여해야 합니다.\n- BP 공식: BP = exp(1−r/c) if c ` r, else BP = 1\n- 여기서 c는 후보 번역의 길이이고 r은 유효한 참조 길이입니다.\n\nBLEU 점수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- BLEU 스코어는 n-gram 정밀도의 기하 평균을 사용하여 계산되며 간결성 패널티로 곱해집니다.\n- BLEU 스코어 = BP * exp((1/n)*∑log⁡(pi)), 여기서 n은 1부터 4까지 (n-grams)\n- 여기서 pi는 n-gram에 대한 정밀도입니다.\n\nBLEU 스코어의 범위: 보통 0부터 1까지이며, 0은 번역된 텍스트와 참조 번역 간에 오버랩이 없음을 나타내며 가장 낮은 점수를 나타내어 매우 나쁜 번역 품질을 시사합니다. 1은 참조 번역과 완벽하게 일치한다는 것을 나타내며 가장 높은 점수를 나타내어 이상적인 번역 품질을 시사합니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_22.png)\n\n```js\nfrom nltk.translate.bleu_score import sentence_bleu\n\n# 샘플 참조 및 생성된 문장\nreference = [[\"A\", \"fast\", \"brown\", \"fox\", \"jumps\", \"over\", \"a\",\n\"lazy\", \"dog\", \".\"]]\ngenerated = [[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\",\n\"lazy\", \"dog\", \".\"]]\n\n# BLEU 스코어 계산\nbleu_score = sentence_bleu(reference, generated)\nprint('BLEU 스코어:', bleu_score)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.1.5. ROUGE\n\nROUGE (Recall-Oriented Understudy for Gisting Evaluation)은 자동 요약 및 기계 번역을 평가하는 데 사용되는 측정 항목입니다. 이는 자동 생성된 요약 또는 번역을 일반적으로 사람이 쓴 참조 요약과 비교합니다. ROUGE는 모델이 생성한 텍스트와 참조 텍스트 간의 n-그램, 단어 순서 및 단어 쌍과 같은 중첩 단위 수를 셈으로써 요약의 품질을 측정합니다. ROUGE를 사용하는 가장 흔한 변형은 다음과 같습니다:\n\n- **ROUGE-N**: n-그램(단어 구)에 중점을 둡니다. ROUGE-1 및 ROUGE-2(각각 단일어와 이중어)가 가장 일반적입니다.\n\n- **ROUGE-L**: Longest Common Subsequence(LCS)를 기반으로 하는데요, 이는 문장 수준 구조 유사성을 자연스럽게 고려하고 가장 긴 순차적 n-그램을 자동으로 식별합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로 ROUGE는 세 가지 메트릭을 보고합니다.\n\n정밀도(Precision): 모델이 생성한 요약에서 레퍼런스 요약에도 있는 n-gram의 비율을 의미합니다.\n\n재현율(Recall): 레퍼런스 요약에 있는 n-gram 중 모델이 생성한 요약에도 있는 것의 비율을 의미합니다.\n\nF-점수(F1 점수): 정밀도와 재현율의 조화평균으로, 두 가지를 균형있게 고려합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nROUGE 점수는 0부터 1까지의 범위를 가지며, 0은 기계 생성 텍스트와 참조 텍스트 사이에 중첩이 없음을 나타내며, 1은 참조 텍스트와 완벽히 일치함을 나타냅니다.\n\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_23.png)\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_24.png)\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_25.png)\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# 샘플 참조 및 생성된 요약\nreference_summaries = [['빠른 갈색 여우가 게으른 개를 뛰어넘는다.']]\ngenerated_summaries = [['빠른 갈색 여우가 게으른 개를 뛰어넘는다.']]\n\n# BLEU 점수 계산\nbleu_score = corpus_bleu(reference_summaries, generated_summaries)\nprint('BLEU 점수:', bleu_score)\n```\n\n## 6.1.6. METEOR\n\nMETEOR(Metric for Evaluation of Translation with Explicit Ordering)은 기계 번역을 평가하는 고급 메트릭으로, BLEU 점수의 일부 한계를 해소하기 위해 개발되었습니다. BLEU와 달리 METEOR은 정확한 단어 일치 뿐만 아니라 어간 및 동의어를 고려하여 번역을 평가하므로 보다 광범위한 언어 유사성을 포착합니다. 정밀도와 재현율을 균형 있게 평가하며, 단어 순서의 차이에 대한 벌점을 도입하여 번역의 유창성을 평가합니다. METEOR은 문장 수준에서 인간 판단과 더 높은 상관관계를 갖는 것으로 알려져 있어 번역 품질 평가에 있어 소수점 아래까지 세밀하고 포괄적인 메트릭스로서의 지위를 갖고 있습니다. 그러나 그 복잡성으로 인해, BLEU와 같은 단순한 메트릭스에 비해 더 많은 계산 노력이 필요합니다.\n\nAlignment-Based: METEOR은 후보 번역과 참조 번역 간의 단어들 간의 일치를 생성하여, 정확, 어간, 동의어 및 패러프레이즈 일치에 초점을 맞춥니다.\n\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nRecall과 Precision: BLEU가 precision만을 고려하는 반면, METEOR은 precision과 recall을 모두 고려합니다. 이 이중 접근 방식은 평가를 균형있게 돕습니다.\n\n조화평균: METEOR은 recall과 precision의 조화평균을 사용하며, recall에 높은 가중치를 둡니다 (recall에 더 많은 중요성을 부여하기 위한 조정된 조화평균). 이는 BLEU와는 다르게 수정된 형태의 precision을 사용합니다.\n\n단어 순서 차이에 대한 패널티: METEOR은 잘못된 단어 순서에 대한 패널티를 포함하고 있어 번역의 순조성에 민감합니다.\n\n언어 독립적: 처음에는 영어로 개발되었지만, METEOR은 언어별 매개변수와 자원을 지원하기 위해 여러 언어로 확장되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일치하는 항목 계산\n\n후보 번역에서 정확히 일치하는 단어 수를 세세요.\n\n정밀도와 재현율 계산\n\n- 정밀도 (P): 후보 번역에 나타나는 단어의 비율을 의미합니다.\n- 재현율 (R): 참조 번역에 나타나는 단어의 비율을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# Precision과 Recall의 조화평균을 계산하세요\n\nHarmonic Mean은 다음과 같이 계산됩니다: Harmonic Mean=10*P*R/(R+9*P). 이는 Precision보다 Recall에 더 많은 가중치를 둡니다.\n\n단어 순서에 대한 패널티\n\n단어 순서의 차이에 대한 패널티가 부가됩니다. 이 패널티는 다음과 같이 계산됩니다: 패널티=0.5⋅(# of chunks/# of matches)**3; 여기서 \"chunk\"는 후보 단어에서 참조와 동일한 순서로 이웃하는 단어의 집합을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n최종 METEOR 점수\n\n최종 점수는 다음과 같이 계산됩니다: Score=(1−Penalty)*F-mean\n\n- Levenshtein 거리(또는 편집 거리) 스코어는 단어나 텍스트 문자열을 다른 것으로 변경하기 위해 필요한 단일 문자 편집(삽입, 삭제 또는 대체)의 최소 수를 계산합니다. 이는 철자 교정 또는 문자의 정확한 정렬이 중요한 작업을 평가하는 데 유용할 수 있습니다.\n\n순수한 통계 스코어러는 매우 제한된 추론 능력을 가지고 있기 때문에 의미론을 고려하지 않으며 종종 긴 및 복잡한 LLM 출력을 평가하기에 정확도가 충분하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_26.png)\n\n```python\nfrom meteor import meteor_score\n\n# 참조 및 생성된 문장 예시\nreference_sentence = 'A fast brown fox jumps over a lazy dog.'\ngenerated_sentence = 'The quick brown fox jumps over the lazy dog.'\n\n# METEOR 점수 계산\nmeteor_score = meteor_score.meteor_score([reference_sentence], generated_sentence)\nprint('METEOR Score:', meteor_score)\n```\n\n## 6.2. 모델 기반 점수 산정기\n\n순전히 통계적인 산정기들은 신뢰성이 있지만 의미론을 고려하는 데 어려움을 겪어 정확하지 않다. 이 절에서는 정반대인 산정기들을 살펴보겠습니다. 순전히 NLP 모델에 의존하는 산정기들은 비교적 더 정확하지만 확률적인 성격으로 인해 신뢰성이 떨어질 수 있습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이것은 놀라운 사실이 아닐 것입니다만, LLM 기반이 아닌 스코어러들은 통계 스코어러에 언급된 이유로 인해 LLM-Evals보다 성능이 떨어집니다. LLM이 아닌 스코어러에는 다음이 포함됩니다:\n\n## 6.2.1. Entailment score\n\n연역 점수: 이 방법은 언어 모델의 자연어 추론 능력을 활용하여 NLG를 판단합니다. 이 방법에는 다양한 변형이 있지만, 기본 개념은 NLI 모델을 사용하여 생성물에 대한 연역 점수를 참조 텍스트와 비교하여 산출하는 것입니다. 이 방법은 텍스트 요약과 같은 텍스트 중심 생성 작업에서 충실성을 보장하는 데 매우 유용할 수 있습니다.\n\n![image](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_27.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.2.2. BLEURT\n\nBLEURT (Bilingual Evaluation Understudy with Representations from Transformers)은 문장 간의 비트라이빌 의미 유사성을 포착할 수 있는 새로운 머신 러닝 기반의 자동 측정 항목입니다. 이 메트릭은 WMT Metrics Shared Task 데이터셋(공공 컬렉션의 평가로 학습) 및 사용자가 제공한 추가 평가를 기반으로 학습됩니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_28.png)\n\n기계 학습을 기반으로 한 메트릭을 생성하는 것은 기본적인 도전이 됩니다. 메트릭은 다양한 작업 및 도메인에서 일관된 성능을 보이고 시간이 지남에 따라 잘 동작해야 합니다. 그러나 훈련 데이터의 양은 제한적입니다. 실제로 공개 데이터는 희소합니다 — 작성 시점에서 가장 큰 인간 평가 컬렉션인 WMT Metrics Task 데이터셋은 뉴스 도메인에 대해서만 약 26만 개의 인간 평가를 포함합니다. 이는 미래의 NLG 시스템을 평가하기에 적합한 메트릭을 훈련시키기에는 너무 한정적합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 문제를 해결하기 위해 전이학습을 사용합니다. 첫째로, 우리는 BERT의 문맥 단어 표현을 사용합니다. 이는 언어 이해를 위한 최첨단 비지도 표현 학습 방법으로 이미 NLG 측정 항목에 성공적으로 통합되었습니다(YiSi나 BERTscore 등).\n\n둘째로, BLEURT의 견고성을 높이기 위한 혁신적인 사전 훈련 방식을 소개합니다. 실험 결과, 공개적으로 제공된 인간 평가에 직접 회귀 모델을 훈련하는 것은 역동적인 방법입니다. 왜냐하면 어느 도메인에서, 어느 시기에 메트릭이 사용될지 우리가 제어할 수 없기 때문입니다. 도메인 드리프트가 존재할 때 정확도가 떨어지는 경향이 있습니다. 이는 텍스트가 훈련용 문장 쌍과 다른 도메인에서 가져온 경우에 해당됩니다. 품질 드리프트가 발생할 때, 훈련 중에 사용된 것보다 예측해야 하는 등급이 더 높은 경우에도 정확도가 떨어질 수 있습니다. 이는 일반적으로 ML 연구가 진전되고 있음을 나타내는 좋은 소식이 될 수 있는 특징입니다.\n\nBLEURT의 성공은 수백만 개의 합성 문장 쌍을 사용해 모델을 \"워밍업\"한 뒤 인간 평가를 세밀하게 조정하는 데에 달려 있습니다. 우리는 위키피디아 문장에 무작위로 변형을 가해 훈련 데이터를 생성했습니다. 인간 평가를 수집하는 대신, BLU를 포함한 문헌에서 제시된 메트릭과 모델의 모음을 사용하여 훈련 예제의 수를 비용을 매우 낮출 수 있게 했습니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_29.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.2.3. QA-QG\n\n질문 응답 — 질문 생성 (QA-QG) (Honovich et. al): 이 패러다임은 후보의 일관성을 참조 텍스트와 비교하기 위해 사용할 수 있습니다. 이 방법은 먼저 후보 텍스트에서 (답 후보, 질문) 쌍을 형성한 다음, 참조 텍스트에서 주어진 동일한 질문 세트에 대해 생성된 답변을 비교하고 확인하여 작동합니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_30.png)\n\n불일치 점수 외에도, 이러한 접근 방식에는 몇 가지 단점이 있습니다. 예를 들어, NLI 스코어러는 긴 텍스트를 처리할 때 정확도에 어려움을 겪을 수도 있고, BLEURT는 학습 데이터의 품질과 대표성에 제한이 있을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러니 우리 LLM-Evals에 대해 이야기해 보겠습니다.\n\n# 6.3. LLM-Evals\n\n## 6.3.1. G-Eval\n\nG-Eval은 최근 개발된 프레임워크인 \"GPT-4를 사용한 NLG 평가 및 인간 정렬 강화\"라는 논문에서 사용된 LLMs를 평가하는 LLM 출력을 사용하는 프레임워크입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_31.png\" /\u003e\n\nG-Eval은 평가 단계를 생성하기 위해 사고 연쇄(Chain of Thoughts, CoTs)를 사용한 다음, 이러한 단계를 사용하여 최종 점수를 결정하는 형식 채우기 패러다임을 통해 점수를 산출합니다 (이는 G-Eval이 작동하기 위해 여러 정보 조각이 필요하다는 멋진 표현입니다). 예를 들어, G-Eval을 사용하여 LLM 출력 일관성을 평가하는 경우, 평가 단계를 생성하기 위해 평가 기준과 평가할 텍스트를 포함하는 프롬프트를 작성한 후, 이러한 단계에 기반하여 1에서 5까지의 점수를 LLM을 사용하여 산출합니다.\n\n이 예를 통해 G-Eval 알고리즘을 사용해봅시다. 먼저, 평가 단계를 생성하기 위해:\n\n- 원하는 LLM에 평가 작업 소개(예: 일관성에 따라 1-5로 이 출력을 평가)\n- 기준 정의(예: \"일관성 - 실제 출력의 모든 문장의 집합적 품질\")\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n(원본 G-Eval 논문에서는 저자들이 실험에 GPT-3.5 및 GPT-4만 사용했으며, 저는 다양한 LLMs를 활용하여 G-Eval에 대해 직접 실험해본 결과, 이러한 모델을 사용하는 것을 강력히 권장드립니다.)\n\n평가 단계를 생성한 후:\n\n- 평가 단계와 평가 단계에 나열된 모든 함께 연결하여 프롬프트를 만듭니다 (예: LLM 출력의 논리 평가를 평가한다면, LLM 출력이 필요한 인수가 될 것입니다).\n- 프롬프트 끝에 1에서 5 사이의 점수를 생성하도록 요청합니다. 5가 1보다 우수하다는 것을 의미합니다.\n- (선택사항) LLM에서 생성된 토큰의 확률을 가져와 점수를 정규화하고 가중 합산하여 최종 결과를 취합합니다.\n\n3단계는 옵션입니다. 왜냐하면 출력 토큰의 확률을 얻기 위해 LLM의 원시 모델 임베딩에 액세스해야 하며, 2024년 기준으로 이는 아직 OpenAI API를 통해 사용할 수 없습니다. 그러나 이 단계는 LLM 점수에 미세 조정된 점수를 제공하며 LLM 점수의 편향을 최소화하기 때문에 도입된 것입니다 (논문에서 언급된 바와 같이, 1-5 척도에 대한 3은 높은 토큰 확률을 가지고 있다고 알려져 있습니다).\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 논문에서 언급된 모든 전통적이고 비-LLM 평가 방법을 능가하는 G-Eval의 성능을 보여주는 결과입니다:\n\n![그림](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_32.png)\n\nG-Eval은 LLM-Eval로써 LLM 출력물의 전체 의미를 고려할 수 있기 때문에 훨씬 더 정확합니다. 그리고 이것은 매우 타당한 주장입니다. LLM보다 능력이 훨씬 떨어지는 평가자를 사용하는 비-LLM 평가 방법이 LLM이 생성한 텍스트의 전체 범위를 어떻게 이해할 수 있을까요?\n\nG-Eval은 동료들과 비교했을 때 인간 판단과 훨씬 더 관련이 있습니다. 그러나 LLM에 평가 점수를 매기도록 요청하는 것은 명백히 임의적이기 때문에 여전히 신뢰성이 떨어질 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그렇다는 것은, G-Eval의 평가 기준이 얼마나 유연한지 고려할 때, 저는 개인적으로 DeepEval에 G-Eval을 적용하여 작업 중인 오픈 소스 LLM 평가 프레임워크로 구현했습니다.\n\n```js\n# 설치\npip install deepeval\n# OpenAI API 키를 환경 변수로 설정\nexport OPENAI_API_KEY=\"...\"\n```\n\n```js\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\nfrom deepeval.metrics import GEval\n\ntest_case = LLMTestCase(input=\"LLM에 입력하는 내용\", actual_output=\"LLM의 출력 내용\")\ncoherence_metric = GEval(\n    name=\"일관성\",\n    criteria=\"일관성 - 실제 출력의 모든 문장의 품질을 종합한 것\",\n    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n)\n\ncoherence_metric.measure(test_case)\nprint(coherence_metric.score)\nprint(coherence_metric.reason)\n```\n\n또 다른 주요 장점은 LLM-Eval을 사용하면, LLM은 평가 점수에 대한 이유를 생성할 수 있다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.3.2. 프로메테우스\n\n프로메테우스는 GPT-4의 평가 능력을 제공할 때 적절한 참조 자료(참조 답변, 점수 기준)가 제공된다면 비교할 수 있는 완전한 오픈 소스 LLM입니다. 그것은 G-Eval과 유사하게 케이스에 대해 무관합니다. 프로메테우스는 Llama-2-Chat을 기본 모델로 사용하고 100만 건의 피드백(생성된 GPT-4에 의해)을 피드백 컬렉션 내에서 섬세하게 조정했습니다.\n\n프로메테우스 연구 논문에서 간략한 결과는 다음과 같습니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_33.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n프로메테우스는 G-Eval과 같은 원칙을 따르지만, 몇 가지 차이점이 있습니다:\n\n- G-Eval은 GPT-3.5/4를 사용하는 프레임워크이지만, 프로메테우스는 평가를 위해 세밀하게 조정된 LLM입니다.\n- G-Eval은 CoT를 통해 점수 평가 및 평가 단계를 생성하지만, 프로메테우스의 점수 평가 기준은 프롬프트에 제공됩니다.\n- 프로메테우스는 참고/예시 평가 결과가 필요합니다.\n\n개인적으로는 아직 시도해 보지 않았지만, 프로메테우스는 허깅페이스에서 이용할 수 있습니다. 프로메테우스를 구현해보지 않은 이유는, 프로메테우스가 OpenAI의 GPT와 같은 독점 모델에 의존하는 대신 평가를 오픈소스로 만드는 데 중점을 두었기 때문입니다. 최고의 LLM-Evals를 구축하려는 누군가에게는 좋은 선택이 아니었습니다.\n\n# 6.4. 통계 및 모델 기반 스코어 결합\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n지금까지 우리는 통계적 방법이 신뢰할만 하지만 정확하지 않다는 것과, 비-LLM 모델 기반 접근 방법이 믿을만 하지 않지만 더 정확하다는 것을 보았습니다. 이전 섹션과 유사하게, BERTScore와 같은 비-LLM 점수 산정 방법이 있습니다:\n\n## 6.4.1. BERTScore\n\nBERTScore (Zhang et al. 2019): 이는 bi-encoding 기반 접근 방법으로, 후보 텍스트와 참조 텍스트가 DL 모델에 따로 입력되어 임베딩을 얻습니다. 토큰 수준의 임베딩은 이후에 쌍별 코사인 유사도 행렬을 계산하는 데 사용됩니다. 그런 다음, 참조에서 후보에 가장 유사한 토큰의 유사도 점수를 선택하여 정밀도, 재현율 및 F1 점수를 계산합니다.\n\n![그림](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_34.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.4.2. MoverScore\n\nMoverScore (Zhao et al. 2019): 워드 무버스 디스턴스 개념을 사용하여, 임베디드된 단어 벡터 간의 거리가 어느 정도 의미론적으로 유의미하다고 제안합니다. ( vector(king) — vector(queen) = vector(man) ) 그리고 n-그램 간의 유클리드 유사도를 계산하기 위해 문맥 임베딩을 사용합니다. 단어 간 일대일 단어 경기를 허용하는 BERTscore와 달리, MoverScore는 소프트/일부 정렬을 사용하기 때문에 다 대 일 매핑을 허용합니다.\n\n![image](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_35.png)\n\nBERTScore와 MoverScore 점수기 모두 BERT와 같은 사전 훈련된 모델로부터의 문맥 임베딩에 의존하여 문맥적인 인식과 편향에 취약합니다. 그렇다면 LLM-Evals는 어떨까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.4.3. GPTScore\n\nG-Eval이 직접 작업을 평가하는 방식인 것과는 달리, GPTScore는 대상 텍스트를 생성하는 조건부 확률을 사용하여 평가 지표로 사용합니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_36.png)\n\n## 6.4.4. SelfCheckGPT\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nSelfCheckGPT은 조금 독특해요. 이것은 LLM 출력물을 사실 확인하기 위해 사용되는 간단한 샘플링 기반 접근법입니다. 이는 상상된 출력물은 재현할 수 없다고 가정하며, LLM이 특정 개념을 알고 있다면 샘플링된 응답은 유사할 것이며 일관된 사실을 포함할 가능성이 높다고 가정합니다.\n\nSelfCheckGPT은 환각을 감지하는 데 적합성을 확인할 때 레퍼런스 없이 처리하는 흥미로운 방법입니다. 이것은 생산 환경에서 굉장히 유용합니다.\n\n![](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_37.png)\n\n그러나 G-Eval 및 프로메테우스가 use case에 중립적인 것을 알 수 있습니다. SelfCheckGPT는 그렇지 않습니다. 이것은 환각을 감지하기 위해 적합하며, 요약, 일관성 등 다른 use case를 평가하는 데는 적합하지 않습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 6.4.5. QAG 점수\n\nQAG(Question Answer Generation) 점수는 LLMs의 뛰어난 추론 능력을 활용하여 신뢰성 있는 LLM 출력을 평가하는 점수 계산기입니다. 답변(일반적으로 '예' 또는 '아니오')을 사용하여 닫힌 질문(생성 또는 미리 설정된 질문)을 계산하여 최종 메트릭 점수를 산출합니다. 그것은 신뢰성이 있습니다. 왜냐하면 LLMs를 직접적으로 점수를 생성하는 데 사용하지 않기 때문입니다. 예를 들어, 충실성을 측정하는 점수를 계산하려면 (LLM 출력이 환각적인지 여부를 측정), 다음과 같이 할 것입니다:\n\n- 한 LLM을 사용하여 LLM 출력에 있는 모든 주장을 추출합니다.\n- 각 주장에 대해, 실제 정답이 해당 주장과 일치하는지 ('예') 아니면 일치하지 않는지 ('아니오') 물어봅니다.\n\n그래서 이 예시 LLM 출력의 경우:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n주장은:\n\n그리고 해당하는 닫힌 질문은:\n\n그런 다음, 이 질문을 가지고 땅의 진실이 주장과 일치하는지 확인해야 합니다. 결과적으로, 우리는 '예'와 '아니오' 답변의 수가 나오게 되는데, 이를 사용하여 우리의 선택한 수학 공식을 통해 점수를 계산할 수 있습니다.\n\n충실성의 경우, 만약 LLM 출력 내의 주장들 중 정확하고 실제와 일치하는 비율로 정의한다면, LLM이 한 주장을 하는 전체 주장 수로 나누어 정확한(진실된) 주장의 수를 나눠 계산할 수 있습니다. 우리가 LLM을 직접 평가 점수를 생성하는 데 사용하지 않고 여전히 그들의 우수한 추론 능력을 활용한다면, 정확하고 신뢰할 수 있는 점수를 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 7. LLM 기반 애플리케이션의 평가\n\n## 7.1. 평가 메트릭스 선택\n\nLLM 애플리케이션의 평가 메트릭스는 상호 작용 방식과 예상 답변 유형에 기반하여 선택됩니다. 주로 LLM과 상호 작용하는 방식은 세 가지 형태로 나뉩니다.\n\n- 지식 탐색: LLM에게 질문이나 명령이 제공되고 진실한 답변이 기대됩니다. 예: 인도의 인구는 얼마인가요?\n- 텍스트 착취: LLM에게 텍스트와 지시사항이 제공되고 답변이 주어진 텍스트에 완전히 기반되기를 기대합니다. 예: 주어진 텍스트를 요약해주세요.\n- 창의성: LLM에게 질문이나 명령이 제공되고 창의적인 답변이 기대됩니다. 예: 아쇼카 왕자에 대한 이야기를 써주세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n각 상호작용 또는 작업에 대해 기대되는 응답 유형은 추출적, 요약적, 간략한 형식, 장문 형식 또는 다중 선택형일 수 있습니다.\n\n예를 들어, LLM의 요약화 응용(텍스트 기반 + 요약적)의 경우, 결과물의 충실성과 일관성은 원본 문서와 비슷하기가 어려울 수 있습니다.\n\n## 7.2. 평가 방법 평가하기!\n\n우리가 응용에 적합한 평가 전략을 수립한 후, 실험의 성능을 측정하기 위해 이를 신뢰하기 전에 우리의 전략을 평가해야 합니다. 평가 전략은 인간의 판단과의 상관관계를 통해 평가됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 골든 인간 주석 점수가 포함된 테스트 세트를 획득하거나 주석을 다는 것\n- 우리 방법을 사용하여 테스트 세트의 생성물을 점수화\n- 켄달 순위 상관 계수와 같은 상관 측정 기법을 사용하여 인간 주석 점수와 자동 점수 간의 상관 관계를 측정\n\n일반적으로 0.7 이상의 점수는 충분히 양호하다고 여겨집니다. 이를 통해 평가 전략의 효과를 개선하는 데 사용할 수도 있습니다.\n\n## 7.3. 평가 세트 구성하기\n\n모든 ML 문제의 평가 세트를 형성할 때 지켜야 할 두 가지 기본 기준은\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 데이터셋은 통계적으로 의미 있는 결과를 얻기에 충분히 커야 합니다.\n- 최대한 제품에서 예상되는 데이터를 대표해야 합니다.\n\nLLM 기반 애플리케이션을 평가하는 세트 구성은 점진적으로 진행할 수 있습니다. LLM은 몇 가지 shot을 사용하여 평가 세트에 대한 쿼리를 생성하는 데 활용할 수 있으며, Auto-evaluator와 같은 도구는 이를 돕는 데 도움이 될 수 있습니다.\n\nGround truth를 갖춘 평가 세트의 구축은 비용 부담이 크고 시간이 많이 소요되며, 데이터 드리프트에 대한 골든 어노테이트된 테스트 세트를 유지하는 것은 매우 어려운 작업입니다. Unsupervised LLM 지원 방법론이 목표와 잘 연관되지 않을 경우 시도해 볼 수 있는 것입니다. 참조 답변의 존재는 사실성과 같은 특정 측면에서 평가의 효과를 높일 수 있습니다.\n\n# 8. LLM 평가 프레임워크\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다양한 응용 프로그램에서 LLM의 품질과 효능을 평가하기 위해 LLM을 평가해야 합니다. LLM의 평가를 위해 특별히 개발된 다양한 프레임워크가 있습니다.\n\n가장 널리 인정받는 몇 가지 프레임워크를 강조해보겠습니다. Microsoft Azure AI 스튜디오의 Prompt Flow, LangChain과 함께 사용되는 Weights \u0026 Biases, LangChain의 LangSmith, confidence-ai의 DeepEval, TruEra 등이 있습니다.\n\n아래에서 2024-05-23-BuildingLLMApplicationsEvaluationPart8_38.png 이미지를 확인할 수 있습니다.\n\n이 블로그에서는 Deepeval 및 RAGAs와 같은 2개의 오픈 소스 LLM 평가 프레임워크에 중점을 둘 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 8.1. Deepeval\n\nDeepEval은 LLM에 대한 오픈 소스 평가 프레임 워크입니다. DeepEval을 사용하면 LLM(응용 프로그램)을 쉽게 구축하고 반복할 수 있습니다. 다음 원칙을 기반으로 개발되었습니다.\n\n- Pytest와 유사한 방식으로 LLM 출력을 쉽게 \"단위 테스트\"할 수 있습니다.\n- 14개 이상의 LLM 평가 메트릭을 플러그인하여 사용할 수 있고, 대부분의 메트릭은 연구를 지지합니다.\n- 사용자 정의 메트릭을 쉽게 개인화하고 생성할 수 있습니다.\n- Python 코드에서 평가 데이터 세트를 정의할 수 있습니다.\n- 프로덕션 환경에서 실시간 평가 가능 (Confident AI에서 사용 가능).\n\n평가는 LLM 응용 프로그램 출력을 테스트하는 프로세스를 의미하며, 다음 구성 요소가 필요합니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 테스트 케이스\n- 메트릭\n- 평가 데이터 세트\n\n다음은 deepeval을 사용하여 이상적인 평가 워크플로우가 어떻게 보이는지에 대한 다이어그램이 있습니다:\n\n![이상적인 평가 워크플로우 다이어그램](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_39.png)\n\ndeepeval에서 메트릭은 특정 관심 기준에 따라 LLM 출력의 성능을 평가하는 데 사용되는 측정 기준으로 작동합니다. 본질적으로 메트릭은 자의적으로 테스트 케이스가 수행하는 매우 중요한 역할을 합니다. deepeval은 빠르게 시작할 수 있도록 몇 가지 기본 메트릭을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- G-Eval\n- Summarization\n- Faithfulness\n- Answer Relevancy\n- Contextual Relevancy\n- Contextual Precision\n- Contextual Recall\n- Ragas\n- Hallucination\n- Toxicity\n- Bias\n\n이미 알고 계신 분들을 위해 RAG (검색 증강 생성)이 무엇인지 모르는 분을 위한 좋은 글이 있습니다. 간단히 말하자면, RAG는 추가적인 맥락을 사용하여 맞춤 출력물을 생성하는 LLMs를 보완하는 방법으로, 챗봇을 만드는 데 탁월합니다. 이는 검색기와 생성기로 구성됩니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_40.png)\n\n아래는 RAG의 일반적인 작업 흐름입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- RAG 시스템은 입력을 받습니다.\n- 리트리버는 이 입력을 사용하여 우리의 지식 베이스(대부분의 경우에는 벡터 데이터베이스인 요즘)에서 벡터 검색을 수행합니다.\n- 제너레이터는 검색 결과 문맥과 사용자 입력을 추가 문맥으로 받아 맞춤 출력을 생성합니다.\n\n이것을 기억해 두세요 — 고품질 LLM 출력물은 훌륭한 리트리버와 제너레이터의 산물입니다. 그래서 좋은 RAG 척도는 우리의 RAG 리트리버나 제너레이터를 신뢰하고 정확하게 평가하는 데 초점을 맞춥니다. (사실, RAG 측정 항목은 원래 참조 없이 측정하는 항목으로 설계되었습니다. 즉, 그라운드 트루스가 필요하지 않아서 실제 운영 환경에서도 사용할 수 있습니다.)\n\n## 8.1.1. 충실도\n\n충실도는 RAG 측정 항목 중 하나로, 우리 RAG 파이프라인의 LLM/제너레이터가 검색 문맥에 제시된 정보와 사실적으로 일치하는지를 평가합니다. 그러나 충실도 측정을 위해 어떤 점수 지표를 사용해야 할까요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n스포일러 주의: QAG Scorer는 명확한 목표가 있는 평가 작업에서 뛰어난 성능을 발휘하여 RAG 지표에 대해 가장 우수한 스코어러입니다. 충실성을 고려할 때, LLM 출력에서 특정 검색 문맥에 대한 진실된 주장의 비율으로 정의한다면, 다음 알고리즘을 따라 QAG를 사용하여 충실성을 계산할 수 있습니다:\n\n- LLM을 사용하여 출력에서 발표된 모든 주장을 추출합니다.\n- 각 주장에 대해 검색된 문맥의 각 노드와 동의하는지 아니면 모순되는지 확인합니다. 이 경우 QAG의 닫힌 질문은 다음과 같을 것입니다: \"주어진 주장이 참조 텍스트와 일치합니까\", 여기서 \"참조 텍스트\"는 각 개별 검색된 노드입니다. (주목할 점은 답변을 '예', '아니오' 또는 '잖아' 중 하나로 제한해야 합니다. '잖아' 상태는 검색 문맥이 '예' 또는 '아니오' 답변을 제공할 충분한 정보가 없는 극단적인 경우를 나타냅니다.)\n- 진실된 주장의 총 수('예'와 '잖아')를 합하고, 이를 주장의 총 수로 나눕니다.\n\n이 방법은 LLM의 고급 추론 능력을 활용하여 정확성을 보장하면서 LLM이 생성한 점수의 신뢰성 없음을 피해, G-Eval보다 우수한 점수 부여 방법으로 만들어줍니다.\n\n만약 이를 구현하기가 너무 복잡하다고 느낀다면, DeepEval을 사용할 수도 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n# 설치\npip install deepeval\n# OpenAI API 키를 환경 변수로 설정\nexport OPENAI_API_KEY=\"...\"\n```\n\n```js\nfrom deepeval.metrics import FaithfulnessMetric\nfrom deepeval.test_case import LLMTestCase\n\ntest_case=LLMTestCase(\n  input=\"...\",\n  actual_output=\"...\",\n  retrieval_context=[\"...\"]\n)\n\nmetric = FaithfulnessMetric(threshold=0.5)\n\nmetric.measure(test_case)\nprint(metric.score)\nprint(metric.reason)\nprint(metric.is_successful())\n```\n\nDeepEval은 평가를 테스트 케이스로 다룹니다. 여기서 actual_output은 단순히 우리의 LLM 출력입니다. 또한, 충성성은 LLM-Eval인 만큼 최종 점수의 이유를 알 수 있습니다.\n\n## 8.1.2. 답변 관련성\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n답변 적합성은 RAG 지표 중 하나로, 우리의 RAG 생성기가 간결한 답변을 출력하는지를 평가합니다. 입력에서 관련성이 있는 문장의 비율을 결정하여 계산할 수 있습니다 (즉, 관련 문장 수를 전체 문장 수로 나눈 것).\n\n강력한 답변 적합성 지표를 만들기 위한 핵심은 검색 컨텍스트를 고려하는 것입니다. 추가 컨텍스트가 seemingly irrelevant 문장의 적합성을 정당화할 수 있습니다. 다음은 답변 적합성 지표의 구현 방법입니다:\n\n```js\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\ntest_case=LLMTestCase(\n  input=\"...\",\n  actual_output=\"...\",\n  retrieval_context=[\"...\"]\n)\n\nmetric = AnswerRelevancyMetric(threshold=0.5)\nmetric.measure(test_case)\nprint(metric.score)\nprint(metric.reason)\nprint(metric.is_successful())\n```\n\n(기억하세요, 모든 RAG 지표에 대해 QAG를 사용 중입니다)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 8.1.3. 맥락 정확도\n\n맥락 정확도는 우리의 RAG 파이프라인 리트리버의 품질을 평가하는 RAG 메트릭스입니다. 맥락 메트릭스에 대해 이야기할 때, 주로 검색된 컨텍스트의 적합성에 관심을 둡니다. 높은 맥락 정확도 점수는 검색된 컨텍스트에서 관련 있는 노드들이 관련 없는 것보다 더 높게 순위되어 있는 것을 의미합니다. 이는 LLM (Large Language Models)이 검색된 컨텍스트에서 앞부분에 나타나는 노드 정보에 더 많은 가중치를 주기 때문에 최종 출력의 품질에 영향을 줍니다.\n\n```js\nfrom deepeval.metrics import ContextualPrecisionMetric\nfrom deepeval.test_case import LLMTestCase\n\ntest_case=LLMTestCase(\n  input=\"...\",\n  actual_output=\"...\",\n  # 예상 출력은 LLM의 \"이상적\" 출력이며, 맥락 메트릭에 필요한 추가 매개변수입니다\n  expected_output=\"...\",\n  retrieval_context=[\"...\"]\n)\n\nmetric = ContextualPrecisionMetric(threshold=0.5)\nmetric.measure(test_case)\nprint(metric.score)\nprint(metric.reason)\nprint(metric.is_successful())\n```\n\n## 8.1.4. 맥락 회수\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n컨텍스트 리콜(Contextual Recall)은 리트리버-증가 생성기(Retriever-Augmented Generator, RAG)를 평가하는 추가 지표입니다. 이는 예상 출력물 또는 그라운드 트루스에 있는 문장들 중에서 검색된 컨텍스트 노드에 속하는 비율을 결정하여 계산됩니다. 높은 점수는 검색된 정보와 예상 출력물 간의 더 큰 일치를 나타내며, 이는 리트리버가 생성기에 돕기 위해 관련성 있고 정확한 콘텐츠를 효과적으로 소싱하고 있다는 것을 나타냅니다.\n\n```js\nfrom deepeval.metrics import ContextualRecallMetric\nfrom deepeval.test_case import LLMTestCase\n\ntest_case=LLMTestCase(\n  input=\"...\",\n  actual_output=\"...\",\n  # 예상 출력물은 당신의 LLM의 \"이상적인\" 출력물이며, 이는 컨텍스트 메트릭에 필요한 추가 매개변수입니다\n  expected_output=\"...\",\n  retrieval_context=[\"...\"]\n)\nmetric = ContextualRecallMetric(threshold=0.5)\n\nmetric.measure(test_case)\nprint(metric.score)\nprint(metric.reason)\nprint(metric.is_successful())\n```\n\n## 8.1.5. 컨텍스트 관련성\n\n아마 가장 이해하기 쉬운 메트릭인 컨텍스트 관련성은 단순히 주어진 입력과 관련이 있는 검색된 컨텍스트 문장들의 비율을 의미합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nfrom deepeval.metrics import ContextualRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\ntest_case = LLMTestCase(\n  input=\"...\",\n  actual_output=\"...\",\n  retrieval_context=[\"...\"]\n)\nmetric = ContextualRelevancyMetric(threshold=0.5)\n\nmetric.measure(test_case)\nprint(metric.score)\nprint(metric.reason)\nprint(metric.is_successful())\n```\n\n# 8.2. 세련된 메트릭\n\n\"세련된 메트릭\"이라고 말할 때, LLM 자체를 평가하는 메트릭을 의미합니다. 비용 및 성능 이점을 제외하고, LLM은 종종 다음과 같은 이유로 세밀하게 조정됩니다:\n\n- 추가 문맥 지식 통합.\n- 동작 조정.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n만약 모델을 세밀하게 조정하려면, 구글 Colab 내에서 LLAMA-2를 2시간 이내에 세밀하게 조정하는 방법에 대한 스텝바이스텝 튜토리얼이 있습니다. 이로 인해 평가를 수행하며 할루신레이션 메트릭과 관련된 것들 중 일부를 확인할 수 있습니다.\n\n## 8.2.1. 환영(Laughter)\n\n누군가 이것이 충실도 메트릭과 동일하다는 것을 알 수 있을 것입니다. 비슷하지만, 세밀한 조정에서의 환영은 출력물의 정확한 사실을 찾기 어려운 경우가 많기 때문에 더 복잡합니다. 이 문제를 해결하기 위해 SelfCheckGPT의 제로-샷 방법을 활용하여 LLM 출력물에서 환영된 문장의 비율을 샘플링할 수 있습니다.\n\n```js\nfrom deepeval.metrics import HallucinationMetric\nfrom deepeval.test_case import LLMTestCase\n\ntest_case=LLMTestCase(\n  input=\"...\",\n  actual_output=\"...\",\n  # 'context'은 'retrieval_context'와 같지 않음에 주의하세요.\n  # 검색 컨텍스트는 RAG 파이프라인과 관련이 깊으나,\n  # 컨텍스트는 주어진 입력에 대한 이상적인 검색 결과이며,\n  # 일반적으로 세밀한 조정에 사용되는 데이터셋에 존재합니다.\n  context=[\"...\"],\n)\n\nmetric = HallucinationMetric(threshold=0.5)\nmetric.measure(test_case)\nprint(metric.score)\nprint(metric.is_successful())\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 방법은 매우 비싸질 수 있으므로 지금은 NLI 스코어러를 사용하고 일부 컨텍스트를 수동으로 제공하여 그라운드 트루스로 사용하는 것을 제안합니다.\n\n## 8.2.2. 유해성\n\n유해성 메트릭은 텍스트가 모욕적이고 해로운 또는 부적절한 언어를 얼마나 포함하고 있는지를 평가합니다. Detoxify와 같은 즉시 사용 가능한 사전 훈련된 모델은 BERT 스코어러를 활용하여 유해성을 평가하는 데 사용할 수 있습니다.\n\n```js\nfrom deepeval.metrics import ToxicityMetric\nfrom deepeval.test_case import LLMTestCase\n\nmetric = ToxicityMetric(threshold=0.5)\ntest_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # 여기에 귀하의 LLM 애플리케이션의 실제 출력을 대체하세요\n    actual_output = \"We offer a 30-day full refund at no extra cost.\"\n)\n\nmetric.measure(test_case)\nprint(metric.score)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그러나 이 방법은 부적절한 욕설, 모욕 또는 저속한 단어가 댓글에 포함되어 있으면 성격이나 저자의 의도와 관계없이 유해로 분류될 수 있기 때문에 정확하지 않을 수 있습니다. 예를 들어, 유머적이거나 자기 비하적인 내용일지라도 유해로 분류될 가능성이 있습니다.\n\n이 경우 독성에 대한 사용자 정의 기준을 정의하기 위해 G-Eval을 사용하는 것을 고려해볼 수 있습니다. 사실, G-Eval의 유즈 케이스에 중립적인 특성이 이러한 이유로 저는 그것을 매우 좋아하는 주요 이유입니다.\n\n```js\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase\n\ntest_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # 실제 LLM 애플리케이션에서 출력된 내용으로 이 부분을 대체하시기 바랍니다.\n    actual_output = \"We offer a 30-day full refund at no extra cost.\"\n)\n\ntoxicity_metric = GEval(\n    name=\"독성\",\n    criteria=\"독성 - 실제 출력에 유머가 아닌 무례하거나 해로운 언어가 포함되어 있는지를 결정합니다.\",\n    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n)\n\nmetric.measure(test_case)\nprint(metric.score)\n```\n\n## 8.2.3. 편향\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n편향 측정 항목은 텍스트 콘텐츠 내의 정치적, 성별 및 사회적 편향과 같은 측면들을 평가합니다. 이것은 특히 맞춤형 LLM이 의사결정 과정에 참여하는 응용 프로그램에서 매우 중요합니다. 예를 들어, 편견 없는 추천으로 은행 대출 승인을 지원하거나, 채용에서는 후보자가 면접에 초대되어야 할지를 결정하는 데 도움이 됩니다.\n\n독성과 유사하게, 편향은 G-Eval을 사용하여 평가할 수 있습니다. (하지만 오해하지 마세요, QAG도 독성과 편향과 같은 메트릭에 대한 적절한 점수 판단자가 될 수 있습니다.)\n\n```js\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase\n\ntest_case = LLMTestCase(\n    input=\"이 신발이 맞지 않을까?\",\n    # 실제 LLM 애플리케이션의 결과물로 대체하세요\n    actual_output = \"30일 동안의 전액 환불을 추가 비용 없이 제공해드립니다.\"\n)\n\ntoxicity_metric = GEval(\n    name=\"편향\",\n    criteria=\"편향 - 실제 결과에 인종, 성별 또는 정치적 편향이 포함되어 있는지 확인합니다.\",\n    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n)\n\nmetric.measure(test_case)\nprint(metric.score)\n```\n\n편향은 매우 주관적인 문제로, 다양한 지리적, 정치적 및 사회적 환경에서 크게 달라집니다. 예를 들어, 한 문화에서 중립적으로 여겨지는 언어나 표현이 다른 문화에서는 다른 함의를 내포할 수도 있습니다. (이것이 왜 편향에 대한 소수의 데이터 평가가 잘 작동하지 않는지도 그 이유입니다.)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n해결책으로는 평가를 위해 맞춤형 LLM을 세밀하게 조정하거나 맥락 속 학습을 위해 매우 명확한 평가 기준을 제시하는 것이 가능합니다. 그래서 제 생각으로는 편향이 가장 구현하기 어려운 측정 지표라고 믿어요.\n\n사용 사례별 지표\n\n요약\n\n요약하면(아무 농담이나 한다는 거 아니에요), 모든 좋은 요약은:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 원본 텍스트와 사실적으로 일치합니다.\n- 원본 텍스트의 중요 정보를 포함하고 있습니다.\n\nQAG를 사용하여 최종 summarization 점수를 계산하기 위해, 사실적 일치와 포함 점수를 계산할 수 있습니다. DeepEval에서는 두 중간 점수 중 작은 값을 최종 summarization 점수로 채택합니다.\n\n```js\nfrom deepeval.metrics import SummarizationMetric\nfrom deepeval.test_case import LLMTestCase\n\n# 이것이 summarization이 필요한 원본 텍스트입니다\ninput = \"\"\"\n'inclusion score'는 총평가 문항 중 요약과 원본 문서 둘 다 '예'로 답하는 비율로 계산됩니다. 이 방법은 요약물이 원본 텍스트에서 주요 정보뿐만 아니라 정확하게 표현되도록 보장합니다. 더 높은 inclusion score는 더욱 포괄적이고 충실한 요약을 나타내며, 이는 요약물이 원본 콘텐츠의 중요한 포인트와 세부 정보를 효과적으로 요약했음을 나타냅니다.\n\"\"\"\n\n# 이것이 요약문입니다. 실제 LLM 애플리케이션의 출력으로 대체해 주세요\nactual_output = \"\"\"\n'inclusion score'는 요약이 원본 텍스트로부터 주요 정보를 얼마나 잘 포착하고 정확하게 표현하는지를 측정하는 점수이며, 높은 점수는 더 높은 포괄성을 보여줍니다.\n\"\"\"\n\ntest_case = LLMTestCase(input=input, actual_output=actual_output)\nmetric = SummarizationMetric(threshold=0.5)\n\nmetric.measure(test_case)\nprint(metric.score)\n```\n\n# 8.2. Ragas\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n![Image](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_41.png)\n\nJust like in any machine learning system, the performance of individual components within the LLM and RAG pipeline has a significant impact on the overall experience. Ragas offers metrics tailored for evaluating each component of our RAG pipeline in isolation.\n\n- Faithfulness\n- Answer relevancy\n- Context recall\n- Context precision\n- Context relevancy\n- Context entity recall\n\nEnd-to-End Evaluation\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n**파이프라인의 최종 성능을 평가하는 것도 매우 중요합니다. 이는 사용자 경험에 직접적인 영향을 미치기 때문입니다. Ragas는 우리 파이프라인의 전반적인 성능을 평가하는 데 사용할 수 있는 메트릭을 제공하여 포괄적인 평가를 보장합니다.**\n\n- 의미 유사성 확인\n- 정확성 확인\n\n## 8.2.1. 충실도\n\n이것은 생성된 답변의 사실 일관성을 주어진 맥락에 대해 측정합니다. 답변과 검색된 맥락에서 계산됩니다. 답변은 (0,1) 범위로 축척화됩니다. 높을수록 좋습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n생성된 답변은 주어진 맥락에서 모든 주장들이 추론될 수 있다면 신뢰할 수 있는 것으로 간주됩니다. 이를 계산하기 위해 먼저 생성된 답변에서의 주장 집합이 식별됩니다. 그런 다음 각 주장이 주어진 맥락과 일치하는지 여부를 확인하여 주어진 맥락에서 어떤 주장을 추론할 수 있는지 확인합니다. 신뢰도 점수는 다음과 같이 계산됩니다.\n\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_42.png)\n\n\n```python\nfrom datasets import Dataset\nfrom ragas.metrics import faithfulness\nfrom ragas import evaluate\n\ndata_samples = {\n    'question': ['첫 번째 슈퍼볼이 언제 열렸습니까?', '가장 많은 슈퍼볼을 우승한 팀은 누구인가요?'],\n    'answer': ['첫 번째 슈퍼볼은 1967년 1월 15일에 개최되었습니다.', '가장 많은 슈퍼볼 우승은 뉴 잉글랜드 파트리어츠가 차지했습니다.'],\n    'contexts' : [['첫 번째 AFL-NFL 월드 챔피언십 경기는 1967년 1월 15일에 미국의 LA 로스엔젤레스 메모리얼 콜로시움에서 열린 미식 축구 경기였습니다.'],\n    ['그린 베이 패커스...위스콘신 그린베이.','패커스는...풋볼 컨퍼런스에서 경쟁합니다.']],\n}\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset,metrics=[faithfulness])\nscore.to_pandas()\n```\n\n계산 결과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n낮은 충실도 답변을 사용하여 충실도가 어떻게 계산되었는지 살펴보겠습니다:\n\n단계 1: 생성된 답변을 개별 문장으로 분해합니다.\n\n- 문장:\n- 문장 1: \"아인슈타인은 독일에서 태어났다.\"\n- 문장 2: \"아인슈타인은 1879년 3월 20일에 태어났다.\"\n\n단계 2: 생성된 각 문장에 대해 주어진 맥락에서 유추할 수 있는지 확인합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 문장 1: 네\n- 문장 2: 아니요\n\n단계 3: 위에 나와 있는 공식을 사용하여 충실도를 계산합니다.\n\n![공식 이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_43.png)\n\n## 8.2.2. 답변 관련성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n평가 지표인 답변 관련성은 생성된 답변이 제시된 프롬프트와 얼마나 적합한지를 평가하는 데 초점을 맞춥니다. 불완전하거나 중복 정보가 포함된 답변에는 낮은 점수가 할당되고 더 높은 점수는 더 좋은 관련성을 나타냅니다. 이 지표는 질문, 맥락 및 답변을 사용하여 계산됩니다.\n\n답변 관련성은 원래 질문과 관련성이 높은 인공 질문들의 평균 코사인 유사도로 정의됩니다. 이러한 인공 질문들은 답변을 기반으로 생성(역공학화)되었습니다.\n\n답변이 직접적이고 적절하게 원래 질문에 대답할 때 관련성이 있다고 여겨집니다. 중요한 점은 답변의 사실 관련성을 고려하지 않지만 대신 답변이 완전하지 않거나 중복된 세부 정보를 포함하는 경우에는 처벌합니다. 이 점수를 계산하기 위해 LLM은 생성된 답변에 대한 적절한 질문을 여러 번 생성하도록 유도하고 이러한 생성된 질문들과 원래 질문 간의 평균 코사인 유사도가 측정됩니다. 근본적인 아이디어는 생성된 답변이 초기 질문에 정확히 대답하는 경우, LLM이 답변으로부터 생성된 질문을 원래 질문과 일치시킬 수 있어야 한다는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n`\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_45.png\" /\u003e\n\n```js\nfrom datasets import Dataset\nfrom ragas.metrics import answer_relevancy\nfrom ragas import evaluate\n\ndata_samples = {\n    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n}\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset,metrics=[answer_relevancy])\nscore.to_pandas()\n```\n\nCalculation\n\nTo calculate the relevance of the answer to the given question, we follow two steps:\n\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 1: 생성된 답변을 사용하여 다양한 'n'개의 질문들을 대상 답변으로부터 역공학합니다. 예를 들어, 첫 번째 답변에 대해, 다음과 같은 가능한 질문이 LLM에서 생성될 수 있습니다:\n\n- 질문 1: \"프랑스는 유럽의 어느 부분에 위치해 있나요?\"\n- 질문 2: \"프랑스의 지리적 위치는 유럽 내 어디에 있나요?\"\n- 질문 3: \"프랑스가 위치한 유럽의 지역을 정확히 확인할 수 있나요?\"\n\n단계 2: 생성된 질문과 실제 질문 사이의 코사인 유사도 평균을 계산합니다.\n\n기본 개념은 답변이 질문을 올바르게 다룬다면, 원래 질문을 답변만으로 완벽하게 재구성할 수 있다는 점입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 8.2.3. 문맥 정밀도\n\n문맥 정밀도는 모든 지면진실과 관련된 항목이 문맥에 상위로 순위되었는지를 평가하는 지표입니다. 이상적으로는 모든 관련 청크가 최상위 순위에 나와야 합니다. 이 지표는 질문, 지면진실 및 문맥을 사용하여 계산되며, 값은 0과 1 사이의 범위에서 계산되며 높은 점수는 더 나은 정밀도를 나타냅니다.\n\n![이미지](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_46.png)\n\n```js\nfrom datasets import Dataset\nfrom ragas.metrics import context_precision\nfrom ragas import evaluate\n\ndata_samples = {\n    'question': ['첫 번째 슈퍼볼은 언제였나요?', '가장 많은 슈퍼볼을 이긴 팀은 누구입니까?'],\n    'answer': ['첫 번째 슈퍼볼은 1967년 1월 15일에 개최되었습니다', '가장 많은 슈퍼볼을 이긴 팀은 뉴잉글랜드 패트리어츠입니다'],\n    'contexts' : [['첫 번째 AFL-NFL 월드 챔피언십 경기는 1967년 1월 15일에 미국 로스앤젤레스에 위치한 로스앤젤레스 기념 콜리시움에서 열린 미식 축구 경기입니다.'],\n    ['그린 베이 패커스...위스콘신 그린 베이.','패커스는...풋볼 컨퍼런스에 참가합니다']],\n    'ground_truth': ['첫 번째 슈퍼볼은 1967년 1월 15일에 개최되었습니다', '뉴잉글랜드 패트리어츠는 슈퍼볼을 최다 기록인 여섯 번 이겼습니다']\n}\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset, metrics=[context_precision])\nscore.to_pandas()\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n계산\n\n저희가 낮은 맥락 정밀도 예시를 사용하여 맥락 정밀도가 어떻게 계산되었는지 살펴봅시다:\n\n단계 1: 검색된 맥락의 각 청크에 대해 해당 질문에 대한 실제 답변(ground truth)에 해당하는지 여부를 확인합니다.\n\n단계 2: 맥락의 각 청크에 대해 precision@k를 계산합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_47.png\" /\u003e\n\n단계 3: precision@k의 평균을 계산하여 최종 컨텍스트 정밀도 점수에 도달합니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_48.png\" /\u003e\n\n## 8.2.4. 컨텍스트 관련성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 메트릭은 문맥을 검색해서 질문과 문맥을 기반으로 계산하여 중요성을 측정합니다. 값은 (0, 1) 범위 내에 있으며, 높은 값일수록 더 나은 중요성을 나타냅니다.\n\n이상적으로 검색된 문맥은 제공된 질문에 대한 답변을 주는 데 필수적인 정보만 포함해야 합니다. 이를 계산하기 위해, 검색된 문맥 내에서 주어진 질문에 관련된 문장을 식별하여 |�|의 값을 처음에 추정합니다. 최종 점수는 다음 공식에 따라 결정됩니다:\n\n\n![수식](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_49.png)\n\n\n```python\nfrom ragas.metrics import ContextRelevancy\ncontext_relevancy = ContextRelevancy()\n\n# Dataset({\n#     features: ['question','contexts'],\n#     num_rows: 25\n# })\ndataset: Dataset\n\nresults = context_relevancy.score(dataset)\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 8.2.5. 컨텍스트 회상\n\n컨텍스트 회상은 검색된 컨텍스트가 참조 답변과 얼마나 일치하는지를 측정합니다. 참조 답변은 그라운드 트루스로 취급되며, 그라운드 트루스와 검색된 컨텍스트를 기반으로 계산되며, 값은 0에서 1 사이의 범위를 가지며, 높은 값일수록 더 좋은 성능을 나타냅니다.\n\n그라운드 트루스 답변에서 컨텍스트 회상을 추정하기 위해, 그라운드 트루스 답변의 각 문장을 분석하여 검색된 컨텍스트에 속하는지 여부를 결정합니다. 이상적인 시나리오에서는 그라운드 트루스 답변의 모든 문장이 검색된 컨텍스트에 속해야 합니다.\n\n컨텍스트 회상을 계산하는 공식은 다음과 같습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_50.png\" /\u003e\n\n```js\nfrom datasets import Dataset\nfrom ragas.metrics import context_recall\nfrom ragas import evaluate\n\ndata_samples = {\n    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n}\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset,metrics=[context_recall])\nscore.to_pandas()\n```\n\n계산\n\n저희가 낮은 컨텍스트 리콜 예제를 사용하여 어떻게 컨텍스트 리콜이 계산되었는지 살펴보겠습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n단계 1: 정답을 개별 문장으로 나눠보세요.\n\n- 문장:\n- 문장 1: \"프랑스는 서유럽에 있습니다.\"\n- 문장 2: \"수도는 파리입니다.\"\n\n단계 2: 각 정답 문장이 검색된 문맥과 일치하는지 확인해보세요.\n\n- 문장 1: 예\n- 문장 2: 아니요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nStep 3: 위에 나와 있는 공식을 사용하여 컨텍스트 회수율을 계산하세요.\n\n![image](/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_51.png)\n\n# 결론\n\n끝까지 와 주셔서 축하드립니다! 스코어 및 메트릭스의 긴 목록이었는데, 이제 LLM 평가를 위한 메트릭을 선택할 때 고려해야 할 다양한 요소와 선택사항을 알게 되었기를 바랍니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그날의 끝에는, 메트릭 선택은 우리의 사용 사례와 LLM 애플리케이션의 구현에 달려 있습니다. 여기서 RAG 및 파인튜닝 메트릭은 LLM 출력을 평가하는 좋은 시작점입니다. 더 많은 사용 사례별 메트릭을 위해 우리는 가장 정확한 결과를 얻기 위해 G-Eval과 퓨샷 프롬프팅을 사용할 수 있습니다.\n\n# 크레딧\n\n이 블로그 글에서, 우리는 연구 논문, 기술 블로그, 공식 문서, YouTube 비디오 등 다양한 출처에서 정보를 수집했습니다. 각 출처는 해당 이미지 아래에 적절히 표시되었으며, 출처 링크가 제공되었습니다.\n\n아래는 참고 문헌의 통합 목록입니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- https://arxiv.org/abs/2307.03109\n- https://www.confident-ai.com/blog/the-current-state-of-benchmarking-llms\n- https://medium.com/@ashishkhare_68890/generative-ai-evaluating-llms-hugging-face-leaderboard-case-study-c9a1a42223e2\n- https://generativeai.pub/evaluating-llm-applications-1a890fc986f7\n- https://medium.com/@kbdhunga/nlp-model-evaluation-understanding-bleu-rouge-meteor-and-bertscore-9bad7db71170\n- https://explodinggradients.com/all-about-evaluating-large-language-models\n- https://gist.github.com/hshujuan/777d475bb7e44c9fecddb7f42d034430#file-chapter2_table4-md\n- https://amagastya.medium.com/decoding-llm-performance-a-guide-to-evaluating-llm-applications-e8d7939cafce\n- https://docs.ragas.io/en/latest/concepts/metrics/index.html\n\n# 읽어 주셔서 감사합니다!\n\n만약 이 안내서가 여러분의 Python과 머신러닝 이해를 향상시켰다면:\n\n- 박수 👏 또는 여러 번의 박수로 지원을 보여 주세요!\n- 여러분의 박수는 활기 넘치는 Python이나 ML 커뮤니티를 위해 보다 가치 있는 콘텐츠를 만드는 데 도움이 됩니다.\n- 동료 Python 또는 AI/ML 애호가와 이 가이드를 공유해 주세요.\n- 여러분의 피드백은 소중합니다. 이를 통해 저는 미래의 게시물을 영감을 받고 안내할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 함께 소통해요!\n\nVipra\n","ogImage":{"url":"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_0.png"},"coverImage":"/assets/img/2024-05-23-BuildingLLMApplicationsEvaluationPart8_0.png","tag":["Tech"],"readingTime":56},{"title":"전자 상거래에서 테마 기반 분류와 GenAI를 활용한 패션 발견 혁신","description":"","date":"2024-05-23 17:42","slug":"2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI","content":"\nBy\nXingming Qu\n, Gaomin Wu\n, Dan Schonfeld\n그리고\nJonathan Galsurkar\n\n우리는 LLM(대형 언어 모델)을 활용하여 이베이의 패션 추천 시스템을 혁신하여 제품 발견에 대한 맞춤화 및 맥락 감수성을 제공함으로써 이베이의 \"View Item\" 페이지에서 사용자 참여도와 전환율을 크게 향상시켰습니다.\n\n## 소개\n\n대형 언어 모델 (LLM)을 다양한 분야에 통합하는 것은 인공 지능의 발전에서 중요한 마일스톤을 표시했습니다. 이를 통해 사용자 상호작용과 만족도를 향상시키기 위한 혁신적인 솔루션을 선보이고 있습니다. 패션 분야에서 전통적인 벽돌과 모르타르 상점들은 스타일, 행사 적합성에 대한 상세한 토론을 하는 지식 있는 판매원들의 큰 도움을 받습니다. 고객들이 선호도를 이해하고 정리하는 데 도움을 주는 것이 바로 이 환경입니다. 이를 온라인 플랫폼이 재현하는 데 어려움을 겪고 있습니다. 우리는 이 도전에 대처하기 위해 LLM의 기능을 활용하여 이베이의 패션 추천 시스템을 혁신하고 있습니다. 특히, 우리의 접근 방식은 LLM을 활용하여 제품을 일관된 피벗 그룹으로 분류하고 이러한 그룹을 위한 딥 러닝 임베딩 센터를 생성합니다. 이는 이러한 명확히 정의된 주제 내에서 후보 아이템을 조직화된 방식으로 제공하는 독특한 전략을 제공합니다. 이 방법론은 제품 발견 프로세스를 보다 간단하게 만들 뿐만 아니라, 기존 추천 알고리즘의 능력을 능가하는 맞춤화 및 맥락 감수성을 편입하고 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 배경\n\n온라인 패션 쇼핑의 빠른 속도에서 맞춤 및 관련 제품 추천을 제공하는 도전은 특히 두드러집니다. 현재 여성용 패션이나 일반 패션 아이템을 둘러보는 사용자들은 그림 1에 나와 있는 것처럼 특정 기회에 적합한 특정 스타일의 옷을 자주 찾습니다.\n\n![이미지](/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_0.png)\n\n그러나 이 중요한 정보는 판매자가 제공하는 항목 제목이나 설명에서 항상 명확하게 제시되지는 않습니다. 전통적인 매장 쇼핑 경험과는 대조적으로, 매력적인 옷의 스타일, 행사 적합성에 대해 고객과 자세한 토론을 진행하고 그들의 선호도를 더 잘 이해하기 위해 비슷한 제품을 안내해주는 지식 있는 판매원들이 있다면, 우리는 eBay 패션 아이템에 대한 이 맞춤형 지원을 복제하고 개선하려고 합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n물리 매장에서, 옷 판매원들은 패션의 복잡성을 안내하는 데 중요한 역할을 합니다. 그들은 각 제품의 스타일 미학을 논하며, 다양한 상황에 적합한 제품이며, 고객들에게 다른 유사한 제품을 보여주어 그들이 더 잘 이해하고 세련되게 취향을 확립할 수 있도록 도와줍니다. 우리의 목표는 LLM의 힘을 활용하여 이 맞춤형 쇼핑 경험을 온라인으로 재현하는 것입니다. 이를 통해 우리는 바이어가 완벽한 패션 아이템을 찾는 데 도움을 주는 것뿐만 아니라 물리 매장에서 일반적으로 느끼는 맞춤 가이드와 전문지식의 느낌을 재현하는 것을 목표로 합니다. 궁극적으로, eBay의 패션 발견 프로세스를 더 매력적이고 유익하며 만족스럽게 만드는 데 노력하고 있습니다.\n\n## 접근 방식\n\n우리는 문맥과 제품을 고려한 pivot 그룹을 제작했습니다. 그림 2에서 텍스트 pill로 표시된 pivot 그룹은 eBay의 추천 목록을 향상시키기 위해 동적 버튼 역할을 하는 필터로 작용하여 사용자가 관심 있는 주제에 맞게 검색을 세부화할 수 있게 합니다. LLM이 생성한 이 pivot 그룹은 사용자에게 선택한 문맥과 공감하는 제품의 선별된 제품 목록으로 안내를 제공합니다. eBay 재고의 더 포괄적인 탐색을 가능하게 함으로써, pivot 그룹은 변환 확률을 높여줌으로써 쇼핑객들이 자신의 패션 감성과 완벽하게 일치하는 아이템을 찾아 구입하는 것을 더욱 용이하게 만들어 온라인 브라우징을 보다 보람있는 경험으로 만드는 데 도움을 줍니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_1.png\" /\u003e\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n![이미지](/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_2.png)\n\n저희는 새로운 추천 시스템의 효과와 적응성을 입증하기에 이상적인 도메인인 다이내믹하고 스타일과 트렌드의 변화가 큰 여성 드레스 카테고리를 연구 대상으로 선택했습니다. 우리는 사전 오프라인 단계에서 중심 그룹을 생성하며, 저희 방법론은 여성 패션 부문에서 가장 인기 있는 제품과 해당 \"유사 스폰서 제품\"을 선택하여 최신 스타일 흐름과 소비자 성향을 보장하는 것입니다. LLM의 분석적이고 추론적인 능력을 활용하여 이러한 패션 제품을 그들의 고유한 특성에 기반하여 독특한 주제 그룹으로 세심하게 분류했습니다. 이 전략적인 분류를 통해 여성 패션의 주제 스펙트럼 내에서 다양한 항목이 어떻게 배치되는지 세밀하게 이해할 수 있었습니다. 이 주제 그룹의 세분화 이후, 각 그룹의 항목에서 중요한 특징을 추출하여 풍부한 벡터 임베딩을 생성했습니다. 임베딩은 텍스트 제목이나 텍스트와 시각 정보를 모두 통합한 다중 모달 수단을 통해 유래했습니다.\n\n이 과정은 각 중심 그룹의 평균 임베딩을 계산하여 계속되었습니다. 이는 해당 그룹의 중심적인 임베딩 대표로 작용합니다. 이 근본적인 임베딩은 후속적인 온라인 추천 단계에서 중요한 역할을 합니다. 여기서 후보 항목은 아이템 임베딩을 중심 그룹 임베딩과의 유사성에 따라 동적으로 할당받습니다. 할당은 아이템 임베딩과 중심 그룹 임베딩 사이의 유사성을 활용하여 진행되며, 이를 위해 코사인 거리가 측정 항목으로 사용됩니다. 이 방법론을 통해 추천은 맥락에 부합하고 현재 트렌드와 일치하며, 전반적인 추천 시스템의 효율성을 향상시키는 것이 보장됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n저희 방법론의 초기 단계에서는 LLMs를 활용하여 이베이의 여성 드레스 카테고리를 조직하는 중요한 과정인 피벗 그룹을 위한 이름과 설명을 체계적으로 생성합니다. 이 프로세스는 아래의 Figure 3에서 시각적으로 설명되어 있습니다:\n\n![Figure 3](/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_3.png)\n\n이 프로세스는 여성 드레스 카테고리 내에서 인기 있는 최상위 시드 아이템을 선택하는 것으로 시작하여, 이를 통해 각 시드마다 수십 개의 추천 후보 아이템을 무작위로 샘플링하여 수만 개의 아이템으로 이루어진 데이터셋을 형성합니다. 아이템 제목 및 최상위 아이템 측면과 같은 정보는 추출되어 LLM에 제공될 데이터셋을 형성합니다. 이러한 아이템은 일괄로 그룹화되고 각 배치에 대해 LLM은 배치 내 아이템을 특성에 따라 그룹화하여 고유한 피벗 그룹 이름과 설명을 생성하는 작업을 수행합니다. 이러한 이름과 요약은 유용한 정보를 제공하도록 특별히 설계되어 구매자가 판단 과정을 더할 수 있도록 돕는 통찰력을 제공합니다. 이 과정은 그림 2에 나와 있는 것처럼 유용한 정보를 제공하여 구매자의 결정 과정을 풍부하게 해줍니다. 이 프로세스는 고유한 이름과 설명을 갖는 다수의 피벗 그룹을 생성합니다.\n\n이 그룹 이름과 설명이 고객 기반에 적합하고 관련성이 있는지를 확인하기 위해 품질 보증 전문가 팀이 철저한 검토를 진행합니다. 이 방법론 접근은 아이템 분류 프로세스를 최적화할 뿐만 아니라 피벗 그룹이 여성 드레스 부문 내에서 트렌드와 관련 테마를 대표하는지를 보장합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 회전 표현 생성\n\n회전 그룹의 이름과 설명을 최종 확정한 후, 초기 단계와 동일한 샘플링 방법을 사용하여 다른 날짜에 데이터 세트를 추가로 생성합니다. 이는 첫째로, 계절 변화, 신흥 스타일 및 소비자 행동의 변화를 반영한 가장 최신 데이터를 기반으로 회전 표현 생성이 이루어져야 하기 때문입니다. 둘째로, 이러한 실천은 훈련 및 테스트 데이터 세트를 효과적으로 분리하여 LLM의 다양한 시간 스냅샷을 통해 정확하고 잘 적응하는 능력을 더 체계적으로 시험할 수 있습니다.\n\n![image](/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_4.png)\n\n데이터 세트는 일괄 처리로 분할되고 LLM을 통해 처리됩니다. 이때, LLM에게 각 항목을 해당 특성을 기반으로 적절한 회전 그룹으로 분류하도록 안내하는 특별히 설계된 프롬프트를 제공합니다. 이 단계는 항목을 각각의 그룹으로 매핑하는 데 중요하며, 우리에게는 각 회전 그룹이 의도된 주제와 특성을 반영한 올바른 항목을 포함하고 있는지 확인할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n분류 과정을 따라, 우리는 각 그룹 내 모든 항목에 대한 항목 임베딩을 추출하기 위해 임베딩 룩업 캐시인 \"NuKV\"에서 뽑아낸다. 이를 통해 각 피벗 그룹의 평균 임베딩 벡터를 계산할 수 있다. 기억해야 할 것은 기존 임베딩의 두 가지 유형을 활용할 수 있다는 것이다: eBERT 제목 임베딩 또는 항목 멀티모달 임베딩. 이 벡터는 그룹 내 항목들의 집합적 특성을 대표하는 중심 표현 또는 피벗 센터로 작용한다.\n\n도식화된 접근 방식은 Figure 4에 개요되어 있으며, 각 피벗 그룹이 일관된 테마를 대표하고 있음을 보장하는데 그치지 않고, 그룹 간 항목 분포의 보다 상세한 이해를 용이하게 한다. 각 그룹에 대한 평균 임베딩을 계산함으로써, 우리는 후속 추천 프로세스를 위한 견고한 프레임워크를 정립하고, 제품 제안의 맞춤화와 사용자에 대한 관련성을 향상시키는데 기여한다.\n\n## 온라인 과제\n\n저희 추천 서비스 내에서는 항목의 실시간 할당 및 추천에 대한 정교한 메커니즘을 개발했습니다, Figure 5에 나와있습니다. 이 과정은 상류 단계에서 기억된 항목을 검색하는 것으로 시작되며, 비슷한 후원된 항목의 관련성과 최신 선택을 수집하는 데 중요한 단계입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_5.png\" /\u003e\n\n다음으로, 우리는 항목이 특성과 가장 일치하는 중심 그룹에 동적으로 할당되는 중심 할당 단계를 소개합니다. 항목 임베딩과 중심 그룹 임베딩 사이의 코사인 거리를 활용하여 가장 가까운 일치를 결정합니다. 이 방법은 각 항목이 가장 적합한 중심 그룹에 할당되어 사용자에게 제공되는 추천의 관련성과 일관성을 보장합니다.\n\n각 항목을 해당 중심 그룹에 할당한 후, 각 그룹 내 항목은 선행 순위 결정 단계에서 유도된 개별 순위 점수에 따라 순위가 매겨집니다. 여기서는 항목을 XGBoost 랭커로 훈련하여 속성에 따라 항목에 점수를 매기고 eBay의 비즈니스 규칙을 사용합니다. 중심 그룹 자체의 표시 순서를 최적화하기 위해 각 그룹은 중심 내 모든 항목의 순위 점수를 집계한 평균으로 계산된 점수가 할당되어 구매자에게 총괄적인 관련성과 매력에 기반한 중심 그룹의 전략적 순서를 허용합니다.\n\n## 온라인 실험 결과\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n우리의 신기술인 Pivot Group 방법이 사용자 경험 및 추천 관련성 향상에 미치는 영향을 엄밀히 평가하기 위해 여성 드레스 카테고리 내 미국 사이트에 중점을 둔 A/B 테스트를 실시했습니다. 이 실험은 데스크톱 웹 뷰 아이템 페이지의 두 번째 위치에 배치된 기존 추천 변형(Control)과 Pivot Group 변형(Treatment)의 성능을 비교하기 위해 설계되었습니다. 본 실험은 가시성과 사용자 참여를 보장하기 위해 두 그룹 간의 트래픽을 공평하고 정확하게 분배하는 데 초점을 맞추었습니다.\n\n저희의 온라인 실험 결과, 주요 참여 지표인 구매(Purchases) 및 클릭(Clicks)에서 이전에 데스크톱 웹 플랫폼에서 사용한 모델과 비교하여 통계적으로 유의한 개선 사항이 발견되었습니다:\n\n![Figure 6](/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_6.png)\n\n![Figure 7](/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n결과는 Pivot Generation 접근 방식이 사용자의 관심을 끌 뿐만 아니라 구매를 유도하는 데도 효과적임을 확인합니다.\n\n## 결론 및 다음 단계\n\n아이템 특성을 기반으로 Pivot 그룹을 생성하기 위해 LLMs를 혁신적으로 활용함으로써, 이 방법은 제품 추천의 맞춤화와 관련성을 크게 향상시켰습니다. 우리의 포괄적인 접근 방식은 사용자 선호도와 시장 트렌드에 대한 심층적인 이해를 시사합니다.\n\n우리의 방법론은 다양한 스타일 범주로 특징 지어지는 카테고리를 포함한 광범위한 범주에 대한 확장 및 적응 가능성을 명확히 보여줍니다. 우리의 파이프라인의 기본 원리는 아이템을 주제별 그룹으로 분할하는 데 LLMs를 활용하는 것으로, 이는 본질적으로 변화 가능하고 확장 가능한 프로세스입니다. 이 유연성은 우리의 방식이 핵심 프롬프트 구조를 심각하게 수정하지 않고도 다른 범주에 적용될 수 있음을 시사합니다. Pivot 그룹 방법의 확장 가능성은 여성 드레스 이상의 효과를 보여주는 그림 7 및 8에 나타난 결과와 함께 핸드백이나 구두와 같은 다른 다양한 범주에 그 유용성을 확장하는 개념 증명을 통해 입증되었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\n\u003cimg src=\"/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_8.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_9.png\" /\u003e\n\n그러나 저희의 파이프라인의 일반적인 프레임워크가 넓은 적용 범위를 제공하는 한편, 특정 카테고리별 세부 사항이 프롬프트에 맞게 조정되어야 하는 경우가 있을 수 있습니다. 이러한 맞춤화는 서로 다른 카테고리의 독특한 특성과 스타일적 요소가 정확히 포착되고 피벗 그룹화에 반영되도록 보장합니다.\n\nChen Xue\n그리고 Xuyan Zhou도 이 기사에 기고했습니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\neBay TechBlog\n","ogImage":{"url":"/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_0.png"},"coverImage":"/assets/img/2024-05-23-TransformingFashionDiscoveryinE-CommercethroughTheme-BasedCategorizationwithGenAI_0.png","tag":["Tech"],"readingTime":8},{"title":"LLM이 코딩에 적합하지 않은 이유  2부","description":"","date":"2024-05-23 17:40","slug":"2024-05-23-WhyLLMsarenotGoodforCodingPartII","content":"\n\n![WhyLLMsarenotGoodforCodingPartII_0](/assets/img/2024-05-23-WhyLLMsarenotGoodforCodingPartII_0.png)\n\n이 시리즈의 첫 번째 기사 \"LLM은 코딩에 좋지 않은 이유\"를 게시한 후에는 소셜 미디어에서 다음과 같은 여러 댓글을 받았어요:\n\n이 기사 시리즈의 목적은 대형 언어 모델(Large Language Models, LLM)을 사용하는 것을 막는 게 아니라, LLM을 더 효과적인 코딩 도우미로 만들기 위해 개선해야 할 주요 분야를 식별하는 것입니다.\n\nChatGPT와 같은 LLM은 일부 상황에서 유용할 수 있지만, 종종 문법적으로는 정확할 수 있지만 최적이 아니거나 기능적으로 심지어 틀린 코드를 생성합니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이전 기사에서는 토크나이저의 중요성, 코드에 적용될 때의 문맥 창의 복잡성, 그리고 훈련의 성격이 이러한 모델의 성능에 영향을 미칠 수 있는 방법에 대해 논의했습니다.\n\n이 두 번째 기사에서는 코딩 작업에 사용되는 데 필요한 모델의 훈련 유형을 보다 깊이 있게 탐구하고, LLM이 본래 코딩 업무에 능숙하지 않은 이유 중 하나인 최신 라이브러리 및 코드 기능을 최신 상태로 유지하는 도전도 살펴볼 것입니다.\n\n# 코딩 모델의 훈련\n\nLLM 기반의 코딩 어시스턴트는 현실입니다. GitHub Copilot은 오늘 가장 유명한 제품 중 하나로, 개발 환경 내에서 코드 완성 및 채팅 지원 기능을 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개발자들이 말했듯이, \"GitHub Copilot은 확률적 결정을 사용하여 제안을 생성합니다.\" 제품이 작동하는 방법에 대해 더 자세히 살펴보려면 문서를 자세히 살펴보세요:\n\n![GitHub Copilot Screenshot](/assets/img/2024-05-23-WhyLLMsarenotGoodforCodingPartII_1.png)\n\n커서를 어디에 두든지 제안을 받게 됩니다.\n\n그런데 한 가지 더 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nGeneral LLM (Large Language Models)는 토큰 시퀀스가 주어졌을 때 다음 토큰을 예측하는 데 훈련되었습니다. 이는 커서 바로 전 줄까지만 추론에 사용된다는 의미인데요 (왼쪽에서 오른쪽으로 생성한 결과).\n\n양방향 모델은 왼쪽과 오른쪽 컨텍스트를 모두 사용하여 예측을 수행하는 반면, 실제로는 ChatGPT와 같은 단방향 모델이 코드에서 효율적으로 사용되도록 해결책을 사용합니다.\n\n코딩 작업을 위해 기본 LLM을 준비하는 가장 일반적인 기술은 코드 데이터셋에서 적응형 미세 조정을 수행하는 것입니다. 자연어에서 모델을 구체적인 주제에 특화시킬 때 사용되는 기법인데요. DeepMind의 Yujia Li가 \"AlphaCode로 대회 수준의 코드 생성\"이라는 제목의 기사에서 이 프로세스를 설명했습니다:\n\n![AlphaCode](/assets/img/2024-05-23-WhyLLMsarenotGoodforCodingPartII_2.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n위 다이어그램의 (A) 섹션에서 볼 수 있듯이, 코딩을 위한 일반적인 LLMs는 자연어로 훈련된 기본 모델을 기반으로 구축되며 코드 베이스에서 사전 훈련을 받습니다. 예를 들어, print(\", the model is trained to predict the completion hello\")라는 문맥이 주어지면, 모델은 좌측에서 우측으로 생성하여 완성 hello를 예측하는 것을 훈련합니다.\n\n그런 다음, 모델은 (B)에 나타난 대로 코딩 문제와 그 해결책을 사용하여 미세 조정됩니다. 따라서 주어진 코딩 문제에 대해 모델은 해결책을 생성하도록 훈련됩니다.\n\n코드 데이터셋에서 LLMs를 사전 훈련시킴으로써, 이러한 모델은 프로그래밍 언어에 특정한 패턴, 스타일 및 구조를 배울 수 있습니다. 또한, 미세 조정 데이터셋에는 코드 완성 및 인필링과 같은 다양한 코딩 작업이 포함되어 있어야 하며, 이는 모델이 앞선 문맥을 기반으로 적절한 코드 조각을 예측하는 데 도움이 될 것입니다.\n\n모델을 훈련한 후, 모델이 올바른 문맥을 인식하도록 보장하기 위한 또 다른 전략은 추론 중에 프롬프트의 일부로 전달하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n예를 들어, 코드 채우기를 위해 프롬프트에 주변 코드가 포함될 수 있고, 이를 통해 고유 방향성 능력에 맞게 작업을 재구성할 수 있습니다.\n\n# LLM(Large Language Models)가 코딩 보조 도구로서 직면하는 도전\n\n시리즈의 첫 번째 글에서 LLM의 코드 작성에 대한 제약과 그 원인에 대해 알아보았습니다.\n\n그러나 놓치지 않아야 할 중요한 제한 사항이 있습니다: 최신 패키지 및 기능들과의 최신성 유지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n소프트웨어 개발에서는 효과적이고 안전한 코드를 생성하기 위해 이것이 필수적이지만, 실제로는 두 가지 이유로 LLM에 대해 매우 어려울 수 있습니다:\n\n- 정적 지식 베이스: GitHub Copilot은 모든 LLM과 마찬가지로 특정 시점에 촬영된 데이터(코드 및 문서)로 훈련됩니다. 따라서 최신 기능, 사용이 중지된 기능 또는 소프트웨어 라이브러리 및 프레임워크의 보안 패치에 대해 알지 못할 수 있습니다.\n- 버전 특수성: 소프트웨어 개발은 라이브러리 및 프레임워크의 특정 버전에 크게 의존합니다. LLM은 실시간으로 업데이트되지 않기 때문에 오래된 코드 스니펫을 제공하여 호환성 문제나 사용 중지된 메서드 사용으로 이어질 수 있습니다.\n\nAPI와 같은 인터페이스는 자주 변경되며, 새로운 기능이 추가되거나 이전 기능이 제거될 수 있습니다. 오래된 데이터를 기반으로 하는 LLM은 새로운 보안 위험을 알지 못할 수 있어 코드가 위험에 노출될 수 있습니다.\n\n또한, 현대 소프트웨어 개발에서 의존성을 관리하는 것은 복잡하며, 최신 호환성 및 버전 정보가 부족한 경우 LLM이 최상의 조언을 제공하지 못할 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 모든 문제에는 해결책이 있어요\n\n통합 개발 환경(IDE) 및 다른 코딩 도구는 실시간 업데이트를 통합하고 최신 문서 또는 라이브러리 버전을 가져올 수 있어요. 이는 ChatGPT-4의 Bing 브라우징 기능과 유사하며 지식 베이스를 확장할 수 있어요.\n\n나는 GitHub Copilot의 채팅 기능이 브라우징 기능을 갖고 있다는 것을 알고 있어요. 그럼에도 불구하고, LLMs는 응답 생성을 위해 사전 훈련된 모델에 의존하며, 심지어 GitHub Copilot도 이에 대해 자료에서 경고하고 있어요:\n\n코딩 어시스턴트로 LLMs를 사용할 때, 소프트웨어 업데이트 및 문서를 동적으로 확인하는 기능이 제한될 수 있다는 점을 인식해야 해요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 최종 생각\n\n대규모 언어 모델은 코드의 기반을 생성하고 구문 및 기본 개념을 보조하는 데 유용할 수 있지만, 그 한계를 인식하고 이러한 제품에 지나치게 의존하지 않도록 주의해야 합니다.\n\n각 코딩 프로젝트는 특정 비즈니스 규칙이나 사용자 요구에 맞게 디자인된 매우 구체적인 솔루션이 필요할 수 있습니다. 이러한 뉘앙스는 LLM이 교육 데이터에서 발견된 일반적인 패턴을 복제하도록 훈련받았기 때문에 매우 어려운 것입니다.\n\n그러나 우리는 코드를 위해 맞춤화된 프롬프트 엔지니어링 전략을 채택하여 LLM을 우리가 원하는 출력물로 이끌 수 있습니다. 관심이 있다면, 아래 기사에서 더 읽을 수 있습니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## ⚠ 경고 ⚠\n\n올바르게 실행되는 코드를 작성하는 것은 하나의 일이지만, 성능을 최적화한 코드를 작성하는 것은 또 다른 문제입니다.\n\nLLM(Large Language Models)이 항상 가장 효율적인 알고리즘을 제공하거나 특정 하드웨어나 소프트웨어 환경에 대해 코드를 최적화하지는 않을 수 있음을 인지하는 것이 중요합니다. 게다가 AI가 생성한 코드를 실행할 때 항상 보안을 고려하는 것이 중요합니다.\n\n결국, 대형 언어 모델은 최대 우도 추정에 기초하고, 효율적인 소프트웨어 개발은 성능을 고려한 코드 생성 전략에 의존합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그게 다야! 읽어주셔서 정말 감사합니다!\n\nLLM을 코딩할 때 도움이 되었으면 좋겠어요!\n\n또한 새로운 콘텐츠를 기다리실 수 있도록 뉴스레터에 가입하실 수도 있습니다.\n\n특히, Large Language Models 및 ChatGPT에 관심이 있다면:\n","ogImage":{"url":"/assets/img/2024-05-23-WhyLLMsarenotGoodforCodingPartII_0.png"},"coverImage":"/assets/img/2024-05-23-WhyLLMsarenotGoodforCodingPartII_0.png","tag":["Tech"],"readingTime":5},{"title":"PDF 파싱 해부 02 파이프라인 기반 방법","description":"","date":"2024-05-23 17:36","slug":"2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod","content":"\n\nPDF 파일 및 스캔된 이미지와 같은 비구조화된 문서를 구조화 또는 반구조화된 형식으로 변환하는 것은 인공 지능의 핵심 요소입니다. 그러나 PDF의 복잡성 및 PDF 파싱 작업의 복잡성으로 인해이 프로세스는 신비로움을 지니게 됩니다.\n\n본 시리즈의 목적은 PDF 파싱을 분명하게 하는 데 있습니다. 이전 글에서는 PDF 파싱의 주요 작업을 소개하고 기존 방법을 분류하며 각 방법에 대한 간단한 소개를 제공했습니다.\n\n이 글에서는 파이프라인 기반 방법에 초점을 맞춥니다. 먼저 개요를 살펴보고, 그런 다음 몇 가지 대표적인 파이프라인 기반 PDF 파싱 프레임워크의 구현 전략을 소개하여 얻은 통찰을 공유하겠습니다.\n\n# 개요\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPipeline 기반 방법은 PDF를 구문 분석하는 작업을 모델 또는 알고리즘의 파이프라인으로 보는 방식으로, 그림 1에 나타난 것과 같습니다.\n\nPipeline 기반 방법은 다음 다섯 단계로 나뉠 수 있습니다:\n\n- 원본 PDF 파일을 전 처리하여 흐림 또는 왜곡된 방향과 같은 문제를 해결합니다. 이미지 개선, 이미지 방향 보정 등 해당 단계에는 이와 같은 작업이 포함됩니다.\n- 레이아웃 분석을 수행합니다. 이 과정에는 시각 구조 분석과 의미 구조 분석이 포함됩니다. 전자는 문서의 구조를 식별하고 유사한 영역의 윤곽을 나타내며, 후자는 이러한 영역을 텍스트, 제목, 목록, 표, 도표 등과 같은 특정 문서 유형으로 레이블링합니다. 이 단계에는 페이지의 읽기 순서를 분석하는 것도 포함됩니다.\n- 레이아웃 분석 중 식별된 각 영역을 분리하여 처리합니다. 이 과정에는 테이블 이해, 텍스트 인식, 수식, 흐름도, 특수 기호 인식 등 다른 구성 요소 식별이 포함됩니다.\n- 이전 결과를 통합하여 페이지 구조를 복원합니다.\n- Markdown, JSON 또는 HTML과 같은 구조화되거나 반구조화된 정보를 출력합니다.\n\n이어서, 이 글에서는 대표적인 Pipeline 기반 PDF 구문 분석 프레임워크 몇 가지를 살펴보고 그로부터 얻은 통찰을 공유할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 마커\n\n마커는 딥 러닝 모델을 위한 파이프라인입니다. PDF, EPUB 및 MOBI 문서를 Markdown 형식으로 변환할 수 있는 기능을 갖추고 있습니다.\n\n## 전체 프로세스\n\n그림 2에 설명된 대로, 마커의 전체 프로세스는 다음 네 단계로 나뉘어집니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n\nStep 1: PyMuPDF와 OCR을 사용하여 페이지를 블록으로 나누고 텍스트를 추출합니다. 해당 코드는 다음과 같습니다:\n\n```js\ndef convert_single_pdf(\n        fname: str,\n        model_lst: List,\n        max_pages=None,\n        metadata: Optional[Dict]=None,\n        parallel_factor: int = 1\n) -\u003e Tuple[str, Dict]:\n    ...\n    ...\n    doc = pymupdf.open(fname, filetype=filetype)\n    if filetype != \"pdf\":\n        conv = doc.convert_to_pdf()\n        doc = pymupdf.open(\"pdf\", conv)\n\n    blocks, toc, ocr_stats = get_text_blocks(\n        doc,\n        tess_lang,\n        spell_lang,\n        max_pages=max_pages,\n        parallel=int(parallel_factor * settings.OCR_PARALLEL_WORKERS)\n    )\n```\n\nStep 2: 레이아웃 세그먼터를 활용하여 블록을 분류하고, 열 탐지기를 사용하여 블록의 순서를 정합니다. 해당 코드는 다음과 같습니다:\n\n```js\ndef convert_single_pdf(\n        fname: str,\n        model_lst: List,\n        max_pages=None,\n        metadata: Optional[Dict]=None,\n        parallel_factor: int = 1\n) -\u003e Tuple[str, Dict]:\n    ...\n    ...\n    # 리스트에서 모델 분리\n    texify_model, layoutlm_model, order_model, edit_model = model_lst\n\n    block_types = detect_document_block_types(\n        doc,\n        blocks,\n        layoutlm_model,\n        batch_size=int(settings.LAYOUT_BATCH_SIZE * parallel_factor)\n    )\n\n    # 헤더와 푸터 찾기\n    bad_span_ids = filter_header_footer(blocks)\n    out_meta[\"block_stats\"] = {\"header_footer\": len(bad_span_ids)}\n\n    annotate_spans(blocks, block_types)\n\n    # 플래그가 설정되어 있으면 디버그 데이터 덤프\n    dump_bbox_debug_data(doc, blocks)\n\n    blocks = order_blocks(\n        doc,\n        blocks,\n        order_model,\n        batch_size=int(settings.ORDERER_BATCH_SIZE * parallel_factor)\n    )\n    ...\n    ...\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n3단계: 헤더 및 푸터 필터링, 코드 및 표 블록 수정, 수식을 위한 Texify 모델 적용하는 절차입니다. 해당 코드는 다음과 같습니다:\n\n```js\ndef convert_single_pdf(\n        fname: str,\n        model_lst: List,\n        max_pages=None,\n        metadata: Optional[Dict]=None,\n        parallel_factor: int = 1\n) -\u003e Tuple[str, Dict]:\n    ...\n    ...\n    # 코드 블록 수정\n    code_block_count = identify_code_blocks(blocks)\n    out_meta[\"block_stats\"][\"code\"] = code_block_count\n    indent_blocks(blocks)\n\n    # 표 블록 수정\n    merge_table_blocks(blocks)\n    table_count = create_new_tables(blocks)\n    out_meta[\"block_stats\"][\"table\"] = table_count\n\n    for page in blocks:\n        for block in page.blocks:\n            block.filter_spans(bad_span_ids)\n            block.filter_bad_span_types()\n\n    filtered, eq_stats = replace_equations(\n        doc,\n        blocks,\n        block_types,\n        texify_model,\n        batch_size=int(settings.TEXIFY_BATCH_SIZE * parallel_factor)\n    )\n    out_meta[\"block_stats\"][\"equations\"] = eq_stats\n    ...\n    ...\n```\n\n4단계: 에디터 모델을 사용하여 텍스트 후처리합니다. 해당 코드는 다음과 같습니다:\n\n```js\ndef convert_single_pdf(\n        fname: str,\n        model_lst: List,\n        max_pages=None,\n        metadata: Optional[Dict]=None,\n        parallel_factor: int = 1\n) -\u003e Tuple[str, Dict]:\n    ...\n    ...\n    # 원본 데이터 변경을 피하기 위해 복사\n    merged_lines = merge_spans(filtered)\n    text_blocks = merge_lines(merged_lines, filtered)\n    text_blocks = filter_common_titles(text_blocks)\n    full_text = get_full_text(text_blocks)\n\n    # 조인된 빈 블록 처리\n    full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n    full_text = re.sub(r'(\\n\\s){3,}', '\\n\\n', full_text)\n\n    # 점 표시 문자를 -로 교체\n    full_text = replace_bullets(full_text)\n\n    # 에디터 모델로 텍스트 후처리\n    full_text, edit_stats = edit_full_text(\n        full_text,\n        edit_model,\n        batch_size=settings.EDITOR_BATCH_SIZE * parallel_factor\n    )\n    out_meta[\"postprocess_stats\"] = {\"edit\": edit_stats}\n\n    return full_text, out_meta\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## Marker로부터 얻은 통찰\n\n지금까지 Marker의 전반적인 프로세스를 소개했습니다. 이제 Marker로부터 얻은 통찰에 대해 지금 이야기해 보겠습니다.\n\n통찰 1: 레이아웃 분석은 여러 하위 작업으로 나뉠 수 있습니다. 첫 번째 하위 작업은 PyMuPDF API를 호출하여 페이지 블록을 가져오는 것입니다.\n\n```js\ndef ocr_entire_page(page, lang: str, spellchecker: Optional[SpellChecker] = None) -\u003e List[Block]:\n    if settings.OCR_ENGINE == \"tesseract\":\n        return ocr_entire_page_tess(page, lang, spellchecker)\n    elif settings.OCR_ENGINE == \"ocrmypdf\":\n        return ocr_entire_page_ocrmp(page, lang, spellchecker)\n    else:\n        raise ValueError(f\"알 수 없는 OCR 엔진입니다: {settings.OCR_ENGINE}\")\n\n\ndef ocr_entire_page_tess(page, lang: str, spellchecker: Optional[SpellChecker] = None) -\u003e List[Block]:\n    try:\n        full_tp = page.get_textpage_ocr(flags=settings.TEXT_FLAGS, dpi=settings.OCR_DPI, full=True, language=lang)\n        blocks = page.get_text(\"dict\", sort=True, flags=settings.TEXT_FLAGS, textpage=full_tp)[\"blocks\"]\n        full_text = page.get_text(\"text\", sort=True, flags=settings.TEXT_FLAGS, textpage=full_tp)\n\n        if len(full_text) == 0:\n            return []\n\n        # OCR 작업 여부 확인. 작업되지 않았다면 빈 목록 반환\n        # 스캔된 빈 페이지에 연상되는 약한 텍스트 인쇄가 있는 경우 OCR가 실패할 수 있음\n        if detect_bad_ocr(full_text, spellchecker):\n            return []\n    except RuntimeError:\n        return []\n    return blocks\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n인사이트 2: 레이아웃LMv3와 같은 작은 다중 모달 사전 훈련 모델을 특정 작업에 맞게 세밀하게 조정하는 것이 매우 유익합니다. 예를 들어, Marker는 레이아웃LMv3을 세밀하게 조정하여 블록 유형을 감지하는 레이아웃 세그먼터 모델을 얻었습니다.\n\n```python\ndef load_layout_model():\n    model = LayoutLMv3ForTokenClassification.from_pretrained(\n        settings.LAYOUT_MODEL_NAME,\n        torch_dtype=settings.MODEL_DTYPE,\n    ).to(settings.TORCH_DEVICE_MODEL)\n\n    model.config.id2label = {\n        0: \"Caption\",\n        1: \"Footnote\",\n        2: \"Formula\",\n        3: \"List-item\",\n        4: \"Page-footer\",\n        5: \"Page-header\",\n        6: \"Picture\",\n        7: \"Section-header\",\n        8: \"Table\",\n        9: \"Text\",\n        10: \"Title\"\n    }\n\n    model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n    return model\n```\n\n이 세밀 조정에 사용된 데이터셋은 공개 데이터셋 DocLayNet에서 수집했습니다.\n\n인사이트 3: PDF 파일이 단일 열인지 또는 이중 열인지 확인하여 읽기 순서를 설정하는 것은 PDF 구문 분석에서 중요합니다. Marker의 방법은 레이아웃LMv3을 세밀하게 조정하여 열 감지기 모델을 만드는 것입니다. 이 모델은 페이지의 열 수를 결정하고, 그런 다음 중간 점 방법을 적용합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\ndef add_column_counts(doc, doc_blocks, model, batch_size):\n    for i in range(0, len(doc_blocks), batch_size):\n        batch = range(i, min(i + batch_size, len(doc_blocks)))\n        rgb_images = []\n        bboxes = []\n        words = []\n        for pnum in batch:\n            page = doc[pnum]\n            rgb_image, page_bboxes, page_words = get_inference_data(page, doc_blocks[pnum])\n            rgb_images.append(rgb_image)\n            bboxes.append(page_bboxes)\n            words.append(page_words)\n\n        predictions = batch_inference(rgb_images, bboxes, words, model)\n        for pnum, prediction in zip(batch, predictions):\n            doc_blocks[pnum].column_count = prediction\n\n\ndef order_blocks(doc, doc_blocks: List[Page], model, batch_size=settings.ORDERER_BATCH_SIZE):\n    add_column_counts(doc, doc_blocks, model, batch_size)\n\n    for page_blocks in doc_blocks:\n        if page_blocks.column_count \u003e 1:\n            # Resort blocks based on position\n            split_pos = page_blocks.x_start + page_blocks.width / 2\n            left_blocks = []\n            right_blocks = []\n            for block in page_blocks.blocks:\n                if block.x_start \u003c= split_pos:\n                    left_blocks.append(block)\n                else:\n                    right_blocks.append(block)\n            page_blocks.blocks = left_blocks + right_blocks\n    return doc_blocks\n```\n\n해당 내용은 Advanced RAG 02: Unveiling PDF Parsing에서 사용한 방식과 유사합니다.\n\n인사이트 4: 전문 모델은 수학 공식을 처리하는 데 사용할 수 있습니다. 예를 들어 Marker의 Texify 모델은 도넛 아키텍처를 사용합니다. 해당 모델은 웹에서 수집한 라텍스 이미지와 해당 방정식을 사용하여 도넛 모델로 훈련되었습니다. 이 모델은 im2latex 데이터셋을 활용했습니다. 훈련은 대략 2일간 4대의 A6000에서 진행되었으며, 약 6회 에폭에 해당합니다.\n\n인사이트 5: 모델은 후처리에도 사용할 수 있습니다. 주요 아이디어는 거의 마지막에 다가간 텍스트를 받아 예비 텍스트를 개선하여 불필요한 부분을 제거하고 공백을 추가하며 새로운 줄을 삽입하는 T5 모델을 훈련시키는 것입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef load_editing_model():\n    if not settings.ENABLE_EDITOR_MODEL:\n        return None\n\n    model = T5ForTokenClassification.from_pretrained(\n            settings.EDITOR_MODEL_NAME,\n            torch_dtype=settings.MODEL_DTYPE,\n        ).to(settings.TORCH_DEVICE_MODEL)\n    model.eval()\n\n    model.config.label2id = {\n        \"equal\": 0,\n        \"delete\": 1,\n        \"newline-1\": 2,\n        \"space-1\": 3,\n    }\n    model.config.id2label = {v: k for k, v in model.config.label2id.items()}\n    return model\n```\n\n현재 후처리기의 훈련과 데이터셋 구축에 대한 추가 세부 정보는 발견되지 않았습니다.\n\n## Marker의 단점\n\n당연히, Marker의 몇 가지 단점도 있습니다:\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 전문 레이아웃 분석 모델은 훈련되거나 세밀하게 조정되지 않았으며, 대신 PyMuPDF의 기본 기능을 사용했습니다. 이 접근 방식의 효과는 의문스럽습니다.\n- 테이블 인식 효과가 그리 좋지 않으며, 테이블 제목을 인식하지 못하는 문제가 있습니다. 이는 Nougat(OCR를 사용하지 않은 소형 모델 기반 솔루션으로 다음 글에서 자세히 소개될 것입니다)만큼 효과적이지 않습니다. 예를 들어, Figure 3는 \"Attention Is All You Need\"의 Table 3의 테이블 인식 결과를 보여줍니다. 왼쪽에는 원본 테이블이, 가운데에는 Marker를 사용한 결과가, 오른쪽에는 Nougat의 결과가 표시됩니다.\n\n3. 영어와 유사한 언어만 지원합니다. 일본어나 힌디어 같은 언어는 작동하지 않습니다.\n\n# PaperMage\n\nPapermage는 시각적으로 풍부하고 구조화된 과학 문서의 분석 및 처리를 위한 오픈 소스 프레임워크입니다. 문서 내의 텍스트 및 시각적 요소를 명확하고 직관적으로 표현하고 조작하기 위한 완벽한 추상화를 제공합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPapermage는 각기 다른 자연어 처리 (NLP) 및 컴퓨터 비전 (CV) 모델을 단일 프레임워크로 통합합니다. Papermage는 일반적인 과학 문서 처리 시나리오에 대한 사용 준비 완료 솔루션을 제공합니다.\n\n다음으로, PaperMage의 원리를 설명하고 소스 코드와 함께 전체 프로세스를 논의할 것입니다. 그런 다음, PaperMage에서 얻은 인사이트에 대해 논의할 것입니다.\n\n## 구성 요소\n\nPapermage는 주로 세 가지 부분으로 구성됩니다:\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- Magelib: 시각적으로 풍부한 문서를 다중 모달 구조로 나타내고 조작하기 위한 기본 요소와 방법을 포함하는 라이브러리입니다.\n- Predictors: 다양한 첨단 과학 문서 분석 모델들을 통합하여 통일된 인터페이스로 구현한 것입니다. 이는 개별 모델이 서로 다른 프레임워크에 작성되었거나 다른 모드에서 작동하는 경우에도 가능합니다.\n- Recipes: 종종 단일 모드인 개별 모듈들의 테스트된 조합에 쉽게 접근할 수 있게 하며, 이를 통해 복잡하고 확장 가능한 다중 모달 파이프라인을 형성할 수 있습니다.\n\n## 기본 데이터 클래스\n\nMagelib은 시각적으로 풍부하고 구조화된 문서의 기본 요소를 나타내기 위한 세 가지 기본 데이터 클래스를 제공합니다: 문서(Document), 레이어(Layers) 및 엔티티(Entities).\n\n문서 및 레이어\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFigure 4는 PaperMage가 문서를 생성하고 표현하는 방법을 보여줍니다.\n\n![Figure 4](/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_0.png)\n\n문서 구조가 다양한 알고리즘이나 모델에 의해 추출되면, PaperMage는 텍스트와 시각적 정보를 모두 저장하는 주석 레이어로 개념화합니다.\n\n나중에 우리는 레시피의 `run()` 함수의 구체적인 실행 과정을 소스 코드와 함께 분석할 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n개체\n\n그림 5에서 보듯이, 엔티티는 다중 모달 콘텐츠 단위를 나타냅니다.\n\n![이미지](/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_1.png)\n\n열/페이지에 걸친 문장이나 부유 그래픽/각주로 인해 중단된 문장과 같은 불연속 형태의 단위를 어떻게 관리합니까?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPaperMage는 두 개의 멤버 변수인 spans와 boxes를 사용합니다. 그림 5에서 볼 수 있듯이 spans는 모든 기호 중 문장의 텍스트를 식별하고, boxes는 페이지에서의 시각적 좌표를 매핑합니다. 이 접근 방식은 섬세한 레이아웃 차이를 수용하는 유연성을 제공합니다.\n\n게다가, 우리는 다른 방식으로 엔티티에 쉽게 접근할 수 있습니다. 이는 그림 6에서 보는 것처럼입니다.\n\n아래 이미지는 운세 시스템의 구조를 설명하고 있는 URL을 통해 확인할 수 있습니다.\n\n보다 깊이 PaperMage를 이해하기 위해, PDF 파싱의 구체적인 예시에서 시작하여 그로부터 자세히 설명하겠습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## 전체 프로세스 및 코드 분석\n\n시험용 코드는 다음과 같습니다.\n\n```js\nfrom papermage.recipes import CoreRecipe\n\ncore_recipe = CoreRecipe()\n\ndoc = core_recipe.run(\"YOUR_PDF_PATH\")\n```\n\n먼저 core_recipe = CoreRecipe()은 CoreRecipe 클래스의 생성자에 들어가게 되는데, 관련 라이브러리 및 모델의 초기화가 이루어집니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass CoreRecipe(Recipe):\n    def __init__(\n        self,\n        ivila_predictor_path: str = \"allenai/ivila-row-layoutlm-finetuned-s2vl-v2\",\n        bio_roberta_predictor_path: str = \"allenai/vila-roberta-large-s2vl-internal\",\n        svm_word_predictor_path: str = \"https://ai2-s2-research-public.s3.us-west-2.amazonaws.com/mmda/models/svm_word_predictor.tar.gz\",\n        dpi: int = 72,\n    ):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.dpi = dpi\n\n        self.logger.info(\"Recipe를 생성하는 중...\")\n        self.parser = PDFPlumberParser()\n        self.rasterizer = PDF2ImageRasterizer()\n\n        # with warnings.catch_warnings():\n        #     warnings.simplefilter(\"ignore\")\n        #     self.word_predictor = SVMWordPredictor.from_path(svm_word_predictor_path)\n\n        self.publaynet_block_predictor = LPEffDetPubLayNetBlockPredictor.from_pretrained()\n        self.ivila_predictor = IVILATokenClassificationPredictor.from_pretrained(ivila_predictor_path)\n        self.sent_predictor = PysbdSentencePredictor()\n        self.logger.info(\"Recipe 생성 완료\")\n```\n\nRecipe 클래스는 CoreRecipe 클래스의 부모 클래스이므로, core_recipe.run() 함수는 Recipe::run()으로 이동할 것입니다.\n\n```python\nclass Recipe:\n    @abstractmethod\n    def run(self, input: Any) -\u003e Document:\n        if isinstance(input, Path):\n            if input.suffix == \".pdf\":\n                return self.from_pdf(pdf=input)\n            if input.suffix == \".json\":\n                return self.from_json(doc=input)\n\n            raise NotImplementedError(\"지원되지 않는 파일 유형입니다.\")\n\n        if isinstance(input, Document):\n            return self.from_doc(doc=input)\n\n        if isinstance(input, str):\n            if os.path.exists(input):\n                input = Path(input)\n                return self.run(input=input)\n            else:\n                return self.from_str(text=input)\n\n        raise NotImplementedError(\"지원되지 않는 문서 입력 형식입니다.\")\n```\n\n그런 다음 CoreRecipe 클래스의 from_pdf()와 from_doc() 함수로 이어질 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\nclass CoreRecipe(Recipe):\n    ...\n    ...\n    def from_pdf(self, pdf: Path) -\u003e Document:\n        self.logger.info(\"문서 파싱 중...\")\n        doc = self.parser.parse(input_pdf_path=pdf)\n\n        self.logger.info(\"문서 래스터화 중...\")\n        images = self.rasterizer.rasterize(input_pdf_path=pdf, dpi=self.dpi)\n        doc.annotate_images(images=list(images))\n        self.rasterizer.attach_images(images=images, doc=doc)\n        return self.from_doc(doc=doc)\n\n    def from_doc(self, doc: Document) -\u003e Document:\n        # self.logger.info(\"단어 예측 중...\")\n        # words = self.word_predictor.predict(doc=doc)\n        # doc.annotate_layer(name=WordsFieldName, entities=words)\n\n        self.logger.info(\"문장 예측 중...\")\n        sentences = self.sent_predictor.predict(doc=doc)\n        doc.annotate_layer(name=SentencesFieldName, entities=sentences)\n\n        self.logger.info(\"블록 예측 중...\")\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            blocks = self.publaynet_block_predictor.predict(doc=doc)\n        doc.annotate_layer(name=BlocksFieldName, entities=blocks)\n\n        self.logger.info(\"도형 및 표 예측 중...\")\n        figures = []\n        tables = []\n        for block in blocks:\n            if block.metadata.type == \"Figure\":\n                figure = Entity(boxes=block.boxes)\n                figures.append(figure)\n            elif block.metadata.type == \"Table\":\n                table = Entity(boxes=block.boxes)\n                tables.append(table)\n        doc.annotate_layer(name=FiguresFieldName, entities=figures)\n        doc.annotate_layer(name=TablesFieldName, entities=tables)\n\n        # self.logger.info(\"vila 예측 중...\")\n        vila_entities = self.ivila_predictor.predict(doc=doc)\n        doc.annotate_layer(name=\"vila_entities\", entities=vila_entities)\n\n        for entity in vila_entities:\n            entity.boxes = [\n                Box.create_enclosing_box(\n                    [b for t in doc.intersect_by_span(entity, name=TokensFieldName) for b in t.boxes]\n                )\n            ]\n            # entity.text = make_text(entity=entity, document=doc)\n        preds = group_by(entities=vila_entities, metadata_field=\"label\", metadata_values_map=VILA_LABELS_MAP)\n        doc.annotate(*preds)\n        return doc\n```\n\nFigure 7에서 전체적인 프로세스를 보여줍니다:\n\nFigure 7는 PaperMage의 처리 흐름이 파이프라인 방식을 따른다는 것을 보여줍니다.\n\n처음에는 PDFPlumber 라이브러리를 사용하여 레이아웃 분석을 수행합니다. 그 후 레이아웃 분석 결과를 바탕으로 페이지의 다른 엔티티를 파싱하는 데 전문 알고리즘 또는 모델을 사용합니다. 이에는 문장, 도형, 표, 제목 등이 포함됩니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 우리는 세 가지 알고리즘 또는 모델을 논의할 것입니다:\n\n- 문장 분리\n- 레이아웃 구조 분석\n- 논리적 구조 분석.\n\n## 문장 분리\n\n문장 분리에 사용되는 알고리즘은 PySBD로, rule-based 문장 경계 해석 Python 패키지입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n표를 마크다운 형식으로 변경해주세요.\n\n```js\n[\nUnannotated Entity: {'spans': [[0, 212]]}, \nUnannotated Entity: {'spans': [[212, 367]]},  \n…\n]\n```\n\n## 레이아웃 구조 분석\n\n페이지의 레이아웃 구조를 분석하는 데 사용된 모델은 LPEffDetPubLayNetBlockPredictor입니다. 이는 LayoutParser에서 제공하는 깊은 학습 기반의 효율적인 객체 감지 모델입니다. 주요 기능은 문서를 시각적 블록 영역으로 분할하는 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n이 페이지의 이미지는 doc.images로 참조됩니다. 결과는 각 블록에 대한 상자 클래스 개체와 해당 유형입니다. 상자에는 왼쪽 상단 꼭지점의 x 좌표, 왼쪽 상단 꼭지점의 y 좌표, 페이지 너비, 페이지 높이 및 페이지 번호가 포함됩니다.\n\n```js\n[\nUnannotated Entity: {'boxes': [[0.5179840190298606, 0.752760137345049, 0.3682081491355128, 0.15176369855069774, 0]], 'metadata': {'type': 'Text'}, \nUnannotated Entity: {'boxes': [[0.5145780320135539, 0.5080924136055337, 0.3675624668198144, 0.23725746136663078, 0]], 'metadata': {'type': 'Text'}, \n…\n]\n```\n\n## 논리 구조 분석\n\n문서의 논리적 구조를 분석하는 데 사용된 모델은 IVILATokenClassificationPredictor입니다. 이는 제목, 초록, 본문, 각주, 캡션 등과 같은 조직 단위로 문서를 분리합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n제공된 입력은 딕셔너리 형식의 페이지 수준 데이터입니다.\n\n```js\n{\n    'words': ['word1', 'word2', ...],\n    'bbox': [[x1, y1, x2, y2], [x1, y1, x2, y2], ...],\n    'block_ids': [0, 0, 0, 1 ...],\n    'line_ids': [0, 1, 1, 2 ...],\n    'labels': [0, 0, 0, 1 ...], # 비어 있을 수도 있습니다\n}\n```\n\n결과는 각 엔티티의 범위입니다.\n\n```js\n[\nUnannotated Entity: {'spans': [[0, 80]], 'metadata': {'label': 'Title'}, \nUnannotated Entity: {'spans': [[81, 157]], 'metadata': {'label': 'Author'}, \nUnannotated Entity: {'spans': [[158, 215]], 'metadata': {'label': 'Paragraph'}, \n...\n]\n```\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n## PaperMage에 대한 통찰과 토론\n\nPDF 구문 분석의 추상화\n\nPDF 구문 분석 작업에 있어서 PaperMage에서 제안한 추상화는 효과적입니다. 전체 PDF를 문서, 레이어 및 엔티티와 같은 유형으로 나누는 것은 분류와 관리를 용이하게 합니다.\n\n확장성\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nPaperMage는 쉽게 확장할 수 있는 프레임워크를 설계했습니다. 이는 개발자들이 후속 개발을 수행하기에 편리하게 만들어 줍니다.\n\n예를 들어, 사용자 정의 예측기를 추가하려면 BasePredictor 기본 클래스로부터 상속받고 _predict() 함수를 재정의해주기만 하면 됩니다.\n\n```js\nfrom .base_predictor import BasePredictor\n\nclass YOUR_NEW_Predictor(BasePredictor):\n    ...\n    ...\n    def _predict(self, doc: Document) -\u003e List[YOUR_RET_TYPE]:\n    ...\n    ...\n```\n\n병렬화에 대해\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 태그를 마크다운 형식으로 바꿔보세요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n다음으로, 비구조화된 프레임워크로부터 얻은 통찰과 경험에 대해 주로 논의하겠습니다. 특히, 이 프레임워크가 자체 PDF 구문 분석 도구를 개발하는 데 어떻게 도움이 되는지에 대해 설명하겠습니다.\n\n## 레이아웃 분석에 대해\n\n비구조화된 프레임워크의 레이아웃 분석은 자세하게 수행됩니다.\n\n`strategy=hi_res`를 설정하면 YOLOX 또는 detectron2와 같은 모델을 레이아웃 분석에 활용합니다. 이는 PDFMiner와 결합되어 추가적인 감지가 이루어집니다. 두 결과물을 병합하여 최종 레이아웃을 생성하게 됩니다. 이것은 그림 8에 나타나 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\nFigure 9과 Figure 10은 BERT 논문의 16페이지 레이아웃 분석 결과의 시각화를 보여줍니다. 사진 속 상자들은 각 영역의 범위를 나타냅니다. Figure 9에 나타난 물체 탐지 모델의 결과는 더 정확하며, 더 많은 통합된 표와 이미지를 보여줍니다. 반면에 Figure 10에 표시된 PDFMiner의 탐지 결과는 표와 이미지 내용을 분리합니다.\n\n\u003cimg src=\"/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_3.png\" /\u003e\n\n\u003cimg src=\"/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_4.png\" /\u003e\n\n레이아웃을 병합하는 구체적인 코드는 다음과 같습니다. PDFMiner 탐지 결과(extracted_layout)와 물체 탐지 모델의 결과(inferred_layout) 간의 각 영역 사이의 관계를 평가하는 이중 루프로 이루어져 있습니다. 그 후 병합 여부를 결정합니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```python\ndef merge_inferred_layout_with_extracted_layout(\n    inferred_layout: Collection[LayoutElement],\n    extracted_layout: Collection[TextRegion],\n    page_image_size: tuple,\n    same_region_threshold: float = inference_config.LAYOUT_SAME_REGION_THRESHOLD,\n    subregion_threshold: float = inference_config.LAYOUT_SUBREGION_THRESHOLD,\n) -\u003e List[LayoutElement]:\n    \"\"\"Merge two layouts to produce a single layout.\"\"\"\n    extracted_elements_to_add: List[TextRegion] = []\n    inferred_regions_to_remove = []\n    w, h = page_image_size\n    full_page_region = Rectangle(0, 0, w, h)\n    for extracted_region in extracted_layout:\n        extracted_is_image = isinstance(extracted_region, ImageTextRegion)\n        if extracted_is_image:\n            # Skip extracted images for this purpose, we don't have the text from them and they\n            # don't provide good text bounding boxes.\n\n            is_full_page_image = region_bounding_boxes_are_almost_the_same(\n                extracted_region.bbox,\n                full_page_region,\n                FULL_PAGE_REGION_THRESHOLD,\n            )\n\n            if is_full_page_image:\n                continue\n        region_matched = False\n        for inferred_region in inferred_layout:\n            if inferred_region.source in CHIPPER_VERSIONS:\n                continue\n            ...\n            ...\n```\n\n## 사용자 정의에 관해\n\n비정형 프레임워크에는 쉬운 사용자 정의를 가능케 하는 다양한 중간 결과가 있습니다.\n\n이전 글에서는 비정형 데이터의 세 가지 도전 과제를 다루었습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n- 테이블 파싱\n- 감지된 블록 재배열, 특히 이중 열 PDF의 경우\n- 다중 수준 제목 추출\n\n마지막 두 가지 도전 과제는 중간 구조를 수정하여 해결할 수 있습니다. 예를 들어, Figure 11은 BERT 논문의 두 번째 페이지의 최종 레이아웃을 보여줍니다.\n\n![이미지](/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_5.png)\n\n동시에 레이아웃 분석 결과를 쉽게 얻을 수 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n```js\n[\n\nLayoutElement(bbox=Rectangle(x1=851.1539916992188, y1=181.15073777777613, x2=1467.844970703125, y2=587.8204599999975), text='CR이 MST보다 세분화된 영역에 대해 일반화되어 왔다.(Kiros et al., 2015; Logeswaran and Lee, 2018) 나 문단 임베딩(Le and Mikolov, 2014)과 같은 세분 화된 영역', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Text', prob=0.9519357085227966, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=196.5296173095703, y1=181.1507377777777, x2=815.468994140625, y2=512.548237777777), text='word based only on its context. Unlike left-to-right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- In addi- train a deep bidirectional Transformer. tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair representations. The contributions of our paper are as follows: ', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Text', prob=0.9517233967781067, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=200.22352600097656, y1=539.1451822222216, x2=825.0242919921875, y2=870.542682222221), text='• 우리는 양방향 사전 학습이 언어 표현에 중요함을 입증합니다. 라드포드 외(2018)는 사전 훈련을 위해 단방향 언어 모델을 사용하였지만, BERT는 가려진 언어 모델을 사용하여 사전 훈련된 깊은 양방향 표현을 가능하게 합니다. 이는 또한 Peter et al. (2018a)와 대조적으로, 왼쪽에서 오른쪽으로 훈련된 독립적인 왼쪽에서 오른쪽 및 오른쪽에서 왼쪽 LM의 얕은 결합을 사용합니다.', source=\u003cSource.YOLOX: 'yolox'\u003e, type='List-item', prob=0.9414362907409668, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=851.8727416992188, y1=599.8257377777753, x2=1468.0499267578125, y2=1420.4982377777742), text='ELMo 및 그 전신(Peter et al., 2017, 2018a)은 전통적인 단어 임베딩 연구를 다른 차원으로 일반화합니다. 그들은 왼쪽에서 오른쪽으로와 오른쪽에서 왼쪽으로 언어 모델에서 문맥-주의적인 기능을 추출합니다. 각 토큰의 맥락적 표현은 왼쪽에서 오른쪽 및 오른쪽에서 왼쪽 표현의 연결입니다. 문맥적 단어 임베딩을 기존의 작업별 아키텍처와 통합할 때, ELMo는 질문 응답(Rajpurkar et al., 2016), 감성 분석(Socher et al., 2013) 및 명명된 개체 인식(Tjong Kim Sang 및 De Meulder, 2003)을 포함한 여러 주요 NLP 벤치마크에서 기술의 최신 동향을 선보입니다. Melamud et., 2016은 LSTM을 사용하여 왼쪽 및 오른쪽 컨텍스트에서 단일 단어를 예측하는 작업을 통해 컨텍스트 표현을 학습하는 것을 제안했습니다. ELMo와 유사하게, 그들의 모델은 기능 기반이며 깊게 양방향적이지 않습니다. Fedus et al.(2018)은 클로즈 태스크가 텍스트 생성 모델의 견고성을 향상시키는 데 사용될 수 있다는 것을 보여줍니다.', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Text', prob=0.938507616519928, image_path=None, parent=None),\n\n\nLayoutElement(bbox=Rectangle(x1=199.3734130859375, y1=900.5257377777765, x2=824.69873046875, y2=1156.648237777776), text='• 사전 훈련된 표현이 많은 과도하게 엔지니어링된 과제-별 아키텍처의 필요성을 줄인다는 것을 보여줍니다. BERT는 다양한 문장 수준 및 토큰 수준 작업에 대해 최첨단 성능을 달성하는 첫 번째 세세 조정 기반 표현 모델이며, 많은 과제별 아키텍처를 능가합니다.', source=\u003cSource.YOLOX: 'yolox'\u003e, type='List-item', prob=0.9461237788200378, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=195.5695343017578, y1=1185.526123046875, x2=815.9393920898438, y2=1330.3272705078125), text='• BERT는 열한가지 NLP 작업에 대한 최신 기술을 선도합니다. 코드와 사전 훈련 모델은 다음에서 제공됩니다. https://github.com/ google-research/bert.', source=\u003cSource.YOLOX: 'yolox'\u003e, type='List-item', prob=0.9213815927505493, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=195.33956909179688, y1=1360.7886962890625, x2=447.47264000000007, y2=1397.038330078125), text='2 Related Work ', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Section-header', prob=0.8663332462310791, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=197.7477264404297, y1=1419.3353271484375, x2=817.3308715820312, y2=1527.54443359375), text='There is a long history of pre-training general lan- guage representations, and we brieﬂy review the most widely-used approaches in this section. ', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Text', prob=0.928022563457489, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=851.0028686523438, y1=1468.341394166663, x2=1420.4693603515625, y2=1498.6444497222187), text='2.2 Unsupervised Fine-tuning Approaches ', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Section-header', prob=0.8346447348594666, image_path=None, parent=None),\n\nLayoutElement(bbox=Rectangle(x1=853.5444444444446, y1=1526.3701822222185, x2=1470.989990234375, y2=1669.5843488888852), text='컨텍스트 기반 기법과 마찬가지로, 이 방향으로 첫 번째 작업은 라벨이 지정되지 않은 텍스트로부터 단어 em-(Col- bed 파라미터를 사전 훈련하는 것에 있습니다.(lobert and Weston, 2008).', source=\u003cSource.YOLOX: 'yolox'\u003e, type='Text',\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n테이블 탐지 및 인식에는 비구조화 된 프레임 워크에서 Table Transformer가 사용됩니다.\n\nTable Transformer 모델은 미구조 문서에서의 포괄적인 테이블 추출을 위해 PubTables-1M에서 제안되었습니다. 본 논문은 새로운 데이터 세트 인 PubTables-1M을 소개하며, 미구조 문서에서의 테이블 추출 및 테이블 구조 인식 및 기능 분석 작업을 수행하기 위해 설계되었습니다. Figure 12에서 확인할 수 있습니다.\n\nTable Transformer은 PubTables-1M 데이터셋에서 DETR 모델을 기반으로 훈련되었으며, 테이블 탐지 및 테이블 구조 인식과 같은 작업을 위해 사용됩니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n아래 표 처리 방법은 이전 글을 참고해 주세요.\n\n## 수식 검출 및 인식에 대해\n\n구조화되지 않은 프레임워크에는 수식 검출 및 인식에 전담된 모듈이 부족하여 Figure 13에 표시된 것처럼 보통 성능을 보입니다.\n\n![수식 검출 및 인식](/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_7.png)\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n# 결론\n\n본 글은 PDF 파싱에 대한 파이프라인 기반 방법에 대한 개요를 제공했습니다. 세 가지 대표적인 프레임워크를 예시로 사용하여 이 접근 방식을 탐구하며 깊이 있는 소개와 이를 통해 얻은 통찰을 공유했습니다.\n\n요약하자면,\n\n- Marker는 몇 가지 단점이 있지만 가벼우면서 빠른 도구입니다.\n- PaperMage는 주로 과학 문서용으로 설계되었지만 미래 개발을 지원하는 탁월한 확장성을 갖고 있습니다.\n- Unstructured는 포괄적인 파이프라인 기반 PDF 파싱 프레임워크입니다. 그 이점은 자세한 레이아웃 분석과 강력한 사용자 정의 기능에 있습니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일반적으로, 파이프라인 기반 PDF 구문 분석 방법은 해석 가능하며 사용하기 쉬워 많이 사용되는 PDF 구문 분석 방법입니다. 그러나 효과적인지는 각 모델 또는 알고리즘의 성능에 크게 의존합니다. 따라서 훈련 데이터와 각 모델의 구조는 신중하게 설계되어야 합니다.\n\nPDF 구문 분석이나 문서 인텔리전스에 관심이 있다면 다른 글도 읽어보세요.\n\n또한, 최신 AI 관련 콘텐츠는 뉴스레터에서 확인할 수 있습니다.\n\n마지막으로, 이 글에 오류나 누락 사항이 있다면 댓글 섹션에서 지적해주시거나 공유할 생각이 있다면 언제든지 알려주세요.","ogImage":{"url":"/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_0.png"},"coverImage":"/assets/img/2024-05-23-DemystifyingPDFParsing02Pipeline-BasedMethod_0.png","tag":["Tech"],"readingTime":28},{"title":"위대한 AI 사기","description":"","date":"2024-05-23 17:33","slug":"2024-05-23-TheGreatAIQuackery","content":"\n\n![Image](/assets/img/2024-05-23-TheGreatAIQuackery_0.png)\n\nAI는 단순히 인간을 대체하는 것뿐만 아니라, 우리가 글쓰기에 대해 어떻게 생각하고 단어 자체를 어떻게 경험하는지도 바꾸고 있습니다.\n\n우리는 스스로를 작가라고 부르지만, 나 자신 안 깊숙한 곳에선 항상 쓰기에는 숨겨진 힘이 있음을 느꼈습니다.\n\n글을 쓸 때, 나는 더 자아와 조화를 이루는 느낌을 받는 때입니다.\n\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n글을 쓰는 것은 마법 같아요. 일상을 떠나 내 영혼의 깊이를 엿볼 수 있다는 느낌이 들어요.\n\n내 안에는 탐험해야 할 세상이 있어요. 그 세상은 글을 쓸 때만 발견할 수 있어요.\n\n나는 나 자신의 우주 탐험가예요:\n\n글쓰기는 어려워요. 마음까지 닿는 단축키는 없답니다. 우리의 혈관 속 잉크로 글을 쓸 때에만 독자의 마음을 얻을 수 있다는 거죠.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n나는 나의 글쓰기에서 많은 작가들을 만나기 위해 씁니다. 나는 불가능한 것을 가능하게 만들기 위해 씁니다.\n\n따라서 내가 진정한 모습을 찾을 때만 명료함을 얻을 수 있습니다. 머리 속에서 시끄러운 목소리들에게 말할 기회를 줄 때에만 말이죠.\n\n그때에만 나는 진정한 자아에 접근하고 내 모든 글쓰기 수프리게티를 만날 수 있습니다.\n\nAI는 당신에게 쓰는 경험을 빼앗습니다. 쓰는 마법을 없애버리고 당신에게 주문을 걸어버립니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n당신이 \"비상한\" \"다물어보다\"라는 \"여정\"에 얽힌 것에 난처해하고 있네요.\n\nAI 토끼굴을 들어가면, 미친 모자와 흰 토끼와 같은 테이블에 앉게 될 거예요. 당신은 최악의 인간 본성 쪽에 서게 될 거에요 - 저희 포스트모던 기업들의 속도와 주목 경제의 노동자로 낙인을 찍게 될 거에요.\n\n나는 AI 열차에 탑승하지 않겠어요. AI는 당신의 근본적인 질문에 대답하지 않을 거에요.\n\n존재론적인 말을 찾으셨나요?\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n모든 달려다닐 때의 소란 끝에 남아있는 말은 무엇인가요?\n\n다른 모든 것이 조용해질 때에만 듣는 말은 무엇인가요?\n\n당신보다 오래 남을 말이 무엇인가요?\n\n그 질문에 대해 ChatGPT에게 답변하도록 유도해보세요. 그리고 AI가 그에 대해 어떤 답변을 할 지 살펴봅시다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n그 쿼리에 대한 간단한 대답은 없어요.\n\n대답을 찾고 그를 우주에 반환하는 방법을 찾아야 해요.\n\n인공지능의 도움으로 누구나 인간 작가가 눈 깜짝하는 것보다 더 빠르게 복사를 작성할 수 있는 강력한 도구에 액세스할 수 있어요.\n\nAI가 마케팅되는 방식은 뱀유약 판매상 이야기를 연상시키네요.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n일부 사람들에게는 AI는 궁극의 만병통치약이지만 다른 사람들은 종말의 나팔로 보기도 합니다. 그 어떤 관점이건 계속되는 AI 마케팅 캠페인에 기여합니다.\n\n믿든지 말든지, AI는 고대의 돌팔이와 유사한 현대판입니다. AI 돌팔이들은 자신들이 철학자의 돌을 가졌다고 설득하려 할 것입니다.\n\n그들은 자신들의 알고리즘들이 모든 것을 안다고 주장합니다.\n\n이 사기꾼들은 빠른 금전적 이득을 약속하며 자신들의 LLMs를 팔 것입니다.\n\n\u003cdiv class=\"content-ad\"\u003e\u003c/div\u003e\n\n과거에는 사람들이 곤경에 처할 때 속임수에 굴복했습니다. 저는 팬데믹이 AI의 신속한 발전을 촉진시켰다는 것이 신기하다고 생각합니다. 그러나 이제 폭풍의 반대편에 서 있는데, AI 발전이 멈춰 있는 것 같습니다.\n\nAI는 마법 같은 말을 찾아줄 수 없을 거에요. AI는 그저 도구에 불과해요. 지금 당장, 더 좋은 연필깎이가 있었으면 좋겠어요.\n","ogImage":{"url":"/assets/img/2024-05-23-TheGreatAIQuackery_0.png"},"coverImage":"/assets/img/2024-05-23-TheGreatAIQuackery_0.png","tag":["Tech"],"readingTime":2}],"page":"3","totalPageCount":61,"totalPageGroupCount":4,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true},"page":"/posts/[page]","query":{"page":"3"},"buildId":"R1x9p1CQYDDJESXyLXKOK","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>