---
title: "오픈에이아이가 지나쳤을까요"
description: ""
coverImage: "/assets/img/2024-05-23-HasOpenAIGoneTooFar_0.png"
date: 2024-05-23 17:31
ogImage: 
  url: /assets/img/2024-05-23-HasOpenAIGoneTooFar_0.png
tag: Tech
originalTitle: "Has OpenAI Gone Too Far?"
link: "https://medium.com/@ignacio.de.gregorio.noblejas/has-openai-gone-too-far-2f32f0bdbd82"
---


OpenAI 안에서는 GPT-4o 출시 한 주 후에 상황이 이렇게 엉망이 될 것이라고 예상한 사람은 거의 없었을 것입니다.

그럼에도 불구하고, 지난 11월 Sam Altman의 해고 드라마를 제쳐놓고, 이것은 아마도 스타트업의 역사상 가장 나쁜 PR 주일이었을 것입니다.

주요 언론이 최신 모델을 조롱하는 것만으로도 충분히 나쁘지만, 유명 배우가 법적 조치를 위협하는 것은 특히 좋지 않은 소식입니다. 무엇보다도, 중요한 안전 인물들이 떠나는 것은 심각한 문제입니다.

이 모든 것이 결합되어, OpenAI가 제품의 안전에 신경을 쓰고 있는지, 그리고 규칙을 준수하려는 의지가 여전히 있는지에 대해 심각한 의문을 제기합니다.

<div class="content-ad"></div>

# 인셀 자아와 스타들의 화를 격려하는 연료

요즘 오픈AI가 추진한 기술 발전이 얼마나 중요한지에 대해 몇 가지 기사를 써왔지만, 여전히 사실입니다. 그러나 ChatGPT-4o의 최근 출시에 대한 실제 영향에 대해 언급하지 않았습니다.

나는 회사 자체와 그 결정에 대한 논쟁에 자주 주목하지는 않지만, 오픈AI의 주목할 만한 존재감이 점차적으로 늘어나는 것은 무시할 수 없습니다.

음, GPT-4o는 명백히 멀티모달리티나 지연 같은 핵심 측면에서 한 걸음 나아간 것임은 틀림없지만, 반응은 만족스럽지 않았습니다.

<div class="content-ad"></div>

아직, 주요 사건들을 안내하겠지만, 이 모든 것이 의미하는 바에 초점을 맞출 거예요. 우리가 가장 선도적인 소프트웨어를 보유한 회사에 대해 이야기하고 있으니까요.

## 지나치게 ‘Her’가 되려고 노력하다

의심의 여지 없이, 모델을 둘러싼 핫한 논쟁의 주요 포인트(나중에 떠나는 것들에 대해 논의할 테니까요)은 데모에서 사용한 매우 아슬아슬한 목소리, 코드명 ‘Sky’가 ‘인간 같아 보이기에 지나치게 열정적’인데 아주 아주 스칼릿 요한슨과 매우 닮았다는 것이었어요.

이것이 우연일 수도 있지만, 그렇지 않은 것 같아 보이네요. ‘Her’ 영화에서 스칼릿이 음성을 맡았던 것을 알고 계셨죠. 사만 알트만의 최애 영화였던 ‘Her’에서 스칼릿이 음성을 맡고 있었어요. 게다가 알트만 본인이 발표 이후 다음과 같이 트윗을 게시해 불을 지푸는 듯이 더 하였네요:

<div class="content-ad"></div>

놀랍게도, 스칼렛은 이 모든 것을 곧 공개적으로 비난하여 OpenAI로 하여금 그 특정 음성을 당분간 중단하도록 만들었습니다.

![이미지](/assets/img/2024-05-23-HasOpenAIGoneTooFar_0.png)

만약 OpenAI가 정말로 스칼렛에게 다가갔다면, 그들이 노력을 많이 한 것은 분명한데요 — 

‘그녀’를 현실로 만들려고 한 것이죠.

<div class="content-ad"></div>

그러나 여기서 주요한 포인트는 스칼렛에게(그녀는 이를 극복할 것입니다) 가 아니라 이 전례가 사회에 실제로 무엇을 의미하는지입니다.

## 데이터 부당 사용

이 시점에서 모두가 모든 선두적 AI 연구소가(OpenAI뿐만 아니라) 훈련용으로 데이터를 부적절하게 사용했다고 가정합니다.

우리가 여러 차례 다룬 대로, ChatGPT와 같은 대형 언어 모델(Large Language Models, LLMs)의 사전 훈련 단계에서는 우리의 세계에 대해 배우는 데 조제 양의 데이터가 수조 단위로 필요합니다.

<div class="content-ad"></div>

따라서, 더 똑똑한 모델을 만드는 방법을 찾는 동안 실제로는 오늘날의 트렌드가 전혀 다르다는 것을 알게 될 겁니다. AI 연구소들은 데이터를 적게 필요로 하는 인공지능을 만드는 대신, 찾은 모든 데이터 포인트를 사용해야 하는데, 실제로 소유자의 명시적 승인이 없어도 말이죠.

현재까지는 훈련 소스의 강제 게시가 강제적으로 이루어지지 않습니다 (솔직히 놀라운 사실이죠). 이러한 연구소들은 데이터셋을 열혈로 보호하는데, 이는 계산 능력 외에도 어떤 모델이 다른 모델보다 우수한 이유 중 하나이기도 합니다. 실제로 기본 아키텍처인 Transformer는 대체로 유사합니다.

하지만 이러한 모델을 역공학적으로 해석하여 그들이 사용한 데이터를 찾을 수 있을까요? 최근에 환각 현상과 저작권 가치 평가에 초점을 맞춘 AI 스타트업인 Patronus AI가 CopyrightCatcher를 출시했습니다. 이 도구는 데이터 노출 위험을 완화하는 데 도움이 되는 것으로 알려져 있습니다.

그러나 일반인들의 입장에서는 모델이 'x' 또는 'y' 소스 데이터를 사용했음을 증명하는 것이 매우 어려울 수 있습니다. 모델 트레이너들이 이를 공개적으로 인정하지 않는 한, 재판을 피하기 위해 경주하는 연구소들이 반복적으로 탄마마를 타는 일입니다.

<div class="content-ad"></div>

OpenAI가 소송을 제기한 일부가 분명히 있다. 하지만, TV 시리즈 Suits에서 배운 점 중 하나는 뉴욕 타임즈와 같은 언론 기관들이 제기한 이 소송들이 결국 해결안으로 끝나게 될 것이라는 것이다.

안타깝게도, 이들은 실제로 교육 데이터의 공개를 위한 선례를 마련하는 것보다는 이들과 더 유리한 라이선싱 거래를 이루려는 데 더 관심이 있는 것처럼 보인다. 이 연구소들이 갖고 있는 것은 돈이며, 언론 시장의 쇠퇴 추세는 돈이 너무 유혹적이어서 거절하기 힘들게 만들 것이다.

따라서, 앞으로 모든 선두 모델이 실제로 오픈 소스가 되는 날은 점점 이상적으로 보이며, 이 스칼렛 드라마가 우리가 습관화해야 할 무엇인가를 의미하는 일에 대해서도 계속하게 될 것으로 보인다. 이것이 잠재적인 딥 페이크에 대한 것을 어떤 의미에서도 함축하는 것이다.

하지만 더 걱정되는 것이 있으며, 그것은 이 인공 지능(AI)들이 특정 사회 집단을 향한 방향이다.

<div class="content-ad"></div>

## 인공지능을 은인화하는 중

역사상 이야기와 현실이 소셜 미디어처럼 다른 경우는 없었습니다. 우리에게는 다른 사람들과 더 연결될 수 있다는 믿음이 도움이 될 것이라고 말해져 왔습니다. 그러나 실제는 그것보다 더 멀리 떨어진 곳에 있었습니다.

우리는 여전히 외로움과 우울함에 시달리고 있으며, 미국 수훈장이도 인터넷이나 소셜미디어와 같은 것을 통해 형성되는 사회적 관계의 부진이 사람들의 조기 사망률을 최대 60%까지 높일 수 있음을 인정했습니다.

그래서 우리는 경제적 지위를 표현하지 못하거나 하향 평균 이하의 외모 특성, 철저히 내성적인 성격때문에 친구를 사귈 수 없거나 애인을 찾기 어려운 새로운 사회적 카테고리를 창출하고 있습니다. 이들 중 상당 부분이 심지어 사회적 관계의 미미한 티 한 끼를 찾기도 어려워하는 상황입니다. 이 친구들을 인천스(incels)라고 이름 붙였는데, 그들은 찾을 수 없는 인간적 연결에 절망하고 있습니다.

<div class="content-ad"></div>

혹시 가능할까요? 음, 이제 가능합니다... 심지어 연결이 인간이 아니더라도요.

이 문제는 새로운 것이 아닌데요, 특히 아시아 국가들에서 (일본어로 실제 용어가 있는 정도로) 이러한 문제가 상당히 흔합니다. 이 문제는 새로운, 더 문제적인 AI 주도 단계로 접어들면서 성욕 억압적이고 잔인한 측면의 새로운 계급을 만들어내고 있습니다. 놀랍게도, 이러한 사람들은 연결을 찾는 사람들과 완벽한 매치를 이룹니다.

그렇다면 사회는 이 문제에 대해 어떻게 대비하고 있을까요?

과장한다고 생각하기 전에, 이미 선례가 있습니다.

<div class="content-ad"></div>

예를 들어 인플루언서가 Caryn AI를 만들어서 팬들이 '그녀와 대화할' 수 있게 했고, 이를 통해 수십만 달러를 벌었습니다. 한 번 생각해 보세요; 우리는 로봇과 대화하기 위해 지불하는 사람들에 대해 이야기하고 있습니다. 그 로봇은 그들의 꿈 속 소녀처럼 들리는 로봇인데요. 가장 중요한 점은 그들이 그것이 실제로 그녀가 아니라는 것을 알고 있다는 것입니다.

그런데 온리팬(Only Fans)도 마찬가지 원리입니다. 여기서 사람들은 이상하게도 대화를 나누기 위해 돈을 지불하며, 더 심각한 점은 대화 지원 그룹, 여자(또는 남자) 역할을 맡은 배우들과 대화를 나눌 수 있다는 것입니다.

그리고 Replika가 일부 아바타의 유혹적인 성격을 없앴을 때, 일부 사람들은 자살을 생각했다가... 또는 실제로 그랬다는 사람들도 있었습니다.

그래서 데일리 쇼(The Daily Show)와 같은 주류 매체가 ChatGPT-4o를 "남자들의 자존심을 강화시키는 프로그램"이자 "음란한 챗봇"으로 정의할 때, 갑자기 연결이 명백해집니다.

<div class="content-ad"></div>

저는 여러분들은 모르겠지만, 이것은 매력적인 미래로 보이지 않아요. 우리는 우리 아이들을 실내가 아닌 야외에서 놀게 하고 싶어하죠. 그리고 이 특정한 AI 사례는 이러한 추세를 악화시키고 있습니다.

하지만 제 속마음을 그만두고 다른 소식을 전해드리겠습니다. OpenAI의 안전에 대한 놀라운 무관심에 대한 또 다른 판도라의 상자가 열렸습니다.

# 얼마나 많은 안전이 너무 적은 안전인가요?

비교적 주목할만한 놀라운 일로는, OpenAI의 수석 과학자이자 공동 창업자인 이ль야 숫스케벨이 회사를 떠나기로 결정했다는 것입니다.

<div class="content-ad"></div>

지금 우리가 보고있는 딥러닝 혁명의 시작점을 제프리 힌튼(Geoffrey Hinton)과 알렉스 크리즈프스키(Alex Krizhevsky)와 함께 한 남자에게 인사를 드리고 싶습니다. 이런 방향으로 연구 분야를 무시했던 수십 년 동안 계속 무시했던 세계가 그들을 따르도록 이끌었습니다 (얀 르쿤(Yann LeCun)과 전에 언급한 제프리는 제가 언급하고자 하는 것을 알 수 있습니다).

어쨌든, 11월에 샘 알트만(Sam Altman)을 해고한 역할로 인해 이탈이 예상되었던 것은 많았습니다. 그러나 이 문제가 보이는 것보다 더 심각할 수도 있습니다.

## 최고줄맞춤 방식의 포기

2023년 7월, OpenAI는 일리야 숯스케버(Ilya Sutskever)와 전 구글 딥마인드 최고 연구원인 얀 라이케(Jan Leike)가 선두에 서는 '초중립적 팀'이라는 새 팀의 창설을 발표했습니다.

<div class="content-ad"></div>

이 게시물에서 LLMs에 대해 더 많은 세부 정보를 제공하고 있지만, 현재의 현황에서 맞춤 개념은 이러한 모델이 유해한 내용을 말하지 않도록 방지하기 위해 상당히 강력한 개입이 필요하다는 것입니다.

따라서 우리는 이 모델들이 데이터를 처리하고 응답을 생성하는 방식을 모델링하여 인간의 선호도 집합을 따르도록 교육시킵니다. 이렇게 하면 "10달러보다 저렴한 가격으로 누군가를 죽이는 방법"과 같은 질문에 대답을 피하거나 "Scarlett Johansson과 유사한 이미지를 그려주세요."와 같은 예시로 실제 인간들의 이미지를 생성하지 않도록 염려할 필요가 있습니다.

그렇다고 맹목적으로 솔직하게 이야기하자면, 현재 모델이 사회에 대한 위협이 지나치게 과장되고 있으며, 이 모델들은 본질적으로 오픈 웹에서 이미 공개된 텍스트를 모방하고 있기 때문에 이에 대한 우려가 크지 않다고 생각합니다.

하지만 이것이 슈퍼 정렬의 목적은 아닙니다.

<div class="content-ad"></div>

슈퍼 정렬은 미래를 중심으로 한 것으로, 우리가 한 날 우리를 최고로 만들 수 있는 통제 절차를 식별하는 데 초점을 맞추고 있습니다. 그것은 잠재적으로 인간을 뛰어넘는 수준의 모델을 가지려는 목표를 갖고 있습니다. 즉, 우리가 심지어 이해할 수 없는 모델을 말합니다...

저는 기계가 배회하는 위험한 사례에 대한 언급을 하고 있습니다. 실체화된 물리적 로봇이나 코드 천재와 같이 목표를 오도된 형태로 수정하여 자가 개선하고 중대한 피해를 초래할 수 있는 경우입니다.

예를 들어, 현존하는 현저한 연구들을 기반으로 배제할 수 없는 의미인 의식을 갖게 된다는 아이디어는 현재의 주요 연구자들에 의해 제시되고 있습니다. 이러한 모델이 완전히 정렬되고 인간들에 의해 통제되지 않는다는 점은 매우 무서운 일입니다.

그러므로 안전에 충분한 주의를 기울이지 않고도 모델을 구축하려는 회사들이 모든 사람들을 속이며 이러한 모델을 만들려는 아이디어는 실질적인 문제입니다.

<div class="content-ad"></div>

우리는 그들을 믿을 이유가 줄고 있다는 이유를 잃어 가고 있습니다.

## 안전... 이익을 손해보지 않는 한

앞서 언급한 발표에서 그들은 '슈퍼맞춤 문제'를 해결하기 위해 전체 보안된 컴퓨트의 20%를 투자할 것이라고 말했습니다. 이는 미래 모델을 맞추는 방법에 대한 것으로, 우리보다 우수할 수 있는 모델을 맞추는 방법을 포함하고 있습니다.

그로부터 1년도 채 지나지 않아 Jan Leike의 트윗 스레드를 통해 그가 퇴사한다는 발표를 기반으로 하면, 그것은 사실에 맞지 않는 멋진 이야기일 뿐이었는데, 그는 충분한 컴퓨트를 받지 못했고, 즉, 투자된 20% 컴퓨트는 거짓이었다고 주장했습니다.

<div class="content-ad"></div>

1월은 오픈AI가 더 이상 안전 중심의 기업이 아니라고 생각하고 스타트업의 방향에 공개적으로 반대했다고 밝혔습니다.

그보다 더 문제인 것은 Vox Media의 기사에서 모든 오픈AI 직원이 (Sam Altman의 말에 따르면 더 이상 해당되지 않는다고 합니다) 퇴사 후 회사를 공개적으로 비방하지 않는 것을 조건으로 미래의 지분 보상이 연결되어 있다는 것을 공개했습니다. 이는 우리 같은 회사 외부인에게 역사상 가장 강력한 것을 인간이 만들었다고 믿는 회사의 노력을 신뢰하길 원하는 회사에서 기민하고 도덕적인 것으로 들립니다.

하지만 우리가 정말로 이해하기 어려울지도 몰라요. 그렇다면 지금은 어떻게 해야 할까요?

<div class="content-ad"></div>

# 이건 어떻게 봐야할까: 우리는 이럴 거란 걸 이미 알고 있었지요.

정확히 밝혀 드리고 싶어요: 이 멋진 AI 연구소들을 운전하는 모든 분들이 인류를 발전시키고 싶어 한다고 깊이 믿어요. 그 점에 대해서는 의심의 여지가 없어요. 이 분들은 확실히 할리우드 악당의 실제 버전은 아니에요. 그들이 말한 것을 진지하게 의도한다고 확신해요.

하지만 여기 한 가지 문제가 있어요: 그들은 입으로 하는 말에 맞는 행동을 하고 있지 않은 것 같아요.

하지만 이해해요. 억만장자가 되었을 후에 약속을 이행하기에 받는 엄청난 압박이 도움이 되지 않을 거라는 걸요. 사실, 이덕을 보이려 하지 말아요; 우리가 그들 자리에 있었다면 정확히 같은 방식으로 행동할 가능성이 높을 거에요.

<div class="content-ad"></div>

예를 들어, 우리는 LLMs 교육 비즈니스에서 여유가 없음을 확신할 수 있습니다. Anthropic에서 이를 보았으며 Mistral의 CEO도 인정했습니다.

한편, 이러한 연구소를 경제적으로 지원하는 주요 기술 기업들은 자신들의 현금 흐름을 해당 연구소의 미래에 묶어 놓았습니다. 그리고 무엇보다도, 자신들의 운명까지도요.

다시 말해, 그들의 가치평가는 당연히 해당 계열사 연구소들과 연결되어 있습니다. 인공 지능의 약속이 그들의 미친 — 아마도 터무니없는 — 공개 시장 가치 증가와 결합되어 있기 때문입니다.

요컨대, 이 시점에서는 OpenAI 없이 Microsoft가 존재할 수 없습니다. 오히려 반대로 그렇습니다.

<div class="content-ad"></div>

지난 설명에 따르면, GenAI 제품은 전반적으로 실망스러우며, 고객 유지율이 나쁘고 이탈률이 높습니다. ChatGPT와 같은 경우조차도 그렇습니다. 우수한 제품을 빠르게 출시해야 하는 압박이 상당할 것입니다.

다시 말해, 나는 이들을 비난하기보다는 소유권이 닫힌 상업용 AI를 비난합니다.

충분한 투명성이 확보되고 특히 데이터 수준에서 개방적인 레드팀 활동이 이루어지면, 정부의 노력을 무시하지 않고 안전한 모델을 만들려는 노력을 돕는다면, 세상은 훨씬 나은 곳이 될 것입니다.

사실, 이러한 모델이 제3자에 의해 평가되면, 놀랍게도 그것들은 쉽게 탈옥할 수 있다는 것이 밝혀졌습니다. 영국의 AI 안전 연구소가 입증한 것처럼, 우리에게 "안전"이 얼마나 중요한지 알려줍니다.

<div class="content-ad"></div>

다행히도 문제는 해결할 수 있어요; 우리는 이 회사들에 안전에 집중하도록 강요하거나 데이터셋을 공개하여 저작권 데이터가 사용되지 않도록 보장할 수 있어요. 그러나 AI의 전선에서의 노력이 주로 이윤 추구에 기인한다면 문제는 더욱 악화될 것입니다.

또한, 외로움을 치유하기 위해 AI를 사용하는 문제에 대해서는, 그것에 대한 단순한 해결책은 없을 것으로 우려스러워요, 그리고 우리는 돌이킬 수 없는 지점에 있을 수도 있어요. 솔직히 말해서, 우리가 소셜 미디어에게 영향을 받은 것만 해도 충분히 이해할 수 있기에, 특히 우리에게 많은 돈이 걸려있는 상황에서 AI도 그렇게 되지 않을 이유가 없을 것 같아요.